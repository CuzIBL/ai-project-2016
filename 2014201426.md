#BOT图片分类
##1.任务目标
	给定十二种不同的动物图片，用caffe训练出模型来判断新的图片属于哪一种动物。
##2.数据预处理
####(一) 获取训练样本
	样本来源：BOT图片集
####(二)划分训练集和测试集
	将BOT数据集的rar压缩包解压，得到预览集和大的数据集，将预览集留作测试，大的数据集用来训练，
	大数据集划分成了train set和development set（用于调节参数防止过拟合）
	处理后得到train.txt,test.txt，txt文件每一行包含图片名和标签（代表所属的类别）和picture文件夹中包含所有图片，
	由于图片包含jpg，jpeg，png，gif格式，在转换为lmdb时会报错（无法打开文件），
	所以这里将所有的非jpg格式的文件转换成jpg（用imagemagick转换），这里可能会损失图片质量，导致最后准确率无法提升。
	代码：	
			python merge.py
			python getTrain.py
			mv data/Dog data/dog
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/1.png) 

####(三)转换成LMDB格式
	 在转换成lmdb格式时要制定图片resize的width和height，这里对应着prototxt中的cropsize，
	 最开始的实验中我在这里使用了256×256,而cropsize设成了32，导致每一次都只截取了图片的一小部分进行训练和测试，
	 导致训练和测试的准确率都只有百分之三十多。
	 代码：bash con2jpg.sh
##3.模型训练
####resnet-20层
	借用了处理cifar10数据集的参数选择，训练和测试的batchsize都设为100
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/2.png) 
##4.训练结果   
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/3.png) 

	可以看到在五万多次左右train set的准确率在不断上升，test set的准确率却在70%左右不变。
####finetune
	所以在保持这个训练到64000次迭代的同时，用51500次迭代后保存的model用测试集进行fine-tune
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/4.png) 

	开始finetune使用训练的起始lr=0.1,但是发现loss太大了，于是立刻停止将lr改为0.01
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/5.png)

#####solver2.prototxt
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/6.png)
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/7.png)

	然后再用新得到的10000次迭代后的weight去finetune训练集
	可以看到在5000次迭代左右测试集准确率稳定在79%左右，测试集在83%到93%
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/8.png)

##5.测试
	分别用64000次迭代不加finetune所得到最后的模型和51500次迭代加上10000次finetune所得到的模型对预览集进行测试
	
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/9.png)
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/10.png)

	从测试结果中可以看出用development set finetune 所得到的模型更具有普适性，更能准确的描述测试集。
##6.反思：
	测试集准确率没有达到90%，考虑有以下几点原因
	一：在转图片格式为jpg以便处理成lmdb格式时损失了信息（个人认为是主要原因）
	二：没有gpu跑起来的速度太慢无法训练更多以尝试不同的参数来达到更高的准确率

####运行
	python merge.py
	python getTrain.py
	mv data/Dog data/dog
	bash con2jpg.sh
	python changeTag.py
	python train.py
	bash getLMDB.sh
	bash train.sh

------------------------------------------------------------------------------------------------

#甄别真假论文
##1.任务目标
	给定一篇（些）论文，判断其是否为真论文，即非scigen生成的论文。
##2.数据预处理
####(一) 获取训练样本
	正样本来源：由老师给定的三千多论文样本
	负样本来源：于MIT scigen上爬取下来的论文，原打算爬三千多篇论文下来，由于有大量重复，最后只有一千六百多篇。
	代码：crawelFalse.py

####(二)转换pdf
	用linux自带的pdftotext将pdf转化为txt
	在pdf转化为txt已经自动进行了去除图和表格
####(三)处理txt
	去除标题和作者信息
	去除Reference 信息
	去除论文引用的标号 []
	将其他括号，标点符号替换为空
	所有单词转化为小写
	数字归一化处理
##3.判定
####聚类
	利用文档距离作为聚类标准，对于一篇论文直接找最近距离的效果并不是很好，
	而是用类似KNN的思想，求出距离后，进行排序，看更接近哪个类。
	
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%871.png) 
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%872.png) 

	代码：JudgeBow.py
####结论

![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%876.png) 

	可以看出效果已经非常的好

####词同现网络
1.词同现网络简介及网络构建

	词同现网络：若两个词汇在同一单元（如邻接、段落）中共现，则认为它们存在关联关系。
	词同现网络的思想是：对于每一个单词，对应词同现网络中的一个节点。若一个句子中的两个词之间间隔为n时存在词同现关系，即在网络中有边相连。
	而大量的实践表明，n=2是比较合适的。两个词在句子中相邻出现是比较常见的。
	如：zhe xue 间距为1，boy next door的boy 和door间距为2。
	而如果n取过大，会引入大量的无关的词语，增加模型的复杂度。
	在构建的词同现网络中，段和段之间相对是独立的，而一个段落里说明的内容是相关的。即一个段落中的两个相邻句子之间是有关联的。对于一个段落中相邻的两个句子，将其处理为第一个句子的最后一个词和第二个句子的第一个词之间的间距为2。
 
	对于一篇论文的词同现网络，计算出它的：结点数、边数、平均度数、平均路径长度、网络直径、聚集系数，并且用这六个特征来表示这个网络。
	
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%874.png) 

	参考：基于频繁词集词共现网络的短文本聚类方法
      基于词同现网络与支持向量机的科学论文甄别方法研究

2.分类器的选择

	分别尝试了SVM和KNN进行训练和识别，结果如下：
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%875.png) 

####3.结论：

	可以看出在训练集比测试集接近2：1的情况下准确率可以达到100%（只有几篇文章被判断错误），
	如果有大量的数据集，使得数据比列接近5：1则效果会更好。

####词向量：word2vec+svm
#####数据预处理上多了一个步骤：
	去除停顿词且长度小于四的单词后用空格连起来，
	每篇文章一行，训练和测试各放一个文件夹，
	真论文和假论文各放文件夹中的一个文件中，每个文章一行。
***
	代码： getSegment.py utils.py
	def make_seg(filename,Dir,frequency=0):
		raw=openfile(filename)
		tokens = word_tokenize(raw)
		tokens=[t.lower() for t in tokens if t not in string.punctuation and t.isalpha()]
		lemmatizer = nltk.WordNetLemmatizer()
		tokens = [lemmatizer.lemmatize(token) for token in tokens]
		stopwords1 = stopwords.words('english')
		tokens = [token for token in tokens if token not in stopwords1]
		tokens = [token for token in tokens if token not in newstop]
		fdist = nltk.FreqDist(tokens)
		tokens = ([w for w in tokens if len(w)> 3 and fdist[w]>frequency])
		filename = filename.split("/")[-1][:-4]
		f = open(Dir+filename+".txt","w")
		f.write(' '.join(tokens)+'\n')
		f.close()
		return fdist
***
	
	用gensim 自带的word2vec生成文章中词汇的词向量，
	然后每篇文章的词向量由单词的词向量加起来归一化后所得。
	128维向量
	    model=word2vec.Word2Vec(size=128,window=6,cbow_mean=0,sample=1e-4,hs=1,negative=0,workers=12)

        for word in line:
            try:
                vec=vec+model[word]
            except:
                wa+=1
        fout.write(str(ss)+' ')
        vec=preprocessing.normalize(vec)[0]
        for i,num in enumerate(vec):
            fout.write(str(1+i)+":"+str(num)+' ')
        fout.write('\n')    
        
####结论
	然后用svm训练词向量得到模型去预测测试集中的论文，结果100%，
	比用传统机器学习的方法准确率更高且速度更快。
	
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/233) 
####得到TXT-final后运行

	python getSegment.py
	python train_w2v.py
	python getvec_w2v.py
	python predict.py


-------------------------------------------------------------------------------------------
#判别论文类别
##1.任务目标
给定一篇（些）论文，判断其属于哪一领域的论文
##2.数据预处理
####(一) 获取训练样本

样本来源：网上收集的四大类计算机领域的论文
####(二)转换pdf
用linux自带的pdftotext将pdf转化为txt
在pdf转化为txt已经自动进行了去除图和表格
##3.判定
	利用文档距离作为聚类标准，对于一篇论文直接找最近距离的效果并不是很好，
	而是用类似KNN的思想，求出距离后，进行排序，看更接近哪个类。

![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%871.png) 
![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%872.png) 

	代码：KNN.py  +最低被计入词汇表的单词频率（不加默认为0）
####结论

![](https://github.com/Victorianuonuo/ai-project-2016/blob/master/2014201426/pictures/%E5%9B%BE%E7%89%873.png) 

	可以看出效果已经非常的好,不过由于样本量太少，仍需改进,
	或者尝试词向量的方式。
