#1.任务目标
	给定一篇（些）论文，判断其是否为真论文，即非scigen生成的论文。
#2.数据预处理
##(一) 获取训练样本
	正样本来源：由老师给定的三千多论文样本
	负样本来源：于MIT scigen上爬取下来的论文，原打算爬三千多篇论文下来，由于有大量重复，最后只有一千六百多篇。
	代码：crawelFalse.py

##（二）转换pdf
	用linux自带的pdftotext将pdf转化为txt
	在pdf转化为txt已经自动进行了去除图和表格
##（三）处理txt
	去除标题和作者信息
	去除Reference 信息
	去除论文引用的标号 []
	将其他括号，标点符号替换为空
	所有单词转化为小写
	数字归一化处理
#3.判定
##聚类
	利用文档距离作为聚类标准，对于一篇论文直接找最近距离的效果并不是很好，而是用类似KNN的思想，求出距离后，进行排序，看更接近哪个类。
![](/home/victoria/Pictures/Wallpapers/图片1.png) 
![](/home/victoria/Pictures/Wallpapers/图片2.png) 
	代码：JudgeBow.py
![](/home/victoria/Pictures/Wallpapers/图片6.png) 

	可以看出效果已经非常的好

##词同现网络
	1.词同现网络简介及网络构建

	词同现网络：若两个词汇在同一单元（如邻接、段落）中共现，则认为它们存在关联关系。
	词同现网络的思想是：对于每一个单词，对应词同现网络中的一个节点。若一个句子中的两个词之间间隔为n时存在词同现关系，即在网络中有边相连。
	而大量的实践表明，n=2是比较合适的。两个词在句子中相邻出现是比较常见的。
	如：zhe xue 间距为1，boy next door的boy 和door间距为2。
	而如果n取过大，会引入大量的无关的词语，增加模型的复杂度。
 
	在构建的词同现网络中，段和段之间相对是独立的，而一个段落里说明的内容是相关的。即一个段落中的两个相邻句子之间是有关联的。对于一个段落中相邻的两个句子，将其处理为第一个句子的最后一个词和第二个句子的第一个词之间的间距为2。
 
	对于一篇论文的词同现网络，计算出它的：结点数、边数、平均度数、平均路径长度、网络直径、聚集系数，并且用这六个特征来表示这个网络。
![](/home/victoria/Pictures/Wallpapers/图片4.png) 

	参考：基于频繁词集词共现网络的短文本聚类方法
      基于词同现网络与支持向量机的科学论文甄别方法研究

	2.分类器的选择
	分别尝试了SVM和KNN进行训练和识别，结果如下：
![](/home/victoria/Pictures/Wallpapers/图片5.png) 

	3.结论：
	可以看出在训练集比测试集接近2：1的情况下准确率可以达到100%（只有几篇文章被判断错误），如果有大量的数据集，使得数据比列接近5：1则效果会更好。

##词向量：word2vec+svm
#####数据预处理上多了一个步骤：	去除停顿词且长度小于四的单词后用空格连起来，每篇文章一行，训练和测试各放一个文件夹，真论文和假论文各放文件夹中的一个文件中，每个文章一行。
***
	代码：processTxt.py getSegment.py
	def make_seg(filename,Dir,frequency=0):
		raw=openfile(filename)
		tokens = word_tokenize(raw)
		tokens=[t.lower() for t in tokens if t not in string.punctuation and t.isalpha()]
		lemmatizer = nltk.WordNetLemmatizer()
		tokens = [lemmatizer.lemmatize(token) for token in tokens]
		stopwords1 = stopwords.words('english')
		tokens = [token for token in tokens if token not in stopwords1]
		tokens = [token for token in tokens if token not in newstop]
		fdist = nltk.FreqDist(tokens)
		tokens = ([w for w in tokens if len(w)> 3 and fdist[w]>frequency])
		filename = filename.split("/")[-1][:-4]
		f = open(Dir+filename+".txt","w")
		f.write(' '.join(tokens)+'\n')
		f.close()
		return fdist
***
	
	用gensim 自带的word2vec生成文章中词汇的词向量，然后每篇文章的词向量由单词的词向量加起来归一化后所得。
	128维向量
	    model=word2vec.Word2Vec(size=128,window=6,cbow_mean=0,sample=1e-4,hs=1,negative=0,workers=12)

        for word in line:
            try:
                vec=vec+model[word]
            except:
                wa+=1
        fout.write(str(ss)+' ')
        vec=preprocessing.normalize(vec)[0]
        for i,num in enumerate(vec):
            fout.write(str(1+i)+":"+str(num)+' ')
        fout.write('\n')    
        
	然后用svm训练词向量得到模型去预测测试集中的论文，结果100%，比用传统机器学习的方法准确率更高且速度更快。
![](/home/victoria/Pictures/233) 
	python getSegment.py
	python train_w2v.py
	python getvec_w2v.py
	python predict.py


-------------------------------------------------------------------------------------------



#1.任务目标
给定一篇（些）论文，判断其属于哪一领域的论文
#2.数据预处理
##(一) 获取训练样本

样本来源：网上收集的四大类计算机领域的论文
##（二）转换pdf
用linux自带的pdftotext将pdf转化为txt
在pdf转化为txt已经自动进行了去除图和表格
#3.判定
利用文档距离作为聚类标准，对于一篇论文直接找最近距离的效果并不是很好，而是用类似KNN的思想，求出距离后，进行排序，看更接近哪个类。
![](/home/victoria/Pictures/Wallpapers/图片1.png) 
![](/home/victoria/Pictures/Wallpapers/图片2.png) 
代码：KNN.py  +最低被计入词汇表的单词频率（不加默认为0）
![](/home/victoria/Pictures/Wallpapers/图片3.png) 
可以看出效果已经非常的好，不过由于样本量太少，仍需改进。
