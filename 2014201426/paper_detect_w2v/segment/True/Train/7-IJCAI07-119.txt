kernel discriminant nonlinear dimensionality reduction feature extraction wide involving highdimensional data gene text data develops effectively integrating boosting boosting kernel discriminant bkda posse appealing property like kernel handle nonlinearity disciplined also computationally attractive introducing pairwise discriminant discriminant criterion simultaneously employing boosting robustly adjust improves classification calculating discriminantinformationin null scatter operator also effectively deal sample size widely encountered fourth taking boosting constitutes ensemblebased framework gene data demonstratethe promising linear discriminant classical feature extraction dimensionality reduction broadly used many task involving data believed classification face tissue classification gene data outperform former optimizes classification maximizing ratio scatter scatter latter simply optimizes reconstruction taking consideration many attractive masip effectively integrate boosting enhance classification hand major linear subspace fail extract nonlinear feature representing statistic overcome limitation kernel dimensionality reduction kernel kpca scholkopf kernel discriminant baudat anouar qian xiong yeung zheng extend linear dimensionality reduction nonlinear kernel trick demonstrating many linear counterpart idea data feature nonlinear mapping linear dimensionality reduction linear counterpart kpcabased classification task suffer sample size training dimensionality feature data overcome many criterion qian zheng calculate optimal discriminant vector null scatter operator discriminant hand lotlikar kothari tang yeung degrades deficiency referred classification optimal criterion scatter classification dimensionality reduction procedure tends overemphasizethe outlier expense close leading overlap scatter animplicit covariance fact dominant covariance simultaneously outlier scatter fail classification minimizing spread outlier neglecting covariance proposing novel boosting bkda effectively integrates boosting pairwise discriminant bkda employ boosting robustly calculate pairwise discriminant integrated scatter operator boosting studied much worthwhile mention appealing property bkda handle nonlinearity disciplined alsocomputationally attractive like kernel introducing pairwise discriminant discriminant criterion simultaneously employing boosting robustly adjust effectively overcomes classification effectively boost discriminant contained null scatter operator simultaneously deal sample size also boost discriminant orthogonalcomplementof null scatter operator constitutes kernel kdaframework taking boosting demonstrate effectiveness bkda effectively extract discriminant feature tissue classification gene data confirm bkda superior dimensionality reduction term classification nonlinear feature extraction kernel discriminant nonlinear feature note infinitedimensional regarded hilbert scatter matrix correspond operator operator null orthogonal complement thus training belonging vector containing xiio denoting ioth xiio label implicit nonlinear mapping xiio scatter operator scatter operator population scatter operator expressed maximize fisher criterion optimal wtsw wtsww many practical major problemswith sample size degeneracy withinclass scatter operator applying pseudoinverse kernel qian zheng explore discriminant null scatter operator discriminant calculate optimal discriminant vector intersection subspace modified criterion many nonlinear feature extraction criterion enhances overallperformancein term classification accuracyand stability pointed chen conventionallda dimensionality training discriminant null scatter operator lost case discriminant discarded avoided choosing kernel appropriately dimensionality nonlinearly mapped training boosting kernel discriminant learner boosting kernel discriminant also suffers bkda combine strength boosting effectively solves time boosting machine learning improving learning boosting referred adaboost used many learning boosting adaptive sense classifier built tweaked favor misclassified classifier freund schapire idea adaboost sample distribution essence hard classify notable multiclass adaboost outperforms many thus prefer effectively overcome simultaneously form connection freund schapire pairwise discriminant distribution introduced mislabel distribution iteration pairwise discriminant distribution calculated mislabel distribution extent difficulty discriminating improper label boosting obviously intuitively worse separability embodying also closer address replace ordinary scatter operator scatter operator weighting monotonically simplicity obviously pairwise discriminant distribution weighting seen well separated thus potentially impair classification heavily weighting scheme also employed alleviate outlier estimating scatter operator iteration replace ordinary scatter operator withinclass scatter operator xiio highlight incorporating ensures influenced slightly outlier reasonable well separated covariance operator compact much classification tang difficulty classifying xiio boosting qiit emphasizes difficult covariance bkda pointed bkda calculate optimal discriminant vector null take discriminant procedure subsection discriminant feature extraction classification boosting process forward sampling procedure employedto artificially weaken correspondingdiscriminant bkda hardest feature extracted discriminant nearest neighbor classifier employed classification consistent adaboost bkda hypothesis built normalized nearest neighbor classifier give identical classical nearest neighbor classifier calculate optimal discriminant vector feature efficiently calculate discriminant null variant variant also population scatter operator optimal discriminant vector criterion criterion culate calculating computation intractable extent intractable eigenanalysis expressed operator explicit computation infeasible cevikalp indirectly calculated calculating high computational time efficiently training xiio mislabels mislabel distribution tmax calculate term hardest qiit form training subset subsection constitute feature extraction yiio build hypothesison subset yiio calculate tmax break mislabel distribution xiio xiio yiio yiio normalize xiio final hypothesis nonlinear feature vector extracted subsection bkda subspace subspace population scatter operator thus step calculate optimal discriminant vector calculate orthonormal calculate orthonormal construct calculate optimal discriminant vector discriminant criterion computation procedure need late orthonormal applying kpca rewritten dimensionality high even kpca carried eigenanalysis size training xiio matrix kernel trick expressed matrix term eigenvalue eigenvector constitute orthonormal qian zheng transformed kpca identity matrix training xiio xiio mapped riio scatter matrix simplicity adopts reconstructed training riio kernel trick scatter matrix ptsbp ptswp kpca calculate null eigenanalysis matrix eigenvectors zero eigenvalue spanned discriminant criterion transformed ztvtptsbpvz eigenvectors vtsbv sorted descending eigenvalue qian zheng clear pvzl constitute optimal discriminant vector criterion nonlinear feature vector extracted procedure rewritten kernel trick matrix term identity matrix gene data involving gene sample size thus effectively extract discriminant feature play role gene data classification bkda conduct gene classification bkda dimensionality reduction mentioned classification data involving demonstrate tumor tumor cancer type subset tumor tumor cancer type subset tumor tumor normal tissue type eight data partitioned disjoint training test training data challenging data data preprocessing gene data feature extraction mean rule euclidean classification simultaneously build normalized hypothesis bkda repeated time classification rate reported kernelmethods kernel kernel noted bkda effectively boost discrimination feature extracted bkda reduce computational cost simultaneously limitation simply feature bkda data bkda zheng kernel zheng effectively calculate discriminant null scatter operator gene data classification essence case highdimensional show effectiveness bkda baseline classification rate variant feature classification rate fixed feature bkda training reveal bkda capable improving kernel data specially bkda effectively feature find fixed bkda fails show effectiveness learner weak comparative bkda tmax bkda bkda training bkda test bkda training test kernel kernel kernel subset kernel subset kernel subset kernel subset bkda linear dimensionality reduction bdlda niyogi kernelbased nonlinear dimensionality reduction kpca scholkopf baudat anouar kdda wkda yeung xiong classification rate data bkda extensive classifier tissue classification gene data statnikov neighbor classifier classification rate data tumor subset subset poly poly poly kpca kdda wkda bkda bdlda tree naive bayes classifier bagging boosting vector machine comparative dimensionality reduction feature extraction past dimensionality reduction playing role face also bkda face yaleb database showing bkda outperforms dimensionality reduction limitation conclusion novel incorporating boosting give bkda effectively integrates strength boosting give framework nonlinear feature extraction simultaneously overcomes sample size encountered extensive empirical bkda many linear nonlineardimensionality reduction gene data classification show bkda promising acknowledgment competitive earmarked grant grant council hong kong administrative
