goal inductive learning generalize well induce accurately hansen salamon showed combining prediction separately trained neural network network backpropagation violated procedure network near origin mean backpropagation reach subset initializing neural network competitive learning intelligently network originally located origin thereby potentially reachable datasets network initialized generalize network initialized main goal classification learning able induce accurately classifies difficulty achieving good learner learner relies inductive bias hopefully accurate classifier scheme introduced address ubiquitous ranging treepruning tree quinlan term neural network hinton address neural network trained backpropagation rumelhart employ partially grant grant previously explored drucker hansen salamon lincoln skrzypek rogova training network uaan network combining novelty type network combine composite classifier backpropagation minimizes cost made learner perhaps term straightforward cost squared predicted target training data backpropagation learns network reduce cost minimizing cost optimal harmed learner inappropriate network topology parameter learning learning rate inappropriate learn learner overt classify noisy data many introduced penalty term cost favor hinton avoid overfitting stopping criterion like patience fahlman lebiere employing validation lang focused choosing network growing fahlman lebiere shrinking network topology exploring topology opitz shavlik combining achieving good great deal empirical exploration coding obvious disadvantage time spent training network disadvantage mitigated trivial parallelize training network trained separately hansen salamon combining prediction classifier work mistake made network combined phrase combining network shorthand mean combining prediction made neural network cost parameter fall dashed line reached gradient marked cross even many must network independence network expect greatly vary network training topology network learning parameter network initializing network investigated empirically good combining network user vary initializing network mean resulting network truly merely network initialized suggest interdependent network initialized normal used idea parameter bias neural network much normal backpropagation seek normally encountered initialization explored mean goal diverse best employing competitive learning rumelhart zipser prototype used hidden unit network moody darken outline competitive learning neural network show data demonstrating work conclusion combining network idea combining neural network train network somehow collection show framework combining network combining prediction kearns seung voting scheme hansen salamon lincoln skrzypek scheme perrone cooper rogova scheme training combiners rost sander wolpert chose combining network scheme simply averaging unit portfolio suggest good extra work mani hansen salamon explored combining network demonstrated network combined rate trained network hansen salamon demonstrate zero necessarily outlier predicted rate necessarily zero percentage predicted gain achieved hansen salamon network combined production neural network initialized tends confine exploration network area training mean network find network interdependent prediction investigated avoided bias focused changing mean initializing network network obvious much generating network work well term producing resulting network suffer flatspot fahlman backpropagation unable refine network activation hidden unit tend close network unit highly tried pick training data prototype hidden unit prototype feature prototype also suffered flatspot feature hidden unit inactive learned multiplied actwatwn activation backpropagating logistic activation cost unit activation near mean little propagated netinput want hidden unit activation need netinput netinput neton netinput desired activation next hidden unit competitive learning seed hidden unit calculate part subclass part subclass call avginclass avgother calculate multiplier bias unit subclass subclass avginciass neton multiply hidden unit bias unit detector subclass repeat process subclass producing network backpropagation recall main goal network widely dispersed variability network come main seed training explained computertime limitation held seed varying seed variability work judge combining neural network four network train network meant neural network train train network selecting network best training data obvious network network best training data network best test data oracle train network selecting oracle network generalizes best practice oracle good baseline surface best train network combine prediction initializing network trained selecting bias centered zero competitive learning test domain test domain handwritten digit shen protein data qian sejnowski data digitized numeral recognize digit binary data subsequence amino acid amino acid take unit central amino acid part helix sheet coil train network cost predicted mitigate overfitting data term cost decay hinton data used validation lang test decay produced poor data cost evaluating trained network interpret unit highest activation predicted testset comparing predicted also classifier five test data crossvalidation data divided possibly composite classifier data classifier test data nine training data note learn training data test data aside learning calculate testset five classifier naturally network classifier train network network achieves lowest training classifier classifier network cheat applying network test data network achieves lowest test data network prediction network competitive learning digit data competitive learning five subclass type digit thus hidden unit five subclass digit network suggested empirical test data cherkauer communication data used hidden unit qian sejnowski concluded used competitive learning helix subclass sheet subclass coil subclass reflect distribution data test four pair backpropagation initialization initialization competitive data used network hidden unit testset rate reported correspond four listed network choosing network network accurate training data choosing network accurate testset mean prediction network data best achieved network initialized competitive learning even network necessarily well reduction network competitive learning training neural network statistically confidence data apparently network initialized competitive learning lead decrease testset note oracle even oracle seem ideal baseline note best reported rate task list shen proteinfolding data best reported rate neural network qian sejnowski neural network maclin shavlik reasoning data leng reasoning also used encoding network combined expect show sharp drop combining netwe rate network network combined initialization initialization competitive learning work gradual decrease conclusion combining prediction neural network wise algorithmically lead sizable reduction testset trained network reduction hold even employ initializing network work plan creating like varying network architecture hidden unit training varying training parameter learning rate also intend optimal network plan scheme filtering network mean recognize network hurt area work combining prediction long history area forecasting granger neural network looked combining prediction lincoln skrzypek hansen salamon explore combining network many ghosh hashem perrone cooper rogova wolpert studied combining prediction robust taking main difficulty pursuing time protein folding train network repeat network fold network take little training confidence prediction learning system neural network learn combine prediction rost sander jacob combining network step evolve subnetworks subnetwork good prediction choosing vary training process network reilly architecture schapire drucker boosting hampshire waibel baxt training network task perrone neural network combining competitive unsupervised backpropagation supervised learning huang lippmann moody darken main work network producing network used focused producing network aware close relationship moody darken goal wanted network partitioning differs moody darken employ sigmoidal unit gaussian unit initialization phase supervised backpropagation learning throughout network hidden work also closely nguyen widrow initializes hidden unit responsible main work competitive learning trying work also relates system unsupervised supervised learning neural network huang lippmann work unsupervised learning separately also transform competitive learning multiplier producing install competitive learning network backpropagation adjust resulting subclass conclusion goal inductive learner generalize well classifier accurately predict straightforward improving combine prediction separately trained neural network productively combined network largely network idea competitive learning network competitive learning network origin suffer greatly slow backpropagation training thus capable reaching wider reachable network initialization step competitive learning cluster subclass step neural network subclass recognized hidden unit backpropagation refine resulting network repeat process time network generalize well testbeds make evidence merit improving novel improves task namely initialization network trained
