learning face robot learning multiagent learning great deal multiagent reinforcement learning stochastic game intuitive mdps agent work game hundred hand robot task continuous robot learning task extensive learning grawolf scalable multiagent learning combine learning wolf learn fast learning rate adversarial multirobot task simultaneous learning show learning simulation real robot demonstrate grawolf learn successful overcoming many learning learning learning containing robot robot goal learning well adapting robot make stationary violating markov property singleagent learning relies upon learning combine multiagent learning learning robot continuous training data great deal work multiagent learning looked learning stochastic game littman bowling veloso greenwald hall stochastic game natural markov process mdps agent well studied game simultaneously optimal nash multiagent system equilibrium equilibrium simply player playing optimally powerful game even learning agent learn agent playing equilibrium multiagent learning stochastic game thus game enumerable robot learning task continuous couple dimension discretizations enumerable also well data considerably costly gather million training learn feasible robot learning make learning tractable training experience combine wolf multiagent learning show empirical applying simultaneous learning adversarial robot task give brief multiagent learning formal stochastic game adversarial robot task learning main grawolf gradient ascent wolf learning rate applying adversarial robot task multiagent learning multiagent learning focused game theoretic framework stochastic game stochastic game tuple agent agent joint transition agent look framework agent selecting next depend joint agent agent grawolf learn fast sound gradient goal agent maximize discounted discount stochastic game natural mdps agent also matrix game stochastic game viewed matrix game payoff joint matrix playing matrix game receiving payoff player transitioned matrix game joint stochastic game mdps matrix game subset framework stochastic deterministic multiagent exploited agent child game player play deterministically player time selecting defeat fact consideration mixed stochastic stochastic mixed probability distribution player show stochastic also learning nash equilibrium even stochastic optimal player player optimal major advancement driven much matrix game game even stochastic game equilibrium nash equilibrium nash nash equilibrium collection player player player player changing player also make equilibrium compelling matrix game stochastic game equilibrium possibly equilibrium game like adversarial task explored nash learning stochastic game stochastic game area reinforcement learning explored learn equilibrium experience player littman greenwald hall iteratively equilibrium game learner claus boutilier bowling veloso learner optimize adversarial robot task robot trying inside circle bottom robot trying stop line show player possibly changing connection equilibrium converge playing must equilibrium bowling veloso neither scaled game hundred game game continuous make enumeration intractable stated form enumeration major limitation examine learning adversarial robot task thought continuous stochastic game build idea learner gradient bowling veloso robot task adversarial robot task adversarial robot task robot attacker trying reach circle robot closer circle defender trying prevent happening attacker reach circle receives defender receives task attacker reach circle elapses trial robot reset attacker meter circle defender robot simultaneously learn seeking maximize discounted discount used full delay multiagent system robot part cmdragons robot soccer team competes robocup league robot relevant learning task learning situated architecture team employ system mounted processed elaborate tracking module accurate velocity robot comprise learning team also robust module obstacle avoidance control learning target obstacle avoidance module situating learning architecture learning robot learn well understood like path planning tracking learning directed heart multirobot system control loop partially inherent latency elapse robot response latency overcome robot velocity predicting latency past opponent robot prediction robot latency effectively observability agent fact view also tactical successful robot fake opponent robot changing suddenly knowing robot able respond full latency task numerous multiagent learning continuous observability system latency violation markov many system memory tracking obstacle avoidance module fact limitation even make equilibrium cease altogether bowling vcloso exploring learning seek learn equilibrium grawolf learning handle learning grawolf grawolf wolf combine idea reinforcement learning gradient learning wolf learning rate gradient learning handle intractable continuous wolf multiagent learning encourages learning converge simultaneous learning briefly combined radicnt ascent gradient sutton colleague gibbs distribution linear feature pair vector parameter identically sized feature vector gibbs distribution defines multiagent system stochastic sutton colleague main convergence iteration rule parameter gibbs distribution parameter taking gibbs distribution sutton colleague showed convergence form gradient ascent derive learning rule visited summation removed updating proportionately contribution visiting onpolicy need discount contribution time passed trial turn whole step updating simultaneously step updating sarsa lsutton barto feature maintaining eligibility trace vector rule time system take transitioning time taking trace vector sarsa parameter cayed learning rate raising take differing time execute process sutton barto step system time sarsa used term form crux grawolf remains learning rate wolf learning rate used learn fast wolf learn fast bowling veloso changing learning rate encourage convergence multiagent reinforcement learning gradient ascent arises simultaneous learning stochastic game agent simply assumed part unchanging wolf agent adjusting quickly slowly agent rate learning guaranteed find locally optimal nonstationary retain property even wolf stochastic game simultaneous learning wolf evidence matrix game empirical evidence game enumerated encourages convergence converge intuition learner adapt quickly poorly cautious player implicitly player learning wolf naturally lends gradient learning rate wolf replace learning rate learning rate used winning losing determination winning losing successful policv time ered winning losing gradient examine vector vector time winning winning parameter task returning robot task grawolf need parameterization need find mapping continuous feature vector filtered velocity robot form navigation observing radial angle attacker circle relevant task arrive seven dimensional seven dimension white overlaid line chose tile coding sutton barto also cmacs construct feature vector tile coding offset tiling fine discretization tile size dimension velocity dimension simplify requiring attacker navigation perpendicular line circle dark overlaid line line time attacker circle discretized seven evenly distributed defender navigates bisects line attacker robot also tiling eighth dimension tile size dimension line distinguishable also agent tenth frame unless feature vector time keep robot oscillating much part learning presenting applying grawolf learning data gathering real robot learning trial practical execute believe demonstrate grawolf practical time real robot standpoint want thorough statistically trial used learning simulator robot team well robot show practical robot extensive evaluating success simultaneous learning show time converging optimal applicable multiagent domain optimal agent optimal changing time agent also learn true agent identical learning success agent necessarily failure hand want learning robot even time main learned learning actually fairly capable agent also challenger examined multiagent system learned opponent simulation line show deviation littman train opponent challenger generality learned challenger demonstrating learned robust wolf learning rate play role keeping learning away exploited graph yaxis show attacker discounted roughly corresponds time take attacker reach circle graph discounted gathered empirically trial training occurred trail take approximately hour training time robot simulation unless noted training done simultaneously learning opponent wolf simulation repeated nine time graph examine learned simulation examine learning robot show learned playing opponent learning middle corresponds player corresponds attacker learned defender corresponds attacker defender learned desired learning learned attacker attacker learned defender demonstrate grawolf improves considerably next explores robust learned wolf show challenger trained simultaneous learning fixed challenger trained trail multiagent system learned playing challenger simulation line show deviation exploitable robust unknown opponent trained challenger learned trial averaging used investigate robustness learned affect wolf learning rate grawolf left side graph show learned defender side show attacker wolf corresponds grawolf slow learning rate wolf winning rate fast losing rate defender attacker fourth column demonstrates learned close equilibrium also mean learned robust difficult wolf learned defender challenger keep challenger learning rate switch attacker learned challenger fast learning rate significantly slow learning rate couple explanation initialization intialized zero side optimistic initialization defender pessimistic attacker attacker mean attacker considers winning defender causing slower learning rate employed time evidence examining percentage training attacker used slower winning rate defender used winning rate initialization grawolf explored certainly dramatic unapproximated bowling veloso wolf learned playing real robot training done half trial simulation last half robot seems converging examine grawolf real robot took trained trial simulation trial training robot evaluated resulting simulator real robot attacker nearly impossible keep reaching circle defender best slow progress even true attacker seen much simulator despite qualitatively identical produced simulation simultaneous learning improves attacker defender adversarial expect good stochastic true learned simulation robot probable probability around attacker learned defender learned nearly deterministic learned stochastic avoid exploitation learning optimal depend agent conclusion introduced grawolf multiagent learning capable learning robot task multirobot even adversarial showed learning adversarial robot task simulation robot experience demonstrated effectiveness gradient learner wolf learning rate noted sutton colleague gradient combine wolf gradient williams baxter bartlett acknowledgement sponsored grant publication necessarily reflect funding agency official endorsement inferred also thank brett browning james bruce robot used work
