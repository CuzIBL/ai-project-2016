windowing procedure memory tree learning lead decrease noisy domain work demonstrated rule learning learn rule independently exploited windowing procedure demonstrate property exploited noisetolerance windowing windowing improving inductive classification learner gain identifying subset training sufficient induced procedure also subsampling windowing quinlan supplement inductive tree learner tackle task exceeded memory despite successful krkn chess endgame domain quinlan windowing played major role machine learning certainly rapid hardware made motivation windowing seem compelling work area database kivinen mannila toivonen intelligent retrieval lewis catlett yang recognized subsampling procedure learning time memory good deal lack attributed empirical wirth catlett showed windowing unlikely gain studied windowing domain concluded recommended procedure improving best achieved noisefree domain mushroom domain learning able windowing noisy domain considerably worse fiirnkranz demonstrated rule learning suited windowing noisefree domain learn rule independently show property exploited conducted framework rule learning gained popularity fiirnkranz learning fiirnkranz widmer successor ripper cohen depend rule learning used achieves learning consistent rule training data pruning rule remaining resulting rule removed training remaining training used learning rule meaningful rule discovered cohen parameter like pruning stopping criterion optimally fiirnkranz widmer used ripper pruning criterion stopping criterion ripper rule optimization heuristic thus ripper also cohen differs closer considers rule pruning considers final hand able handle missing limitation enhancement dealing windowing noise windowing quinlan picking sample size initsize inducing classifier learning case briefly last tested remaining misclassifies moved test window parameter maxincsize keeping window size exceeded tested next iteration window tested iteration take care remain untested iteration tested iteration windowed windowing noisy domain endeavor learning like inside windowing loop lead gain noisy domain contrary true main windowing noisy domain fact eventually incorporate noisy learning window misclassified good hand window subset learning thus iteration proportion noisy learning window much noise data make learning considerably harder learned domain misclassified noise next iteration noisy misclassified window thus doubling size window also contained noise half window erroneous classification window fact mostly assumed many window inherent data hypothesis consistent wirth catlett icatlett windowing highly sensitive noise windowing windowing furnkranz applicable domain rule learning good rule iteration windowing procedure consistent rule window nevertheless iteration rule detected early removed window thus gaining computational fturnkranz achieves separating covered rule consistent windowing iteration learn rule uncovered part idea beginning proceeds like selects subset learns test remaining contrary merely incorrectly classified window next iteration also remove window covered good rule good rule test rule learned window data computes procedure parameter windowing consistency windowing suitable learning data noisy domain noisetolerant learning rule consistent training data thus elaborate criterion must used experimented criterion literature insufficient turned training size likelihood ratio significance test clark niblett deem rule learned even distribution covered deviate slightly distribution training eventually settled criterion rule learned window accwin window acctot furnkranz also selects maxincsize window sampling covered byinsignificant rule regular windowing hope avoid part outlined stick uncovered rule discovered proportion remaining training considerably decrease chance picking sampling also decrease uncovered lead rule discovered part criterion eventually window make optimization minimize testing considers case learning unable learn rule window maxincsize uncovered window deal case doubling window size sample size think lead convergence case systematically tested hypothesis remove semantically redundant rule postprocessing phase rule training also covered rule refer furnkranz subset specified training size preclassified identical data resulted fact training data learned conducted initsize maxincsize well domain fiirnkranz made appropriateness noisy domain tested mushroom database database form rough windowing tree learner gain pure wirth catlett slightly modified windowing used able quinlan left column show runtime parameter term regular windowing domain training size clearly fastest term latter able compensate weakness size data furnkranz widmer significantly worse slightly rule detrimental noisefree domain windowing fact able gain runtime losing thus confirming furnkranz testing series propositional classification task used benchmark learning goal learn rule recognizing illegal chess white king white rook black king propositional domain binary encode validity invalidity like adjacent piece chess domain used testing learned training subsampling artificial noise replacing classification training classification fair coin mushroom domain middle column show domain moderate noise regular windowing gain contrary twice even need training time much outperform term term seems heavily reasonably well little behind size good seems correlation noise data noise confirmed furnkranz domain also series analyzing varying artificial term inconclusive term iwin outperforms noise converse true noise data rule learned window size bear significance thus successively window size able remove covered rule learned iteration consequently much learns data reasonable noise significantly outperforms runtime learning noise time decrease five time highest training size take evidence chance outperforming training size redundancy data binary domain able handle continuous missing nothing prevents dealing like threshold turning task binary task unfortunately able natural domain reasonable size data repository meet decided discretized quinlan thyroid disease simplified domain pruning unpruned tree achieves pruned tree default respective tree size take evidence data moderate noise consequently windowing procedure inefficient take twice long growing tree data parameter default parameter column show domain significantly outperforms data used training testing irip maintains raise suspicion overfits data domain significance test used able extent evaluating predictive simpler rule learned window size training parameter window size window parameter well domain furnkranz encountered evidence parameter suitable noisy domain crucial parameter parameter used significance test employed seen domain good noisy domain must used also seen parameter sensitive lead exploding cost high lead overgeneralization automating highly desirable handle numeric data thresholding affect expect fact fewer threshold size windowing learned rule hypothesis stated catlett never empirically verified fact surprised threshold like contained window gave chance overfitting thus even predictive windowing work successfully redundancy domain rule good learned subset training ftirnkranz dataset hold consequently windowing estimating redundancy domain valuable work subsampling differ windowing tree dynamic subsampling node optimal test idea originally evaluated breiman explored catlett work peepholing catlett sophisticated procedure subsampling eliminate unpromising threshold consideration closely windowing uncertainty sampling lewis catlett window misclassified learner confidence learned classified confidence training next iteration successively learning window john langley extrapolation learning curve promise gain note gain incremental learning work partitioning splitting segment size combining rule learned also produced promising noisy domain substantially decreased learning domain domingo besides seems tailored learning applicable windowing good rule size training window kept final removed training thus size window next iteration window sampling covered insignificant rule covered rule iteration used fixed rule learning throughout windowing rule learner acknowledgement sponsored austrian fund forderung wissenschaftlichen forschung financial austrian artificial intelligence austrian federal ministry transport like thank mooney lisp publicly used gerhard widmer comment maintainer contributor machine learning repository reviewer valuable suggestion pointer relevant literature
