novel fuse ensemble stacking dynamic integration regression major computational overhead intention benefit varying stacking data robust empirical referred meta combiner wmetacomb stacking dynamic weighting empirical consisted four recorded diverse data base feature base learning differed term latter base learning used demonstrate wmetacomb able outperform stacking fuse successfully ensemble learning integrate base learning ensemble ensemble base learning base learning learning ensemble learning schema homogeneous heterogeneous clearly former case base learning identical theoretically ensemble base learning sufficiently accurate diverse prediction krogh vedelsby diversity term regression ambiguity variance prediction term homogeneous learning manipulate training data feature sampling manipulate learning parameter learning brown survey ensemble diverse sufficiently accurate ensemble integration successful ensemble ensemble integration combine learning process referred stacking wolpert form formed prediction base training consequence training belonging training metainstance formed prediction base size ensemble prediction unbiased training data divided process subset iteration crossvalidation process subset removed training data remaining data prediction made base held trained accumulated cross validation process base trained whole training data breiman showed successful regression linear regression coefficient linear regression constrained linear regression hastie linear used particularly area classification seewald eroski enko scope stacking seen variant thereof tsymbal referred dynamic integration also cross validation process base process creates metainstance prediction made base training forming derived dynamic integration knearest neighbour lazy find nearest neighbour feature subspace test determines training base recorded neighbour used dynamic base base make prediction test tsymbal describes dynamic integration classification adapted regression rooney variant referred dynamic weighting applies weighting base localized make prediction excludes base deemed inaccurate injecting diversity homogeneous ensemble subspacing subspacing process whereby base built data subspaced feature tsymbal revised feature subspaced creating ensemble integrated classification empirically stacking dynamic integration integrating subspacing sufficiently give varying data rooney main computational stacking dynamic integration base form little computational overhead ensemble cross validation ensemble base consequence able stacking dynamic integration ensemble form prediction test ensemble referred metacombiner wmetacomb investigated wmetacomb outperform stacking regression data base learning used homogeneous subspaced process wmetacomb training phase wmetacomb wmetacomb training phase step initialise mdsr initialise mddi dtrain dtest build learning dtrain dtest form form errn mdsr mddi endfor endfor build mdsr build mddi step dtest totalerrsr totalerrdi dtest step dtrain dtest build learning dtrain dtest form form errn mdsr mddi endfor retrain build mdsr build mddi training phase wmetacomb step training phase wmetacomb fold training data step step testing remaining training fold indicator even tested data trained give reliable test recorded absolute totalerrorsr totalerrordi step rebuilt completed step computational overhead wmetacomb centre cost training lazy nearest neighbour learner cost trivial prediction made wmetacomb made step totalerr totalerrorsr totalerrordi normalized normerrori totalerrori normalized weighting norm normerrori influenced weighting wolpert macready combining base stochastic process used bagging breiman weighting parameter determines much relative influence give normalized give normalized give imbalance weighting high equally metalearners regardless normalized norm norm weighting parameter consideration heuristic imbalance weighting dependent great normalized differ wmetacomb form prediction test prediction made metamodels norm norm investigated wmetacomb work carried weka ensemble consisted base homogeneous data base subspaced tsymbal note subspacing process feature ensemble ensemble base learning data stacking wmetacomb tree quinlan hypothesis linear regression nearest neighbour affected size data data empirical gave relatively training phase metatechnique ensemble chose data weka witten frank data synthetic domain varying size many mixture discrete continuous data sampled reduce size data make computationally tractable missing data replaced mean modal data carried four differed purely base learning deployed base ensemble calculated relative root mean squared rrmse setiono ensemble data fold cross validation data data size data size tinuous discrete abalone autohorse autompg autoprice bodyfat breasttumor housing cloud echomonths elevator fishcatch housing house house lowbwt machine pollution pyrim sensory servo sleep strike veteran data rrmse percentage ensemble also rrmse built whole unsampled data learning used ensemble base repeated four time time learning base learning make computationally tractable also learning hypothesis wmetacomb robust base learning deployed base learning used four nearest neighbour linear regression locally regression atkeson tree learner assessed tailed paired test cross fold rrmse data summarised form loss data outperformed significantly loss data outperformed significantly significance gain loss zero gain gain indicative slight significance rrmse averaged data outperformed significance ratio gave percentage degree outperformed outperformed indication supplementary considerably loss conversely even much loss reduces considerably divided base learning base learner show base learning clearly ensemble strongly outperformed whereby term significance wmetacomb largest gain showed fewer data wmetacomb wmetacomb outperformed gain showed gain showed significance gain wmetacomb wmetacomb base learning base learner show ensemble outperformed wmetacomb showing largest gain wmetacomb outperformed term significance outperformed significance gain wmetacomb wmetacomb base learning base learner show ensemble outperformed wmetacomb showing largest significance gain wmetacomb outperformed term significance outperformed significance gain wmetacomb wmetacomb base learning show ensemble outperformed wmetacomb showing largest significance gain wmetacomb outperformed term significance outperformed significance gain base learner wmetacomb wmetacomb base learning show much ensemble base learning fact showed wmetacomb term significance show learning indicative case learning subspacing able sufficiently accurate base need enhanced ensemble tsymbal wmetacomb showed lesser degree term significance seen wmetacomb gave strongest ensemble base learning term significance gain tied learning wmetacomb also highest wmetacomb outperformed learning largest recorded significance gain largest recorded lowest recorded gain lowest wmetacomb outperformed four base learning degree largest significance gain largest lowest term gain wmetacomb able fuse effectively overlapping expertise seen case learning employed wmetacomb benefit stronger case indicating robustness conclusion wmetacomb merges stacking regression dynamic integration regression showed successfully able benefit metatechniques complementary expertise data demonstrated base learning homogeneous ensemble wmetacomb outperformed stacking dynamic integration dynamic weighting intend generalize metaensemble stacking learning dynamic integration
