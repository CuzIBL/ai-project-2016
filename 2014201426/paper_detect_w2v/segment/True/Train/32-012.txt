time learning near optimal stochastic game incorporates integrates kearns reinforcement learning monderer tennenholtz repeated game stochastic game face exploration exploitation dilemma markov process namely part game matrix much agent invest learning unknown part address controller stochastic game stochastic game stochastic game extend markov process mdps classical stochastic game shapley player agent adversary engage series competitive thus game agent adversary game player obtains game player stochastic game much like mdps agent goal find optimal mapping game mdps optimal mixed game mapped probability distribution optimization criterion used mdps cumulative discounted cumulative undiscounted step finite unbounded discounted undiscounted concentrate last criterion unfortunately stationary optimal infinite horizon stochastic game undiscounted criterion used time computing game machine learning moshe tennenholtz faculty industrial technion haifa israel moshet stochastic game stationary optimal infinite horizon undiscounted case time type game stochastic game scsgs horijk kallenberg parthasarathy raghavan vrieze name derives fact game transition depend agent adversary influence concentrate learning scsgs containing game stochastic game full brafman tennenholtz learning kearns learning formed introduced idea area reinforcement learning idea raise stem existence adversary unknown adversary hide agent refraining taking rarely playing never agent take unbounded time learn mdps emulate phase agent learn enters exploitation phase possibility continuous learning stochastic game complicated form exploration exploitation arises recall exploration exploitation dilemma refers play optimally risk unknown loss kearns kearns mdps fact know optimal examine learned case agent need explore unfortunately case adversary hide part game matrix many case lack calculate overcome employ introduced derer tennenholtz learning repeated game namely explore exploration learn fact game exploration extend game adversary exploration part contrasted exploration phase take time address yield near optimal agent time parameter best stochastic game learning littman wellman concerned analytic treatment dealt exploration exploitation littman asymptotic convergence stochastic game main make idea idea kearns idea spirit work learning repeated game work monderer tennenholtz synthesis idea full papa brafman tennenholtz explains stochastic game pose explains case preliminary scsg game game strategic form player player termed agent player termed adversary probabilistic transition probability tmnsitionfivm stot player termed agent play ease exposition normalize payoff game real player payoff joint also take history history history history scsg agent mapping fromh probability distribution determines probability choosing history stationary associate probability distribution scsg natural undiscounted adversary sequel scsg ergodic sense stationary agent probability transition pair regardless adversary make stationary optimal know scsgs loss generality stationary ergodicity consistent treatment kearns natural markov absorbing subset subset agent enters remain learning agent know lead absorbing really influence absorbing like quickly learn behave basically contribution identification central parameter upon learning mdps must namely mixing time argue unreasonable refer learning referring convergence desired mixing time stationary smallest payoff formally scsgs belongs stationary mixing time time undiscounted payoff adversary employ step accumulated sufficiently close agent know optimal mixing time need much time clearly expect agent lacking brafman tennenholtz probability failure learning column deviation unknown column time probability offailing learn deviation learn rameters proportion follow need show desired fact probability correspond adversary deviation ofthe reached desired adversary behaves nicely modify obtains desired learns fact stales overwhelming probability thus tradeoff exploitation exploration adversary play unknown column polynomialy many tune learn column step adversary rarely play column rarely encounter loss stemming randomization surely insignificant sufficiently long step desired step sufficiently parameter near optimal guaranteed staled empty sample execute foil owing sampling visited time lemma lemma column game computation guaranteed adversary correspond column machine learning guaranteed halted deviation adversary column agent deviate readied unknown column payoff exploration step unknown reached guaranteed lemma reach outside probability step case game learned kept memory clear lemma lead near optimal time parameter remains yield desired probability show four failure fourth stem need exploration poor true distribution chernoff alon case mdps show time explored sufficiently probability wish enables ignore fact column partially repeated attempted exploration fail expose failure reach unknown failure sample unknown view exploration step followed wandering bernoulli trial probability success reaching unknown exploring trial trial treated trial failure probability need exploitation exploration reach handled fact polynomially exploitation carried probability failure chernoff optimal exploration make fact deviation agent payoff know column learn unknown column handled lemma failure probability failure probability able desired remove mixing time straightforward identical treatment kearns decide explore lemma sufficiently high probability reaching quickly calculate probability exploration probability exceeds desired employing overwhelming probability remain time optimal safely exploration bias next must deal lack idea property deduce parameter step guaranteed probability desirable simply time mixing time desirable final halting time continuously long agent functioning caveat mixing time exponential mixing time step exponential exponential step true worth mentioning scsgs agent never know column consequently optimal full game agent actually converge differs yield best adversary guaranteed mixing time differ mixing time guaranteed mixing time time agent subject deviation afforded conclusion learning restricted stochastic game extends work kearns learning mdps monderer tennenholtz learning repeated game stochastic game explained full brafman tennenholtz unfortunately adversary influence transition lead mixing time slower convergence describing aimed clarity sole time careful lead time worth noting concurrently agent adversary stochastic game stationary equilibrium guaranteed attain near optimal game allowed seem natural learning used acknowledgement thank reviewer comment partially funded paul ivanier robotics production
