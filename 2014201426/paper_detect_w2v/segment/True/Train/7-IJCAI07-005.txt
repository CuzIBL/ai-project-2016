network efficiently expertise automatic reformulation framework learning implied network assumed user learned network process cardinality show significance encoding network efficiently toolkit difficult task expertise main bottleneck widespread puget time high refine propagation ilog user manual time graph coloring page ilog regin sport league scheduling implied smith studied automatic generating implied colton miguel colton miguel formation implied hnich hnich implied linear derived prolog system sterling framework derived structural property depend domain framework idea bessiere framework contribution automatic implied permit learn parameter implied learned domain learning feature permit reduce propagator parameter nvalue hold seen parameter lead allowed tuples take expressiveness parameter chance find implied network learning learn smallest nvalue implied tighter learned reduce many beldiceanu used implied involving parameter learning implied domain learned take network closer real know derive domain derive anything even difficulty find depend complexinteractions domain motivation task scheduled task duration task resource step time time resource used task maxr time task must separated step time precedence time task makespan maximumtime best makespan time find feasible schedule naive choco choco time task domain makespan resource unit time naive implied maxr node time node time unsat unsat naive left implied task duration task resource task time ensuring consistency separation time primitive used expressing resource maxr user default heuristic choco domain smallest cutoff left task requiring resource precedence show even naive hard want task definitely need time task induce task makespan scheduling task separated unit task soon mean task time zero task time expressed cardinality hold time even time even naive lead show nonexpert user even lead implied dramatically reduce cost network domain scope specifies tuples allowed assignment domain satisfied implied network network implied network alldifferent instantiated scope size fixed seen parameter like quimper parameter take like regin hold parameter beldiceanu contejean bessiere scheme scheme specified type parametric scheme parameter parametric scheme parametric derived parameter scheme scope scheme parameter tuples allowed parameter take parametric network parametric tuples satisfied instantiation tuple scheme satisfying tuple derived parametric derived scope parameter tuple occurrence occurrence tuples tuples parametric parameter allowed practice propagation restriction parameter expressed parametric tuples parameter lead parametric implied network worth tuples parameter tight implied learning implied parametric learn implied accept discarded tighter learned promising filtering learning implied parametric network parametric learning parametric implied tight implied tuples implied implied target give process learn parameter implied goal deal type network parameter scheme universal mean scheme implied learning task find superset target parametric subset implied subset implied word represents parameter preserve proved lose give incrementally tighten keeping target implied network implied replaced testing equivalenceof network handled classical relax checked implied network inconsistent implied replaced weakness clue subset test inconsistency know implied remove network help implied network implied satisfies parameter implied belongs implied network satisfies shrink possibility parameter computation implied network work parameter parametric network optionally target tuples parameter lost network learning optionally tighter else pick satisfies highly predictable inefficient explore exponential parameter checking inconsistent line npcomplete even learning phase subpart whole heuristic process fortunately practice used next property property parametric parameter filtering differs used case studied considering allowed tuples parameter partitioning parameter practice many parametric property parameter correspond disjoint case parameter imposes fixed occurrence parametric instantiation tuple satisfies nice property target lemma network parametric target next tell updating target case implied network parameterpartitioning tuple satisfies construction contributes line parameter know parametric parameter separately mean target must cartesian target parameter word derived parameter expressive considering subset learning process handle specialized implied network removed parametric inconsistent network parameter take discarded implied network parameter tuple satisfies modified deal case parameter tell must parameter tell removed parameter parameter parameter case must consecutive possibility modifying parameter shrink case restricts even possibility parameter allowed simplifies learning process specialized implied network parameter resp resp resp implied network parameter tuple satisfies thanks tightening simply done checking forcing take lead inconsistency network know removed learning process tractable operation learning call network containing parametric consistent line npcomplete idea tackle implied learned relaxation network implied network network network implied implied selecting subset thanks subset network want learn implied learned subnetwork implied network posted representing task implied subnetwork neither resource precedence thus network optimization optimization instantiation cost maximal network cost cubf maxr node learning unsat learning implied naive time learn time task duration task resource cubf accepting tuples cost fixed concentrate cost implied accepts optimal idea thus branch fixed time best classical learning process launched network cubf note learned implied improves phase permit quickly find continue learning process cubf learned implied tighter cubf relaxation cubf observe nice cooperation learning process process time case subnetworkeasy case find decrease cost consistency test line time consistency test finished next loop learning propagate scheme quimper cardinality parameter take propagate regin quimper many feature cardinality thus good implied learn cardinality used choco choco satisfaction learning subnetwork resource precedence discarded learning thus fast millisecond fixed consistency test subnetwork line maxr node unsat unsat unsat naive task task duration task resource maxr node learning unsat unsat unsat learning implied task time learn time task duration task resource show containing learned implied time corresponds learning process time implied user show learning give robustness process consistently easy type naive show task precedence stopped able size learning time solvable implied even minute show taking naive naive learning robustness term time asked user intuition maybe hidden ordering task much studying hand implied task duration optimization montpellier student totally ordered list prefer strictly preferredto goal assign student maximizing satisfaction student satisfied obtains obtains obtaining list worst satisfaction implied node node learning hour learning implied optimization student teacher student novice student domain preferred plus mean preferred list pair student mutual exclusion student measuring satisfaction preferencevariables linked minimize referred wish learn implied student launched time real student derived real data student ratio numberof student distribution left hand side show time node student hand side show learning implied student fixed consistency test line cutoff thus learning time grows linearly size learning time show tremendous refined learned implied show learning implied optimization greatly conclusion essential improving generic framework automatically learning implied parameter permit derive implied learns implied domain derived structural syntactical property generic framework property show spent learning implied dramatically time
