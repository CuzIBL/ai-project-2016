proposes membership probability case classified tree smooth probability modification tree data applies posteriori training case relies induced tree training sample used onedimension density estimator membership probability geometric give good even pruned tree intelligibility tree preserved tree widely used classification relatively intelligible data contrary learning intelligibility desirable property artificial intelligence considering hand classification system need confusion matrix rate like specificity sensitivity likelihood ratio cost used diagnosis system valuable membership probability unfortunately piecewise probability case classified leaf probability consequence main leaf highly biased contrary highly suitable probability intelligible work improving probability leaf smoothing specialized tree combined tree combined fuzzy ensemble actually smoothing induce drastic property tree tree modified main intelligibility improving probability modifying tree preserve intelligibility besides case feature induced inverse label feature seen probability expect membership probability closely case geometric like vector machine defines hyperplane feature classify data hyperplane used probability platt case piece hyperplanes hyperplane training case adapting idea smyth train density estimator case feature work probability probability database repository smoothing geometrically subset training enhance probability make comment concluding estimating probability tree probability piecewise leaf also inaccurate thus ranking risk work done probability build tree concern consist smoothing probability leaf training case label classified leaf training case classified leaf main smoothing laplace correction variant like correction probability toward probability cestnik zadrozny elkan improves probability keep tree unchanged probability leaf bypass smoothing used unpruned tree great leaf tree learn probability provost domingo intelligibility case specialized pruning developped ferri hernandez smooth probability kohavi deploys naive bayes classifier leaf specialized induction thus nbtree classical independance leaf idea zang step tree essentialy probability propagating case path node mainly fuzzy like umano quinlan ling managing uncertainty training database smyth gray fayyad keep tree leaf training path leaf dimension subspace path resulting dimension nevertheless high kind used practice reduce dimension dimension preserve tree theoretically applies soon probability algebraic feature operating data test tree note induced tree piece hyperplanes normal also possibly cost utility case label tree divide area possibly area labeled name tree orient case stand area algebraic algebraic algebraic extends induced tree inverse area case area actually algebraic case tree case operating data computes algebraic adapted alvarez projecting case onto leaf label differs turn leaf label differs leaf label nearest give algebraicdistance gather leaf verifies projectionontoleaf sign projectionontoleaf size test threshold onto leaf straightforward case area classified leaf test worst case test tree tree main continuous relatively robust uncertainty case noise training data threshold test modified kernel density algebraic density widely used density constantly bandwidth bandwidth venables ripley univariate basically contribution training case density kernel density sample computes kernel bandwidth vary many used framework kernel probability membership also kernel regression used simplicity algebraic density distribution algebraic probability training case subset case density population density distribution algebraic density distribution algebraic derive bayes rule probability kernel probability kernel bility straightforward note training used build tree distancebasedprobest algebraic distancey algebraicdistance subset probability density default possibility used kernel density simplest whole sample algebraic globally consideration concerning case call probability kernel probability show probability varies test sample laplace probability piecewise laplace probability test sample wdbc database highlighted studied kernel probability database repository blake merz missing simplicity treated chose lowest frequency database database divided bootstrap sample training test proportion respecting frequency database certainly best build accurate tree unbalanced datasets cost interested accurate tree want kernel probability grow tree default weka witten frank many case build tree unpruned tree disabled collapsing used laplace correction smoothing probability leaf pruned tree unpruned tree also built nbtree sample used data mean deviation system parameter training sample bootstrap sample avoid computation outside practice guided data sensor suggests adapted system kernel density algebraic simplicity used venables ripley default parameter kernel optimal bandwidth specialized also dedicated package certainly give systematically used default bandwidth inappropriate bayes rule used bandwidth fraction algebraic parameter used control sophisticated obviously used bayes rule reformulated bandwidth used kernel probability probability used area receiving operator curve bradley widely ranking tree distinguish good good probability make hypothesis true probability true probability unknown clear good probability also used mean squared make hypothesis true probability show kernel probability laplace correction case give laplace correction confidence intelligibility viewpoint interresting note probability pruned tree smoothing unpruned tree smoothing pruned tree also wilcoxon unlateral test batch sample test confirm exactly term vehicle pima glass reach used bandwidth algebraic vary smoothly bandwidth show mean probability test true probability summed test sample probability diminishes quickly normal dataset normal unpruned nbtree bupa glass iono iris thyroid pendig pima segment sonar vehicle vowel wdbc wine mean probability laplace correction pruned tree laplace pruned unpruned tree mean deviation insignificant italic bold remains comprehensible kernel tends erase sharp consequence probability reach loss give useless infinite idea leaf refine used kernel density step simply leaf classifies argue show severe drawback geometry preferred laplace nbtree dataset bupathyroid glass iono iris satsegment sonarvehicle vowelwdbcwine mean unilateral wilcoxon paired test probability laplace correction normal italic bold pruned tree nbtree rationale behind normal surface axis defining relate linear piece surface separator used node tree learning sample sample associate separator subset comprises onto belongs generically onto belong separator case associate separator defining largest explained previously illustrate leaf learning distributed probability axis growing probability grows smoothly sigmoid centered growing probability decrease sharply around tree sample distribution leaf yield separation yield separation gray area leaf thus generates subset bottom show subset separation show kernel parzen window width leaf probability used sample give close kernel axis case leaf introduces deviation probability main explanation deviation side artificially probability around border case leaf actually bias estimating probability leaf leaf introduces artificial symmetry side leaf averaging illustrated good chance leaf side border leaf induce distortion avoided artificial symmetry introduced leaf separator leaf tree bottom subset gray gray separator show kernel smoothing leaf database necessarily correlated globally probability kernel estimator width parzen window leaf bottom separator black true probability gray normal dataset normal unpruned normal bupathyroidpimasatsonarvehiclevowelwdbcwine glass iono iris pendigits dkpe smoothing mean expect insignificant italic bold conclusion geometric membership probability case classified tree applies posteriori tree geometric depend type splitting pruning criterion used build tree induced tree used real computing seen euclidean density estimator trained learning sample used build tree probability experimentation done tree kernel show geometric probability well also probability relies leaf main numeric ordered used unordered modality also used oblique tree euclidean work progress feature linked nearest part used train kernel density estimator dimension
