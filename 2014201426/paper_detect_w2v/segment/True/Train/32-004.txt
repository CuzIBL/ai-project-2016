unsupervised learning hierarchy text data thomas hofmann berkeley berkeley hofmann novel statistical latent text mining interactive access learning architecture purely data driven utilizes word occurrence statistic intertwined fashion extract hierarchical well abstractive annealed likelihood parameter derived benefit interactive retrieval automated cluster summarization investigated experimentally intelligent processing text ultimately natural statistical learning word occurrence applicability interactive access purely make background rely predefined list statistical latent mixture mclachlan basford organizes hierarchy agglomerative clustering jardine rijsbergen croft willett feature probabilistic sound foundation statistic probabilistic inference principled fitting machine learning explicit hierarchical namely clustering discriminative descriptor annealing nonhierarchical probabilistic clustering full hierarchical probabilistic clustering emphasize clustering introducing simplified probabilistic clustering closely distributional clustering pereira used word clustering text categorization baker mccallum word word stem refer vector word constituting word frequency summarized word occurred latent assumed belongs exactly cluster cluster assumed fixed introducing word distribution probability stacked parameter vector factorial reflects independence word occurrence dempster latent employed step alternated expectation estimating probability unobserved clustering parameter maximization data lihood probability parameter likelihood step converges mild bayes rule yield distributional clustering stationary differentiating intuitive encode probabilistic clustering uments conditionals word distribution belonging simplified flat clustering deficit severe lack inadequacy distribution emphasize discriminative word fact dominated frequent word occurrence cure flaw main goal hierarchical hierarchy hierarchical clustering utilize agglomerative cluster hierarchy dendogram successive cluster merging willett explicit achieved extending mixture capture specificity word assumed word occurrence node latter identified terminal node cluster hierarchy formalize sketched idea latent vector introduced assign word exactly node hierarchy topology node hierarchy cluster imposed used shortcut refer node terminal node hierarchy admissible latent latent restricted node hierarchy predecessor break node well cluster node hierarchy utilized word terminal node subtree pictorial node word occurrence restricted highlighted vertical path think mixture horizontal mixture cluster vertical mixture horizontal mixture vertical path root vertical horizontal tree topology generalizing probability distribution word attached node terminal hierarchy rule data joint probability latent specified step note vertical mixing proportion node cluster introduced case make simplifying mixing proportion cluster lution degenerate distributional clustering parsimonious data fraction word held spirit interpolation jelinek mercer distributional clustering derive fitting joint probability form applying sketch assigning rences term latent rule obtains word distribution evaluated data worth taking closer look predictive word probability distribution simplicity hard clustering case word probability modeled mixture occurrence reflects reasonable mixture word ranging term ordinary highly term specialty word annealed also need addressed successful importantly avoid overfitting machine learning meaningful tree topology terminal node also want find reduce sensitivity procedure answer annealed hofmann puzicha annealed closely deterministic annealing many clustering rose pereira thorough annealed scope background skipped procedural idea deterministic annealing temperature parameter applying annealing clustering replacing dampens likelihood tribution linearly scale entropy annealed probability annealed utilized control parameter initialized high successively lowered heldout data decrease annealing advantageous fitting inexpensive regularization avoids overfitting improves also tree topology annealing lead phase transition cluster lowered perplexity inverse word probability data automatically defines terminal node hierarchy subject hofmann puzicha conclusion used preprocessed word suffix stripping word stemmer stop word list utilized eliminate frequent word rarely occurring word also eliminated saul learning boltnnann tree neural computation verbatim word stem introduces family boitimann machine trained introdnc larg famili boltzmann machin train gradient network hidden unit gradient network connectivity show supervised learning unit implem supervis learn boltzmann machine exactly resort simulated rithm boltzmann machin exactli simul anneal annealing stochastic yield gradient stochast averag yield gradient decimation techniqu pariti detec parity hidden symmetry hidden symmetri ghost writer base gener process differ provid studi propos optim gener neural paramet appli gener neural propos inform data approxim dynamic neural rule process recogni rate classif propos gener neuron time properti data converg neural optim rule rate dynamic process paramet studi statist adapt perceptron exampl gener rale onlin calcul deriv backpropag simpl asymptot separ unsupervis neural architectur entropi statist multilay activ backpropag gener phase teacher delta introdnc sampl decai nois projec correl student temperatur gain dynamic predic learn collection term word stem word lowest perplexity word occurring differentiated hierarchy exemplary node frequent word highest probability word respective node term depicted reported much datasets form core prototype system collection titleword machine learning learn dataset title cluster probability word occurrence text statistical show probable word text abstractive helpful distinguish trivial unspecific word suggestion like highly term like benefit extraction visualized dataset learn cluster hierarchical satisfying topological cluster seems capture multiresolution distribution node hierarchy coarsening procedure sort averaging respective subtree hierarchy fact lead node make probability suitable cluster summarization node capture vocabulary cluster subtree term automatically probable word distribution node term stress probability distribution averaging respective subtree summarizes exemplary showing averaging mostly high probability unspecific term node highly discriminative word distribution thus principled satisfying term opposed many circulating heuristic distinguish topical term interactive retrieval cluster collection depicted pretend interested clustering segmentation real interactive word shifting window hierarchy node word distribution also possibility attach prototypical node maximal probability probable type cluster cluster hierarchy learn dataset node probable word successor node depicted left successor node successor interactive retrieval segmentation hierarchy machine learning locally discriminant distribution node automatic prototypical particularly beneficial interactive retrieval process cluster comprehensible derived averaging hierarchy refine even utilized actively user specification conclusion novel statistical text mining sound foundation likelihood dual cluster hierarchy make particularly interactive retrieval carried scale collection emphasized extract hierarchical resolution dependent cluster summarization scale database seems promising cluster hierarchy cluster dataset comment acknowledgment helpful comment puzicha sebastian thrun andrew mccallum mitchell hagit shatkay reviewer greatly acknowledged thomas hofmann daad postdoctoral fellowship
