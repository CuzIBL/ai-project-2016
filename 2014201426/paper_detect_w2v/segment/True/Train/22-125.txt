classification statistical neural machine learning four data data previously analyzed reported statistical medical machine learning literature data characterized statisucal uncertainty accurate training testing resampling used true rate classification neural back propagation relatively hypothesis feature machine learning procedure rule induction tree induction clearly many fall classification clancey kulikowski james diagnostic empirical learning classification span roughly statistical duda hart fukunaga neural mcclelland rumelhart machine learning induction tree production rule applicable procedure differ radically final supervised learning used classify sample tree modularized clearly explained compatible reasoning procedure system base statistical relatively mature studied many year highly duda hart fukunaga year surge newer classification machine learning neural induction tree empirical data studied artificial intelligence statistic quinlan quinlan quinlan procedure induction tree well machine learning classification regression tree cart breiman friedman olshen stone procedure major nonparametric classification statistician production rule tree path tree part grant distinct production rule tree disjunctive production rule need mutually exclusive induction production rule empirical data michalski system michalski mozetic hong lavrac work quinlan deriving production rule collection tree quinlan neural increased dramatically many successful classification hidden unit back propagation learning area exploring learning evolving explored classification occasionally classical discriminant arecited newer neural nearest neighbor extensive classification data heightened neural back propagation many deal uncertainty medical determining survive cancer accurate answer relatively data data examined previously literature data consisting feature classification data assumed sample population task classify rate unlimited case training testing rate readily rate test case fewer case must resampling estimating rate next estimating rate well apparent rate classifier training lead highly misleading duda hart overspecialization classifier estimating rate widely studied statistic efron duda hart fukunaga literature simplest honestly estimating rate holdout train test sample case broken case training test classifier independently derived training case classifier test case train test case misleading size test sample confidence highleyman test case virtually rate test case close true rate relying train test test train train test classifier derived rate rate classifier derived independently resampling train test case resampling fukunaga efron elegant straightforward estimating classifier rate computationally relatively sample sample size classifier case tested remaining case repeated time time classifier case used test case time nearly case used classifier rate test case divided evidence superiority lachenbruch mickey efron preferred sample computationally sample size grows train test tneir estimating kanal chandrasekaran case cross validation stone cross validation case divided mutually exclusive test approximately size case test independently used training resulting classifier tested test rate rate cart procedure extensively tested varying cross validation seemed adequate accurate particularly sample computationally ibreiman friedman olshen stone sample bootstrapping resampling much promise variance estimator classifier efron jain dubes chen crawford area statistic classifier constructed simply sample identical belong yield perfect classification sample case also stratification case train test percentage prevalence sample machine learning sample case rate rate iteration early computational make much viable sample classification steen estimating rate besides resampling goal separating sample case training testing help classifier rate train test case training lead poor classifier test case lead erroneous lesser extent resampling accurate rate training case classifier resampling data readily duplicate train test introduces possibility variability divergence classification classification used review mathematics thev readily goal data statistical classical used list wellknown reader referred duda hart give used simplifies normality covariance matrix probably used form discriminant used canned imsl demonstrated game playing quadratic classifier used nearest neighbor euclidean simplest conceptually cited used casebased reasoning waltz bayes rule optimal presentation classification classification viewed bayes optimal classifier bayes optimal classifier probability data dependency invocation real impossible simplifying made usual simplification independence dozen classifier built particularly medical szolovits pauker bayes rule independence also reported literature unsupervised learning cheeseman independence assumed mathematical incorporate correlation tried bayes independence bayes bahadur neural neural hidden back propagation procedure mcclelland rumelhart employed outline data gorman followed used mcclelland rumelhart learning rate momentum learning gorman corresponds cross validation examined back propagation commences network thus even presentation case learned network unlikely match network also possibility procedure reaching train test learned time test averaged time usual training trial must learning made data repeated network hidden unit resampling numoer hidden unit data separable hidden unit took sufficient computation time train test trained time sample size hidden unit epoch presentation data sufficient hidden unit fitting case lesser hidden unit data sampled epoch squared continued indicated progress made unit used hypothesis highest conclusion classifier rate outline procedure followed theme data computational reduce repeated trial averaged back propagation computational procedure train test data readily separable thus computation month time expended neural machine learning logistic indicated explored machine learning statistic posed production rule tree disjunction used well operator continuous predictive maximization galen tadepalli tried data heuristic procedure find best rule disjunctive normal form viewed heuristic exhaustive applicable relatively rule good many tree procedure node considering tree procedure preferable rule tree size resampling data exhaustive optimal rule disjunctive normal form data tree successor review classification four data data attempted consistent previously iris data iris data used fisher derivation linear discriminant fisher discriminant used statistical routine imsl linear quadratic discriminants normality well data iris discriminated continuous feature data case summarizes rate apparent rate case rate kapouleas machine learning cancer data data evaluating prognosis breast cancer recurrence analyzed michalski rule induction reported michalski mozetic hong lavrac reported rate physician rate rate pruned tree procedure assistant kononenko bratko roskar descendant derived rate resampling time train test sample consist sample test sampled data train test tried four data averaged thus consistent summarizes rate apparent rate training case rate rate test case comparative cancer data rule neural apparent rate training trial testing testing trained epoch best neural term hidden unit listed relationship hidden unit rate listed thyroid data quinlan reported hypothyroid data quinlan quinlan referred clinic hypothyroid thyroid relatively sample sample consist case year case used used training case used test case test missing test deemed unnecessary neural rate cancer data filled mean summarizes rate rate training case rate rate test case medical perspective test excellent classification achieved diagnosing thyroid dysfunction data answer stored sample derived system australia rate note sample represents sample hypothyroid acceptable classifier must significantly comparative thyroid data rule neural apparent rate best trial trained epoch best neural term testing hidden unit relationship hidden unit rate listed kapouleas neural rate thyroid data time training neural back propagation size data great hidden unit epoch hour time unit hour apparent rate hidden unit hypothesis training initiated fewer hidden trial unlimited time long slight progress made indicated sampling epoch size neural epoch summarizes best encountered sampling occurred hidden unit listed neural network training thyroid data reasonable cross prototypical widely encountered many characterized uncertainty classification cancer data feature relatively weak good predictive unlikely thyroid data feature prediction data resampling used case resampling give excellent true rate momentum learning rate help prevent machine learning fact data iris reviewed many year made note wish avoid multivariate distribution covariance matrix trivial rule many fact major logistic rule tree compatible elementary reasoning explanation also compatible system thus everything many logistic everything case logistic exceeded posed rule work tree indicated note largest studied thyroid biased logistic endpoint derived system apparently test threshold high hypothesis necessarily extrapolated experience numerous cart breiman friedman olshen stone demonstrated tree superior alternative statistical classification statistical classifier consistently expectation linear classifier normal distribution gave good case thyroid classifier widely used training rate hold well lest case natural quadratic classifier normally distributed data degrades rapidly nonnormal data poorly bayes independence moderately well good test data nearest neighbor well good feature tends degrade many poor feature many alternative statistical classifier tried nonparametric piecewise linear classifier foroutan sklansky reduce feature training feature many actually test case feature neural well statistical classifier well thyroid best classifier consumed enormous time sometimes equaled classifier improving neural training relationship hidden unit rate followed classical classifier hidden unit increased apparent classifier overfits data true rate curve flattens even much tree node production rule rule remains open well classifier many feature many possibly exclusive also many case actually learn answer many help show fashion many used classification induced rule iris petal iris setosa petal petal width iris virginica appendicitis mnea mbap cancer node thyroid hypothyroid compensated hypothyroid
