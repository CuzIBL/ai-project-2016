neighborhood minmax nmmp supervised dimensionality reduction learning linear transformation pairwise neighbor transformation pairwise close formulate constrained optimization optimum effectively efficiently supervised linear discriminant able extract discriminative feature deal case distribution morecomplex gaussian singularity naturally data demonstrates effectiveness linear dimensionality reduction facing data many past year jolliffe linear discriminant fukunaga widely used unsupervised take supervised dimensionality reduction classification drawback drawback suffers sample size dealing high dimensional data case withinclass scatter matrix make difficult many address belhumeur chen yang variant discard subspace thus discriminative lost drawback distribution optimal case data distribution gaussian satisfied real distribution gaussian fail find optimal discriminative duda insufficient many classification nearest neighbor classifier learning play vital role learn mahalanobis xing goldberger weinberger linear dimensionality reduction viewed case learning mahalanobisdistance viewpoint give reasonable fact nearest neighbor classifier linear dimensionality reduction supervised linear dimensionality reduction neighborhood minmax nmmp largely inspired classical supervised linear dimensionality reduction learning nearest neighbor lmnn classification weinberger pairwise neighbor transformation pull pairwise close take goal achieved formulating task constrained optimization optimum effectively efficiently avoids drawback lmnn computationally much data demonstrates effectiveness left belong circle neighborhood neighborhood neighborhood transformation pull close belongs belongs circle neighborhood neighborhood neighborhood transformation push data matrix goal learn linear transformationw identity matrix transformed vector data kind neighborhood neighborhood neighborhood data nearest neighbor data nearest neighbor obviously data pairwise neighbor transformation hope pairwise minimized maximized transformation euclidean pairwise formulated trace operator matrix label label obviously euclidean pairwise goal maximize minimize used difficult suitable former latter optimization fact latter case former automatically formulate constrained optimization fortunately globally optimal efficiently calculated next constrained optimization constrained optimization address optimization form constrained optimization real symmetric matrix matrix rank find matrix maximize wtbw lemma show wtbw zero matrix rank hold wtbw rayleigh quotient golub loan smallest eigenvalue rank wtbw mintr thus optimization case case lemma ensures optimalvalue finite case optimal derived note easy hand largest eigenvalue optimal optimal vice versa thus optimal subsequently give suitable need optimal real symmetric matrix matrix rank lemma lemma give loss generality note note time lemma know dimension optimal decreased monotonously optimal wtaw hand wtaw largest eigenvalue smallest eigenvalue wtaw optimal wtaw obtaining optimal step precise note need calculate inverse thus singularity naturally case case null matrix wtbw infinite reasonably replace optimization ztaz eigenvectors zero eigenvalue know largest eigenvectors ztaz case final real symmetric matrix matrix rank matrix case largest eigenvalue smallest eigenvalue calculate largest eigenvalue else largest eigenvectors case largest eigenvectors ztaz eigenvectors zero eigenvalue optimization neighborhood minmax neighborhood minmax nmmp used preprocessing step nmmp covariance matrix data null orthogonalcomplementof preprocessing eliminate null covariance matrix data data rank calculate calculate nmmp well null eliminated lose fact easy null comprises null null demonstrates eliminating null covariance matrix data affect thus eliminate null covariance matrix data closely supervised dimensionality reduction goal also maximize scatter minimize scatter matrix parallel scatter matrix scatter matrix fact neighbor reach neighbor data data impose faraway pairwise close make discriminability property distribution data gaussian give illustrate data dimension distributed concentric circle eight dimension gaussian noise variance show subspace learned nmmp illustrates nmmp find transformation manifold discriminability able extract discriminative feature singularity naturally nmmp dimension tendimensional data nmmp illustrates nmmp find transformation manifold discriminability learning classification learning mahalanobis learn semidefinite matrix mahalanobis replace euclidean note semidefinite eigenvalue eigenvectors mahalanobis formulated vtxi vtxj vtxi vtxj form learning mahalanobis learn weightedorthogonallinear transformation nmmp learns linear transformation viewed case learning mahalanobis note learning matrix difficult formulated semidefinite computation burden learn transformation learning matrix much evaluated nmmp data lmnn data used belong brief data list preprocessing step eliminate null data covariance matrix singularity reduce dimension data scatter matrix nonsingular sample training remaining sample testing deviation reported classification neighbor classifier iris face usps news training testing dimensionality dimensionality brief data data training time iris baseline lmnn nmmp baseline lmnn nmmp face baseline lmnn nmmp baseline lmnn nmmp usps baseline lmnn nmmp news baseline lmnn nmmp data worth noting parameter sensitive fact simply training reported preprocessing baseline describethe data data iris balance machine learning distribution data work well also demonstrates competitive face face database formerly database distinct time samaria harter facial database size preprocessings preprocessing step much baseline lmnn good computation burden also many face database demonstrates much database nene viewed varying angle five degree resulting size computation time face lmnn much baseline note face distribute manifold nmmp preserve manifold discriminability digit digit task usps handwritten digit data digit used four data baseline work well make little fails case demonstrates insufficient data distribution gaussian text categorization investigated task text categorization data auto motorcycle baseball hockey preprocessed procedure zhou vector normalized tfidf lmnn bring comparing baseline data insufficient task conclusion neighborhood minmax nmmp supervised dimensionality reduction nmmp pairwise neighbor dimensionality reduction nmmp minimizes pairwise maximizes nmmp discriminability property distribution data gaussian real validate disadvantage singularity limitation dimension also avoided linear dimensionality reduction nmmp viewed case learning mahalanobis computation burden learning mahalanobis formulates constrained optimization optimum effectively efficiently demonstrate competitive mahalanobis learning lmnn computation cost much
