extracting rule rbfs trivial task nonlinear high dimensionality case hidden unit network tendency even contribute address lrex rule extraction tackle extracting rule hrex extract rule examining hidden unit assignment mrex extract rule mapping rule extracted contrasted competing rule extraction system central network radial network suitable architecture gaussian amenable rule extraction neural network many largescale considerable robust classifier generalize imprecise data bishop robust classification speech signal well prediction system modeling process understood highly nonlinear neural network gained acceptance many industrial widely used practitioner mission engaged aerospace military medical system understandable neural network lend normal process extraction forming rule parameter neural network becoming overcoming limitation shavlik extracting network classed type neural network parameter responding subset also specialized type neural architecture extracted rule examined comprehensibility rule antecedent contained rule structured describes motivation extraction describes architecture radial functionnetwork particularysuitable knowledgeextraction four outline extraction rule network five explains work conclusion work extraction motivation extraction network network localized moody darken amenable extraction extract series rule able simply accurately contained neural network year great deal researching extracting rule neural network rule extraction carried upon neural network type perceptrons thrun kohonennetworks recurrent network omlin giles extracting rule neural network summarized learned neural network difficult provision interpret network mapping form rule deficiency training identified thus network improvedby identification superfluous network parameter removal also enhance network previously unknown relationship data feature huge mining possibility induction craven shavlik providingan explanation rule extraction recognisedas powerful integration system mcgarry extraction system data flow data transformation radial network radial neural network many neuron nervous system cell responsive narrow stimulus cochlear stereocilla cell locally tuned frequency sound moody darken show network trained noisy data illustration network feature four hidden unit network feedforward architecture hidden unit linear unit simply parameter network trained noisy transfer vector hidden unit form localized response learning normally undertaken process unsupervised process centre hidden unit positioned optimum width training sample learning calculating hidden unit unit achieved matrix transformation radial hidden kernel operate localized area kernel allocated centre width radial gaussian response response unit calculated simply matrix hidden unit activation vector parameter vector width receptive training train network acceptable data varied data literature examined guidance achieved stated best test training network parameter spread width hidden unit fewer hidden unit used reached training terminate hidden unit training test data construction network data good benchmarking used mixture well benchmark data well vibration data test data mainly collection maintained california irvine vibration data produced part concerned industrial machinery data synthetic real varying feature composition data used work data binary continuous iris vowell peterson vowell deterding protein yeast protein ecoli japanese australian diabetes pima sonar vibration vibration give data column ouput feature feature data continuous data discrete data last column data missing lrex rule extraction lrex motivated architecture network suggested rule extracted little work extracting rule ordinary network lowe work fill substantial rule extraction lrex composed module mrex module extract type rule premise hidden unit uniquely centre hidden unit vector mapped work simpler data tended reinforce belief hidden unit sharing network trained data phenomenon reduces rule hidden unit amongst module hrex identify hidden unit hidden unit contributes extracted rule type rule hidden next mrex hrex module user complimentary type extracted rule operation network mrex mapping mrex hidden centre gaussian radius spread statistical training rule hidden unit procedure train network data collate training hidden unit hidden unit correlation label build rule antecedent join antecedent label rule file mrex mrex matrix identify allocation hidden unit next calculate antecedent adjusting centre gaussian spread adjusted statistical gained training classified hidden unit used empirically expand antecedent training rule iris domain note four extracted rule hidden unit rule sepallength sepalwidth petallength petalwidth rule sepallength sepalwidth petallength petalwidth rule sepallength sepalwidth petallength petalwidth rule sepallength sepalwidth petallength petalwidth mrex extracted rule iris domain hrex hidden unit rule extraction hrex quantization clustering network parameter activation form operation extracted rule user rule extracted feature enables tradeoff made rule size rule comprehensibility achieved parameter determines postive quantized cutoff quantized participate rule extraction determines hidden unit activation hidden unit activation cutoff quantized play part rule extraction determines cluster training divided process distinct rule identify parameter empirically satisfactory arrangement show note rule consist quantized quantized activation rule hidden unit must lable satisified hidden unit activation training data quantization modifier hidden unit activation quantization modifier cluster training sorted quantized quantized hidden unit activation quantized hidden unit activation rule cluster procedure quantize quantize hidden unit activation training cluster cluster identify activation calculate cluster identify attached build rule hidden unit belongs rule join hidden unit label rule file hrex hrules rule extracted ecoli domainare rule fire antecedent must satisfied hidden unit must seen hidden unit participates hrex rule identifying structural relationship formed hidden unit rule true true true true true true true true rule true true true true true hrex extracted rule ecoli domain strated network poor network hrex rule exhibit degree hidden unit sharing worse case fail hrex rule show hrex rule rule size comprehensibility network trained vibration monk sonar data vibration show steady rule cluster size rule extracted sonar actually lose reach steady generating rule monk optimum cluster size reached oscillating rule extraction system andrew geva andrew geva mcrbp build network specialized activation network trained rulex used extract rule rule extracted rulex produced system show work column identifies data column mrex alongside column hrex next fourth column show rulex hrex rule size mrex hrex rulex data mrex hrex rulex binary continuous iris vowell peterson vowell deterding protein yeast protein ecoli japanese australian diabetes pima sonar vibration vibration show rule system rule size quoted lrex unmodified rulex extract highly compact rule lrex majority domain rule unfortunately rulex failed rule domain tracked mcrbp network unable form viable classifier training data rule extracted invalid rulex also failed rule vibration domain training mcrbp network took fewer reach acceptable network form viable network vowel sonar vibration domain specialized architecture cope rule size mrex hrex rulex data mrex hrex rulex binary continuous iris vowell peterson vowell deterding protein yeast protein ecoli japanese australian diabetes pima sonar vibration vibration feature data form rule fewer rule form classifier hrex fewer rule mrex accurate rule enables operation network hrex rule proved network hidden unit tend network implie parameter training optimal badly width conclusion work describedin paperhas tackled difficult extraction network avoided literature overlapping neuron rule extracted lrex network form mapping regarding hidden unit participate classification extracted mrex transforms network rule classifier make mapping network transparent open scrutiny rule produced dependent hidden unit rule obscure comprehensibility partially hrex rule user tradeoff rule size comprehensibility network naturally rule accurate good network modeled hidden unit used case hrex regarding extent andrew geva andrew geva initialising neural network proceeding neural processing page perth western australia bishop bishop neural network oxford craven shavlik craven shavlik neural network data mining system lowe lowe inversion network statistical proceeding artificial neural network page bournemouth mcgarry mcgarry wermter macintyre neural system coupling integrated neural network neural computing survey moody darken moody darken fast learning network locally tuned processing unit neural computation page omlin giles omlin giles extraction recurrent neural network artificial intelligenceand neural network step principled integration page academic diego shavlik shavlik frameworkfor combiningsymbolic neural learning machine learning rule extraction extraction planning reinforcement learner proceeding ieee joint neural network lake como italy thrun thrun extracting rule artificial neural network distributed touretzky leen neural processing system mateo learning constrained prediction benjamin minglun qian electrical coordinated laboratory illinois urbana prediction artificial neural network anns traditionally formulated unconstrained optimization unconstrained little guidance stuck poor constrained violation guidance formulate learning prediction nonlinear constrained optimization lagrange multiplier discrete constrained optimization learning violation guided vgbp computes gradient introduces annealing avoid blind acceptance trial applies convergence extensive benchmark work show prediction learning predicting stationary artificial neural network anns anns modeling temporal implicitly feedback neural network tdnn neural network latter recurrent neural network haykin anns studied architecture recurrent neuron filter link aeronautics administration proc joint conf artificial intelligence aaai believe architecture powerful modeling unknown temporal prediction anns traditionally formulated unconstrained optimization minimizing mean squared node desired outputsof time vector training data consist extensive conducted past anns generalize well learning success little guidance unconstrained stuck case squared violated best trajectory move address lack guidance constrained qian learning training prescribes unit training constrained beneficial difficult training violated guidance leading trajectory reduces violation prediction cross validation used prevent data type validation true data validation predicted iteration validation training validation test test used testing learning completed traditionallearningwith cross validation part training priori validation used learning objectivein learningis imize validation validation learning problematic validation learning excludes validation used learning learning converge regime training scarce constrained qian defines validation learning validation validation suitable inadequate training data stationary regime validation unit validation normalized mean squared variance true time series validation note test constrained resp resp validation predefined constrained nonlinear prediction handled lagrangianmethodsthat requirethe differentiability penalty difficulty convergencewhen penalty properly sampling wang lagrange multiplier discrete constrained optimization continuous discretized inefficient learning address learning vgbp lagrange multiplier discrete constrained optimization lagrangian transform augmented lagrangian differentiable discretize finely discrete lagrange multiplier discrete constrained optimization constrained discrete constrained continuous continuous discrete discrete summarized finite reachable step reach anypoint discrete feasible smallest note feasible saddle satisfies real vector sufficient discrete satisfies mean show discrete much note hold continuous learning procedure discrete constrained prediction shaded represents routine look stand look lagrangian shaded chen show framework part subspace ascent subspace indicated sampling lagrangian discrete inefficient gradient probe gradient lead infeasible annealing help escape infeasible last property constrained learning successively tighten relaxed satisfied depicted left framework look loop subspace generating accepting deterministic annealing rule occasionally loop ascent subspace generating subspace accepting deterministic annealing rule subsection leave next subsection learning training essential differentiable gradient applying gradient mean squared generating trial gradient step size mapping trial discretized training lagrange multiplier contribute gradient leading suppression violation step size used deriving must dynamic repeatedly fixed deterministic gradient adapt dynamically acceptance ratio latter high leading step size hand step size terrain leading decrease step size extensive adjust true violation progress learning predict tolerance rule penalizes violated relative probabilistically like guidance take soon deterministic lead deterministic acceptance probabilistic acceptance subspace gradient cross validation step size heuristically stuck infeasible restarts used help escape uncontrolled restarts lead loss valuable collected annealing decides metropolis probability parameter introduced control acceptance probability plot progress mean squared training predict fixed temperature used throughout unless specified combined restarts accepts everytrial generatedin illustrates showing decrease violation fixed temperature schedule decreasing temperature adjusted decrease violation training violation tolerance broken line solid line explores stuck infeasible restarted keeping history hand accept trial reject poor high probability consequently keep implicitly history searched past progress smoothly escaping poor blindly annealing schedule high temperature decrease temperature zero time fixed temperature throughout fixed temperature carried annealing temperature opportunity explore show violation fixed temperature used temperature dynamic temperature schedule undesirable violation tolerance initially know violation tolerance achieved considerably violation leading rugged difficult hand loose initially satisfied violation progress differs considerably fixed illustrated show violation five little violation increased decrease violation eventually curve satisfied specified impossible topology also show steeper rate decrease violation convergence dynamically adjusting fastest convergence rate done choosing loose initially tightening violation satisfies largest time switch convergence illustrates initially leading steepest convergence convergence switch tightening leading steepest convergence used repeatedly tightening convergence lead envelope best convergence time convergence long steeper curve shorter time tighten around convergence sensitive work well constrained learning activation magnitude constrained violation vary difficult relaxation work well constrained parameter vgbp summarize parameter used vgbp training training normalized default proportional proportional approximately test laser test also show boxed best stand data training prediction prediction network scalenet geva vgbp vgbp robustness vgbp predicting sunspot violation likewise proportional affect show vgbp sunspot vgbp robust wide default adjusted dynamically initialization parameter vgbp default automatically tuning user evaluated vgbp benchmark laser chaotic intensity pulsation laser santa competition competition firnn took show vgbp improves term prediction well used sunspot yearly sunspot data training prediction four duration show vgbp achieves much prediction prediction vgbp work chaotic time series henon lorenz attractor ikeda attractor specified aussem test sunspot wnet comm scalenet geva drnn aussem boxed best stand data represents used training testing wnet ssnet drnn comm scalenet vgbp term prediction show vgbp achieves impressive magnitude vgbp achieves much accurate prediction aussem vgbp able duration applying training lead plot prediction vgbp show prediction accurate many step hard predict chaotic time series step unpredictable drnn aussem emphasize prediction iterativeprediction aussem work predict step time series chaotic time series prediction much harder latter step henon achieves constrained vgbp lead superior prediction benchmark tested aussem aussem dynamical recurrentneural network prediction modeling dynamical five carbon copy linear drnn aussem vgbp carbon copy simply predicts next data proceeding data training resp testing used learning resp testing lorenz attractor data stream labeled ikeda attractor stream real imaginary part plane wave benchmark training testing linear drnn vgbp henon lorenz ikeda prediction solid line data long dashed line predicted data vgbp dashed line prediction training tems neurocomputing aussem aussem communication march geva geva scalenet multiscale neuralnetwork architecture time series prediction ieee neural network haykin haykin blind deconvolution prentice hall englewood cliff chen chen constrained genetic nonlinear constrained optimization proc conf tool artificial intelligence page ieee november qian qian timeseries prediction constrained cross validation proc conf intelligent processing ifip congress page kluwer academic august wang wang simulated annealing asymptotic convergence nonlinear constrained optimization practice page october discrete lagrange multiplier nonlinear discrete optimization practice page october finite impulse response neural network time series prediction thesis standford combining fossil sunspot committee prediction ieee conf nerual network volume page houston june robot learning delayed response task extraction road sign fredrik linaker henrik jacobsson skovde sweden sheffield united kingdom show extraction used handling delayed response task arbitrary delay stimulus response processing lowest work sensory data data classified unsupervised clustering work classified data extraction detects signal transition form also sketch filtering constructed discard irrelevant data stream fourth used delayed response task irrelevant distracting delay automated robot driver navigates street reach goal journey encounter road sign describing upcoming junction detected roadsign system need able make stimulus system need make response junction reached even minute vehicle traveling rylatt czarnecki turn ulbricht fact delayed response task task involving association time rylatt czarnecki neural network inept handling sort rylatt czarnecki showed fact even delayed response task adapted ulbricht robot travel past stimulus left side continues corridor step reach junction time robot need decide turn left passed task specially constructed recurrent neural network architecture unable learn association rylatt czarnecki work unsupervised system extract work show time fact arbitrarily affecting system drastic also road sign distraction delay processing depicted sensor data rylatt czarnecki approached delayed response task argue like nolfi tani task dependency manifest neuronal much slower time scale involving even minute rylatt czarnecki simplified simulation specially modified neural network architecture able learn dependency robot system high sampling frequency rylatt czarnecki system sufficient khepera robot sensor sampling rate approximately time rylatt czarnecki system able learn task involving even delay show extraction delay arbitrarily enables system handle realistic dependency classification process whereby multidimensional sensor data divided nehmzow smithers employed manually fixed tani nolfi system thereby user intervention tani nolfi manually divide training phase also distinct frequent training learning process slow lina niklasson constructed flexible classification system aravq able swiftly classify dynamic automatically extracted overcoming tani nolfi system aravq system used simulation data tagged label alloted alphabet rewritten loss long letter ulbricht approached road sign trying learn association letter letter classification thereby ungrounded system difficulty learning dependency rylatt czarnecki system extraction process whereby transition membership data extracted thereby filtering repetition fairly straightforward long membership exclusive belonging succeeding classified differently generates signal next interspersed long time considerably slower mean detected noted tani nolfi tani nolfi robot system worked coupling back merely classified filtered acted upon system extracted control robot idle observer going extracted used learn delayed response task also used identify filtering filtering process discard irrelevant accomplishing task process rated sort relevant achieving task rating ideally delayed reinforcement learning system rewardsin real come simpler realistic recurrent neural network learnt supervised task idea behind task learnt processing access time horizon relevantfor achievingthe task updatedon even slower thus handle occurred even worth noting upwards system work size virtually dimensionality sensory system relaxes processing storage system compact case next describes system constructed straightforward block processing architecture processing system need able control robot response responding come possibility mean coupled last proportionate accommodate reflex association upwards widely interspersed time appropriateas response response ideally affect time next repeatedly execute next keep turning next rigid inflexible system modify response smoother affect modulate mapping modulation also give system sensor subset response show manually constructing mapping behaviour behaviour work subset sensory channel actuator also need something caused layered control architecture brook subsumption architecture architecture summarized architecture realizing processing mean trolling agent robot sensor aravq network classified classification became generatedand localistic winner recurrent network learnt association behaviour time specified behaviour robot employ next occurred aravq adaptive resource allocating vector quantization aravq network lina niklasson vector quantization network vector representing series novel stable encountered system dynamically incorporates vector allocated vector signal turn reflects underlies sensory flow aravq network also biased aravq parameter aravq four parameter novelty criterion stability criterion buffer size learning rate explained cope noisy aravq filter signal last vector stored buffer buffer averaged reliable filtered rest network finite moving calculated last vector representing initially empty aravq buffer filled vector allocated novel stable encountered criterion fulfilled novel euclidean vector last moving last stable moving threshold convenience vector euclidean filtered vector incorporation stability novelty criterion filtered incorporated vector classification winning vector indicating filtered match winning vector match filtered closely filtered vector modified match even closer learning rate aravq network depicted aravq network last buffered used calculate filtered network allocated vector vector allocated automatically novel stable encountered vector convenience labeled recurrent network elman network copy hidden activation feed next memory trace enables network learn association arbitrary node hidden activation time used identifying node used hidden node time used bias introduced matrix indexed sigmoid activation behaviour mapping behaviour constructed corridor follower left wall follower wall follower behaviour subset sensor khepera robot used eight infrared proximity sensor activation denoting sensor denoting obstacle close robot separately wheel denoting backward spinning wheel rotates wheel forward corridor follower sensor left else sensor left else left left left wall follower sensor left else sensor left else left wall follower sensor left else sensor left else left khepera robot hand crafted behaviour hand crafted behaviour detected node code behaviour simulated khepera robot used activation eight sensor robot sensor concert sensor normalized architecture aravq network parameter aravq extraction eight constructed allocated shade winner classification process plotted leaving trail extraction discard repetition leaving long manually eight extracted presentational interpretationof case delayed response task resulting classification simulated robot path subscript denoting repetition aravq seems vector corridor corridor left corridor junction wall left side wall side corner corner eight automatically extracted vector behaviour also time creating alphabet behaviour corridor left side wall side wall hand crafted behaviour worked threecharacter alphabet hidden node used network initialized learning rate momentum used training extracted left turn network trained epoch throughtime bptt unfolds recurrentconnections network bptt unfoldedthe network time problemslearning association case left turn turn extracted vector winner behaviour path stimulus behaviour occurring system turn irrespective delay removed extraction road sign thus rylatt czarnecki call road sign road sign delay stimulus response irrelevant extraction system handling distracting delay drastically delay mean relationship stimulus response harder distracting left turn corridor stimulus passed generating relevance task serve distraction distracting turn happening stimulus junction managed find association trained simpler task continued training difficult hidden node activation plot learnt task depicted note numberof cluster formed robot hidden node activation irrelevant corner remains relatively stable stay hidden node activation functionally left stimulus activation jump corner stay responding arrives time activation quickly jump left corner robot activate left wall behaviour effectively turn left junction stimulus bottom left corner used stimulus encountered corner robot bias turning left junction also sketch road sign look note hidden activation clustered aravq network functionally unimportant repetition winner lead perturbation functionally stimulus jump activation thereby leading perhaps becoming best match filtering process used repetition irrelevant removed note system successfully learnt association relatively filtering extract informationwhich effectivelylets next higherlevel handle delay arbitrary distracting filtered aforementioned process conclusion layered processing system constructed handle delayed response task like road sign rylatt czarnecki attacking sensory data learning system case considerably task redundant data filtered letting system work discrete grounded extraction mean handling arbitrarily long delay stimulus response handling road sign suggest road sign distraction delay thereby putting system lose track supposed work filtered stream filtering taskspecific system learnt task relevant believe steer road acquiring intelligent behaviour robotic friend acknowledgment funded grant foundation competence sweden skovde sweden elman elman time cognitive lina niklasson lina niklasson time series segmentation adaptive resource allocating vector quantization network proc joint conf neural network volume page ieee nehmzow smithers nehmzow smithers mapbuilding network really robot proc conf adaptive page nolfi tani nolfi tani extracting regularity time cascade prediction network connection rylatt czarnecki rylatt czarnecki embedding connectionist autonomous agent time sign neural processing letter tani nolfi tani nolfi learning perceive articulated proc fifth conf adaptive page ulbricht ulbricht handling neural network proc fourth conf adaptive page norn finance forecaster neural recurrent network finance prediction raymond james computing hong kong polytechnic hung hong kong csstlee csnkliu financial prediction contemporary integrated stock prediction system norn finance forecaster neural recurrent network finance prediction system prediction stock price prediction major system automation financial market noegm neural elastic graph integration recurrent neural network integrated automated tool analytic investigation stock investment view stock pricing major hong kong stock adopted system training contemporary neural prediction system achieved challenging term financial prediction stock prediction currency rate forecast hottest financial fact school devised namely murphy ritchie owing fact tool totally give rise contradictory besides widely used financial analyst market technician actually numerous market believe exhibit repeated market move occurrence financial prediction market classified major zirilli identification predict financial identification highly subjective prone major obstacle automation process highly variant sense appearance integrated neural networkbased financial prediction system automates process prediction stock forecast contemporary neural financial prediction integrated automated tool namely market identification scheme prediction oscillatorybased relative strength view stock major hong kong stock data used contemporary time series neural network prediction feedforward backpropagation neuroforecaster genetica norn achieved challenging term integrate norn system framework system neural recurrent network norn integration market system radial recurrent network stock prediction norn basically module automatic market module noegm recurrent network tdsl structural learning stock prediction major norn hand provision automatic stock prediction integrated recurrent neural network hand integrate degree market noegm noegm system neural oscillatory elastic graph noegm main module band feature extraction stock gabor filter automatic segmentation neural oscillatory market elastic graph dynamic link egdlm actually noegm face scene latest tropical cyclone satellite picture composite neural oscillator neural framework schematic noegm gabor filter feature extraction neural oscillatory segmentation phase feature extraction module phase figureground segmentation phase egdlm encoding dynamic link template market egdlm elastic graph module quantified schematic noegm market feature extraction module module gabor filter frequency band orientation used feature vector extracted stock filter segmentation neural oscillatory neural oscillatory neuron cortex numerous neural oscillator neural oscillator excitatory neuron inhibitory neuron interacts oscillates neuron stimulus represents whole horizon perspective column neural oscillator sensor modeled feature vector presence subject stock neuron belong segment oscillated mode zero phase sufficient neural oscillation neuron segment oscillated phase uncorrelated neural oscillatory stock graph broken mesh composite neural oscillator site site column neural oscillator neural oscillation feature response gabor filter excitatory inhibitory neuron neural dynamic composite neural oscillator oscillatory vertical horizontal excitatory connection neural oscillator column mutually activated strength feature neural oscillator activated eight closest neighboring excitatory composite neuron strength vertical horizontal inhibitory neuron control unexpected phase locking vertical horizontal inhibitor introduced governed inhibitory strength inhibitory neuron inhibitory neuron receives excitation excitatory neural oscillator inhibits excitatory unit signal neural dynamic dviq segmentation criterion correlation segmentation criterion governed correlation binding strength phase relationship composite neural oscillator nearest neighbor market elastic graph dynamic link egdlm egdlm elastic graph graph vector memory graph gallery memory schematic egdlm network architecture market striking feature egdlm property network topological neural oscillator dynamic link encoded network process resembled graph transformation translation rotation reflection dilation occlusion occurred egdlm process dynamic link initialization elastic graph module dynamic link initialization process dynamic link memory graph market template initialized rule elastic graph module graph neural oscillator vector node correlation link edge matched graph gallery minimizing minimized gradient egdlm network architecture market radial network nonlinear time series stock prediction network hrbfn system network hrbfn incorporates main network temporal time series prediction structural learning integrates forgetting ishikawa decay corporated network strengthen temporal time series data network training hrbfn past network feedback network major concerned prediction past network enter network unit also affected decay governed time series prediction network predict outcome time past size major influence outcome time series time convenience used throughout network node hidden node predictive step node time simplicity wijt connection node node time hrbfn structural learning main idea learning forgetting decay make redundancy fade quickly cost structural learning wijt learning term penalty criterion delta rule used learning rule weijtt hrbfn structural learning dynamic neural network adapt temporal time series feedback data code temporal decay feedback hybridized learning time series prediction series past time predictive predicted time importantly started time word network overlapping hint network learning temporal sutton besides temporal operation time prediction nearer normally confidence decay operator integrated learning reflect integration learning modified phlt wijt norn stock prediction norn automatic time series prediction illustrate integrated stock prediction stock prediction stock price prediction schematic integrated norn stock prediction schematic integrated norn stock prediction integrated norn main module stock data module noegm market module network module stock prediction stock data module view daily stock data major hong kong stock adopted daily daily high closing stock hrbf recurrent network time series stock data market forecasting norn stock prediction time series stock data processed differently prediction price forecasting data feature extraction prediction prediction interested stock daily price fluctuation type used normalized daily closing stock normalized sell signal clow chigh clow main data feature extraction extract closing price close normalized dividing closing price scaling signal prediction normalized sell signal adopted actually signal system financial previously kind oscillator stock prediction neural oscillator rise sell threshold signal alert market approaching side hand neural indicator fall threshold signal alert market approaching side oscillatory node indicator prediction data feature extraction prediction prediction stock price prediction fluctuation stock daily creating daily price open close high predict next high close normalization type open close high volatility calculated normalized desired high close also normalized fact kind normalization predicted predicted stock price transformed noegm market module neural elastic graph noegm market time series stock stock extracted market template major reversal major continuation view type market extracted time series stock price grouped node reversal node reversal bottom node continuation node network module stock prediction schematic norn time series stock hong kong major stock network time series stock data stock normalization transformation previously window size ranging prediction shortterm stock prediction market extracted identified time series stock quantified correlation previously system validation view type test conducted market test noegm validation window size test norn test market test test four stock four main business banking finance investment public utility property used system hsbc holding banking tian investment finance investment hong kong telecom public utility cheung kong holding property major market stock stock automatic cnom market test market rate market hsbc holdgs tian invest hong kong telecom cheung kong holdgs reversal reversal bottom continuation note rate matched case case rate illustrated market identified four stock series rate matched rate ranging continuation reversal actually view rate type market rate maintain acceptable around bottom reversal bottom continuation bring test case identified subjective justification explained scored degree ambiguity template processing test time segmentation work composite neural oscillatory cnom cloud segmentation lesser degree window size test window size stock prediction deterministic network test aimed optimized window size prediction stock price prediction test window size prediction price prediction used predicted prediction prediction sell signal price prediction high close four stock item used test show term percentage norn window size norn window size test prediction price prediction window size window size illustrated optimal window size stock prediction percentage achieved norn test test stock major hong kong stock adopted neural network forecasting forecaster neuro intelligent business adopted test actually neurotm forecaster neural network time series ffbp transfer sigmoid hyperbolic tangent neurofuzzy builder genetic construction optimization network norn test stock price prediction business type neuro forecaster norn sigmoid hyberbolic tangent neurofuzzy genetica banking finance investment public utility property ease stock item grouped four business namely banking finance investment public utility property stock price prediction conclusion innovative automatic integrated stock prediction serve stock prediction stock price forecast contemporary neural network financial prediction vellido financial prediction neural explores neural network financial neural network successfully integrated financial tool effectiveness financial prediction acknowledgment grateful departmental grant central grant polyu ijade hong kong polytechnic ishikawa ishikawa structural learning forgetting neural network face elastic encoding structural proceeding ieee system cybernetics tokyo japan page oscillatory elastic graph scene proceeding imaging system vega page automatic satellite tropical cyclone elastic graph dynamic link artificial intelligence raymond james tropical cyclone identification tracking system integrated neural oscillatory elastic graph network track mining ieee transaction neural network murphy murphy market york finance prentice hall york ritchie ritchie investment guide selecting stock chicago irwin professional sutton sutton learning predict temporal machine learning page vellido vellido lisboa vaughan neural network business survey system zirilli zirilli financial prediction neural network thomson updating rule discrete neural network delay shenshan eric tsangb daniel yeungb xizhao wangb bdept computing hong kong polytechnic asouth guangzhou ausqiu ssqiu csetsang csdaniel csxzwang hopfield neural network delay hnnd studied standpoint regarding optimized computational updating rule network delay gurd neural network delay optimization characterized dynamic threshold proved updating rule mode gurd monotonously converges stable network diagonal connection matrix influence convergence process relationship stable network ordinary dhnn gurd convergence gurd relaxed nonnegative diagonal connection matrix removed convergence updating rule mode restrictive network convergence connectionist discrete hopfield neural network dhnn hopfield wilson pawley bruck continuous hopfield neural network chnn hopfield network delay clouse studied extensively hopfield tank hopfield dhnn traveling salesman dhnn hardware demonstrated wide combinatorial optimization hopfield tank subjected severe criticism wilson pawley concerning dhnn computational orponen computational finite automaton equivalency dynamic system simulation dhnn scope well dhnn delay property converges stable operating serial mode operating parallel mode hopfield wilson pawley bruck convergence neural network optimization matrix updating rule mode convergence domain deficiency hopfield network optimal parameter infeasible elimination made maintain hopfield network modify network dhnn delay constructed serial updating rule mode hopfield network delay also convergence strictly constrained also dependent updating rule mode well specification convergence dhnn remains open much convergence relaxed fact convergence network architecture network well updating rule mode intimately extend convergence updating rule mode make decrease monotonically evolution time modified mean modified dhnn well updating rule mode convergence dhnn delay give brief review dhnn delay introduces convergence dhnn delay main last concludes hopfield network delay discrete hopfield neural network dhnn hopfield wilson pawley bruck network time vector network time network time network thus parameter node evolved time step node updating next time step hhii neuron allowed time step evolving node unchanged network said operating serial mode node time step evolving network said operating parallel mode dhnn delay computational multilayer perceptron connection feedforward dhnn delay multilayer perceptron node dhnn delay consist node step also time step bruck orponen neuron simplify case represents neuron updating rule mode quadruple used quadruple represents dhnn updating rule dhnn delay vector assumes updating rule dhnn delay extend convergence establish network escape updating rule dhnn delay vector neural subset neural network form neural subset mean defines updating rule xxii otherwiseif updating mode updating rule neural network delay gurd neural subset satisfying evolution mode serial mode neural network delay neural subset give illustrated updating noted updating mode threshold neuron neural subset determines updating mode threshold updating time hopfield neural network threshold stable network constrained word threshold parameter network fixed stable fixed updating mode serial mode parallel mode hand stable gurd mode dynamically distributed neural subset updating mode affected updating mode thus dynamically distributed delay network escape reach peng identify delay item regarded noise motivates investigation updating mode delay neural network code vector code neuron submax submax submax submax submax submax updating procedure bivariate bivariate main gurd mode neural network delay convergence property dhnn delay operating gurd mode symmetric matrix diagonal dominant matrix guaranteed equality hold threshold vector gurd operating mode case thus sufficient case thus sufficient case column diagonal dominant matrix nonzero gurd case updating main show converge submaximum hopfield neural network hopfield neural network modifying diagonal matrix stable coded neither submax greatest code stable efvalue submax submax submax submax submax submax updating revised modifying threshold vector noise gaussian noise diagonal updating show much stable submaximum surprise convergence show adjusting threshold network parameter decrease come time updating test parameter threshold remains open threshold adjusted code stable submax submax submax submax submax submax submax submax updating updating rule summarized illustration used code vector neuron submax submax submax submax submax submax submax submax updating procedure evolving operational process neural subset largest bivariant code represents neuron vector code code efvalue mean updating neural subset efvalue converges stable dhnn delay operating gurd mode symmetric matrix diagonal dominant matrix hold nonempty threshold vector wjixj loss generality column diagonal dominant matrix nonempty diagonal dominant matrix hand diagonal dominant matrix satisfied contradiction indexi completes updating rule noted network trajectory evolves nondecreasing network trajectory step step updating step updating step modification accelerate rate convergence simplify gurd gurd rule dhnn delay vector neural subset neural network neural subset mean updating rule xxii otherwiseif updating mode also simplified updating rule neural network delay sgurd noted symmetric matrix diagonally dominant matrix sgurd comparing constrained invariable case decrease mean reach updating time word need execute sgurd rule step hopfield network updating rule step stable reached bruck noted generalizes extend constrained matrix diagonal nonnegative serial mode nonnegative definite parallel mode gurd mode updating rule process network activation process neuron characterized dynamical process sigmoid remains network activation serial parallel partially parallel mode updating rule mode mode finite gurd mode neural network delay converge fixed fact fixed hand convergence property indispensable practical neural network delay conclusion demonstrated updating rule monotonously neural network delay gurd mode operate updating mode neural network delay converges operating gurd mode mhnn partially parallel mode hopfield wilson pawley bruck derived main acknowledgement like thank reviewer comment suggestion helped improving manuscript partially foundation foundation south hopfield hopfield neural network system emergent collective computational proc acad wilson pawley wilson pawley stability traveling salesman hopfield tank cybernetics bruck bruck convergence property hopfield proceeding ieee clouse daniel clouse giles neural network induction machine ieee transaction neural network orponen orponen neural network symposium mathematical foundation page lecture note berlin germany zongbin guoqing chungping kwong asymmetric network neural network updating rule modified hopfield neural network quadratic optimization neurocompting shenshan eric tsang daniel yeung stability discrete hopfield neural network proceeding ieee system cybernetics page nashville tennessee october shenshan xiaofei mingzhu yadong wang convergence discrete hopfieldtype neural network serial mode shenshan xiaofei chunsheng mingzhu matrix criterion dynamic discrete neural network delay stability hopfield network simultaneous mode ieee transaction neural network peng mengkang peng narendra allistair armitage investigation hopfield network neural network neural network genetic neural network genetic genetic selective neural network ensemble zhou yuan jiang chen laboratory novel nanjing nanjing zhou wujx chensf neural network ensemble learning paradigm neural network jointly used relationship neural network ensemble correlation neural network analyzed reveals ensembling selective subset network superior ensembling network case gasen train neural network employ genetic optimum subset network constitute ensemble show comparing ensemble averaging theoretically optimum selective ensemble enumerating gasen preferable generating ensemble relatively computational cost neural computing rigorous framework neural network successful practitioner experience practitioner neural computing chance gaining success user little neural computing neural network beginning hansen salamon showed neural network system significantly ensembling neural network training neural network combining sollich krogh neural network ensemble collection finite neural network trained task behaves remarkably well easy neural network ensemble regarded promising profit neural computing also ordinary engineer realworld neural network ensemble used many real domain handwritten digit hansen cherkauer face gutta wechsler huang seismic signal classification shimshoni intrator many work done investigating neural network ensemble work classical krogh vedelsby work derived famous clearly demonstrates ensemble ambiguity neural network constitutes ensemble relationship neural network ensemble correlation neural network analyzed reveals case ensembling subset network superior prevailing ensemble scheme ensembling network hand upon subset network difficult genetic gasen genetic selective ensemble train neural network employ genetic optimum network constitute ensemble show gasen superior ensemble averaging network unit also show gasen superior selective ensemble theoretically optimum enumerating subset network selects best subset make ensemble ensemble network independently trained independence many network gasen ambiguity ensemble selecting subset network training make many ensemble rest relationship ensemble correlation neural network analyzed gasen find optimum subset network averaging enumerating gasen reported work overviewed conclusion drawn work indicated correlation learning task ensemble comprises neural network prediction network combined averaging network satisfies ensemble network convenience network approximated note derivation network sampled distribution neural network neural network ensemble network ensemble neural network distribution ensemble distribution neural network neural network distribution correlation neural network note satisfies considering considering krogh vedelsby utilizes correlation neural network ensemble computation refers prediction neural network combined averaging neural network deleted ensemble ensemble considering obvious satisfied mean ensemble omits accurate considering conclusion arrived neural network trained case ensembling subset neural network superior ensembling network network omitted gasen note neural network omitted hard extensive computation noteworthy neural network ensemble unknown employ validation used enumerating utilized find subset network subset selects best subset make ensemble enumerating achieves optimum enumerating nearly impossible realized excessive computational cost ensemble practical routine find subset neural network assign neural network optimum exhibit ensemble network bigger threshold constitute ensemble averaging neural network satisfies vector optimum minimize ensemble considering optimum vector wopt expressed wopt argmin wjcij wopt lagrange multiplier satisfies wiwjcij simplified considering satisfies wopt rarely work well ensemble neural network make correlation matrix ensemble inreversible matrix viewed optimization considering success genetic optimization area goldberg gasen find subset network network trained gasen employ genetic evolve optimum vector wopt gasen selects network optimum bigger threshold constitute ensemble note network washed evolved optimum vector bigger network used constitute ensemble believe network satisfying noteworthy ensemble averaging word evolved optimum vector used network believe vector network prediction easy overfitting gasen realized utilizing genetic goldberg floating coding scheme represents vector evolving population coded byte network note gasen viewed concrete realized employing diversified kind genetic coding scheme validation correlation neural network cijv considering neural network ensemble evolving population jcijv obvious goodness fitness note violate evolving normalization evolved optimum normalization scheme four regression ensemble averaging enumerating gasen friedman friedman continuous data noise item satisfies normal distribution satisfies distribution size training test boston housing machine learning repository blake continuous categorical data comprises make training rest make test ozone breiman friedman continuous data comprises intention dealing missing missing omitted briedman data continuous make training rest make test fourth servo machine learning repository categorical data comprises make training rest make test note quinlan believe difficult kind nonlinearity bagging training rumelhart network training process network epoch validation bootstrap sampling training consecutive epoch training network terminated avoid overfitting averaging enumerating gasen ensemble trained network genetic employed gasen gaot toolbox houck genetic operator crossover mutation system parameter crossover probability mutation probability stopping criterion default gaot threshold used gasen validation used gasen also bootstrap sampling training mean squared deviation averaging enumerating gasen averaging enumerating gasen data deviation deviation deviation friedman boston housing ozone servo test ensemble tabulated pairwise gasen significantly averaging friedman boston housing ozone servo believe gasen superior averaging ensemble accurate averaging case pairwise also gasen enumerating data considering enumerating hardly work network extensive computational cost believe gasen superior enumerating ensemble comparable cost much computation analyzing ensemble find gasen enumerating averagely subset comprises network network constitute ensemble subset gasen enumerating friedman boston housing ozone servo obvious frequency appearance much exhibit considering enumerating optimum size ensemble believe verifies goodness gasen explaining high frequency appearance explored work neural network ensemble area besides achievement cited brief review area sharkey work employed genetic evolve population neural network choosing best neural network last final regarded population neural network ensemble combining last make best contained population genetic used work many conspicuous utilize contained genetic population network ensemble learning correlation learning network trained simultaneously correlation penalty term generating unbiased network uncorrelated correlation learning negatively correlated network encourage cooperation network main correlation learning ensemble ambiguity item famous gasen neural network evolved optimum vector ambiguity ensemble also increased derived ensemble gasen ensemble comprises network rank toppest conclusion relationship neural network ensemble correlation network analyzed reveals case ensembling selective subset network superior ensembling network genetic ensemble gasen show gasen promising ensemble superior averaging enumerating many work left near firstly gasen averaging enumerating data plan ensemble data secondly threshold parameter gasen determines neural network constitute ensemble hope find relationship ensemble thirdly want explore high frequency gasen enumerating subset network make ensemble acknowledgement comment suggestion reviewer greatly natural foundation natural foundation jiangsu province blake blake keogh merz repository machine learning database http california irvine california breiman breiman bagging predictor machine learning breiman friedman breiman friedman estimating optimal transformation regression correlation american statistical association cherkauer cherkauer task system combined artificial neural network proceeding aaai integrating learned improving scaling machine learning page aaai friedman friedman multivariate adaptive regression spline annals statistic goldberg goldberg genetic optimization machine learning addisonwesley gutta wechsler gutta wechsler face classifier system proceeding ieee neural network page ieee hansen salamon hansen salamon neural network ensemble ieee machine intelligence hansen hansen liisberg salamon ensemble handwritten digit proceeding neural network signal processing page ieee houck houck joines genetic optimization matlab north carolina huang huang zhou chen pose face proceeding ieee automatic face page grenoble france ieee krogh vedelsby krogh vedelsby neural network ensemble cross validation learning neural processing system page ensemble learning correlation neural network case bagging boosting ensemble neural network proceeding ieee joint neural network page ieee quinlan quinlan machine learning morgan kaufmann mateo california rumelhart rumelhart hinton williams learning backpropagating sharkey sharkey combining artificial neural ensemble modular system london shimshoni intrator shimshoni intrator classification seismic signal integrating ensemble neural network ieee signal processing sollich krogh sollich krogh learning ensemble neural processing system page population evolutionary artificial neural network ieee system cybernetics part cybernetics neural network learning genetic chew henry chia school computing singapore singapore tancl chiawaik neural network neulonet neural network system strength learn rule originally employed neulonet learning backpropagation resulting adjustment lead loss rule neulonet learn composing rule genetic demonstrate exciting capturing also made rule boolean negation disjunction evolutionary computation fair training neural network genetic golubski feuring gaudet genetic neural network basically negation disjunction adaptive network armstrong thomas linear threshold unit leaf node parent node network composed unit neural network also nonstandard neural network simply neulonet emulates expressed neatly focusing integration neulonets genetically programmed neulonets show combined form system evolving neulonets genetic operation crossover mutation adapted neulonet evolution next demonstrate system effectively classification extract rule evolved neulonet substantiate neulonets network genetic neural network neulonet ordered pair node connection node node node connection ordered pair node take namely true false know undefined activation determines threshold neural network neulonet applying network network behaves like system rule operation truth neulonet behaves like disjunction rule conformity unanimity overriding priority disjunction majority influence veto silence mean consent unless overridden call network rule rudimentary network chained network like system rule boolean operation wide rule show rule emulate maker process expressed neatly reasoning biased giving degree operation inadequate representing varying degree bias rule priority exhibit degree influence outcome unknown seen rule silence mean consent represents default opinion person default case majority rule combine form composite rule complicated rule unanimity rule veto operation realized neulonet combine strength neural network system engineer encode rule real refinement training adjust neulonet training case mean backpropagation pervasively modify network neulonet improves refinement training backpropagation training rule decipherable rule altered trained rule reused view novel neulonet training mean genetic paradigm introduced genetic genetic koza genetic subjecting genetic evolution gene population neulonet evolution stated node rule node containing genetic construct neulonet represents induced carried step population neulonets comprising composition rule rule node node composite rule iteratively substeps termination criterion satisfied fire neulonet population assign fitness fitness population neulonets applying operation neulonets probability fitness reproduce neulonet copying population neulonets genetically recombining part crossover operation crossover neulonet neulonet mutation operation mutation neulonet identified best designated neulonet undergoes evolution labeled tree keith martin population neulonets fashion koza generative normalized fitness produced neulonet well size neulonet used weigh size fitness emphasis accurate expense size neulonet crossover operation swapping part neulonets imposed swapping process preserve syntactic swapping leave dangling node node able receive firing illustrate crossover operation neulonets operation mutation operation neulonet mutation picked neulonet root mutation replaced neulonet show mutation operation neulonet conformity rule replaced priority rule probability reproduction crossover mutation neulonets crossover operation neulonets crossover operation note conformity rule redundant activation neulonet deemed anything evolutionary process kind rule intron biology chromosome never expressed spacing gene angeline levenick note intron genetic assumed engineer construct neulonet subjected refinement training engineer construct neulonet train resultant neulonet ordinary neural network difficult interpret semantics seemingly meaningless arising activation process difficult find genetic paradigm alternative constructing neulonet illustrate process shuttle landing domain michie neulonet mutated rule data comprises show classified conform neulonet distinct pair boolean transformed data unknown data transforms data boolean stable sign wind magnitude auto xstab noauto stab noauto stab noauto stab tail noauto outofrange noauto stab auto stab medium auto stab auto stab head auto stab head medium auto stab tail auto stab tail medium auto stab head noauto stab tail auto shuttle landing domain data neulonet evolution accurate produced consisted rule evolution produced accurate consisted priority rule rooted rule rooted negation rule rooted classification extracting rule neulonets extracting rule neural network studied many gallant gaudet setiono towell shavlik work basically utilizes negation disjunction rule extracted form tree tree classification data work goal extract neulonets extraction fact straightforward process neulonets constructed composition rule case shuttle landing domain classification rule identified negation magnitude sign wind tail priority visibility layman term shuttle biased sign tail wind magnitude presence absence shuttle unknown visibility empirical show rule genetic well data encompass form ordered reasoning confirmed wish rule unit comparable merely rule comprising boolean negation disjunction rule data used publicly irvine data repository blake merz data containing discrete data type transformed binary data attributevalue pair data containing discretizer motoda transformation case data data boolean classification summarizes data used data mushroom data boolean transformed data rule used rule unless overridden rule used population cater wide neulonets evolved distributed parallel fujitsu distributed memory parallel processing system consisting node connection niwa data neulonet mushroom depicting classification best node used population four rule allowed evolve evolutionary process proceed classification size fittest unchanged last weighting used fitness simplify best evolved neulonet eight used simplification rule recursively identify rule elimination recombination evolving best neulonet rule boolean classification term classification neulonet evolution comparable evolution case true data ordered reasoning case shuttle landing domain classification rule encompassed quantification boolean data classification rule thrun exactly clearly difficult high classification composition boolean unit case neulonet evolution expressive inherent rule allowed accurate tree evolved size neulonet handle rule evolution compact boolean counterpart classification particularly data classification rule best evolution empirical data profile node observe profile comparable case classification indicating classification rule learned thus relatively increased apparent neulonets deriving rule resulted significantly size profile node rule boolean evolution drawback rule evolution time converge ideal size rule noted conducted actually gave rule negation disjunction population size neulonet evolution kept opportunity rule quickly evolve ideal population thus involving simpler evolution attained case neulonet evolution profile data profile rule boolean evolution evolution slightly increased became increasingly difficult evolve rule expressing rule neulonet evolution hand produced rate convergence conclusion genetic prof paradigm constructing neulonets prescribed rule paradigm also amenable refinement training engineer constructing neulonets constructed neulonets subjected genetic evolutionary process thus step genetic population skipped process step onwards mode neural network learning learning scratch learning refining network preserve semantics understandable size rule determines granularity step processing time enhances expressing nuance expense time converge carried investigate variant crossover mutation process fitness also studied long term plan genetic fuzzy neural network network ordered pair genetic thus evolve best fuzzy rule envision even exciting horizon fuzzy neulonet learning genetic angeline peter angeline genetic emergent intelligence kinnear genetic cambridge mass armstrong thomas william armstrong monroe thomas adaptive network handbook neural computation fieseler beale physic publishing oxford blake merz catherine blake merz repository machine learning database http irvine california gallant stephen gallant extracting rule neural network neural network learning system gaudet vincent gaudet genetic neural network genetic wang boca raton golubski feuring wolfgang golubski thomas feuring evolving neural network mean genetic genetic progamming proceeding european ricardo poli lecture note keith martin mike keith martin martin genetic kinnear genetic cambridge mass koza john koza genetic mean natural cambridge levenick james levenick inserting intron improves genetic success rate taking biology proceeding fourth genetic belew booker mateo morgan kaufmann publisher motoda huan hiroshi motoda feature data mining kluwer series kluwer academic publisher boston michie donald michie fifth unbridged universal turing machine halfcentury survey rolf herken oxford niwa tatsuya niwa hitoshi distributed genetic empirical genetic proceeding annual edited john koza cambridge mass setiono rudy setiono huan neural network hwee cascade artmap integrating neural computation processing ieee transaction neural network chew tong seng quah hoon heng artificial neural network hoon heng neural network neural network neural network singapore thrun sebastian thrun monk learning carnegie mellon pittsburgh towell shavlik geoffrey towell jude shavlik extracting refined rule neural network machine learning sensitivity multilayer daniel yeung xuequan xiaoqin zeng computing hong kong polytechnic sensitivity neural network network pich used stochastic multilayer perceptron match true closely severe limitation imposed perturbation generalize pich stochastic derive universal sensitivity sigmoidal activation restriction perturbation network parameter neuron activation analyzed network sensitivity help network well training sensitivity neural network investigated year hoff used ndimensional geometry analyze sensitivity adaline hoff glanz simplified hoff winter derived analytical probability madaline caused perturbation stevenson continued winter work established sensitivity madaline stevension stevension cheng yeung geometrical analyze sensitivity neocognitrons neuron threshold activation binary applicable continuous activation yeung wang cheng yeung sensitivity choi choi thorough idea sensitivity differentiable activation systematic applicable network trained used network construction pich statistical relate madaline activation linear sigmoid threshold pich stochastic simplification true hold true perturbation proposes stochastic suited true also derive universal analytical sensitivity sigmoidal activation restriction amplitude perturbation parameter characterize activation perturbation neuron also activation sensitivity analyzed universal analytical stochastic pich stochastic neuron activation network perturbation perturbation identically distributed iidr zero mean statistically weakness bias omitted expectation network zero many network variance also difficult many work hong kong polytechnic grant hong kong cerg correlation perturbation even associative perturbation correlation perturbation successive stochastic overcomes shortcoming network expectation variance expectation bias variance perturbation zero mean variance variance bias perturbation correlation network perturbation expressed correlation coefficient perturbation zero mean variance correlation perturbation also expressed correlation coefficient correlation perturbation perturbation true introduced indexed node activation neuron investigate sensitivity neuron whole perturbation vector neuron perturbation vector variance absolute sensitivity neuron perturbation relative sensitivity neuron perturbation wild wilt neuron neuron absolute sensitivity perturbation snll relative sensitivity perturbation sensitivity calculated difficult analytical sensitivity unless restrict amplitude perturbation severely sensitivity overcome idea activation characterized parameter used activation concerned form convenient sensitivity computation threshold sigmoidal activation used mlps characterized obliquity activation main idea form minimized antisymmetric used sigmoidal activation form sensitivity neuron absolute relative sensitivity need derive variance neuron unit wilt wilt covariance perturbation perturbation xixli andwl wijwl correlation coefficient variance expectation wilt wilt wilt wilt wilt wilt wilt lindeberg central normally distributed mean variance correlation coefficient wilt distribution joint distribution expressed expectation eedudv variance dedudv polar transformation eyil arctg neuron former covariance calculated derived exactly activation need restrict perturbation network sensitivity expressed term activation enables investigate activation network sensitivity sensitivity sensitivity sensitivity vector neuron calculate sensitivity neuron successively calculate sensitivity network expectation variance network perturbation ratio correlation coefficient perturbation variance perturbation ratio correlation coefficient perturbation activation neuron calculate applying goto step else calculate absolute relative sensitivity terminated parameter sensitivity many affect sensitivity network perturbation perturbation network architecture parameter neuron activation affect sensitivity criterion neural network main perturbation ratio affecting sensitivity absolute relative sensitivity ratio network sensitivity absolute expectation variance normalization network help decrease network sensitivity variance network sensitivity amplitude training process neuron absolute relative sensitivity significantly sensitivity nearly remain neuron influence upon absolute sensitivity absolute relative sensitivity sensitivity rapidly flat perturbation ratio absolute relative sensitivity well sensitivity supposed perturbation ratio obliquity activation sensitive sensitivity threshold sensitive activation suitable tanh relative sensitivity neuron neuron obliquity activation activation relative sensitivity neuron activation absolute sensitivity relative sensitivity simplify network variance perturbation ratio identical neuron sensitivity neuron sensitivity network sensitivity vector parameter network sample parameter goal find unknown parameter acceptable relative sensitivity unknown parameter sensitivity mapping mapping satisfies show distributed correlation coefficient perturbation construct perturbation ratio relative sensitivity parameter parameter sensitivity calculate mapping plot contour obliquity contour line contour line sensitivity used network numerically permitted network training find permitted variance neuron hidden must restrict variance training obliquity activation reduces mean activation obliquity permit neuron hidden variance network training contour plot relative contour plot relative sensitivity sensitivity conclusion stochastic used universal network sensitivity sigmoidal activation sensitivity investigate network parameter network sensitivity also used network architecture guideline network training
