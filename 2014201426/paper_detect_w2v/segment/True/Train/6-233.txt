reinforcement learning modeled composite markov process modeled mdps coupled reinforcement learning address optimal mdps merging composite task optimized separately well merged composite searching optimal mdps find good composite task reinforcement learning reinforcement learning successfully task main area reinforcement learning scaling scaling take form best hierarchically goal subgoals learning agent address task last form scaling concerned naive learning composite task relevant agent learn joint receiving accomplished suffers curse dimensionality dimension size joint grows exponentially promising sort well train learning module handle learning module dana ballard rochester rochester dana rochester used fairly distribute control module independently explored humphrys karlsson attractive simplicity good empirical domain highlight previously unrecognized modular learn highly composite task take fact module forced control show replacing closely sarsa learning rule resulting show sample formalized formalism many reinforcement learning markov process transition probability arriving payoff taking goal reinforcement learning discover optimal maximize discounted long term discovering joint throughout subscript distinguish mdps distinct execute time step case agent simultaneously faced goal mdps implicitly composite formally goal find optimal composite optimal composite maximizes summed discounted mdps composite cross mdps composite case poster mdps composite transition case mdps composite transition depend dependency composite practical size composite grow exponentially mdps modular humphrys karlsson humphrys karlsson independently reinforcement learning idea learning module agent take module trained rule learning rate parameter discount goal reinforcement learning used rank also used degree preference module used compromise execute referred karlsson suggestion call greatest mass module summed execute refer greatest mass humphrys considers greatest mass raise objection highest particularly good module module able reach goal explores alternative constrain optimal module module promotes module largest allowed execute preferred simplest generating refer thus giving control module highest suffers drawback module highest preference module stand lose great deal sometimes exhibit reasonable strongly dependent alternative referred negotiated grant control module stand lose long term module discovered examining refer humphrys training food gathering task training divided trial lasting time step data suspending training trial computing mean trial exploration exploration linearly half trial fixed learning rate discount modular attractive reinforcement learning chief fact learning mean guaranteed converge optimal regardless followed training long pair visited infinitely fact make easy convergence composite reinforcement learning introduced easy module guaranteed converge optimal deterministically composite also guaranteed converge concerning composite unfortunately also serious limitation difficulty module optimally compromise module control mean converge composite bias modular sarsa poster bias replace learning explore sarsa rummery niranjan sutton sutton rule sarsa rule virtually identical replaced pair actually next step case mdps sarsa proved converge optimal long exploration rate asymptotically decayed toward zero schedule sarsa suffer bias actually best expect sarsa module discover closer true composite recast sarsa train module greatest mass refer resulting recall goal maximize summed mdps trustworthy utility module make sense highest summed utility module lead greatest summed long term reasoning hold utility inaccurate composite thus convergence gmsarsa refer sprague ballard convergence demonstrates sample composite task task adapted cohn goal agent task gather stationary food item avoiding predator grid food item time agent move eight time step move made probability agent contact food item receives item moved agent receives time step avoids predator predator move deterministically toward agent time step food item well agent predator million distinct monolithic tabular learning practical task good modular reinforcement learning decomposed mdps describes agent predator mdps food item mdps show well task four exhibit best conclusion learning approximately optimal composite markov process empirical demonstrate work proving convergence work sprague ballard acknowledgment upon work grant education grant grant grant grant foundation grant opinion conclusion recommendation expressed necessarily reflect view mentioned institution
