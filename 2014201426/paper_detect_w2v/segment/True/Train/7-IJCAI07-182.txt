kernel nonlinear feature extraction dimensionality reduction widely used preprocessing step classification data mining task definite kernel well data implicitly mapped feature high dimensionality goal find dimensional subspace feature retains classification data subspace kernel feature extraction transformed kernel parameter learning projecting data dimensional subspace feature parameter used describing subspace regarded parameter kernel projected data kernel parameter learning adapted optimize parameterized kernel validate effectiveness feature extraction dimensionality reduction widely used step classification data mining task extracting proper feature reduce noise remove redundant data irrelevant classification data task data data feature extraction linear discriminant linear data dimensional subspace constructing nonlinear kernel scholkopf smola proved successful definite kernel data implicitly mapped feature high dimensionality kernel applies linear mapped data kernel kpca scholkopf smola used extract subspace linear kernel powerful explore nonlinear data flexible linear simply linear kernel kernel dimensionality high even infinite helpful separating data high dimensional redundancy irrelevant even noisy classification data mining task case feature extraction also helpful classification data mining task find dimensional subspace many kernel feature extraction find dimensional subspace feature kpca scholkopf smola widely used task mentioned linear feature goal find data variance largest feature extraction improving classification classification data label belongs seen kpca classification unsupervised feature extraction ignores label data supervised make data label like kpca also linear feature extraction linear dimensionality reduction feature kernel fisher discriminant kfda mika find data minimizing variance maximizing variance simultaneously thus achieving good discrimination also constructing label variant kfda xiong distinct property scale feature extracted fixed wold widely domain chemometrics extract feature variance data covariance label guide extraction feature kernel kpls rosipal trejo orthogonal centroid park park linear dimensionality reduction preserve cluster data data firstly clustered projected spanned centroid cluster orthogonal subspace applying matrix column consist cluster centroid dimensionality reduction text classification task exhibit good kernel nonlinear kernel orthogonal centroid also park park incorporate label treat data cluster extracted feature cluster subspace kernel nonlinear feature extraction transformed kernel parameter learning rest idea formulate subspace kernel connection optimize subspace kernel last nonlinear feature extraction kernel parameter learning idea mentioned definite kernel implicitly introduces mapping data high dimensional feature projecting subspace kernel modified correspondingly feature convenience call modified kernel subspace kernel parameter used describing also parameter subspace kernel kernel parameter learning adapted optimize kernel find discriminating subspace data well separated idea formulating aforementioned subspace kernel subspace kernel dimensional subspace matrix column constitute orthogonal subspace spanned mapped data uniquely decomposed part contained orthogonal complement case subspace vector expressed linear vector rnnf matrix coefficient linearly dimensional subspace spanned vector thus subspace lemma lemma projecting data kernel matrix projected data kernel matrix data calculate subspace spanned column need orthogonal build rnfnf diagonal matrix eigenvalue matrix matrix column eigenvectors lead precisely widely done literature feature extraction dimensionality reduction used extracted feature classification kernel matrix projected data refers matrix projected data unit matrix column form orthogonal subspace fors matrix column projected data pute subspace used line line fifth line identical lemma proven also tell data subspace introduced subspace kernel subspace kernel empirical kernel scholkopf smola illustrates ters ofthe subspacekw find discriminating subspaceis also serve kernel paramewhere data well separated sturn optimize subspace kernel connection work feature kernel parameter learning weston chapelle kernel parameter learning adopted feature kernel form vector namely optimizing kernel parameter maximization radiusmargin chapelle penalizer feature done choosing feature optimized feature locates discriminating subspace also kernel parameter learning find discriminating subspace address feature extraction feature subspace want find contained feature sparse kernel learning subspace kernel form column matrix vector feature show kernel relates work case column preimage scholkopf smola vector subspace spanned convenience note case subspace kernel sparse classifier slmc build sparse vector machine vapnik vector pointed slmc kernel slmc find dimensional subspace spanned data linearly well separated kernel lagrange adopted kernel case subspace kernel seen subspace kernel case derived optimizing optimize alignment cristianini quantity degree fitness kernel learning task maximization wheredenotes frobenius matrix size equally sized matrix gram matrix label label reflect label kyij belong make data thus maximizing find subspace closer good classification data projected subspace note subspace kernel many kernel parameter learning feature extraction also chapelle simplicity gradient used maximize conjugate gradient utilize fact thus decompose column column time ncnf computing frobenius time inspired investigate arbitrary vector actually linear algebra straightforwardly vector vector note vector need calculated calculated empirically investigate classification task kpls subspace kernel feature extraction skfe scheme xiong feature extracted neighbor classifier classification test data used extracted feature also classification data mentioned classification feature extracted fixed skfe also feature extracted skfe varied kpls tried best reported skfe convex optimization good guess subspace initialization tried kpls microarray gene data subsection take seven microarray gene datasets test brain brain prostate tumor dlbcl datasets datasets data much data dimensionality datasets adopted seven microarray gene datasets last seven text datasets data dimensionality gene gene gene gene gene dlbcl gene tumor gene text text text text text text hitech text gaussian kernel used five fold cross validation conducted parameter best cross validation rate used repeated time independently show mean cross validation deviation observe skfe kpls favorably skfe improves case used skfe also seen skfe kpls competitive significantly judged dlbcl tumor kpls skfe brain skfe outperforms kpls brain prostate tumor text classification subsection investigate text classification task cluster text data equivalently linear kernel keep used dimensionality reduction text classification task exhibit good seven text datasets trec collection adopted hitech seven datasets microarray gene data data used text classification task also high dimensionality seven datasets highly unbalanced mean data contained dataset data contained seventh data ninth former dataset half data form training remaining data test done linear kernel used dataset repeated independently time test deviation reported illustrates skfe outperforms datasets also seen case data whilst data dimensionality dramatically skfe favorably term classification computational cost reported kpls take skfe take optimization step skfe matlab conclusion subspace kernel nonlinear feature extraction conducted kernel parameter learning connection work explained spare classifier slmc illustrates case derived subspace kernel also optimize subspace kernel alignment cristianini maximization kernel parameter learning also validate effectiveness
