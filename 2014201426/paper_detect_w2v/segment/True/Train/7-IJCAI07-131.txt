employ improvereinforcement learning achieved identifying tree constructed used locating derived optimal utilized defining essential reflecting onto experience acquired learning broader effectiveness demonstrated empirically reinforcementlearning problemfaced agent must learn dynamicenvironment gainingperceptsand taking affect realistic domain task agent composed subtasks hierarchical formed barto mahadevan subtasks repeat many time subtask subtasks identical self guidance agent learn independently going learning affect learning process difficult converge optimal reasonable time main lack connection subtasks scattered throughout build connection temporally taas generalize primitive last time sutton dietterich barto mahadevan taas extensive domain difficult constructed work technological council turkey grant ically generating stolle precup menache mannor simsek identifying occurring trajectory mcgovern girgin subtasks probably subgoals term subgoal discovered treated separately capable combining subtask sharing inductively fact constitute subtasks equivalence used sharing reflecting experience acquired connection subtasks established implicitly fact reduces repetition learning consequently improves equivalence closely markov process mdps givan equivalence upon stochastic bisimilarity said optimal homomorphism ravindran barto equivalence pair reduction case bisimilarity equivalence framework derive compact redundancy relativized relativized user automatically zinkevich balch also addressed symmetry mdps used accelerate learning employing equivalence stateaction pair case permutation feature agent prominent formalizing homomorphism identify requiring equivalence show integrated framework learning collected history trace fragment tree form efficiently identify reflected expanding influence experience treated metaheuristic guide demonstrate effectiveness reporting test domain namely taxi room doorway substantial test case also work hierarchical test demonstrate applicability effectiveness rest briefly framework discrete time finite mdps equivalence homomorphism show used learning process identified learning process reported conclusion background motivation briefly background introduced defining tuple finite finite admissible pair transition stationary mapping defines probability selecting infinite discounted agent gain agent take resulting pair agent find optimal maximizes experience form sample collected simulated find approximating optimal kaelbling sutton barto barto mahadevan inherently subtasks need agent subtasks simpler mdps locally preserve transition dynamic markov process homomorphic directed edge transition edge label causing transition mdps absorbing admissible transition back certainty surjection homomorphism pmdph pair also block transition homomorphic pair said bijection pair equivalence ravindran barto proved homomorphism optimal constructed optimal make homomorphic structurally simpler construction viable case pmdphs optimal depend absorbing deterministic discount optimal homomorphic absorbing accordingly optimal optimal demonstrates unless carefully employing homomorphicimages equivalence pair used learning process pair homomorphic imageunder surjection learning optimal agent selects take mapped accessible taking probability transition mapped probability experiencing transition upon taking upon taking also pair mapped absorbing regarded virtual experience tuple pretending transitioned agent assumes equivalence pmdphs learning restricted pmdphs identify transition history show next experience gathered reflected learning rest restrict pmdphs absorbing homomorphic identity mapping restriction longest prefix pmdph restricted consequently list actionreward property lead natural calculate andbe pair homomorphic pmdph inverse restriction mapping counterpart sinceare calculated ratio ofwith close term transition close case differ considerably note even eventually deviate follow subtask part threshold inevitably decrease permissible contrary high also optimal subtasks robustness take kmin kmax used combine approximately degree calculated regarded threshold likewise restricting pair approximately identified learning dynamic system consequently accessible history agent must incrementally kmax learning enumerate actionreward calculate auxiliary path tree prefix actionreward path tree labeled rooted tree node node represents actionreward edge label indicating appending root node represents empty node hold list oftuples stating restriction arbitrarily path tree initially containing root node repeat repeat step execute derived sufficient exploration observe next append terminal history kmax traverse eligibility tuples node prune tuples eligibility threshold clear history termination hold eligibility representing occurrence frequency incremented time path tree gradually decremented path tree root node edge label node path tree initially root node node edge label tupleis incremented tuple node containing tuple node edge label node node episode termination reaching peak case task consecutive fragment kmax path tree procedure eligibility tuples node tree decremented decay eligibility decay rate tuples eligibility threshold threshold removed keep tree manageable size used path tree calculated incrementally traversing keeping node containing node containing tuple list initially tree tuple list node processed incremented accordingly processing kmin kmax pair tuples list node latter note eligibility stored node path tree occurrence frequency extend incorporating eligibility normalization also discrimination domain high degree likelihood trajectory also consideration simplify opted omit calculated pair identified incorporated learning process test domain taxi also examined parameter eligibility decay rate test case qvalue probability used averaged unless stated path tree eligibility decay rate decay eligibility threshold threshold parameter employed kmin kmax maze taxi maze taxi experience replay decay kmin taxi size path tree show agent goal reached maze sarsa testing used learning rate discount sarsa calculated episode backward reflection sarsa converges much learning curve early learning effectively utilize considerably convergence attained episode sarsa indistinguishable taxi showing agent taxi agent passenger probability sarsa episode episode agent gain experience path tree maze sarsa learns regular smdp bradtke duff primitive agent execute move agent predefined numberof step althoughsmdp steep learning curve utilizing symmetry effectively long learning curve reveals proposedmethod successful identifyingsimilar lead early agent learn task efficiently taxi demonstrate evident idea updating lead qvalue experience remembering past experience reprocessing agent repeatedly experienced experienced experience replay learning process accelerating propagation experience replay also experience test gain term learning simply fact made experience replay applying regular taxi even learning improves experience replay fall also reflecting semantically rich neutrally experience replay turn experience replay transition time past experience relevant dynamic sequencetree handle well analyze parameter affect learning conducted decay eligibility decay decrease path tree also decrease dominate consequently identified also converges regular qlearning show affect taxi domain kmax show indistinguishable path tree shrink considerably kmax decrease despite fact inherently kmax near kmin well restriction pmdphs also pmdphs conclusion analyzed learning identifies reflects experimentsconductedon highlighted applicability effectiveness utilizing learning process reported test demonstrate experience transfer attractive make learning system parameter eligibility decay must carefully reduce computational cost keep size tree manageable also actionreward exactly matched partially grouped neighborhood case distribution continuous domain also varies thresholding reflecting whole experience make degree updating proportion work examine extending improving
