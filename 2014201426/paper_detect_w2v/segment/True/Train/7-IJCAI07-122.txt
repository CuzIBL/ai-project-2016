bayesian locally learning novel varying coefficient multivariate regression spatially localised linear learns adapts data driven fashion derive parameter variational bayesian reveal excellent robust beside surprisingly time property time brings computational adaptability locally learning scheme modelling bayesian locally regression lwpr vijayakumar prime area localised learning scheme resulted powerful regression capable operating high dimensional learning proven work many real supervised learning sensorimotor dynamic degree freedom anthropomorphic robotic system vijayakumar locally scheme lwpr validity adaptive data driven fashion achieved minimising sort cross validation cost gradient initialization parameter forgetting learning rate gradient necessitate careful hand tuning open parameter trivially achieved many real domain also proper probabilistic learning framework bayesian retaining flexibility nonparametric localised learning attractive localised learning scheme learning rule combine blend prediction avoiding interference schaal atkeson property also asynchronous learning leading preserve property generative probabilistic derive learning rule show novel robustly estimating competes learn truly incremental fashion storing data surprisinglyefficient computational varying coefficient modelling spatially localized linear probabilistic framework deriving case linear bandwidth combined prediction novel data additionally capable modelling data learning bandwidth data part corresponds linear accomplish formulating probabilistic varying coefficient build upon idea coefficient longford locally linear centered around generative data represents subtracted bias augmented vector represents regression coefficient gaussian mean zero noise deviation data assumed generative fashion crucially regression coefficient distribution assumed gaussian centered around confidence covariance covariance proportional close distribution peaked around resulting linear around illustrated schematically close assign fairly tight around mean away much broader covariance matrix restrict diagonal diagonal varying quadratically bandwidth parameter kernel defining extent dimension kernel parametrization conjugate gamma imply lesser amongst coefficient linearity bandwidth modulates tradeoff unconstrained likelihood maximization favor confidence data gamma regularizer bandwidth parameter favor relatively leading localised gamma assign noninformative normal parameter noninformative inverse gamma hyperparameters regularizer hyperparameters summarizes resultant probabilistic marginalize hidden cixi note form likelihood corresponds heteroscedastic regression used prediction next deal computing parameter resultant ensemble learning learn parameter hyperparameters joint used vector training data parameter rendered intractable difficulty evaluating denominator necessitates variational bayesian learn regulariser hyperparameters variational learn parameter maximize marginal likelihood parameter treating hidden marginal likelihood jensen optimal make tight joint intractable make expressed form ensemble variational beal substituting factorised expectation distribution optimal probability iteratively maximizing fapprox distribution keeping distribution fixed akin procedure procedure factorised iteration skipping derivation procedureyields distribution gamma part derived woodbury expectation spect respective vector diagonal also need learn regulariser hyperparameters likelihood hyperparameters maximizing fapprox hyperparameters keeping distribution fixed considering term involving hyperparameters maximising hyperparameters minimising divergence distribution parametric form divergence minimised parametersof distributionsmatch lead rule hyperparameters hyperparameters initialised correspondingpriors initialisation ensures hand regulariser hyperparameters initialised encourages sufficiently ensures bias used carried prediction committee dealt coherent probabilistic derived inference procedure parameter ensemble trained predict response take normalised predictive distribution close spirit paradigm hinton bayesian committee machine tresp predictivedistribution form integrate delta mode final tive distribution ckhmode refers subtracted augmented bias blending prediction taking normalising normal distribution mean confidence expressed prediction variance diag learning rule parameter data rewritten form exploiting bayesian formalism batch mode posteriorn likelihoodi expressed posteriori likelihoodi priori posteriori transform batch derived repeat data till converge represents step illustrated learner adapted deletion predictive likelihood data sufficiently learner need increased lead heuristic wherein data predictive probability training data fixed threshold data serf sufficient overlap redundant pruned overlap confidence expressed prediction test deletion heuristic used used schaal atkeson training initialise hyperparameters repeat posteriorhyperparameters hyperparameters regulariser convergence time dominated computation rewritten avoid explicit computation diagonal cixi also fact diagonal hand gixi turn also matrix inverse also fact diagonal matrix time dimension data stored sufficient statistic stored independence also mean time brought parallel processor time prediction mean confidence time fact match lwpr real time learning demonstrate salient looking empirical test robustness benchmark datasets bandwidth confidence learned show linear test learned noisy training data varying spatial hard learn high bias tends oversmooth nonlinear tend noise linear roughly corresponds tangential line bandwidth bottom plot converged bandwidth parameter dimension illustrating adapt parameter data driven illustration dense uniformgrid inputspace target confidence learned gaussian process williams note deliberately avoided training data confidence nicely reflect confidence motorcycle dataset learned centered distributed next illustrate heteroscedastic data data varying noise illustrates confidence learnt motorcycle data rasmussen gharamani confidence adapts varying noise data confidence learnt squared exponential kernel localised learning used batch mode derived make derived confidence motorcycle dataset learned gaussian process learning trained data sinc corrupted noise data sinc boston ozone learning dynamic sinc training data learner learning distributed test allowed iteration data fair lwpr resulting dynamic exhibit sharply decreasing curve surprising considering batch away training data prediction lwpr find converges roughly attributed bayesian learning rule parameter likelihood sparse data learning ensures distribution parameter turn prediction learner reasonable also optimization regularizer hyperparameters data convergence artificial well real datasets sinc dataset bruntz boston housing dataset repository used benchmarkdatasets ozone dataset dimensional dataset data training test dimensional boston dataset training test learner namely lwpr trained epoch repeated presentation training data till convergence lwpr careful tuning initialization learning rate performancereported opposed uninformative used lwpr statistic accumulated asymptotically well sinc data achieving nmse ozone dataset highly nonlinear lwpr boston dataset find close lwpr slightly inferior statistically insignificant major contribution bayesian spatially localised learner multivariate nonlinear regression used novel data dependent carve locally linear regionswhile avoiding competition amongst behaviour learning bayesian regularizer hyperpriors guard danger overfitting automatic bandwidth evaluated artificial well real data matched lwpr avoiding cumbersome parameter tuning initialization achieves competitive performancecompared batch much computationally linear training data dimensionality opposed cubic training data computational coupled grow data driven fashion make practical real time learning
