investigate gradient reinforcement learning gradient amenable geometric lead natural controller parameterization considering manifold probability distribution path induced stochastic controller investigation lead covariant gradient ascent rule property rule style reinforcement learning computationally lead dramatic noncovariant rule much work reinforcement learning stochastic optimal control focused numerous domain encoded representational domain convergent scored encouraging success bagnell schneider baxter helicopter control gradient lead gradient applying jacobian gradient intuitively difficult justify gradient computation actually indicating steepest well recognized patternrecognition statistic area kakade best identify reinforcement learning suggest geometry valuable inspired work amari nagaoka kakade natural gradient kakade scheme generating parameter property convincingly kakade showed empirical evidence usefulness kakade applies tetri bertsekas tsitsiklis particularly bertsekas tsitsiklis demonstrate iteration improves dramatically iteration much worse normal gradient conjugate also ineffective even tremendous round mildly kakade show rapid achieves significantly peak comparable time despite recognizing defect intuiting apparently powerful kakade concludes also fails covariant leaving open believe kakade work proper probability manifold collection kakade must rely generating take proper manifold distribution path induced controller case kakade scheme give rise natural despite learning believe perhaps note parametric invariance numerous covariant stronger probabilistically natural invariance used motivate importantly pathdistribution natural kakade finite horizon discounted reinforcement learning well nally completeness discovered kakade relating compatible actorcritic sutton stochastic control path also system trajectory distribution pathspace finite control indexed time throughout considering partially markov system compose path render past pomdp next transition probability also measurable controller parameterized history distribution control derivation term memoryless stochastic controller distribution finite window recurrent operate goal control maximize reinforcement assumed controller path pomdp additive time infinite time case discounted averaged necessarily predicated reinforcement learning adaptive control attept maximize reinforcment sampling jectories improving throughout parameter clear vector distribution covariance riemannian manifold steepest ascent interested steepest ascent parameter wish maximize subject infinitesimal reparameterize controller term infinitesimal term parameter remain naive size lead steepest rule learning rule demonstrate transit control self transition earns label simply transition transition odds logp horizontal axis corresponds curve correspond varying transit achieves parameterized probabilistic form arbitrary scale parameter demonstrate even mildly parameterization lead dramatically plot resulting track ratio probability make scale plot odds clear graph note difficult good wrong overwhelmingly sampling used gradient nearly never sample need path distribution manifold control coupled optimization integration path motivates idea considering arbitrary term parameterization term distribution path resulting view distribution path parameterized manifold nominally embedded dimension take work visualize path distribution path smoothly parameter left visualization embedding probabilistic planning probability manifold path axis represents probability path manifold formed vary parameter defining throughout domain attach tangent parameter parameter redundancy dimensional manifold case pictured distribution path dimensionality path probability manifold tremendously path manifold consideration probability distribution path path parameterized manifold like pictured domain differential geometry interested establishing riemannian manifold path mean wish establish tangent linear manifold parameter gent spanned parameter definite matrix natural rotation scaling exactly definite matrix vary throughout manifold depict tangent parameterization linear steepest ascent riemannian manifold naturally steepest riemannian riemannian manifold path sense natural quickly answer pursue next lagrange multiplier schematically make easy form steepest form lagrangian take derivative zero optimal definite invertible giving steepest simply normal gradient time inverse evaluated tangency call natural gradient reimannian manifold amari nagaoka invariance chentsov confusion seems surround probability manifold answer even supposition parametric invariance suggested subtle transforms parameter jacobian connecting parameter meet type parametric covariance natural suggest preserve essential probabilistic manifold markov mapping congruent cmbeddings sufficient statistic viewpoint distribution path natural recoverable mapping distribution path path congruent embedding depicted path mapping interchange role path path probability probabilistically manifold path path imagine path smoothly parameterized distribution path uniquely probability parameterized distribution path embedded path equivalence actually phrased theoretic morphisms congruent embeddings ichentsov congruent embeddings thought simply depicted arbitrary permutation arbitrary probability path stemming path well composition control arise permutation redundant natural congruent embedding isometry tangent preserve vector make size distribution path carrying invariance congruent embeddings lead scale manifold chentsov statistical inference fisher matrix degroot derive think probability relative entropy distribution natural divergence distribution also manifestly think derivative parameter differential discover also taylor agrees fisher scale note irrelevant famari nagaoka manifold derive fisher path distribution turn case process markovian easy computation make likelihood ratio reinforcement learning derivation path fisher computing fortunately easy essential gradient like reinforce gpomdp clever computing thus gradient correlation fisher matrix unbiased converges surely regularity baxter sample path infinity computation markov property invariance transition probability simply extracted well potentially sample natural gradient simply invert matrix multiply gradient limiting infinite horizon statistic degroot give form fisher regularity quickly derive show give form path probability line integrating part fifth observing probability show lead infinite horizon case convergent must normalize path scale perfectly justified ergodic markov process fisher matrix fisher control stationary distribution markov process likelihood ratio probabilistic planning limiting bution thus infinite horizon undiscounted give weighting differing partially partiallyobserved markov process distribution easy derive tuple also markov subtle limiting distribution compatible actor critic kakade noticed fascinating connection limiting equality noting compatible sutton lihood ratio transition probability konda tsitsiklis kakade probability ergodic compatible aptheorem last line fisher proximator linear tion term line vanishes probability also happens startstate discounted case like path distribution naturally weigh necessarily infinite horizon case discount undiscounted trajectory terminates probability step fact derive discounted formalism discounted markov process fisher matrix fisher formation control limiting distribution similiar infinite horizon case simply sketch type approximator initially suggested used true gradient practice clear brings gradient routine baxter infinitesimal iteration moving best good property significantly outperforms gradient natural gradient insight minimize squared sutton easy kakade exactly natural gradient seen simply differentiating minimize noting sutton demonstration consequence ikakade demonstrating effectiveness natural gradient compatible approximators implicitly computing demonstration analytically natural gradient covariant scaled grad covariant learning dramatically outperforms gradient achieves iteration path showing horizontal axis covariant scaled grad note path scaling covariant reveals property natural gradient checked boltzmann natural gradient computes mean similiar gradient remove weighting stationary distribution treated equally lead much reasonable derivative shrink initially plot natural gradient scaling gradient optimizing note graph natural gradient noncovariant simply step size heuristic computing step illustrated unfortunately difficult considering induced manifold used geometry natural covariant lead insight practical fortunately agrees heuristic suggested kakade despite suggestion actually covariant infinite horizon case extends peter independently robot dynamic control work investigation also yield deeper insight relationship
