many multiagent distributed sensor network agent formed agent neighbor distributed pomdps capture realworld uncertainty multiagent domain fail distributed optimization dcop capture fails capture planning uncertainty synthesized distributed pomdps dcops networked distributed pomdps exploiting network enables distributed many multiagent distributed sensor network agent formed agent neighbor distributed sensor sensor agent must neighboring agent track target moving area motivated lesser sensor node scan four north south east west track target sensor overlapping scanning area must scanning area simultaneously target target uncertain unaffected sensor agent additionally sensor receives area scanning false false agent cost scanning target cost turned distributed pomdp montemerlo nair becker hansen rich capture uncertainty sensor domain unlikely work well domain geared take even agent trying distributed pomdp distributed satisfaction distributed optimization dcop sensor capture uncertainty domain networked distributed pomdp pomdp dcop handle uncertainty domain well take exploiting network enables novel distributed agent tuple refers agent unaffectable unaffectable refers part affected agent environmental like target agent control joint agent transition distributed pomdp transition joint resulting agent transition unaffectable transition becker also relied transition independence goldman zilberstein introduced possibility uncontrollable feature work assumed collectively observable hold domain joint agent make observational independence joint alki refer agent sensor grid expressed sensor agent overlapping area agent cost sensing construct hypergraph subset agent comprise hypergraph agent vertex edge neighborhood jnisj refers neighborhood distribution refer distribution unaffectable qjni agent refine make agent goal joint maximizes team finite horizon refers agent mapping history refer joint agent thought dcop node agent thought corresponds graph next push analogy taking inspiration yokoo hirayama distributed satisfaction neighborhood utility agent accruing agent trying find best agent neighbor need nonneighbors property used locally optimal locally optimal locally interacting distributed joint equilibrium yokoo hirayama jesp nair agent neighbor distributed initially agent neighbor evaluates contribution belief neighbor agent upon getvalue agent best response neighbor agent computes gain make neighborhood utility gain neighbor gain neighbor sends neighbor process trying continued termination maintaining exchanging counter gaini omitted simplify presentation computing best response used jesp episode agent time neighbor cies fixed treating episode agent pomdp transition stni getvalue optimal agent transition getvalue iteration agent pomdp used sensor domain benchmark agent prevv termination detected prevv agent gaini getvalue agent prevv gaini maxgain maxjni gainj winner argmaxjni gainj maxgain winner findpolicy agent communicate else maxgain receive winner winner jesp nair jesp centralized processor find locally optimal joint graph benchmark graph show time logscale finite horizon show finite horizon time averaging seen jesp lidjesp converged optimum jesp comparing time noted outperforms jesp acknowledgment upon work force laboratoryunder view conclusion contained representing official expressed implied defense agency government
