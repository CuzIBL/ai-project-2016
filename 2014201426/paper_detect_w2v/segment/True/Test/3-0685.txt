realistic reinforcement learning agent unknown pomdp must follow continuous uninterrupted experience access reset offline simulation pomdps near optimal convergence rate exponentially horizon time optimal dependence unobservable main block reset show pomdp balancing exploration exploitation address lifelong learning partially observable markov process pomdp agent unknown pomdp desire near optimal agent forced obey dynamic permit reset lifelong learning well studied observable mdps kearns finite time agent obtains near optimal unfortunately applicable challenging pomdp fact none literature learning mdps pomdps partially observability pomdps balancing exploitation exploration little literature pomdps planning sondik lovejoy hauskrecht cassandra learning parr russell peshkin goal reset button fact best literature even asymptotic pomdps agent near optimal part difficulty belief tracking showing divergence belief eventually crudely belief tracked show continually degrade time agent eventually loose track belief infinite horizon observable boyen koller address belief tracking perfectly goal keep compact belief note belief tracking much simpler agent acting fixed finite horizon accumulation horizon learning pomdps agent optimal finite time convergence rate exponential dependence horizon time optimal dependence pomdp reminiscent trajectory tree kearns dependency assumed access generative allowed simulation pomdp plethora literature planning pomdps lusena feel dependency best hope central reset homing idea reset literature homing used learning deterministic finite automaton rivest schapire reset agent homing move approximately reset show finite convergence rate also depend time take approximately reset note existence imply agent must take reset spent exploring exploiting turn homing exploring exploiting fact homing infinitely unfortunately detracts exploiting able show ratio time homing used time spent exploiting decreasing sufficiently rapidly near optimal preliminary partially observable markov process pomdp finite probability observing transition probability transition probability history finite pomdp mapping history belief distribution belief probability observing belief last optimal belief make pomdp reach probability make ergodicity note pomdp disconnected best hope optimal connectivity maximizes formally depend limt suprt refer time optimal timescale optimal achieves close restart belief mean reset history distributed homing clearly reset agent designated test disposal utilize reset show subtle reset must reset reset done homing homing agent neither exploring exploiting moving fixed homing hope move toward fixed unknown belief pomdp periodic stopping time must stopping time fictitious agent take mean homing decides take time true pomdp permit agent ignores obtains homing execute agent homing real agent real pomdp stay reset reset homing belief belief reached homing real approximately reset poor show amplify homing show homing lemma reset reset consecutively time belief contraction induction arbitrary fact linear operator term distribution vector used fact show walk reset pomdp periodic reset disposal lemma pomdps walk constitutes reset connectivity reach probability walk probability moving markov irreducible markov aperiodic thus stationary distribution time convergence linearity expectation belief step reinforcement learning homing demonstrate rate convergence success homing exploration exploitation exploration idea time exploration trajectory reset approximately grounded belief recall idea exploration find good exploitation goal unfortunately guaranteed well step time homing back close rerun gradually process homing wasting time neither exploiting exploring reset phase foreach fordo step repeatedly time trial phase argmaxt time time exploration phase step repeatedly runtimes homing asymptotically never stop homing nonetheless able show obtains near optimal pomdp ratio time spent exploiting homing decrease sufficiently fast pomdp obtains optimal probability convergence rate simpler establishes take reset walk crude reset work phase interleaving exploration phase exploitation phase describing exploration phase resetting step exploration phase obtaining trial followed reset bias variance variance stochastic pomdp bias fact never exactly reset time parameter phase fixed latter lemma belief close lemma show accurate lemma phase reset homing time satisfies probability deal bias belief homing time must straightforward belief belief time linearity expectation variance hoeffding imply close expectation expectation trajectory probability exploitation highest exploration phase lemma close largest note guaranteed step like time reset time time reset close close unfortunately mean spend step like fraction time spend resetting note fraction desire thought accurate reset reset exploitation phase long time comparable last exploitation phase lemma time phase time time satisfies probability proving lemma probability importantly note loss term infinity phase know optimal home infinitely ratio time spent homing time spent exploitation going show exploitation phase time exploitation phase time time spent time plus time spent next exploration phase latter case time exploration phase exploitation phase show used satisfies probability lemma probability probability exploitation close probability exploitation good used find exploitation phase reset exploitation step step reset exploitation fraction simplest demonstrate inefficient testing doubly exponential resembles kearns mcallester exponential horizon time dependence pomdp convergence rate term horizon time optimal term homing time recall time pomdp walk pomdp probability achieves close optimal step pomdp exponential computational runtime exponential next page exploration phase build transition probability history occurred phase build history exploitation phase best markov homing note exploration phase take step reset homing reset time step step time time exploration phase time step step time done empirical frequency trajectory form empirical probability observing conditioned history followed taking history unlikely empirical need accurate history history followed lemma phase reset homing time probability note probability history hoeffding hold probability last used fact exploitation dynamic note pomdp history exploitation phase best interleaving homing step lemma satisfies probability sketch observe ignoring history view node tree optimal empirical decreased fact true history probability node node next optimal empirical loses tree node history backward induction show true optimal history base case leaf history hold encoded imperfect reset marginal distribution lemma induction hold marginal distribution induction summing term completes induction step subsection long last exploitation lemma time phase time time satisfies probability lemma sketch note exploitation exploration phase next exploration phase lemma exploitation near optimal satisfies probability lemma observe bias exploitation variance hoeffding exploitation time probability last loss reset exploitation well probability acknowledgement work part programme european network excellence grant israel foundation faculty award publication reflects view
