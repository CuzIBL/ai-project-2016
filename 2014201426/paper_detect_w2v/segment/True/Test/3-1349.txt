criterion evaluating learning multiagent system little main agent adapt learn well extending criterion adaptive opponent memory show provably achieves response richer opponent simultaneously guaranteeing payoff opponent well also demonstrates empirical test opponent wide work learning forth judging bowling veloso conitzer sandholm shoham arguing merit also demonstrated meeting criterion unfortunately even criterion applicable behave well presence stationary opponent dodging arise opponent adapting agent past play criterion bowling veloso agent converge stationary opponent agent play best response opponent converges stationary criterion satisfied player ultimately repeatedly playing nash equilibrium game provably meet criterion player normal form game player conitzer sandholm adopt restatement criterion achieves criterion arbitrary repeated game note neither make payoff achieved opponent potentially exploited arbitrarily adaptive opponent chang kaelbling work bowling address vulnerability agent experience zero traditionally payoff achieved playing stationary opponent history move minus payoff agent proven zero hart jafari game work fudenberg levine literature also limitation whole inability capitalize opponent play address limitation stronger consistency achieves fudenberg levine limitation payoff opponent ignores possibility move played opponent dependent agent move justified game player serious liability repeated player aware much work dealing limitation farias megiddo address rational learning kalai lehrer handle adaptive arbitrary long probability failure adaptive opponent hurt repeated prisoner dilemma game prisoner dilemma extensively studied axelrod numerous agent cooperate advantageous cooperation outcome exploited simplest perhaps cooperating thereafter repeat whatever opponent played last note considers stationary opponent must play defect best response stationary opponent ever payoff playing cooperate yield payoff clearly best response richer considering adaptive opponent playing stackelberg game repeatedly strictly dominated regardless opponent chooses agent prefer play opponent learning presumably prompt play left resulting payoff agent played seemingly suboptimal opponent learn play giving agent payoff teaching play much role achieving desirable outcome learning game successful cooperate opponent manipulate opponent weakness reliance stationarity previously acknowledged shoham targeted optimality target opponent achieves best response opponent compatibility achieves payoff nash equilibrium pareto dominated nash equilibrium safety opponent receives game parameterized target opponent optimal also address adaptive agent work stationary opponent work adopt criterion analyze behave well opponent adapt past experience repeated game player repeatedly play simultaneous move normal form game tuple player player agent round agent accumulate joint outcome observe agent agent assumed trying maximize normal form game full game payoff agent refer player opponent mean imply adversarial full game cooperatedefect left cooperateup defectdown prisoner dilemma stackelberg game game player payoff column player payoff adaptive opponent goal work expand opponent best response need opponent depend arbitrarily history play lose learn anything repeated game ever history opponent history limiting history requiring opponent play depend past history outcome game opponent default past history game note even capture many unable properly handle criterion opponent best response unreasonable many prisoner dilemma game opponent playing grim play cooperate grim opponent initially playing cooperate switch playing defect indefinitely opponent ever play defect history note learning best response opponent must play defect distinguish cooperating grim remedying constrain opponent sufficient opponent agent past opponent play assign probability past history alternative relax best response target requiring agent best played game target highest achieved arbitrary move need exploration even restriction target response high probability exponential exploration find good outcome agent need sample exponential history opponent considers opponent unbounded unless probability playing manipulative metastrategy introduced shoham stationary opponent much intuition behind adaptive opponent idea behind metastrategy teaching phase payoff opponent play play opponent consistent target adopts best response achieves target getting opponent adopt best response teaching continues playing selects default play long exceeds reverting maximin payoff drop show target membr calculates best response maintains opponent history calculate agent highest agent repeated subsequence sufficient response target opponent calculate probability opponent play consistent target comparing distribution play history time measuring deviation profile continue minimax replace generous stochastic godfather littman stone godfather motivated folk repeated game selects outcome game matrix payoff agent stochgodfather time step play time step avgv alue vgodfather probability membr play opponentintargetset time step play membr opponentintargetset beststrategy membr else beststrategy else stochgodfather avgv alue vgodfather beststrategy stochgodfather else beststrategy membr game avgv alue vsecurity play maximin else play beststrategy manipulator play target outcome opponent ever play target outcome agent play force opponent opponent play target stochastic godfather selects mixed agent target opponent joint give opponent also denies advantageous deviation want godfather implementable history need make sure opponent profit deviating turn playing target next incurring punishment parameter specified desired empirical used reusing framework used metastrategy minor modification show main must agent play mixed beginning game sufficient test opponent play consistent target mainly long hoeffding hoeffding manipulator satisfies property stated memory training probability opponent assigns opponent agent last round game gamut game payoff test used comprehensive testing shoham nudelman besides metastrategy opponent bully godfather littman stone tesauro watkins dayan smooth fictitious play fudenberg levine bowling veloso also stationary stochmem learns best response history show successful distinct normal form game adaptive fare well stationary opponent bully manipulator lesser degree fare best memory adaptive godfather stochmem manipulator godfather also opponent learn best response manipulator fall outside target metastrategy manipulator metastrategy slight mainly play pure manipulator constrained explore opponent coordination manipulator well achieving highest payoff tested metastrategy stationary miss opportunity cooperation game turn type game show relative achieved successful game gamut averaged opponent manipulator best nearly game game like prisoner dilemma hawk dove godfather able manipulating opponent yielding sending clearer message exploration waiting opponent adapt stubbornness prof undoing game adapt opponent dispersion game metastrategy shapley game seems stem default fictitious play exploiting localq manipulator metastrategy game like prisoner dilemma traveler dilemma equilibrium repeated game paretodominate equilibrium game last round averaged opponent game gamut divided achieved agent make manipulator demonstrates consistent wide game mean claiming best fare nearly well adversarial game like matchingpennies rochambeau shapleysgame surprising unable find deal godfather opponent adaptive opponent alternate adapting watkins dayan footstep numerous attempting find learning littman claus boutilier tesauro qlearning learns chooses maximizes alternative dealing adaptive opponent incorporate history learn history learn conducted test conditioned history demonstrated adaptive smoothfp resulting game neither well game dominated manipulator opponent tested full game also lack manipulator profit requiring game payoff additionally identify particularly lead powerful portfolio suggested pointed forth modelling opponent many used literature rationality neyman papadimitriou yannakakis agent modelled finite automaton note automaton comprehensive opponent memory modelled automaton stochastic automaton modelled finite fixed history case automaton deterministic transition modify manipulator handle godfather replacing best response note learning best response opponent modelled unknown finite automaton best unknown pomdp investigated chrisman nikovski nourbakhsh difficult computation able property alternate opponent need handle finite automaton ergodic automaton arbitrarily transition probability conclusion work feel addressing adaptive opponent learning system seems combining teaching manipulates adaptive opponent playing agent adapts best opponent adaptive opponent simultaneously guaranteeing payoff opponent well high probability translate good empirical wide clearly work done analyzed adaptive opponent memory considering best incorporate empirical additionally restriction considers identify five limitation opponent criterion clearly definedfor game player criterion clearly forrepeated game stochastic game criterion game inwhich agent care aggregated discounted full observability agent need perfect observationsof opponent move game game need know thepayoffs agent beginning game minor modification transformation discounted markedly viewing test much relax restriction turn hope work serve step widely applicable
