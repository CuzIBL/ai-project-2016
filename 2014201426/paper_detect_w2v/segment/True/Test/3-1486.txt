statistically verifying controller deriving practical binary classifier loss case deriving probability false show derive classification derive probability false well tradeoff false false statistically verifying computational property controller organic vehicle computational control vary considerably arises computation system control environmental disturbance operational mode system analytically hard system computation must time command issued actuator next sample instant uncertainty pose controller deterministic time preferred many despite lesser bring practical acceptance control compromise make type controller highperformance used safe operational envelope guaranteed allocated time outside computationally simpler used simulation data safe statistical identifying binary classification false merely mean conservative controller false drastic consequence loss vehicle thus like statistical safe classifier provably probability false criterion trivial classifier classifies everything namely goal push keeping probability false computational property controller ducted propulsion unit control movable vane propwash vane situated propulsion airflow consequently propulsion control surface highly trim computational time elgersma morton interested reliably used statistical learning derive statistical soes classification loss classification false loss case interested practical asymptotically competitive literature vast collection majority stated proved probability misclassification applicable derived little emphasis obtaining optimal emphasis empirical loss true case false loss interested deriving loss preliminary probability expectation variance rademacher dependent symmetric pair fixed probability training finite sample drawn independently probability expectation hypothesis loss loss hypothesis loss empirical loss training loss finite classification classification binary deal binary classification classification incur zero loss emerges loss refer classification false false idea false costly false good made type loss misclassification probability classifier false training hypothesis loss loss statistical learning probabilistic answer form succinctly confidence reasonably sample size empirical loss hypothesis dimension subset note binary classifier growth clearly dimension vapnik applying vapnik learning binary classifier loss thus natural many address majority formulated proved goal examine loss turn fall applicable applicable preliminary sauer lemma sauer lemma sauer hypothesis empirical loss like probability bounding major classical vapnik chervonenkis concentration talagrand classical classical satisfies sample size take former sample size referred ghost sample empirical loss ghost sample bounding reduces bounding covering step bounding symmetrization step covering step bounding intuitively empirical loss sample empirical loss ghost sample also next step permutation next expectation mapping expectation cardinality sauer lemma summand hoeffding thus arrived lemma vapnik chervonenkis upon lemma idea bounding probability absolute discrepancy probability relative discrepancy weaken proceed identically replaced arrive lemma vapnik chervonenkis vapnik chervonenkis considering relative discrepancy managed term resulting tighter lemma tightening hoeffding blumer combinatorial lead lemma blumer symmetrization step bounding ignore probability equivalently hypothesis suffices intuitively quantity empirical loss wherever loss bernstein coupled lemma parameter unspecified minimize loss binary binomial parameter thus binomial case combine lemma vapnik chervonenkis case combine lemma blumer sample size sample derived offor previously stated proved loss extends case show hold covering remains symmetrization binomial parameter regardless talagrand observe suphh supremum changing thus mcdiarmid mcdiarmid next quantity expectation supremum accomplished rademacher real rademacher role played loss symmetrization bartlett suphh thus remains rademacher wellestablished covering lemma dudley last piece puzzle reveals dimension lemma haussler combining algebra absolute discrepancy deviation term come surprise natural used analyze form relative discrepancy loss empirical loss algebra show consequently mcdiarmid render unbounded thisp term originates work talagrand concentration bounding suprema empirical process talagrand talagrand socalled entropy best lemma bousquet countable supff supff supff lemma proceed massart referred peeling nonincreasing bartlett fixed show final step done koltchinskii panchenko dudley entropy integral asymptotically comparable classical albeit worse sample size hypothesis empirical loss loss answer form high probability hypothesis finite dimension case assumed seems binomial tail much simpler used idea convergence relative discrepancy ucrd even viewed degenerate case ucrd combinatorial replacing hoeffding convergence absolute discrepancy lemma vapnik chervonenkis refer pessimistic case simply pessimistic loose need hypothesis loss close need concern hypothesis zero empirical loss learning literature vast collection case learning case learning binary classifier work derived binary classifier penalty originate seminal work vapnik chervonenkis contribution case talagrand arrive loss asymptotically classical analyzes mean median panchenko supremum sometimes discrepancy empirical loss talagrand concentration completed invariably symmetrization dudley entropy integral haussler packing resulting much derived identify four affect computational time hypothesis dimension hypothesis dimension thus need sample need sample sample sample best sampled converges turn roughly converges despite high empirical rate success practice loses controller fixed deterministic computation time next hypothesis sampled converge unsafe eliminate zero false safe outside hyperrectangle false fewest false looking able come safe unsafe hypothesis thus false note false constitute hypothesis surface well nevertheless trivial empty dimension thus sample statistical read probability false confidence alternative procedure zero empirical loss restrictive sample lead hypothesis false false reduction maintaining confidence probability false replace criterion false criterion prefers hypothesis volume willing make tradeoff false false false costly thousand false work said divide practice grand canyon proportion perhaps loose practice counterargument form verifying controller demonstrated industrial military deriving safe operating envelope control control engineer principled increasingly replace control maintaining statistically high confidence safety attractive deriving practical binary classification hitherto much build upon unweighted binary classification show significantly vapnik precisely pointed false penalty namely symmetrization successfully verification framework control reported expect outside controller verification practical view used mostly bartlett aside also used control system deriving robust control system identification vidyasagar also machine learning long recognized learning classifier loss work area referred learning turney derived consist empirical loss confidence term probability sample thus necessarily loose need hold distribution sample focused hypothesis rademacher koltchinskii bartlett relies talagrand yield worse classical used derive entirely data priori hypothesis dimension bartlett fixed empirical opposed loss empirical rademacher opposed used thus priori dimension hypothesis probability practice computing estimating fixed easy bartlett made progress acknowledgement work part defense agency darpa force laboratory thank michael elgersma help xuanlong nguyen many comment
