classifier natural generative data estimating parameter training data bayes rule classifier many made generative evidently wrong leaving open work learning major statistical learning natural linear statistical hypothesis learning exhibit robustness property many statistical learner used natural naive bayes markov entropy hypothesis explaining robustness predictor even probabilistic hold coherent view learning work help learning role learning natural inference generative probability principled statistical classification domain natural generative data parameter training data bayes rule classifier natural classifier derived probabilistic probability sentence bayes rule decompose probability probability generative grant natural processing relevant history predicting note token word scheme used derive classifier natural speech rabiner part speech tagging kupiec schiitze disambiguation gale spelling correction golding bayes rule harmless work statistical modeling ambiguity resolution devoted estimating term form generative used term make markov independence evident looking data patently false dependency sentence hidden markov generative tagging estimating probability part speech word word sentence preceding surprising poor probability distribution density classifier built false nevertheless seem behave robustly many case learning phenomenon show used learning natural make prediction linear statistical hypothesis family linear predictor fairly feature independence probabilistic assumed success classification derived incorrect probabilistic density expressive derived classifier robustness property linear statistical hypothesis evidence lack hypothesis namely show hypothesis feature well training data learning well previously unseen data irrespective probabilistic hold show used literature cast hypothesis selecting statistical feature appropriately affect robustness derived hypothesis main contribution unified learning widely used natural hope coherent explanation learning work help developing learning role learning natural inference learning preliminary linear statistical hypothesis property show cast probabilistic predictor framework learning learning preliminary learning valiant learner need close unknown target labeled unknown sumed distributed unknown probability distribution learning draw sample labeled eventually hypothesis hypothesis rate hypothesis learner goal probability hypothesis rate parameter confidence parameter sample size relevant parameter learner learner learning done time relevant parameter realistic variant agnostic learning haussler kearns applies want labeled training arise target assumes data sampled arbitrary distribution simply reflect distribution data contradiction label rule true hypothesis goal learner high probability hypothesis true learner task distribution agnostic learner hypothesis sample size time hypothesis relevant parameter practice true many case hard hypothesis close optimal learning sample labeled learner find hypothesis empirical hope behaves well hope classifier learned training well previously unseen learning valiant vapnik stated informally training data test data sampled distribution good training sample good test data good true quantified convergence dimension vapnik combinatorial parameter richness practice sample size fixed simply much true hypothesis also computational choosing hypothesis minimizes empirical even approximates hard kearns hoffgen simon main training data test data sampled distribution golding roth robust learning defines learning hypothesis property show capture many probabilistic learning used learning statistical kearns viewed learning interacts restricted viewing request statistic distribution labeled construct hypothesis probability labeled feature indicator defines subset mapped indicator viewed transformation mapped simplify sometimes view feature indicator labeled statistical form feature restriction imposed parameter call oracle satisfies clear omit statistical learning construct hypothesis oracle said make form usual said good learning high probability hypothesis sample size relevant parameter simulate oracle distribution simply draw sample labeled chernoff tolerance probability learning introduced kearns studied decatur aslam decatur viewed tool demonstrating learning learning learner tolerate noisy label label probability inconsistent target classification noise well form corrupted malicious noise noise learning learns sense hypothesis true noise tolerant hypothesis true even trained noisy data time data sample depend tolerance statistical turn noise tolerate next robustness satisfying practical view natural processing linear statistical hypothesis feature linear statistical hypothesis predicts clearly linear discriminator feature coefficient potentially generalizes naturally classifier label case discriminator predicting linear learning hypothesis naive bayes predictor duda hart derived label feature statistically pendent consequently bayes optimal prediction probability fraction labeled tional feature probability fraction labeled feature naive bayes alfeatures note formalized done golding feature assumed unobserved naive bayes feature false yield feature coefficient roth derivation hypothesis linear yiedls robustness dimension anthony holden restriction imposes coefficient dimension hypothesis feature used form linearly dimension hypothesis exactly learning previously unseen sampled distribution training maintain scale linearly feature used distributional robustness hypothesis call oracle applies evaluates hypothesis make prediction introduced tool demonstrating learning practical learning fixed sample data hypothesis tested sample hopefully training sample goal show able learn restriction namely interacting guaranteed robust hypothesis intuitively predictor really depend statistical property data tolerance well presence data sampled distribution long statistical property tolerance next formalize intuition show proved adapted formally test sample stest distribution care hypothesis sample training sample strain distribution show hypothesis trained strain well stest distribution easy sure convenient biased strictly also used case yielding robustness learning distribution inversely also learning esis close behaves well hypothesis behave yamanishi show well divergence well simply oracle call erance probability occurring thus procedure simulates implying well behaved hypothesis simulate accurate tolerance need sample polynomially final regarding robustness observe show tolerance practice sample robustness distribution sample size richness feature play role tolerance feature need sample sample size sample simpler feature robustness turn traded learned simpler feature probabilistic classifier cast widely used probabilistic classifier hypothesis implying subject property naive bayes naive bayes predictor hypothesis implication widely used natural task gale golding satisfies naive bayes robust learning sense bayesian classifier naive bayes predictor naive hidden allowed simplest distribution postulating hidden hidden observable independently motivating case disambiguation tagging natural task viewed hidden prediction predict remaining show predictor feature observable naive bayes case grove roth roth naive bayes independence around predicted complicated bayesian network trivial naive bayes network case care predicting hard bayes optimal predictor hypothesis feature case degree neighbor network lack concentrate case great natural markov markov hidden markov hmms used modeling disambiguation task tagging rabiner familiarity tagger transition probability probability emission probability probability word transition emission probability training data markov probability part speech sentence sentence rust computes markov namely assumes word sentence word word yielding term word preceding yielding sentence need maximize term assignment done dynamic viterbi interested main computational step process predicting word sentence neighboring word disregard optimization best simultaneous assignment word call predictor markov predictor prediction easy interestingly sometimes optimization prediction actually delcher roth zelenko natural processing predict equivalently word maximizes word resp markov predictor singleton pair word feature introduced clear markov predictor hypothesis formalization word sentence yield training feature word predicted markov predictor singleton pair word markov predictor robust learning sense fact naturally generalizes markov predictor expressivity feature entropy entropy jaynes exponential distribution feature distribution term feature training used successfully predictor disambiguation task ratnaparkhi ratnaparkhi estimating probability probability distribution maximizes entropy consistent feature must form normalization parameter parameter corresponds exactly feature viewed feature prediction probability distribution sentence predict equivalently taking logarithm linear surface feature optimal coefficient predictor also feature statistic training data darroch ratcliff approximates optimal coefficient converge efficiently predictor thus linear predictor strictly speaking optimal form consistent learning learning naive bayes classifier case studied machine learning hypothesis consistent training data rely good hand need seek consistency simply computes hypothesis response oracle hypothesis make prediction verified hypothesis produced naive bayes consistent data used nevertheless data satisfies probabilistic used derive predictor namely feature label hypothesis optimal predictor make many mistake probabilistic hold success predictor success training data consistency consistency trivial fact hard come structural property naive bayes optimal predictor even target boolean consistency achieved naive bayes good predictor boolean distribution exactly half necessarily probability probability independently domingo pazzani naive bayes optimal boolean disjunction hold distribution naive bayes hold viewed counterexample anything else overthe probability probability probability independently anything else thatp easy naive bayes make mistake predict incorrectly half clearly indicative predictor data sampled distribution practitioner simply hypothesis confident well hold represents likelihood hypothesis spend time choosing feature predictor well training achieved predictor behaves well previously unseen happens regardless probabilistic limitation growing feature expressiveness feature sample size tolerance alluded role probabilistic case viewed supplying guidance selecting good feature even seemingly hard task good feature fairly unnecessarily resort expressive feature orthogonal address companion sake sufficient realize rely good training thus behave well even probabilistic used derive predictor hold conclusion last year seen surge empirical work natural part work done statistical machine learning roth investigated used preliminary step developing work work continues foundation learning natural focusing evidence golding roth feature probability virtue satisfying independence nevertheless significantly worse prediction task roth probabilistic density natural prediction learning developing learning work step studying role learning natural inference acknowledgment grateful jerry dejong lenny pitt referee comment
