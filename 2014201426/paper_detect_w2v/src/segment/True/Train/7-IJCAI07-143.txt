searching feature subset yielding optimum tends cardinality feature high text categorization particularly true massive datasets learning worse linear scaling linear vector machine svms performer text classification domain work best rich feature even benefit feature sometimes extent work alternative optimum feature subset substantial benefit term computational able demonstrate compromise term made case gain achieved linear vector machine svms vapnik best performer task involving text categorization joachim lewis richness natural text categorization characterized feature even infrequent feature removed dimensionality tends high learner pose computational lead overfitting training data case linear svms good many case achieved little feature joachim lewis mladenic best feature mean guaranteed gabrilovich markovitch relatively easy identify text classification best achieved aggressive feature even optimum achieved dependence classification feature used exhibit saturation whereby feature able curve classifier informative feature achieves unfortunately optimum feature repetitive retraining work investigate alternative reinduction feature able demonstrate substantial gain term computational actually lead accurate lead feature criterion feature ranking induced text categorization domain favorably mladenic mainstream feature criterion gain rogati yang feature svms linear creates classification attempting vapnik linearly separable identifying subset exactly vector linearly separable case vector enriched training classified cortes vapnik balance loss training data case vector derived convex optimization linear vector expressed feature training feature lagrange multiplier unless vector svms lagrange multiplier must label svms proven robust dealing feature overfitting tends learner text classification domain feature need svms reported well established feature forman work mladenic investigated efficacy feature ranking imposed trained ranking absolute feature outperforms established alternative justified mladenic fact sensitivity separating linear feature dependent sensitivity norm vector expressed mladenic feature receiving absolute little orientation optimum hyperplane mladenic criterion filter fashion john whereby ranking identify optimum subset feature ordering guyon followed alternative wrapper john subset relevant feature removed trained remaining ranking used identify next subset feature remove hardin assign feature redundant presence relevant feature recursive guyon thus successful compensating desired identify fairly relevant feature text domain optimum feature tends utilizing recursive yang kalousis work exclusively filter variant feature mladenic avoiding feature traditionally investigation efficacy feature subset relies inducing subset comparing achieved type learner repetitive cost term computational time case linear svms cost quadratic term training linear term feature used massive datasets rich feature text categorization identifying optimum thus feature good feature ranking text stable subject feature differently interested significantly reoptimized restricted domain feature linear svms orientation hyperplane normal vector final hyperplane bias term adjusted utility misclassification cost recall feature subset hyperplane normal onto feature ignoring masking removed feature contribution address projected hyperplane normal substantially normal induced feature classification projected hyperplane normal induced feature membership supportvector affected dimensionality training data normal vector feature masking procedure derive operates feature unchanged vector intersect feature subset retained feature masking transformation masked expressed feature masked gain insight property masking feature masked optimal linearly separable minimizes lagrangian subject thus satisfies mask feature vector training vector keeping lagrange multiplier unchanged optimization dimensional derivative lagrangian remain thus projected lowerdimensional meet optimality well true feature ignored also need maximize meet originally equality training vector sake hold outside true violated feature actually sparsity text fraction violation seen lead departure optimum represents vector feature masked expect keeping lagrange multiplier fixed masking feature close optimum validity feature infrequent fewer training affected feature absolute departure optimality feature masking normalization text domain transforms feature vector unit norm compensate variability joachim dumais leopold kindermann transformation introduces feature weighting feature varied feature masking preserve feature weighting relevant feature removed counter extent normalization alternative retain lagrange multiplier renormalize training vector feature removed thus ization note renormalization default feature used linear induced feature baseline feature ranking feature examine feature extent effectiveness alternative collection dataset represents collection newswire lewis used collection consisted training test divided belonged collection represents largest corpus used involving text categorization training test lewis grouping restricted ontology training data belong case hierarchical relationship subtopic open directory purposefullyrepresent svms feature concerned gabrilovich markovitch preprocessed removing markup punctuation converting case extracting feature token alphanumerics delimited whitespace case collection used feature file corpus website term frequency ignored feature appeared zero binary experience well coupled documentlength normalization importantly work binary magnitude perturbation depended independently derived normalization beneficial text classification joachim dumais leopold kindermann word occurred training corpus eliminated procedure categorization broken series task treated turn target remaining playing role dataset naturally consisted task fairly well balanced comparable training test target classification term area receiver operating curve best attainable tuned adjustment test data reported term macroaverage best task induced feature ranked absolute feature removed fraction ranking feature computing fraction also best acknowledgesthat optimum feature labeling reinduction feature masking mask masked renormalized labeled averageauc best collection best feature seen best numerically close dataset feature also statistically insignificant used macro outlined yang masking collection best achieved feature best varied illustrated case apparent feature beneficial exemplifies fact optimality feature dependent criterion note high fraction ranking feature retained feature feature paired ttest masking feature pronounced collection consisted balanced reporting classification gabrilovich markovitch used paired significance classification best feature feature note approachesto optimize feature substantially feature statistically show statistically masking underperformed case illustrate benefit feature surprising gain correlation best feature collection pairwise macro also determining variant best best aside measuring feature classification investigate pearson correlation coefficient cosine masked vector vector retraining feature bottom vector feature show dependence pearson correlation coefficient cosine feature used datasets vector close even fraction feature used hyperplane normal vector thus weakly dependent relevant feature projecting vector onto ranking feature obtains oriented close optimum show overlap feature overlap vector training feature bottom fraction containment also tends decrease fewer feature used also show fraction part feature seen overlap high exceeds long sizable fraction feature used overlap mainly decrease case fraction used remains consistently high overlap containment fraction feature used fraction feature discarding thus apparent feature deemed irrelevant fact correlation masked vector vector reinduction high despite overlap vector training significantly alters lagrange multiplier compensate vector normalization normalization relative contribution noneliminated feature fewer feature seems happen substantially altering orientation optimum hyperplane conclusion work stipulated stability robustness svms unnecessary searching optimum feature subset able demonstrate experimentally feature masking reinduction fast retraining showed orientation normal hyperplane vector remain stable feature pruning relevant particularly true orientation vector feature masking evidence text categorization hyperplane normal mostly affected relevant feature marginally influenced relevant feature utilizing feature masking practically attractive much runtime cost masking feature ranking fast gain feature reinduction identify optimum feature magnitude fact type feature scale scalable learner naive bayes
