backward inhibition connectionist network used alternative enhancement competitive unsupervised learning feature backward inhibition superior competitive feature feature independence controllable grain feature distributed classification hierarchy feature discovered connectionist neural network consisting network processing node posse cognitive hinton lang fukushima malsburg bienenstock dalenoort learning grossberg ackley rumelhart storing hopfield anderson connectionist automatic correction noisy graceful degradation cell erroneous inherent parallelism connectionist transparent preferable even form learning parallel distributed processing straightforward form learning supervised learning associating backpropagation rumelhart boltzman machine learning ackley analogical mathematical discriminant regression kohonen supervised learning winston quinlan kodratoff ganascia data sufficient supervised learning response case merely criterion response survival criterion living organism reaching emotional lead modification supervised learning response reinforcement criterion even weaker reinforcement learning response fail high illustrated backpropagation rumehart encoding materialized connectionist system system learning scheme system scale poorly system five seem computationally intractable obvious limitation supervised learning many turn intractable latter ballard network modular part network find autoassociation form unsupervised learning ballard distributed highly supervised learning discovered learning capable unsupervised classification discovering feature regularity data rumelhart zipser grossberg clustering network form analogy conceptual clustering stepp michalski lebowitz suffer deficiency argued rumelhart zipser suffers uncontrollable grain feature constructing redundant feature difficulty feature hierarchy novel type feature backward inhibition suffer deficiency mathematically variance cluster past feature classified competitive representant critically examined mathematical explanation backward inhibition rudimentary correlation treated noncompetitive correlation referred adaptive node node feature feature vector connection feature node activation vector activation rule learns widely used correlation learning rule continuous form rule case studied kohonen page case kohonen vector converges dominant eigenvector eigenvector largest dominant eigenvalue correlation matrix equally true vector feature unit discover feature dominant eigenvector serious feature modified satisfactorily competitive correlation modification competitive rumelhart zipser grossberg feature feature node activating feature proportionally feature cell feature unit maximal activation maximal vector vector vector fire modified obvious learning process vector feature maximal vector learns moving toward vector competition vector attract nearest feature partitioning exclusive cluster competition feature probably fire feature fire system converge stable feature corresponds cluster stimulus cluster significantly cluster formal learning process rumelhart zipser shortcoming feature cell remain inactive never fire outperformed consequence behaviour coarse feature refine grain discovered feature feature cell feature cell probability many remain inactive remedy rumelhart zipser case exhibit competitive hrycej deficiency competitive feature competitive disjoint substantially feature satisfying hrycej guaranteed contrary feature frequent fact feature feature simply complement feature activates feature node posse advantageous property correction coding automatic hinton distributed none feature discovered competitive represents finer partitioning disjointness feature hierarchical hierarchical modification system cluster discovered partitioned arrangement priori structuring network tree inefficient successor node inactive feature node remain inactive drawback parallel operation parallel alternative maximization reggia chun note criticism concern property competitive valuable property competition noise reduction contour enhancement covered alternative backward inhibition drawback competitive lack discover finer feature simultaneously coarse seems competition backward inhibition cope backward inhibition reflection idea subtle feature discovered feature overshadowing suppressed idea feature successfully learned suppressed time optimal suppress feature next vector isolate orthogonal suppressed feature subtract orthogonal vector feature vector feature suppressed parallel distributed processing corrected feature unit suppressed feature unit activated suppression simulated propagating inhibitory signal size feature activation backwards unit feature node activated activation connection strength node inhibits node property backward inhibition used next constructing feature eigenvectors feature adaptive rule discovers dominant eigenvector correlation matrix backward inhibition furhter feature converge eigenvectors eigenvectors correlation matrix successively applying backward inhibition eigenvectors correlation matrix property make intuitively good meaningful feature extract highly correlated line line mutually eigenvectors unit vector dominant eigenvector corresponds line largest variance feature line correlated dominant eigenvector corresponds vector variance correlation matrix diagonal correlation matrix partitioned diagonal submatrices mutually highly correlated line eigenvectors eigenvectors correlation matrix symmetric matrix orthogonal thus linear transformation recodes dimensionality word variability compact sense main eigenvectors seems feature system latency time stated eigenvectors correlation matrix successively applying backward inhibition simplest learns dominant eigenvector vector feature node converge dominant eigenvector inhibits dominant feature orthogonal eigenvector largest eigenvalue dominant eigenvector modified turn discovered feature node eigenvectors discovered fact process orthogonal kohonen page ordering feature corresponds descending ordering eigenvectors absolute eigenvalue feature learn feature adaptive rule feature obvious drawback cognitive inadequacy sequentiality degree intrinsic rule learns dominant feature word feature enabled come shadow feature filtering feature sequentially control made parallel feature node activation rule simultaneously feature step activate feature node adaptive activation rule step feature node reached temporarily stable feature node backward inhibition node latency time learning rule remains identical adaptive learning latent feature node suspended latency time criterion stability test zero inherently varying thus never stable used used exponentially past edwi edwabs ratio absolute edwabs edwabs ratio converges zero periodically absolute relatively adaptive rule feature superior competitive eigenvectors correlation matrix orthogonal correlation matrix symmetric thus grain feature latency time parameter latency time feature rapidly learned time weak feature discovered latency time long week feature discovered feature distributed eigenvectors ordered eigenvalue implicit hierarchy illustrated binary line symmetry substituted zero theoretically feature satisfied feature feature discovered backward inhibition contained activation feature multiplied feature column orientation abridged code middle column feature activation feature activation classification hierarchy formed descending eigenvalue dominant feature paterns diagonally oriented diagonally oriented partitioned feature bottom left bottom left feature balancedness diagonal hierarchy explicit feature vector implicit encoding associative network association depend feature like diagonal vers encoded feature cell weaker like diagonal hrycej orientation encoded feature feature cell encoding corresponds path hierarchy tree feature system used classify novel illustrated four novel experimentally classified system note classified bottom left balanced undecided diagonal diagonal balanced undecided note latency time eigenvectors residue suppressing eigenvectors substantially differ feature weak eigenvectors substantially differ eigenvectors case tested showed high orthogonality good property feature discovered preserved parallel distributed processing competitive backward inhibition also used grain control competitive feature unit competition incrementally learned feature vector partially suppressed time parameter control extent suppression thus grain feature competition feature correspond eigenvectors difficult treat mathematically supposed verified experimentally even weak feature chance attract vector grain feature refined illustrate grain control five vector correlation matrix case showed poor convergence seen competitive partitioned matter five feature unit five five five vector backward inhibition strength conclusion backward inhibition connectionist network used alternative enhancement competitive feature noncompetitive backward inhibition discovers feature form correlation matrix eigenvectors thus satisfies independence controllable grain distributed eigenvectors ordered absolute eigenvalue form feature hierarchy competitive backward inhibition operates analogically competitive mean controlling grain feature inhibition parameter backward inhibition numeric cluster variance parallel neural network construction distributed used processing
