conceptual framework creating converge optimal equilibrium cooperative multiagent framework sufficient optimal system demonstrate efficacy framework analyze learning employing tool construct novel multiagent learning multiagent reinforcement learning system many benefit distributed artificial intelligence parallel increased autonomy simplicity agent stone veloso watkins natural studying system simplicity convergence also able learning multiagent coaxing concurrentlylearning trivial task agent constantly modifying learning process agent faced unpredictable invalidate convergence even agent converge individually optimal optimal system poor universal rule many emergent coordination learner maes brook schaerf failed coordination motivate host cooperative multiagent claus boutilier lauer riedmiller littman wang sandholm step multiagent multiagent reinforcement learning intractable agent system rely perception agent optimal equilibrium system reinforcement learning qlearning need cited able work effectively facilitate success isolating system behave poorly suboptimal convergence shadowing equilibrium absence sufficient optimal cooperative system effectively address well system mean addressing consistent system background terminology simplicity watkins frequent reinforcement learning permit clear concise multiagent coordination multiagent coordination consequence thus applicable reinforcement learning paradigm well agent mapping agent maintains list discounted agent learn optimal maximizes discounted time step agent chooses receives maxq learning rate discount time agent best optimal learned argmaxa learned differ optimal learned also differs exploration learning learning note employ identical learned exploration learning tsitsiklis converge optimal probability transitioning agent converged optimal optimal learned identical argmaxa multiagent system agent system system expressed vector agent agent resulting joint combined system mapping time step joint agent receives note agent receive joint agent thus joint aliased perception agent sometimes system term joint agent prefer maintained joint preferred joint mapping represents joint discounted agent agent joint joint agent joint followed thereafter relationship joint agent probability joint agent selects note probability probability probability agent word joint exploration agent optimal joint probability transitioning agent preferred joint term optimal joint argmaxa system agent cooperative joint maximizes agent also maximizes agent cooperation agent signal joint agent joint thus agent preference differ agent need must simultaneously satisfied system optimal nash equilibrium cooperative optimality restricted subset nash equilibrium coordination equilibrium coordination equilibrium strict strict strict exactly joint agent receiving case joint sort coordinated equilibrium joint coordination equilibrium poor system identify poor system guise literature shadowing familiar reader next show absence sufficient system optimally poor agent learned individually optimal argmaxai best response player guaranteed converge optimal optimal case convergence break multiagent system changing agent creates despite loss agent converge optimal multiagent agent necessarily need converge optimal execute optimal agent playing optimally must settle nash equilibrium nash equilibrium tend agent literature particularly shadowing shadowing even potentially superior agent maintain receive joint system consequence agent optimal preclude possibility coordination equilibrium joint shadowed case maximal shadowing shadowed joint maximal affected agent agent experience maximal shadowing shadowed shadowing qvalue agent multiagent system time step agent receives joint consequently agent unable distinguish distinct aliased shadowing consequence assignment precisely thus addressable shadowing sometimes prevented learning agent seeking maximize tend gravitate toward coordination equilibrium onpolicy learning assure shadowing maximal shadowing despite learning failed coordination punished penalty game claus boutilier maximal shadowing coordination agent conflict equilibrium equilibrium coordination agent selecting system optimal stricter term refers task selecting optimal equilibrium possibly suboptimal equilibrium equilibrium existence equilibrium necessarily suboptimal system equilibrium creates agent happens depend task well exploration used system partially exploitive exploration proven particularly encouraging convergence mutually compatible claus boutilier sekaran achieving optimal cooperating behave optimally agent learns individually optimal maximal shadowing equilibrium absent cooperative system system optimal hold joint learned joint system know joint enables joint maximize agent joint system cooperative joint maximizes discounted agent must maximize agent well joint know joint maximizes discounted agent joint agent enables joint maximizes discounted must case maximizes discounted agent strict coordination equilibrium must system optimal naturally sufficient optimal system make framework preferable many possibility addressed modifying learning placing cooperative learning improving system framework imposed preventing coordination grouped affect system shadowing equilibrium suboptimal convergence prevalent literature broad examined system control task modify learning taskoriented constrained shadowing equilibrium agent cope effectively superior creation learner learn regardless acquainted constrained need address implicitly resolved shadowing dominant dominant maximizes agent payoff regardless agent task structured creates dominant agent agent experience shadowing shadowing joint learning agent able perceive counterpart able distinguish high payoff joint indiscriminately attributing payoff joint learning claus boutilier joint learning littman nash wellman cooperative learner optimistic optimistic exploration deterministic distributed reinforcement learning lauer riedmiller joint learning giving agent extra agent optimistically agent maximize thus utility stochastic domain heuristic kapetanakis kudenko heuristic convergence optimal equilibrium stochastic climbing game stochastic learning rate addressing minimize learning agent agent learning wolf variant bowling learning rate updating holding agent learn temporarily stationary equilibrium optimal simplest prevent equilibrium system optimal case agent need selecting optimal equilibrium premise behind convergence wellman littman also possibility avoiding equilibrium wolf bowling note wolf variant focused adversarial game cooperative equilibrium emergent coordination emergent coordination describes tendency reinforcement learner learn compatible agent constantly seeking best response agent demonstrated hexapedal robot locomotion maes brook network load balancing schaerf cooperative boxpushing task social social applies agent driving side street premise behind social learner mataric homo egualis agent nowe also used coordination system lauer riedmiller strategic learner strategic learner agent counterpart optimal game agent fictitious play claus boutilier variant adaptive play wang sandholm young concurrent reinforcement learner mundhe applying framework incremental learning learning addressing incremental learning address optimal convergence shadowing equilibrium consistently learns play coordination equilibrium deterministic achieving optimal incremental learning achieves optimal preventing shadowing claus boutilier incremental learning prevents shadowing learning joint agent perceive counterpart signal learn joint enables agent clearly lead coordination equilibrium addressing equilibrium incremental learning incremental adjustment optimal equilibrium agent maintains probability distribution probability initialized modified incremental learning initialization arbitrarily time step agent selects probability distribution agent receives signal joint probability rmax stored joint rmax rmax target equilibrium preferred joint intuitively rmax probability distribution skewed toward resulted receipt sketch meet criterion thus optimal system proceeds agent agent allowed joint meeting fulda ventura agent joint converge coordination agent consistently learning play coordination equilibrium strictly next agent learning coordination simultaneously agent repeatedly play stateless game agent five cell payoff matrix initialized payoff agent five coordination equilibrium payoff agent tested deterministic stochastic signal summed gaussian noise show averaged trial incremental learning learned simultaneously agent agent sometimes settle coordination equilibrium distinguished desirable joint qvalues converge agent settle turn enables agent learn coordination equilibrium interestingly even degrades gracefully used incremental learning demonstrated powerful successfully achieves optimal avoids maximal shadowing address equilibrium learn optimal cooperative incremental learning satisfies deterministic sufficiently fact well even violated incremental learning particularly suited interacting agent agent addressing shadowing joint learning alternative subset joint fulda ventura conclusion identified sufficient optimal system cooperative concurrently learning agent thus enabling creation learning suited task learning address major framework satisfied many sufficient optimal task iterated strict dominance game generic coordination equilibrium stated assist creating uniquely adapted addressed work concentrate generalizing competitive eventually conflicting battle prisoner dilemma replaces qfunction agent preference differ seeking compromise preference converting adversarial system cooperative wellman nash agent seek play nash equilibrium seeking maximize wellman also framework make independence assumes agent time affect agent time generalize work case independence hold
