data scarce alphabet smoothing probability inescapable estimating form smoothing exploiting alphabet idea view probability smooth graph eigenvectors graph probability coefficient regularized logistic regression demonstrate superiority graph relevant whilst remains competitive smoothing even lack statistical capture regularity natural statistical construct distribution linguistic unit morpheme word phrase sentence categorical vocabulary natural statistical must parameter thus depend critically training data training data rarely lack sufficiently rich dataset like straightforward likelihood tend poor smoothing used refer combine probability accurate enormous smoothing discounting recursively backing linearly interpolating excellent survey chen goodman clustering make smoothing idea also make word brown greedy clustering find data likelihood handle trigram ueberla heuristic randomization used clustering process slight loss early work rely span relationship bigram trigram tend cluster bellegarda span relationship word cluster semantically oriented idea thought semantically homogenous sentence knowledgesource exploited semantically meaningful form matrix training data resulting matrix cluster clustering case constructing alphabet word dagan smoothing kernel built derived distribution reduction perplexity unseen bigram scheme toutanova investigated markov priori stationary distribution used prepositional phrase contribution form smoothing exploiting alphabet idea view probability smooth graph eigenvectors graph probability coefficient regularized logistic regression demonstrate superiority graph relevant whilst remains competitive even lack prediction word prediction task vocabulary probability word history constrain size history word well sake simplicity deal bigram readily used bigram probability logprobabilities bigram form clearly simplest socalled euclidean kronecker delta fitting dataset likelihood training main idea construct facilitates unseen case construct form overcomplete resort form regularization prevent overfitting incorporating word vocabulary probability distribution used hidden word undirected graph edge assigns wordpairs mathematical convenience matrix edge idea construct accomplish spectral graph clustering yield used integrable chung fact spectral graph clustering construct natural constructing geodesically smooth target word smoothness edge graph case mean word probability left work diffusion wavelet coifman maggioni matrix diagonalvalency matrix diagonal operator spectrally normalized graph laplacian operator fact elementary algebra yield spectrum graph laplacian close relationship property graph dimension bottleneck cluster mixing time walk spectral employed motivated smoothness graph term control size whilst control gradient norm expressed desire find loglikelihood linear spanned eigenvectors laplacian smoothest sense influenced ding decomposed practical truncated frobenius norm truncated column vector vector largest kept proper normalization vector multiplied root respective dhillon ding column denoting column unreliable euclidean even automatically constructed useless chance handle unknown word resort nystrom baker extend vector word suffices know word fact good used graph natural graph simply merging resulting like regression well fact anova also used practice rarely used sufficiently good combined parameter rapidly regularized fitting high regression depend kronecker delta raised take form normalizing matrix parameter learned parameter word vector formed evaluated call feature vector training data data markov transition probability modeled multinomial data take form const likelihood parameter vector vector maximizes prevent overtraining advisable prefer smooth word feature vector high dimensional enforce smoothness distribution parameter work studied behaviour used laplacianprior gaussianprior training maximization penalty term wish corresponds laplacian whilst corresponds gaussian used smlr java krishnapuram laplacian gaussian find sketch performanceof smoothing syntheticand data plain bigramestimates interpolated smoothing smoothing word training data probability distribution build graph word calculate normalizedmatrix calculate mapping operator vectorsof train logistic regression parametersusing training data best smoothing goodman calculated empirical trained data probability transition true distribution synthetic datasets data distribution training data know mild close true distribution data entropy divergence quantity closer true test calculated cross entropy bigram smoothing case synthetic datasets also entropy true transition probability graph labeled test synthetic data generating synthetic data good limitation tested data designated bigram make test decided clustered markov form probability observing cluster word idea next past past yield identical distribution note thought hidden markov transition whilst probability unconstrained specified matrix matrix computing matrix permutation matrix size perturb noise transition probability nonzero transition computing idealized blockstructured matrix perturb ofare distributed block clearly identifiable knowing infer probability hidden noted collapse hidden markov blurred used whilst note roughly corresponds noise well reflects block data perfect assigns word cluster whilst assigns cluster matrix disturbed noise matrix distributed taking noise parameter perturbed matrix wise matrix matrix word reduces case lost training test datasets training dataset normally changing parameter drawing normaldistribution mean variance test dataset training dataset procedure repeated time sensitivity test parameter influence studied noise mask identifiability block noise matrix gradually decrease training sentence cluster parameter whilst kept default default cluster cluster cluster bigger thus kept laplacian gaussian labeled gauss lapl graph sensitivity noise masking used infer masking noise increased harder built masking noise parameter show significantly spectrum peer fact masking noise bigger converges bigram hand able maintain whole gaussian slightly laplace believe laplacian prefers sparse high sparse preferable robustness degradation next investigated sensitivity gradually increased similarityinformation noise parameter perfect whilst lost built noise parameter governing weaker degrades converges bigram notable even verypoor well euclidean expect spectrum matrix covering spectrum vector thus automatically retains sparsity training data main promise capable achieving even training data validate sentence training data varied gaussian laplacian outperformed wide note gaussian much sentence clusterstructure tested cluster ranging cluster vocabulary size summarized clear cluster word cluster structural notable case able well hand hidden greatly outperforms built sentence training data cluster stddev comparedwith cluster test real data task predict wall street corpus extracted directed graph sens syntactic wordnet fellbaum word graph built brown collected word next distribution word word brown corpus graph wordnet graph link wordnet graph side link arrive word live link live bouncy wordnet graph live bouncy link increased next step calculated outlined beforehand graph built connecting prefix grouped training test corpus kept training corpus sentence whilst test corpus consisted sentence vocabulary size graph extracted wordnet used failed good investigating graph turned sparse actually used graph performancebecamepractically indistinguishableto guess principled graph task suited alphabet unfortunately smlr tool used training scale poorly alphabet size seriously limiting feasible concentrating robustness perplexity reduction bigram time training corpus perplexity reduction mild whilst bigram also sentence even little conclusion novel smoothing train construct automated spectral graph able outperform competitor substantially relevant whilst remained competitive even informationwas helpful motivation fact continuous domain smoothness target density yield convergence discrete domain appropriatenotion smoothness spectral graph suggests laplacian graph eigendecomposition normalized laplacian operator yield smooth topology geodesic graph best work suggests automatically constructed feature probabilistic modeling discrete domain simplest case constructing bigram idea ranging even must constructed possibility accomplish appropriately constructed kernel investigation left work
