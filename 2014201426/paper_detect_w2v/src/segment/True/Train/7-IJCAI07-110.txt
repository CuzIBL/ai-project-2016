work heuristically accelerated hammq heuristic wellknown multiagent reinforcement learning heuristic influence characterises hammq preference must empirical conducted simplified simulator robot soccer domain show even heuristic enhances significantly multiagent reinforcement learning reinforcement learning attractive multiagent system cited attractiveness many convergence equilibrium szepesvari littman sound foundation learning adequate suboptimal control besides also easy wide control planning neither analytical sampling priori learning carried line agent receiving penalty multiagent system agent behaviour agent multiagent reinforcement learning need address agent must unfortunately convergence achieved extensive exploration time consuming existence agent size worsening multiagent despite successfully minimaxq littman littman nash wellman alternative convergence rate heuristic selecting guide exploration heuristic reinforcement learning bianchi successfully used learning simulated robotic investigates heuristic concurrent learning multiagent domain proposesa algorithmthat incorporatesheuristics heuristically accelerated hammq empirical hammq carried simplified simulator robot soccer domain littman simulation show even heuristic learning organised briefly review describes show learning rate heuristic learning process modified minimaxq describes robotic soccer domain used show conclusion multiagent reinforcement learning markov game also stochastic game markov process mdps game modelling system agent compete accomplish task formally littman finite collection agent transition agent agent receives computing maximizes agent time considers player agent opponent opposite goal zerosum markov game zsmg learning agent maximize opponent minimize player zsmg littman quintuple finite finite agent finite opponent transition probability distribution defines probability transition time learning agent opponent specifies agent opponent optimal zsmg trivial agent crucially opponent minimax russell norvig evaluates agent regarding opponent choosing maximizes payoff zsmg littman minimax choosing qlearning work opponent take linear strang agent probability distribution probability taking opponent player take consecutive turn alternating markov game initialise repeat visit therule execute observe opponent receive reinforcement observe next stop criterion reached case agent know opponent deterministic simplified maxminq case optimal argmaxa mino rule used arandom probability parameter defines probability arandom elsewhere littman banerjee domain robotic soccer littman bowling veloso economy tesauro agent iteratively learning early basically exploration also made pair time exploration take alleviate next multiagent reinforcement learning banerjee modification admits next predefined probability separating greedy predefined probability distribution banerjee work combining eligibility trace eligibility trace initially sutton used learning process tracking visited visited episode updating pair iteration pair eligibility zero carried pair banerjee also combine strength learning process experience temporal spatial ribeiro accomplishes spatial combining spatial spreading receiving reinforcement pair experience also done coding domain spreading experience loop cost consequence taking spread pair real experience time actually combining heuristic multiagent reinforcement learning hammq heuristically accelerated minimax hammq zsmg explicit heuristic functionto influence learning process defines heuristic desirability agent opponent heuristic preference must said heuristic defines heuristic tentative used accelerate learning process heuristic derived domain clue suggested learning process used agent rule defines agent rule used hammq modification rule heuristic whereis heuristic subscript real used influence heuristic rule used hammq influence minimize real suggested heuristic heuristic used exploration carried operation modified many conclusion remain hammq hammq agent learning deterministic zsmg finite discount used heuristic hmin hmax agent converge probability stateaction pair visited infinitely obeys infinite visitation hammq depend heuristic littman szepesvary list convergence hammq jeopardy necessity visiting pair infinitely considers exploration greedy regardless fact influenced heuristic visitation guaranteed converges pair must visited infinite time practice also boltzmann exploration kaelbling intercalating step make alternate heuristic exploration step heuristic time shorter learning time heuristic made hammq explores free training consequence suitable heuristic learning process delay learning convergence prevent converging optimal initialise repeat visit modified execute observe opponent receive reinforcement observe next stop criterion reached hammq hammq worth noticing hammq rule existence step updating robotic soccer hammq playing robotic soccer game task team robot dynamic domain great relevance artificial intelligence posse real robotic automation system seen robot assembly task mission robot tambe mention carried robotic soccer domain introduced littman modelled zsmg agent domain player compete grid cell occupied player take turn allowed agent move north south east west keep agent ball player circle around agent player finish cell occupied opponent loos ball stay cell agent lead agent stand player ball opponent goal move team beginning game agent positioned depicted possession ball player hold ball move move alternated agent used hammq littman picture show agent heuristic used arrow heuristic used rule holding ball opponent goal show heuristic player note heuristic take opponent leaving task deviate opponentto learning process heuristic heuristic parameter used hammq learning rate initiated decay exploitation rate discount parameter identical used littman reinforcement used reaching goal goal scored opponent initiated programmed linux thirty training session session consisting match game game finish goal scored agent move completed show learning curve training session agent learns play opponent moving goal balance scored learning agent goal balance hammq opponent littman robotic soccer goal balance hammq agent littman robotic soccer match worse hammq learning phase match proceed learning curve training session learning playing learning opponent case clearly seen hammq beginning learning process match agent convergeto equilibrium student spiegel used hypothesis heuristic learning process show test learning minimaxq hammq opponent data student test hammq training opponent student test minimaxq hammq training agent hammq match comparable confidence learning agent playing opponent show cumulative goal balance game match training session match agent match ended draw stand goal scored hammq beginning learning process match opponent learning opponent training cumulative goal balance hammq hammq cumulative goal balance match training session training match hammq hammq match match training session conclusion work heuristically accelerated hammq heuristic multiagent reinforcement learning domain robotic soccer game showed hammq learned trained player hammq trained outperformed heuristic convergence time many real time also incorporated well multiagent minimaxsarsa work obtaining domain robocup simulation size league robot kitano preliminary heuristic make devising convenient heuristic domain analyzing obtaining automatically also worthwhile
