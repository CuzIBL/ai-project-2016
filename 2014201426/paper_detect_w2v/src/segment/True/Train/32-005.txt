statistical collaborative filtering investigates latent predicting preference preference probabilistic latent preference convex preference clustering simultaneously person cluster variant derive variational clustering benefit experimentally investigated movie data rapid growth data repository overwhelming supply communication network bear risk overload filtering refers separating nuisance data possibly preference opinion judgment taste quest automated filtering system take diversity preference relativity distinguishes major resnik contentbased filtering organizes property preference carrier text collaborative filtering goldberg social filtering exploiting preference person speculating preference machine learning filtering filtering system domain fraction area deal system architecture interface take viewpoint clarify statistical foundation collaborative filtering presupposition made preference neither property book message movie person user cineasts hypothesis unrealistic seem sight many system interact collect much data avoid timeconsuming questionnaire true property sometimes difficult property make relevant person integrate step deriving probability feature updating prediction preference dyadic data thus formal pair dyadic data mann simplest case representing like person person participates case also preference simplest case corresponds preference modeling like person learning addressed probabilistic modeling covery argue statistical suitable task prediction recommendation clustering introduced pursues goal identifying meaningful cluster person belong family mixture latent discrete latent main motivation behind latent filtering preference preference assumed underly data process probabilistic modeling mainly overcome omnipresent data sparseness parameter data sensitive overfitting also interested structural captured latent cluster specification hofmann latent servation made conditioned probability thus simply nominal distribution probability perfectly symmetric entity also asymmetric identity dual reversing role intuitively appealing probability modeled convex case collaborative filtering preference person modeled preference distribution neither assumed person form stipulated partitioned high degree flexibility modeling preference person multitude fact expressed perfectly well also case case high probability fitting procedure likelihood latent expectation maximization dempster alternate step expectation step probability latent parameter maximization step parameter probability symmetric parameterization bayes rule yield arrives time pair alternating defines convergent procedure implicit derivation multinomial sampling possibility also hypergeometric sampling statistical wisdom yield population preference extending capture binary preference distinguish case person announces preference retrospect part modeled triggered process handle arbitrary preference likelihood preference scale hofmann puzicha graphical preference case case case case integrate conditionally implication employed tive preference variant also considerably flexibility also parameter straightforward modify appropriately show variant minor obtains comparing observes replaced treated fixed conditioning note conditioning reveals metry introduced replacing multinomial vector bernoulli probability collaborative reversing role yield dual counterpart prediction indirectly combining type dependency multinet worth clustering specification clustering assumpwhere time tion made person belongs exactly preference person belongs exactly also derive latent mapping effectively enlarged also combine variant independence resulting combined corresponds bayesian multinet geiger heckerman case case multinomial sampling selecting pair conditioned adequate thus modification replace multinomial bernoulli probability conditioned modification spirit leitmotif convex prototypical expect clustering flexible modeling ences accurate prediction fact verified empirically nevertheless valuable formalize parameter person probability assigning cluster importantly cluster association parameter pair cluster probabilistic factorial latent machine learning star trek star star trek star trek fifth strangeiove clockwork orange delicatessen cinema paradiso brazil pinocchio aristocats snow white seven dwarf jungle book lion king richard miserables madness king george name father visitor visiteurs rock eraser independence mission impossible trainspotting piano remains name father forrest gump shadowlands wear love circle friend dolores claiborne love woman como agua para chocolate color color blue color white piano movie extracted eachmovie probability completes specification association parameter decrease probability observing pair cluster pair relative unconditional independence proper probabilistic normalization constrains admissible association parameter variational fitting main difficulty clustering coupling latent mapping cluster association parameter admissible also seems reach procedure admissible expectation probability distribution free parameter maximize hofmann puzicha technically introduces grange multiplier enforce marginals marginal intuitive considering hard clustering case reduces mutual pair numerator simply time person belonging cluster cluster denominator reduces probability independently observe person cluster remains variational choosing minimize true distribution also hofmann puzicha brevity final form variational form highly coupled system transcendental iteration alternate computation latent precisely probability vice versa alternating computation hofmann puzicha four wedding funeral home sleepless seattle dave pretty woman piano apollo batman batman forever star trek stargate goldeneye extraterrestrial wonderland cinderella yeller mary poppins hound full metal jacket bridge river kwai apocalypse chinatown shining kalifornia smoke rock west romeo bleeding crumb movie cluster extracted eachmovie interleaved parameter term cancelations exploited derivation resulting alternation scheme optimizes maintains probability distribution clustering clustering preference like clustering multinomial sampling independently occurrence pair preference conditioned pair modify replacing association parameter bernoulli parameter likelihood distribution latent mapping yield hard clustering simplifies many person cluster like like cluster ignoring missing denominator corresponds vote belonging belonging factorial refined also preference machine learning cluster pref perplexity eachmovie type column derived ungar foster clustering identical bernoulli gibbs sampling fitting voted computationally much variational demonstrate utility latent collaborative filtering series eachmovie dataset data collected internet million preference vote scale converted preference thresholding eachmovie database used able train gibbs sampling immense computational dataset data recommendation data recommendation star trek picture star trek star trek star trek star trek star trek empire strike back star trek contact raider lost stargate terminator jedi pulp fiction fargo smoke color blue four wedding funeral odyssey silence lamb story dead walking batman leaving vega piano exemplary recommendation rizes perplexity clustering latent significantly clustering achieves reduction roughly marginal independence baseline annealing hofmann puzicha slightly bracket give impression extracted movie movie cluster look like displayed cluster highest probability movie piano also subjected test recommendation system perfectly satisfying view hope reader also spot valuable recommendation conclusion systematically type latent utilized collaborative filtering variant sampling modeling goal emphasizing flexibility richness latent prediction work address alternative loss deal acknowledgment work daad postdoctoral fellowship german foundation grant eachmovie perplexity inverse probability test data preference data courtesy equipment corporation generously paul mcjones
