learning remember data receive training phase prediction time computation take database datapoints build regression predict review autonomous system also note ensuing cost hopelessly slow computation database grows structuring database accessing maintains learn combat cost instancebased learning sacrificed explicit retention data applicable instancebased prediction near neighbor ntrodtice tctt training phase form interpolative data build multiresolution data summarize database experience resolution simultaneously permit database flexibility linear greatly computational cost learning stanhll atkeson ahaet moore highly flexible prediction data also closely remember data prediction time computation differs machine learning training reception data prediction nearest neighbor kernel regression locally linear regression training phase neural network tree sometimes preferable form approximator main flexible inductive bias little data nearest neighbor give sensible conservative prediction wildlv extrapolate data nearest neighbor neural network default property locally data learn piecewise continuous arbitrary preci sion high dimensional distributed data enormous practical much nearest neighbor form linear data remove noise prediction atkeson grosse learning parameter need fixed advanc many learning parameter concern extent smoothing noise traded goodness parameter determining discrete relevant need decide learning parameter whichever parameter desire prediction entirely prediction immense autono mous system prediction tuning learning parameter data arriving instancebased must parameter train parameter retrain also remember data spectrum necessarily predictor handful datapoints particularly highly noisy data data base many noisy system linear smooth case best approximator closest datapoints form prediction deng moore case nearly linear linear regression approximator property make particularly desirable autonomous system spend life able manually tweak learning parameter operation unfortunately serious database grows increasingly make prediction prediction searching database find datapoints mean hopelessly slow merely thousand prediction attempted deal grosse moore skalak none avoids sacrificing benefit instancebased describes multiresolulion hierarchical structuring data retains property learning fast instancebased asymptotically reduces cost linear logarithmic datapoints regression wide concentrate well kernel regression ifranke datapoints consist pair xnyn vector scalar date datapoints prediction vector quer predict ycsl neighbor find datapoints vector closest take yest kernel regression idea database used closest largest thus datapoint memory close zero tlie calculated decreasing euclidean learning bigger parameter flatter curve mean many memory pointi contribute evenly regression tends infinity prediction database closely neighboring datapoints make contribution smoothing parameter kernel regression data relatively noisy expect prediction relatively data noise free avoid smearing away fine illustrated noiseless data give besl regression term predictive noisy daia bollom preferable drawback kernel regression expense merating memory address reviewed multiresolution structuring datapoints main idea multiresolution regression grouping show case calculate grouping data deng moore learning tenon intuition cutoff datapoints node algebra reveals used thus hopefully considerably cutoff rule know fortunately used real make cutoff system review multiresolution kernel regression hrst trigonometric noise vector height gaussian noise deviation datapoints kernel width cutoff threshold show testset test regular kernel regres sion regular multiresolution kernel regression multires graphed close indicating multires wide kernel width close regular show computational cost term summation dominate cost regular cost multires substantially cheaper particularly show trigonometric five show prediction regular cost regular worst case computational multires cost impressive fixed dimensionality kernel width cost rise linearly loganthmicallv datapomts datasel time size show data effectiveness deng moore multires apparent case datapoints cost increased cost regular increased rigun relative computational cost multires againsl cutoff threshold real datasets multires data datasets datasets maron moore thev industrial packaging process slowness prediction reasonable concern encouragingly multires prediction discernible prediction multires regular tabulated cost test nolabl datasets pool datapoints robot high dimensional high dimensional data final concerned well learning relatively robot learning vector embedded manfold datapoints vector distributed vector distributed manifold room flexibility multires built make kernel width also euclidean subset ignored manhattan also classification regression remembered succeeded cost learning outlined literature multires preferable upon extent need learning flexibility work throughout spectrum make prediction parameter needing retraining phase multires hardware alternative fast kernel regression parallelized full used learning nearest neighbor searching searching preparata kernel regression find adtree practical kernel width datapoints searching individually multires visit relatively node locally cheap even case kernel width data multiresolution preferable need leaf node cost learning editing prototype datapoints forgotten particularly used kibler skalak kibler idea datapoints datapoints appli cable even wide kernel width degree averaging must decided multires kernel width rebuilding proiotvpes occasional prediction prototype must lose averaging else datapoints stored proiotvpes parallel prototype multires node tree thought labncaled prototype summarizing data tree previously used cache mapping tree leaf grosse moore omohundro quinlan fast access tree built need built time learning parameter multiresolution resulting prediction tree substantial discontinuity grosse continuity enforced cost exponential dimensionality weakness multires diminishing approximately dimension data distributed inherent seems data high dimension datapoints exactly break kernel regression applying exactly locally regression prediction minimize locally squared node regression matn tree permit fast prediction also fasl compulation confidence variance multires applicable numeric feature avenue work extend binary feature conclusion make database multidi mensional data multiresolution mean quickly even data enormous message accessing base case base memory base resort throwing data away intelligent structuring alternative acknowledgement wish thank ijcai reviewer insightful comment work suppon gift corporation initiation award
