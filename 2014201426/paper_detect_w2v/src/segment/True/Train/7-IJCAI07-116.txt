year growing rich reinforcement learning expressive many term reasoning extending step learning exploitation transition learned augmentingrelational reinforcement learning planning benefit robustness evaluated experimentally reinforcement learning agent observe goal maximize domain dynamic unable handle domain structuring essential reinforcement learning tadepalli framework expressive describing also able handle vector block block block stand floor block block exactly block clear clear fact true clear clear agent take clear block clear block floor move move move block clear vector clear intuitive work learning transition beneficial agent learned used help learning process generating training simulation happens dyna architecture sutton agent planning tesauro exploitation mode make exploration mode lookahead baxter latter note complementary combined focused growing extending learning exploiting otterlo task expressive inevitably harder learn dyna fairly easy learn keeping probability distribution case probability distribution harder statistical learning work investigates feasibility benefit learned lookahead beneficial learn perfect marlie reinforcement learning expressive system learn transition contribution threefold transition facilitates incremental learning learning exploitation learning zettlemoyer exploited avoiding much agent gain poor trying learn good work kersting need also note consideringthe full problemin techniquedoes reset generative fern experimentally benefit examine influence learned background show transition learned describes look step ahead improves work reinforcement learning mdps reinforcement learning sutton barto formulated formalism markov process mdps need domain formalization mdps rmdps fern kersting raedt kersting form rmdp ground atom form ground atom ground atom transition defines probability distribution next probability landing defines task reinforcement learning optimal initially unknown usual discounted cumulative find maximizes discount relative driessens applies domain regression knowing optimal constructed argmaxa learning transition efficiently learned module learns form probability distribution rmdp said ground atom binary ground atom time dynamic bayesian network dean kanazawa transition time atom time currentstate randomvariables representing agent probability distribution modeled agent reasoning process turn specifies layered network dependency describing preceding avoid learning network hard case learning revision interfere learning probability uninvestigated cpds compactly probability tree neville fierens main idea probability tree tree used probability distribution give ground atom probability true next maximal tree atom avoids arising generalizing type semantics show probability tree clear block probability tree used note learned probability tree necessarily preceding network node learning uninstantiated part probability tree learning reduces learning tree incremental tree learning driessens probability tree show probability block clear move node tree block clear frame case clear block moved block clear clear afterstate block moved block learning agent experience note fact next suitable sampling exploitation partially transition learned enables agent predict investigate lookahead tree give agent informed looking step selecting lookahead tree sparse lookahead tree used kearns mdps transition stochastic uncertainty incomplete learning need sampled time accurate sampling width parameter root node directed edge label edge node tail edge represents head node continued tree reach deepest learned constructed regular sample averaged bellman optimization scheme precondition planning domain remains unchanged agent illegal transition indirectly precondition learning extra binary true prune away predicted illegal besides sampling width also parameter influence lookahead tree investigating empirical empirical want incremental tree learner able build want investigate much agent improves lookahead want robustness lookahead beneficial learner able build domain driessens system used transition domain learned sampling width used agent also learns modeling precondition prune lookahead tree exploration step lookahead eliminate influence ordering independence assumed show test episode episode greedy percentage episode used convergencemeasure block block stackgoal goal driessens agent receives block block block stack block floor agent rewarded block floor comparable exploration episode step optimal testing optimal episode allowed stackproblem block seven block goal block varied episode four seven bias used block driessens logistics domain logistics domain containing truck city goal transport domain load load specified specified truck city unload take truck movesit depot city truck located move truck specified city truckin also make bias used tree learning test truck city truck goal bring city exploration step allowed testing episode step domain four city truck goal bring time step made learned transition test transition learned classifier fact predict true resulting next show percentage made step false false atom incorrectly predicted true false block seven block transition learned rapidly optimal episode logistics domain five truck city reasonable transition learned rapidly never reach optimal next lookahead planning lookahead learned show stack goal block logistics domain plotted block goal block stack goal show episode good much looking ahead learning lookahead take computation time also time good learning show time learn goal omitted lack computational cost learning lookahead step lookahead learning computational bottleneck lookahead mainly sampling learning something work step lookahead outperforms lookahead logistics block goal robustness generating training transition step extending time learn good transition episode test transition classification learned step lookahead plotted learning rate showing horizontal line show learning agent episode lookahead show even learned accurate beneficial lookahead inaccurately drop work part work indirect modelbased propositional dyna architecture sutton mentioned agent learns transition hypothetical extra prioritized sweeping moore atkeson hypothetical part mentioned orthogonal explored work learning environmentbecomes task learning transition zettlemoyeret learning probabilistic rule dataset transition applicable noisy stochastic main learning part applicable reinforcement learning work incrementally domain emerging statistical learning seen work upgrade bayesian network sanghai defines dynamic bayesian network rdbns stochastic process vary time combining successful past game playing baxter davy used domain seen learning many lately indirect sanner ground naive bayes learned major work time game playing restrict undiscounted domain terminal failure success conclusion work marlie reinforcement learning system learning exploiting argued system work comparedto learning wider argue learning difficult marlie build probabilistic uncertainty dynamic time exploiting soon work sampling looking ahead deepening sampling lazily beneficial part learned transition good resource need invested stay regular time hand learned part satisfactory process also investigate poor planning reasoning acknowledgment croonenborghs flemish promotion technological industry ramon hendrik blockeel postdoctoral fellow fund flanders
