argued learning domain training real classification integrating suitable discriminative classification scheme suggested discriminate training characterize show viability discriminator characterizers integrated tested novel characterization make statistical distribution feature extracted training used strongly thesis learn tree quinlan quinlan nearest neighbor family backpropagation rumelhart learn discriminative learn discriminate training consequence classification scheme suffer inability detecting training actually training resulting undesired misclassifications ignored machine learning arises real characterized open domain hutchinson simply domain learning machine kind used bank office task sort coin country reject coin supposing learned practical impossible train learning system kind coin genuine faked system trained kind coin supposed accept system medical diagnosis cost misclassification high remain silent give incorrect diagnosis pointed smyth mellstrom learn characterizes regard regardless occurrence training thus potentially able reject training call learn characterizers learn discriminative discriminator holte modified learn form besides memorization training constructing simplest learn disjunct feature training belonging construct accepts multidimensional delimited nominal feature training belonging make distinction kind characterizers term much generalize learn michalski larson case degenerate discriminator mentioned modification learns inadequate tend static sense controlling degree belief control degree essential real jected misclassified must balanced accordance cost misclassification high rejection desirable uncertain case rejected belonging training kept misclassifications desirable reject many training disadvantage characterizes good discriminator discriminate actually training simply take extracted training data extent integrating discrimination characterization weakness learning discriminative correspond closely strength learn vice versa discriminative good discriminating training unable identify training able good discriminate training consequence combine strength time reduce weakness discriminates training characterizes separately thus learning divided phase discrimination characterization obvious know entity disjuncts characterize made discrimination also classification computationally test discriminate instantiation discriminator phase leaf case classification hand consist applying discriminative preliminary classification decide reject accept refer multidimensional subspace case acceptance thus acceptance classified outside acceptance rejected inside acceptance classified preliminary classification made discriminator discriminator characterizers suitable integration indicated discriminator characterization learning focused creating discriminative many good discriminator besides invented characterization phase pointed static sense controlling degree size acceptance sensitive noise next characterization control degree novel characterization central idea make statistical concerning distribution feature hidden training data referred univariate computes explicit feature suitable tree multivariate able capture covariation feature suitable integration discriminator nearest neighbor univariate feature probability feature explicit make retain tree induced discrimination augmenting subtree leaf characterize leaf davidsson belonging belongs control degree consequently mentioned choosing lesser misclassified rejected thus misclassify high jected acceptable high turn statistical normally distributed stochastic mean deviation follow line feature disjunct disjunctive normally distributed indicated seems mean deviation training belonging multivariate make multivariate classified dependent rejected feature normally distributed solid ellipsoid satisfying probability covariance matrix distribution feature johnson wichern thus parameter mean feature represents covariance pair feature analogy univariate case parameter computing mean vector covariance matrix training belonging main limitation applicable feature applicable also argued crude computing elaborate confidence actually idea turned complicates classification significantly learning ordered nominal feature make combining form able handle kind ordered feature feature rest feature empirical idea behind emerged real concerning learning coin sorting machine successful briefly presentation davidsson properly data well repository machine learning database data desired kind data consisting training test simulate data take data leave training test best classify belonging training reject belonging training sight seem strange actually know iris famous iris data anderson sure discovered specie iris fact pointed real reasonable relevant training shortage besides coin sorting classification achieved data iris data discrimination relatively easy surprising task turned difficult data discrimination davidsson studying learning used discriminator nearest neighbor combined univariate multivariate classification training left training percentage integrated univariate integrated multivariate natural coin sorting database used describing canadian coin cent dollar describing hong kong coin also cent dollar diameter thickness permeability kind conductivity canada hong kong database manufacturer creating rule manual extent coin causing training testing show classification training hong kong coin difficult case coin canadian coin rejected misclassifications hong kong coin must reject type coin misclassifications clearly illustrates able control degree applying even pleasing fact misclassify note must used excellent classification iris database iris database type iris plant setosa versicolor virginica data divided half used training testing thus used training classification classification testing show classification trained iris setosa iris virginica difficult case varying control rejected misclassified zero misclassifications rejection rate also show encouraging sdmulti selecting classification misclassifications multivariate creates acceptance closer match distribution feature many fewer training misclassified classification davidsson training containing hong kong coin percentage discriminator discriminator combined combined univariate combined multivariate wnek system learns rule counterexample learn discriminative parameter many best learning learn parameter trim spec mean rule learned involving selector wnek page plain control degree classified also reflecting confident classification thus accept reject degree confidence accept confidence reject confidence plain confidence seems balance best favorable also coin sorting coin sorting discriminator slightly combined learning training containing iris setosa iris virginica discriminator misclassify coin plain suffers reject many training hong kong coin decreasing confidence misclassification much high clearly superior confidence corresponds plain produced relevant disadvantage control confidence hand able degree want note also misclassifications training discriminator combined iris database show classification trained iris setosa iris virginica also discriminate slightly outperforms adequate task misclassifies many training impossible misclassifications confidence produced discriminator combine arbitrary discriminator arbitrary characterization practice take consideration need retain tree constructed tree forced chose sduni characterization sdmulti take consideration domain junctive clearly cluster discriminator learn disjunctive backpropagation nearest neighbor acceptance acceptance unnecessarily implying many misclassified conclusion work argued real learning classification discriminative classification scheme integrated former used discriminate training latter characterize novel characterization suggested property control degree continuously crucial real classification integrating discriminative classification scheme good selecting work developing discriminator characterizers developing characterizers control degree also nonnumeric feature trying shed tension discrimination characterization acknowledgment wish thank michalski bloedorn mlic
