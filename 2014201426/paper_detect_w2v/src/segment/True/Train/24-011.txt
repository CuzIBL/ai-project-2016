architecture rule induction emphasizes compact rule heuristic covering rule sample data refines production rule iteratively replacing rule best rule induction combine covering refinement scheme help reduce rule pruning resampling judicious linear discriminants datasets reviewed tree relatively poorly simpler rule predictive exceeds previously reported learning neural tree many induction tree neural linear discriminants used classification goal prediction central classifier tradeoff goodness increasingly classifier made training sample prediction case inferior simpler classifier expanded tree training sample tree apparent training case accurate prediction case classifier prediction highly learning sample data parameter determination parameter accurately sample characterization learning classifier sample data equally well simpler preferred fewer parameter prediction accurate foundation form wallace freeman best rissanen rissanen practice learning system implicitly recognized many simplification characterized relatively compact tree numerous heuristic tree splitting reduce learning acquisition tree size breiman friedman olshen stone quinlan tree pruning breiman friedman olshen stone quinlan heuristic selecting pruned tree breiman friedman olshen stone parametric statistical linear discriminants heuristic reduce feature discriminant james rule induction simplification achieved pruning rule implicit tree quinlan hidden neural hidden unit used apparent true rate follow classical statistical kapouleas thus developing learning sample compact generating rule induction work galen tadepalli disjunctive normal form sometimes superior empirically applicable rule inducing dimension unified competing rule classification sample case case composed feature classification find best rule rsbest rate case errtrue rsbest examine posed disjunctive normal form classified disjunctive production term term test formed evaluating truth feature thresholding feature assumes sample tree implicit production mutually exclusive mutual exclusivity rule production mutually exclusive rule potentially satisfied simultaneously conflict resolved assigning rule priority ordering last default rule induced heart disease data rule tree induction remains widely learning system learning success exclusive rule induction system quinlan quinlan clark niblett pagallo haussler variant family rule induction system michalski mozetic hong lavrac aforementioned rule induction emerge rule induction characterized term covering scheme rule refinement covering induced tree best rule case covered rule removed training sample next rule induced case remain rule refined pruning applying statistical test covering procedure tree covering covering mentioned look ahead specialize tree rule heuristic mathematical used entropy gini breiman friedman olshen stone tree covering heuristic tend work well many combinatorics optimal make alternative procedure impractical like tree induction expand rule time test rule predictive make training rule relatively procedure never look back look ahead test procedure constantly look back made test step form best rule make best swap rule deleting swap best rule formally describes procedure galen tadepalli best evaluated predictive percentage rule predictive case coverage secondary criterion swapping terminate predictive reached process generating best rule seen rule step maximize predictive rule swapped favor best test step best rule step swapped refining previously rule final step swapped step training case empty repeat rule predictive make best swap deleting case swap best endwhile rule predictive case empty find rule deleted affecting case endwhile halt procedure step predictive rule swapping rule swapped thus seen test swapped necessarily stay back improves predictive rule completed rule best rule proceeds usual removal covered case reapplication construction procedure remaining case pure covering procedure covering distributed indurkhya indurkhya exceeds rule induction match fringe pagallo tree induction data procedure lead fragmentation data covering many rule rule predictive case predictive rule rule induced swapping predictive indurkhya procedure coverage rule optimal even rule task optimization traveling salesman kernighan swapping find excellent rule find best rule devising give reasonable answer predictive highly quinlan rivest central remains find rule minimizes true rate determining true rate varying rule sample rule rsbest collection varying rule rsbest make fewest case practice optimal incomplete sample limitation time insufficient case train accurately rate rule rule rule thousand test case sufficient give highly accurate rate classifier highleyman fewer case resampling give accurate rate stone procedure breiman friedman olshen stone accurate case hundred many classifier trained approximately case full sample learning train sample case ordered best thus practice must induce errcrsj tree parameter rate estimator little bias used resampling breiman friedman olshen stone pruning also costcomplexity pruning breiman friedman olshen stone used prune rule form rule covering rule pruning weakest link rule weakest link avenge training letting case mutually exclusive test breiman friedman olshen stone close learning acquisition make training case size weakest link fewest deleted introduced quinlan rule pruned deleting rule weakest link repeated pruning rule sequentially form rule next process repeated final rule selecting largest pruning ordered series decreasing rule term alternatively size test case pruning apparent rate true rate errtest rate test case sample thousand test case resampling preferable accurate bneiman friedman oishen stone pruning training fold auxiliary rule induced training fold approximately size weakest link pruning test test case fold rate fold size errcv true rate consistent rule case weakest rule removed pruning rule stable accurate tree pruning coverage pruned highly pruning subtree retains full coverage data pruning rule leave coverage size size tree induction node fixed refined swapping minimize apparent training process refining rule modifying rule iteratively checked best deletion rule deletion swap best thus rule refined refined rule size rule make fewer process rate varying rule illustrated resampling rheumatic disease summarized rule list rule rule apparent rate training case test rate test test pruning plotting rate increasingly illustrates classical apparent rate training case true test case apparent rate decrease true rate flattens eventually mixed unit readily comparable adopted rule unit tree decomposed rule path terminal node rule rule unit tree node seems unit term linear discriminants neural minimize rule difficult term unit true mixed dimensional geometric term rulebased find parallel efficiently even linear feature incrementally embed alternative piecewise linear discriminants breiman friedman olshen stone perceptrons utgoff tree nonparametric incremental used parametric linear posed linear derived unknown classified applying choosing linear magnitude greatest heuristic stepwise feature used find linear feature subset feature james parametric stepwise linear discriminant highly tendency comparable training test case discriminant derived rule induction training case used artificial feature binary feature specified feature simply training linear discriminant rule induction system feature feature applying linear used encoding simplified binary feature preserve classification discriminant interplay linear discriminant induced rule rbps true linear discriminant rbps test heart disease mixed rule efficacy learning four publication comparing learning neural tree note tree relatively poorly summarizes data used true rate summarizes reported unit neural linear discriminants rule rule node review nettalk originally reported sejnowski rosenberg rigorous comparative tree propagation neural reported diettrich hild bakri goal spoken word formulated letter pair classification mold mutually exclusive used relatively nonmutually exclusive encoding done domain suitability neural classification phoneme stress pair case training despite training case relatively case many rule induced training test case test ordered decreasing predictive rule previously reported best neural rate rate rule previously reported rate relatively weak rate induced tree heart disease classification heart disease originally reported detrano comparative tree neural reported shavlik mooney towell comparative test training testing used best neural rate crossvalidation yield rate reported tree shavlik mooney towell pruning rule binary encoding linear discriminant feature rule rsbest simply linear discriminant rate towell shavlik noordeweir learning task term grammar reformulated neural network refined yielding pure empirical learning rate best reported pure learning neural rate tree relatively poorly rate rate competitive linear discriminant feature induced rule pruned back linear discriminant discriminant rate linear discriminant success stepwise feature feature case many feature used discriminant desirable rheumatic disease diagnosis preliminary system diagnosing rheumatic disease evaluated base data used heuristic refinement system ginsberg politakis modify rule base system restricted minor refinement assure preservation base close form radical revision base massive rule ginsberg data used revision rtls reported rate five zero rule radically base worthwhile considering well pure empirical learning system best reported tree purely empirical learning perspective rtls rule good true rate rtls also cited rate scored answer marked agreed refinement rate true rate rtls empirical learning system handle dataset half feature missing cart induce tree surrogate handling missing yield high rate even missing covering case compact many missing tree traced exclusivity previously reported four classification superior term rate equally previously reported particularly interested tree relatively poorly competitive learning rule induction readily able exceed tree pure rulebased competitive alternative combining stepwise parametric linear discriminant feature able exceed expense mixed interestingly indicated solely linear discriminant rule exhibit much interplay discriminant feature showed classifier incorporated arbitrates many pure priori preferred linear classifier yield good kapouleas central theme learning reduction rule architecture minimizing rule rule maximizing rule coverage case iteratively swapping rule goal minimizing rule entirely consistent goal maximizing predictive accomplish task also borrowed many weakest link pruning resampling proven learning system directed minimizing restricted neural compact minimize also predictive cited simplification yield previously used nettalk neural benefit limiting also effectiveness learning computational time computationally overwhelming highly swap exceeds best swap indurkhya dropped nettalk minute time nettalk steadily compuiationai look forward computationally intensive extract sample data acknowledgment part grant thank kevin kern colleague supplying data
