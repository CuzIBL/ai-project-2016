take look property discrete time associative memory show adapted natural processing many task benefit associative particularly traditionally regarded driven word sense disambiguation time convergence hopfield network trained representing sentence corpus simulation time convergence unbiased uncorrected linear associative memory well studied statistical physic bruce amit tanaka yamada biophysics hopfield recall robustly even recalled network highly corrupted form robustness face noise recall stored association make memory potentially valuable natural processing task traditionally regarded driven word sense disambiguation take look property discrete time discrete hopfield network explore time convergence previously investigated storage recall clearly derived natural sentence reflect linguistic distribution evidence correlated biased suggests network visiting fellow toshiba corporation komukai kawasaki japan nigel neural network storage want investigate property network also effected worthwhile undertaking adapt associative memory decouple time linguistic stored data gone forming device processing looking onelayered discrete hopfield network hopfield network autoassociative coupled tanaka yamada simulate adapted boltzmann machine generalisation trying system biased hopfield network unit take activation unit threshold represents threshold unit come internally unit network externally network processing processing take asynchronously updating unit converges indicating stable reached koiran koiran investigation evidence convergence time work associative network surprising find emphasis storage convergence time consequently seem many unanswered merit investigation time discrete hopfield network tanaka yamada successful recall unit network remaining unknown time failed recall recall time correlated biased secondary relates scale tanaka yamada simulation carried nice confirm network finite size well come type mean time recall successful network converges stable attractor vector training recall failed recall recorded used mean time convergence recall time flip network settle stable hardware case hopfield network convergence stable guaranteed case network expect network settle confine investigation layered network interested measuring termed network stability great knowing failed recall take much successful recall processing time reduction simply abandon recall failure processing time exceeds expectation network size early bruce flow time convergence depend storage ratio secondary system size updating schedule training train network localist hebb rule matrix encodes stored artificial construction give rich natural training corpus text simulation corpus english japanese text collier newspaper editorial english word sense tagged label denoting sense undertaken japanese lexical translation english word japanese sentence corpus resulting corpus sense tagged sense tagged corpus sentence extracted subcorpora used training construction take lexicon corpus constituent word giving word sense sentence vector occurrence word sentence word occupies lexicon thus matrix node network corresponds word corpus lexicon localist looked generating high dimensional feature word constitute sentence feature encoding optimal term storage wealth statistical physic term representational adequacy encoding capture constituency word capture cooccurrence sentence word pair view modelling contextual inadequate bought closer ideal matrix followed capturing parameter matrix subcorpora show finite size substantially used simulation tanaka yamada also storage ratio predicted unbiased seen collier necessarily finite thus giving continuous degradation storage catastrophic discontinuous predicted infinite also know biased system training storage ratio upwards bias corpus calculated probability neural network simulation follow work amit relate mean perfect recall training mean convergence rate network make corrupt test noise probability test flipped noise noise contained expect convergence time need reset calculate fraction recall looked stable network nominated stable autoassociation test fractional hamming calculate fraction free recall ensamble test training trial convergence time mean flip network reach stable flip network unit changing quantity also calculated trial noise increased mean convergence time numerically test training mean fraction recalled noise trial repeated noise simulation training corpus give mean free fraction show storage ratio good recall show continuous degrading recall collier reported lead storage ratio somewhere around perforance show degrading looking mean spin flip convergence training well recall convergence time clearly take much converge stable take closer look fact convergence time slightly confirming degraded seems agree bruce conclusion convergence time simulation convergence time governed induced noise collier mean flip network convergence flip noise mean flip network convergence flip noise vergence much tanaka yamada size system storage ratio induced noise conclusion estimating convergence time successful failed recall spin flip clearly scope need confirming analytically also natural training nevertheless something convergence time linear associative network hopfield type seen predicted convergence rate training possibly correlation bias despite system size tanaka yamada bruce convergence rate linked storage ratio major recall fails behaviour convergence time governed system size storage ratio clearly dominant acknowledgement like thank tsujii many helpful suggestion computational linguistic work also gratefully acknowledge kind permission asahi newspaper japan editorial corpus funding economics social council award
