data many machine learning term computational case unknown hand introduces relative triple form like construct kernel preserve relationship learning kernel base kernel well defining optimization linear semidefinite kernel learning show construct convex arrive linear employing subset definite matrix extend formulating novel form feature kernel much publicly data experimentally demonstrate introduced show excellent practice comparing baseline much computationally work data machine learning term verified looking role play nearest neighbor vector machine kernel task dissimilar many task task dependency noted seek build classification regression training data cohen xing schultz joachim lanckriet athitsos interested automatically good task user rely label lanckriet globally dissimilar xing wagstaff explore form user supervision like interested consistent user also interested obtaining thus evaluated looking part feature dimension implying form feature formally like satisfies well approximated rely mercer kernel cristianini kernel thus mixture kernel throughoutthis work convex obtaining extend idea motivation framework work learning rosales fung kernel show relative involving triple form used learn think parallel learning rely supervised learning label dependent semidefinite graepel lanckriet show linear introduced also sufficient practice much computationally much extend form feature lead linear kernel also optimizes succinct feature kernel year idea kernel learning considerable machine learning traditionally kernel choosing parametric family kernel gaussian learning parameter tuning procedure procedure case suffers drawback calculating kernel matrix parameter computationally prohibitive training data little none desired incorporated kernel learning procedure bennett lanckriet fung linear kernel belong family superset kernel parameter transforms choosing kernel optimal linear kernel family need predefine kernel final kernel constructed classification also mixture depend attached machine learning classification regression inference inspired fact kernel seen rely explicit subset training dataset user satisfied reflected desired kernel learned relative explained concentrate proximity comparisonsamong triple type like choosing type relative relationship inspired athitsos cohen schultz joachim rosales fung relationship ranking relative form kernel note interested absolute much difficult absolute imply relative converse true kernel kernel implied relationship linear relationship kernel kernel matrix expressed linear referred restricting kernel family kernel depend feature time able learn kernel depend feature seen implicitly feature data benefit linear kernel background vector assumed column vector unless transposed vector superscript scalar vector ddimensional real column vector arbitrary dimension zero kernel matrix kernel simply linear kernel linear kernel appropriatesimilarity unknown relative formally kernel kernel user able relationship sparsely interested kernel satisfies relationship rest kernel kernel mapping gaussian kernel composed linear kernel pointed lanckriet seen predefined guess kernel matrix note kernel matrix parameter goal find kernel satisfies relative kernel specified user achieving slack regularizer kernel matrix alternatively kernel control parameter control satisfaction regularization strength tuning note seems sufficient optimize semidefinite linear suitable task hand relatively semidefinite enforcing fung lanckriet restricting kernel strongly attainable kernel restriction explore alternative kernel anywithout compromisingcomputational requiring well characterized subfamily matrix diagonal dominant matrix motivation next stated golub loan diagonal dominance symmetric strict definite diagonal dominance matrix diagonal simplifying letting arrive alternative training used triple defining auxiliary rewritten linear dimensionality note simplify overloaded clear motivation note minimizing minimizing implicitly minimizing optimal combining diagonal dominant semidefinite learning kernel depend fewer feature next modify learn kernel depend subset feature know optimization feature kernel feature implicit feature induced kernel feature kernel lead nonlinear nonconvex optimization problemsthat difficult nonlinearity feature introduced kernel mapping orderto overcomethis difficultywe idea kernel depend feature comprised weak kernel depend feature time vector weak gaussian kernel feature kernel depend feature linear weak kernel note depend feature motivates next feature weak onedimensional kernel indexed feature kernel note feature minimizing depend featuref used collection nine publicly datasets part datasets datasets used machine learning benchmark datasets motivated evaluating competing xing aimed learn kernel comparing gaussian width linear kernel kernel also used mixture kernel thus reasonable baseline compareboth implicit feature defining weak kernel full kernel matrix depend dimension also xing obeys identical type supervision explained code data made public outperformed constrained wagstaff task good clustering benchmark datasets name dims ionosphere iris wine balance scale wisc soybean protein pima diabetes datasets employed used classification label explicit label introduced relative subset clearly label label triple training belong belongs triple remaining case xing supervision form dissimilar identify label build pair likewise dissimilar pair supervision find optimal mahalanobis matrix closer xing triple used learning learning xing dissimilar believe fair supervision roughly superset construction data training testing training training triple explained training testing repeatedly label imply differentclass label relationship learned closer used thus percentage simply proportion test sampled relationship balancing parameter cross validation splitting training half tested parameter parameter dimension employed favor fewer kernel dimension likewise favor kernel matrix trace show gaussian kernel linear kernel identically kernel degree omitted graph mean deviation kernel sampling dataset consistent sample used testing triple expect optimal mixture kernel performancein case full weak kernel mixture kernel case seen case predetermined kernel suboptimal case weak kernel kernel case mixture full kernel case kernel simply unlucky sampling test datasets deviation justify possibility much imposed restriction mixture kernel dataset recall concentrated subspace matrix diagonal dominant matrix full cone matrix incorporated kernel mixture good base kernel mixture kernel note much computational cost datasets interestingly mixture weak kernel superior degree freedom mixing parameter also feature overfitting even case datasets suggest case theoretically guaranteed dimension feature representable linear feature show optimal dimension process cross validation dimension identified counting automatic dimensionality done explained valuable property note kernel nine datasets show full kernel matrix weak kernel show training applicable kernel mixture term percentage test relationship label triple used training show deviation effectively reduces feature dataset show weak kernel xing show percentage averaged data triple test comparing note dataset clearly outperforms competing interestingly dataset optimal dimension dimensionality implicit implied kernel employed believe superior performanceof mixture weak kernel automatic identification relevant dimension reduction dimensionality time simpler preferable blumer reduce overfitting practice computational perspective test time able data succinctly advantageous calculated computationaltime step precomputed retrieval stored conceptual view also tool data identify dimension high relevance task dimensionality reduction dimension dimension dataset optimal parameter triple competing weak kernel nine datasets show term percentage test relationship label triple used training show deviation conclusion work novel learning kernel fromrelative generalformulation consisted convex optimization requiring semidefinite practical requiring linear showed diagonal dominance kernel matrix lead efficiently extend implicit feature kernel indicated weak kernel outperforms learn relative used potentially imposed sign magnitude restriction kernel impose isometry weinberger request kernel preserve believe implication suggest case replaced linear suggested goal
