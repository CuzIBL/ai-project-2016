tree grafting node tree prediction grafting considers training data leaf tree case fail test path leaf demonstrated retain reduction grafting dramatically time inferred tree reveal grafting operated variance reduction reduces bias variance committee notably adaboost freund schapire bagging breiman demonstrated spectacular success tree wide learning task quinlan bauer kohavi base learning time form committee classifier committee vote classify unseen case success demonstrated room upon case prediction tree learning committee deliver cost tree straight forward interpret comprehension committee juxtaposition constituent infeasible case construct tree inherent committee even committee exponential size derived tree relative unmodified base learning quinlan domain reduce ease comprehension critically tree grafting obtains benefit committee creating tree webb machine learning grafting postprocess inferred tree identifies occupied training occupied misclassified training considers alternative classification classification considering alternative branch ancestor node leaf containing alternative branch stronger alternative classification tree branch grafted tree imposes classification tree much forming tree committee also also continuous valued tree grafting extend grafting discrete valued dramatically reduce induction time reduce inferred tree prediction also grafting reduction demonstrating variance reduction reduces bias variance grafting tree grafting investigate utility occam razor webb initially conceived practical learning success prediction aimed creating practical learning postprocessor release demonstrated frequent modest reduction prediction unmodified wide learning task webb grafting handle discrete valued investigates turn leaf tree leaf climb tree investigating ancestor node alternative encompassed sess alternative evidence fall actually imposed ancestor evidence stronger alternative classification classification leaf imposition identified ancestor ordered imposed best classification leaf applying laplace training belong reasonably consistent classical tree induction deficiency computational leaf tree tree training case infeasible much tree learning derived data majority processing leaf subset training data need consideration training data reach ancestor leaf leaf training data root must processed leaf concern grafting considers graft must substantially risk overfitting data substantial risk graft good chance viewed form oversearch quinlan justification grafting outside leaf deciding classify leaf occupied training used tree located relative leaf fall close investigated little influence classification happens divided close root tree training case separated closer leaf influence ancestor climb tree training case fails test leading much influence case fails test root tree even adjoin fall partitioned highlighted leaf site considers imposed root tree grafting grafting mean estimating leaf training case quinlan leaf training case herein atbop atbop leaf separated covered surface reached case fails test path root tree illustrated replicated webb projected geometric represents dashed line tree release training data vertical line left horizontal line projected atbop leaf highlighted illustrate webb tree atbop leaf formed removing surface enclose explores tree grafting atbop ancestor node atbop evidence supporting alternative classification leaf ancestor node computational greatly data leaf case atbop ancestor data considerably largest data used training data root imposed learning affect outcome root treated identically deepest tree motivated alternative evaluated considerably decreasing opportunity chance overfitting training data resulting identical step replaced utility grafting four data repository blake keogh merz data employed grafting webb augmented wide discrete valued data note data employed slightly disadvantage grafting discordant sick data variant hypo data outperformed conducted data prediction grafting prediction grafting well four modified release incorporates ease grafting pruned tree suggested best machine learning mance webb ultra dual data evaluated round cross validation round data divided subset subset turn tree learned remaining subset evaluated procedure used bias variance variance degree prediction learned classifier vary training data variance prediction test item made training data must bias central tendency learning kohavi wolpert applicable domain close numeric regression geman bienenstock doursat also intrinsic noise domain followed kohavi wolpert aggregating quantity bias round cross validation replicated kohavi wolpert substituted sampling ensuring item used time training time item also used time testing uniformity time item utilized role consistency procedure kohavi wolpert procedure training used zero time training zero time testing variability statistical variance bias learning variance mean declassification divided classification thirty tree learned evaluated learning domain last mean data seen grafting narrowly outperform plain latter treated indicative clear extent rate data commensurable pair learning labeled geometric mean ratio nominated geometric geometric mean aggregate ratio outcome arithmetic mean data data anneal audio auto breast cancer slov breast cancer wise cleveland aust german discordant echocardiogram glass heart hepatitis hungarian hypo iris lens lymphography pima diabetes tumor promoter segment sick sonar soybean splice junction waveform data column relates relates column labeled data labeled binomial probability obtaining relevant outcome equiprobable chance achieves twice many data reverse geometric mean ratio slightly favor notable latter attributed mainly hypo data differs mean also disadvantage arithmetic mean desirable antisymmetry ratio outcome tailed test used consistency throughout prediction made outcome pairwise relative high ratio hypo excluded geometric mean ratio favor statistically many data continuous denying chance alter outperform geometric mean ratio latter statistically sign test outperforms geometric mean ratio achieves data reverse latter statistically suggest hold relative bias aggregate labeled mean corresponds final mean data vary slightly comparative statistic suggest enjoys slight statistically enjoys consistent significance relative variance grafting enjoy consistent enjoy statistically straight forward explanation outcome grafting imposed high tree superimposed tree training data lead predicted outperform argued sign test employed case outcome webb relative bias relative variance tribute node high tree variance enhanced superimposition alternative tree grafting counteract upwards influence variance considering data ancestor node opportunity superimpose consequence supporting evidence atbop comparative node tree produced grafting consistently must consistently tree grafting mean tree produced approximately twice ratio major reduction computational grafting time four system grafting time induction tree postprocessing final tree consistently time exclude training test data disk classifier test data minor overhead measuring bias variance data triple time greatest machine learning relative time grafting segment data time time conclusion grafting atbop demonstrated grafting signficantly affecting grafting atbop dramatically reduces time size inferred tree grafting previously evaluated term bias variance revealed grafting operated variance reduction grafting atbop slightly variance reduction introduces compensating bias reduction operational profile grafting bagging reduces variance bauer kohavi atbop grafting reduction profile boosting reduces bias variance bauer kohavi reduction grafting much magnitude bagging boosting atbop grafting tree much straight forward interpret committee tree produced boosting bagging consequence grafting deserves serious consideration machine learning desirable minimize producing comprehensible classifier training case reach node unless case case case training case reach parent training case training case laplace training case training case upperlim continuous ancestor node branch upperlim determines reach lowerlim continuous ancestor node branch lowerlim determines reach prob probability obtaining probability selecting leaf dominated tuples containing test continuous find ancestor case case lowerlim maximize laplace case lowerlim tuple find ancestor case case upperlim maximize laplace case upperlim tuple discrete test ancestor find ancestor maximize laplace case tuple remove tuples laplace case prob laplace remove tuples tuple ordered highest lowest replace node test branch lead leaf branch lead else replace node test branch lead leaf branch lead else must replace node test branch lead leaf branch subset branch lead
