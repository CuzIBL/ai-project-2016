crfs successfully domain training remains challenging task novel training crfs virtual evidence boosting simultaneously feature parameter extend boosting handle virtual evidence specified distribution unified framework learning compatibility feature crfs synthetic data well real classification training outperforms training likelihood boosted crfs undirected graphical labeling data lafferty modeling distribution hidden crfs make dependency crfs thus suitable classification task overlapped training crfs feature continuous feature challenging training likelihood inference iteration optimization inference intractable dense network resort inference loopy belief propagation markov monte carlo consequence learning procedure converge suboptimal even diverge alternative maximize training data besag essential idea convert patch patch hidden node true neighbor applying simplified successful domain guidance safely used dependency parameter geyer thompson neither feature neither able adequately handle continuous limitation make unsuitable task real sensor data identifying feature classification alternatively boosting successfully used feature classification viola jones data remains unsolved assumes independence hidden label show seamlessly integrate boosting training thereby combining paradigm integration achieved cutting patch done patch training boosting framework neighbor label treated virtual evidence belief seen soft able avoid neighborhood dependency happens main contribution extend boosting handle feature virtual evidence form likelihood deterministic quantity boosting training crfs able feature parameter unified compatibility feature thus learn dependency data handle discrete continuous validation real classification task well synthetic data crfs degree consistently outperforms training rest work next extend boosting virtual evidence training crfs show work feature training discrete feature mccallum suggested feature induction iteratively dietterich gradient tree boosting feature crfs combine boosting parameter work closest boosted brfs torralba task graph densely brfs combine boosting belief propagation compatibility dependency well feature learning compatibility dependency brfs linear belief weak learner feature compatible inference brfs graph weak pairwise connection show dependency brfs poorly brfs formulate induction compatibility feature differently type feature separately treat type feature consistent thus type iteration automatically pick reliable feature beginning selecting compatibility feature accumulating evidence boosting virtual evidence boosting supervised learning freund schapire training data feature vector label boosting work sequentially learning weak classifier combining final boosting feature deterministic extend virtual evidence pearl feature distribution domain generalize logitboost friedman handle probability closely simplicity logitboost binary classification case friedman logitboost logitboost minimizes term independence label logistic regression refers feature ensemble weak learner logitboost minimizes stepwise ensemble logitboost selects next weak learner newton step ensemble computing newton step wlse argmin response sample virtual evidence extend logitboost handle virtual evidence belief feature feature vector boosting probabilistic distribution feature domain opposed distribution feature domain minimize represents probability true label conditioned virtual evidence term computes replace also helpful think message thus consistent belief belief propagation next weak learner modify logitboost criterion taking expectation virtual evidence argmin argmin logitboost construct iteration iteration formulates wlse line solves training data approximately minimizes likelihood response best extending logitboost virtual evidence next weak learner line deterministic thus exactly logitboost logitboost able handle deterministic evidence well property logitboost minimizes newton step virtual evidence boosting training boosting assumes independence training crfs label dependent convert patch patch hidden node neighbor true label neighbor message neighbor virtual evidence boosting patch feature parameter idea training crfs virtual evidence boosting handle continuous discrete used crfs arbitrary binary classification readily labeling done crucial logitboost virtual evidence handled considers type evidence node type hard evidence corresponds training data type soft evidence message neighboring node message belief propagation ensemble linear feature boosting treat type virtual evidence learn feature compatibility feature unified framework training data learned virtual evidence likelihood response best training crfs note virtual evidence must iteration message keep changing virtual evidence neighbor distinguish message multiplying compatibility message message iteratively belief propagation used normalization seen distribution sending node recipient node prefer correspond exactly message sent regular belief propagation message virtual evidence belief propagation combined virtual evidence node stacking virtual evidence message combined virtual evidence used distribution equivalently belief propagation used normalization summarize iteration virtual evidence belief propagation feature compatibility contained initially distribution proceeds exactly logitboost weak learner sufficient belief propagation iteration learning greatly feature step step find best weak learner wlse note weak learner crfs kind feature feature parameter weak learner linear feature type neighbor nutshell best weak learner enumerates type neighbor type computes optimal parameter weak learner pick well optimal parameter formulate weak learner crfs binary label optimal parameter efficiently case weak learner continuous discrete neighbor relationship case treated like regular logitboost boosting neighbor relationship handle virtual evidence evidence belief propagation continuous weak learner linear stump threshold feature approximately optimal wlse heuristic maximize gain optimal analytically derivative zero thus discrete weak learner expressed feature indicator optimal calculated type neighbor virtual evidence weak learner indicator compatibility feature wlse virtual evidence optimal unify case computing optimal feature feature data crfs patch feature real compatibility feature also parameter sharing node neighbor type thus parameter simply feature counting make whole pick reliable message neighbor close beginning iteration picking compatibility feature message alternative boosted likelihood synthetic data datasets well iteration iteration iteration belief propagation learning shrinkage zero mean unit variance optimization procedure likelihood bethe yedidia gradient belief propagation learned tested belief propagation inference torralba calculated label memory synthetic data assumes graph densely thereby message informative make connectivity true torralba invalid many examine dependency node stronger synthetic data markov binary label emphasize learning compatibility feature intentionally weak label binary probability sampled adjust transition probability label classification synthetic data confidence transition probability pairwise dependency turn weak learning feature label transition linearchain additionally time generating duration confidence clear compatibility dependency transition probability give dependency stronger dramatically outperforms mainly weak hold feature many real dependency modeled markov practice impossible know markov dependency data simulate generating synthetic data markov transition probability label past label unknown pick transition probability repeat unknown connection pair node trained reliably identify picking pairwise feature also feature learning well classification robustly extract sparse significantly outperforms time training take training take minute training boosting boosting indoor real data crfs task real sensor data overlap relationship training crfs handle continuous sensor straightforward terrible simply continuous feature fact feature correspond gaussian truth trick circumvent difficulty learn stump heuristic boosting logitboost stump feature stump done friedman indoor person collected audio acceleration sensor data stayed indoors wearable device data minute recorded goal recognize person major indoor meal meeting watching sleeping segmented data chunk manually labeled minute supervised learning testing chunk data feature frequency band linear signal autocorrelation entropy feature linear person vary significantly meal hard recognize sleeping relatively easy recognize vary significantly deviation matter incorporate continuous even overlap significantly competitor reported simultaneously inferring spatial omitted simplicity training boosting adaboost inferring recognizing spatial subramanya simultaneously recognizing stationary walking driving going stair spatial indoors outdoors vehicle wearable sensor train feature adaboost incorporate boosted classifier infers jointly data episode label task data capture pairwise learning loopy crfs intractable finish take hour training confidence clearly outperforms well conclusion work novel unified training crfs virtual evidence boosting simultaneously informative feature discrete continuous optimal parameter part training logitboost handle virtual evidence treating neighborhood feature also learns connectivity crfs demonstrate virtual evidence boosting significantly outperforms training synthetic data real work want taskar explore possibility learning feature acknowledgment thank benson limketkai help preparing last dataset work grant educator fellowship also darpa assist calo subcontract
