classification putting semantic answering introduces subtree mining classification formulate classifying tree label label subtrees forest training data tree classification entropy boosting used classifier classification data show subtrees entropy boosting promising achieves comparable even kernel also improves testing retrieval system retrieval relevant user really want precise answer american user really want answer alan shepard read word american answer need asked classification impose plausible answer also suggest processing system understands american asks person name plausible answer significantly fact answering system classification module classification answering system classification little classification word word thus word like else omitted dimension reduction step process creating feature word classification also word frequency play role classification frequency thus significantly contribute classification shortness make feature sparse thus make feature difficult classification main classification classification handcrafted rule reported voorhees voorhees voorhees manually heuristic rule task tremendous tedious work form type reasonable manually constructed classification applies machine learning classification radev machine learning tool ripper utilized trained tested classification trec dataset reported system high classification roth snow good system feature relwords word constructed automatically also reported feature classification task word part speech chunking label entity label indicated hierarchical classification natural classification classified classifier classifies coarse classifies fine employed tree kernel classifier indicated syntactic well tree kernel suitable showed coarse classifying fine hand connection data mining machine learning show data mining help classifier morhishita morhishita showed association rule mining helpful enhancing boosting berzal also berzal family list association rule mined training data reported good data roth mining subtrees helpful syntactic parsing kudo well ensemble learning semantic parsing nguyen success natural subtree mining helpful classification paperinvestigatesthe connectionof subtree mining entropy boosting formulate classification tree classification utilizing subtrees forest training data entropy boosting tree classification remainder formulates tree classification entropy boosting subtree feature subtree mining show give conclusion plan work classifier tree sentence parsed tree tree word dependency tree syntactic tree problemof classification equivalentto classifying syntactic tree tree classification induce mapping training labeled ordered tree label training data feature vector labeled ordered tree labeled ordered tree tree node label ordered sibling child child child labeled ordered tree match subtree node satisfying preserve preserve sibling preserve label labeled ordered tree label stump classifier tree show labeled ordered tree subtree labeled ordered tree subtree stump training data tree label incorporated entropy boosting boosting subtree feature stump classifier tree inaccurate real final relies existence tree boosted boosting schapire boosting repeatedly call weak learner hypothesis linear hypothesis produced weak learner weak learner built iteration distribution calculated hard focused many boosting variant best adaboost schapire used adaboost stump serving weak entropy subtree feature entropy berger make unnecessary independence able optimally integrate whatever believe potentially transformation task entropy entropy able feature beneficial effectively ignore irrelevant probability vector feature normalization feature vector binary feature feature feature feature optimization nocedal entropy subtrees feature subtrees label feature training data framework entropy subtrees mined incorporated feature mentioned subtree mining goal subtrees efficiently boosting subtrees corpus generating subtrees corpus seems intractable fortunately zaki zaki enumerate subtrees tree tree consisting node expands tree size attaching node tree tree size inefficient expand node arbitrary tree duplicated enumeration inevitable rightmost avoids duplicated enumeration restricting mine subtrees used parameter bellow minsup frequency subtree data maxpt subtree minpt subtree show mining subtrees mined corpus frequency subtree vbzfeatures vbnconsidered vbncredited vbndealt vbninvented subtree mining used entropy boosting subsection subtrees used boosting subtree boosting subtree boosting stump eliminate subtree gain boosting process borrow kudo kudo bellow training data tree labeled normalized find optimal rule maximizes gain naive exhaustive enumerate subtrees calculate gain subtrees impractical subtrees exponential size find optimal rule modeled variant summarized canonical whole subtrees tree enumerated find optimal rule traversing prune proposing criterion gain canonical mining subtree zaki used gain process subtree subtree morhishita gain note used binary classification adapt multi classification correcting code multi classification binary classification subtrees mentioned binary classification boosting efficiently prune spanned gain traverse subtree lattice built recursive process rightmost maintain temporally suboptimal gain gain calculated previously delta gain delta safely prune spanned subtree prune branch selecting subtrees listed note mean subtree algorithmfork work binary training subtrees used label subtree subtree entropy conducted subtree find optimal rule mined zaki frequency subtree used subtrees frequency subtree suitably used feature entropy framework entropy used feature gained classification promising boosting find subtree feature entropy subtree boosting subtree feature subtrees boosting data fact data label classification data subtree feature applying boosting tree data combined feature entropy boosting subtrees entropy find optimization subtree feature entropy optimization nocedal opinion subtree feature maximu entropy linear adaboost experimentswere conducted pentium tested data work collected four english manually constructed rare trec trec also trec data used work classification roth label followed twolayered taxonomy roth coarse grained fine grained bellow coarse grained fine grained abbrs abbreviation desc enty animal body color creation currency food instrument letter plant religion sport substance term vehicle word title city country mountain code date money temperature size subtrees initially used chaniak parser charniak parse sentence training testing data training data consisting tree label mined subtrees zaki show entropy subtrees train entropy used nocedal gaussian smoothing entropy subtrees feature enty adjp advp enty adjp inof enty adjp jjcelebrated adjp jjcommon adjp jjdead enty substance adjp rbsmost jjcommon adjp infor dtthe nnnumber classification used roth correctpredictions prediction conducted confirm subtree mining classification entropy boosting done test confident show subtree entropy subtree boosting stboost subtree boosting entropy tree kernel coarse grained show boosting achieved competitive tree kernel outperforms best classification subtrees coarse grained label classification subtrees fine grained label show subtreeboost classification also show word feature entropy vector machine tree kernel reported tree kernel fine grained reported fine grained subtrees entropy boosting semantic feature word work highest utilizes subtree feature boosting entropy subtree semantic feature entity type word overlap mixing feature classification task also boosting outperformed subtrees computational time boosting training time entropy approximately time boosting approximately hour finish training process boosting training data entropy approximately hour computational time subtree boosting feature entropy comparable subtree boosting testing also approximately time boosting guess boosting find optimal rule training process label affect computational time depicted boosting subtrees minsup maxpt mean frequency subtree reported subtree mined parameter classification subtrees fine grained label parameter boosting entropy mathematical worth investigating subtree mining classification relies parser syntactic tree parser charniak parser used targeted parse parser trained penn treebank corpus composed manually labeled newswire text surprising parser high parsing training corpus hermjakob parsing dramatically improves complementing penn treebank corpus marcus labeled training believe parser beneficial summarily show subtree mined corpus task classification also indicated subtrees used feature entropy weak boosting give alternative subtrees classification showed mining subtrees significantly contribution classification investigate syntactic tree overlap entity type word tree syntactic semantic beneficial like investigate semantic parsing semantic role labelling task conclusion proposes incorporating subtrees mined training data classification formulate classification classifying tree prefixed entropy booting subtrees feature weak considering subtree mining subtree feature process show boosting achieved high work semantic feature boosting outperformed entropy subtrees computational time slower time boosting entropy subtree feature achieved substantially term computational time work also focused extending learning efficiently learnt labeled unlabeled data also plan hierarchical classification wordnet semantic roth roth hope extend work interactive answering task answering system able interact user lead contextual subtree mining depend task classification believe suitable task text classification text summarization data tree acknowledgment like thank reviewer helpful comment manuscript thanks cognitive computation uiuc opening datasets thank mary jaist correcting grammatical work jaist century verifiable evolvable
