modelling data structured domain establishing scale arise data multiscale also dynamic must modelled particularly case data unsegmented probabilistic graphical predominant framework labelling unsegmented data structured domain degree priori hierarchical system connectionist temporal classification labelling unsegmented data scale recurrent neural network spoken digit show system outperforms hidden markov fewer domain assigning label unsegmented stream data goal practical task speech handwriting domain scale captured hierarchical assist process labelling probabilistic graphical hidden markov rabiner hmms lafferty crfs predominant framework labelling novel connectionist temporal classification graf label unsegmented data recurrent neural network rnns like crfs hmms discriminant crfs hmms labelling sense explicit statistical property data explicit hierarchical architecture used hmms estimating parameter hierarchy maximises hierarchy nonetheless hmms efficiently scale hierarchical form witten describes hierarchy crfs kumar hebert classifying hierarchy trained globally sequentially parameter parameter fixed transition matrix hierarchical extend applicability labelling structured domain hierarchical hctc successive network predicts label feed forward next label data scale signal network trained gradient relative prediction adjusted degree uncertainty target label depend variability data case used training network potentially discover data accurate final prediction next briefly introduces describes architecture hctc hierarchical hmms hctc speech task guideline work final conclusion connectionist temporal classification labelling unsegmented data rnns graf idea behind interpret network probability distribution label conditioned data distribution derived maximises probability labellings differentiable network trained backpropagation time werbos network time conditionally network long feedback connection network network softmax bridle unit label task activation extra unit probability observing blank label time step activation unit probability observing label blank unit label consecutively label trained network series spike separated blank spike corresponds data guaranteed find precise alignment classification label alphabet label choosing label blank timestep probability network defines probability distribution alphabet label blank disambiguate label refer path probability path activation unit time path mapped label removing repeated label removing blank path path onto labelling probability labelling probability path implied process classifier probable labelling argmaxp intractable graf used assumes probable path correspond probable labelling argmaxp concatenation training derived likelihood maximise probability labelling training training consisting pair target minimised network trained gradient differentiating network achieved calculating probability dynamic recursion used hmms graf probability passed time probability passing rest label reached time blank path label modified label blank beginning inserted pair label calculating transition blank label also pair distinct label blank blank last find noting label blank repeated time labelling label empty differentiate network differentiating signal network schematic hctc network labelling unsegmented data stream correspond probability detected label emitting blank label time training unnormalised normalised softmax normalization hierarchical connectionist temporal classification hierarchy network series network network receiving signal network receiving network architecture illustrated note network hierarchy softmax force hierarchical system make degree data facilitated probability modular flexibility system architecture feeding hierarchy long mathematical training hctc target hierarchy training training schematic signal flow hctc network network trained globally gradient signal injected contribution signal adjusted pair target system trained globally gradient illustrates signal flow hctc network signal target network simplify network network signal targeti network contribution term signal unnormalised activation unit networkis unit softmax unit signal unit connection unit activation softmax signal network contribution contribution hand target uncertain itargeti backpropi digit phoneme zero four five seven eight nine phonetic label used digit case target network free make minimises backpropi hierarchy training done globally network potentially discover data accurate prediction hierarchy validate hctc chose speech task scale hmms remain speech hctc hmms task find digit spoken utterance phoneme word database leonard doddington thousand digit spoken woman child database recorded dialectically balanced utterance distributed test training five training validation left utterance training validation test eleven digit database zero nine utterance consist seven digit digit phonetic used seen nineteen phonetic used nine digit sample digitized quantization acoustic signal transformed frequency cepstral coefficient mfcc toolkit young spectral carried channel filter bank coefficient used spectral tilt twelve mfcc plus coefficient hamming window long delta acceleration coefficient giving vector coefficient network coefficient normalised mean zero deviation training system toolkit young used nineteen phonetic silence pause allowed digit also probability modelled mixture gaussians grammar allowed preceded followed silence digit neither linguistic probability system gaussians penalty optimised validation training gaussians increased step validation stabilised case decreased slightly gaussians time gaussians increased parameter herest twice collected validation penalty varying step best validation gaussians penalty parameter hctc used target digit stream bottom used target phoneme target digit network lstm recurrent neural network graf schmidhuber graf phoneme task rnns reported graf also formalism best realised recurrent neural network schuster paliwal network time past hidden hidden size forward backward block size phonetic plus blank size also forward backward block size eleven digit plus blank label cell activation hyperbolic tangent gate used logistic sigmoid hctc network training hctc network done gradient training case learning rate momentum initialized training gaussian noise deviation generalisation system hctc hctc label rate tidigits hctc mean give confidence degree freedom test hctc best coincides best normalised edit label rate target label label system rate system tested seen best system achieved rate continuous digit label rate hctc confidence approximately half parameter best injecting signal phoneme label good best achieved case activation seven unit twenty encode dynamic data make accurate prediction digit utterance seven unit time digit digit nonetheless phonetic encode type signal work inserting hierarchy target label target label phonetic label used suspected invalid presence dialectal dataset variability data specified accurately system capable experimentally mean system difficult train goal hctc target label phonetic achieved albeit mean nevertheless hctc suffered extent reasonable target label specified priori train hctc decreased significantly remove contribution signal target label free partially trained network explore alternative maximise hierarchy also like explore improving system scalability vocabulary speech system work many thousand word size hctc practical assigning unit label explored assigning label activation unit modifying training like investigate hctc word spotting task discriminant hctc rate discriminating speech besides hctc posteriori probability help confidence prediction generative hmms unnormalized likelihood conclusion hctc hierarchy recurrent neural network label unsegmented data structured domain hctc capable accurate prediction hierarchy neural network flexible modelling domain time system trained globally demonstrated outperformed hidden markov speech task acknowledgment funded grant
