reinforcement learning good balance need explore need gain exploiting much devoted many aimed simply ensuring sample gathered well bellman kalaba constructing system paired markov maintained unfortunately intractable bandit give rise gittins optimal exploration explore idea computationally tractable maintain environmentas markov process sample trajectory infinite tree idea sparse sampling node sparse subtree expressed optimization linear illustrate domain exploration reinforcement learning posed need explore sufficiently discover time exploiting agent taking yield high literature boltzmann exploration aimed exploration enables agent convergence reinforcement learning view sample gathered agent repeatedly harmful extensive line devoted directed exploration heuristic help agent data trying protect danger thrun meuleau bourgine term sample agent attain optimal kearns brafman tennenholtz strehl littman strehl explore bayesian exploration idea bellman suggested keeping agent learned agent view grows exponentially horizon make inapplicable case bandit give rise gittins body work bayesian reinforcement learning approximating procedure case mdps dearden duff wang pursue idea linear procedure sampling expand horizon process background linear empirical domain background finite markov process tuple finite finite rmax agent distribution next deterministic stationary determines take goal reinforcement learning find maximizes discounted time step discount discounted note hyperstate mdps mdps transition probability unknown agent maintains transition probability convenient distribution closed observing transition mdps finite transition probability maintained matrix size martin matrix beta distribution closed natural distribution case indexing parameter distribution maij simply transition transition probability letting matrix transition martin distribution parameter also matrix beta distribution dija distribution operation observing transition going summarized matrix matrix beta distribution viewed agent accumulated form hmdp hmdp hmdp consist hyperstate summarizing agent experience distribution consistent data time step precisely transition hyperstate taking hyperstate dija dija innformation reflect transition note affect parameter parameter fixed hyperstatein hmdp successor successor hyperstate uniquely indexed hmdp infinite tree note also hmdp take transition even bellman optimality hmdp exactly yield optimal bellman kalaba martin clear hmdp infinite intractable also clear hyperstates exponentially tree subset tree shallow computing empirically good learning finite subset tree left work comparing exploration work optimal learning seen year much work spurred kearns guaranteed converge optimal learning time drawback difficulty intuition necessarily scale well practical brafman tennenholtz realtime dynamic strehl littman strehl explore assigning exploratory drawback kearns strehl myopic long term hyperstates exploration overcomes myopic hyperstates introduced bellman kalaba refers adaptive control process mathematically rich algorithmic idea duff heuristic approximating adaptive control process empirically good wang sampling infinite hypertree manageable tree exactly mdps path sampling give thompson sampling expand tree locally promising unsampled effectively last sampled node planning horizon sparse sampling kearns used decide optimal correction procedure used desired horizon reached empirical demonstrate well sparse sampling enables able handle infinite hyperstate take wang maintained pair hyperstate sample qlearning sutton barto easy good hyperstates linear linear around long time well understood elegant puterman work linear farias hauskrecht kveton suggests used work continuous linear hopeful eventually used generalize continuous mdps discrete mdps schweitzer seidmann optimality hyperstate formulated linear minimize refer hyperstates mean linear leastvariables prohibitive kearns overcome obstacle constructing sparse sample tree idea construct sampling tree incrementally precisely sample hyperstate decide node dynamic system sample next node trajectory trajectory completed desired horizon continue constructing trajectory desired sample reached encounter hyperstate true optimal used note unsampled constitute subtree hyperstate unsampled subtrees hyperstate root subtree correspondinng effectively many want linear sampled hyper tree built linear gather sample greedily linear enter unsampled greedy attached decide construct hyper tree maxsamples numsteps maxepochs matrix distribution epoch maxepochs construct maxsamples sampled trajectory sample hypertree greedily hyperstate observe transition hyper parameter transition numsteps greedily hyperstate observe transition hyper parameter transition epoch epoch outline maxepochs parameter defines many sample gathered parameter describes expand sampled hypertree maxsamples parameter control many trajectory sampled width tree parameter control linear numsteps parameter defines many step recomputed trade computation time obviously sample sample truly believed best wait selecting imperfect computation significantly empirical tested domain averaged agent dynamic left parameter varying wang also experimented myopic agent acting agent chooses last worse omitted clarity experimentedwith differentparametersettings explained primal dual also constructed desired epoch completed dynamic acquired obviously latter extra computation also imprecise left panel intuitively best reached keep graph parameter numsteps maxsamples qlearning wang plot episode note wang able handle sample outperformed graph sample construct hypertree rest interestingly case also give last step dynamic bandit bandit slot machine bandit modeled dynamic dynamic left panel solid dashed solid grey leftmost corresponds winning middle corresponds winning machine rightmost corresponds winning machine cost gambling comparisonbetween parameter numsteps maxsamples wang able handle sample well outperformed even sample well best qlearning grid gridworld everywhere else four north south west east probability remaining agent move wall deterministically stay parameter used numsteps maxsamples able find good clear dynamic advantageous task returned yield initially probably hmdp gridworld much probably size hypertree bigger sample closer true noted even exactly case hmdps superior note also experimented boltzmann exploration omitted plot clarity case bandit dynamic left parameter influence left influence parameter gridworld task time demonstrate term term computation time hyperstates obviously much slower plot time episode returned domain well time wang worth observing returned considerably slower wang returned gridworld hmdp slower wang wang insignificant superior demonstrated hmdp wang bandit grid time conclusion work demonstrate bayesian learning sparse sampling linear hyperstates lead exploring unknown used simpler exploration even wang drawback leave area unexplored desired agent robot task exploring want agent avoid area harm mechanical part even around explore sufficiently reach high like justify computation sparse sampling hypertree thus agent myopic able make bayesian learning good exploratory agent next step also like strehl possibly duff empirically latest bayesian also continuous placing gaussian sampled assist approximating full hypertree work full done farias hauskrecht kveton mdps acknowledgment work part nserc fqrnt
