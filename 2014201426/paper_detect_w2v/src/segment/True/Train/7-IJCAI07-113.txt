linear discriminant tool studying relationship data major disadvantage fails discover geometrical data manifold novel linear discriminant sensitive discriminant lsda sufficient training sample importantthan globalstructure discriminantanalysis discovering manifold lsda find maximizes data area data mapped subspace nearby label close nearbypoints differentlabels carried face database show clear practical supervised machine learning degrade prediction faced many feature predicting desired machine learning extract good feature resolve dimensionality reduction linear discriminant duda unsupervised data maximal variance supervised data requiring data close spectral eigenvalue covariance matrix scatter matrix scatter matrix betweenclass scatter matrix intrinsically statistic mean covariance fail sufficient sample effectively euclidean fail discover data life close submanifold ambient geometrically motivated data high dimensional isoamp tenenbaum laplacian eigenmap belkin niyogi locally linear embedding roweis saul discovering geometrical manifold unsupervised fail discover discriminant data meantime manifold learning attracted considerable zhou belkin make labeled unlabeled sample labeled sample used discover discriminant unlabeled sample used discover geometrical unlabeled sample outperform supervised learning vector machine regression belkin face unlabeled sample thus learning novel supervised dimensionality reduction sensitive discriminant geometry data manifold construct nearest neighbor graph geometrical manifold graph graph graph label geometrical discriminant data manifold accurately characterized graph graph laplacian chung find linear transformation matrix data subspace linear transformation optimally preserve neighborhood well discriminant neighborhood data maximized structured brief review linear discriminant sensitive discriminant lsda introduced lsda reproducing kernel hilbert rkhs give rise kernel lsda concluding work generic linear dimensionality reduction find transformation matrix represents atxi linear discriminant seek discrimination data belong label atsba aopt argmax atswa sample mean vector sample vector sample call scatter matrix scatter matrix eigenvectors largest eigenvalue clearly preserve relationship data fails discover intrinsic geometrical data manifold many real face sufficient training sample case able accurately besides graph linear dimensionality reduction niyogi discriminant embedding chen marginal fisher graph geometrical data graph discriminant data implicitly equally reduces flexibility sensitive discriminant sensitive discriminant discriminant geometrical data sensitive discriminant sensitive discriminant dimensionality reduction previously naturally occurring data structured system possibly much fewer degree freedom ambient dimension suggest thus case data life close submanifold ambient hope geometrical discriminant property submanifold lying unknown submanifold maximizing data sampled submanifold build nearest neighbor graph geometrical data find nearest neighbor edge neighbor nearest neighbor thus matrix nearest neighbor graph matrix characterizes geometry data manifold used manifold learning belkin niyogi tenenbaum roweis saul niyogi graph fails discover discriminant data discover geometrical discriminant data manifold construct graph graph graph label data naturally subset neighbor sharing label neighbor label five neighbor color belong graph connects nearby label graph connects nearby label sensitive discriminant maximized clearly matrix clear nearest neighbor graph thought graph graph mapping graph graph line stay close stay distant reasonable criterion choosing good optimize graph incurs penalty neighboring mapped actually likewise graph incurs penalty neighboring mapped close actually belong minimizing close sharing label close well also maximizing close label learning procedure illustrated optimal linear embedding subsection sensitive discriminant solves vector matrix algebra atxdwxta atxwwxta diagonal matrix column symmetric atxlbxta diagonal matrix column symmetric laplacian matrix note matrix natural data containing high density around bigger impose ytdwy atxdwxta thus atxwwxta equivalently atxwwxta rewritten atxlbxta optimization reduces suitable vector minimizes eigenvalue eigenvalue column vector ordered eigenvalue thus embedding atxi vector matrix note sample feature rank quently fact xdwxt matrix case remove zero eigenvalue kernel lsda lsda linear fail discover intrinsic geometry data manifold highly nonlinear lsda reproducing kernel hilbert rkhs give rise kernel lsda training sample problemin feature induced nonlinear mapping proper make reproducing kernel hilbert rkhs hold kernel kernel gaussian kernel sigmoid kernel vector orthonormal lead mapping euclidean look help preserve geometrical discriminant data manifold data matrix rkhs eigenvector rkhs eigenvector linear coefficient algebraic kernel matrix column vector test onto eigenvectors vector training investigate lsda face eigenface turk pentland fisherface belhumeur marginal fisher brief data preparation data preparation sample face yale database subject face lighting facial yale database train train train train baseline eigenfaces fisherfaces lsda face database tested yale preprocessing locate face normalized scale orientation aligned facial area cropped final size cropped pixel gray pixel thus vector preprocessing done classifier face nearest neighbor turk pentland bayesian moghaddam vector machine phillips nearest neighbor classifier simplicity nearest neighbor parameter leave cross validation process step calculate face subspace training face face identified projected subspace face identified nearest neighbor classifier face yale database yale face database constructed yale computational control grayscale demonstrate lighting facial normal happy sleepy surprised wink glass show sample training sample rest used testing training used learn face subspace lsda eigenface fisherface subspace repeated process time calculate rate rate varies dimension face subspace best well dimensionality optimal subspace baseline simply face note dimensionality fisherface duda seen outperformed eigenface worst case baseline note training sample best fisherface dimensionalsubspace lsda reach best dimension property show lsda suffer dimensionality crucial subspace learning face face database olivetti laboratory face database used test face sample person captured time open closed smiling nonsmiling facial glass glass tolerancefor tilting rotationof face degree sample displayed training rest used testing givenl randomsplits best optimal subspace dimensionality seen lsda best case fisherface comparatively lsda size training optimal dimensionality lsda fisherface much eigenface face database systematically revealed lsda fisherface performedbetter optimal face subspace dimensionality reduction discover intrinsic face manifold rate lsda consistentlyoutperformed eigenface fisherface size training lsda significantly outperformed fisherface probably fact fisherface fails accurately scatter matrix training sample sample face database subject face differentfacial database train train train train baseline eigenfaces fisherfaces lsda eigenface fails gain probably eigneface encode discriminating optimal dimensionality lsda practice computational major concern simply face dimensional subspace conclusion introduced novel linear dimensionality reduction sensitive discriminant lsda spectrally dimensionality reduction optimizes fundamentally criterion classical dimensionality reduction fisher criterion prominent property lsda preservation discriminant geometrical data hand preserve discriminant geometrical ignored face yale database conducted demonstrate effectiveness
