connectionist learning differ inductive explore system paradigm system prediction training system show distinct domain identify system suggest path machine learning grown rapidly year learning system accepts case history classified hypothyroid disease training system form base accurately classify historically dominant assumes base flat treestructured rule defines membership connectionist hinton mcclelland rumelhart base interconnected node computes ject feature arithmetically combined propagated network process terminates computation classification learning alters classification correctness connectionist differ well inductive employed explores implication experimenting system paradigm backpropagation work grant vanderbilt council machine learning propagation system term prediction attained natural domain training describes processing well system respective paradigm qualifies relate suggests foundation work system quinlan learning system construct tree training node tree training partitioned theoretic used prediction membership guess training recursively decomposed remaining improves prediction statisticallysignificant parameter nominal finite color blue tree disjunctive normal form path leaf joined root disjunction quinlan verified attains high absolute sense relative system favorable natural domain idealized noisy connectionist node feature forward node node linked node next interconnection activation node encode nominal unit dedicated unit rest representing feature presence absence unit allocated used sejnowski rosenberg nominal encoding node corresponds classified node highest activation ideally activation node rumelhart hinton williams mcclelland rumelhart adjusts match ideal unit ject belongs ideal unit backpropagated network adjustment proportional size sign degree node contributed node node activation parameter rate used vary adjustment hidden unit converge perfect classification noise vary domain hidden unit linearly separable recognizable properly subset feature subset size must qualify membership inclusive disjunction case conjunctive case feature must disjunctive case feature need describes empirical kind justify parameter domain encoding fair system accept system disparate even agree domain fair must realize system superior dimension cost correctness system hopefully reader avoid unfounded paradigm analyzes system qualify comparative suggest path fertilization tested natural domain thyroid disease case history soybean disease case history stepp congressional voting domain systematically noise domain test system artificial domain linearly separable domain tested varying hidden unit learning rate hidden unit learning rate consistently optimizes come close optimal asymptotic prediction learning domain tested varying confidence confidence well domain training schedule noise feature nominal binary encoding membership encoded natural domain converge asymptotic thus training drawn fixed pool disjoint subset test base intermittent training incremental testing learning resume network derived training nonincremental testing constructed anew training used previously plus newly trial natural domain graph show learning curve congressional thyroid domain congressional domain achieves maintains approximately asymptote thyroid domain attain reach maintains presentation soybean domain averaged significantly confidence hidden unit fisher mckusick test reached perfect presentation domain reach slightly significantly many training presentation relates curve quickly achieves high training gradually converges asymptotic slope curve gradual training presentation unit cost incremental shavlik mooney towell training repeatedly network converges near perfect prediction test classification batch assumes regardless time repeated unit cost approximately fisher mckusick mooney shavlik towell reconciled basically incremental cost presentation inexpensive many machine learning batch cost fewer case time convergence eral magnitude property graph reflect noise noise also explored suggested quintan replaced probability reflected noise asymptotic congressional thyroid domain noise graphed regardless noise domain attained significantly revealed graph learning curve rapidly peak rise slowly artificial domain artificial domain constructed made circumstance systematically vary degree sufficient membership sufficient sufficient individually sufficient necessity presence absence conditioned presence sufficient sufficiency four artificial domain contained describable singly sufficient distinguish four domain varied singlysufficient domain individually sufficient domain sufficient domain sufficient domain sufficient sufficient thus irrelevant classification reached perfect regardless sensitive sufficient distinguish membership convergence perfect prediction hidden unit used sufficient perfect hidden unit sufficient thus sufficient decreased reach asymptotic necessity domain domain describable singly sufficient distinguish conjunctive sufficient distinguish membership size conjunctive varied neither averaged perfect domain allotted training remained perfect prediction size reached perfect training achieved perfect prediction size averaged asymptotic size junctive target grew became difficult spot individually transmitted case size note identical lone sufficient conjunctive domain contained contained disallow size varied consistently attained vicinity achieved reached final artificial domain insists exhibit like membership exactly linearly separable used quinlan capable learning averaging confidence threshold describes descendenf quinlan undoubtedly reach perfect prediction domain exclusive also learned hidden unit hidden unit approximately approximately achieves asymptotic noisy artificial domain considerably presentation identifies system extent distinction connectionist paradigm qualifies encourages exploration connectionist learning avoid distinction proved unhelpful distinction promote progress distinction artificially segregate conveyed prescriptive unknown paradigm must observe believe distinction descriptive fisher mckusick bias attributable size form explored system primitive evidence generalizes primitive combinators inclusive disjunction finer granularity enables back propagation converge hardware primitive must specialized training arbitrary primitive also suggests take bigger step approximates final quickly accept optimal optimum subsumtion suggests harnessed sort heuristic rapid followed slower refinement utgoff perceptron tree tree linear threshold unit leaf tree brings final convergence left leaf must await experimentation nonetheless represents conceptual system probabilistic classification machine learning probabilistic smith medin probabilistic classify summation evidence connectionist network node fisher cobweb viewed constructing tree evidence summation unit nonlinear threshold node highest summation arithmetic evidence traditionally distinguished connectionist learning cobweb illustrates prescriptive monothetic polythetic classification weakness monothetic learning considers utility time predictive merit presumably detriment prediction seems evident polythetic simultaneously summed monothetic property mote machine learning traditionally learning intensive precisely simultaneously utility many quinlan introduced polythetic build monothetic tree confidence terminate convert production rule rule polythetic prompted quinlan genetic classifier connectionist impetus continued quinlan occurred artificial domain promise characterize wide domain determination polythetic learning system overcome granularity must await experimentation polythetic classification learning system need preclude probabilistic noted incremental nonincremental assumes simultaneously processing back propagation process distinction true paradigm spawned incremental variant schlirnmer fisher utgoft incremental learning becoming learning thread system connectionist cobweb finer granularity conservative step conservatism early training many inconsistent evolving irrevocably incomplete cept constructive convergent perhaps overt distinction connectionist system explore respective system reconstruct upon implicitly operator connectionist system preenumerate subset implicit interconnection node arise much slow convergence little learned preenurnerated step reconciling explored schlirnmer granger hampson volper schlirnmer granger stagger system adapts connectionist evidence procedure upon emphasis transfer system hampson volper stress transfer connectionist specialized disjunctive node enumerated integrated thus facilitating rapid convergence picking system well suited enumerating subspace refined concluding empirical uncovered disadvantage learning system system paradigm impetus pursued must scope learning system learning fisher mckusick mooney shavlik towell exploring alternative training combine incremental batch batch training incrementally processed convergence subbatch suggest batch size significantly reduces presentation asymptotic achieved pursuing hope flesh explanation phenomenon term notably view process simulating casebased reasoning best classifier departure usual training moving indiscriminate hopefully exploration training time detrimentally impacting acknowledgement thank mooney jude shavlik geoff towell influential debate comment richard sutton correctness clarity comment ijcai reviewer bibliography fisher acquisition incremental conceptual clustering machine learning fisher mckusick mooney shavlik towell processing connectionist learning system proceeding sixth machine learning ithaca morgan kaufmann hinton connectionist learning procedure artificial intelligence hampson volper disjunctive boolean learning cybernetics mcclelland rumelhart exploration parallel distributed processing bridge quinlan induction tree machine learning quinlan proceeding fifth machine learning arbor morgan kaufmann rumelhart hinton williams learning propagation parallel distributed processing rumelhart mcclelland schlimrner fisher case incremental learning proceeding fifth artificial intelligence philadelphia morgan kaufmann schlimrner granger incremental learning noisy data machine learning sejnowski rosenberg parallel network learn pronounce english text system shavlik mooney towell neural learning wisconsin madison smith medin cambridge harvard stepp conjunctive conceptual clustering experimentation doctoral dissertation illinois urban achampaign utgoff incremental proceeding fifth machine learning arbor morgankaufman utgoff percetron tree case proceeding seventh artificial intelligence paul morgan kaufmann fisher mckusick
