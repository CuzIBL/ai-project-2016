learning discontinuity switching marc toussaint sethu vijayakumar perception school informatics edinburgh king mayfield road edinburgh mtoussai locally learning lwpr vijayakumar successfully used regression robustness crucial robotic domain inverse articulated dynamic robot learned highdimensional joint angle velocity desired signal torque mapping assumed smooth real many case truly discontinuous contact ground part body joint fact many manifest discontinuity sensorimotor data show discontinuous switching regression learned switching ghahramani hinton pavlovic inverse wolpert kawato receives responsibility modeled hidden generative mixture case responsibility predicted robot thus inferring corresponds classifying domain robotic domain learning crucial prevent interference responsibility composition classifier pairwise classifier concatenated construct form capable learning sharply domain lieu gaussian kernel learning family training data goal learn family datum explained acknowledges german foundation emmy noether fellowship predicting responsible explained next formal mixture distribution accounting background noise outlier hidden generates datum localized impose mean trained eligible precisely ploc scalar eligible eligible ploc eligible family infer responsibility datum bayes rule calculating assignment associate training datum sufficient statistic data labeled unmodeled inferred used family heuristic ransac datum unmodeled data closest neighbor euclidean training data poisson mean dimensionality receive responsibility discarded process repeated kernel sigmoids scheme family learning realized type linear learned regression involving proven vijayakumar sigmoids switching goal learn predictive latent responsibility precise uninformed ploc data easy decide potentially neighbored data pair neighbored learn sigmoidal sigmoids around submodel defines coefficient associate submodel normalizes indicated sigmoids scalar illustrates kind kernel sigmoids sigmoids meant likelihood responsible conditioned responsible comparable voting labeling introduced used train sigmoids linear learned tested piecewise linear discontinuous test test parameter dimension linear piece composed noise slope linear piece sampled learning lwpr curve test domain family best fitting eligible averaged test data classification sigmoids predicts best fitting argmaxii find reliably generates family optimal family noise dimension displayed classification rapidly converges zero dimension classification converges around test learned switching iteration training data blended switching lwpr family classification test bold line curve address handling discontinuity naturally arise sensorimotor data structured extends learning responsibility learned sigmoids much versatile gaussian kernel initialization kernel width heuristic submodels circumvented robust incremental allocation consistently used regression machinery utilize learner well classifier work investigate learner well classifier
