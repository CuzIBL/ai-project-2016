agent continually face task agent able learned task task reasonable suggested experience agent must potentially priori applicability efficiently choosing amongst unknown markov process task domain task amongst agent sufficiently face task good task agent reasonable task mapping analogous task know taking take many reasonable agent experience priori agent markov process learned optimal face unknown agent mapping agent terminology used supervised learning think advise agent agent must mediate leverage learning task agent simply ignore advice learn task scratch ideally learning time desire sample dependent size enforce restriction make even expect fromsuch agent best largest asymptotic agent time step near asymptotic clearly unreasonable even agent knew identity mediator need time close unknown mixing time intuitively mixing time time take thereafter close asymptotic thus need reasonable mediator time accomplishes close asymptotic best mixing time thus mediator time competes favorably subset relationship work idea mapping mdps mdps entirely novel homomorphism mdps used compact induce full work restrict mapping seek optimal mapping optimal mapping difficult impossible calculate reasonable mapping heuristically efficiently amongst find good albeit suboptimal quickly much work learning advice team task traditionally focused supervised learning clearly bandit long stood canonical tradeoff learning agent slot machine pull yield drawn unknown fixed distribution agent goal minimize gotten pulling best actually receives robbins pull achieves logt clear relied fact fixed distribution time case bandit removed statistical adversary distributionof time step auer even adversarial auer assumed adversary creates priori fixed distribution affected maker choosing dynamic nevertheless clear empirical work closely family collectively exploration exploitation amongst adversarial chooses agent past giving agent time step achievable step history step close highest achievable time case farias megiddo governed depend history family considering case characterize dependence mixing time quantity formally also give intuition preliminary markov process agent perceives finite decides probabilistically transition agent receives drawn distribution former hold latter easy signal probability distribution conditioned time probability taking time stationary remainder mentioned assumed stationary unless stated mean probability traversing ergodic step infinity probability fixed limiting unichain stationary ergodic restrict unichain mdps tpath undiscounted undiscounted asymptotic undiscounted limt note ergodic asymptotic hereafter drop explicit dependence unknown fixed agent acting unknown stationary step agent must behalf perceive receive signal goal agent undiscounted close asymptotic best time central agent know mixing time never sure finite step good asymptotic address atease trusting exploration phase texp step recording sort suspicious exploitation phase goto highest batch texp step batch remove goto ltexp step goto goto pseudocode atease stylized facilitates alteration call atease alternating trusting exploration suspicious exploitation proceeds iteration indexed iteration trusting exploration phase followed suspicious exploitation phase trusting exploration phase atease texp step texp fixed regardless disappointment caused regardless poorly fixed time suspicious exploitationphase atease rank exploration phase best batch call texp step long batch much exploration phase atease stop proceeds next best process elimination continued last long exploitation phase eliminated phase repeated duration possibility long mixing time well long pseudocode note ever reach iteration explorationtime mixing time choosing best precisely stochastic bandit unfortunately agent ever know reached thus iteration conceptually like bandit trying amongst unknown mixing time exploration time rejected exploitation minimize mixed atease take confidence parameter parameter rmax show high probability time atease favorably best time mixing time weaker applicable mixing time also mixing time ergodic time smallest maxi stationary ergodic mixing time texp time mixing time atease close high probability formalize farias megiddo parameter rmax atease probability time rmax note mixing time asymptotically best atease compete time asymptotically best quickly atease favorably quickly even much mixing time glance seems imply sample atease case mixing time dependent size mixing time asymptotically best exponential size pointed avoid best mixing time dependence atease entirely unavoidable dependence mixing time empirical illustration applicability atease despite existence sophisticated chooses look best probability chooses difficult beat practice bandit previously slightly practical atease ateasel modification help convergence mixing time step time step text averaged ateasel differs atease exploration time exploitation batch chooses eliminated exploitation phase full iteration exploration time multiplied incremented jump exploration time help expand mixing quickly exploitation batch exploration time iteration suspicious exploitation phase abandoned batch fall next best minus ensures even best mixed continue exploited alternative sake clarity exploitation phase suspicious bandit giving chooses rewarding probabil asymptotic mixing time beginning eventually ateasel surpassed illustrates ateasel namely continues explore lowreturn time hope long case domain long mixing time case much slower convergence time bandit sawtooth seen show clearly alternating exploration exploitation phase choses thus asymptotic quickly choses choses probability probability asymptotic take highlight strength ateasel neither stay long receive good ateasel discovers quickly adopts iteration superior seen exerimentsare highlight strength weakness atease seen atease stochastic bandit problemin mixing time unknown significantly outperform take mixing time robocup soccer keepaway domain delivery domain text transfer learning demonstratethe utility transfer imagine agent applies task mapping mapping combined induces agent able identify optimal mapping advised mapping automatically discovering reasonable mapping deep well outside scope mapping heuristically hand transfer learning agent task learned mapping discard make simpler imagine agent loses sensor agent mapping must educated guess advice richer robocup soccer keepaway robocup ateasel sarsa greedy training time hour training time hour training time hour keepaway show ateasel solid line sarsa dashed line used modified stone robocup keepaway testbed domain simulates team robot keeper taker keeper keep ball taker long multiagent learning simpler fixing agent modification made signal originallyposed step agent next recieved ball case used signal agent recieved episode recieved incidentally reinforcment learning agent learned signal used sarsa linear approximator train agent keepaway episode asked resulting well keepaway mapped keepaway ignoring keeper criterion closest furthest open spectrum keepaway ateasel linear sarsa learning keepaway scratch domain best longest mixing time surprise ateasel best time case able quickly avoid also note unless optimal ateasel never optimal learning eventually surpass ateasel learner spends time poorly transient poor transfer learning avoid ateasel effectively note episodic domain episode unbiased time episode thus thinking time term episode expressed stochastic bandit ateasel pull chooses full episode show expect seems whole ateasel ateasel competitively also delivery domain learning ateasel greedy time step delivery domain averaged comparing ateasel label qxhy represents assumes queue pick queue assumes agent holding delivery domain delivery domain robot deliver queue conveyor belt queue type destined conveyor belt delivered queue producesjobs type probability type destined conveyor belt delivered obstacle open circle move probability agent incurs penalty collides obstacle observable sensor agent nothing switch lane move move pick drop pick agent queue drop appropriateconveyorbelt agent hold tadepalli trained agent reinforcement learning asked used agent lose sensor held queue agent holding imagine fill missing sensor queue queue holding type leaf sensor fill missing sensor take four queue time time agent slightly sophisticated regarding remaining sensor agent carrying piece historical tell agent conveyor belt tried failed drop thus agent attempted drop failed know exactly carrying agent attempted drop must make guess help historical feature queue agent picked holding assumes picking queue type type carrying pick queue assumes contained queue show ateasel scratch fairness historical used learning scratch eventually surpasses ateasel ateasel well quickly worse unlikely long demonstrate conclusion mediating simultaneously competes favorably step mixing time transfer learning induced mapping avoiding transient poor experienced uninformed learner episodic domain mixing time domain mixing time unknown case take mixing time atease significantly outperform acknowledgement erik talvitie funded stiet fellowship michigan satinder funded grant grant darpa ipto opinion conclusion recommendation expressed necessarily reflect view darpa
