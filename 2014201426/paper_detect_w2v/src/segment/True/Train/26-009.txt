investigates alternative estimator learned bootstrap estimator studied synthetic training data foil learning contradict statistic advocate bootstrap superior crossvalidation nevertheless also suggest conclusion machine learning unreliable true learned foil independently drawn varies widely true high variability approximately unbiased bootstrap estimator variability systematically biased induction also classification kononenko bratko prediction statistical literature efron perhaps intensively studied machine learning training classified drawn domain stating invent inspired training classify drawn domain learned used implicitly call true learned used work part foundation award timothy bailey genome predoctoral training grant towell weinstein monte carlo alternative done fitzmaurice sanchez cepeda efron tend inferior evaluating learning novelty investigating variance bias estimator true also examine randomness training lead variance true need able true exactly synthetic data true rate also underlies novelty work investigate correlation estimator true distinguishing downward optimistic upward pessimistic bias novelty learning used foll quinlan rule learning sometimes inductive area foil variant widely used view novelty foil insensitive presence duplicate training learning used rest laid true evaluating learned estimating foil learning synthetic data used conclusion measuring learned induction learning training consisting part vector construct predict training acbailey elkan cording unknown probability distribution classification used criterion goodness true rate probability incorrectly classify drawn distribution training sometimes referred well generalizes learning true efron learning construct prediction rule training prediction rule learned rule formally true rate probability incorrectly classifying expectation reader familiar learning true rate subsumes hypothesis target haussler used theorist framework assumes hypothesis hypothesis target deterministic make learned target framework estimating true must true rate distribution unknown conceptually straightforward draw test training take mean test size test terr unfortunately test estimating infeasible collecting also user learning want constructing classification rule data cheap test used estimating test unattractive distribution much work machine learning distribution assumed deterministtcally dependent subsumes usual case training estimating efron show bootstrap jackknife mathematically bootstrap nonparametric likelihood estimator jackknife quadratic bootstrap form jackknife work efron argues empirically analytically modified bootstrap bootstrap superior focused bootstrap estimator work reserving part training testing learned training subset train subset test subset subset left training used test size training subset singleton training data learning largest basing unseen data intuitively true learned close trying true learned subset training intuitively poorer training bootstrap bootstrap efron estimating creates resample training choosing sample training resamples multisets bootstrap estimator proportion resample misclassified rule learned resample proportion training misclassified rule learned whole training practice averaged many resamples bootstrap reported efron true five bootstrap crossvalidation comprehensive linear discriminant classifier confirm good bootstrap fitzmaurice sanchez cepeda main rion evaluating estimator mean squared estimator bias variance insensitive bias upward downward examine variance bias bias separately bootstrap replaces true distribution nonparametric likelihood empirical probability distribution putting mass sample nearest neighbor classifier bootstrap reported poorly composite suggested kulikowski linear classifier nearest neighbor induction foil tree learning work used learned work cart tree learning bootstrap crawford conclusion bootstrap best framework describes learning data used crossvalidation bootstrap estimator true learned foil quinlan functionfree clause training foil encoded list ground tuples designated target foil learn intensional term foil linkedto extensionally asked find intensional transitive closure foil succeed intensional many encoding classification greatly affect foil foil learning background form used forming foil greedy build clause time clause built literal time trying variabilizations highest gain clause backtracking literal gain remove last literal clause replaces gain synthetic data constructed synthetic data able true exactly data real biology data extracted eukaryotic promoter genetic database work synthetic data contained disjunctive normal form binary contained clause clause majority noise irrelevant used distribution training mixture distribution probability word chose probability binary training process repeated desired monte carlo simulation synthetic data foil true rate crossvalidation bootstrap brief discovered bootstrap poorly learning target bootstrap variance crossvalidation poor correlation strongly biased data target dnfl procedure distribution foil learning limitation plotted dnfl qualitatively computing accomplished exhaustively comparing learned true relevant relevant mentioned true learned relevant exhaustively prohibitive scatter plot procedure repeated time distribution size training bailey elkan true scatter plot line crossvalidation bootstrap sample gave combined scatter plot bootstrap high variance estimator also apparent quantity trying high variance well foil learns perfectly training size learns high training size fact make correlation estimator quantity correlation bootstrap close zero correlation much around note leastsquares regression estimator give line intercept nonzero even target learned perfectly show mean sample deviation bootstrap scatter plot clearly bootstrap biased upward four bias estimator hand variance bootstrap much training learning curve foil dnfl considering bootstrap efron scatter plot show pitfall estimating outlier scatter plot show training greatly overor true dangerous learned solely bias relatively high correlation good estimator learning curve behind poor bootstrap training size conducted construct learning curve foil four target learning curve consisted repeating procedure time training size distribution mean bootstrap repeated size training plotted dnfl show plus minus sample deviation quantity clarity seen well training size estimating mean training size bias hand bias bootstrap downward training upward training bootstrap estimator reported variance confirms sample deviation bootstrap case dnfl bootstrap tends flat wide training size target bootstrap sample deviation training size foil learning learning curve foil dnfl bootstrap plotted sample size training consequently bootstrap thus lose main explaining failure bootstrap poor bootstrap surprising view statistical literature fitzmaurice efron work bootstrap training data multivariate normal best rule classifying data nonzero true sufficient belongs data came perfectly discriminated bootstrap mentioned previously erroreb sample left resampling resubstitution foil tends learn training thus resubstitution tends close zero bootstrap noticeable good sample seen plot sample axis bootstrap resampling training multisets distinct resample time size explained foil learns bootstrap resampled multiset removing duplicate resample show learning curve foil resamples duplicate removed confirm suspicion poor bootstrap used foil probability dataset resampling approximately fact foil benefit duplicate multiset training learned foil duplicate seen fact curve worth noting classification bootstrap reported poorly learning duplicate training influence learning bootstrap reported work well notably fisher linear discriminant used fitzmaurice efron cart tree used crawford strongly influenced duplicate studied bootstrap estimator learned synthetic data foil true learned high variance learned foil varies widely bootstrap estimator variance biased bias upward downward high variance approximately unbiased implication work comparing learning also applies foil statistical test invalid deciding significantly data concern bailey elkan foil learns poorly multisets training multiset resamples resamples duplicate removed learning data weinstein backpropagation neural network hidden unit fisher linear discriminant data data incorrect hidden unit linear discriminant statistical test test backpropagation neural network hidden unit significantly fisher linear discriminant null hypothesis test learned binomial trial probability success extra randomness training used chance backpropagation well linear discriminant poorly despite data well indistinguishable data identically distributed data disappointing work concluded bootstrap poor bootstrap caused fact foil learns multiset work learning bootstrap unless learning tested sensitive repeated training despite caution listed lead recommend continuing evaluating learning recommend perfect correlation experimenter must keep mind unfortunately estimator learned best learning used
