substantial devoted subspace learning tensor dater tensor subspace vital unsolved computational convergency guaranteed work novel procedure subspace learning followed convergency matrix extensive realworld database high convergence procedure well superiority classification procedure subspace learning brand turk pentland linear discriminant belhumeur traditionally data vector feature real extracted feature form multidimensional tensor vectorization process destroys intrinsic tensor form drawback brought vectorization process curse dimensionality greatly degrade algorithmic learnability sample size case substantial devoted employment tensor improving algorithmic learnability vasilescu terzopoulos dater tensorized higherorder optimization procedure used collective encountered procedure procedure guaranteed converge iteration optimization approximately simplified trace ratio form argmax ratio trace form argmaxuk uktskuk uktskpuk iteration consequently derived matrix unnecessary converge greatly unclear iteration optimal even sense work graph embedding dimensionality reduction procedure subspace learning tensor iteration transforming ratio trace form transform trace ratio optimization trace optimization maxuk objectivefunction iteration iteration efficiently eigenvalue fukunaga justify namely monotonously also matrix converge fixed hogan worthwhile highlight procedure subspace learning tensor guaranteed tomonotonously matrix proved converge property algorithmic effectiveness applicability eigenvalue foriterative optimization make whole suffer singularity encountered traditionalgeneralized eigenvaluedecomposition used ratio trace optimization consequent brought sound foundationis enhanced potentialclassification derived subspace learning rest review subspace learning tensor procedure convergency taking marginal fisher convergency property procedure classification derived lowdimensional examined database subspace learning tensor data subspace learning framework encoding data tensor arbitrary taking data vector tensor production production matrix unfolding referred work graph embedding tensor sample sample undirected graph intrinsic graph vertex matrix diagonal matrix laplacian matrix graph task graph embedding lowdimensional vertex preserve pair data highdimensional feature embedding vertex embedding vertex production series column orthogonal matrix anidentity matrix maintain vertex pair graph criterion argmin argmin pose extra graph criterion mean part work havetwo kind scale normalization diagonal matrix relies graph referred penalty graph matrix losing generality penalty matrix simplicity scale normalization deduction procedure subspace learning expressed shashua levin dimensionality reduction data encoded tensor outperform data vector training sample representing matrix vector correlation column exploited subspace learning work utilized procedure matrix initialized arbitrarily matrix refined fixing matrix optimization argmax uktskpuk argmaxuk uktskuk unfolding matrix tensor optimization intractable traditionally approximated transforming tractable form namely ratio trace form argmaxk uktskuk uktskpuk eigenvalue distortion lead computational subsection computational objectivefunction iteration trace ratio form ratio trace form deduced neither monotonously converge fixed work convergent procedure optimization procedure convergency procedure subspace learning give convergency mentioned optimization optimization also iteration refine matrix fixing refinement ratio trace optimization transform trace ratio optimization trace optimization argmax matrix iteration proceduremay converge optimum optimization monotonously proved lead superiority ratio trace optimization procedure latter unnecessarily optimal iteratively refine matrix procedure subspace learning listed monotonous property rewrite term procedure subspace learning initialization arbitrary column orthogonal matrix optimization tmax matrix conduct eigenvalue eigenvector largest eigenvalue reshape sake orthogonal transformation invariance unfolding tensor conduct eigenvalue svui column vector matrix leading eigenvectors namely work break matrix easy matrix itive semidefinite zero eigenvalue utktskutk zero eigenvalue iteration usps database database database mean procedure ratio trace optimization monotonously convergency convergency matrix need collection subset procedure subspace learning orthogonal transformation strict monotony generates rule continuous strict monotony imply orthogonal matrix data rithm proved strictly monotonic strictly monotonic strict monotony satisfied take also satisfied orthogonal kind orthogonal transformation normalized reshaping step also satisfied strictly monotonic meyer strictly monotonic generates compact normed conclusion converge optimum compact norm systematically examine convergency property procedure subspace learning take marginal fisher subspace learning superior many subspace learning linear discriminant referred evaluatethe classification derived procedure procedure matrix tensor used matrix transformed correspondingvector data step matrix iteration usps database database data used usps handwritten dataset handwritten digit pixel ranging benchmark face database face database affine transform sample mouth database person normalized size pixel pose illumination database facial subset five near frontal pose database illumination indexed used normalized size monotony subsection examine monotony property procedure optimization procedure transforms ratio trace form usps database used ratio trace procedure converge procedure monotonous procedure converge iteration final converged procedure much iteration ratio trace procedure convergency matrix convergencyproperty ratio trace optimization procedure calculate norm matrix successive iteration displayed demonstrates matrix converge iteration procedure procedure oscillation converge rate sensitive oscillation caused unconvergent matrix classification degraded dramatically rate database tmfa tmfa rate database tmfa tmfa face iteration iteration rate iteration database database subsection conduct classification benchmark face database tensor marginal fisher procedure tmfa ratio trace tensor marginal fisher tmfa ratio trace trace ratio mfatr mean conduct training conducted preprocess step dimension sample case fisherface belhumeur graph configuration nearest neighbor intrinsic graph penalty graph adopted tensor subspace learning converge terminate process iteration classification feature dimensionality reduction also reported baseline nearest neighbor used final classification dimension final evaluated best reported database test configuration training testing sake statistical confidence asfor subject training remaining subject used testing listed tmfa mostly outperforms concerned work case database trace ratio consistently superior ratio trace subspace learning tensor theclassification trace ratio ratio trace subspace learning conclusion novel procedure optimize subspace learning tensor convergence matrix monotony property proven best work give convergent subspace learning acknowledgement work grant grant council hong kong administrative
