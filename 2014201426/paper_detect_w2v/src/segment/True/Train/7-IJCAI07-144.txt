framework reinforcement learning agent build skill learned agent used task learning agentspace feature retains semantics successive problemspace reused task demonstrating transferrable skill show best used much reinforcement learning focused hierarchical barto mahadevan framework sutton integrates reinforcementlearning framework learning work agent time lead learning task learned reused task distinct learning portable markov task hand markov retained successive task possibly size dimensionality konidaris barto learned reused semantics remain consistent task showing learned significantly distinct task best used background framework probability distribution initiation indicator elsewhere termination giving probability terminating sutton framework learning planning temporally reinforcement learning framework sutton barto learning must determining expand initiation termination learn learning reinforcement learning agent many simultaneously taking sutton creation termination identification goal reach goal terminate initiation goal reachable goal varierty frequency hengst graph partitioning salience focused extracting exploiting commonality collection thrun schwartz bernstein perkins precup pickett barto learn agent reinforcement learning thus reused jonsson barto hengst automatic subset learning explicit transformation ravindran barto task concerned agent distinct task agent experience physic type sensation receives agent creates descriptor sufficient distinguish markov induces markov process fixed agent transition probability depend agent agent thus work transition probability task call agent also feature consistently retain semantics task task call stem differentrepresentationalrequirements markov task potentially commonality task thus task distinct reinforcement learning task bernstein perkins precup pickett barto thrun schwartz task must task remain consistent task distinct konidaris barto used distinction learn shaping learning task task robot equipped laser finder reach target room finder noisy nonmarkov robot build explores thus forming finder form agent consistent robot eventually learn like moving nearest door solely sensation agent referencing used learning robot encounter agent view task consisting descriptor sufficient distinguish agentspace descriptor goal reinforcement learning task find maximizes agent also learns higherlevel reduce time task portable task form descriptor task form descriptor agent learning task type simultaneously agent receives descriptor lightworld domain agent environmentconsisting room room containing locked door lock possibly leave room agent must unlock door step unlock door must move lock room agent must holding successfully unlock door agent moving picking agent receives leaving door final room step penalty four grid pickup deterministic unsuccessful moving wall equip agent twelve sensor grouped side sensor triplet detects blue sensor responds side agent ranging away open door emit floor held agent emit lock emit blue show five piece data form lightworld room agent room agent door open sensor semantics consistent lightworld case continuous dimension type agent used five type reinforcement learning agent agent agent agent perfect agent agent type lightworld agent used sarsa learn stateaction pair agent initially unlearned salient picking unlocking lock walking door learned used parameter agent used offpolicy precup learning completed successfully used discount room exceeds threshold learned must relearned lightworld agent perfect prelearned salient identical agent agent perfectly transferrable learned agent learned learned agent employed picking going open door unlocking door twelve sensor sensor continuous used linear gradient agent gave upon used step penalty discount exceeded threshold learned lightworld agent type agent learn portable skill simultaneously note agent used discrete task descriptor markov room lightworlds lightworlds consisting room width height cell door lock room room resulted approximately pair evaluated agent type lightworlds sample lightworld agent gained experience sample sample agent training training experience training experience sample lightworld consisted episode training lightworld remaining agent sample lightworld discarded next training experience never training lightworld show learning curve agent employing show agent employing time agent encounter lightworld agent evidenced topmost learning curve rapidly improves experience lightworlds experiencing training lightworld agent much shallower learning curve agent problemspace experience learning curve agent perfect curve even never trained lightworld tested show successfully lightworld show learning curve agent employing type time agent encounter lightworld well agent highest curve thereafter rapidly agent agentspace experience nearly well agent perfect conjecture agentspace much learn scratch learning curve agent learning curve agent varying training experience learning curve agent varying training experience explains agent training experience like agent like agent learned subgoals learned must approximated slightly subgoal explains agent type long agent show mean step step episode agent learned perfect training experience dark type training experience episode agent perfect type experience training rapidly drop step nearly agent perfect also clearly show agent type consistently note decrease experience indicating consistent transfer conveyor belt domain conveyorbelt system must move feeder type triangle type issued time feeder must directed dropping type decrement feeder opposing conveyorbelt belt pair fixed system conveyor belt move step belt move move connection penalty dropped spare case conveyor belt system conveyor belt system camera track indicating unit connector belt camera conveyorbelt retains semantics agentspace discrete relatively learn inability distinguish belt used descriptor conveyor belt belt belt technically omit good belt interconnection resulting experimentswhere agent learned move belt moving belt moving belt used agent type show learning curve agent employing perfectoptions agent employing agent employing type show agent experience initially quickly eventually agent problemspace training experience roughly curve agent training experience never good perfect probably camera fact locally markov even subgoals show agent type experience outperform able generalise belt show mean type agent agent eventually outperform agent even much morelimited agent type consistently outperform agent type eventually performanceof agent closely deictic agre chapman whereobjects view agent frame expect robotics egocentric manipulation task objectcentric involving spatial expect learning curve agent learning curve agent varying training experience learning curve agent type varying training experience closely allocentric egocentric guazzelli also expect learning actually harder problemspace case learning type simultaneously learning experience simultaneously learn portable skill skill bootstrap descriptor reinforcement learning framework introduces problemsimilar episode agent learned perfect training experience dark type training experience additionally learning descriptor ideally markov agentspace descriptor form affect learned learning care learning shaping konidaris barto note learning skill framework idea used hierarchical reinforcement learning framework maxq dietterich hierarchy machine parr russell conclusion introduced learning portable skill reinforcement learning agent show successfully task significantly task acknowledgement thank sarah osentoski ozgur aron culotta ashvin shah chris vigorito ferguson andrew stout khashayar rohanimanesh pippin wolfe gene novark comment assistance andrew barto george konidaris part foundation grant andrew barto part subcontract rutgers award darpa
