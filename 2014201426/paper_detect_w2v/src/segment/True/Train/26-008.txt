nearest neighbor induction studied many assumes conjunctive target boolean distribution calculate probability encounter test prototype probability nearest stored training case test probability classification training case relevant irrelevant also explore behavioral implication presenting predicted learning curve artificial domain give domain reasoning nearest neighbor learning form experience memory explored wide tree quinlan multilayer network rumelhart mcclelland probabilistic fisher year growing case memory many name learning reasoning many task simplest widely studied nearest neighbor originated hart dasarathy applies classification task learning trivial simply training memory come retrieval process test find stored training case nearest note retrieved case predicts many variant stanfill waltz studied retrieves closest base nearest neighbor wayne branch mail stop nasa ames moffett vote incorporating stored test case referred neighbor hart kibler albert studied alternative case memory upon thus memory load retrieval time little reduction like learning intriguing tighter relate decided pursue line hirschberg pazzani induction langley thompson probabilistic sake tractability focused training case base prediction nearest neighbor simplicity mean lack quinlan sophisticated inducing tree four natural domain cleveland hungarian involving prediction heart disease symptom concerning diagnosis tumor fourth involving prediction affiliation congress voting domain trained approximately case tested remaining averaging cleveland data indistinguishable tumor voting nearly reached nearest neighbor fared much worse hungarian data modification comparable domain argues deserves closer inspection case remainder nearest neighbor presenting langley machine learning computing probability classifying test case prototype stored training case depicts probability show probability central probability term contribution nearest stored training nearest farthest corresponds central conflict classification process retrieve training case test case irrelevant located step away prototype test case prototype nearest stored case step away prototype toward step away prototype away somewhere also anywhere middle band open possibility handle must nearest stored case must multiply probability occurring formally decompose term probability exactly training case test case remaining case nearest giving formal stored case must case multiplying subaccuracy probability occurrence give selects training case term represents probability step away test case turn step away prototype caexpand term probability training case fall test probability training seen satisfies turn test case training simpler test case nontrivial guaranteed classify test case nearest stored training test case anywhere giving langley note must numerator dealing test case must take computing probability step away test case case zero term predict nearest neighbor training relevant irrelevant behavioral implication formal characterization nearest neighbor implication obvious domain systematically varied domain parameter examined predicted computing prediction also collected learning curve summarized datum curve classification averaged training distributed case mean confidence show degree predicted learning curve reasoning identified show relevant conjunctive target held irrelevant varied training relevant learning curve gradually improves encounter training target also make sense relevant feature machine learning predictive nearest neighbor conjunctive presence irrelevant training relevant line learning curve giving early induction process also requiring training case reach asymptote producing crossover learning rate seems degrade gracefully learning curve close agreement lends confidence sensitivity nearest neighbor irrelevant dramatic graph summarizes training irrelevant held relevant examined parameter degradation learning rate graceful realizes reduce proportion relevant consistent sensitivity nearest neighbor irrelevant also nearest neighbor induction pazzani sarrett studied wholist initializes feature training remove feature fails langley thompson analyzed bayesian classifier probabilistic predictive nearest neighbor conjunctive presence relevant training irrelevant line learning curve base rate probability pazzani sarrett note whollst learning rate unaffected relevant clearly scale dimension nearest neighbor bayesian classifier difficult langley examined probability assumes differ domain learning relies handle many irrelevant feature many relevant vein analytically training wholist logarithm irrelevant derived analytic nearest neighbor probabilistic scale dimension graph predicted training irrelevant target involving relevant feature distribution quantity interpolate learning curve reveals bayesian classifier scale well irrelevant dependent growing linear training nearest neighbor grows much precise superlinear also consistent conclusion response many irrelevant feature training interpolated reach nearest neighbor bayesian classifier conjunctive presence relevant irrelevant entirely fair neither bayesian classifier handle disjunctive obstacle even simplest nearest neighbor conjunctive obscured strength also variant nearest neighbor retains statistic usefulness fare domain many irrelevant term nevertheless make type careful formal insight relative strength learning nearest neighbor treatment assumes target conjunctive free noise boolean distributed relevant irrelevant classification training explore implication plotted predicted graceful degradation relevants irrelevants increased stronger artificial domain predicted closely correcting reasoning empirical revealed langley wide natural domain full explanation must incorporate influence noise done langley langley must also handle probability distribution hirschberg pazzani even must extend framework handle broader target nearest neighbor well suited feature prototype sufficient membership prototype play central role believe extending handle feasible many training memory acquire disjunctive prototype hope framework handle also generalize neighbor lead turney work onto natural domain distribution data noise target informed guess learning curve predicted extend applicability artificial domain test date believe insight nearest neighbor favorably induction domain consistent intuition sensitivity irrelevant also believe framework handle challenging target complicate learning task thus solid base learning acknowledgement like thank stephanie sage david helped clarify idea david reviewer comment nilsson encouragement machine learning contributed work stanford learning expertise
