laplacian eigenfunctions automatically construct mdps show success explained drawing connection spectrum laplacian explanation help identify precisely good modification laplacian derive analytical show augmented krylov used sparse linear system empirically demonstrate construction augmented krylov significantly outperform laplacian term markov process puterman framework planning uncertainty discounted infinite horizon discount also finite time many practical precisely motivated mdps sparse structured mdps bertsekas tsitsiklis linear analyze linear scheme linear vector optimal calculated iteration linear bertsekas tsitsiklis play role used insight topological property sutton barto framework automatically constructing mahadevan neighborhood inspired spectral used task framework vector matrix identity matrix size property many guaranteed converge approximately optimal iteration maximal optimal optimal munos true approximated step norm quadratic norm distribution distribution arbitrary transition matrix munos hold bellman residual whenis replaced transition matrix hold also norm bertsekas tsitsiklis refer main construction good minimizein iteration quadratic describes spectral main contribution explanation good spectral connection sparse linear system also alternative show demonstrate significantly outperform previously spectral mahadevan spectral graph framework construct linear topology fixed node undirected graph graph symmetric adjacency matrix represents node diagonal matrix node degree graph laplacian normalized laplacian normalized laplacian symmetric thus eigenvectors orthogonal closely walk laplacian combinatorial laplacian used motivate laplacian connection walk graph chung graph mapping vertex smoothness graph sobolev norm bottom eigenvectors seen good smooth sobolev norm chung spectral framework straightforward seen graph node correspond edge node probability transiting fixed transition must symmetric transition probability smooth induced graph spectral graph framework lead good adjacency matrix edge used mahadevan also scheme suffers opinion good sufficiently well explained construction adjacency matrix well motivated construction adjacency matrix make hard analyze show transition matrix adjacency matrix lead motivated easy derive analyze smooth partially resolved diffusion wavelet mahadevan maggioni maggioni mahadevan briefly diffusion wavelet construct inverse matrix disadvantage wavelet high computational overhead construct inverse inverse constructed reused long transition matrix fixed thus diffusion wavelet goal computational show transition matrix walk laplacian adjacency matrix well justified analyzed transition matrix reasonable past transition symmetric adjacency matrix eigenvectors eigenvalue eigenvalue spectral transition matrix fixed markov puterman equality neumann series fact synchronous backup modified iteration calculate series term iteration transition matrix diagonalizable matrix eigenvectors eigenvalue loss generality matrix diagonalizable linearly decompose pixj ijxj considering subset eigenvectors lead well approximated considering greatest best minimize eigenvectors high identical taking eigenvectors walk laplacian spectral framework transition matrix used subsection krylov also base besides eigenvectors vector neumann series vector calculating progressively vector series done modified iteration potentially infinite iteration linear linear vector need linearly vector preferable calculate interestingly need degree ipsen meyer even taking fewer vector good many case show vector sufficient precisely algebraic rigorous derivation ipsen meyer golub loan spanned krylov subspace ously used gmres lancoz arnoldi golub loan also combine eigenvectors krylov augmented krylov saad actually subsume simply considering largest eigenvectors subsection constructing good deal arbitrarily incorporated iteration refer spectral form eigenvectors transition matrix mahadevan laplacians vector greatest choosing largest lead practical face major obstacle eigenvector efficiently calculate eigenvectors regard sparse matrix transition matrix diagonalizable need calculate unstable eigenvectors eigenvalue eigenvectors vector real eigenvectors ifthen augmented krylov construction revision iteration resolved viable alternative vector augmented krylov combine vector krylov eigenvectors pseudocode calculates orthonormal augmented krylov modified krylov eliminates nondiagonalizable transition matrix eigenvectors combine take property intuitively krylov vector capture eigenvectors capture reliable rule augmenting eigenvectors preferable keep relatively vector krylov briefly guaranteed augmented krylov characterizes constructed transition matrix vector bounding quadratic norm also norm show applies minimize bellman residual bertsekas tsitsiklis also neumann series inverse krylov vector eigenvectors also ellipse focal major constructed diagonalizable matrix diagonal matrix eigenvalue eigenvectors chebyshev kind degree parameter eigenvalue eigenvalue angle subspace eigenvectors perpendicular symmetric simplify show krylov saad ignoring eigenvectors case find minimizes defines multiplied degree satisfies expressed minimizes golub loan eigenvalue practical chebyshev saad ellipse eigenvalue eigenvectors expressed matrix note perpendicular chebyshev matrixvalued golub loan show subspace corresponds eigenvalue also show highest degree perpendicular remaining eigenvectors also decrease size ellipse eigenvalue demonstrate used mahadevan mahadevan maggioni stochastic planning encountered grid doorway middle wall move four main probability taking size room cell size intentionally make explicit walk laplacian identical eigenvectors transition matrix choosing eigenvectors spectral used vector discount transition vector synthetically constructed show disadvantage projected onto grid vector represents arbitrary smooth vector made perpendicular eigenvectors walk laplacian discount rate favorable krylov closer also seen shrink eigenvalue favorable krylov high discount rate mean squared regard vector calculated equiprobable true evaluating true need used vector projected onto grid mean squared discount axis scale laplacian eigenvectors walk laplacian adjacency matrix mahadevan normalized laplacian practically identical walk laplacian spectral subsection determines optimal eigenvectors used krylov augmented krylov eigenvectors subsection augmented krylov eigenvectors eigenvectors domain intentionally violates smoothness easiest laplacian case spectral walk laplacian outperform ordinary krylov vector vector krylov augmented krylov significantly outperform suggest krylov superior eigenvectors laplacian many sparse linear system saad well construction also constructing krylov constructing eigenvectors matlab took calculate eigenvectors took calculate vector krylov alternative explanation success laplacian used explanation precisely work show closely augmented krylov demonstrated constructed augmented krylov superior constructed eigenvectors spectral augmented krylov smooth calculating vector krylov cheaper calculating eigenvectors compression partially observable mdps pomdp also krylov poupart boutilier pomdps mdps practical mdps implicitly assumes true mdps mean squared discount axis scale address enumerated factored mdps vector krylov whole enumerated line poupart boutilier reinforcement learning need transition matrix sampling sampled addressed eigenvectors mahadevan thus augmenting krylov eigenvectors also focused mainly fixed considering control part combined arbitrarily suggest exploring connection construction sparse linear bring acknowledgement foundation grant also thank sridhar mahadevan hala mostafa shlomo zilberstein reviewer valuable comment
