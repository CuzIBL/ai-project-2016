employed multilevel hierarchical bayesian task exploiting relevant high cardinality classification overfitting calculate probability combining training probability recursively strictly generic achieved bayesian network like naive bayes tree augmented naive bayes bayesian network probability substituted gate default tree graph bayesian network constructed cardinality reduction preprocessing phase agglomerative bottleneck country imported good must declared importer belong custom code good classified custom code mean custom duty also administrative sanitary safety goal tool considering four declared custom code importer country production receiving country probability misclassification used latter system allocates resource type operation data classification call misclassification dataset distinct fourth dataset imbalanced handled resampling chawla resampling retraining classifier assignment cost false false cost priority acording vary false cost thus train classifier cost assignment hand reliable probability dataset work resource allocation system much time rate match resource task detecting wrong custom code considering antifraud moment verified naturally misclassification allocation system also combine probability cost vary retraining also unnecessary custom administration cost criterion thus decided concentrate bayesian resampling retraining cost domain specialist involving make probability significantly looking separately call benefit like bayesian network pearl node parent node call considering node parent node vector contained node assessed frequency training distribution parent assumed dirichlet probability distribution adopted applying bayes rule integrating njki simultaneous training parameter dirichlet probability distribution sample size probability distribution dirichlet probability distribution assumed noninformative yield parameter dirichlet distribution smoothing andmj numberof node call node node node parent probability node parameter clear rarely seen tends unreliable probability dominated noninformative probability distribution suggests cpts overfitting network lead achieved limiting parent network node naive bayes duda hart parent node parent node tree augmented naive bayes friedman tree naif bayes connecting nonclass thus parent node limiting parent also capture benefit thus prefer high cardinality creating trouble reasonable idea preprocess data cardinality agglomerative bottleneck aibn slonim tishby task process cardinality blind thus unlikely cardinality reduction capture depend probability size training fill probability cpts satisfactorily pearl recommends adoption resort causal independence like gate parameter probability distribution node parent proportional cardinality parent proportional cardinality causal independence incompatible goal capturing flexible probability distribution node parent like default friedman goldszmidt tree friedman goldszmidt graph chickering friedman goldszmidt adequate learning procedure induces emulate real data resulting network tend term fewer parameter fewer parameter overfitting hand cpts probability distribution node parent distribution actually identical reflect parameter proportional actually distribution distribution node parent probability distribution node parent identical neither hold gelman asserted modeling hierarchical data nonhierarchically lead poor parameter nonhierarchical data accurately many parameter data well lead inferior prediction data word overfit hierarchical data well overfitting reflect distribution equality observing slight modification used friedman smoothing schema data used node parent hierarchical defines sample size probability distribution call probability distribution assessed wider population build informative probability distribution narrower population hierarchical also used cestnik consequence noninformative dirichlet probability distribution adopting dirichlet probability distribution closer true probability distribution discrimination significantly linear closer true probability distribution counterpart thus discrimination jump population training population whole training call hierarchical bayes move slowly population benefiting discrimination hierarchical bayes classifier employ hierarchy combine influence dominate well combine independence calibration work classification nominal training pair label calculates pair form alue said undefined missing generic generic satisfies strictly generic strictly generic hierarchical calculates probability probability parameter dirichlet probability distribution used smoothing coefficient consequently training satisfying training satisfying label calculate idea belongingto calculate recursively subset influence influence captured recursive process step evaluated recursively bayes joint probability marginal probability calibration calibration coefficient need calculate applying bayes likelihood training belonging training happens zero case zero know substitute side able clear identical need worry done recursively recursion empty case calibration spite independence naive bayes know well many domain misclassification rate domingo pazzani naive bayes also know unbalanced probability sense close zero close obtaining probability distribution calibration compensate overly confident prediction naive bayes bennett zadrozny stronger independence naive bayes naive bayes assumes assumes aggregation many aggregation know false main consequence stronger unrealistic even probability naive bayes compensated calibration calibration analogous used zadrozny calibration tree probability selecting coefficient specification coefficient classification optimal case coefficient heuristic motivated fact aggregation probability stronger must calibration thus made coefficient case coefficient employ greed optimization family move toward family containing exactly possibly coefficient fixed family generic family coefficient need specified calculate belonging coefficient leave cross validation maximize area curve induced calculate training classification tested weka experimenter tool witten frank fold cross validation classifier built naive bayes cpts smoothing schema friedman cpts cpts trained dataset cardinality reduction aibn previously binary merges enabled binary enabled enabled merges enabled tree learned hill climbing scoring default learned trivial classifier assigns probability involving constructed chickering involving constructed friedman goldszmidt scoring tried parameterizations sticked parameter best best mean best area curve rate axis chose recall absolute form curve make interpretationeasier rate scale emphasizethe beginning curve besides curve probability distribution distribution actually test root mean squared rmse mean cross entropy show parameter explained smoothing coefficient employed automatically optimized optimization leave cross validation take absolutely training fold cross validation varies training eliminating possibility fitting test coefficient heuristic varied enumeration sticked produced best fold cross validation process avoid fine tuning reshuffled data fold cross validation process came process rate recall avoid pollution curve subset tested optimization parameter smoothing stan aibn smoothing saibn mutual varying stan enumeration saibn enumeration enumeration best triple besttriple stan saibn whole grid observe abrupt curve covered neighbor besttriplewe believe exhaustivecoveringwas bndg bndg bndg bndg recall rate rmse parameterization besttriple infinity bndg bndg bndg bndg area curve probability optimal parametrization parameter optimization smoothing case exhaustively varied enumeration enumeration magnitude smoothing time avoids fine tuning naive bayes unable capture explore explains beginning curve tree augmented naive bayes explores naive bayes benefit involving many decisive rate applying cardinality reduction constructing lead curve substituting cpts bayesian network tree default graph binary disabled made curve worse learned default learned tree graph consequence case resulting little discrimination graph binary enabled separated resulted beginning curve left resulted loss discrimination rate hypothesis test show significantly classifier regard rmse also hypothesis test rate step significantly classifier rate significantly aibn bndg bndg benefit involving many even also considers influence consequence well rate conclusion domain preselection imported good verification constitute influence probability high cardinality domain exploiting overfitting challenging addressed novel classification multilevel hierarchical bayesian capable capturing influence overfitting loosing discrimination exhaustion test resulted curve unambiguously also produced probability validated specialized domain reflect particularity domain characterized high cardinality relevant thus good domain good domain training time exponential linear training cardinality thus applicable domain domain much training time cardinality full bayesian sense gelman parameter assumed drawn distribution probability distribution done considering evidence probability distribution population probability distribution population smoothing used cestnik friedman sense gelman empirical hierarchical bayesian widely used marketing name hierarchical bayes allenby lenk also used medical domain andreassen robotics stewart aware hierarchical bayesian employed handle high cardinality relevant classification make relevant differs dealing multi hierarchy recursively also handling fact contained overlapping literature friedman goldszmidt expect bayesian network default tree graph emulate real data serious overfitting contribution show spite motivation actually simpler like practical domain truly preprocessing data cardinality agglomerative bottleneck curve selecting smoothing calibration coefficient simplistic leave work
