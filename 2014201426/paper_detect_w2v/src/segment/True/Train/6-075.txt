ensemble like bagging boosting combine hypothesis strongest machine learning diversity ensemble determining generating ensemble construct diverse hypothesis training metalearner learner base classifier build diverse committee induction base learner demonstrate consistently achieves predictive base classifier bagging boosting occasionally decrease also obtains boosting early learning curve training data major inductive learning past ensemble committee learn retain hypothesis combine classification dietterich boosting freund schapire ensemble learns series weak classifier focusing correcting made best generic inductive classification hastie constructing diverse committee hypothesis decorrelated ensemble maintaining consistency training data theoretically property good committee krogh vedelsby successful ensemble encourage diversity extent focused goal maximizing diversity achieving diversity opitz shavlik rosen fairly like bagging breiman boosting base learner committee witten frank decorate diverse ensemble creation oppositional relabeling artificial training learner high training data build diverse committee fairly straightforward accomplished constructed training committee artificially constructed label disagree committee thereby diversity classifier trained augmented data committee boosting bagging diversity training training ensemble diversity decorate ensures diversity arbitrarily artificial hypothesis training wide data comparing boosting bagging decorate induction java quinlan introduced witten frank base learner learning curve hypothesis decorated tree classification training ensemble diversity ensemble classifier disagree krogh vedelsby refer disagreement diversity ensemble ensemble diversity kuncheva whitaker dependent regression mean squared used variance used diversity diversity classifier learning prediction classifier ensemble krogh show ensemble expressed mean diversity ensemble classification loss used diversity classifier case linear relationship hold believe diversity decrease ensemble zenobi cunningham build ensemble classifier consistent training data maximize diversity decorate decorate ensemble iteratively learning classifier iteration ensemble ensemble classifier trained training data classifier successive iteration trained training data also artificial data iteration artificial training data distribution specified fraction training size label artificially training differ maximally ensemble prediction construction artificial data explained refer labeled artificially training diversity data train classifier training data diversity data classifier ensemble ensemble training reject classifier else ensemble process repeated reach desired committee size exceed iteration classify unlabeled employ base classifier ensemble probability membership probability belonging classifier membership probability ensemble probability belonging probable label construction artificial data artificial training data picking data distribution numeric mean deviation training gaussian distribution nominal probability occurrence distinct domain distribution laplace smoothing nominal training probability occurrence constructing artificial data make simplifying accurately joint probability distribution time consuming data iteration artificially labeled ensemble find membership probability predicted ensemble replacing zero probability label probability inversely proportional ensemble prediction learning decorate data repository blake merz used webb quinlan decorate adaboost bagging base learner ensemble weka witten frank ensemble ensemble size note case decorate ensemble size terminates iteration exceeds even desired ensemble size reached iteration decorate varying artificially data rsize vary much rsize adversely affect decorate insufficient artificial data give rise high diversity rsize artificially training size learning evaluated data equalsize segment averaged trial trial segment aside testing remaining data training test varying training data learning curve testing system training subset training data like summarize data size percentage size learning curve learning domain employ statistic used webb namely geometric mean ratio data worse classification also statistically loss counted paired geometric mean ratio mean domain geometric mean ratio vice versa ratio capture degree loss outcome summarized cell decorate statistically bold varied training size data learning curve expect bottom statistic learning curve decorate loss bagging learning curve decorate also outperforms bagging geometric mean ratio suggests even case bagging beat decorate decorate bagging rest case decorate outperforms adaboost early learning curve geometric mean ratio reversed data note even training data decorate competitive adaboost data decorate data webb bauer kohavi adaboost significantly reduces base learner occasionally extent decorate clear many data decorate achieves bagging adaboost many fewer training show learning curve clearly demonstrate domain little data acquiring label decorate ensemble analyze role diversity play reduction decorate rsize ranging thus varying diversity ensemble produced diversity ensemble reduction diversity ensemble mean diversity ensemble ensemble diversity ensemble reduction ensemble ensemble cunningham carney correlation coefficient diversity ensemble reduction fairly diversity base reduction base classifier ensemble base reduction give indication ensemble base classifier correlation diversity base reduction note even correlation weak statistically correlation reinforce belief ensemble diversity good decorate ensemble size size training data decorate noticeable learning curve lack datasets five datasets datasets note decorate ensemble size datasets ensemble size probability getting correlation chance true correlation zero learning decorate adaboost bagging work ensemble diversity rosen simultaneously train neural network ensemble correlation penalty term opitz shavlik genetic good ensemble network guide incorporates diversity term zenobi build ensemble feature subset feature done classifier diversity classifier rejected lead substantial deterioration substantial threshold ensemble built attempting simultaneously optimize diversity ensemble decorate goal minimize ensemble diversity training ensemble learning base classifier possibility none ensemble boosting bagging fopitz shavlik bagging boosting boosting weak base learner training data boosting terminates construct hypothesis zero training decorate learner artificial diversity training data prevent adequately fitting real data applying boosting base learner must appropriately weakened benefit boosting decorate preferable ensemble learner ensemble utilize artificial training data learning introduced icohn goal committee good training training data also label artificial hypothesis faithfully diversity cohn label artificial data encourage learning hypothesis work conclusion encouraging diversity artificial training many domain unlabeled data unlabeled label diversity data decorate form semisupervised learning labeled unlabeled data nigam used base learner expect good base learner induction used base learner ensemble work neural network naive bayes bauer kohavi opitz maclin decorating learner area work learning manipulating artificial training decorate able base learner diverse ensemble demonstrate particularly producing highly accurate ensemble training data outperforming bagging boosting learning curve empirical success decorate raise developing sound effectiveness idea artificial unlabeled construction ensemble seems promising worthy acknowledgment work darpa eeld grant
