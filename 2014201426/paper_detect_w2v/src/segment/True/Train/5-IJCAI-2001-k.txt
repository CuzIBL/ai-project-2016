agent learn concurrently payoff agent dependent agent agent learn agent make learningin multiagentsystems moredifficult learning convergence equilibrium system experimentally competitive domain equivalence competitive domain rate convergence investigate also variant convergence show domain well reinforcement learning paradigm agent optimize environmental payoff presence central learning violates convergence agent reinforcement learning like guaranteed converge convergence concurrent learner equilibrium optimal agent markov game framework markov process controller used learning agent purely competitive domain littman work learning rule evaluated experimentally rule learning particularly suited game attractive uncertain wellman littman frameworkto agent converge nash equilibrium also multiagent learning empirical multiagent domain sandholm crites mentioned converge show identical purely competitive domain convergence rate minimaxq rule extend incorporate sarsa rummery sutton burto peng williams convergence rate competitive soccer domain littman multiagent markov process quadruple transition probability distribution multiagent looked upon agent agent agent redefined unknown goal agent find maximizes discounted agent time discount opponent bimatrix game pair matrix size game payoff agent joint agent agent matrix looked upon bimatrix game zerosum game bimatrix game nash equilibrium bimatrix game pair probability vector agent player game incentive unilateral deviation nash equilibrium equilibrium profile arbitrary finite bimatrix game nash learner need learn transition qlearning learning greedy agent arbitrary qvalues pair repeatedly chooses noting transition updating learning rate watkins dayan proved iteration converges optimal infinite sampling pair schedule learning rate case multiagent learning iteration work maximization insufficient presence actor opponent negatively correlated greedilyfor opponent andpessimistically oneself payoff littman simultaneousmove game agent agent opponentrespectively vector learner correspondingto opponent linearprogramming constrained minimax optimization learning rule proved converge optimal szepesvari littman game agent need know absence opponent game agent observe agent maintains wellman agent case agent bimatrix game mangasarian stone game simplifies converges nash equilibrium restrictive wellman imposed satisfies true hold zerosum domain deviation opponent decrease payoff thus modeler payoff convergence guaranteed domain littman wellman call latter differ structurally rule updating contrary made wellman functionally game game thie game informal represents column treated distribution hand pure also mixed consequently equality observe identical domain expediting learning purely competitive domain maintenance opponent turn explore fast last reinforcement substantially slow updating singleagent integrate rewardestimation scheme producing rule peng williams experimentation used postponed wiering schmidhuber learning scheme defining case convergence hold learning learning rule next used sarsa hand learning rule heavily learning followed rummery sutton burto control exploration reinforcementlearning algorithmscannot despite approximationin agent learning superior control well prediction boyan moore sutton tsitsiklis learn behaveconsistently exploration sutton burto natural combine eligibility trace raise endeavor answer equally well multiagent domain case possibly convergence sarsa rule sarsa modifies rule thus sarsa rule learns converge optimal learning chooses optimal multiagent rule replaced agent rule convergence rule follow satisfies need infinite exploration minimax decay call infinite mlie convergence rest lemma established lemma stochastic process measurable measurable hold finite converges converges probability rule sarsa agent also note fixed rule szepesvari littman agent convergence learning lemma learning rule specified converges probability mlie scheme step finite variance stored lookup learning rate satisfies lemma opponent play greedily outline lemma defining unless give rise measurability variance satisfied operator contraction outline remaining task show vanishes mlie exploration case case pected vanishes opponent play greedily case vanishes mlie part agent opponent established ishes tuple vanishes tuple vanishes mlie exploration optimal play opponent lemma rule converges mlie exploration probability opponent play greedily note case need boundedness argued greedy play opponent restrictive convergence agent gain term learning cost offset greedy play competitive domain scheme used purely competitive soccer domain littman grid containing agent occupy distinct goal agent left show agent ball agent game agent agent fixed five going left staying soccer domain agent agent bump onto stationary agent receives ball fails agent receives reinforcement goal sameside opponent goal opponent maintain game case reinforcement zero game reset configuration call agent littman training phase symmetric training ordinary magents rule sarsa rule respective learnt recorded iteration training lasted iteration used identical littman learningrate game played left horizontal axis percentage vertical axis former case plotted averaged game played percentage averaged former case plotted test phase allowed play test iteration draw probability break deadlock resultant percentage opponent reported left suggests ordinary initially dominates gradually catch outperforms former playing case rule outperforms ordinary beginning gradually lose edge ordinary rule learns progressively corroboratesthese performswell decay note percentage game good comparative estimator estimator deviation convergencevalues latter calculated domain extensive computation also stress reported convergence equally well beat understood next minimax next postpones relying expect catch left learning continues domain note rule applicable well rationale minimizing opponent learner maximizing opponent sarsa work domain experimentation domain call tightly coupled navigation gridworld left corner cell agent reaching cell corner agent domain cell agent reach remain cell agent tightly coupled must occupy cell agent coupled move remain absorbing agent reach goal receives iteration game restarts agent reshifted realistic domain agent steering wheel hand move move wheel move path agent wish follow reach goal tightly coupled must strike compromise find path maximally satisfied coupling symmetrically trained agent domain exploration probability forthe agent iterationwere goal absorbing statestate tightly coupled navigation domain soccer domain varied probability rewardgeneration iteration stand case agent reach goal wanted infrequent realistic practical domain convergence convergence rate fall infrequent convergence deviation learned plotted training lasted iteration iteration mean deviation plot solid ordinary probability rewardgeneration sarsa rule ordinary minimaxq rule case decrease monotonically suggests rule eventually converge fall probability scrutiny show minimax path learned agent path corroborated learned iteration mean deviation plot solid ordinary probability rewardgeneration iteration mean deviation plot solid ordinary probability rewardgeneration conclusion work sarsa littman minimaxq rule rule latter rule work well aware convergence property exploring property open area also note form probably expedient naturally combining disjoint area expedience seen plot awaited also substitute nashq specialized fast learning proceduresfor domain convergence line plan conduct boyan moore boyan moore reinforcement learning safely approximating neural processing system page wellman wellman multiagent reinforcement learning framework proc conf machine learning page francisco morgan kaufmann littman littman markov game framework reinforcement learning proc conf machine learning page mateo morgan kaufmann mangasarian stone mangasarian stone game quadratic mathematical nash john nash game annals mathematics peng williams peng williams incremental machine learning rummery rummery reinforcement learning thesis cambridge sandholm crites sandholm crites multiagent domain learning system page sutton burto sutton burto reinforcement learning sutton sutton reinforcement learning successful sparse coarse coding neural processing system szepesvari littman csaba szepesvari littman unified neural computation submitted thie thie linear game john wiley edition tsitsiklis tsitsiklis learning ieee transaction automatic control wiering schmidhuber wiering schmidhuber fast machine learning page machine learning data mining reinforcement system incremental gradient reinforcement learning alain dutech olivier buffet franc charpillet buffet dutech charp loria france reinforcement learning system realistic situated agent perception task automatically coordinated system crucial reactive agent learn decentralized cope difficulty inherent used framework incremental learning agent face task illustrate framework agent reach goal system system autonomous entity agent interact growing artificial intelligence term ferber build hand simulation tune system reach desired deal automated fashion learning learning take many form stone veloso opted reinforcement learning sutton barto teacher knowing hand agent scalar learn system composed reactive situated agent perception thus system build nevertheless decentralized framework agent face simpler task suffers combinatorial explosion decentralization learning process bring benefit agent learns simplifies task learned besides consistent localized realistic situated agent major difficulty burden hidden usual situated agent rely imperfect perception system stay unknown prevents classical optimal assignment evident positively good many agent even crucial must also decide agent answer decentralized incremental learning classical find stochastic actually agent stochastic adapted perception incremental mean agent progressively pitted harder harder task progressively learn incremental learning learning agent minimize coordination export task agent eventually refined learning demanding thus originality twofold learning decentralized incremental learning multiagent system tested idea give framework conducted test viability work highlighted work conclusive framework part kind agent show face reinforcement learning lastly incremental learning brings agent interested system agent learn decided work reactive agent besides concentrate learning many agent characterized situated perception perception thoroughly possibly heterogeneous learning process agent learn individually agent acquire cooperative agent goal reach nothing communication framework incorporated agent limitation classical reinforcement learning appealing learning optimal memoryless agent scalar feedback system agent learn besides used uncertainty evolution convergence like proven markov process tuple finite finite system probability system transition generates find optimal mapping maximize time expressed utility mapping pointed boutilier evolution kind interested classical reinforcement learning system composition agent joint composed agent thus centralized view quickly illustrated besides mean centralized noncentralized agent learn unfortunately bernstein agent perception system nexpcomplete provably face major difficulty transition reactive agent view joint fact agent unpredictable consequence transition system seen agent probability agent move ahead greatly adjacent agent observability agent perception know hand belongs partially markov littman classical stationary partially markov process nearly impossible hundred dutech make incremental gradient reinforcement learning find incremental gradient main idea progressively scale agent gradient baxter bartlett stochastic partially observedmdp stochastic deterministic buffet conclusive regard chose work gradient reinforcement learning suited stochastic baxter baxter bartlett incremental learning learning reduce assignment incremental learning obvious growing agent learning agent learning must agent agent agent refined learning incremental learning also dimension agent pitted harder harder task task near term reinforcement away progressive task learning task agent heavily guided task learning progress task made harder giving freedom agent perception situated agent perception reduce centralized huge mean agent know make difficult perception prerequisite incremental learning agent nearby thus scale agent dimension task experimenting incremental learning incremental learning give incremental task used harder task agent task agent yellow blue grid goal push yellow cube blue agent attain goal pair cube temporarily disappear simultaneously agent responsible fusion receive goal merge many cube time considering agent simply four moving north east south west move agent push agent cube consequence stochastic agent perception nearest agent opposite color nearest yellow cube nearest blue cube near yellow cube eight nearest near blue cube eight nearest combining perception reduce symmetry totally centralized also size creditassignment agent identified originated fusion cube case zero agent coherent agent simulation learning avoid cube border cell grid cube reappear grid cell yellow cube blue cube agent perception agent learning used said agent gradient learn precisely baxter baxter bartlett parameter lead utility gradient permit find locally optimal maximizes parameter chose real valued parameter probability case step incremental learning scheme sized agent agent cube color beginning reinforcement agent face harder harder progressive task lead thus incremental learning agent face task learn ending keep learning precise beginning must repeated sufficiently time succession agent trainer progressive help learning asada training used incremental learning agent cube incremental learning distant goal asada showed show used help agent training configuration need move reach goal agent pushing toward cube step reach explore move time show evolution pair agent progressive learning curve showing learning assisted training step training delay elapsed notably reached assistance cube agent merges step agent cube next step incremental learning agent used reach goal cube look learned used agent cube agent evolve populated note agent learned case help never learned anything influence agent cube part work agent deal cube taking agent cube color agent fixed learned case supposed cube nevertheless give good reaction good test carried cube agent size used series consecutive step series beginning configuration blocking sometime series made case give twentyfive case merges made step quickly show seems optimal agent cube bold growing agent improves cramp bring many coordination growing cube optimal agent hesitate cube work fall oscillatory move agent agent whatever incremental learning agent agent fixed learned agent cube case seen growing cube induces step evolution learning scratch curve practically confounded abscissa axis step evolution learning sufficient agent incremental learning keeping learning agent show evolving time learning case agent learning scratch agent learning scratch slow evolution agent cube learning rise rate agent seems reached fixed confirms learning optimal reached agent incremental learning reached learning scratch forecast agent come find optimum work explicit coordination show work benefit addressing coordination case agent cube able push pair cube many agent work boutilier boutilier studied view markov process framework agent globally optimal coordination optimal joint coordination reached social communication learning centralized planning framework used decentralized work like wellman oriented reinforcement learning settle nash equilibrium coordination work difficult agent deal taking agent learning planning carmel markovitch vidal durfee agent build agent next case seems modeling modelization lead agent communication also used explicit coordination must paid fact communication cost xuan worthwhile cost simply putting agent seems even coordination agent really like near subject like investigate term memory agent help mataric work system mataric took adapting agent perception reinforcement kind boolean process goal smoother adapted give frequent hint agent seen size greatly learning hand kind agent made learn task changing much agent learning process coin framework wolpert collective intelligence framework learning main idea behind work adapt cluster agent build system dealing mean creating sharing conducted show learning difficult slow framework agent idea seem adapted learning automatic incremental learning explained need decompose hand task process automatically learning agent classical agent explore mark work around thus learned transfered agent conclusion addressed automatically system framework agent reinforcement learning adapt desired task learning unsolvable decentralized classical limitation like observability assignment combinatorial explosion emphasized incremental learning scheme agent faced harder harder generic adapted task adapting agent hand tested simulated agent reach goal fusion colored cube give framework show incremental learning incremental learning lead learning rate unassisted learning learn task incremental learning learning task scratch whole framework room overcome limitation like communication addressing explicit coordination memory intention deal oscillatory reinforcement learning possibly adequate learning also modeling agent mean predict thus agent acknowledgment particularly thankful bruno scherrer iadine vincent chevrier numerous invaluable help asada asada noda tawaratsumida hosoda purposive acquisition real robot reinforcement learning machine learning baxter bartlett baxter bartlett reinforcement learning gradient australian canberra australia bernstein bernstein zilberstein immerman decentralized control markov process proc conf uncertainty artificial intelligence boutilier boutilier planning learning coordination multiagent process proc zeeuwse stromen netherlands buffet buffet dutech charpillet incremental reinforcement learning multiagent system proc carmel markovitch carmel markovitch opponent modeling learning system proceeding springer dutech dutech pomdp proc european conf artificial intelligence ferber ferber system distributed artificial intelligence john wiley york wellman wellman multiagent reinforcement learning framework proc littman littman cassandra kaelbling learning partially observable scaling proc mataric mataric reinforcement learning domain autonomous robot jaakkola learning partially observable markovian process proc stone veloso stone veloso multiagent system survey machine learning perspective autonomous robotics sutton barto sutton barto reinforcement learning bradford book vidal durfee vidal durfee agent learning agent framework collected multiagent learning aaai wolpert wolpert wheeler tumer multiagent system proc seattle page xuan xuan lesser zilberstein communication markov process proc icmas game theoretic theoretic agent robot weightlifting michael rosenstein andrew barto massachusetts amherst barto describes structuring robot learning task suitably parameterized show biologically motivated mean skill acquisition framework make robot counterpart learning imitation control synergy demonstrate learning coordinated emerges crude difficult robot weightlifting task exhibit remarkable proficiency task handwriting juggling machine assembly characterize success task mastery redundant degree freedom bernstein ease coordination sharply robot task walking throwing pose roboticists artificial intelligence path planning trajectory tracking work well highly structured nonlinear dynamic inadequate best examine form learning mean skill acquisition simulated robotic learns raise timely actuator learning fall near take take mind reinforcement learning agent interacts repeatedly learn optimal mapping maximizes criterion make adjustment copyright joint conferenceon artificial intelligence tend build form case distinguishing feature learning exploratory evaluative feedback reinforces successful reinforcement learning sutton watkins dayan particularly take capture regularity transition implicitly task reinforcement learning typicallyignore solutionby treating featureless mapping reinforcement learning framework tsitsiklis variableresolution moore atkeson factored mdps boutilier decison sutton constrain form strucutre nevertheless remains anderson baird moore baxter bartlett moriarty away take evolutionary moriarty reinforce regularity parameterized genetic restricting reproduction fittest population optimization swann make continuity biasing toward promising parameter ignore adjustment little rely extensive explorationto gather build optimal benefit domain utilize simplicity believe superior goal show biologicallymotivated make learning simulated robotic configuration payload goal joint angle degree proximal distal control certainly look biology inspiration regard machine learning control work novel developing coordinated robot robot weightlifting exploitingthe dynamic inertia elasticity weightlifter difficult coordination raise twice bodyweight head simulated robotic weightlifter loosely olympic weightlifter inspired numerous studied machine learning control show robot configuration task lift payload stable equilibrium unstable equilibrium torque joint restricts feasible execute coordinated swing momentum intersegmental dynamic kinematic redundancy joint obstacle striped well robot also complicate task robotic simulated frictionless pendulum link mass joint torque middle distal joint confined degree iteratively walker orin numerically euler integration step size robot motionless goal hypercube velocity degree centered goal configuration trial robot entered goal quantified integrated torque magnitude trial lifter exceeded contacted obstacle failed reach goal five trial terminated cost task optimal control torque kinematic optimal control dynamic optimization weightlifting task wang used feasible path triple recommended payload puma industrial robot precise system identification protracted offline computation robot lift make borrow learning difficult machine learning structured parameterization coordination focused control learning relatively little learning leaf robot many artificial intelligence adaptive control form learning control biologically motivated parameterized seeded adapted learning motivation desired configuration robotic used morimoto doya robot task regard specification primitive converges manipulator configuration primitive suggestive style control feldman whereby endpoint programmed property muscle convergence endpoint benefit equilibriumpoint control limb dynamic ignored need activation numerous actuator drawback complicated equilibriumpoint give simplicity requiring virtual trajectory equilibrium induce desired target convergence weightlifting task control primitive brings manipulator goal configuration payload obstacle linear feedback controller accomplishes task fails payload primitive represents configuration avoid leftmost obstacle also reduce moment proximal joint imitation successful lift schaal case convey crude path successful trajectory convergence goal extends payload thus robotic weightlifter control also drawback robot intrinsic dynamic ignored difficulty well benefit turn virtual trajectory dynamic primitive learning hierarchical feedback controller goal adjustable followed fixed controller regulate locally goal convert ballpark greene schmidt handle minor closedloop control build construct controller joint velocity vector joint torque subject saturation torque gain matrix target equilibrium nominal proportional derivative gain target equilibrium initialized goal configuration initialized identity matrix controller initially uncoupled servomechanism joint next robot imitation trial convergence followed threshold test establishes convergence convergence time elapsed controller trial mark switching time control trial time switch made followed controller trial terminates fixed copy flexibility learning system convergence close goal play role imitation trial free parameter switching time adjust learning shortly free parameter adapt learning form shaping payload nominal rate trial learning payload learning proceeds phase shorter phase trial adjusts followed phase trial adjusts controller summarizes robot weightlifting task parameter centered base perturbation normally distributed zero mean deviation size test evaluated best kept weightliftingtask give simulated trial parameter base made taking step toward test probability toward best probability thus easy adjustment tradeoff pure walk rough gradient even zero considerable exploration decay iteration step size size decay size best ybest best repeat ybest best ybest best convergence iteration best control trajectory payload learning obstacle imitation trial payload payload unexpectedsolution payload goal configuration step size parameter size decay controller parameter property make nice learning easy wide optimization like swann need derivative feature cost gradient difficult deterministic subject noise also neurobiological albeit speculative anderson beveridge schechter simplex make rapid progress numerous exploratory move establish build simplex learning dynamic payload robotic weightlifter qualitatively reach goal configuration obstacle path goal imitation trial take path configuration converging payload feasible solutionsshrinks represents learning system favorable learning pass coordinating joint robot intrinsic dynamic robust sensor noise well variabilityin also show unexpected reversal robot move entirely configuration indicated controller initially behaves linear learning transforms robot analogue synergy bernstein greene smart nonlinear controller intersegmental describes linear controller saturation represents nonlinear possibly inactive learning system examine coupling controller quantify joint coordination diagonal dominance controller gain matrix show throughout coupling diagonal trial half mass gain matrix fall main diagonal payload coupling mirror drop lightest payload observe statistically coupling thus coordination control system play role weightlifting task significance remains work trial payload learning controller joint coordination averaged solid marker statistically trial reversal payload manipulator nearest early learning middle distal joint induces counterclockwise proximal controller clockwise away toward word passive mechanincal coupling conflict initially uncoupled control learning best intersegmental dynamic swing momentum reversing sign shoulder torque rare learning system discovered reversal throughout much middle distal joint opposite sign reversal abandon supplied interestingly reversal increased variability involving sensor noise size know alternative parameterization coaching robotic weightlifter encourage counterclockwise swing inserting extra extra controller separately also encourage reversal depicted frame five alternative resulted statistically summarized thus injecting turned reversal rare occurrence gauge conclusion much success supplied parameterized suspect sophisticated form optimization swann yield term learning time qualitative remain possibility work continuing control baxter bartlett sutton particularly extending outlined coaching payloadlifted successfully learning averaged task occasional continuing robotic manipulator classic reinforcement learning construction kinematic trajectory avoids obstacle singularity robot intrinsic dynamic something compensate something even optimized trajectory imprecise dynamic lead overly conservative acceleration craig adaptive control deal inaccurate gave stability control learning framework motivated control part work plan test framework real robot seven degree freedom like reinforcement learning geared toward optimization thus resulting inherently dynamic mentioned criticism equilibriumpoint style control need virtual trajectory complicated ingen schenau also argue kinematic task dynamic control contact force agree criticism equilibriumpoint control nevertheless play role work primitive goal configuration supply hierarchical kinematic learning transforms cancel dynamic acknowledgment thank theodore perkins richard emmerik comment helpful foundation grant anderson russell anderson biased randomwalk learning neurobiological correlate omid omidvar judith dayhoff neural network page academic diego anderson anderson approximating approximating colorado baird moore baird moore gradient reinforcement learning michael kearns sara solla david cohn neural processing system page cambridge baxter bartlett jonathan baxter peter bartlett reinforcement learning pomdps gradient ascent proceeding seventeenth machine learning bernstein bernstein regulation pergamon oxford beveridge schechter gordon beveridge robert schechter optimization practice book york boutilier craig boutilier richard dearden moises goldszmidt stochastic dynamic factored artificial intelligence craig john craig adaptive control mechanical manipulator publishing company feldman feldman hypothesis control greene greene system rosen snell progress biology volume page academic york greene greene easy control moore atkeson moore atkeson resolutionreinforcement learning multidimensional machine learning moriarty david moriarty alan schultz john grefenstette evolutionary reinforcement learning artificial intelligence morimoto doya morimoto doya reinforcement learning dynamic learning stand proceeding intelligent robot system volume page schaal stefan schaal imitation learning route humanoid robot cognitive schmidt richard schmidt schema discrete skill learning psychological review sutton richard sutton doina precup satinder mdps framework temporal reinforcement learning artificial intelligence sutton richard sutton david mcallester satinder yishay mansour gradient reinforcement learning sara solla todd leen muller neural processing system page cambridge sutton richard sutton learning predict temporal machine learning swann swann murray unconstrained optimization page academic york tsitsiklis tsitsiklis learning ieee transaction automatic control ingen schenau ingen schenau soest gabreels horstink control relies walker orin michael walker david orin dynamic simulation robotic dynamic system control wang wang timoszyk bobrow weightlifting planning puma robot proceeding ieee robotics automation volume page watkins dayan watkins dayan machine learning machine learning data mining inductive soundness refutation completeness floriana esposito nicola fanizzi stefano ferilli giovanni semeraro dipartimento informatica universita bari orabona bari esposito fanizzi ferilli semeraro weakening implication identity bias novel manageable ordering relationship clausal give namely soundness refutation completeness subsumption derivation procedure make relationship particularly appealing inducing clausal implication relationship clausal particularly hard handle many descending intrinsic gottlob fermuller subsumption tractable efficiently implementable stronger relationship relationship entail exploited inductive made weaken subsumption implication manageable ordering relationship clause form subsumption weak subsumption badea stanciu weaker decidable form implication algebraic refinement operator affected ordering relationship weakening implication identity bias allowed novel relationship clause make manageable esposito existence ideal refinement operator ordered resulting form implication clausal wolf particularly refinement dense badea stanciu computational resulting operator much appealing refinement operator identity framework machine learning also exploited operator inductive learning well abduction reasoning framework multistrategy learning adapted esposito implication identity introduced main demonstrated namely soundness derivation procedure compliant mentioned bias subsumption demonstration procedure also open investigation property form implication compactness decidability algebraic property ordered relationship induced form implication semantics complying identity soundness procedure refutation completeness subsumption proven draw conclusion outlining ongoing framework adopt expressing made clause clausal lloyd wolf framework relies employed defining bicycle mention wheel rule monocycle identification wheel avoided bicycle thorough treatment datalog clausal offered semeraro want extend expressive clausal restriction term want avoid identification term identity clause term must distinct entity domain viewed reiter name reiter identity equational clause upon brings modified equational consisting axiom equality lloyd namely pair distinct term occurring clause wolf embedded equality semantics demonstrating completeness sldnf resolution dealing normal yielding wolf badea stanciu weak subsumption deal substitution identify literal adopt founded semantic capturing identity substitution semantics khardon semantics follow identity semantics clausal defining term domain deal identity exactly domain preinterpretation mapping need term domain domain assignment identity mapping domain mapped assignment mapped term assignment identity term mapping term distinct term mapped term mapped term combining term assignment identity domain identity oiinterpretation assigns mapping assignment identity term assignment truth atom truth truth connective truth truth truth legal assignment identity truth truth truth legal assignment identity truth tautology contradiction satisfiability consistency straightforwardly semantics clause closed truth depend assignment ground literal true closed oiinterpretation identity satisfies closed conversely show case oimodel assumed domain herbrand stated false truth clause universally quantified identity sufficient true hold semantics true form implication compliant identity consequence identity thus hold also semantics deduction consequence also unsatisfiable unsatisfiable identity concern clause semantics concentrate herbrand lloyd clause herbrand brand domain herbrand universe occurring true obvious soundness subsumption identity subsumption implication identity goal relationship constructive form resolution coping identity property substitution fulfill fact substitution regarded mapping term property preserve identity term substitution omit term obvious composition oisubstitutions ground renaming well clause ground clause variant cope identity relationship derived semeraro classic exploiting esposito clause term induces upon clause semeraro besides equivalence clause equivalence literal subsuming clause onto literal subsumed clause must literal thus ordered made clause yield equivalence ordered done substitution unifier fulfilling finite term mguoi clause proper subset used standardizing parent clause step resolution esposito clause standardized clause unifiable mguoi robinson robinson chang taking complementary pair literal resolution step esposito clause inductively rnoi rnoi rnoi rnoi also case linear resolution chang originally exploited esposito derived mean zero linear step clause linear operator stand closure soundness preliminary lemma clause hold clause standardized resolved upon subset literal mguoi employed domain assignment identity false literal true note made literal false literal true inductively proving soundness soundness clause clause induction hold thus obviously thesis hold obvious inductive hypothesis lemma give next justify formally showing equivalence clause identity tautological clause clause equivalence easy oiimplication strictly stronger ordering relationship clause made tautology next mean subsumption refutation completeness soundness bridge give completeness make step wolf case ground clause lift case preliminary simplest case ground lemma ground clause ground clause case implication wolf assignment made clause ground case finite clause mean compactness subsumption clause ground need unsatisfiability proven also identity assumed clause unsatisfiable identity finite unsatisfiable ground clause wolf come deduction clause ground clause finite ground clause wolf case exploited show lift step case ground parent clause unrestricted clause lemma clause clause oiresolvent standardized subset literal resolved upon obtaining mguoi letfor clause standardized unified unifier mguoi thus resolved identity upon generalize lifting case oiderivations step lemma finite clause clause clause oiderivation clause induction derivation case made clause hypothesis thesis proven derivation resolvent clause hypothesis lemma instrument proving lemma finite clause ground clause tautology look clause fulfilling thesis finite clause ground clause lemma clause oiderivation mean lemma lifted oisubsumes subsumption consequence preliminarily recall substitution used next clause clause consts skolem substitution extensible easy skolem substitution also main consequence subsumption finite clause clause clause tautology skolem oisubstitution ground clause tautology lemma clause derived yielded replacing binding replaces thesis hold soundness oisubsumption consequence case nearly straightforward demonstrate originally gottlob clause ambivalent literal opposite sign besides unify clause recursive unification show coincide clause recursive tautological oisubsumes hold recursive part obvious clause literal clause observe hypothesis hold thus note construction literal tautology must hold oisubsumes observing made literal analogously property used decidability implication identity subject scope sufficient equivalence hold clause ambivalent ambivalent thus tautology hold recursive literal hold ambivalent thus recursive tautological subsumption showed derive conclusion drawn implication identity implication resolution deduction step unsatisfiable clause identity refutation completeness finite clause unsatisfiable identity finite clause subsumption must empty clause soundness case infinite compactness like boolos jeffrey demonstrated interested concerning finite clause conclusion work framework novel implication identity bias clausal able unified exploiting soundness subsumption proving refutation completeness procedure relationship stronger induces clausal conferring algebraic property implication expect encouraging viewpoint concerned demonstration compactness decidability relationship stream concern algebraic property resulting calculating long term perspective integrated framework multistrategy learning benefit badea stanciu badea stanciu refinement operator weakly perfect dzeroski flach proceeding inductive volume lnai page springer boolos jeffrey boolos jeffrey computability cambridge cambridge edition chang chang mechanical proving academic diego esposito esposito fanizzi ferilli semeraro ideal refinement identity langley proceeding machine learning page palo alto morgan kaufmann gottlob fermuller gottlob fermuller removing redundancy clause artificial intelligence gottlob gottlob subsumption implication processing letter clause implication artificial intelligence khardon khardon learning horn machine learning december lloyd lloyd foundation springer edition wolf nienhuyscheng wolf subsumption inductive fact fallacy raedt inductive page wolf nienhuyscheng wolf foundation inductive volume lnai springer reiter reiter equality domain closure database robinson robinson resolution january implication clause undecidable semeraro semeraro esposito malerba fanizzi ferilli framework incremental inductive synthesis datalog fuchs proceeding volume lncs page springer levelwise fragment raedt stefan kramer freiburg freiburg breisgau germany deraedt skramer tight integration mitchell agrawal apriori used data imposed generality imposing frequency data framework fragment compound fragment linearly substructure compound well preliminary mannila toivonen mannila toivonen formulate task database expressing find viewed sentence make true generic make task association rule frequent inclusion dependency dependency frequent episode also task mannila toivonen type relies frequency past data mining spent efficiently frequency apriori agrawal extend data mining user generality resp well frequency frequency impose frequency data combined discover frequency dataset frequency data flexible declarative view work also inductive database framework imielinski mannila raedt discovering primitive efficiently combine primitive mitchell mitchell primitive frequency mannila toivonen frequency property hold primitive also specified hirsh intersection hirsh applying hirsh framework employ tighter integration apriori demonstrate relevance well domain fragment fragment linearly atom induction structureactivity relationship sars statistical relate chemical automatically derived fragment sars originates system rosenkranz system extensively used predictive toxicology system area compound fragment occurring compound regard contribution enables regarding fragment user precisely fragment interested also answer thus algorithmic view process fragment also studied inductive database warmr dehaspe toivonen inokuchi inokuchi used warmr system discovering succeeding datalog thus restricted fragment inokuchi deal arbitrary frequent subgraphs thus restricted linear fragment differ domain expressive frequent linear fragment fragment propositionalization kramer frank handle frequency threshold work specification sort regarding generality frequency also time pose frequency fragment frequency like stress integration compact resulting understandability also learning fragment feature organised fragment task primitive querying fragment fragment touch upon work framework fragment task integrated apriori framework fragment fragment linearly atom fragment oxygen atom bond sulfur atom bond carbon atom bond double bond triple bond aromatic bond literature atom system database compound compound database atom bond double triple aromatic compound compound fragment occurring fragment compound graph subgraph fragment compound property fragment fragment partially ordered fragment fragment syntactically fragment reversal substructure subsequence subsequence reversal maximally fragment empty fragment maximally fragment convenience artificial note fragment relatively restricted employed data mining dehaspe toivonen subgraphs inokuchi fragment relatively restricted chemical easy trained chemist recognize fragment thus fragment reveals meet fragment task addressed fragment primitive primitive imposed unknown target fragment unknown target fragment type primitive specified fragment specifies subsequence frequency fragment frequency fragment dataset frequency dataset resp target fragment frequency primitive conjunctively combined declaratively target fragment note datasets imposing frequency inactive fragment subsequence frequency frequency find conjunctive fact primitive find taking intersection primitive secondly primitive monotonic generality mannila toivonen resp monotonic generality resp framework monotonic negation monotic vice versa monotonic border fact data mining literature mannila toivonen border well machine learning literature mitchell used border need maximal generality fragment border primitive proper proper monotonic mitchell framework last property resp proper border resp monotone primitive characterized conjunctive also characterized computing border tive computing border addressed variant data mining mannila toivonen hirsh merging probably clear need integrate taking loose coupling sketched tighter integration show lead computational integrated computes border incrementally initializes border maximal repeatedly primitive border regard primitive involving generality employ mellish identification mellish extends mitchell process type also handle dual secondly frequency integrated variant border mellish identification formulate mellish identification need operation fragment smallest merged fragment largest subfragments smallest fragment largest fragment operator fragment operation used instantiate mellish primitive case case case case variant outlined employ refinement operator refinement operator extending fragment atom operator removing atom side fragment deal frequency employ downwards type satisfies infrequent fragment endwhile case case behave roughly fragment size keep track frequent fragment well infrequent repeatedly refinement fragment frequent looking frequency resulting possibly frequent fragment database process continues empty modification made concerned fact need fragment secondly frequent fragment finements infrequent also realized dual case proceed dual resulting upwards type satisfies infrequent fragment endwhile bottom work efficiently depend consideration remains open work efficiently also modify exploiting duality handle monotonic frequency form case dual type satisfies frequent fragment endwhile optimisation optimisation adopted border handling frequency also adopt bayardo gunopulos longest secondly made combined give also done fragment frequent frequent fragment variant combined well thirdly keep track fragment canonical form indicated fragment reversal canonical form fragment lexicographic ordering fragment reversal operator take care fourthly framework fragment stem fact bottom fragment manipulated thus upwards downwards process frequency upwards undefined concrete fragment optimisation primitive primitive seem minimize maximize imagine interested fragment frequency dataset minimally easy extend framework primitive find fragment conjunctive maximal regard specified criterion criterion monotonic frequency generality find regard optimisation primitive computes regard selects maximal regard criterion validate predictive toxicology dataset srinivasan srinivasan data compound take mbyte memory prolog encoding used benchmark predictive toxicology artificial intelligence goal discover fragment relatively frequent carcinogenic compound infrequent compound activating toxic fragment structural alert toxicological literature ashby patton rediscover alert summarize experience systematic open toxicological role chlorinated compound carcinogenicity framework concerning chlorinated fragment frequent compound infrequent inactive look concern existence activating nonhalogenated fragment quantitative gather quantitative evidence systematic domain side clear structural alert relatively rare carcinogenicity safely assumed alert frequency compound also interested fragment frequency resp seeking fragment statistically overrepresented compound inactives frequency contingency occurrence fragment allowable frequency inactive compound frequency threshold methodwise fragment handling frequency handling frequency differ dealing frequency searching upwards dual column downwards runtimes pentium column column filter fragment frequent dataset summarizes runtimes frequency parameter time outcome clear priori bookkeeping done show case outperforms term computation time make upwards downwards frequency perhaps outcome answer constitute suitable compact also outlined computational time answering reasonable feature construction propositionalization kramer raedt conclusion work work contributes data mining machine learning integration framework inductive database fragment fragment briefly review contribution domain relate relevant work regard work build raedt integration expand work work show validity framework fragment thus work time evidence framework also regard regard framework data mining resulting framework extends border levelwise sketched mannila toivonen link mitchell evidence form viable classical border characterize inductive also done dong emerging emerging itemsets significantly dataset also closely fragment discover test primitive seem dong border dong employ levelwise rely bayardo must adapt bayardo gunopulos framework concern work also regarded domain inductive database imielinski mannila sketched inductive database user many data mining literature frequency data seems also note work discovering regularity also system buchanan mitchell agrawal agrawal imielinski swami mining association rule item database proceeding sigmod data ashby patton ashby paton influence chemical extent site carcinogenesis rodent carcinogen carcinogen exposure mutation bayardo bayardo efficiently mining long database proceeding sigmod data buchanan mitchell buchanan mitchell learning production rule waterman patterndirected inference system academic york dehaspe toivonen dehaspe toivonen frequent datalog data mining raedt raedt inductive database mining proceeding artificial intelligence computation lecture note artificial intelligence springer verlag raedt raedt database mining proceeding inductive lecture note artificial intelligence springer verlag dong dong mining emerging discovering proceeding gunopulos gunopulos mannila saluja discovering sentence foto afrati phokion kolaitis database icdt lecture note springer lakshmanan multidimensional data mining mining frequent proceeding sigmod data hirsh hirsh generalizing machine learning imielinski mannila imielinski mannila database perspective communication inokuchi inokuchi washio motoda mining frequent substructure graph data zighed komorowski zyktow proceeding pkdd lecture note artificial intelligence springerverlag kramer frank kramer frank bottomup propositionalization proceeding track inductive kramer raedt kramer raedt feature construction biochemical proceeding machine learning morgan kaufmann mannila toivonen mannila toivonen levelwise border data mining psaila ceri mining association rule data mining mellish mellish identification artificial intelligence mitchell mitchell artificial intelligence lkshmanan pang exploratory mining pruning optimization constrained association rule proceeding sigmod data rosenkranz rosenkranz cunningham clayhamp macina sussmann grant klopman characterization predictivetoxicology qsar environmental srinivasan srinivasan king bristol assessment submission made predictive toxicology proc machine learning data mining probabilistic learning learning bayesian network simon tong stanford daphne koller stanford koller task causal empirical data many area data crucial accomplishing task must great care learning informative uncovering formalize causal learning task learning causal bayesian network learner allowed conduct intervenes domain framework learning actively chooses learned show learning substantially reduce domain determining causal domain manysituations bayesiannetworks pearl compact graphical joint probability distribution also viewed causal domain pearl graphical represents causal domain formalize discovering causal domain task learning data last year substantial work discovering purely observational data inherent limitation discover sampled data data intervene vital full determinationof causal obtaining data time consuming costly thus must care learning selects informative revealing causal learning next data case upon seen possibility learning arise naturally domain variant interventional learning learner involving intervention type learning norm sort food intervention probabilistic dependenciesin replaced intervention pearl eats normally observing causal influence case purely observational data inadequate learning actively need tell next formal framework learning bayesian network bayesian learning maintain distribution bayesian network data distribution selects greedy much domain showing learning substantially accurate data interestingly learning even case intervene type thus applicable even learning learning bayesian network taking finite domain bayesian network pair represents distribution joint directed acyclic graph node correspond encodes independence property joint distribution parent parameter quantify network probability distribution cpds bayesian network represents joint distribution rule bayesian network viewed probabilistic answer form assignment also viewed causal pearl perspective also used answer interventional probability intervene forcibly take pearl framework intervention causal node replaces causal forced take graphical term intervention corresponds mutilating cutting incoming edge intuitively depend parent fact give evidential reasoning parent fact tell nothingabout parent fault diagnosis observe battery charged evidentially alternator belt possibly defective deliberately drain battery fact empty obviously give alternator belt thus resulting distribution mutilate eliminate incoming edge node cpds node probability goal learn data hidden make causal markov data bayesian network faithfulness distribution induced satisfies independence implied goal reconstruct data clearly data reconstruct uniquely network form equally consistent sample best hope identify markov equivalence pearl network induce precisely independence markov equivalence skeleton network pair fixed pair edge fixed edge directed spirtes well observational data identify much cooper intuitively trying edge data intervenes distribution intervening distribution edge bayesian learning data goal learning learn learning data allowed control intervening formalize idea subset learner instantiation request response randomlysampled conditioned word intervened take imply sampled mutilated bayesian framework learn precisely maintain distribution parameter parameter bayesian conditioning data heckerman make modularity form parameter independence parameter modularity graph also parameter multinomial parameter distribution conjugate dirichlet distribution hold distribution satisfying parameter modularity sampled bayes rule distribution proportional marginal likelihood data expressed integral parameter sample interventional resulting response need distribution response break identity thus need parameter density also distribution term network distribution parameter clear resulting parameter node forced take note bias updating parameter ancestor incoming edge node thus updateable interventional rule parameter density density response bayesian updating parameter case sampled dirichlet distribution updateable node distribution read density performingquery obtaining response note density bayesian conditioning also note interventional parameter preserve parameter modularity distribution distribution obtaining response tell interventional response satisfies parameter independence parameter modularity outline note data learning goal merely distribution interventional data want actively learn myopic learner selects upon distribution take resulting response distribution repeat process process task construct deciding next distribution loss work tong koller step distribution graph parameter extent distribution thereby selecting next formally distribution graph parameter loss loss distribution graph parameter loss exploss loss lead loss lowest note loss computationally need maintain distribution node loss computation exponential response make framework concrete must pick loss recall goal learn interested presence edge graph node edge relationship ther distribution graph parameter induces distribution edge relationship extent sure relationship entropy induced distribution entropy sure relationship form edge entropy loss loss domain determining relationship pair node reflect desire loss introducing scaling front term loss distribution task find computing loss relative note distribution conditioned data initially data bayesian conditioning obtaining tractable idea friedman koller simpler restricting network consistent ordering distribution ordering fixed ordering ordering restrict network consistent edge friedman also node parent fixed round domain construct mentioned friedman koller data node parent node consistent ordering note induced exponential restriction fixed ordering parent node parent node buntine friedman koller consequence give closed form efficiently computable quantity probability response probability edge bayesian averaging graph probability edge high merely indirect causal influence loss exploss also give fact applying rewrite loss exploss summation exponential summation resembles computing marginal probability bayesian network inference marginalizing fact regarded graphical inference procedure lauritzen spiegelhalter effectively restriction parent node ensures applying bayesian network inference need inference pair restricted parent edge thus computational cost computing loss cost bayesian network inference unrestricted ordering closed form computing loss ordering generalize derivation removing restriction fixed ordering loss rewritten exploss loss loss expectation ordering approximated sampling ordering distribution graph parameter friedman koller sampling ordering done effectively markov monte carlo mcmc inside expectation ordering loss fixed ordering must entropy term restricting ordering entropy term probability relationship node term inside expectation naively tation pling ordering clearly impractical substantially reduces computational cost mcmc generates ordering sampled many case data distribution ordering sample reasonably good sample distribution thus sampled orderingsto approximateeq note accumulate parameter merely predict round note fixed ordering case entropy term thus bayesian network inference property summarize sample ordering distribution graph parameter orderingsto computethe entropy term next ordering bayesian network inference ordering final give loss asking give lowest loss computational ordering need bayesian network inference inference inference take time exponential time next sampled ordering cost inference need sampled ordering friedman koller greatly reduce cost process also show markov fairly rapidly thereby numberof step requiredto sample reduce step even initially ordering easy ordering markov response distribution ordering ordering fairly close stationary distribution thus mcmc step ordering give ordering close sampled cancer network cancer network cancer network cancer network darker edge probability edge edge probabilty omitted reduce clutter evaluated reconstruct network data network experimentedwith commonlyused network cancer five node asia eight node troubleshooter twelve node test network maintained ordering restricted parent size took minute next learning sampling querying node distribution probability edge pair domain true network edge hold zero benefit sampling obvious access intervene thus cancer node four node node edge asia pair node cancer pair node cancer edge entropy pair node edge entropy legend reflect curve zoomed resolution nated restricting learning root root causal simply selecting data match give male need causal intervention response root node arises many domain medical domain subject gender ethnicity assumed root node informed node root parent empty batch parent node node true parent generating network parent show learningcurves cancer network experimented dirichlet also informed simulated sampling data true type made little qualitative comparative learning graph graph significantly sampling querying domain determining existence causal influence node experimented network modified edge edge entropy used make determining relationship node fuelsubsystem enginestart node time regular pair node used node network node learning substantially note true causal intervention identify asymptotically identify skeleton edge forced markov equivalence identifying edge true causal network even learning derive significantly learning interventional permitted pair node node node sampling also choosing node pair node node performedon asia cancer network informed batch also experimented choosing parent node alternative node true parent chose highest mutual practice observational data empirically choosing parent gave despite fact node network true parent node happened parent mutual mutual criterion choosing parent show network significantly outperforms also prediction graph graph edge entropy distribution overstructures show edge entropy reasonable surrogate predictive show causal edge probability sampling querying querying cancer network demonstrates observational data learn many edge show learning creates causal querying fact trial recovered edge perfectly discarding probablity edge able limitation also tends much placing edge indirectly causally network distribution learned summarized probability edge cancer coma cancer papilledema conclusion introduces learning bayesian network interventional formal framework task resulting adaptively selecting substantially prediction regarding sampling process interventional surprisingly achieves even restricted querying root network intervening body work optimal atkinson bailey learning causal domain fixed actively learning bayesian network tong koller parameter assumed learning causal purely observational data spirtes heckerman cooper learning causal network mixture observational data learning derive scoring full network learn relationship pair confounded hand framework permit combining observational data learning domain distinguish much finer taking consideration indirect causation confounding influence many extend work treatment continuous temporal process cost dealing hidden missing data learning decide extra observe extra piece missing data best learn exciting learning uncover existence hidden domain acknowledgement phrog system lise getoor lerner taskar work darpa assurance subcontract muri atkinson bailey atkinson bailey hundred year page biometrika biometrika buntine buntine refinement bayesian network proc cooper cooper causal mixture observational data proc friedman koller friedman koller bayesian network proc friedman friedman nachman learning bayesian network massive datasets sparse proc heckerman heckerman geiger chickering learning bayesian network statistical data machine learning heckerman heckerman meek cooper bayesian causal microsoft lauritzen spiegelhalter lauritzen spiegelhalter computation probability graphical system royal statistical pearl pearl probabilistic reasoning intelligent system morgan kaufmann pearl pearl causality reasoning inference cambridge spirtes spirtes glymour scheines causation prediction tong koller tong koller learning parameter bayesian network proc probabilistic classification clustering data taskar dept stanford stanford btaskar eran segal dept stanford stanford erans daphne koller dept stanford stanford koller supervised unsupervised learning traditionally focused data consisting type many domain best type domain citation also case label entity correlated label entity classification clustering domain capture probabilistic dependency show learn efficiently data empirical real data transductive classification significantly modeling dependency automatically induces natural help classify turn help classify unsupervised produced coherent cluster natural even type supervised unsupervised learning data identically distributed numerous classification clustering work flat data data vector duda survey many data much richer involving type hypertext page link domain citation also clearly violated linked citation growing learning richly structured datasets link proved classification clustering hypertext domain slattery craven kleinberg intuitively learning reach conclusion like propagate cite turn propagate informationto cite also like help reach conclusion area classification line influence propagation idea neville jensen classification process exactly iteratively assigning label test classifier confident label classify slattery mitchell classifying page belonging student none proposes coherent correlation forced purely procedural classification step combined unifying clustering emphasis dyadic data hofmann puzicha citation cohn chang link cohn hofmann kleinberg gene data kleinberg link mutually reinforcing relationship page good page many good good page pointed many good viewed clustering type page word hyperlink word occurrence like richer many real domain type relationship movie database type movie actor director producer type also database word cite well wrote like identify type segment generative probabilistic classification clustering data probabilistic database capture domain work build framework probabilistic prms koller pfeffer extend bayesian network prms capture probabilistic dependency coherent dependency principled propagating like generative probabilistic accommodate spectrum purely supervised classification purely unsupervised clustering thus learn data label also deal case type introducing latent unobserved cluster note impossible segment data training test training test interconnected naive sampling training sever link training test data circumvent difficulty transductive learning test data albeit label training phase even type training phase learning latent learning prms latent database task induce dependency latent entity data renderingstandard approachesintractable scale linearly thus data domain dataset database movie actor director classification show substantial boost clustering task show able find coherent cluster even type generative data probabilistic classification clustering viewed generative perspective density task data assumed identically distributed sample mixture distribution belongs exactly cluster clustering latent cluster assumed sample distribution powerful used distribution naive bayes naive bayes assumed conditionally independence unrealistic nevertheless proven robust classification clustering wide duda cheeseman stutz classification clustering parameter naive bayes clustering significantly difficult presence latent made classification clustering inappropriate rich domain correlated probabilistic classification clustering domain entity construction utilizes framework probabilistic prms koller pfeffer friedman probabilistic template probability distribution database schema specifies probabilistic entity probabilistic dependency defines joint probability distribution schema schema describes type type type also typed binary associate type actor role movie actor appeared case role ranking actor explicit type case role meryl streep sophie role wouldbe actor meryl streep movie sophie distinguish entity type type instantiation specifies type hold skeleton specifies type imdb domain cora domain fragment unrolled network cora probabilistic probabilistic specifies probability distribution instantiation schema precisely template instantiated skeleton instantiation probabilistic skeleton view compact representing bayesian network skeleton schema qualitative dependency parameter dependency associating parent parent form prms also dependency omit simplicity presentation skeleton induces unrolled bayesian network probabilistically parent form note singlevalued actually address dependence dependence aggregatefunction modeor mean multiset quantitative part specifies parameterization parent probability associating probability distribution specifies parent form represents dependence aggregate used unrolled network thus repeated many time network aggregate many aggregation operator dependency obvious categorical mode aggregate computes parent precisely parent wish aggregate domain note domain mode aggregator distribution multiset distribution multiset mode aggregator sensitive distribution parent differentiate highly skewed fairly frequent aggregate reflects distribution stochastic mode aggregator case distribution aggregator distribution frequency accomplish aggregate aggregate also take take easy aggregator exactly desired note aggregate also viewed selector node chooses parent take appealing consequence like stochastic decomposed scale linearly parent simply decompose aggregate cascading binary tree computes aggregate disjoint pair aggregate selecting parent repeat procedurefor disjoint pair construction also case omit lack classification clustering framework classification clustering flat probabilistic generative cluster classification task usual deal clustering task introducing latent thus entity designated flat classification clustering depend simplicity naive bayes dependency parent note entity type type entity type thus depend note dependence case singlevalued aggregate interestingly also dependence entity thus dependence vice versa case aggregate show movie dataset extracted internet movie database imdb role well defining movie actor case depend movie actor show domain derived cora dataset mccallum case cite connects type make cited depend citing note dependency cyclic type recall template instantiated skeleton unrolled network show fragment network citation domain unrolled network acyclic induces coherent probability skeleton friedman also latent dyadic clustering domain movie responds person rating movie case rating representing rating depend cluster leadingnaturallyto flexible accommodate much richer also person perhaps movie take consideration constructing cluster learning show learn data training instantiation schema everything view data missing note view data latent thus task parameter parameter case probabilistic dependency need parameter cpds likelihood find maximize instantiation likelihood likelihood parameter simply counting occurrence data recall thus parameter simply time parent jointly take sufficient statistic friedman case incomplete data substantially case likelihood expectation maximization dempster likelihood guess parameter iterates step computes distribution unobservedvariables data parameter letting unobserved cluster sufficient statistic distribution hidden must inference reestimates parameter maximizing likelihood distribution belief propagation step step need distribution unobserved data inference unrolled network decompose task inference task correlated case unrolled network treated separately unrolled network fairly involving many network thousand node inference network clearly impractical must resort inference wide scheme bayesian network chose belief propagation belief propagation message passing introduced pearl pearl guaranteed converge marginal probability node singly bayesian network empirical murphy show converges network marginals good converge marginals node inaccurate happens rarely affect convergence brief outline variant referring murphy bayesian network node case convert graph family graph node containing parent node domain also encompasses evidence observe elsewhere distribution normalizing belief propagation iteration family node simultaneously send message normalizing family neighbor family graph marginal distribution family process repeated belief converge convergence give marginal distribution family unrolled network marginals precisely need computation sufficient statistic note occasionally converge alleviate guess show work well practice influence propagation motivation dependency like propagate help reach conclusion process line influence propagation idea neville jensen classification build classifier training classifier base entity type classifier test base classified high confidence temporarily labeled predictedclass classification algorithmis rerun process repeat time classification substantially process iterates slattery mitchell idea classifying page belonging student train classifier labeled classify test classify test suggest combining classification test page test motivating page page classified studenthome page theirapproachtries identify page student directory page page also student page show classification improves exploiting neither proposes coherent dependency thus combine classification step unifyingprinciple approachachieves influence propagation probabilistic influence induced unrolled bayesian network domain cora domain network correlation cite thus belief influence belief probabilistic influence flow path unrolled network belief cluster influence indirectly belief propagation propagating message family family graph network propagates belief demonstrate property next spreadinginfluenceis particularlyuseful framework construct probability derived belief propagation train probabilistic inference process spread construct classifier turn even perspective test classification also learn classifier show process substantial iteration note bootstrappingability arises naturally probabilistic framework also compelling convergence evaluated cora imdb data cora cora dataset used subset machine learning classified seven probablistic neural network reinforcement learning rule learning casebased fraction labeled loopyiteration classification influence propagation evaluated classification took data classification fraction constructed data unobserved resulting used classify test type transduction test also used train albeit label investigate benefit exploiting four vary baseline multinomial naive bayes word word full make citation fragment incorporates eliminating citation citation four trained trained initialized cpds word cpds naive bayes trained data initialized cpds varied percentage labeled ranging percentage tested classification five five correspond seen incorporating dependency significantly improves classification outperform baseline combined achieves highest message passing loopy belief propagation resembles process spreading influenceof belief forum particularinstance cite labeled upon initialization belief word iteration belief reflect label cite peaked around confidence iteration unlabeled cite well unlabeled cite reflect increased confidence examine belief unlabeled iteration loopy belief propagation iteration fraction high confidence belief threshold show series dataset labeled series show iteration seventh iteration series show gradual fraction confident fairly iteration around seventh iteration loopy belief propagation inference step loopy guaranteed converge converges good make progress show classification improves iteration also demonstrates performanceimprovementobtained bootstrapping classification imdb imdb database latent variablemodel used genre actually refers binary comedy note actor director descriptive clustered meaningfully considering subset database movie actor director show cluster listing highest confidence cluster cluster movie consist movie predominantly genre time popularity movie cluster labeled classic musical child film cluster corresponds roughly movie cluster actor director induced movie cluster actor swiss family robinson cinderella sound music love wizard pollyanna parent trap mary poppins hood prince thief batman hunt october batman forever mission impossible goldeneye terminator judgment starship trooper hitchcock alfred kubrick stanley coppola francis ford lean david forman milo gilliam terry wyler william spielberg steven cameron james mctiernan john burton scott tony schumacher joel stallone sylvester russell kurt schwarzenegger arnold costner kevin damme willis bruce ford harrison seagal steven jones tommy niro robert hopkins anthony keitel harvey freeman morgan oldman gary cluster movie actor director movie actor actor play drama director cluster corresponds director drama director adventure film conclusion many domain rich interacting entity biomedical datasets machine learning ignore rich flattening vector growing learning domain classification clustering richly structureddata coherent probabilistic semantics build powerful tool probabilistic reasoning learning linear scaling thus domain transduction task substantially flat classification scheme also anecdotally construct cluster induces compelling used reach conclusion formal framework many work obvious used predetermined edge dependency cluster latent framework incorporate great deal semantics domain domain expertise lacking automatic construction crucial extend bayesian network friedman cheeseman stutz learning best suit data acknowledgment work darpa hpkb eran segal also stanford graduate fellowship cheeseman stutz cheeseman stutz bayesian classification autoclass fayyad smyth uthuruasmy data mining page aaai menlo park cohn chang cohn chang probabilistically identifying authoritative proc sigir cohn hofmann cohn hofmann missing link probabilistic hypertext connectivity proc dempster dempster laird rubin likelihood incomplete data royal statistical duda duda hart stork classification john wiley york friedman friedman getoor koller pfeffer learning probabilistic proc ijcai friedman friedman bayesian structural proc hofmann puzicha hofmann puzicha latent collaborative filtering proc ijcai kleinberg kleinberg authoritative hyperlinked proc symposium discrete koller pfeffer koller pfeffer probabilistic system proc aaai mccallum mccallum nigam rennie seymore automating construction internet portal machine learning retrieval murphy murphy loopy belief propagation inference empirical neville jensen neville jensen classification data proc learning statistical data page aaai pearl pearl probabilistic reasoning intelligent system morgan kaufmann slattery craven slattery craven combining statistical hypertext domain proc slattery mitchell slattery mitchell discovering test regularity domain proc icml machine learning data mining machine learning data mining adaptive navigation wireless device corin anderson pedro domingo daniel weld washington seattle corin pedrod weld visitor browse wireless cell pager stymied interface optimized simply replacing text reformatting deep link minute traverse minpath automatically improves wireless navigation suggesting shortcut link real time minpath find shortcut learned visitor shortcut link suggests best link explore predictive bayes mixture mixture markov empirical evidence minpath find shortcut save substantial navigational perkowitz etzioni challenged build adaptive site site automatically presentation learning visitor access spirit many perkowitz etzioni fink juhne joachim pazzani billsus sarukkai many like much visitor browsing color fast network connection work visitor site viewing many page opposed consequently emphasize graphical highlighting hypertext link automatic link many page growing visitor mold wireless visitor browse cell pager wireless device bandwidth counterpart site adaptive nonetheless deliver device experience visitor seldom browse many page interested like reach page link also largely true visitor link followed particularly poignant visitor simply link page visitor spend considerable time scrolling page tiny screen bandwidth high latency wireless network link even page text take long consequently navigating even four five page take minute frustrating visitor abandoning believe adaptive site hold great promise improving experience wireless device conversely wireless browsing killer adaptive site ongoing anderson framework adapting site wireless visitor take visitor receive viewing page reach page scrolling link followed explore framework automatically shortcut link page visitor request wireless visitor dominated gathering task accomplished viewing page site shortcut link page linked help visitor reach quickly link path shortcut save visitor navigation perhaps scrolling avoided interim page tradeoff shortcut link usefulness absurd shortcut pair page site visitor reach link link practically impossible thus concentrate generating high shortcut many wireless device scrolling make contribution minpath automatically shortcut visitor real time offline minpath learns server access runtime predict visitor ultimate minpath shortcut incorporates probability link visitor visitor bayes mixture mixture markov applicability minpath evidence suggests mixture markov best minpath minpath substantially reduces link visitor need follow thus navigational next minpath shortcut explores predicting evaluates minpath shortcut minpath defining terminology facilitate trail wexelblat maes page request made visitor coherent time coherence time request trail fixed time window request coherence request link page precisely time request page time trail page request time time time timeout trail link followed perspective adaptive site watching visitor midway trail prefix trail suffix need hypothesized adaptive site shortcut work shortcut link visitor help shorten long trail system shortcut link page visitor request ideally shortcut suggested help visitor reach trail link shortcut link precisely visitor trail prefix shortcut list shortcut minimizes link visitor must follow visitor last page trail prefix page visitor requested page shortcut calculate shortcut link visitor avoid shortcut know trail link shortcut lead page trail link skipped subtract visitor must follow link shortcut link shortcut lead elsewhere minpath trail selecting best shortcut page easy simply runtime visitor viewed trail prefix adaptive site must infer remaining page relies visitor probability trail suffix site intuitively suffix trail originating suffix probability assign shortcut suffix probability suffix link shortcut note shortcut many trail suffix many trail suffix page shortcut shortcut suffix brief elucidate idea visitor requested trail prefix wish find shortcut page visitor exactly page visitor trail probability probability shortcut trail page probability shortcut save link shortcut contribution suffix minpath expectedsavings construct trail suffix traversing directed graph induced site link graph page last requested visitor expectedsavings computes probability link recursively traverse graph probability viewing page fall threshold exceeded page currentsavings probability reaching page suffix link minpath collates best shortcut next describes minpath predictive minpath success predictive evaluated probabilistic minpath must predict next page request trail prefix visitor identity identity lead past site demographic probability part even none data explore simplify sink page visitor implicitly request trail prefix page requested visitor identity shortcut minpath expectedsavings sort page best shortcut page recursive traversal trail prefix page request visitor identity trail suffix hypothesized page traversal probability suffix suffix shortcut expectedsavings probability threshold currentsavings else currentsavings currentsavings else currentsavings trail concatenate link probability trail concatenate expectedsavings minpath browsing trail thus probability probability visitor request page trail note learned offline minpath must real time unconditional simplest predicts next page request conditioning learn measuring proportion request page site training time requested page request visitor view page linked page thus minpath force probability page linked page zero renormalizes probability link page minpath calculates page link volume training data approximately page request approximately site page build predicts page many page requested infrequently reliably probability page aggregate replace page request label much spirit zukerman hierarchy directory imposes hierarchical clustering page node closest leaf traffic site page node prefix stem label node varying threshold fewer node minpath correspondingly affected bayes mixture unconditional assumes trail site sufficient accurately capture intuition suggests false visitor trail even visitor follow trail visit alternative hypothesize trail belongs distribution cluster thus probability requesting page conditioning cluster identity mixture combine probability distribution bayes calculate make bayes page request trail cluster thus resulting bayes mixture used autoclass cheeseman learn parameter cluster assignment probability dempster mixture probability soft assignment trail cluster cluster contributes fractionally alternatively hard assignment trail probable cluster explore possibility fixed holdout data likelihood holdout data previously learned maximizes holdout likelihood piece selecting cluster assignment visitor identity incorporate conditioning page request visitor cluster side trail visitor well mixture thus visitor mixture proportion visitor history predicted trail visitor produced site visit probability cluster produced trail markov unconditional bayes mixture ignore piece access page trail markov hand incorporates conditioning probability next page page markov trained counting transition page training data counting page request trail replaced page request stem volume relevant training data need transformation even markov quadratically probability unconditional link rare markov also minpath mixture markov cadez build mixture learn bayes mixture positional conditioning probability last requested page also conditioning ordinal request visitor trail effectively training unconditional markov trail practical treat inspection training trail hypothesize predict conditioning training data properly minpath home institution site used access data september training trail approximately test trail approximately time test trail drawn occurred strictly training training testing page requested population site trail link shorter trail minpath link probability threshold probability threshold proved tighter minpath link visitor must follow reach trail visitor shortcut shortcut lead visitor trail visitor shortcut lead farthest trail visitor greedily selects apparently best shortcut shortcut lead page visitor trail visitor follow next link trail visitor erroneously follow shortcut note minpath shortcut page visitor request visitor follow shortcut link trail shortcut trail test link oracle predict visitor trail minpath reduce trail exactly link link link minpath visitor explored relationship threshold minpath threshold stem stem stem training data minpath improves stem threshold fall link trail hypothesize data sparseness learn well thus best threshold ongoing work evaluating threshold minpath substantially training data next minpath column show link followed unmodified site column minpath unconditional markov shortcut last minpath mixture cluster selects distribution mixture trail prefix ignoring past visitor graph demonstrates minpath reduce link visitor must follow mixture markov suggesting shortcut minpath save link markov conditioning outperforms unconditional substantially shortcut suggested markov five shortcut unconditional mixture slight bayes mixture unconditional trail mixture test trail gain unmodified unconditional markov bayes mixture mixture markov minpath column show link followed trail mixture column annotated cluster confidence varying assignment four series represents assignment ferences cluster mixture statistically selecting mixture distribution trail prefix mixture column show assignment type hard soft assignment distribution soft assignment hard assignment visitor past trail visitor trail past trail conclusion soft assignment mixture past trail trail prefix help minpath assignment cluster feature significantly trail prefix bayes mixture slightly worse trail mixture markov surprising prefix past trail valuable apparently even page request trail sufficient assign cluster work investigate remains true site last probability ordinal page request trail unconditional markov positional choosing case minpath significantly differently positional ignoring note minpath time minpath learned offline process minute trail prefix minpath find shortcut fast suggest shortcut real time wireless visitor work perkowitz address shortcut link simpler shortcut prediction page viewed site page viewed trail page requested shortcut page effectively probability visitor eventually view counting occured training data minpath also probability composing page transition probability trail site reduces data sparseness expense hold practice experience speech computational biology suggests outweighs disadvantage minpath relative perkowitz minpath build accurate visitor clustering visitor mixture perkowitz build shortcut visitor site minpath admits versatile shortcut extending minpath calculate shortcut existence shortcut requested page perkowitz take derives recommendation data minpath many trait page recommendation system year letizia lieberman agent browse tandem visitor visitor link visitor followed visitor page bookmark file letizia visitor page minpath resides server letizia constrained resource visitor browsing device thus well suited wireless letizia leverage past experience visitor site letizia know visitor webwatcher joachim ariadne juhne adaptive site agent pazzani billsus tour guide agent help visitor browse site suggesting link visitor view next assistance tour guide visitor follow trail viewed avoid becoming lost tour guide page trail suggesting link page follow next opposed creating shortcut page surflen pagegather perkowitz etzioni suggest page visit page request past suggest page visitor session presenting list link surflen constructing page containing link pagegather system visitor navigate lengthy list shortcut thus perhaps dozen suggested link minpath improves factoring relative benefit shortcut suggesting best link page request predictive work prediction mining work numerous review mention closely work webcanvas cadez system visualizing cluster visitor mixture markov goal build predictive webcanvas emphasizes visualizing cluster sarukkai markov suggest probable link visitor follow note need reduce size clustering work explores well many link link probability sort resulting suggestion conclusion wireless device soon outnumber browser site must prepared deliver suited need high cost navigation device shortcut link fruitful augment made contribution minpath find shortcut link targeted visitor gathering explored predictive evaluated minpath empirical evidence minpath find shortcut link mixture markov minpath save wireless visitor link minpath many fruitful line continued exploring studying minpath scale site page link page traffic automatically selecting concise descriptive anchor text shortcut link integrating minpath framework adapting site elaborate visitor incorporates cost shortcut link probability erroneously shortcut conducting user minpath fielded site anderson anderson domingo weld personalizing site user proc intl conf cadez cadez heckerman meek smyth white visualization navigation site clustering proc intl conf data mining cheeseman cheeseman kelly self stutz taylor freeman autoclass bayesian classification system proc intl conf machine learning dempster dempster laird rubin likelihood incomplete data royal stat fink fink kobsa nill adaptivity adaptability avanti empirical microsoft usability budzik hammond mining navigation history recommendation proc conf intelligent user interface joachim joachim freitag mitchell webwatcher tour guide wide proc intl joint conf juhne juhne jensen adne guided tour system wide proc intl conf lieberman lieberman letizia agent assist browsing proc intl joint conf pazzani billsus pazzani billsus adaptive site agent proc intl conf autonomous agent perkowitz etzioni perkowitz etzioni adaptive site proc intl joint conf perkowitz etzioni perkowitz etzioni adaptive site conceptual framework case perkowitz perkowitz adaptive site cluster mining conceptual clustering page synthesis thesis dept comp univ washington sarukkai sarukkai link prediction path markov proc intl conf wexelblat maes wexelblat maes footprint tool foraging proc conf comp jacobsen dayal user access patter dynamic hypertext linking proc intl conf zukerman zukerman albrecht nicholson doktor trading granularity predictive domain proc intl pacific conf text classifier classification sofus macskassy haym hirsh arunava banerjee aynur dayanik sofmac arunava aynur hirsh rutgers frelinghuysen piscataway considerasupervisedlearningprobleminwhichexamplescontain theuseofa thereby makingthemdirectlyusablebytextclassificationmethods moreimportantly iscompetitivewithmaturenumericalclassificationmethodssuchas machine learning spent many year developing robust quinlan ripper cohen many year numericaland task last also considerable textclassification sebastiani yang treating presence absence ofeach wordasa booleanfeature thisiscommonlyperformedeither generatinga largenumber feature word indirectly feature cohen examplesis viewed feature word retrieval spent many year developing robust retrieval applicable many retrieval task concerning salton best area many year retrieval task last seen task retrieval engine yang chute closely rocchio relevance feedback creating retrieving nearest schapire theironyis thatalthoughwe nowhavemuchexperienceon placing realm numericalclassification little brought effectively brought realm doneeffectivelythey broadening base classification importantly fact many feature came confronting email classification wanted explore text message also message time macskassy know pure classification problematic treat feature take distinct word proceed combinationof true word problemis makesthe dissimilar unrelated token classification like applying feature able discerned classification converting feature feature presumes treat piece text word representing text unorderedset token presentin text word butoccasionallytokensof morecomplexderivation core roughly convert token close token respective farther idea landmark feature assign token landmark feature landmark feature landmark landmark token landmark token used feature converted token containing differing reflect side landmark pick landmark show even fairly naive selecting landmark well appeal body work feature discretization machine learning catlett kerber fayyad irani dougherty kohavi sahami frank witten learning normallyhaveto considera largenumberofpossibletests oneach numericalfeature worst case consecutive pair feature take discretization heuristic mean identify modest tractable subset test learning process thusto applysuch methodsto identifya oflandmarkvalues feature token landmark exactly landmark word used textclassification theremainderof thispaperwe describeourapproach converting feature feature used ourapproach thelearningmethods evaluationmethodsused show performcompetitively ripper data final converting token convert token close done feature landmark feature legitimate analyzing feature take training feature feature token representing exactly token classification task feature representing story artificially invent demonstrate process splitpoint token side splitpoint token feature construction rectangle message thereby feature converted token word feature feature abstractly hypothetical feature plotted line training feature fall note long fall consecutive main themdl discretization find good fayyad irani training data restricting ignore feature harvesting node tree kohavi sahami call density selects yield fall consecutive stay roughly rate rest concluding howwe handleexamplesthat missing feature fayyad irani kohavi sahami make analyse numeric feature splitpoints high gain recursive fashion subset stopping criterion lengthpriciples rissanen feature treated separately yielding splitpoints feature limitation refer reader citation discretization density density algorithmic form obtaining sorting data feature yielding ordered list thus feature desired density feature size rounding token numeric feature maxsplitswhile numsplits maxsplits argmaxfnumerical feature feature numsplits feature divide data train test learner train currerror test currerror maxsplits else lasterror currerror numsplits maxsplits else numsplits numsplits lasterror currerror lasterror density final splitpoints growing geometrically rate reach final density actually twice time volumetric density duplicate time duplicate removed yielding approximately distinct thus feature list whichever yielded rate give final whichever fewer missing occurrence many learning missing feature complicating learning phase assessing feature forming learning classification unavailable simply deleting data feature remove occurrence imputing feature learning something median mean mode training data used learning creating handle missing idea simply token feature feature missing committing token feature mightotherwise neither contributes detracts classification process rely remaining feature assigning label learning briefly learning recall goal demonstrate credibly classification crafted show sampling four text classification text retrieval mentioned label tfidf joachim schapireet sebastiani also probabilistic classification text classification naive bayes domingo pazzani joachim mitchell entropy nigam also ripper rule learning system cohen handling feature handle text classification fairly fashion used part rainbow system mccallum baseline shelf learning release quinlan ripper note ripper mentioned twice originalnumericalfeaturesin fashion used thus ripper used classification used missing handled simply generating token feature ripper used numericalfeatures handling missing tfidf classifier relevance feedback rocchio vector retrieval thisalgorithmrepresentsdocumentsasvectorssothat vector vector corresponds term word tfidf weighting scheme word many time learning phase prototype vector formed negativeexamplesof classify cosine prototype vector vector calculated vector highest cosine naive bayes probabilistic inductive learning posteriori probability belongs feature independenceof feature posteriori probability naive bayes classifier used text classification entropy classifier labeled maxent conditionaldistributionof label feature idea roughly distributionsshouldbepreferedin absenceof externalknowledge derived labeled training data feature characterize expectation distribution lead distribution entropy scaling nigam ripper learning form rule rule test feature rule returned ordered list successful rule prediction label importantly ripper take numeric nominal feature test item part take make ripper particularly convenient text data listing word feature feature word used rule formed greedy fashion rule built time test thatcause ruleto excludeadditionalnegativedata hopefully covering many rule formed sufficient data covered final pruning adjusts rule resulting full rule data widely used tree learning fixed creates tree classify fixed step remaining predicts chooses highest gain creates training subset discrete feature subset thresholdcomparison continuous feature recursively node final threshold tree built prune tree avoid overfitting theuse feature used data repository blake andmerz show datasets feature numeric final designated feature numeric rest discrete learner done stratified kohavi dataset four feature encoding ripper encoding density four encoding four encoding five textclassifications accomplished converting english word five twopointthree represents baseline machine learning classification base dataset instancesfeaturesnumericclassesaccuracy feature bcancerw diabetes glass hungarian ionosphere iris liver musk segmentation sonar vehicle wine arrhythmia auto cleveland horse sponge soitisomittedinanycomparisonsonthemdlmethod property datasets next final simply treat distinct word regard extent yield competitive learning classification show comparing four mdlalgorithm feature classification represents data ripper four text line case inferior line case superior qualitative flavor graph feature methodsto performcrediblyin many case exceedingnumerical methodsin case althoughperformingless successfullyinmanycasesaswell plotin graph comparing four text feature outlier case bottom graph data feature numeric preceding graph collapse eight four text tfidf maxent ripper comparing featurization density coupled four labeling column numeric comparing learning feature numeric learning picture also show many data text beat featurization methodused textmethodcomparedto numericalclassification column label text used used next density case classification come followed ripper show case converting feature data beat learning absolutle ranging high show unconditionally superior classification show merit consideration classification next questionwe featurization dominate mdlvsdens show represents data text learning representing learning representing density learning show neither clearly superior perhaps case going density began stating obvious converting feature convert token word density numeric comparinglearningwith numeric learning density comparing learning feature feature perhaps tokenization effectively show data representing tokenization textclassification represents featurization learning clearly evident tokenization sophisticated noting kohavi sahami discretization simply data ignoring feature splitpoints kohavi sahami show slightly inferior inferior discretization learning imply must case test four text classification show pointrepresentsa data learning represents represents clear final converting numeric feature enabling traditionally solely methcompare tokenization token comparingthe naive tokenization comparing additionto openingup data feature yield numericalclassification show thatin resultingmethodsoutperform highly optimized also importantly show yield vast naive converting numeric textual token many taking work motivation work broaden learning data done work motivated work data requiresa ofbenchmarkproblems something thatdoesnotpresentlyexist thereforeinthe processof creating data process also notedthatour approachyieldsan intriguingway deal data missing benefit liability remains hope explore case comparing learning successful competing case difficult make definitive successful conjecture missing differentdata numeric data unable discern remainsan also plan acknowledgment like thank foster provost lyle ungar rutgers machine learning helpful comment blakeandmerz ofmachinelearningdatabases catlett onchangingcontinuousattributesinto proceeding european session learning berling cohen fasteffectiveruleinduction proceeding twelfth machine learning laketahoe california cohen domingosandpazzani beyondindependence conditionsfortheoptimalityofsimple machine learning dougherty proceeding machine learning fayyadandirani joint frankandwitten proceeding machine learning slovenia joachim fourteenth machine learning kerber proceeding artificial intelligence menlopark kohaviandsahami proceeding data mining menlopark kohavi proceeding joint artificial intelligence sanfrancisco kaufmann macskassy emailvalet learninguserpreferencesforwireless email proceeding learning user stockholm sweden mccallum atoolkitforstatisticallanguagemodeling textretrieval mitchell machine learning mcgrawhill nigam inproceedings machine learning informationfilteringworkshop stockholm sweden quinlan machine sanmateo rissanen encyclopedia statistical rocchio smart retrieval system inautomatic processing salton schapire sigir sebastiani machinelearninginautomated textcategorisation yangandchute transaction system yang categorization retrieval association rule siegfried nijssen joost snijssen joost leiden leiden niels bohrweg leiden netherlands combine association rule resulted usable little payed idea turn step process discovering rule frequent item coined farmer show idea much comparable association rule formalism association rule introduced agrawal basket step rule construction frequent item item bought supermarket transaction step time obligatory reasonably fast much done wellknown resulting apriori many variant aprioritid agrawal introduced pijls bioch hand done extend usability association rule case basket dehaspe raedt atom item incorporation inductive rule also take background consequently also data mining data spread reasonably merged warmr usefulness demonstrated dehaspe also showed major shortcoming proved even taking gain tackling property warmr remove need prolog sophisticated datastructure borrowed apriori depend time consuming test equivalence ressemblance developped blockeel tackle time consuming step warmr test equivalence subsumption step alternative restriction show warmr show considerable warmr summarize association rule work introduced warmr fourth introduces modification verified giving fifth sixth concludes apriori apriori introduced pijls bioch task apriori database subset item discovers frequent item subset exceeds predefined threshold item size item property subset transaction property turn levelwise apriori frequent infrequent breadth levelwise size process repeated counting item item trie datastructure step trie datastructure path root node corresponds item leaf deepest correspond item path reach deepest maintained correspond frequent item displayed dotted line sake clarity trie used fashion step counting tree traversal transaction item transaction child checked recursively leaf reached item increased step frequent item child consisting frequent brother expanded frequent brother copying item take care generating item step distinguishes apriori tree round procedure efficiently construct merely copying node counting phase pass tree existence transaction subset transaction hash node counted make variant apriori suitable warmr item dehaspe raedt atom also refer nearly existentially quantified ordered free illustrated horn clause thought describes client buying refers containing property client sequel abbreviation show combined elegantly item formalized binding proved binding proved horn clause base prolog item straightforward case atom also many possibility bias warmr refinement operator mode mode prescribes mode parameter parameter introduces last parameter parameter mode call parameter parameter parameter parameter parameter mode many time mode atom item turn difficult reasonable subset atom subset implication warmr subsumption atom subsumes atom substitution induces equivalence property also hold atom frequent size infrequent size warmr refinement unless restriction remove apriori infrequent restriction remove previously frequent illustrate mode lead clause logically imply eachother major warmr heavily good subsumption prohibitive kietz lubbe farmer modification warmr make subsection base taking closer look mode seen mapped procedure fact mode mode procedure mode procedure farmer idea incorporated binding mode procedure type boolean procedure vector true false outputting procedure vector vector empty need entirely used kind procedure used mode parameter kind used outputting mode data base prolog fact accessed procedure mode multidimensional matrix allocated matrix corresponds truth list fact read mode matrix accordingly take time truth atom matrix core specified fact procedure farmer differs warmr manipulates trie datastructure bias trie datastructure tree path root leaf trie tree used counting generating trie farmer counting giving capital refers atom tree capital refers assignment containing mapping true atom proved assignment aforementioned procedure used assignment assignment give occuring remaining unbound assignment aforementioned outputting procedure used tree traversed recursively leaf disable true else child disabled true disable initially node enabled node checked long child satisfied idea also blockeel show integration procedure mode giving afterwards warmr idea atom moved beginning hereby violating mode atom violation atom subdivided atom last atom call dependent atom atom copy last atom name atom superscript construction tree subdivision used trie ordered mode trie expanded expand child expand else frequent child left dependent atom frequent brother copy allowed else remove tree assumed typed bias frequent superscript also case used node serf introducing atom previously atom introduced mode deterministic used dependent atom brother eachother take care subset afterwards infrequent keeping child subset equivalently permutation dependent atom give name make sure remain intentionally separated repeating node desirable many case bias explicitely duplication atom allowed atom kind well distinction introduced repeating node case exchanged parent identical node overcome also disadvantage atom absence seen farmer prune many warmr show restriction bias make sure farmer warmr used prune infrequent counting remove mean used latter influence infrequent even pruned equivalence determining mean strict equality substitution unordered atom substitution correspondence expressed plotkin clause clause imply plotkin atom clear also hold thus combining prof also atom never subsumption differ show restricted bias farmer atom show farmer atom substitution restricted bias farmer subsume eachother redundancy restricted bias obey rule used repetition atom attom differs name allowed mode parameter differ unless parameter parameter rule prevents rule disallows bias consequently remains redundancy restricted bias farmer atom trie redundancy restricted bias show subset never true must atom mapped atom pair construct substitution bias atom must restriction must restriction tree procedure atom brother construct substitution unifies substitution allowed mode parameter whole atom introduced used possibility atom outputting parameter atom never mode used farmer atom atom whatever atom occured mode allowed restricted bias thus redundant redundancy restricted bias farmer never substitution ordered atom deterministic numbering used note must size substition thus substitution eachother suffices find permutation atom followed renumbering make show farmer generates permutation restricted bias atom unordered atom mode parameter determines atom depicted graph atom bias node node incoming arrow incoming arrow outside mode deterministic numbering corresponds farmer corresponds dependent node tree node node take care recursion acting sort lifo queue redundancy restricted bias farmer never eachother farmer warmr datasets used warmr tree datastructure blockeel fair blockeel revealed warmr bongard bongard dataset bongard artificial task discover eachother redundancy restricted bias used depicted show find farmer case also bias time paying fact scale logarithmic considerable dataset frequent itemsets farmer test case binary digit dataset used binary coded used apriori pijls bioch farmer many introduced comparable depicted dataset frequent itemsets rapidly farmer warmr bongard dataset time note scale digit dataset frequent itemsets time time consumed itemset note scale logarithmic axis beneath time time graph farmer comparable apriori exponentially warmr behaviour high time carried graph combined time itemset make clear react find time warmr time decrease overhead itemset farmer explained hooked acceptable decreasing explained overlapping itemsets conclusion work introduced discovering tree datastructure showed restricted type bias warmr much believe restricted bias considerable expressive propositional association rule looking possibility overcome restriction case restriction lifted rule considerably investigating possibility tree sophisticated default plan successfully database million find behaviour farmer datasets size agrawal rakesh agrawal heikki mannila ramakrishnan srikant hannu toivonen inkeri verkamo fast association rule data mining page blockeel blockeel dehaspe demoen janssens ramon vandecasteele pack proceeding inductive bongard bongard hayden book company spartan book dehaspe raedt dehaspe raedt mining association rule dzeroski lavrac proceeding inductive volume page dehaspe dehaspe toivonen king frequent substructure chemical compound agrawal stolorz piatetskyshapiro data mining page aaai kietz lubbe kietz lubbe subsumption inductive wrobel proceeding inductive volume page gesellschaft mathematik datenverarbeitung nijssen nijssen data mining master thesis leiden pijls bioch pijls bioch mining frequent itemsets database postma proceeding eleventh artificial intelligence page plotkin plotkin note inductive meltzer michie machine intelligence page edinburgh edinburgh feature text classification soucy mineau dept universit laval qubec canada text classification main identify word best suited classify discriminate feature identify word feature computationally combine well feature criterion gain selects feature feature composed best feature high gain brief avoid considering feature discrimination sufficiently covered feature size feature used characterize feature predetermined parameter experimentation automatic text classification main identify word best suited classify predefined feature identify word many feature relevancy term regard classification criterion gain assessed word encountered text chooses best word lewis ringuette computational learning best suited learn rate maximized case little guidance naive predetermine learning data data even data used learn opinion arbitrary unless data characterized previously tested data feature computationally major text classification corpus like many thousand feature filter hall smith learning process feature corpus combine feature best text classification yang pedersen thresholding gain selects feature coocurrence training feature composed best feature high gain pool feature predetermined parameter threshold experimentation also show threshold used domain render good term relevancy feature extracted classification testbeds learning feature used learning widely used text classification retrieval knearest neighbor feature weighting rainbow naive bayes classification text classification mccallum frnkranz lack feature rainbow poorly task name dataset balance webkbcourse reuterscornweat wwwprisoner wwwbeethoven data used data binary classification task size data balance training testing note training voluntarily kept believe manual labeling prohibitive appendice briefly describes weighting tested nonweighted success data used data previously studied work data data used data categorized type concise vocabulary freely former data reuters brief news webkb page mainly retrieved latter noisy data consisting email belonging junk email prisoner show beethoven composer page retrieved altavista news unmoderated usenet newsgroups history christianism difficulty classification task highly type vocabulary used scott matwin word stemmed porter rainbow feature gain text classification reduction entropy heterogeneity gained term presence absence membership used text classification joachim nigam sahami used tree learning best step tree construction quinlan best feature gain text classification task word rank highest highly kind corpus task bayesian classification many noticed naive bayesian classifier work best many feature feature used noticed seemed feature leading reasonable varies feature craven feature sahami corpus show great size vocabulary bayesian classifier suffer irrelevant feature kept feature difficult justify predefined feature optimize rate classifier actually data reuters word fewer feature need data involving newsgroups email message seems obvious feature highly corpus learning characterize corpus term feature guideline proper inferred proposes term composing vocabulary regard discrimination relative gain threshold feature look feature individually regard classification task feature threshold yang perdersen relevant bayesian classifier easy feature characterize handful probability distribution whole bayesian classifier rely probability infrequently used dimension noise anything else matter fact seems fewer feature keep best feature joachim bias mostly kind vector weakness fact feature seemed successful cosine assigns feature feature promising term characterization presence many irrelevant feature diminish influence good relevant feature choosing fixed feature lead irrelevant reuters data high remove good feature junk email classification even good bayes classifier looking data individually noticed restrict feature even reuters data binary classification task classifying relevance corn wheat word best high feature classification alternative case useless complementary cooccurrence highly ranked feature eliminate alternative high feature conducted extensive feature ranging threshold ranging best threshold classify data identify term feature covered feature term discrimination explained refinement term improves classifier interpretability characterization produced feature sensibly feature proposes refinement feature offering threshold removing feature cooccur high feature cooccurrence kind dependency feature weakness statistical used characterize feature computational dependency term ignored identify subset term know relevant classification task hand comp dependency term preselected term cooccurrence feature mcooccurrence containing word corpus sorted gain decreasing pool pool igain thres cooc thres nbkeep pool nbkeep feature word pool igain thres elsif cooc pool cooc thres endif endfor cooc pool pool nbkeep feature cooc pool cooccurence probability word pool considering whole training mean pool probability find take word corn perfectly divide training starch corn starch corn corn starch unlikely bring term discrimination feature removed needle reality perfect removing feature totally covered feature term discrimination remove feature aggressive coverage term thus effectively reduce vocabulary feature meaningful term discrimination mean cooccurrence term term nbkeep highest ranked feature pool nbkeep cooc thres empirically data prisoner beethoven news test conducted previously studied data reuters webkb prevent fitting parameter data lack show parameterization concluded test used hold data seem hold data experiencing show feature tested corpus learning feature column rainbow bayesian classifier last five column show feature used thereafter learning subsequently column show feature threshold gain feature threshold assessed empirically ranging resulting cooccurrence refinement feature column feature significantly rate eliminate useless feature term prediction learning used help characterization learned vocabulary facilitates reduces computation cost classify wondered feature reached fine tuning used feature corpus rate column obviously feature case fluctuation rate great corpus term feature considering good rate surprising fluctuation rate classifier tends show threshold validated feature reduce removing useless feature also suggests type corpus sound relevant impossible bayesian classifier observe phenomenon forth column give feature rate training rainbow feature also trained rainbow feature fifth column suggested craven comparing forth fifth column feature case word discrimination like reuters beethoven corpus despite loss feature nevertheless significantly undoubtedly help learned next sixth column cooccurrence refinement reduce even size feature rate facilitating even learned feature feature slightly rate seventh column unbounded feature rate bayesian classifier last column illustrated phenomenon also hold classifier high relevant much case text classification need proper training classifier relevant size vocabulary size pool choosing cooccurrence vocabulary pool avoid possibility pair feature producing assessing cooccurrence term computing feature data cooc bayes bayes cooc bayes bayes feature bayes webkbcourse reuterscornwheat wwwprisoner wwwbeethoven feature feature data feature learning feature summation column calculates classified data ordering corpus dominating assessment cooccurrence term consequently work independently done resulting initially assessing feature vocabulary consequently cooccurrence feature asymptotic feature solely computes feature feature hidden doubled nevertheless computationally conclusion work feature text classification refines feature previously criterion refinement cooccurrence feature predetermined subset highly ranked feature pool hoping avoid feature discrimination regard task partially covered feature pool seek classification learner reduce used learned show reached corpus studied certainly proved promising type text classification task demonstrated glance tempted cooccurrence remove feature brings conducted validity investigation also conducted cooccurrence seems feature bayesian classifier feel probability ponder involvement explanation feature failed surpass used despite extensive experimentation wide domingo pazzani demonstrated naive bayes learner well spite word independence investigation word dependence also extend cooccurrence handle learning conducted binary classification task investigation multistrategic feature gain feature base feature tree built feature vocabulary accordingly cooccurrence threshold extensive experimentation parameter classification task regard type corpus learning bias mainly neighbor weighting adjusts neighbor weighting contribution classification feature studied used assessment feature surprisingly simplest worked feature feature feature word feature goal reached feature stretch neighborhood irrelevant feature feature weighting tested feature word feature frequency feature tfidf term frequency also tried many variant squaring feature high assignment experimented euclidian cosine cossim craven craven dipasquo freitag mccallum mitchell nigam slattery learning extract wide departement carnegie mellon domingo pazzani domingo pazzani independence optimality bayesian classifier machine learning frnkranz frnkranz mitchell riloff case linguistic phrase text categorization note learning text categorization hall smith hall smith feature machine learning comparing correlationbased filter wrapper proceeding florida artificial intelligence symposium joachim joachim text categorization vector machine learning many relevant feature proceeding european machine learning springer lewis ringuette lewis ringuette learning text categorization annual symposium retrieval vega mccallum mccallum toolkit statistical modeling text retrieval classification clustering http nigam nigam lafferty mccallum entropy text classification machine learning filtering quinlan quinlan induction tree machine learning sahami sahami dumais heckerman horvitz bayesian filtering junk learning text categorization wisconsin salton salton automatic text processing transformation retrieval scott matwin scott matwin feature text classification proceeding machine learning morgan kaufmann publisher yang perdersen yang pedersen comparative feature text categorization proceeding fourteenth machine learning link eigenvectors stability andrew zheng michael dept statistic berkeley berkeley berkeley berkeley berkeley berkeley pagerank eigenvector identifying authoritative influential hyperlink citation give consistent answer surely desideratum address give stable ranking perturbation hyperlink tool matrix perturbation markov stable give instability violated also briefly modification improves stability year seen growing identifying authoritative influential webpage hyperlink citation data kleinberg google pagerank brin page attracted many also osareh bibliometrics literature eigenvector assign originally link readily citation academic citation graph link pagerank relates authoritativeness embodied thus user constitutes authoritative page domain pagerank evaluated user subjective flavor stability link upon perturbation link collection view stability desirable feature link authoritativeness embodies truly authoritative influential surely link citation make mind site influential even fixed link dynamic unreliableinfrastructuresuch give view occasion ideally link insensitive perturbation matrix perturbation coupled markov characterize stability rank pagerank improving stability also briefly metioned algorithmic studied empirical cora database mccallum collection containing citation thousand pagerank subset cora database consisting machine learning stability also constructed five perturbed database base deleted cora database crawl chance mishap retrieved truly authoritative hope identify subset base column rank full machine learning five rightmost column rank perturbed database substantial genetic optimization goldberg natural artificial system holland genetic koza genetic jong crossover genetic syswerda artificial intelligence simulated fogel survey evolution optimization control parameter genetic grefenstette genitor pressure whitley genetic data michalewicz genetic automatic discovey koza learning learning predict temporal sutton machine learning checker samuel neuronlike difficult practical learning tesauro classification scene classification regression tree repository machine learning database irrelevant feature subset induction probabilistic reasoning intelligent system pearl thought variability intrinsic case pagerank much stable genetic optimization goldberg learning natural artificial system holland classification regression tree probabilistic reasoning intelligent system pearl genetic koza learning predict temporal sutton classification scene likelihood incomplete data repository machine learning database parallel distributed processing neural computation stated outset conclusion unstable pagerank subtle involving consideration relationship eigenvectors subspace wish suggest stability need turn brief pagerank followed pagerank collection page academic linking pagerank implicitly construct matrix capturing citation determines computing eigenvector kleinberg posit high linked many page high page high link many authoritative page precisely page retrieved response form adjacency matrix page link page iterates mean page link page vector renormalized unit also iteration initialized vector obtaining eigenvector matrix golub loan mild eigenvectors authoritativeness page likewise pagerank page adjacency matrix previously pagerank brin page construct probability transition matrix renormalizing imago surfer time step page decides page visit next step probability pick hyperlink page jump page link probability reset jumping page picked parameter process defines markov page transition matrix transition matrix transition probability vector pagerank stationary distribution markov equivalently eigenvector transition matrix golub loan stationary distribution satisfies asymptotic chance visiting page authoritativeness page showing collection page eigenvectors returned collection page page linking http page jittered scatterplot hyperlink graph linking http adjacency matrix zero column page eigenvector jittered scatterplot link page eigenvectors eigenvectors five page trickle collection happen link show plot eigenvectors changeddramatically principaleigenvector near line thus relatively perturbation collection caused phenomenonis pervasive need addressed eigenvectors next give characterization suffer eigenvector show stability eigenvector perturbation eigengap largest largest eigenvalue shed eigengap plot contour matrix solid line dashed line additive perturbation made eigenvalue matrix indicated ellipsis matrix eigengap perturbation ellipse eigenvectors away eigenvectors matrix eigengap perturbed eigenvectors nearly eigenvectors size eigengap affect stability eigenvectors reader contour matrix eigengaps miliar plot multivariate gaussians also think contour gaussian perturbation imposed inverse covariance matrix sequel tilde perturbed quantity perturbed give long eigengap insensitive eigenvector eigengap page perturb graph deleting link page perturbed eigenvector perturbed matrix satisfies eigengap insensitive perturbation proved showing eigenvector much magnitude relevant eigenvalue much eigenvector overtake eigenvector frobenius matrix perturbationtheory stewart symmetric matrix eigenvalue eigenvector eigengap symmetric perturbation hold eigenpair eigenpair denominator complementary eigenspace orthonormal column eigenvectors diagonal eigenvalue also simply reversing link interchanging frobenius norm hold largest eigenvalue stewart show turn eigenpair deleting link page perturbation straightforward show thus norm perturbation hold arrive also ensures also denominator previously stated next give converseof eigengap eigenvectors sensitive perturbation symmetric matrix eigengap eigenvector diagonalized orthogonal column eigenvectors column pick norm perturbation eigenpair orthogonal ground illustrate give fact graph connectedcomponents principaleigenvalue node largest picture connectedcomponent moreformally componentwith largest eigenvalue imagine subset much graph solid arrow hyperlink dashed arrow represents link eigenvalue link easy jump part graph subcommunities originally biggest subcommunity eigenvalue link graph biggest subcommunity eigenvectornow node zero elsewhere pagerank analyze sensitivity pagerank perturbation eigenvector transition matrix pagerank satisfies thus close show long page high pagerank unperturbed pagerank perturbed pagerank construct coupled markov pair drawn probability vector stationary distribution pagerank surfer transition work step decide probability reset case page collection reset unperturbedpages page linked page case page linked page independently page linked page thus coupled markov former transition probability latter transition correlated reset transition distribution asymptotic distribution must note letting perturbed page derive used fact construction perturbed page fact iterating term asymptotic thus drawn stationary distribution correlated marginal distribution chance taking distribution must precisely coupling lemma aldous variational distribution must also quantity show concludes connection latent semantic indexing deerwester insight stability also cohn chang collection matrix word vocabulary computes left vector equivalently eigenvectors left vector dimension vocabulary size strength word membership informal hope synonym grouped vector column projected onto subspace spanned vector automatically expanded synonym word leading retrieval constructing citation graph node word node word link eigenvector eigenvector italian french english italian french english corpus node adjacency matrix graph graph find none anything documentnodes vector exactly left vector connection transfer insight vein conducted corpus sampling english french italian corpus distinct retrieval clustering synonymidentification exceedingly interested stability stability collection examined eigenvectors eigenvector high dimensional english french italian degree eigenvector fifteen independentrepetitions process carried plotted despite presence clear cluster corpus eigenvectors highly variability persists eigenvectors corpus taking paragraph novel word vocabulary consisted word collection also manually balanced equally done picking vector proportional frequency word english thought canonical english taking english absolute magnitude french italian note variability inherent feature variant algorithmthat brieflydescribe studied significantly perturbation cora database also page recall cora database subset database perturbation subset deleting used cora base largely replicated cohn chang returned genetics database perturbed returned seminal broader area repeating excluding slightly five trial classification regression tree classification scene repository machine learning database learning irrelevant feature subset classification rule well holte machine learning quinlan probabilistic reasoning intelligent system pearl induction learning boolean monk thrun inferring tree quinlan discretization continuous learning pathfinding conservation schaffer feature likelihood incomplete data learning predict temporal sutton neural computation unifying view mitchell robust layered control system robot brook ranked remaining unstable pearl bookwas originallyranked trial dropped rank brook rank jumped rank trial variability intrinsic pagerank pagerank classification regression tree probabilistic reasoning intelligent system pearl learning classification scene robust layered control system robot brook likelihood incomplete data learning predict temporal sutton repository machine learning database recipe parallel distributed processing neural computation largest rank drop much stable closer examination review jump ranking pagerank tended remain fairly also carried page kleinberg describes obtaining collection page exactly perturbed natural sake brevity give player long truncated http http http http http http http http http http http http http http http http http http http http pagerank returned http http http http http http http http http http http http http http pagerank ranking undergo ranking mass flipping perturbation pagerank fourteen nineteen displayed mass flip roughly trial accordance removal rate time italian recipe note mean page removed trial perturbation rank http http http http http http http http http http http http http http http http http http http http http http pagerank hand returned http http http http http http http http prov http http http http http http well linear algebra subspace spanned eigenvectors stable perturbation eigenvectors stewart fact outputof algorithmis subspace stability consideration matter concern case goal data onto subspace wish interpret eigenvectors stability matter serious concern eigenvectors term seen empirical exercising considerable caution principaleigenvectormay reliable utilize eigenvectors kleinberg suggested examining eigenvectors obtaining problematic interpret eigenvectors fact variability eigenvectors alternative automatically combine eigenvectors identifies subspace framework explored fact pagerank relatively immune stability concern matter considerable belief pagerank feature regard explore incorporates feature construct markov probability follow hyperlink page forward time step follow hyperlink backwards even time step probability reset page asymptotic visitation distribution step even step show insensitive perturbation pagerank well runningthis significantly stable also explored acknowledgment thank andrew mccallum cora citation data used also thank andy zimdars helpful comment work muri grant aldous david aldous walk finite rapidly mixing markov dold eckmann seminaire probabilit xvii lecture note mathematics page bharat henzinger bharat henzinger distillation hyperlinked proc annual intl sigir page brin page brin page anatomy hypertextual engine seventh wide chung chung spectral graph american mathematical cohn chang cohn chang probabilistically identifying authoritative proc machine learning deerwester deerwester dumais furnas landauer harshman indexing latent semantic american golub loan golub loan matrix computation john hopkins univ kleinberg kleinberg authoritative hyperlinked proc symposium discrete mccallum andrew mccallum kamal nigam jason rennie kristie seymore automating contruction internet portal machine learning retrieval andrew zheng michael stable link proc annual intl sigir osareh farideh osareh bibliometrics citation review literature libri page lawrence page sergey brin rajeev motwani terry winograd pagerank citation ranking bringing unpublished manuscript stewart stewart matrix perturbation academic learning probability ranking maytal foster provost system leonard stern school business york many supervised learning task costly training data label learning acquires data incrementally learned help identify data labeling empirical learning focused learning classifier many probability membership used rank case learning probability ranking selects data labeling variance probability learning bootstrap sample labeled data show empirically reduces data item must labeled wide data also uncertainty sampling learning maximize classification show dominates surprisingly also preferable accelerating maximization supervised classifier learning data label many procuring label costly learn diagnostic need analyze many historical case learn classifier need read many assign label learn response consumer costly incentive reveal preference learning process training data incrementally learned particularly helpful training labeling successful learning reduce must labeled particularly empirical learning address classification task assign case fixed many classification probability membership probability cpes combined minimize cost maximize benefit target marketing probability respond combined profit produced zadrozny elkan ranking case flexibility user agree turney turney machine learning system able take cost well labeling cost training size learning curve data learning accurate cpes ranking show desired learner horizontal axis represents training data vertical axis represents rate probability produced learned learning curve show rate decrease training data used curve represents decrease selecting training data curve represents learning curve form banana early curve comparable learning learning curve soon accelerates careful training data data catch learning bootstrap sample training data examine variance probability data show empirically wide data decrease labeled accurate probability alternatively probability fixed training data also show surprisingly even maximization learning work learning long history machine learning simon winston simon machine learning type learning simultaneous hypothesis searching hypothesis affect searched winston best next learning near miss miss subsequently showed training data substantially carefully angluin valiant term learning coined induction control training cohn labeled unlabeled inducer stopping criterion actively phase stopping criterion next phase inducer effectiveness subset size remove label estimator induced final labeled generic learning generic learning learner labeled subsequently phase unlabeled predefined labeling exhausted phase effectiveness contribution reflecting magnitude contribution learning simply contribute ranked effectiveness labeling phase computational label querying learner next cohn cohn identifying uncertainty inconsistent uncertainty redetermined phase main practical uncertainty increasingly difficult uncertainty initially span domain well understood rendering process ineffective closely committee seung classifier sampled disagree labeling pose computational practical particularly assumes existence hypothesis sampling well data also address learning classification target prediction membership evenly binary ensemble classifier alternatively probabilistic classifier assigns near indicating uncertainty lewis gale lewis gale uncertainty sampling probabilistic classifier employed probability membership closest labeling mamitsuka mamitsuka classifier classifier close evenly iyengar iyengar classifier assign wrong employ classifier assign unlabeled informative learning misclassified ensemble classifier maximizing classification optimizing cpes ranking concern closely cohn cohn statistical learning phase expectation variance resulting training variance modeling variance variance cohen domain well computation closed form learner variance render impracticable arbitrary used arbitrary actively sample learn probability cpes pertains binary label wish evidence pertaining poorly understood subspace ideally indication probability discrepancy probability true probability true probability variance variance refers variance high infer difficult learner data thus desirable learning interpret indication probability well learned contrary difficult decrease likelihood variance empirically bootstrap subsamples efron tibshirani inducer subsample estimator variance cpes estimator determines probability sampled proportional variance cpes distribution sampled probability estimator assigns belongs arbitrary variance identical probability minority estimator normalizing distribution labeled sampled unlabeled inducer stopping criterion sample size stopping criterion bootstrap subsamples inducer subsample induce estimator sample probability distribution subset remove label estimator induced note case equally training data high variance domain minority high probability relatively understood variance majority high probability latter case probability exhibiting high variance simply lack minority training data benefit oversampling respected divide variance probability minority sample interested comprehensible tree probability applies learning cpes particularly probability estimator probability tree unpruned tree quinlan laplace correction cestnik leaf laplace correction cpes produced bauer kohavi provost provost domingo evaluating true probability distribution estimator probability true probability membership probability induced phase best estimator surrogate true probability induced superior probability bauer kohavi provost provost domingo mean absolute estimator bmae cifically probability byeb probability examined data machine learning repository blake used previously rulelearning cohen singer data mapped estimator induced inducer training size sampled show size labeled sample size data data phase data varying phase averaged data labeled unlabeled test estimator evaluated control used banana curve show relative data learning refers estimator induced decrease initially exhibiting fewer demonstrates actively labeled informative inducer construct estimator fewer data exhibit even dramatic show pendigits data training learning curve pendigits data achieves data succeed accelerating learning much weather data note neither curve consistently resides comparable training size learning curve weather data data motivation applying learning learning fewer pertaining gained column show phase produced fewer call fourth column show percentage gained applying gain calculated used used percentage calculated used natural banana even ideal case estimator induced sample considerably final phase thus well merely indication superior also observe part banana benefit learning concentrated stable assessment best gain largest gain column show gained gain viewed tandem column fact banana graph also pertaining rate achieved column reduction sampling phase exhibiting highest reduction data training considerably even data used training reduction also last column gain percentage reduction latter referred maximal gain adult data exhibited gain constitutes data used training data phase gain gained gained gained gained maximal gain abalone adult breast contraceptive hypothyroid optdigits pendigits weather yeast german database letter plot tried comprehensive perspective superiority gain reduction maximal gain comparable inferior seen bold data exhibited superior particularly data used data gradually improves fewer pertain maximal gain much data weather exhibited gain showing fewer phase examined exhibit comparable learning curve data pertaining gained gain complement insight gained help difficulty reduction term reduction gain show data exhibited gain exactly indicating exhibit superior learning curve also assessed alternative bmae mean squared bauer kohavi well area curve bradley evaluates ranking agree bmae lead fatter curve fewer data exhibit insignificant training seem contribute reduction rate learning curve atypical weather data bring reduction decreasing marginal reduction easy good learning contributes reduction equally regardless many used training thus intelligent learning learning significantly comprehensible many best probability learner experimented comprehensible markedly superior cpes bauer kohavi provost provost domingo agree particularly data exhibited gain data data show exhibited achieves lowest considerably fewer learning curve hypothyroid data uncertainty sampling lewis gale binary text classification sample well understood classification well exhibited uncertainty sampling baseline uncertainty sampling data phase gain gained gained gained gained maximal gain abalone adult breast contraceptive german hypothyroid optdigits pendigits weather yeast uncertainty sampling exhibit markedly superior uncertainty sampling particularly superior data data exhibit comparable uncertainty sampling exhibit superior data consistently probability data superior reduction demonstrate solid uncertainty sampling probability data uncertainty sampling inferior emphasize uncertainty sampling probability classification also uncertainty sampling improving classification cpes classification necessarily probability even classification well understood thereby wasting classification classification show data exhibited superior maximization uncertainty sampling superior data exhibited comparable remaining superior uncertainty sampling classification task viable yield much interestingly case dominate phase uncertainty sampling surpasses phase phenomenon demonstrated data classification rate recall uncertainty sampling cpes contribution learning sensitive poor cpes produced phase undermine data uncertainty sampling hand phase accurate probability process contrary early improving cpes well even early learning curve waste actively improving classification phase uncertainty sampling switch open conclusion limitation introduced learning fewer labeled training data probability fewer labeled data showed empirically remarkably well also showed competitive uncertainty sampling even maximization inspecting last also suggests even identify particularly informative training economize labeling cost address computational concern lewis catlett lewis catlett computationally intensive need induce phase bootstrap sample learning curve training size marginal reduction insignificant learning sampling employed thus intelligent learning early part curve relatively used training long training remains relatively induction sample constitute considerable computational toll labeling cost computational cost concern accurate ranking costly labeling acknowledgment thank vijay iyengar helpful comment faculty partnership award mamitsuka mamitsuka learning boosting bagging proceeding fifteenth machine learning angluin angluin learning machine learning bauer kohavi bauer kohavi empirical voting classification bagging boosting variant machine learning blake blake merz repository machine learning database irvine california http bradley bradley area curve machine learning cestnik cestnik estimating probability crucial task machine learning proceeding ninth european artificial intelligence sweden cohn cohn atlas ladner learning machine learning cohn cohn ghahramani learning statistical artificial intelligence cohen singer cohen singer fast rule learner efron tibshirani efron tibshirani bootstrap chapman hall iyengar iyengar apte learning adaptive resampling sixth sigkdd intl conf data mining lewis gale lewis gale training text classifier lewis catlett lewis catlett heterogeneous uncertainty sampling proceeding eleventh machine learning provost provost fawcett kohavi case comparing classifier proc intl conf machine learning provost domingo provost domingo welltrained improving probability tree ceder stern school business quinlan quinlan machine learning morgan kaufman mateo california seung seung opper smopolinsky committee proceeding fifth annual computational learning simon herbert simon glenn rule induction unified view gregg cognitiom chap potomac erlbaum turney turney type cost inductive learning learning stanford california valiant valiant learnable communication winston winston learning structural psychology winston york zadrozny elkan zadrozny elkan learning cost probability unknown dept diego january machine learning data mining machine learning learning phase transition edge alessandro serra attilio giordana lorenza saitta dipartimento scienze tecnologie avanzate piemonte orientale corso borsalino alessandria italy learning fail learn involving resides emergence phase transition covering test proposes alternative learning combining monte carlo stochastic deterministic main benefit hand substantial term increased learning possibility cost learning target giordana giordana learning hard target involving presence phase transition covering test prosser giordana saitta abrupt probability inductive hypothesis hypothesis size reach learner discriminant hypothesis phase transition giordana giordana saitta heuristic used guide learning quinlan botta giordana consequence induction process blind step path lost major contribution reported giordana thorough learner formal difficulty learning term density target hypothesis probability detecting also target phase transition induction combining monte carlo stochastic brassard bratley deterministic partially avoid pitfall fail jump hypothesis gain heuristic good chance successful continuing exploiting classical hill climbing analyzed probabilistic framework difficulty induction task experimentally evaluated hard induction giordana show good agreement covering test learning step machine learning covering test checking belonging learning verifies hypothesis mitchell learning ground literal belonging muggleton raedt literal containing covering test universe substitution satisfies searched process show phase transition universe literal hypothesis hogg prosser botta graph plane probability reported covering botta refer phase transition locus probability phase transition located correspondence waterfall left phase transition yesregion high plateau verifies hypothesis phase transition plateau verifies hypothesis steep step corresponds mushy probability drop reach reported giordana botta phase transition covering test hypothesis four binary tuples probability peak phase transition move dependency phase transition upon hypothesis induction learning conjunctive pair represents covering test plane learning corresponds cloud giordana case learning corresponds inductive hypothesis built learner good cloud plane induction process proceeds generating hypothesis family line line parallel axis investigate presence phase transition plane affect learner extensive experimentation done artificially learning reported giordana recalled base line work artificial learning procedure plane sampled plane learning built target built training test collection size sampling binary balanced training test plane modified also also giordana training test amples experimentation done foil quinlan accurate learner botta giordana anglano also tried reporting agreement foil case summarized tried learning generating lying close edge phase transition giordana saitta phenomenon systematically artificial real learning soon hypothesis blind spot located mushy target tried learner able find good target surprisingly learning solvable induction located phase transition computational inductive hypothesis lying mushy render whole induction process infeasible giordana saitta suggested stochastic expense correctness learning foil failure legend success legend contour plot corresponds probability covering test induction misled possibly heuristic clearly foil learner fail blind spot preliminary showed heuristic relying hypothesis quinlan botta giordana rissanen reliable wrong target plot hypothesis clear induces even deviation actually deviation learning even target distinguished purely hypothesis contrary hypothesis much expect find four conjecture agrees fact many actually foil even target four reported line corresponds phase transition target four show foil hypothesis deviation four acceptable contrary show target four foil able find reported corresponds fact closer phase transition observe probability approximated literal fact literal subformulas well expect difficulty learning decrease conclusion learning existed heuristic unable grow inductive hypothesis hand find learning capable cope inside stochastic absence reliable heuristic stochastic alternative combined deterministic monte carlo deterministic brassard bratley step creates hypothesis literal sufficient reach border yesregion mushy sampling hypothesis step guided gain heuristic halt hold foil four hypothesis containing plane foil figura hypothesis close mushy make hillclimbing guided gain locally best hypothesis trial replace target conjunctive hypothesis monte carlo long find good learning want answer expect mean trial reach probability specially lucky case evidence guided gain surely find happens explores gain reliable probability probability step probability generic literal ratio literal hypothesis literal analytical serra binary contour plot proportion subformulas target parameter curve sion contour plot ratio decrease exponentially remains exponential decay upon upon plot dependency upon case dotted line case continuous line phase transition consequently magnitude learning much learning four stated learning occurring proportion evaluated edge phase transition probability improving stochastic graph trial reach confidence planned trial reach confidence high stop next step trial returned acceptable stop step cost generating evaluating hypothesis sampling phase much cost hypothesis evaluated step experimentally term literal hypothesis cross phase transition term alternative step hypothesis evaluated hillclimbing phase magnitude stochastic trial worth noticing gain heuristic reliable hypothesis also scored learning prefixed mean assumed mean consideration practice trial reach confidence parameter hypothesis rank hypothesis gain ranked hypothesis step best ranked hypothesis best step hypothesis looking promising thus computational parameter arbitrary confidence guaranteed reached practice relatively tends tested learning reported trying laying started hypothesis parameter approximately corresponds edge phase transition even foresees able find also many afterwards hypothesis progressively firstly foreseen phase transition reached excessive cost foreseen used everywhere reported comparing many lying blind spot practice phase transition line foil affordable cluster pentium used time ranging minute hour elapsed time comparable foil machine progressively increased hypothesis combining stochastic deterministic learn approximated classical successful even used stringent conjunctive difficult extend cope disjunctive learned integrating covering made learner quinlan botta giordana emerges framework opinion outcome estimating learning hypothesis predicting cost testing hypothesis criterion deciding stop testing hypothesis concern possibility learning many even simplistic relevant task look hard many requiring rise exponentially considering presence irrelevant hold density target close phase transition even tiny difficulty anglano anglano giordana lobello saitta coevolutive learning proceeding machine learning page madison july botta giordana botta giordana learning tool proceeding thirteenth joint artificial intelligence page chambery france botta botta giordana saitta phase transition proceeding joint artificial intelligence page stockholm sweden brassard bratley brassard bratley algorithmics practice prentice hall englewood cliff giordana saitta giordana saitta phase transition learning machine learning giordana giordana saitta sebag botta phase transition proceeding machine learning stanford hogg hogg huberman williams artificial intelligence frontier phase transition volume elsevier mitchell mitchell mchine learning mcgrow hill muggleton raedt muggleton raedt inductive prosser prosser empirical phase transition binary satisfaction artificial intelligence quinlan quinlan learning machine learning rissanen rissanen modeling shortest data automatica serra serra apprendimento concetti relazionali dista universita pieminte orietale smith dyer smith dyer locating phase transition binary satisfaction artificial intelligence williams hogg williams hogg exploiting deep artificial intelligence additive improving fabio aiolli alessandro sperduti corso italia pisa italy aiolli perso sample scheme inspired idea training sample classified sufficient convergence distribution case scheme involving classifier vector quantization accommodates tangent tangent tangent able outperform additive able distribution training computational learning vapnik schapire bartlett tightly linked risk classifier probability misclassification drawn distribution distribution training expect best test data high aforementioned impressive vector machine boosting boosting effectiveness largely fact effectively training find largest training mapped kernel case permit impressive even high dimensional data supposed separated svms kernel kernel proven side boosting famous adaboost ensemble hypothesis trained minimize empirical difficult distribution training schapire boosting procedure linear weak hypothesis minimizes loss dependent training literally boosting faced optimization training done defining cost searching weak hypothesis minimize mason follow find eventually linear optimal hypothesis optimality term dependent distribution training minimize loss maintains training iteratively hypothesis reach predefined threshold distribution training data induced hypothesis improves expectation distribution distribution converges distribution subset training scheme classification tangent classifier simard obtaining basically build learning vector quantization procedure kohonen adapted tangent refer tangent vector quantization regularization distribution training hold property convergence distribution scheme considers discriminative ratio empirical comparing regularization learning take tend influence discriminant classifier discriminant viewed resource client pure empirical risk resource used wrong high probability entirely used fraction training formally tell preferable regularize discriminant make sharing equally inspired idea optimization procedure applicable eventually permit regularize parameter discriminant hypothesis many training generality loss training real taking representing confidence classifier prediction label binary classifier perceptron target sifier anyway monotonic linear sigmoidal transformation case must correspond classification hypothesis associate want lossfunction minimized permit hypothesis fixed threshold many training minimize basically slack training null threshold linear threshold suggest minimize indirectly simultaneously priori distribution hypothesis approximates hypothesis gradient procedure distribution improves converges distribution threshold iteration hypothesis threshold training hypothesis find series trial optimization process maximizes distribution work artificial training initialized training fixed threshold augmenting density consequently contribution optimization process note occurrence training procedure make distribution approaching distribution convergence distribution trial training training sample complementary occurrence time density suitable rule occurences take note iteration easy monotonicity iteration fact time show distribution converges demonstrating tend zero iteration module quantity tends show converge cumulative mean ratio epoch convergence optimization process maximizes going stable distribution tend distribution null elsewhere understood well made gamma iteration calculate minimize algebra rewrite thus reached note consistent modulated term decreasing iteration dependent used gamma viewed sort annealing introduced process framework tangent training fixed give classified classifier squared nearest nearest training take representing confidence prediction classifier also viewed indication discriminative classified scheme zero particularly interested transformation refer tangent simard hastie schwenk milgram computes linear subspace approximating manifold induced transforming transformation transformation priori learn defining tangent compounded centroid prototype vector tangent vector orthonormal base linear subspace resorting nent also fact minimized choosing sample eigenvectors covariance matrix tangent twosided hastie hastie onesided tangent used coincide sided simply referred tangent easy squared tangent transpose consequently hypothesis maximizing gradient ascent distribution improving driven gradient ascent considering tangent scalar thus derivate centroid tangent vector nearest obtaining considering derivative nearest nearest differs changing sign exchanging derivative null thus maximize training classifier move nearest suggested gradient note like training nearest maximizing distribution usual learning rate parameter brevity referring whole iteration threshold nearest accumulate nearest orthonormalize tangent distribution rule normalize distribution training closest closest accordingly density training processed gradient ascent training fixed distribution augmented force gradient ascent concentrate hardest training distribution simply incorrectly classified augmented training initialization done default training size prohibitive drastically taking parameter tangent codebooks tangent tangent euclidean prototype linear poly poly poly test ever case initialization identical invalidate procedure case distribution size training relatively centroid generating tangent initialization criterion negligible expect convergence initialization drastically fact initialized good optimal hypothesis initialization implicitly poor final distribution mistake iteration svms tdneuron sona exactly dataset consisting digit dataset binary digit transformed counting preprocessing elimination empty border training consisted digit remaining digit used test test data summarized reported best rejection dataset training used package kernel svms linear degree used default parameter svms binary classifier built svms prediction label best test kernel degree four architecture also centroid best test data reported best worst particularly centroid corresponds codebooks compact response time classification fact svms need vector worst case returned vector centroid plus tangent curve training test well distribution training induced distribution test reported plot easy show overfitting also confirmed involving final distribution training clearly steep distribution correspondence expense even minor extent distribution test data reported rejection curve competitive best resulting best whole conclusion provably convergent scheme improving difficult vector quantization tangent experimentally outperformed classifier compactness confirm control distribution great tangent initialization test training cumulative training iteration cumulative test comparing observe statistical learning convergence empirical risk ideal risk distribution work resorting predefined kernel proceed iteration rejection curve rejection criterion nearest adopted boosting boosting hypothesis combined hypothesis justifies adoption additive scheme multiplicative scheme committee machine acknowledgment fabio aiolli wish thank centro meta consorzio pisa ricerche supporting fellowship bartlett bartlett sample classification neural network size size network ieee infor hastie hastie simard sackinger learning prototype tangent teasauro touretzky leen neur inform proc system volume page kohonen kohonen hynninen kangas laaksonen torkkola learning vector quantization package helsinki univ tech inform january mason mason bartlett baxter explicit optimization dept australian schapire schapire freund bartlett boosting explanation effectiveness voting stat schapire schapire view boosting computational learning proc european schwenk milgram schwenk milgram learning discriminant tangent handwritten intern conf artif neur netw page schwenk milgram schwenk milgram transformation autoassociation handwritten teasauro touretzky leen neur inform proc system volume page simard simard lecun denker transformation hanson cowan giles neural processing system volume page morgan kaufmann simard simard computation hierarchical filtering cowan teasauro alspector neur inform proc system volume page morgan kaufmann sona sona sperduti starita discriminant transformation neuron neur comput vapnik vapnik statistical learning wiley machine learning data mining acquisition process jihie yolanda southern california admiralty marina jihie helping user build process many many investigated verifying validating base ontology rule easy checking process checking refining planning tend automated plan helping user process complementary help user process system kanal relates piece process analyzing piece build interdependency find show kanal able find process suggest process essential many process tool user process show process cell biology lambda virus invades cell able user step appropriately type connection step link step substeps ordering disjunctive alternative even process size step connection user benefit assistance intelligent acquisition tool help process user forget link step wrong link weapon production built subject matter even relatively process consisting step many looked step specified substep step substeps ordering missing user serious difficulty noticing fixing graphical tool process draw connection step abound tool process semantics step enables user process step mean accomplish feature step process thoroughly system background cell entered barrier also helpful tool know enter step agent domain ontology middle ontology used kind background tool much helpful checking process make sense background past validation verification base address rule base preece shinghal ontology mcguinness addressed process interactive tool acquire planning also chien myers huffman laird acquiring plan acquiring plan plan authoring tool closer spirit process authoring tool aiming kanal used plan produced plan editing tool formal hierarchical planning desirable property plan justifiability correctness kambhampati knoblock yang yang tate generative planner sipe noah nonlin wilkins sacerdoti tate critic plan planning much work planning background ontology believe crucial process modeling tool process specified user generates suggestion user system kanal help user build modify process detecting invalid pointing need acquired need modified inspired work expect interdependency swartout interdependency piece derived analyzing used analyzing interdependency acquisition tool inconsistency missing alert user base interdependency tool guide user fixing correcting inconsistency kanal analyzes interdependency step process simulation step kanal analyze interdependenciesbetween step step step supposed take process fact describing process assumed kanal interdependency feature process next kanal kind preliminary show kanal introduced originally process suggesting representing process describes briefly process consistent process ontology pddl ghallab nist tissot gruninger process composed step step precondition precondition satisfied activate step step enter step precondition enter near entrance outside inside also contained precondition list list strip operator step process step kind link link user invade step arrive enter take control substeps substeps substeps temporal link user ordering step modeling virus invasion take control step follow enter step disjunctive link task alternative disjunctive link lambda virus entering cell integrated host chromosome causal link user step kanal causal relationship step simulation examining outcome step precondition checked step causal link used validating step role enter step play role agent play role entered enter step instantiated virus invasion process assigning virus agent role enter cell role role assignment interdependency base role must consistent process step acquiring process system kanal module ambitious system subject matter entering domain part darpa rapid formation user build process conceptcomposition clark porter user build process retrieving connecting kind link user interface kanal interface process integrated kanal preliminary done illustrate scale process virus invasion interface built show used build process linking base user invade virus invasion agent role virus enter linked subevent kanal built process constructed composition also used checking process procedural intelligent tutoring system ongoing sometimes simulator build refine scholer also investigating kanal help teacher formalize process used lesson tutoring system interdependency past work interdependencymodels successfully used checking expect used analyzing base interact used interdependency piece procedural used subgoal kind interdependency interdependency factual procedural interdependency missing piece used predict piece show novel interdependencymodels process guide user developing process kanal build interdependency base kind static dynamic static posing feature process dynamic simulating process work date focused dynamic static plan maintain list sample template retrieving link type role step list instantiated template user list also answer trace answer interdependency reflects piece answer dynamic done simulated process simulation show step temporal ordering causal relationship also show produced step resulting interdependency enables checking step properly linked precondition step satisfied simulation achieved unexpected impossible path also interdependency changing ordering reinstate disabled step unachieved missing link work dynamic simulation tool interdependencymodels next describes process simulation checking process kanal built reasoning system clark porter invokes simulator alternative simulation process simulation process seen linearization process skolem execute step list step kanal execute step kind precondition test fails undoable step undoable step kanal interrupt simulation unreached step step unreached simulation stop step simulated also finish simulation simulation detected simulation kanal interdependency subsection type kanal checking unachieved precondition precondition achieved step step undo precondition integrate step virus host chromosomemay undo precondition viral exposed step synthesize protein able synthesize viral protein step reinstate exposure preconditionsis simulation skolem collect failed step collect unachieved precondition failed step show user help user suggest missing step find base precondition failed step suggest inserting somewhere process failed step suggest missing ordering find step failed step undid unachieved precondition find step follow failed step assert unachieved precondition suggest inserting ordering step failed step suggest modifying step precondition achieved type failure kanal suggests step changing ordering integrate step deleting modifying synthesize step suggestion user looking checking kanal informs user step simulation user process anticipated user kanal expect case process happens user simulation postconditions composed process user expect virus invasion cell viral nucleic acid located inside cell checked looking simulation simulation accumulation deleted fact disjunctive branch path accumulated separately kanal path expectation unachieved kanal step modify step kanal unexpected planning highlight unexpected user examine fact user simulation skolem collect unachieved path andrecord step failed path show user help user suggest missing step find base suggest inserting somewhere process suggest modifying step find step potentially role unachieved suggest modifying step suggest missing ordering find step undid find assert suggest inserting ordering maintain user specifies invasion viral nucleic acid located inside cell also user forgot enter step virus invasion kanal suggests step move enter virus modifying arrive step virus step user enter step fixed checking unordered step sometimes user forget link step wrong link weapon production mentioned detected mapping step base ordering specified substeps simulation unachieved precondition simulation kanal walk step substeps user specified link ordering simulation interrupted step reached lack orderingconstraints step undoable kanal highlight proposes changing ordering step show lack checking inappropriate step kanal also find modeling watching step assertion deleted step true step assertion reported also step assertion kanal well type step role wrong composition also step incorrect assignment produced unexpected step appropriately kanal proposes modifying step changing role assignment modifying step checking invalid simulation kanal many assertion precondition test test tested undefined kanal accessing undefined invalid checking loop loop necessarily process repeated time unintended repetition user defines many ordering step kanal warning case user loop fact checking disjunctive branch disjunctive branch user alternative fact kanal expose branch showing alternative substeps case loop disjunctive branch necessarily kanal simply informs user checking causal link simulation kanal computes interdependency step step satisfied precondition step synthesizing viral protein enables step causal link indicated user kanal informs user causal link help validate also informs user specified temporal seem causal link step justify ordering show kind detected kanal simplified lambda virus invasion lambda virus move next cell arrive enters cell enter lambda form circle circularize integrated simplified lambda virus invasion host chromosome integrate synthesizing viral protein formation virus synthesize integrated dormant chromosome cell divide environmental induce genome leave host chromosome disintegrate synthesizing viral protein replicate albert inappropriate step agent virus arrive step specified kanal detects inappropriate step assertion invalid virus case kanal proposes modify assignment arrive step step arrive assert deleted link call latter eventually lead user mention call indirect unachieved precondition case missing agent arrive step precondition enter step virus near cell also fail failure arrive step kanal detects unachieved precondition proposes fixing modify arrive step virus move arrive step precondition modify enter step precondition note case modifying arrive step suggested inappropriate arrive unachieved precondition enter also resolve assignment virus enter missing precondition near fails kanal message proposes case failed virus inside cell assignment cell enter step missing virus enters cell achieved missing ordering ordering step missing link arrive step enter step specified kanal interrupt simulation proposes modify temporal link step simulator execute unreached step ambiguity ordering generates warning asking explicit ordering step kanal simulation causal link causal link reported arrive enabled enter achieving virus near cell integrate enabled disintegrate achieving integrated chromosome synthesize enabled replicate viral protein disjunctive link disjunctive branch stemming circularize user alternative path simulated path simulated path disjunctive branch arrive enter circularize integrate divide disintegrate synthesize replicate arrive enter circularize synthesize replicate kanal abnormality lead missing ordering make step unreached simulation also lead failed unexecuted unreached step unreached step tends failed user want fixing unreached step case hold failed precondition unreached step failed preconditionsinterrupt simulation leading unreached step help avoid confusion kanal selectively user concentrate case failed precondition unreached step kanal failed precondition user want preliminary kanal integrated composition explanation tool interface system mentioned planning extensive user integrated system preliminary kanal module virus invasion lambda virus drain link invasion test case detected virus invasion lambda virus drain link invasion test case detected virus invasion lambda virus drain link invasion test case detected kanal process kanal help detecting fixing used process virus invasion process lambda virus invasion procedure high pressure compressor hpac domain also used acquiring process lesson intelligent tutoring system scholer last evaluated kanal help user kind forget link role assignment virus invasion user need make link assignment made test case taking process deleting subset link assignment user need make show preliminary show case tested show test case show many detected kanal kanal able kanal miss case lambda virus invasion invade fact cell explicit violation kanal able need examine slot tested fact belong cell cytoplasm kanal missed link deleted link link interrupt simulation detected unless step simulated fourth case kanal detected able step role refer deleted slot step agent enter step refers agent invade step enter step failed missing agent invade step failed step step missing link harder find planning examine follow link step trace back case lack happened time hidden kanal selective presentation help expect fixing time user fifth indirect sixth detected kanal able preliminary show kanal virtually test case detected made suggestion user case kanal missed process perfectly consistent overgeneral noticed user detecting impaired detecting step simulation kanal detected case link assignment missing detected process fixed next kanal detected kanal pointed conclusion mentioned planning user biologist kanal integrated system also able test usefulness kanal type show selecting wrong base verification validation wallace basili many applicable many process maintenance cost reuse planning examine kanal built composition base assumed acquiring procedural intelligent tutoring system expect incomplete operator used procedure refined instructor autonomous learning experimentation believe kanal also acknowledgment like thank bruce porter peter clark help integrating kanal simulator virus invasion andrew scholer help hpac process simulator tutoring system mentioned also like thank vinay chaudhri mabry tyson jerome thomere comment feedback work kevin knight helpful comment draft funded darpa rapid formation subcontract albert albert bray johnson lewis raff robert walter essential cell biology biology cell garland publishing basili basili selby comparing effectiveness testing ieee transaction chien chien static acquisition validation maintenance planning base clark porter clark porter reusable proceeding clark porter clark porter machine http ghallab ghallab howe knoblock mcdermott veloso weld wilkins pddl planning domain yale http huffman laird huffman laird flexibly instructable agent artificial intelligence kambhampati knoblock yang kambhampati knoblock yang planing refinement unified framework evaluating tradeoff planning artificial intelligence deriving expectation guide base creation proceeding acquiring problemsolving user putting interdependency test proceeding mcguinness mcguinness fikes rice wilder merging testing ontology proceeding myers myers strategic advice hierarchical planner proceeding system verification validation system preece shinghal preece shinghal foundation base verification intelligent system sacerdoti sacerdoti plan york elsevier scholer scholer rickel angros johnson learning domain teaching procedural task fall symposium learning swartout swartout expect explicit flexible acquisition proceeding tate tate generating network joint artificial intelligence tate tate representing plan drabble proc artificial intelligence planning system edinburgh aaai pointer tissot gruninger tissot gruninger nist process specification nist wallace wallace ippolito cuthill verification validation process nist publication wilkins wilkins practical planning extending classical planning paradigm morgan kaufmann yang yang formalizing planning hierarchical planning computational intelligence integrating expectation help user acquire procedural blythe southern california marina blythe explicit successful acquiring domain system acquiring procedural type interdependency piece successful acquiring procedural domain show combined benefit extend acquisition tool dynamically generates user flexible user tool generates expectation procedural procedure refined expectation interdependency turn refine used system tool broader previously system preliminary travel planning domain show user little training executable procedural customize intelligent system successful deployed intelligent system must able cope task specification user make modification system control task task system travel planning assistant locate flight hotel help user itinerary trip many system user hotel cost hotel user prefer flight unless double price cheapest connecting flight flight arrives late evening hotel near airport near meeting travel assistant tool leaving user hand severely limiting tool usefulness like integrated tool essential meet wide user need suggested user tool know flight arrival time processing criterion case price connecting flight multiplying procedural purely factual like cost tool incorporate procedural like user applicability well originally envisaged difficult user procedural system next user face expectation entered user expectation belief user entering used constrain possibility entered next expectation govern type user entering system used interpret entered feedback help user enter work acquisition user explicit domainindependent tool process psms breuker velde eriksson encourage structured intelligent system well guide acquisition used session user acquired approachesthat assisting engineer domain fensel benjamin fensel motta tool salt marcus mcdermott take domain fill role used acquire musen musen argues guidance lack flexibility constructing system problemsolving term address puerta advocate psms constructed puerta melz melz address encoding part inspected user partially completed analyzed find missing form role filled done part interdependency expect swartout look factual used intelligent system work acquire role dynamically melz tool adequate user structured user tool like salt role form unstructured list item difficult user missing piece user must work proceduresyntax user guide user procedural view tool mentioned kind expectation entered background form psms interdependency framework tool backgroundtheory help user process also interdependency help user refine procedure executable system built expect module expectation form characterization procedural sharing mutually beneficial background next user face defining procedural next expectation show integrated tool constable user constable demonstrate user find difficult enter procedural user face defining procedural sketch addressed highlight role played expectation user know intelligent system piece form recognizable system simply beginning process difficult even experienced know system well expectation help identify procedural eriksson user know formal structured english user modify english paraphrase procedure formal blythe ramachandran user fragment paraphrase list suggested fragment automatically expectation hide procedure avoiding full natural processing user know free formal type trying multiply subprocedure hotel user procedure choosing list effectively eliminate detected warning procedure fragment contributes suggested formulated helping user user know procedure formal standpoint desired constable test soon entered help find take step user lost user realize forget side expectation used guide user step done script next show acquisition script tallis user face help kind expectation procedural next show help kind expectation integrated constable describing expectation task system make expectation derived analyzing procedure interdependency expectation integrated expressing expectation background term type expectation expectation background expectation derived background used help clarify procedural identifying framework identification made template user refine desired task expectation background help user process defining procedural task background main ontology task generic procedural subtask wide generic task plan illustrate plan belong domainindependent agent judge alternative plan criterion alternative plan suited task term commonkads breuker velde case assessment criterion judging plan framework experience intelligent system plan valente identified criterion evaluated blythe regularity planning domain guidance acquisition ontology plan judgment criterion critique partially represents critique evaluated checking property plan identified evaluating plan generic procedural attached also plan resource type criterion judging plan part background plan task generic procedural attached subtasks domain plan domain subtasks generic critique type step satisfies critique property task estimating property user tool satisfies boolean background constable background used plan system domain defining critique ontology procedure critique travel planning domain plan itinerary travel step plan flight hotel rental critique flight cost defining critique subclass procedure used critique resulting flightcost step user completes defining retrieving cost flight allowed show main window user defines critique constable tool kind aimed classifying critique ontology user refine default procedural phrase show attached critique ontology tool asks classify critique warn used classify critique classification made tool give user refine procedural attached generic procedural default allowed user refine default flight cost background classify default procedural help user face creating procedural started tool asking default procedure guaranteed applicable system task also used break manageable piece step hard user refining allowed flight cost daunting task requiring tool assistance expectation interdependency assistance expectation interdependency expectation last come background task expectation come comparing procedural procedure procedural factual refer interdependency expectation expectation used procedure body rule syntactic construct else expectation comparing procedural factual used base procedural used procedural used type used procedure multiply hotel expectation derived interdependency show constable procedural used allowed cost flight body procedure automatically paraphrased english blythe ramachandran user selects part procedure case pittsburgh suggests panel constable used allowed cost flight distinguish tool interdependency expectation help user procedural hard soft expectation hard expectation tool procedural violates expectation expressed user construct procedure suggested user never procedure violates construct expectation hard expectation soft expectation tool user proceduresthat violate expectation providewarnings possibly remedy violation violates type expectation allowed cost flight user specifies pittsburgh formal must followed code boolean show message panel pittsburgh replaced suggests pittsburgh boolean city pittsburgh origin city pittsburgh produced anytime suggestion also violation pittsburgh choosing make expectation hard soft user procedure successive user confused form procedure empirically user find build violate interdependency expectation undefined procedure user concentrate fixing violation soft expectation constable user violate syntactic expectation hard constable tool expectation hard soft optimal also depend skill user compiler find inputoutput expectation constable suggesting modification code address suggestion sentence taking userselected fragment node term formal expanded applying applicable procedure terminates term reached resolve expectation violation match infinite suggestion module time time user programmerscan daunted need executable procedural precise subgoals returning typed expectation hard soft form help integrating expectation expectation background expectation valuable help user procedural kind expectation assistance part emerging procedural exchanged flow constable used flight cost critique summarized window expectation background task classify critique ontology refine attached procedural procedure type expectation sent task module help guide user constable task assign type desired type call procedure allowed desired type warn user suggest remedy type expectation default refined analyzer considering interdependency user defines flight cost flight type inferred task come type find flight cost trip user flight expect interdependency show take flight passed back task analyzer analyzer also used task classifying critique user completes procedure flight analyzer note used task classify critique ontology warn flight cost classify critique lowerbound produced type airline asked task classify critique extensional critique preferred test critique passed task analyzer user defines critique analyzer make type expectation task help user also pass refined type task procedural interdependency task analyzer classifies task interplay expectation significantly improves guidance user preliminary preliminary evaluating subject modifying critique travel planning domain training user followed modify critique critique constable testing user asked modify many critique list subject took hour training phase thirty minute test critique simplest training task representing hotel used meeting task hotel twice closest hotel procedure multiply find find hotel subject entered tool even unlikely success rate achieved constable test task training task modifying critique step completed test task counted step also counted classifying critique ontology step show step completed four test task subject able make modification four subject able hotel cost critique city hotel pittsburgh hotel cost defining kind critique scope tool user procedural task step completed step modify city step completed subject four test task critique subject attempted critique plan tool critique subject worked training test case affect subject succeeded critique critique subject constable identified critique ontology used tool shortcoming tool subject able conceptsand domain show procedure helpful user procedure also need also time subject spend thinking critique hope build tool subject daily constable investigate completing subject travel planning volunteered critique work critique expressed entail procedural reasoning critique expressed critique ontology flight arrives late evening hotel near airport near meeting prefer hotel close meeting procedural intelligent system challenging task system wide user novel expectation background interdependencyanalysis integratedto guide user process well assistance subset task system able combine help procedural concert factual constable tool factual tool expectation blythe constable travel planning tool retrieves flight hotel travel plan help user assemble itinerary integrating constable help user preference assemble itinerary also intend constable acquire procedural reasoning biochemistry blythe blythe ramachandran integrated acquisition best proc intelligent user interface blythe blythe plan critiquing proc twelfth acquisition system blythe ramachandran blythe ramachandran acquisition englishbased proc twelfth acquisition system breuker velde breuker velde commonkads expertise modelling reusable eriksson eriksson shahar puerta musen task modelingwith reusable artificial intelligence fensel benjamin fensel benjamin automated reuse proc european artificial intelligence fensel motta fensel motta proc eleventh acquisition knowledgebased system melz melz explicit acquisition proc thirteenth artificial intelligence deriving expectation guide creation proc sixteenth artificial intelligence aaai marcus mcdermott marcus mcdermott salt acquisition system artificial intelligence musen musen overcoming limitation acquisition puerta puerta egar musen acquisition automatic acquisition tool acquisition swartout swartout expect explicit flexible acquisition proc ninth acquisition knowledgebased system tallis tallis script guide user modifying system proc sixteenth artificial intelligence aaai valente valente blythe swartout role control system experience inspect proc jfacc symposium control machine learning data mining reinforcement learning time reinforcement learning ronen brafman israel brafman moshe tennenholtz stanford stanford moshe reinforcement learning attain time agent maintains possibly inaccurate optimal derived initialized optimistic fashion maximal name agent improves upon simpler kearns covering zerosum stochastic game resolving exploration exploitation dilemma formally justifies optimism uncertainty bias used many much simpler brafman algorithmfor learning controller stochastic game generalizes monderer tennenholtz learning repeated game learning repeated game much simpler alternative banos megiddo reinforcement learning attracted time many reinforcement learning convergence rate kearns kearns provably time learning markovdecision process mdps extendedlater handle controller stochastic game scsgs brafman tennenholtz well structured mdps kearns koller agent learns updating statistic collect learning permanent address faculty industrial technion haifa israel process continues long done relatively efficiently case agent learned optimal success rest property agent learning guaranteed optimal learned approximately optimal real difficulty generalizing adverserial game stem adversary influence probabilityof reachingdifferent game agent control adversary predict difficulty predicting outcome lead consequently unlikely agent exploration exploitation adverserial used restricted scsg adversary influence game dynamic overcome suggest agent never learn agent optimize albeit fictitious optimal lead learning assumes agent obtains familiar maximal optimal agent fictitious property real optimally lead learning agent know optimizing learning efficiently thus agent explore efficiently knowing ahead time parameter learn long learning done efficiently agent spends step exploring rest time spent exploiting thus resulting said implicit explore opposed kearns explicit explore learning call converges stochastic game markov process consequence generalizes kearns adverserial agent considers stochastic inappropriate opting nondeterministic handle stochastic game brafman tennenholtz attains also improves upon learning repeated game aumann maschler megiddo megiddo banos banos time game know much simpler generalizes monderer tennenholtz monderer tennenholtz handle probabilistic maximin safety criterion referred optimism face uncertainty heuristic kaelbling heading sutton barto optimistic referred trick stationary optimistic bias used reinforcement learning kaelbling exploration kaelbling exploration bonus dyna sutton exploration schmidhuber exploration prioritized sweeping moore atkenson tadepalli tadepalli reinforcement learning work undiscounted used variant noted none work justification natural bias thus contribution formal justification optimism uncertainty bias learning precisely relevant parameter sketch yield time limitation full work preliminary stochastic game markov process necessarily stochastically operates parameter influencing time stochastic game game game player chooses player combined outcome numerically form payoff vector vector player concentrate game game payoff vector refer player control agent player adversary game matrix game strategic form matrix correspond agent column correspond adversary column game matrix agent adversary agent play adversary play make simplifying size agent adversary identical size trivial stochastic game player play possibly infinite game game playing game player receive payoff dictated game matrix move game identity game stochastically game player game formally player game game strategic form player matrix probabilistic transition probability transition player agent play player adversary play lead transition main transition agent transition agent adversary agent adversary associate game term game interchangeably stochastic game used mdps wish stochastically case view agent alternative probability distribution lead maximizing adversary agent meet entity formulated learning stochastic game applicable repeated game mdps degenerate form repeated game adversary ease exposition normalize player payoff game real also take history history history history agent mapping probability distribution determines probability choosing history criterion natural undiscounted adversary omit subscript denoting confusion step optimality make central agent recognizes identity reached payoff transition probability playing game know adversary payoff maximal ahead time latter removed next wish central parameter mixing time identified kearns kearns argue unreasonable refer learning referring convergence desired mixing time smallest expectedpayoffof case haveto take existence adversary adjust slightly belongs mixing time adversary matter adversary need repeatedly resulting remains relevant parameter step accumulated sufficiently close mean agent perfect game transition step optimal optimal mixing time clearly expect agent lacking optimal undiscounted looking optimal time interested approaching time failure probability size game reader noticed make learning task easy attain maximal reasonable lead learning high execute reach learning guess done fact must outcome thus expect agent learn agent visit sufficiently many time learn consequence finite sample sufficiently many time learn behave fact probably make sense restrict subset hard mdps kearns refer ergodicity hoffman karp refer irreducibility said irreducible fixing pure stationary player irreducible reachable case irreducibility precisely ergodicity property used kearns irreduciblesgs numberof nice property hoffman karp maximal implying optimal optimal stationary stagegame believe game recall stochastic game consisting agent adversary associate matrix game pair consisting agent adversary playing game probabilistic transition probability transition agent played adversary played convenient think parameter transition joint game ease exposition throughout mixing time optimal show relaxed construct consisting correspond real game correspond real fictitious game game matrix maintain game boolean initialized unknown reached playing joint many time player playing joint item initially empty repeat optimal execute observe followingeach joint adversary time reached playing reached mark transition probability frequency adversary play role seen parameter assumes joint yield maximal rewardand lead probability fictitious optimal followed joint agent arrives stagegame transition recorded joint lead joint recompute repeat step optimality convergence tool ultimately lead mixing time optimal achievable probability attain step main lemma proving show agent optimal maintains step attain desired statistic unknown slot sufficiently high probability implicit explore property agent know ahead time exploring exploiting part adversary control predict know matter adversary proceed show sample mark slot parameter iteration parameter iteration time loss exploration stating main lemma extend kearns simulation lemma kearns slightly well transition joint carried agent adversary lemma agent adversary next induced induced kearns induced used agent marked unknown correspondingto joint stagegame marked unknown identical transition identical transition probability agent adversary unknown optimal induced clear refer implicit explore lemma lemma arbitrary adversary unknown played step probability practice adversary ahead time thus know attain reach unknown sufficient probability crucial matter adversary outline wish show stated must real used induced iteration implicit explore lemma applicable step lead close optimal adversary lead learning optimal must close optimal know alternative assigning arbitrary counterpart suitable well phase exploring polynomially fact parameter learn probability statistic thus phase close optimal wish done actually guaranteed probability probabilitythat algorithmfails attain expectedvalue parameter sampling time chernoff show variance sufficiently close probability sufficiently sample remove mixing time proceed kearns deduce parameter step guaranteed probability desired repeatedlyexecute algorithmfor time step mixing time step desired algorithmdoes final halting time continuously long agent functioning caveat mixing time exponential mixing time step exponential exponential step true worth noting agent never know slot game adversary consequently optimal full game agent actually converge differs yield best adversary guaranteed mixing time differ mixing time guaranteed mixing time time agent subject deviation afforded conclusion reinforcement learning guaranteed convergence nearoptimal stochastic game optimistic formally justifies optimism face uncertainty bias many kearns agent need contemplate explore fact agent never learn optimal game play optimal knowing optimal clever agent catch adversary adversary play well lead agent payoff agent sufficient probability learn something eventually many unpleasant learning phase agent optimal mdps repeated game latter game efficiently stochastic game covered brafman tennenholtz fall much simpler conceptually also attains payoff agent must multiplicative learning appeared literature littman littman describes variant qlearning minimax stochastic game asymptotic convergence littman szepesvri wellman wellman framework multiagent game framework framework treated dealt game wellman well converges optimal case nash equilibrium convergenceis joint visited infinitely note adversary prevent learning agent learning game indefinitely time convergence optimal payoff guaranteed even joint never encountered repeated game stochastic game satisfies repeated game transition probabilitiesto learn need fictitious much simpler used resulting much simpler much megiddo megiddo banos banos convergence proven acknowledgment thank amos beimel help improving lemma referee comment partially paul ivanier robotics production aumann maschler aumannand maschler repeated game incomplete agent need employ optimal optimal adversary play banos banos pseudo game annals mathematical statistic brafman tennenholtz brafman tennenholtz time learning stochastic game artificial intelligence hoffman karp hoffmanand karp nonterminating stochastic game wellman wellman multiagent reinforcement learning framework proc machine learning kaelbling kaelbling littman moore reinforcement learning survey kaelbling kaelbling learning embedded system kearns koller kearns koller reinforcement learning factored mdps proc joint artificial intelligence ijcai page kearns kearns nearoptimal reinforcement learning time conf machine learning littman szepesvri littman csaba szepesvri convergence apllications proc intl conf machine learning page littman littman markov game framework reinforcementlearning proc intl conf machine learning page megiddo megiddo repeated game incomplete played player game monderer tennenholtz monderer tennenholtz dynamic decisionmaking moore atkenson moore atkenson prioratized sweeping reinforcement learning data real time machine learning schmidhuber schmidhuber curious modelbuilding control system proc intl joint conf neural network page sutton barto sutton barto reinforcement learning sutton sutton integrated architecture learning planning reacting approximating dynamic proc intl conf machine learning morgan kaufmann tadepalli tadepalli modelbased reinforcement learning artificial intelligence asymptotic fred erick garcia florent serre inra unite biometrie intelligence artificielle auzeville castanet tolosan cedex france fgarcia reinforcement learning combine eligibility trace asymptotic watkins accumulative eligibility trace asymptotic approximationof gain matrix variant qlearning optimal gain matrix maximizes rate convergence toward optimal optimal gain asymptotic gain explains relative latter minimizing gain optimal parameter decreasing learning rate optimal strongly exploration learning robust learning parameter lead show close resemblance schwartz demonstrated simulation undoubtedly reinforcement learning learning optimal rule used many reinforcement learning stochastic dynamic efficiently evaluating basically rule convergence rate qlearning integrate rule resulted family watkins peng williams believed improves behaviour definite come conjecture convergence rational remains dayan kearns hampered difficulty eligibility trace many trace transition time high spite cichosz wiering schmidhuber rule remains problematic eliminate shortcoming relies asymptotic watkins gain matrix variant optimal convergence rate lead optimal gain matrix gain explains relative minimizing gain able nearoptimal learning rate reinforcement learning aqlearning show close resemblance schwartz schwartz confirm soundness asymptotic nearoptimal behaviour watkins like reinforcement learning framework markov process puterman stationary size size markovian dynamic characterized transition probability moving applying instantaneous transition assigns defines trajectory probability optimization discounted markov find maximizes discounted trajectory discount learnt trajectory maintaining transition probability sutton also learn optimal watkins qlearning regularly optimal characterized optimality optimal derived combine temporal used accelerate convergence discounted potentially infinite trajectory lead eligibility trace transition temporal stepsize eligibility trace stepsizes decay regularly toward dynamic eligibility trace like increased pair decay exponentially parameter pair eligibility trace dynamic watkins peng williams sutton simplest watkins watkins derived accumulative eligibility trace supplementary resetting whole trace vector greedy trace accumulative eligibility trace exactly corresponds qlearning transition convergence proved bertsekas tsitsiklis borkar meyn convergence overcomingof onlybeen experimentally peng williams wiering schmidhuber practice main drawback relative cost proportional size computation time high asymptotic intend asymptotic watkins made iteration exploration greedy probability asymptotically converged toward process defines markov markov regular recurrent aperiodic irreducible distribution asymptotic accumulating trace trajectory converges toward generalizing work garcia serre asymptotic behaviour accumulative trace trajectory calculated trace regularly distribution case role trace rule pair particularly give asymptotic rule derive asymptotic expectation trace time conditioned pair asymptotic asymptotic substituting rule trace diagonal matrix diagonal eligibility matrix pair ordered column heading block dimension temporal vector vector asymptotic optimal gain matrix gain matrix variant qlearning gain matrix guide accelerate convergence stochastic adaptive stochastic benveniste kushner calculate optimal gain matrix maximizes rate convergence ordinary differential analytic tool analysing convergence stochastic target vector time classic reinforcement learning like analysed bertsekas tsitsiklis borkar meyn convergence concern optimization rate convergence stochastic show stability benveniste gain matrix jacobian matrix converges toward asymptotic variance finite horizon garcia ndiaye garcia serre jacobian matrix inverse calculated diagonal matrix diagonal ficients diagonal matrix diagonal coefficient transition matrix markov rectangular matrix coefficient developing show optimal gain matrix diagonal matrix diagonal coefficient rectangular matrix coefficient probability moving applying optimal next transition optimal gain lead ideal theoretically maximizes rate convergence minimizing asymptotic variance ture greedy exploratory modified note parameter characterizes exploration distribution costly coefficient next parameter minimize gain matrix optimization consistent assumes stepsize case diagonal matrix coefficient optimize parameter minimizing quadratic norm gain optimal gain matrix matrix coefficient block lead left block garcia serre thus equality exhibit note gain identical minimizing yield like exactly practice mdps transition probability nearly experimentally lead surprising fact convergence eligibility trace bertsekas tsitsiklis case admissible minimized thus able optimized rule asymptotic rule proportional fortunately derived iteration rule vector coefficient relative scaling expressed note greedy rule replacing stepsize time visited time updating replaced lead simulate learning trajectory particularly iteration namely watkins distinct optimized minimizes asymptotic variance around like asynchronous iteration clear feature crucial regarding time simulation next amazingly close schwartz schwartz supposed converge gain optimal maximize identical learning rate fact gain matrix variant demonstrated outperform despite fact optimality criterion convergence think establish checking stability gain matrix exploiting borkar borkar scale case modify rule simulation made relative watkins reinforcement learning markov learning computation time kind learning fairly behaviour long trajectory parameter initially stepsizes chose exploration exactly calculated fromthe data show relative learning deviation trajectory size graph illustrates differentrates convergence confirm rate convergence converges nearly fast ideal even slightly learning rate considerably illustrates computation time relative learning size observe graph slow despite fact iteration main really time conclusion asymptotic efficiently rule relative scaling variant discounted criterion show convergence toward optimal watkins accumulating eligibility trace case replacing trace sutton peng williams peng williams trace never reset wiering schmidhuber wiering schmidhuber work concern reinforcement learning seen optimal gain exploration parameterized best remains open need schwartz schwartz benveniste benveniste metivier priouret adaptive stochastic berlin york bertsekas tsitsiklis bertsekas tsitsiklis athena belmont borkar meyn borkar meyn convergence stochastic reinforcement learning siam control optimization borkar borkar stochastic scale system control letter cichosz cichosz truncating temporal reinforcement learning artificial intelligence jair garcia ndiaye garcia ndiaye learning rate machine learning volume madison garcia serre garcia serre asymptotic temporal learning proceeding european artificial intelligence ecai berlin kearns kearns biasvariance temporal proceeding colt kushner kushner stochastic springer peng williams peng williams incremental machine learning volume page puterman puterman markov process discrete stochastic dynamic wileyinterscience york schwartz schwartz reinforcement learning maximizing undiscounted machine learning volume dayan dayan analytical mean squared curve temporal learning machine learning sutton sutton reinforcement learning replacing eligibility trace machine learning sutton sutton learning predict temporal machine learning watkins watkins learning delayed thesis cambridge cambridge england wiering schmidhuber wiering schmidhuber fast machine learning exploiting secondary reinforcer gradient reinforcement learning greg grudic ircs grasp pennsylvania philadelphia grudic lyle ungar pennsylvania philadelphia ungar reinforcement learning depend reinforcement guide optimal rare converging impractically slow domain readily quantity secondary reinforcer guide augment gradient reinforcement learning domain desired relative secondary reinforcement quantity establish reinforcement sampled calculate gradient secondary reinforcer increased gradient used relative secondary process iterates maximized converges optimum secondary rate convergenceof performancegradient secondary size demonstrate converge many magnitude gradient reinforcement learning trial procedure agent learns receives core procedure directs hypothesis test process agent discovers behaviour governed size agent domain size effectiveness procedure used domain used yield achievable mean kaelbling little doubt mammal well system successfully form reinforcement learning sutton barto mammal live varied unpredictable implying domain blindly searched animal thus mammal must employ intelligent learn receive employed mammal secondary reinforcer stimulus reinforcement thus reinforcing secondary reinforcer organism learn stimulus secondary reinforcement reinforcement organism seek goal successful breeding long term survival rarely sampled little guidance survival stimulus fear hunger cold pain pleasure much readily consequence animal long term learns hanging around kitchen secondary reinforcer sometimes reinforcer assuaging hunger starving kitchen food learn modify attain secondary reinforcer minsky minsky argue secondary reinforcer ingredient creating artificial intelligence able learn reinforcement learning motivating behind widely successful reinforcement learning secondary reinforcer sutton barto work extend secondary reinforcer rate convergence date secondary reinforcer largely simply noting stimulus correlated agent learn maximize stimulus independently blindly maximizing secondary reinforcer stimulus naive potentiallycomplicated secondary reinforcer domain fact agent gain controlling relative stimulus reinforcer concentrating achieving maximal even learning spend time kitchen food fall floor best served spending time kitchen also time wander greatly curtail long term survival thus want control percentage time spends kitchen minimize risk even controlling much secondary reinforcer agent experience student must hour exam hour exam finish next exam scheduled know studying getting good night sleep getting good mark exam secondary reinforcer modify secondary reinforcer shorter converselyshe learn shorter night sleep secondaryreinforcer simply spends next hour sleeping force next hour poorly exam thus maximize exam mark need drink coffee spend time drink warm milk must ratio secondary reinforcer sleep srpg work secondary reinforcer gradient srpg determining relative secondary reinforcer agent need experience framework assumes agent domain defining secondary reinforcer relative experienced secondary reinforcer readily thus agent learn experience stimulus ratio repeated step step domain establish desired relative secondary reinforcer stimulus learn yield stimulus step agent need never experience stimulus secondary step established step gradient calculated secondary reinforcer stimulus gradient used desired relative secondary reinforcer step repeated secondary reinforcer guess relative ratio wisely show theoretically experimentally agent receives significantly used case reinforcer infrequently experienced agent rate convergencemeasureused work time agent need experience applying framework exam studying student knowing sleep stimulus need eight hour sleep twelve hour thus student spend many hour learning sleep eight hour twelve ever taking exam time take exam good sleeping eight hour studying twelve observes exam grade learn maximize grade systematically adjusting ratio secondary reinforcer tell mark adjusting drinking milk coffee exam mark rarely observes derived gradient reinforcement learning pgrl williams baird moore sutton konda tsitsiklis baxter bartlett grudic ungar reinforcement learning vfrl learning sutton barto motivation threefold vfrl work learning defines thus agent valuable grows exponentially domain learning infeasible kaelbling generalizing functionsin much open conversely pgrl work parameterized defines probability fewer parameter used thus addressing need pgrl work calculating gradient increased parameter incrementally modifying parameter pgrl theoretically guaranteedto convergeto locally optimal vfrl find globally optimal practice feasible convergeto globally optimal domain case computational cost pgrl grows linearly parameter used sharp exponential growth vfrl pgrl parameterizedpolicy relatively incorporates parametric form know parameterization reflects vfrl realized many real domain partially observable agent must inferred many time step vfrl difficult domain conversely pgrl work effectively partially observable domain peshkin framework make usual markov process simplicity loss generality agent interacts series episode duration agent time note discrete continuous time step agent chooses discrete dynamic characterized transition probability gradient followed agent characterized parameter vector probability agent sutton differentiable reinforcer agent receives time agent goal optimize discounted discount note assumed agent parameterized secondary reinforcer time agent sens stimulus stimulus constitute agent secondary reinforcer stimulus episode reinforcer termed secondary reinforcer frequency secondaryreward stimulus agent parameter srpg time step desired secondary reinforcer frequency secondary reinforcer gradient srpg step step learn achieves desired frequency secondary reinforcer step agent establishes secondary reinforcer frequency learning parameter system nonlinear step calculate gradient increased secondary reinforcer desired secondary reinforcer frequency gradient desired secondary reinforcer frequency next secondary reinforcer frequency step size goto step note step agent need observe agent must nonlinear system done briefly procedure used gradient step size converge happens encounter note made gradient williams sutton konda tsitsiklis baxter bartlett sutton stationary distribution step agent must numerically calculate gradient numerically vector zero perturbation system outlined observing thus step agent must observe time secondary stimulus convergence establishes srpg converges satisfied srpg converges established remainder page bertsekas tsitsiklis prof rate convergence gradient reinforcer size agent finite smoothness also variance reinforcer arbitrary convergence reinforcer variance gradient time satisfied choosing noting time discrete mdps agent agent boltzmann distribution transition probability time agent interacts series episode lasting time step agent episode visit thus agent achieves spends time time initially parameter secondary reinforcer used srpg agent desired secondary reinforcer frequency step srpg srpg learning rate show learning curve averaged indicating deviation note predicted convergence affected rate convergence srpg sutton stationary distribution episodeby countingthe numberof time visited learningcurve functionof numberof primaryreinforcer algorithmfor srpg learning step size note convergence took approximately convergence took convergence time stopped also show learning curve averaged showing deviation note srpg converges appreciably agent conclusion domain essential agent effectively learn reinforcement domain agent open secondary reinforcer stimulus learning curve reinforcer reinforcement thus relative property agent show secondary reinforcer fruitful domain effectively used guide reinforcement rarely secondary reinforcer readily observable rate convergence significantly secondary reinforcer blindly maximized make full secondary reinforcer agent control relative environmental feedback gradient secondary reinforcer gradient srpg iterates secondary reinforcer stimulus locally optimal step step relative secondary reinforcer stimulus agent need establish good step agent learns achieves relative stimulus gradient property srpg need step convergence slowed rarely step gradient increased secondary reinforcer stimulus secondary reinforcer step reinforcer sampled time showing rate convergence gradient size viable high dimensional gradient used relative desired secondary reinforcer step establish relative stimulus thus improving step repeated converges optimum show srpg convergence gradient requiring many magnitude fewer converge showing srpg converges locally optimal secondary reinforcer hold step must able establish achieves specified relative secondary reinforcer stimulus used nonlinear system gradient prone convergence srpg converged close optimal even encountered step sufficiently thus continue converge establishing relaxed srpg converge open open secondaryreinforcerswithin reinforcementlearning framework sutton barto actor critic combine gradient thus successful fail benefit domain derived secondary reinforcer open list secondary reinforcer formulate find subset used acknowledgement thanks jane mulligan work funded grasp ircs pennsylvania darpa grant baird moore baird moore gradient reinforcement learning michael michael kearns sara solla neural processing system volume cambridge baxter bartlett jonathan baxter peter bartlett reinforcement learning pomdp gradient ascent proceeding seventeenth machine learning page stanford june bertsekas tsitsiklis bertsekas tsitsiklis athena grudic ungar grudic ungar localizing gradient transition proceeding seventeenth machine learning volume page morgan kaufmann june july grudic ungar grudic ungar localizing reinforcement learning proceeding seventeenth artificial intelligence volume page menlo park aaai cambridge july august kaelbling kaelbling littman moore reinforcement learning survey artificial intelligence konda tsitsiklis konda tsitsiklis solla leen mller neural processing system volume cambridge minsky marvin minsky step toward artificial intelligence feigenbaum feldman thought page mcgrawhill book company york peshkin leonid peshkin nicolas meuleau pack kaelbling learning cooperate proceeding sixteenth uncertainty artificial intelligence stanford june flannery teukolsky vetterling recipe cambridge york sutton barto sutton barto pavlovian reinforcement gabriel moore learning computational neuroscience foundation adaptive network page sutton barto sutton barto reinforcement learning cambridge sutton sutton mcallester mansour gradient reinforcement learning solla leen mller neural processing system volume cambridge williams williams gradientestimating reinforcement learning neural network proceeding ieee neural network diego williams williams statistical gradientfollowing connectionist reinforcement learning machine learning machine learning data mining machine learning data mining foundation learning charles elkan california diego jolla california elkan revisits optimal learning misclassification incur penalty characterize precisely intuitively cost matrix reasonable show avoid mistake defining cost matrix economically incoherent case show proportion training make optimal classification classifier learned learning argue changing balance training little classifier produced bayesian tree learning accordingly recommended applying domain differing misclassification cost learn classifier training optimal probability classifier cost matrix specification cost incorrect prediction predicted lead lowest cost expectation probability mathematically cost matrix cost predicting true prediction prediction incorrect optimal prediction minimizes cost necessarily monetary cost also waste time severity illness alternative possibility true framework role learning classifier probability true prediction mean acting true essence optimal true even probable rational approve card transaction even transaction legitimate cost matrix property cost matrix predict predict followed cost matrix correspond alternative predicted column correspond cost false cost false conceptually cost labeling incorrectly cost labeling mathematically case call reasonableness reasonableness violated case optimal label optimal label leave case reasonableness violated reader analyze margineantu pointed cost matrix label never predicted optimal intuitive criterion happens dominates cost matrix case cost predicting cost predicting regardless true optimal never predict case optimal prediction dominated cost matrix reasonableness cost matrix imply neither matrix dominates cost matrix optimal unchanged matrix multiplied scaling corresponds changing unit cost optimal unchanged matrix shifting corresponds changing baseline away cost scaling shifting cost matrix satisfies reasonableness transformed simpler matrix lead matrix perspective cost matrix effectively degree freedom cost benefit machine learning used terminology cost accounting term benefit preferable avoiding mistake natural baseline benefit baseline agent take regarding agent made benefit benefit thinking term cost easy posit cost matrix logically contradictory matrix baseline german dataset part statlog michie cost matrix dataset good predict predict good loan bank good mean repay loan mean default predict deny loan cashflow relative baseline prediction regardless good true economically reasonable cost matrix domain predict must cost benefit baseline baseline must fixed opportunity cost foregone benefit missed opportunity penalty easy make mistake measuring opportunity cost baseline erroneous cost matrix justified informally cost approving good zero cost rejecting zero case made good rejected cost opportunity cost foregone profit approved loan cost lost loan concretely reasoning quote incorrect bank four type clearly cost matrix imply asset bank alternatively four receive loan repay asset regardless baseline accounting give erroneous cost matrix give cost give cost cell cost benefit matrix card transaction domain benefit matrix fraudulent legitimate refuse approve size transaction dollar approving fraudulent transaction cost transaction bank liable expense fraud refusing legitimate transaction cost annoys refusing fraudulent transaction nontrivial benefit prevent fraud lead arrest criminal learning cost beginning zadrozny elkan optimal case optimal prediction cost prediction cost predicting fact equality predicting optimal threshold optimal reasonableness optimal prediction rearranging lead denominator nonzero implied reasonableness show cost matrix degree freedom perspective degree freedom matrix perspective apparent contradiction optimal nonlinear cost matrix achieving rebalancing turn classifier learning yield classifier maximize case classifier implicitly make probability threshold conclusion need classifier target threshold learning made classifier make achieving rebalance training learning proportion training training rebalancing idea make target probability threshold correspond probability threshold training multiplied correctness defer next case threshold used learning training multiplied case used breiman directionality learning yield classifier make prediction probability threshold training desired probability training changing training give desired classifier learning training must oversampling undersampling oversampling mean duplicating undersampling mean deleting sampling done deterministically deterministic sampling reduce variance risk introducing bias duplicate eliminate correlated property undersampling deterministic sense fraction feature held stratified sampling well changing many domain rare keep rare case call rare discarding duplicating rare probability base rate happens also tool answer predicted membership probability response base rate drawn population base rate fact drawn population base rate make base rate population belongs formally subpopulation probability unchanged show bayes rule mutually exclusive denominator note true probability base rate rely probability learning process independence independence made naive bayesian classifier classifier yield probability base rate probability base rate view remarkable classifier learned training drawn probability distribution test drawn probability distribution thus relaxes machine learning training test drawn population insight ratio probability find simultaneous fortunately need know ratio case worked independently provost case also know base rate time learn classifier reasonable training probability population test base rate plotted lemma slight make target probability threshold correspond probability threshold training multiplied want adjusted base rate classifier trained base rate probability corresponds probability classifier trained base rate need adjusted collecting term left give adjusted base rate training multiplied adjusted base rate note cardinality subset training must distribution subset changing base rate changing training prevalence learning natural learning separately many duplicating discarding rare learning prevalence approximately kubat matwin japkowicz investigate changing base rate bayesian learning bayesian classifier applies bayes rule probability learned training training frequency indirectly bayesian learning learns separately frequencyof training base rate little expect bayesian classifier base rate naive bayesian classifier case bayesian classification naive bayesian fier classifier tend give inaccurate probability domingo pazzani naive bayesian classifier computes close close ranking naive bayesian sifiers tends fact suggests sensitive optimal probability threshold empirically threshold procedure changing proportion threshold tree growing turn tree learning phase phase tree grown phase node pruned tree separately phase changing proportion training splitting criterion homogeneous induced subset training partitioned discrete case splitting criterion form probability frequency training impurity heterogeneity subset training qualitatively drummond holte twovalued impurity suggested kearns mansour proportion training data applies discretevalued show impurity gini breiman base rate collection minimizes regardless base rate training bayes rule grouping give base rate brought outside time tribute mined depend base rate base rate unless desired correlated splitting reasonable changing proportion training tree tree growing impurity criterion criterion entropy impurity criterion drummond holte dietterich show criterion normally lead unpruned tree sometimes lead accurate tree never lead much accurate tree recommend regardless impurity criterion applying much influence growing phase tree learning tree pruning pruning tree highly sensitive prevalence training rare prune tree node classifies classifier useless failing recognize rare examined good probability tree bradford provost domingo zadrozny elkan clear smoothing adjust probability leaf tree clear pruning best bauer kohavi suggest pruning best tree probability smoothing conclusion bradford best pruning pruning call laplace pruning idea laplace pruning laplace smoothing training reach node node loss node smoothed probability cost matrix training loss node loss child prune child show intuitively laplace pruning pruning absence probability smoothing loss node loss child equality hold optimal predicted child optimal predicted parent absence smoothing step tree predicted tree laplace pruning pruning probability smoothing loss node loss child must caused smoothing smoothing presumably equality pruning child simplification leaf tree unchanged note laplace smoothing tree node node growing tree done tree probability preferable pruning cost made smoothed probability cost fixed cost matrix node unpruned tree labeled optimal predicted leaf leaf node labeled subtree node eliminated simplification make tree prediction conclusion reviewed behind optimal learning misclassification loss case rigorously decrease proportion training make optimal classification classifier learned learning investigated bayesian tree learning concluded changing balance training little learned classifier accordingly recommended domain differing misclassification cost learn classifier training smoothing probability adjusting threshold empirically bauer kohavi eric bauer kohavi empirical voting classification bagging boosting variant machine learning bradford bradford kunz kohavi brunk brodley pruning tree misclassification cost proceeding european machine learning page breiman breiman friedman olshen stone classification regression tree wadswoth belmont california dietterich dietterich kearns mansour applying weak learning framework proceeding thirteenth machine learning page morgan kaufmann domingo pazzani pedro domingo michael pazzani independence optimality bayesian classifier proceeding thirteenth machine learning page morgan kaufmann drummond holte chris drummond robert holte exploiting cost sensitivity tree splitting criterion proceeding seventeenth machine learning page japkowicz japkowicz imbalance significance proceeding artificial intelligence vega june kearns mansour kearns mansour boosting tree learning proceeding annual symposium computing page kubat matwin kubat matwin addressing curse imbalanced training sampling proceeding fourteenth machine learning page morgan kaufmann margineantu dragos margineantu probability classifier note learning machine learning june michie michie spiegelhalter taylor machine learning neural statistical classification elli horwood provost domingo foster provost pedro domingo improving probability tree cder stern school business york provost gary foster provost distribution classifier learning rutgers zadrozny elkan bianca zadrozny charles elkan learning cost probability unknown california diego january zadrozny elkan bianca zadrozny charles elkan obtaining calibrated probability tree naive bayesian classifier proceeding eighteenth machine learning mining rule textual data yong nahm raymond mooney texas austin pebronia mooney text mining concern unstructured textual data task rule relate word phrase task learn rule utilize work textual data rule induction system textrise feature combining learning applying textrise corpus book patent retrieved rule text mining discovering unstructured text data mining attracting hearst feldman mladenic mining rule text hard criterion rule feldman hirsh text processing form soft utilizes give superior salton cohen yang induction rule text textrise learning softmatching rule text modification rise domingo learning good match text mining rule soft specified textrise retrieval salton applying textrise text database book extracted bookstore patent discovered rule data measuring predicted text text comparing prediction made mined association rule demonstrate mining rule background mining rule text rule induction discover relationship textual data fact feldman hirsh discovers rule text association rule mining discovered rule iraq iran kuwait bahrain saudi arabia corpus reuters news ahonen also data mining discover episode rule text chemical processing consequent word word storage word episode rule discovered collection finnish legal tree rule learner foil ripper used discover textual data nahm mooney ghani discover rule requiring match mining extracted text nahm mooney introduced alternative framework text mining integration extraction data mining form shallow text locates piece data text data mining assumes form database unfortunately many form unstructured address transforming corpus textual structured database module extract data text resulting database processed data mining work extracted textual data mined rule induction system quinlan ripper cohen rule induced predicting text slot extracted slot heterogeneity textual database referred textual clear vast dynamic warehouse text target text mining centralized moderator highly heterogeneous difficult strict text extracted cohen retrieval used text vector real corresponds word frequency also cosine angle vector representing calculated norm vector tfidf term frequency inverse frequency weighting scheme salton used assign distinguished term tfidf make term term term frequency throughthe collection characterize well inverse frequency tfidf framework term frequency term collection term rise learning rule rise induction unifies learning domingo requiring rule match exactly rise make prediction selecting closest rule used modified euclidian generating rule remembering make prediction elegantly combine property rule induction instancebased learning rule acquired induction maximally rule repeatedly minimally generalizes rule nearest unless decrease book title harry potter goblet fire book joanna rowling comment book best book ever read excitement book want read subject fiction mystery magic child school juvenile fiction fantasy wizard joanna rowling title harry potter goblet fire book comment book book read excitement read subject fiction mystery magic child school juvenile fiction fantasy wizard book rule base training data process repeat decrease classifying nearest rule used predict used rule base training data classified rule extensive rise fairly consistently accurate alternative instancebased training also reasonably computationally requiring time textrise rise applicableto mining rule extracted text learns rule classification text prediction textrise address list word slot filler eliminate stemming show book extendedto obviousway peterson learned rule antecedent subset slot conclusion predicted slot cosine used extracted rule slot intersection used rule slot rule said antecedent extend classification text prediction rule cosine predicted filler filler predicted rule rule possibly empty rule rule training rule textrise repeat rule covered replaced identical rule textrise eralizing rule learning rule straightforward modification rise used induce rule predicting filler slot slot make mccallum text processing interestingness textrise unordered soft rule ranking rule interestingness help user promising relationship evaluating interestingness goodness mined rule confidence bayardo agrawal match consequently modify judging goodness rule rule antecedent consequent antecedent data word closest rule rule base rule consequents data replace rule weaker relative rule rule base rule data domain employed textrise book data patent data downloaded manually system wrapper automatically extract labeled html text extracted slot processed removal remaining html command book data composed subset fiction literary fiction mystery romance child book title genre make size book data wrapper extract slot title subject term synopsis review comment sample rule domain word occurrence rule also patent collected dynamically page returned artificial intelligence four slot title extracted patent sample rule rule learner predicts presence absence slot textrise predicts slot textrise measuring cosine predicted slot filler slot system show textrise compressed rule base superior predicting prediction made selecting closest text antecedent slot also tested extraction show benefit text mining clearly show role made treat antecedent slot book cross validation learning curve predicting title slot graph show confidence statistically evaluated paired rule book title nancy drew synopsis nancy subject child fiction mystery detective juvenile espionage keene carolyn synopsis role protein absorption metabolism vitamin mineral review subject physiology title nutrition beatrice gormley synopsis witness landing subject fiction review alien book title charlotte perkins gilman synopsis work utopia herland ourland review gilman subject literature criticism classic woman literary comment utopia feminist title dance subject romance fiction sample rule book training size pair system textrise nearestneighbor extraction statistically textrise best worst show trise successfully summarizes data form prediction rule rate textrise mean rule textrise originally stored rule base conducted slot predicting slot predicting slot neither extraction textrise improves textual analog recall recall predicted rule patent device invention system data control stored user flow operation storage title title automated device determining comprising plurality comprises apparatus sample rule patent training book data title harmonic mean recall learning curve textrise conclusion many rule moderately increased textrise rule mining association rule apriori agrawal srikant publicly borgelt treated word slot item association rule word slot predicted test prediction made conclusion association rule default training book data title parameter confidence prediction lowered confidence memory occurredon ultra sparc lowest confidence remains training training strongly suggest usefulness rule prediction task textual data system mine rule text feldman hirsh ahonen discover rule automated extraction ghani rule induction database corporationsautomatically extracted rule advertising agency tend located york discovered learned rule must exactly match extracted text whirl processing system combine database introducing soft join operation cohen whirl textrise rule text processing rule whirl must user textrise discover rule automatically work system generalize closest rule nearest rule prediction rule combined taking vector consequents likewise learning rule nearest uncovered averaging possibly rounding training book data title maintain simplify resulting rule potentially semantic hierarchy wordnet fellbaum term thermodynamics optic physic extracted edit wagner fisher textual cosine evaluating interestingness textmined rule clearly idea semantic network like wordnet semantic word antecedent consequent rule preferring surprising rule ranking rule beer diaper beer pretzel beer pretzel food closer wordnet preliminary encouraging planning corpus database patent grant foundation gathered cora researchindex conclusion discovering textual data exiting area data mining system discover rule exactly substring variability diversity data form soft textual system textrise learning discover rule textual database automatically constructed corpus extraction encouraging preliminary showed induce accurate predictive rule despite heterogeneityof automatically extracted textual database acknowledgement foundation grant agrawal srikant rakesh agrawal ramakrishnan srikant fast mining association rule proceeding database page santiago chile september ahonen helena ahonen oskari heinonen mika klemettinen inkeri verkamo applying data mining descriptive phrase extraction collection proceeding ieee forum page santa barbara helena oskari heinonen mika klemettinen inkeri verkamo text phrase combining frequent ronen feldman proceeding sixteenth joint artificial intelligence text mining foundation page stockholm sweden august bayardo agrawal roberto bayardo rakesh agrawal mining rule proceeding fifth data mining page diego august borgelt christian borgelt apriori http cohen william cohen fast rule induction proceeding twelfth machine learning page francisco cohen william cohen access textual proceeding sigmod data page seattle june domingo pedro domingo unifying instancebased induction machine learning feldman hirsh ronen feldman haym hirsh mining association text presence background proceeding data mining page portland august feldman ronen feldman proceeding sixteenth joint artificial intelligence text mining foundation stockholm sweden august fellbaum christiane fellbaum wordnet electronic lexical database cambridge ghani rayid ghani rosie jones dunja mladenic kamal nigam sean slattery data mining extracted dunja mladenic proceeding sixth internationalconference data mining text mining page boston august hearst marti hearst untangling text data mining proceeding annual meeting association computational linguistics page college park june mccallum andrew kachites mccallum toolkit statistical modeling text retrieval classification clustering http mladenic dunja mladenic proceeding sixth data mining text mining boston august nahm mooney yong nahm raymond mooney mutually beneficial integration data mining extraction proceeding seventeenth artificial intelligence page austin july nahm mooney yong nahm raymond mooney extraction prediction rule text proceeding sixth data mining text mining page boston august peterson james peterson computation system august quinlan ross quinlan machine learning morgan kaufmann mateo salton gerard salton automatic text processing transformation retrieval wagner fisher robert wagner michael fisher correction association computing machinery yang yiming yang statistical text categorization retrieval weightvector color agent cube true agent case step agent make move simultaneously follow mitchell terminology work dual border maximally maximally data mining work full koller observational data incorporated simply empty observational data rule bayesian updating precisely throughout dirichlet session like trail relaxes coherence aswewillexplainlater theremaybefewervaluesin caseofmissingvalues datasets disadvantage learning hold part database disk carried size data computationally manageable case subset data page size fact relevant well data http filtering corpus http page manually classified relevance subject show prisoner page manually classified relevance subject beethoven biography usenet collected newsgroups message thread retrieved scott matwin explanation worth noting collection retrieved response pagerank term distinction play role kleinberg heuristic regarding
