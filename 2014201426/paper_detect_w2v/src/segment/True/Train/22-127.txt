show instancebased learning yield high classification storage supervised learning learning highly sensitive noisy training describes instancebased learning detecting removing noisy evidence significantly good classifier used classification task show degrades slowly presence noise improves classification reduces storage artificial database learning notable employ incremental learning cost storage exemplar learn continuous kibler learn nonlinearly separable successfully varied task speech bradshaw handwritten letter identification kurtzberg connell utgoff load forecasting jabbour thyroid disease diagnosis kibler highly sensitive noise tolerate noise necessity robust practical learning demonstrate graceful degradation noisy data pruning test statistical significance tree learning tolerate noisy data quinlan niblett bratko noisetolerant rule learning clark niblett work partially grant hughes malibu machine learning introduces also form significance testing identifies eliminates noisy show resulting classification degrade linearly linear noise artificial domain improves classification complicated domain also quinlan tolerates noise learning induce neither rule tree type solely pair incrementally derive training classification made derived classification framework precisely consist normalized yield classification classified yield classification classification expressed name memory updating classified classification normalized necessarily fair relative saliency negation euclidean left also employ tolerating missing calculating computing pairwise pair missing assumed maximally employ nearest neighbor neighbor classification former classifies latter take majority vote family eight summarized differ memory updating simplest proximity save training domain exhibit regularity make thern amenable remaining save misclassified training assumed differ memory training classified discard classified replace classifying classified growth disjunctive spanning bradshaw identical former latter also variant storagereducing ntgrowth disjunctive ntgrowth growth disjunctive spanning growth respective differ respective maintain classification incorrect classification training significantly good classification acceptable classification task discard noisy classification poor classification training classification acceptable training none acceptable closely rnimic acceptable classification kibler employ significance test decide acceptable noisy neither acceptable classification statistically significantly frequency dropped statistically significantly confidence constructed around frequency lowest frequency greatest dropped highest frequency lowest confidence constructed hogg tanis page make difficult employing high confidence acceptance confidence dropping like drop even moderately poor classification machine learning benefit trained growth ntgrowth drawn containing reveals training mislabeled probability trial growth mislabeled noisy invariably recorded poor classification training ntgrowth accepts significantly good classification distinguish noisy good classification fact show ntgrowth successfully filtered noisy ntgrowth good classification case ntgrowth averaged trial growth noisy mere ntgrowth mislabeled experimented proximity growth ntgrowth averaged trial noise summarized discover degrade noise varied dependent classification storage percentage mislabeled classification equally well noise ntgrowth degraded slowly linearly noise storage proximity training growth storage much asymptoting ntgrowth significantly growth asymptoted zero none significantly good high noise ntgrowth percentage noisy proximity increased linearly noise growth percentage noise rose quickly ntgrowth filtering drastically slowed influx noisy classification noisy distinguish noisy poor classification nearby neighbor invariably classification ntgrowth degraded slowly artificial domain encouraged test noisetolerant challenging domain benefit recur practical quinlan modification quinlan pruning also tested database summarized also benchmark frequency guess highest frequency comparative utility disjoint training test database waveform domain breiman artificial domain noise chance noisy waveform noise domain outperformed respective unextended growth training reach kibler also recorded equally good classification incomparably storage respective proximity cleveland hungarian database consist cardiological recorded cleveland clinic foundation hungarian cardiology domain great deal noise detrano reported discriminant predicting heart disease resulted approximately significantly outperformed voting domain noise payoff noisy domain ntgrowth disjunctive spanning well tumor domain growth training well growth disjunctive spanning recorded classification storage ancestor also classification good proximity growth recorded storage learning curve much slower learner ancestor able good growth ntgrowth disjunctive spanning recorded consistently high classification ntgrowth highest recorded database well database limitation suspect tree learning learn namely disjuncts piecewise linear kibler behavioral tree rule learning form extensive training hyperrectangular form closely resemble piecewise linear tree probably learn slowly aligned dimension fact reported learn tree shepard cart tree learning employ linear perceptrons node form piecewise linear breiman requiring explicit best hyperplane separator machine learning node convenient learning type incremental learning requiring examination size training significantly fewer examination incremental variant schlimmer fisher examination incorporate incremental variant utgoff examination plus examination pullup iaii size domain pullup process neither cost recursive pullups partitioning pruning indicated tree build huge tree learning relevant bloom ntgrowth address learning relative relevancy bloom overlapping graded tree build tree recorded classification ntgrowth summarize combining instancebased tree concise hierarchy thus incorporation classification cost tree benefit storage incremental learning cost reasoning indexing scheme bareiss porter likewise assist assist judging utility case distinguish noisy case made distinction conclusion learning showed ntgrowth degrades gracefully presence noise recorded storage classification domain artificial gain occurred decreased noisy used classification contribution voting combined statistical test assist removal noisy like pruning tree quinlan niblett bratko testing clark niblett upon significance test tolerates noise gathering evidence correctness employing classification amalgamation yield compilation form rule tree superior learning updating cost storage concisely acknowledgement thanks peter clark doug fisher wayne langley haym hirsh jeff schlimmer wogulis david ruby marc albert stephanie valuable comment draft like thank zwitter soklic oncology ljubljana yugoslavia donating tumor database robert detrano long beach cleveland database andras janosi hungarian cardiology budapest hungarian database database repository machine learning database
