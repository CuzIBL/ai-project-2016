identify type laser data feature extraction classifier collective classification utilizes associative markov network amns transform feature vector separable linear hyperplanes learned classifier extensive recorded indoor scene well classifier classification rate substantially exceeds identifying believe segmentation data many area registration robot localization manipulation robot able classify data acquired sensor data scan registration carried reliably area task aimed robocup rescue initiative make hard fact segmentation classification segment simultaneously segmentation classification collective classification many domain spatial neighbor data tend label anguelov formulate segmenting data supervised learning task collective classification task associative markov network amns combine learning collective classification amns used mean linear relationship extracted feature parameter learned classification learn feature many linear separability feature justified overcome extend subject ongoing combining idea classification classifier robust introduced feature vector feature transform feature show feature suited classification next give work area summarize collective classification associative markov network shortly learning inference done amns illustrate classification work recognizing data studied intensively past distinguished feature used spin johnson alarcon frome rotationally translationally descriptor spin also used feature work type feature tensor mian feature guskov osada distribution view deal partially seen additionally huber classification nearby part identify help guide classification idea detecting introduced surface boykov huttenlocher markov mrfs classify camera incorporating statistical relationship nearby part mrfs limketkai idea spatial relationship nearby case consist line segment work mostly anguelovet used supervised learning classify data combine classification classification scan classification data scene data scan grid cell occupancy grid identical case data also feature vector feature task find label label optimal feature optimality classification supervised learning likelihood label feature classification subdivided task need find good parameter likelihood seek good label maximize likelihood training data label hand formulate classification learning step find argmaxp inference step find argmaxyp collective classification classification bayes classification nearest neighbor adaboost classification data feature labeling nearby data practice observes statistical dependence labelings neighboring data planarity scan feature happen likelihood label label scan vicinity belong neighboring data collective classification chakrabarti indyk collective classification classify scan data anguelov associative markov network amns type markov taskar briefly amns associative markov network associative markov network undirected graph node case discrete correspond label data node edge graph also node edge node reflect fact feature vector label edge encode label neighboring node edge feature node edge high label label pair probability label feature also high probability network expressed remains node edge taskar vector introduced label node accordingly edge note vector node edge convenience slightly namely indicator label refinement step idea edge node label penalized edge equally labeled node last specification amns make inference step efficiently graph boykov taskar learning inference amns learning inference carried amns reformulate probability parameter expressed vector plugging yikykj logzw note label learning mentioned supervised learning task goal maximize arises mean maximizing intractable need done maximize optimal labeling labeling term cancel maximization done efficiently referred optimization omitted sake brevity note quadratic form minw slack refer taskar inference optimal calculated inference unlabeled test data done label maximize mentioned depend maximization carried considering last term imposed lead linear form ykij ykij ykij feature transform feature left training data test data ground truth labeling test data applying introduced ykij representing label edge last linearization ykij ourcurrentimplementation performthe learning inference step quadratic linear ooqp gertz mentioned inference step efficiently data comparably linear turned fast main drawback classifier linearly assumes feature separable hyperplanes justified hold classifier nearestneighbor classifier classification data label corresponds training data feature closest feature learning step classifier simply training data training parameter also lazy classification idea combine instancebased classification collective classifier restricted linear separability next transformed feature vector training data test data ground truth iamn data occupancy grid classification training data feature vector label label feature vector corresponds label closest training feature yield optimal classification label label feature depicted left part feature training data true label arbitrary test data triangle label closest training feature probability label proportional gaussian distribution training label closest feature variance left side hyperplanes case line lead severe classification feature transform transformed feature separable hyperplanes illustrated side classifier chooses label smallest mean transformed feature label seen border hyperplanes passing origin case separating hyperplane line nearest neighbor classifier assignment label labeling training namely feature vector closest feature training also close labeled differently closest training feature label assigning label wrong presence noise proceed feature vector corresponds nearest training used transformed feature vector show classification rate turned good series data iamn classifier demonstrate iamn outperforms feature used learning process need feature vector nearest neighbor feature efficiently training feature vector computational nearest neighbor lookup logarithmic stored thus training step computing feature training data transforming feature assigning transformed feature node edge feature consist scalar anguelov quadratic vector inference step transformed feature test data node feature edge feature feature computation data used type feature data case grid data geometrical feature extracted laser scan covering view laser simulated centered representing free mozos geometrical feature huge best adaboost data spin johnson size spherical neighborhood computing spin radius resolution data ground truth iamn classification scene multi data data reduction classifying data many scan scan time memory classifier grows case data reduction inserting data tree prune tree fixed subtrees merged mean spin feature whole data high density feature extraction annotation classification used occupancy grid interior annotated label namely divided submaps training testing show submap used training ground truth classification show iamn seen iamn give best classification rate classification noisy iamn consistent neighboring data highest classification rate scan classification evaluated classification data scanned scene scan laser scanner data recorded scene varying pose scan labeled four data scan scan data screen standing last data multi seven scan type data data evaluated cross validation multi data used training occluded classification scan show classification scan multi test assigns wrong label neighbor classified show area tend classified label restriction linearly separable data many part misclassified like chair iamn even transformed feature vector iamn suited classification separating show resulting classification rate iamn classifier outperforms data statistical test significance seen multi significantly classification depicted wchich demonstrates iamn yield highly accurate data data iamn multi classification data conclusion combine associative markov network nearest neighbor classifier identifying classification rate scene test significance data multi data classification data suggested anguelovet able classify diverse feature feature nearest neighbor transformed feature linearly separable accordingly improves training step inference task drawback storing resulting classifier lazy classification inference step classified training computational utilizing data like computational logarithmic stored despite encouraging warrant classify classifying part idea huber feature distributed compactly feature acknowledgment work partly german foundation european
