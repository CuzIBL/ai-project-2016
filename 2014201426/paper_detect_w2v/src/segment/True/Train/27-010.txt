optimaj uncertain central artificial intelligence time modeled markov mdps studied extensively many determining timal realistic case partially observable partially observable markov process pomdps much best inefficient lime smooth partially observable spova quickly yield good time mediod combined reinforcement learning meth test case markov process mdps proven domain framework used practical unfortunately many domain modeled mdps domain stale observable time modeled mdps partially observable markov process pomdps extend framework partially observable able able mdps pomdp much computationally tensive counterpart uncertainty true induces probability distribution work determining finite discrete pomdp forced deal probability distribution discrete optimization continuous manifested pomdp learning best take prohibitively time even smooth partially observable approx imation spova smooth adjusted gradient rule amenable reinforcement learning permit agent gradually time lest case agent rule rapidly tion time pomdp formalism show observability introduces smooth spova gradient spova find optimal test simulated exploration reinforcement learning introduced showing rapidly find good briefly work concluding markov process modeling uncertain domain markov process divide transition probability mate summarizes probability next slate acuon formally wesay forany slate markov property finite stales finite mapping distribution paradigm distribution distribution specifies slate mapping explicit mapping implicit real represents garnered forward assign choosing maximizes succeeding type step permitted hard fixed case time incorporating cost step discounting discount optimal used induce optimal agent acting optimal great agent determining optimal formdps determining infinite horizon case iteration bellman transition probability reinforcement learning sutton used learn optimal exploration maintained reinforcement learning used learn learning watkins maintain regard simply distinction observability realize uncertain outcome mdps never uncertainly slate taking agent uncertain consequence agent know outcome unrealistic agent sensor distinguish slate partially observable markov process pomdp like attached slate thought sensor lhai uncertain hint true sensor formally pomdp stales note thai assigns stale stale pomdp slate observable pomdps make distnbutions dislnbutions lhal determines iniual distribution stales observability force type agent acting must maintain observable case agent know slate partially observ able case agent wish optimally musi maintain considerably possibility history made arbitranly maintenance joint probability distribution stales tractable distnbution sometimes referred belief markov properly hold belief stales induced pomdp mean pnnciple construct belief pomdp find optimal pomdp unfortunately pomdps induce infinite belief stales pomdps impractical survey pomdp algonlhms lovejoy show lhal many pomdp algonlhms work constructing finite belief stales ihen iteralively updating expanding desired reached sondik finite expanding converges stable practice infinite horizon approximalcd long finite horizon even convergence regardless convergence take exponential time even exponential size drawback algonlhms work discretizing belief dynamically adjust resolution discretization part belief unclear done efficiently worth noting reader unfamiliar area pomdps stales pomdps stales lake anywhere minute convergence deter mining infinite horizon pomdps mediod differs continuous differenliable differentiable perhaps thai must made sondik showed optimal finilehonzon finite linear belief belief stale vector representing distribution vector dimension defining plane belief fixed maximizing find best belief littman indepth vector graphically hyperplane form convex piecewise linear surface significance sondik potentially compact mean representing optimal make tractably horizon smooth comprised vector infinite horizon comprised infinite piece mean smooth part belief case linear convex good candi date differentiate infinite horizon parr russell closeness vajue convex behaves like work nicely foundation smooth made spova keep true come loss generality part done replacing offset denva live dimension convex behave like smoothed corner show work graphed differentiate approximator convex piecewise linear smooth curve belief dimension minus convex surface linear height induced relative absolute assignment parameter give great deal flexibility believe infinite horizon linear close hand believe optimal infinite horizon highly textured requiring time spate smooth partially compensate vector spova main continuous gradient adjust soft mardnetz ctal learning parameter ideally data optimal func lion construct iteration mdps make look like know iteration optimal mdpmust pomdp induces belief pomdp know must hold optimal pomdps well give improving inconsistency adjust parameter rectus minimizes inconsistency done computing bellman residual bellman next belief slate reachable taking adjust minimizes smooth able gradient step size learning rale case correspond vector thejth gradient appealing property part contribution make contribute multiplied reflecting influence probability gradient also interpret system many gradient permit minuscule show spova algonthm spova impossible sample belief used selecting belief empirically best varied linearly reached requested make smoother also spread wider area sense system gradual thought form simulated annealing repealed termination dition fixed iteration consecutive sample processed exceeding threshold perhaps something convergence algo nthm optimistic iteration decaying sufficiently lends toward infinily converge optimal finite piecewise linear arbitrarily close bellman residual system move stable equilibrium consistent optimal belief addressed pick vector vector gradient adjust vector even convergence automatically optimal vector converge practical corporate code hand binary find smallest vector give optimal worse best produced vector spova tried spova initialized vector grid thai appeared literature cassandra adjacent permitted four compass move edge returning agent indication thai anything unusual happened slate zero appearance bottom nght stale distinctive appearance distribution bottom boliom transition maining zero slate probability distribution interested optimal discount momenl thought clear opumal alternate moving east south thus mean optimal infinite iteration gradient obtainable fact belief stales reachable optimal belief vector sondik gradient vector iteration resulting optimal iteration laking snapshot func tion simulating step counting slep garnered opumal lime show graph garnered step iteration horizontal line optimal witness cassandra perhaps fastest algonthms time minulcs russell andnorvig obstruction labeled pair left discounting penalty charged step stales zero absorbing stale originally used observable made partially observable limning lhat wall detector activated uiere wall adjacent make indistinguishable slate nonterminal stales transition deterministic succeeds wiih probability fails proba bility morning agent perpendicular obstructed wall ihen agent stay moving move agent witi probability probability nowhere probability gradient iteration vector many sample minute data relevant calculate approximately optimal slep hour parh russell witness vector case witness converge littman convergence near convergence case good witness perhaps surprising thai vector drastically initially surprised discover vector linear part come fact simulation reachable belief like witness construct belief also many vector perspective sufficient know relative belief much assigns belief slate suggest agent closer southeast comer sufficient linear smooth also reduce vector bend formed many hyperplanes approximated closely smooth bend reinforcement learning straightforward gradient bnng fairly close sufficient iteration shortcoming learning transition applies gradient belief stale encountered simu lation maximizing successor belief time belief time sufficient exploration chose vector guaranteed overestimate belief forced disprove optimistic visiting area belief simplistic sufficient investigating used mdps convergence stronger belief covered find approximately optimal iteration iteration focusing belief able learn nearly optimal extraordinarily quickly come expense accurate rarely visited acceptable price many domain final investigated vector intuitively linear much worse definitely show namely effectively approximates optimal showing epoch showing epoch domain requiring nonlinear labelled indistinguishable lead agent lead distinctive enables agent find vector spova find horizontal line vector quickly find optimal onevector iteration abandon favour eventually reach optimal horizontal line work many pomdp done determining best ordinary agent consult simulate step next sidles agenl need look ahead fashion case learning even construct learned agent experience property pomdps agenl must know something dynamic compact stale maintained time evolved agent forced guess true enure history thus reengineenng pomdp func fundamentally alleged mdps fact spova trivial observability simply pretend sensor correspond exactly deterministic constructed sensor fail miserably resulting looping alleviated extent tondomized kind game jaakola learn reinforcement demonstrating thai unreasonable case linear approximator combined clever learning mccallum chnsman generalize like spova neural network used mitchell vanely parr russell thai make agent history learn hidden stale idea smoothed soft around idea behind boltzman distribution watkins used neural network martinetz suspect adapt approximators pomdps investigated work littman rule inde pendently case spova adequate determining good conclusion work investigated spova scheme partially observable markov continuous differentiate iteration gradient sampling find approximately optimal sample belief slate conjectured many sample correspond unlikely even unreachable belief reinforcement learning algonthm belief slate encountered exploration slate spova able quickly suggesting optimism concerning pomdps justified nexl step tackle vergence incorporate learning investigating algonthm learning dynamic probabilistic network dpns russell find decomposed stale handled thermore belief stale facilitate plan learn automobile acknowledgement berkeley daphne koller helpful suggestion feedback idea contained grateful tony cassandra michael littman sharing witness algonthm algonthm michael littman also extensive comment early draft huang help formatting
