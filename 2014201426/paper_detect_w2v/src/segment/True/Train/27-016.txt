review crossvalidation bootstrap artificial data cult restricted selecting good classifier classifier largescale half million parameter gonthms datascts crossvalidation vary fold fold stratified bootstrap vary bootstrap sample datasets best fold stratified cross validation even computation fold emphasized whatsoever made practice real made cross validation real wolpcrt estimating classifier induced supervised learning predict prediction also choosing classifier combining classifier wolpert estimating final classifier like bias variance classifier combine classifier absolute willing trade bias retrieved anony mous starry htanford variance bias affect classifier eslimates pessimistic made concrete fails corrert time wolpert schaffer inter ested identifying well suited bias rids real datasets alwa case computational cost beneficial especiallhy relative unbiased high variance leading unreliable efron linear asymptotically consistent sense probability selecting best predictive converge lolal infinity shao describes computing confidence hold work comparing variant bootstrap variant lion conelude classifier unlabelled label data inducer induction build classifier dataset cart brennan friedman olshen stone quinlan tree inducer build tree classifier interested inducing classifier access dataset inducer unlabelled kohavi label labelled dataset possibly multiset consisting labelled classifier beled inducer dataset classifier label unlabelled stance classifier built inducer dataset distribution labelled dataset independently identically distributed misclassificalion cost loss loss classifier probability clasaifying randoml probability distribution distribution used inducer training finite dataset like custimate classifier induced inducer dataset meaningless confidence thus identify weakness also identify case fail holdout holdout sometimes test sample data mutually exclusive subset training test holdout lommon designate data training remaining test training inducer induced classifier tested test formally holdout subset size holdout inducer seen holdout pessimistic estimator data inducer training leave test bias fewer test mean confidence wider test viewed bernoulli trial incorrect prediction classification test distributed bmomially bernoulli trial reasonably holdout distribution approximately normal mean true learning classifier variance thus quanl normal distribution confidence determines inverts inversion lead quadratic root high confidence conditioned dataset probability dataset must holdout training test sampling holdout repeated time eslimated derived averaging slandard deviation dewation holdout violated subsampling independence test training training test formed dalaset subset demonstrate simulated fisher famous dataset used majority inducer build classifier dieting prevalent training iris dataset describes plant four continuous feature task classify setosa versicolour virginica label exactly label thus expect prediction test prevalent training predicted holdout deviation averaging holdout practice dataset size finite like holdout make inefficient data dataset used training inducer stratification sometimes rotation dataset mutually exclusive subset fold approximately size inducer trained tested fold possibil ities choosing exrept crossvalidation estimating data fold repeating lime spill fold monte arlo complele cost stratified fold stratified thai tlicy approximately proportion label dataset inducer stable dataset perturbal induces classifier thai make prediction perturbed datasets variance fold dataset inducer inductr stable bations caused deleting fold fold cross validation stnnate unbiastd tail ance approximately accrv datasi classifier produced make prediction binomial distribution trial probabihly success classifier confidence reality inducer unlikely stable perturbation unless reached maximal learning expect perturbation induced classifier stable size perturbation stability hold expect stability hold stable holdout resubstitution inducer stable dataset help made inducer unstable dataset perturbation introduced unreliable inducer stable dataset expect reliable next take idea slightly show empirically variance cross validation fold varied variance dataset inductr inducer stable undfi tituibuhoris aused deleting test fold valuts vartanct variance depend inducer liktly inherently stable show must also take dalaset perturba failure lusher dataset leading expect majority indu acruraov eombmation dataset majority inducer unstable perturbation deleted dalaset label minority training thus majority inducer predicts errs classifying test majont inducer dataset fold thus deviation fold giving unjustified assurance stable show inherent crossvalidation applies majority inducer dataset label best induction predict majority dataset label majontv best inducer predict bootstrap family introduced efron efron tibshirani dataset size bootstrap sample sampling data dataset sampled probability sample kohavi distinct dataset appearing teat thus derived bootstrap sample training rest testing bootstrap sample bootstrap sample bootstrap resubstitution full dataset training variance puting variance sample made bootstrap basically stability dataset closely real bootstrap fails give classifier perfect memonzer unpruned tree nearest neighbor classifier dataset resubstitution plugging bootstrap real bootstrap fail memonzer module inducer adjust prediction memonzer remembers training make prediction test training adjusting prediction make resubstitution thus bias want work comparing previously done artificial datasets efron conducted five sampling variant bootstrap estimator seem considerably sample give nearly unbiased unacceptably high variability particularly sample bootstrap best breiman conducted tree pruning chose cart claimed satisfactory choosing tree claimed risk rule tends much accurate learning jain dubes chen bootstrap nearest neighbor classifier artificial data claimed confidence bootstrap estimator followed line stratified bootstrap nearest neighbor classifier stratified cross validation relatively variance superior breiman spector conducted feature subset regression stratified biascorrected bootstrap test done artificial datasets bias root mean crossvalidation bias many feature pessimistic bias sample significantly sample size bailey elkan crossahdation bootstrap foil inducer four synthetic datasets involving boolean high variability little bias variability bias indurkyha indurkhya conducted real data applicability tree pruning sample size stratified pruning yield unbiased tree optimal size conduct decided naive bayesian classifier quinlan descendent build tree classifier langley thompson used kohavi john long manley pfleger ratio nominal feature assumes gaussian distribution continuous feature crucial interested internals induction hypothesis tree statistic hope conclusion induction induction target unknown used holdout bootstrap datasets looked learning curve supervised classification dataaets irvine repository murphy contained datasets felt testing true real dataset know target esti mate true holdout true taking sample size computing rest dataset test repeating time chose datasets wide domain learning curve flatten early hundred also dalastt rand boolean feature boolean label dataset vehicle deteriorated iven nomenon shuttle dataset phenomenon predicted srhaffer wolpert schaffer wolpert surprised real datasets well form sampled dataset training desired size induction algorihm training tested classifier rest dataset repeated time learning curve sloping fold sample bootstrap used show significance bias follow variance lack omit graph approximately bias bias parameter minus unbiased zero bias show bias variance datasets breast cancer dataset clearly show pessimistically biased five fold learning curve derivative pessimism bias varying fold fold stand leave confidence mean gray confidence true curaries note axis validation apparent reasonably good fold fold unbiased stratified cross validation pessimism soybe fold vehicle thus stratification seems biased show bias variance bootstrap bootstrap unbiased chess hypothyroid mushroom inducer highly biased soybean vehicle inducer rand inducer bias vehicle variance bias case poor high variance formed confidence deviation mean switch deviation population deviation practice mean reported deviation averaged kohavi true datasets classifier sample size bias bootstrap varying sample good mushroom hypothyroid chess biased optimistically vehicle rand biased soybean deviation drawn deviation show deviation naive bayes varying fold stratified slightly variance show bootstrap high variance high variance file seven datasets stratification reduces variance slightly thus seems bias vanance learning deviation population line style used help differentiate curve reviewed holdout bootstrap showed fails good latter datasets differing show induction stable dataset variance crossvalidation approximately fold induction stable approximately stable fold moderate reduces variance bias decrease sample size variance instability training igure bootstrap deviation accrat population leading variance apparent datasets many soybean stratification seems help stratification scheme term bias variance regular bootstrap variance bias recommend stratified fold acknowledgment thank david wolpert thorough many thank bylander brad efron jerry friedman holte george john langley tibshiram sholom helpful nients suggestion sommcrfield bootstrap conducted partly partly funded grant grant
