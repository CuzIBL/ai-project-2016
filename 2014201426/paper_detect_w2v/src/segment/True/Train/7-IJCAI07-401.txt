proposes develops learning discriminative generative probability graph propagation linear regression datasets show superior learning labeled subset prof insufficient meaningful many real classification task labeled prohibitive cost manually labeling data unlabeled data easy classification supervised learning make labeled data insufficient address learning make unlabeled data boost supervised learning learning proved many digit classification medical segmentation grady word sense disambiguation retrieval learning tsvm joachim find hyperplane labeled unlabeled data learning make data distribution revealed unlabeled data learning graph constructed labeled unlabeled data vertex many viewed estimating graph nearby feature label locally smooth consistent labeled data classification label comparing threshold gaussian harmonic learning formulated term gaussian graph mean serf consistency iteratively propagated neighbor label zhou graph mincut corresponds partitioning graph roughly minimizes pair label blum chawla mincut take binary till learning approached discriminative perspective graph corresponds probability discriminative unlabeled data necessarily clear explanation graph correspond probability statistic view learning generative perspective probability graph theoretically justified ideal case separable term eigenvectors graph converge probability training data infinity case good probability make labeled unlabeled data compensate lack label many practical show lead datasets stronger justification empirical rest probability deal followed outline give conclusion hint work estimating probability binary classification training sample sample labeled sample remaining sample unlabeled goal predict label computing probability bayes rule predicted generative calculate need estimating probability next form affinity matrix measuring diagonal matrix vector labeled ideal case ideal case case label data knew label sample sample affinity matrix thus symmetric matrix sample represents zero matrix sample training matrix diagonal matrix diagonal connects probability diagonal parameter satisfies supu nlimvn nlimnvn infinity converges straightforward kernel density kernel density labeled data labeled unlabeled data distribution time labeled data probability label ever unlabeled know corresponds address make eigenvectors easy show largest eigenvalue form graph eigenvectors chung construct eigenvectors eigenvalue zero vector obviously correspond convergence orthogonal converge converge upon convergence proportion probability normalizing empirical converges true infinity give density ideal case show training data moon labeled marked star show distribution density ideal case training data distribution case case satisfies sample infinity verge omit seen case fact limn lead case tends form graph eigenvector corresponds eigenvalue iterate convergence converge eigenvector hand operation seen labeled data gradually spreading nearby iteration step unlimited data equally influenced labeled data leading stopping criterion iteration process stopped criterion satisfied estimating probability iteration step normalizing summing probability labeled sample likelihood sample stop iteration derivative iteration step cross criterion justified iteration step data nearby labeled rate iteration proceeds data accumulated high propagate majority rate gradually converge data stable rate decrease reach plot curve iteration step convex concave convergence iteration step away labeled kind manifold stop explore data distribution misclassification cluster data transition convex concave stopping trade prematurity excessive propagation stopping criterion derived probability iteration step stopping criterion satisfied necessarily give density case showing effectiveness criterion show curve curve iteration step arrow curve criterion satisfied show distribution middle distribution moon recovered fairly well density case training data iteration distribution note stopping criterion heuristic trying stopping criterion principled estimating estimating learning labeled implicitly zhou obviously real proportion labeled data true labeled unlabeled data probability feed form linear regression estimator labeled data probability accurate thus reliable beta distribution distribution parameter labeled smoothing proportion sample labeled labeled data unlabeled data exploited compensate proportion labeled labeled data labeled data dominate prediction testing data classify data training calculate probability kernel regression probability training calculate probability make prediction procedure estimating summarized form affinity matrix calculate labeled assign normalize calculate likelihood labeled step unless satisfied remains converged remain derivative iteration step cross linear regression estimator calculate smoothed proportion sample labeled comparative datasets cedar buffalo binary digit database hull dataset learning gaussian consistency zhou supervised learning nearest neighbor proved gaussian kind balanced unbalanced balanced case ratio labeled unbalanced case explained labeled perturb labeled around gaussian distribution mean deviation gradually labeled data trial labeled data volume volume cedar buffalo binary digit database cedar buffalo binary digit database hull classification task classifying digit even digit digit data used data nearest neighbor case comparable balanced case mass normalization procedure adopted gaussian labeled make labeled unlabeled robust perturbation proportion data labeled genre dataset genre classification classify style political movie review genre dataset genre biography interview script movie review review release website political newspaper editorial politics news engine compose whole dataset processed vector frequent word dataset stemming header stop word removed unbalanced classification correspond balanced anced case balanced case gaussian consistency nice property unbalanced case gaussian tends suffer contrary robust despite perturbation classify labeled stable gaussian consistency largely affected misleading labeled depend labeled implicitly conclusion work novel learning probability generative discriminative ideal case probability proved converge true case reasonable probability data outside training probability kernel regression estimating effectively unlabeled data make labeled data unrepresentative distribution datasets demonstrate superiority learning proportion labeled case even labeled labeled brings classification case many labeled labeled help incorporating learning framework particularly interested determining invoke learning label biggest gain minimizing incremental labeling cost
