fisher introduced task conceptual clustering flexible prediction arbitrary simply prediction extends considering noise environmental degradation flexible prediction noise mitigated prediction identify prediction inspired pruning learning extend tolerant untutored learning prediction preference shed relationship conceptual clustering default reasoning machine learning traditionally concerned learning quinlan assumes identified priori disease learner must characterize conceptual clustering michalski stepp fisher cheeseman kelly self stutz taylor freeman discover well characterize meaningful learning organism task learning task apparent prediction membership diagnose illness otherhand conceptual clustering traditionally task michalski stepp cheeseman kelly self stutz taylor freeman fisher proposes task conceptual clustering flexible prediction prediction simply learning work grant vanderbilt council system optimize rect prediction congressperson political congressional voting like vote farm missile flexible prediction concerned simultaneously improving prediction dimension farm missile task learning implication system construction quinlan bareiss porter flexible prediction permeates reasoning despite flexible prediction system concerned kolodner lebowitz much systematically characterized explores noise flexible prediction fisher cobweb system weakness face noise motivate cobweb inspired pruning learning applicable untutored learning system shed relationship conceptual clustering default reasoning flexible prediction cobweb incrementally build classification tree nominal pair tree voting senator fisher stored node distribution classified node node classifies many senator voting probability node conditioned membership node parent probability trivially conditioned classification root probability classification node probabilistic smith medin classification tree probabilistic tree tree sibling fisher distribution permanently process recursively subtrees rooted child leaf reached leaf singleton represents previously predominantly incorporated operator also node creation node merging node splitting cobweb fisher incorporation adapted classification flexible prediction utility guide path node leaf missing predicted leaf cobweb tree reminiscent tree probabilistic polythetic guide classification missing utility summation probability classification tree constructed training distinct testing repeatedly classified tree repetition removed must predicted prediction test prediction tested size training curve show learning curve domain soybean case history stepp curve illustrate learning difficulty vary considerably quickly effectively learned simply guessing frequent yield training somewhere cobweb learns predict success probabilistic tree learning system separately fisher cobweb tree tree investigates environmental flexible prediction foremost noise incorrect reporting noise alters correlation inductive system learning replace soybean domain probability artificially noise prediction degrades roughly influence prediction extent training also perceived statistical relationship investigation motivates machine learning tual clustering preference quinlan extensively investigated detrimental noise prediction decomposed training classified tree node quinlan demonstrates tends data noisy domain classification leaf guided spurious unjustified rule benefit actually detract prediction quinlan breiman friedman olshen stone explored pruning unjustified rule mitigate noise pruning quinlan tree node decomposed lead distribution child significantly distribution node divided distribution differ significantly subtree pruned deeper classification benefit prediction benefit pruning learning noisy domain motivate exploration benefit flexible prediction cobweb modified test statistical significance distribution node distribution node child distribution differ significantly confidence threshold prediction note flexible prediction simply prune subtree significance test pruning finality identify preference test maintained prediction classification proceed preference deeper preference predicted heuristic natural domain soybean disease case history domain poisonous edible mushroom voting vote prediction test checked regular training varying noise confidence threshold threshold case confidence distribution threshold classification leaf identical cobweb confidence never achieved disallows classification root prediction training plot learning curve mushroom domain noise confidence threshold confidence best early training predictive exposed reached statistical significance degree confidence preference high threshold converge learning also fisher schlimmer graph examines noise graph show averaged training noise optimal threshold tends noise correlation noise optimal confidence threshold pearson coefficient sample noise deeper classification beginning confidence overfitting noisy domain confidence significantly data optimal confidence threshold noise training asymptoting high confidence threshold early fisher training noiseless classification leaf optimal confidence threshold maximize learning thus unless make priori noise training data inconsistent weakness introduces mean preference identification make noise training preference ward heuristic prediction missing node historically facilitated greatest prediction training recursively classified node node predicted node node maintained time predicted node training also kept time predicted node descendent latter recursive classification procedure unwinds prediction node remembered used ancestor leaf predicts descendant predict initialized initialized predict missing classification proceeds node reached historically outperformed descendent term predicting missing forwarded answer intent breiman friedman machine learning shen stone pruning quinlan pruning latter tree constructed training tree used classify test test item classified determination made classified node node path leaf tree pruned node maximized prediction test test tree engineered training past applicable flexible prediction test tradeoff simultaneously prediction many introduces tradeoff identical preference roughly match optimal threshold averaged training noise importantly optimal threshold differs training noise mushroom soybean congressional domain past mean significantly confidence ranging confidence confidence averaged hide fluctuation conveniently accurately reflect past simplicity effectiveness preference underscore overt best simplest noise training past make priori noise extent training preference accumulated prediction node descendent flexible prediction suggest conceptual clustering area default reasoning default used uncertain reasoning brachman automated reasoning literature make mention prescriptive mean assigning default coincidentally little work default maintenance learning prescription default identification kolodner cyrus system probabilistic default normative true percentage normative default toplevel node congressman vote classification deeper democrat vote unfortunately normative parameter delimit true fail capture subtle best predict true preference address demarcating predicted nonetheless remain unaddressed preference absolute presumption goal process default sensitive evidence feature idea cease classification utility best node alternative sufficiently evidence sufficient distinguish classification path default unless evidence classification procedure line automated default reasoning work seek flesh relationship incremental formation prescriptive default identification exploitation reasoning reasoning emerged subfield automated reasoning learning subfield distinguished reliance case emergence subfield unfortunate segregating specification differ perhaps cobweb prediction best leaf previously thus cobweb reasoning help identify case logarithmic time linear time case concern illustrates utility inferencing reasoning productive training noise ashley rissland overfits data training noise past emergent case early training case initially tree viewed predicting seen tend formative gradually solidifying correlation node reliable classifier past emergent viewed probabilistic conservative optimal prediction alternative reasoning retain case presumably selective retention overcomes hamper nonselective reasoner selective retention employed bareiss porter kibler respective learning system nonetheless retention difficult guidance abstracted kibler difficulty dealing irrelevant bareiss porter implicit abstracted strict adherence feasible learning difficult imagine flexible prediction many prediction dimension must simultaneously coordinated fisher concluding cobweb uncovered data overfitting mitigated adapting pruning flexible prediction constantthreshold like lead nonoptimal predict noise training limitation preference past classification cease historically outperformed descendent thus impose noise training statistical interdependence cobweb constructing clustering whatever interdependency evident prediction nothing past used guide classification also implicit cobweb utility expectation prediction afforded tied task benefit clustering explicit consideration task improves learning distinguishes cobweb clustering work free system parameter distributional kind acknowledgement thank jungsoon yang bareiss gautam biswas informative comment draft dennis kibler john gennari style considerably reviewer helped clarity correctness
