accurate tracking goal ubiquitous computing wearable sensor gather rich datasets automatically identifying feature modeling remains largely unsolved recognizing combine boosting discriminatively feature learn ensemble static classifier recognize hidden markov hmms capture temporal regularity smoothness tested system hour data collected volunteer natural unconstrained succeeded identifying maximally informative feature able identify task modeling sensor year ubiquitous computing ubicomp intille lukowicz patterson originally done sensor gavrila pentland increasingly dominated type wearable sensor like accelerometer audio fertile domain care arena elder care assisting cognitive disorder modeling tracking routine ritual social majority wearable device concentrated sensor modality accelerometer body intille kern placement sensor obtrusive limitation ultimate goal embed device clothing commercially widely sensing device integrated cell appealing user garner user acceptance work intille sensor subset significantly system five sensor sensor hypothesis incorporating sensor modality offset lost sensing device modality suited rich perceptual modality fails capture modality promise lukowicz capture diverse sound ongoing built sensing unit eight sensor accelerometer audio highfrequency barometric pressure humidity temperature compass sensor collected annotated dataset volunteer week hundred feature eight sensor modality capture signal sensor feature derived driven intuition practicality feature crucial developing framework systematically identify modality feature machine discrimination natural want accurately recognize track system lightweight device like cell many thus minimizing computation cost system also goal main used classification machine learning generative distribution discriminative learning rubinstein hastie used extensively wearable sensing recognizing work combine modified adaboost viola jones used automatically best feature learn ensemble discriminative static classifier wish recognize classification static classifier used probability used discriminative classifier tuned make distinguishable static classification ensures temporal smoothness continuously track rest system feature discriminative classifier training describes classifier combined hmms describes system conclusion system address systematic identification modality feature well suited accurate natural tackle feature effectively used accurately recognize track give brief system sensing feature extraction shoulder mounted collect approximately sample data reduce dimensionality bring data feature linear frequency coefficient cepstral coefficient spectral entropy band filter coefficient integral mean variance combine feature sensor dimensional feature vector sensor sampling rate feature window operate data calculating feature integral feature incorporate time window varies long minute restrict time window data past system latency feature discriminative work discriminative outperform generative classification task additionally bagging boosting combine weak classifier training data schapire viola jones boosting used combining classifier also selecting discriminative feature fraction feature train ensemble classifier recognize broad capturing temporal regularity natural regularity temporal smoothness abruptly switch back forth walking driving thus history help predicting probability instantaneous classifier train hidden markov hmms significantly smoothness system incorporating static classification overcome weakness hmms classifier jaakkola haussler selecting feature rich sensor data feature classifier work best feature classifier discriminate well remove feature even confuse classifier hand pick optimal feature viable sensor signal intuitive automatic feature increasingly practical activityrecognition system feature simplest high feature classification boosted stump engage type also training data sample training feature vector extracted sensor flow describing classification system sensor sensor recording feature vector pick fifty feature feature vector supply ensemble stump classifier stump classifier time converted probability fitting sigmoid probability supplied classifier likelihood highest likelihood classified interested ranking feature usefulness recognizing want find ranked feature feature significantly classifier behind estimating reduce computational classifier extracting feature final goal classifier device user wear computational cost classifier iteratively train ensemble weak binary classifier ranking feature adaboost viola jones weak classifier constrained feature iteration boosting feature weak learner minimizes training data used data next iteration process ranking feature feature discriminating also weak classifier classifier final weak classifier estimating feature used also find forci data prediction sign classifier feature fraction feature tried weak classifier system discriminative generative bayes probability distribution modeled histogram weak classifier stump consistently outperformed bayes classifier classifier stump find optimal threshold feature minimizes boosted static classifier classification data reflect confidence prediction schapire fraction weak classifier vote sign constructing classifier probability want combine classifier computing probability sigmoid ensemble classifier platt case probability derived static classifier predicts label data independently time independence clearly invalid prediction data help classification temporal confidence prediction classifier feature recognize continuous time chunk also learn transition thereby learn next combine confidence static classifier build incorporating prediction history hidden markov hmms successfully used modeling type data speech testing rate feature feature testing leveled data graphed averaged feature tracking hmms capture temporal dynamic feature trained hmms probability static classifier probability take discriminatively trained classifier well reduce hmms work jaakkola haussler also benefit combining generative discriminative classifier deriving kernel probability clarkson pentland oliver used used static classifier hmms speech probability classifier platt learned construct feature probability learn parameter testing continuous likelihood time sliding window duration final segmentation classification highest likelihood maxi accelerometer ambient audio barometric pressure compass relative humidity temp barometer temp relative humidity visible percentage feature originated sensor averaged alternatively trained recognize learn transition primitive transition statistic meaningful believe hmms used train dynamic transition statistic also informative validate recorded hour data consisting sitting walking jogging riding bike driving wearable dataset collected volunteer indoor outdoor recording done long stretch hour duration ranged entering hour driving volunteer asked series naturally walk around inside capture collected week hour data around feature feature data training training derived ranking feature individually boosted stump procedure show testing feature used classification classification taper around feature pick feature improves slightly testing improving feature practical feature significantly reduce computational burden resource constrained device drastically affecting static stump classifier classifier trained probability static classifier continuous minute segment data overlaid ground truth annotating recorded webcam worn volunteer used determining ground truth sensor boosting reweighting data selecting discriminatory feature successively much taking selecting best feature true true boosted feature list contribution sensor final classifier majority feature came accelerometer audio barometric pressure sensor barometric pressure data distinguishing floor transition walking stair elevator sensor sensitive pick pressure floor static classification feature tested ensemble classifier weak classifier stump discriminative bayes generative duration test dataset five half hour stump outperformed bayes classifier percentage show true true false recall true true false dataset ensemble stump list recall bayes well stump classifier continuous classification stump good illustrates classification encountered continuous trace majority trace tends classified stump scattered misclassifications static classifier help smooth classification line parameter hmms trained scene minute scene hidden gaussian probability classification sliding window overlap show sliding window probability tested concatenated test scene case note ground truth differ natural realistic classifying ground truth walking sitting standing fact reveals deficiency ground truth segment ground truth marked walking fact standing experimenter recognized standing trained hmms used feature static classifier hmms classified stump sitting standing walking jogging walking stair walking stair riding bicycle driving riding elevator riding elevator sitting standing walking jogging walking stair walking stair riding bicycle driving riding elevator riding elevator sitting standing walking jogging walking stair walking stair riding bicycle driving riding elevator riding elevator recall stump classifier data aside used test sitting standing walking jogging walking stair walking stair riding bicycle driving riding elevator riding elevator recall classifier probability bayes stump stump feature recall recall generative discriminative classifier used evaluating system significantly worse even worse static classifier demonstrating discriminative classifier distinguishing recall classifier used recognize modeling generative believe discriminative classifier sensor data primitive reduce sensor noise learn effectively conclusion recognizing sensor data diverse statistical need actively distinguished need incorporate fact time approached combining discriminative ensemble stump generative temporal hmms discriminative generative classifier classifier discriminative classifier discriminative classifier also smooth accurate show feature play role system improving classifier also creating practical system best feature leaf feature reduces feature computational upon optimizing feature take subset coefficient goertzel subset feature process optimal stopping thus work automatic pursued wearable sensor cheap lightweight unobtrusive mean obtaining richly data unconstrained long time richness sensor mass data collected suitable preprocessing selecting informative feature computational cost probably attainable opportunity combined complementary strength boosted stump hmms meeting computational yielded high rate suggesting fruitful work incorporating recognize cooking cleaning idea hope lead system move real offering area diverse smart room ethnography care young aging population acknowledgment like thank dieter helpful comment also thank undergraduate student collected many hour data used
