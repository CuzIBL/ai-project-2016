neural network learning dynamical system supervised recurrent neural network rnns suffer vanishing signal prevents learning past numerous framework learning evolution recurrent system linear evolino evolino evolution discover good hidden node linear regression quadratic optimal linear mapping hidden long shortterm memory architecture tested domain superimposed sine wave system evolino exceptionally well task show notable deficiency real observe generates unknown modeling system accurately predict huge potentially area broad speech financial forecasting artificial neural network feedback connection recurrent neural network rnns werbos robinson fallside williams zipser attractive formalism modeling dynamical system arbitrary siegelmann sontag training rnns gradient practical time window sufficient predict system temporal dependency gradient vanishes signal propagated back time network never adjusted past hochreiter echo network esns deal temporal dependency simply ignoring gradient hidden neuron composed pool neuron hundred thousand fixed esns trained computing analytically pool unit fast linear regression idea many hidden unit pool capable rich dynamic need tapped adjusting title holder benchmark improving much magnitude drawback esns truly computationally powerful nonlinear part learn mean seemingly task generating superimposed sine wave fails experience also able grammar task gers schmidhuber esns processing unit prone overfitting poor adapts succeeds gradient learn dependency long memory lstm hochreiter schmidhuber gers schmidhuber lstm specialized network architecture linear memory cell sustain activation indefinitely cell gate learn open close time outside cell activation potentially affect cell network cell enables lstm gradient learn dependency arbitrarily long time span case gradient little numerous lstm competitive alternative training rnns neuroevolution neural network network parameter searched parallel natural population chromosome encoding network connectivity evaluated chromosome awarded fitness quantifies relative highly chromosome combined exchanging substring crossover changing mutation producing evolino network recurrent neural network receives vector time step linearly combined matrix yield network vector evolved fast optimal linear regression quadratic lutions hopefully upon population continuous partially observable reinforcement learning task gradient outperforming sarsa difficult learning benchmark moriarty miikkulainen gomez miikkulainen neuroevolution rarely used supervised learning task time series prediction difficulty parameter network prevailing maxim gradient used novel framework evolution recurrent system linear evolino combine aforementioned address disadvantage extending idea feedforward network radial rbfs maillard gueriot lstm architecture evolino task esns achieves continuous task gradient rnns lstm explains evolino describes used evolino domain grammar continuous summarize conclusion evolino framework evolino framework supervised learning combine neuroevolution evolution neural network analytical linear optimal sense linear regression quadratic underlyingprinciple evolino linear property property recurrence dealt evolution illustrates operation evolino network network time recurrent neural network matrix note network recurrent history case classification vapnik quadratic mean squared linear regression evolve minimizes system modeled evolino evolutionary stipulates network evaluated procedure phase training system network time successively propagated recurrent network vector activation stored matrix target vector containing time step seen linear regression column vector training form combined linearly phase training network propagated recurrent network newly connection prediction prediction residual used fitness minimized evolution neuroevolutionis normally reinforcementlearning task network target priori neuroevolution supervised learning circumvent timeseries prediction evolve network make prediction network vector form linear regression intuition sufficiently good trying find network system accurately evolino instantiated enforced subpopulation evolve lstm network next lstm combined evolino framework enforced subpopulation enforced subpopulation differs neuroevolution evolving network coevolves subpopulation network neuron evolution proceeds enforced subpopulation population neuron segregated subpopulation network formed selecting neuron subpopulation neuron accumulates fitness fitness network participated best neuron subpopulation mated form neuron network lstm network four memory cell triangular initialization hidden unit network evolved specified subpopulation neuron chromosomesis hidden unit chromosome encodes neuron recurrent connection real neuron eachof subpopulation combined form recurrent network network evaluated task awarded fitness cumulative fitness neuron participated network recombination subpopulation neuron areranked fitness quartile recombined crossover mutated cauchy distributed noise neuron replace half subpopulation repeat sufficiently network network indirectly sampling network constructed subpopulation neuron network serve fitness statistic used neuron eventuallybe combinedto form successful network cooperative coevolutionary symbiotic adaptive neuroevolution sane moriarty miikkulainen also evolves neuron population subpopulation accelerates neuron long memory show lstm memory cell cell forget gate determines much attenuated time step gate control access cell summed unit gate control much cell fire dark node form good network evolving preventedfrom mating subpopulation also reduce noise neuron fitness evolving neuron type guaranteed network formed evolve recurrent network sane predetermined burst mutation used idea burst mutation modification best burst mutation activated best neuron subpopulation neuron deleted neuron subpopulation cauchy distributed noise neuron evolution resume searching neighborhood around best burst mutation injects diversity subpopulation continue evolving subpopulation converged long memory lstm recurrent neural network purposely learn dependency gradient feature lstm architecture memory cell capable maintaining activation indefinitely memory cell consist linear unit hold cell gate open close time gate protects neuron gate open affect neuron gate part network forget gate enables leak cell neti giin giforget gforget activation forget gate indicated neti xwijcellcj xwikcelluk identity cell tanh gjout gout gate cell gate memory cell open closed time calculated gitype xwijtypecj xwiktypeuk type forget sigmoid gate receive cell network combining lstm evolino evolino framework lstm architecture evolution regression computing linear mapping hidden coevolves subpopulation memory cell recurrent neuron chromosome containing forget gate memory cell chromosome memory cell network four gate cell receive outside cell cell normally crossover recombine neuron evolino variant fine desirable mutation quarter chromosome subpopulation duplicated copy mutated cauchy noise linear regression used pseudoinverse fast optimal sense minimizes summed squared penrose maillard gueriot feedforward vector cell connection memory cell refer connection unit peephole connection peer interior cell continuous backprojection teacher forcing terminology used predicted back next time step duringtraining correcttarget backprojected clamping network testing network backprojects prediction also used esns esns backprojection connection evolino evolves treating like network backprojection training data gradient lstm evolino lstm anbncn lstm lstm left column show legal used train column show able accept training lstm gradient gers schmidhuber continuous task interferes extent discrete task carried test contextsensitive superimposed sine wave time series highlight evolino well discrete continuous domain used reader wierstra mackeyglass system compareevolino esns widely used time series benchmark grammar learning recognize difficult intractable rnns unlimited memory recognizing anbncn entail countingthe numberof consecutiveas potentially remember quantity whole read lstm previously used learn anbncn gers schmidhuber lstm four simulation training legal anbncn network time network unit time step network predicts come next termination activating unit unit activation evolved lstm network memory cell initialized cauchy noise parameter mutation burst mutation mutation kept evolution terminated best network simulation tested summarized lstm learns approximately minute importantly able generalize substantially lstm evolino triple superimposed sine wave task plot show network produced step left vertical dashed line used training data rest must predicted network testing timesteps show network prediction dashed curve testing plotted system solid curve inset magnified clearly show curve superimposed sine wave echo network unable learn composed superimposed oscillator like sine amplitude frequency esns difficulty dynamic neuron pool coupled truly task attractor evolved network memory cell predict aforementioned double sine network cell triple sine evolino used parameter backprojection used network task evolved predict time step tested data summed squared training double sine triple sine test barely visible show triple sine wave evolino network magnified inset illustrates even time training network make accurate prediction prediction system mackey glass benchmark chaotic time series prediction system irregular time series produced differential parameter system chaotic delay delay modeled accurately feedforward network evolino esns best domain show precise prediction used network trained time step series washout time step washout time vector collected calculating evolved network memory cell cauchy noise bias network backprojection scaled testing clamped target step network backprojected prediction next cell squashed tanh evolino cell esns neuron evolino reported show evolino network even fewer memory cell network fewer parameter unable neuron demonstrates evolino learn quickly case approximately minute time real strength evolino framework generality prediction able compete best convincingly outperform case much lstm grammar task superimposed sine wave task esns suggest evolino widely applicable modeling process discrete continuous property speech evolino avoids vanishing gradient normally training searching network parallel evolution lstm memory cell evolino biased toward extracting retaining relating discrete time borrowing idea linear regression esns evolino capable precise prediction task like benchmark versatility evolino esns parsimonious esns pool neuron overfit data evolino network made much potentially susceptible noise comprehensible rule extraction evolino template instantiated plugging alternative analytical computing optimal linear mapping hidden neuroevolution recurrent network architecture used evolino plot show system prediction made lstm network evolved obvious system prediction step washout time inset show magnification clearly showing deviation curve mean squarederrorand linearregression couldas well optimality criterion vapnik quadratic find optimal linear mapping hidden classification obtaining hitherto unknown specie vector machine also neuroevolution evolve network topology well network also genetic rnns network used lstm nonlinear readout nonlinear neural network obvious also training lstm evolino pure gradient work explore potentially even powerful predictor classifier conclusion introduced evolution recurrent system linear evolino framework combine evolution recurrent neural network analytical linear learning task evolino combined enforced subpopulation term memory network yielded versatile task memoryof discrete continuous benchmark superimposed sine wave acknowledgment partially funded csem alpnach mindraces
