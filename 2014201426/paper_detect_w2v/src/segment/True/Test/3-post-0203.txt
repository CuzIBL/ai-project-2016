xiaofeng william cheung hong kong baptist kowloon tong hong kong xfzhang william massive distributed data achieving highly accurate data data preserved increasingly adopt gaussian mixture data aggregate parameter learning learning solely parameter virtual data aggregated novel derived synthetic datasets demonstrated able comparable data regeneration much computational cost machine learning data mining work data pooled centralized data repository growing case data physically distributed practical concern need tackle call distributed data mining distributed data mining data combine form kargupta major limitation loss parameter generic type alternative adopt flexible data parameter adopt inspired merugu ghosh gaussian mixture bishop data parameter interpolate data well rough ensemble illustration thus sharing parameter degree data preservation bandwidth distributed readily learning aggregated data data parameter probability density plocal plocal jlpj gmms parameter density sent server learn data learning parameter derivation learning considering expectation maximization indicator generating data indicator data probability data approximating pilthsource data rlkl rlkl adopted resulting mexp pglobal pglobal probabilistic derived trace synthetic data four data ground truth mixture data partitioned four gmms learned trained data regeneration ranging highly comparable comparing learned data regeneration true data also sizeable dataset learned true divergence decreased rapidly increased started saturate optimal threshold show identified conclusion plan novel learning aggregated parameter promising synthetic datasets extending learn sophisticated investigating negotiation learned learnedbased gaussian aggregated sizeable dataset mediator data compromising data adopted acknowledgement work partially central allocation grant hkbu
