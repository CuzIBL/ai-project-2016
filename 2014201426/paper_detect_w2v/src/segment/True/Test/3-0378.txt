learning scheme stacked learning stacked learning arbitrary base learner augmented make aware label nearby partitioning characterized long identical label demonstrate stacking consistently improves base learner stacking improves learner crfs task sequentially stacked learner outperforms crfs probabilistic learner partitioning task partitioning task classification task characterized long identical label task segmentation gene motivated anomalous learning partitioning task derive learning scheme stacked learning like boosting stacked learning arbitrary base learner case learner aware label nearby stacking virtually base learner imposes overhead training time sequentially stacked base learner train seven time slowly partitioning task stacking consistently improves nonsequential base learner surprisingly stacking also improves learner task discriminatively trained hmms benchmark sequentially stacked maximumentropy learner outperforms motivation hard task memms motivate novel learning analyze learner carvalho cohen evaluated learning recognizing email message line email message handcrafted feature line line blank email message vector line message line labeled part label message vector label line dataset labeled line email message line labeled fall message last line data training test remaining used feature carvalho cohen dataset learning learning dataset vector label clearly ordinary learning used learning ignoring carvalho cohen reported learner nonsequential entropy learner berger noise memm memm learner henceforth lafferty henceforth crfs plausible learning task maximumentropy markov memms mccallum also tagger ratnaparkhi markov klein manning recurrent sliding window dietterich probability label sequenceq learned construct dataset collection form feature vector augmented feature call call history feature note label constructing train dataset inference done viterbi find best label memms nice property relative memms easy inference done learning time relatively quick train memms also replacing history form replacing viterbi beam learner scale well history size unfortunately show memms badly rate many time rate crfs fact memms much worse nonsequential learner memm threshold used classify letting probability memm learner threshold rule give lowest test rate column labeled give best memms much high many test email message learned memm make false classification somewhere stuck mark line part consistent limitation memms memms proper subset distribution crfs lafferty label bias memms worse memms clearly strictly distribution klein manning also bias memms give little history feature case relative memm seems give much history feature encouraged memm downweight history feature noise training test data training turn probability swap line label noise double rate crfs reduces rate memms type noise affect nonsequential hypothesis memm overweighting history feature stacked learning poor memm intuitively explained mismatch data used train memm data used test training data case line followed line memm tends history feature heavily regularity hold test data viterbi prediction made mismatch compensated viterbi turn driven confidence produced violated accounted confidence incorrect mismatch modify dataset used train true replaced predicted outline sample size learning work stacking wolpert suggests scheme constructing sample pair vector predicted disjoint subset learn stacked learning parameter history size size crossvalidation parameter learning sample learning construct sample prediction disjoint subset construct dataset converting xwhere label vector paired inference vector step stacking construct word pair intent prediction produced learned sample procedure sample learning used data pair vector prediction used dataset simplest case simply vector composed form label used train learner step process memm history feature derived true history prediction classifier inference time must prediction feature keep test distribution training distribution used loop viterbi process prediction produced nonsequential learned simply generalizes idea arbitrary learner history feature parameterized feature introduced refinement history feature simply predicted numeric indicating make accessible confidence previously used viterbi stacked learning base learner henceforth dataset used obtains rate baseline crfs dramatically history feature feature like memms efficiently handle history task history size reduces slightly predicted also follow explored window size window size mean predicted label reduces rate reduction rate statistically stacked learning learner evaluated stacked crfs henceforth varying window size reduces rate statistically crfs moderately window little graphical view memms usual node shaded node defines probability distribution graphical view classifier learned stacking inference done middle inferred bottom inferred middle node middle partly shaded memm stacking stacking graphical view alternative scheme interpret mean inference robust node treated ordinary unobserved rely heavily confidence assessment forcing treat quantity prediction made correlate accept downweight prediction suggested dotted line stacking conceptually creates firewall insulating confidence made show stacking window simplify edge eventually lead node note stacking base learning approximately note stacking training classifier classifier used final classifier data plentiful training time also simply dataset disjoint half train classifier prediction produced scheme leaf training time approximately unchanged base learner decrease training time base learner superlinear time segmentation task also evaluated memms crfs partitioning task stacking used window size parameter explored made stacking made task task classifying line label like header answer trailer used feature adopted mccallum task memm mailsig rate nine benchmark task rate rate memm crfs task adopted dietterich data long task labeled line stacking binary label label trailer answer task leading benchmark task segmentation task goal take shot adjacent frame camera classify anchor news weather dataset clip shot feature produced applying color histogram central frame shot data chen constructed partitioning task label eight task task long identical label leading regularity constructed history feature rate learning eight task task case used rate lowest rate observe memms suffer high rate task answer line suggesting anomalous uncommon partitioning task also comparing improves rate task leaf unchanged rate crfs time rate case memms rate seems preferable older memms crfs made apparent scatter plot plot learner thus line case outperforms learner readability axis highest rate memm stacking also improves consistent improves rate task leaf unchanged twice rate twice stacked learner lowest rate task applying sign test relative statistically relative statistically sign test rate clear rate lowered substantially raised proportion benchmark crfs classification task final test explored task classifying song emotion task code prospective test stacking collection song annotated student scale happy happy task memm music classification task tral pearson correlation walpole used interannotator agreement pearson correlation coefficient perfect disagreement perfect agreement agreement student learn song classifier song long frame frame labeled emotion song learned classifier labeled classify unknown song used classifier label frame song labeled song frequent predicted frame label frame produced extracting property waveform song averaging frame feature mean deviation property property marsyas toolkit tzanetakis cook fourier transform tonality cepstral coefficient music dataset frame song feature frame looked predicting five label predicting happy label preliminary suggested window thus used parameter window size summarized outperform counterpart outperforms crfs improves substantially unstacked crfs rate conclusion partitioning task classification task characterized long identical label task segmentation gene evaluated probabilistic learner partitioning task memms sometimes high rate suggests neither label bias lafferty bias klein manning mismatch data used train memm data memm tested memms trained true label tested predicted label correlation adjacent label partitioning task misleading memm learning motivated derived novel used mismatch scheme stacked learning stacking virtually base learner imposes overhead learning time fold plus partitioning task stacking consistently improves base learner dramatically benchmark stacking learner base learner outperforms crfs time also prospective test conducted task stacking improves crfs learner leading case reduction crfs surprising stacking also improves learner task conducted base learner voted perceptron freund schapire training scheme hmms collins vphmms collins qualitatively entity suggest stacking work plan explore experimentation acknowledgement wish thank many friend colleague david mcallister also grateful chen videosegmentation data chao music dataset label upon work defense agency darpa opinion conclusion recommendation expressed necessarily reflect view defense agency business
