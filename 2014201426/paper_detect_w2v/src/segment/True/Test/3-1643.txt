learning reduces manually annotated sentence training statistical parser uncertainty sampling selects sentence parser exhibit certainty quantify confidence statistical confident frequency novel twostage target sentence reliably uncertainty sampling applies uncertainty sampling remaining sentence show pure uncertainty sampling ensemble bagged ensemble parser collins charniak manually annotated training penn treebank marcus high creating labelled data costly learning promise reduce cost requesting highly informative annotation informativity degree uncertainty learner label lewis gale disagreement committee learner seung concerned manual annotation train lexicalised parser collins sampling successfully sentence manual annotation entropy probability distribution competing high entropy quantifies degree uncertainty sentence learning uncertainty sampling dealing consequence parse tree probability peaked distribution depend probability unreliably rarely case certainty confident like confidence entropy probability distribution involving frequency sentence predicted parse infrequent well informative entropy reliably labelling need novel selects unparsable sentence bagged parser applies uncertainty sampling remaining sentence trained parser show parser uncertainty sampling ensemble bagged ensemble show empirically entropy negatively correlated thus entropy tends acquire annotation sentence oraclebased demonstrates preferably selecting sentence beneficial explains uncertainty sampling successful find exactly sentence target show correlation entropy fmeasure word entropy reliably identify informative subset even sentence particularly help successful learning learning informativity uncertainty classifier disagreement ensemble classifier sampling tree entropy chooses high entropy probability distribution parser fmte logpm parse tree sentence stochastic parser parameter spiked distribution entropy uncertainty parse thus know true parse tree learning ensemble classifier show high degree disagreement divergence mean quantifies ensemble disagreement pereira mccallum nigam kullbackleibler divergence distribution mean distribution ensemble pavg mean distribution ensemble pavg distribution acquire manual annotation sentence high mean learning text classification mccallum nigam novel acquiring sentence predicted infrequent well informative entropy reliably labelling need eliminate infrequent parsing simply bagging training retraining parser identify labelling bagging machine learning reduces variance training breiman aggregate classifier trained bootstrap replicates training data creating bootstrap replicate entail sampling training bootstrap replicate perturb degree inevitably eliminate frequency type operates sentence unparsable bagged parser possibly parsable trained remaining sentence highest entropy trained formally mtwo fmte failure fmte tree entropy trained failure infinity sentence parsable bagged parser employ lexicalised parser collins employ default expending optimise parameter considerably training learning learning efficacy simulation label sentence penn treebank marcus ignoring sentence word statistical significance fold sample labelled training fixed size sentence test sentence remaining sentence constitute pool unlabelled sentence sentence realistic test tagger parser brant train sentence resource parser labelled labelled recall trained sentence test sample subset sentence pool iteration subset sentence manual annotation sample annotated sentence training consistent test parser trained entirety labeled training data step regardless ensemble balanced sampling learning parsing sentence need labelling confound sample normalise tree entropy normalised sentence binary logarithm parser control sentence sample batch size sample sentence pool sentence sentence pool highest sample sampled batch pool batch distribution reflects pool distribution distribution sentence sampling learning curve maximal training sentence constituent constituent scale effectively reproduces sentence profile corpus construction guard sentence biased subset equally applicable note applicable sample data expect find correlation sample relevant learning parsing evaluated term achieving labelling expenditure cost acquiring manually annotated training term constituent composite term composed recall black show learning curve sampling recall rate well advantageous aggressively recall formulate stronger want achieving pursue sentence parsed address unparsable sentence manual annotation gain novel twostage sample bagged employ bagged ensemble learning curve constituent scale cost reduction unparsed entropy annotation cost reach reduction sampling unparsed sentence unparsed sentence batch acquiring parse tree unparsable sentence size grammar presumably help coverage test unparsed preferably unparsed pool sentence batch unparsed sentence fall batch size sample parsable sentence pool fill batch entropy selects parsable sentence high entropy preferably selects unparsed pool sentence fill batch high entropy view composed binary parsability gradual uncertainty baseline parser trained sampled training size training sampled sentence containing labeled constituent continue round sentence sampled constituent employ balanced sampling unparsed consistently note nearly identical constituent labelled entropy consistently constituent uncertainty sampling prec unparsed entropy parseval constituent annotated worse unparsed reduce labeled constituent fmeasure around entropy actually cost also annotation show recall labelling constituent unparsed considerably coverage entropy coverage show comparable differ decidedly recall aggressively pursue unparsed sentence unparsed recall entropy accordingly entropy confirm unparsed sentence help achieving coverage recall translates fmeasure accordingly unparsed sentence batch purely show clearly naive uncertainty sampling adverse consequence phenomenon targeting cost reduction annotation cost reach reduction sampling novel address sentence reliably learning expect gain preferably sentence unparsable parser trained bagged training batch filled parsable sentence high entropy trained parser composite preferably unparsed sentence nearly well considerably training able observe sentence constituent continue round sentence sampled constituent consistently reduces labelled data reach central reach reduces constituent constituent reduction also consistently recall labelling constituent constituent learning curve constituent scale prec parseval constituent annotated ensemble ensemble namely mean ensemble bagged parser subsec preferably selects unparsed pool sentence fill batch sentence high mean sentence unparsed ensemble fails deliver sentence continue round ensemble size show ensemble constituent clearly actually surprising unparsable sentence bagged parser entropy conjecture filtered difficult make trained parser rate conceptually simpler also quicker acquiring annotation objectively difficult sentence parser employ sentence pearson parsable unreliable correlation coefficient entropy periment test oracle selects sentence preferred parse tree grammar tree show oracle consistently best suggests successfully target difficult sentence well train parser sampled training sentence test sentence interested degree correlation preferred tree tree tree entropy correlation parsable sentence show negatively correlated pearson coefficient size data correlation highly entropy thus tend pick sentence oracle beneficial target sentence explains entropy bagged parser test sentence unparsable eliminate infrequent parse focusing unreliable sentence parsable trained bagged find pearson coefficient entropy close word entropy uncorrelated entropy reliably difficult sentence unreliable sentence percentage indicating acquiring true parse tree particularly note target exactly kind unreliable sentence demonstrate successful work preferably selecting high entropy parsing selecting unparsed sentence previously suggested high uncertainty thompson best quantified bagging ensemble alternatively perturbation explored learning dagan mccallum nigam dagan indicated target frequency bagging boosting parser ensemble employed parser henderson brill bagged parser ensemble learning density suggested guard selecting outlier mccallum nigam tang orthogonal suggested combining well even conclusion demonstrated investigated targeting unparsed sentence labelled recall thereby used implicitly quantified previously secondly novel particularly target sentence reliably learning showed work uncertainty sampling also favourably ensemble bagging indicated targeting objectively difficult sentence good demonstrated entropy fmeasure significantly correlated uncorrelated exactly sentence take care explains well work like investigate parsing acknowledgment like thank james henderson mirella lapata andrew smith valuable comment work
