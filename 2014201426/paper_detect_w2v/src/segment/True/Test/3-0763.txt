andstudy novel feature multiperturbation shapley framework relies game usefulness iteratively usefulness feature selects accordingly forward backward elimination empirical feature show backward elimination variant lead accurate classification datasets feature refers selecting feature relevant predicting target dataset feature benefit defyingthe curse dimensionality enhance prediction storage training prediction time namely selecting maximizethe classifier previously unseen data suggest recast feature coalitional game game perspective yield feature intent optimizing classifier unseen data combine filter wrapper filter feature reranked step classifier black ranking shapley shapley well game feature task hand taking feature throughout distribution fromwhich dataset drawn represents vector feature represents discrete target containing sampled form train validation test representing training validation test induction stand classifier constructed training induction narrowed namely label form domain task feature subset maximize classifier test optimizing classifier optimize area curve balanced rate rest introduces background game empirical feature accompanied empirical insight success backward elimination classification coalitional game cooperative game introduces coalitional game player payoff real benefit achieved game formally coalitional gameis pair playersand real associating worth coalition game pursues representing contribution player game constructing assigns player correspond contribution player achieving high payoff contribution shapley shapley intuitive shapley academic runninga decided distribute yearly bonus student fair reflects contribution student academic success year student form spontaneous coalition student work publishes summarizing work coalition also assembled rank composing payoff annual data student coalition payoff shapley fair distribute bonus student contribution year shapley marginal importanceof player coalition shapley payoff permutation player appearing player permutation shapley player mean marginal averaged subset player transforming game arena feature estimating contribution feature generating classifier player mapped feature dataset payoff classifier feature shapley feature justified axiomatic axiom normalization pareto optimality game hold feature axiom dataset divided feature axiom permutation invariance symmetry permutation hold axiom altered arbitrarily renaming reordering feature axiom preservation carrier game hold axiom dummy feature influence classifier receives contribution axiom additivity aggregation game hold axiom applies payoff feature classification task area curve false rate false rate case shapley feature contribution combined shapley linearity shapley consequence property namely payoff multiplied real shapley scaled namely word multiplying ranking feature vital property scheme rank feature estimating feature contribution shapley summing subset player impractical case keinan unbiased estimator shapley sampling permutation estimator considers feature calculate contribution feature shapley heuristically contribution feature task feature realistic case size feature much feature calculating contribution permutation sampled whole player permutation size filter feature forest breiman contribution sampled permutation subset size coupled shapley yield robust contribution feature task classification framework theoreticalbackgroundsee keinan adopt forward backward elimination subroutine contribution rank feature contribution selects feature highest contribution forward eliminates feature lowest contribution backward elimination elimination repeat phase calculating contribution remaining feature eliminated selecting eliminating feature contribution feature contribution goto else feature contribution threshold maximal permutation size calculating contribution feature phase contribution routine calculates contribution feature routine selects feature highest contribution exceed backward elimination replaced eliminates feature phase halting criterion accordingly exceed contribution threshold forward fall contribution threshold backward elimination specification generalizationof filter main idea contribution filter contribution feature assistance improvingthe classifier generatedusing induction feature optimizes classifier contribution forward calculates contribution payoff classifier training train validation validation case case handled returning largest divided classifier selects frequent backward elimination payoff calculated sampling permutation feature left phase elimination maximal permutation size role deciding contribution feature ensures feature interact inspected demonstrated feature subroutine control redundancy feature name feature train size test size arrhythmia internet dexter arcene datasets used feature redundant contribution minimizes redundancy dependency feature accelerates convergence halting criterion designates feature classifier validation forward choosing mean selects feature long feature classifier selects feature increased opposite size final feature intuitive halting criterion stop gain achieved restrictive halting criterion enables feature proved verified empirically datasets data benchmark test empirically seven datasets feature ranging dataset dataset constructed koller sahami collection arrhythmia database repository perkins internet advertisement database repository blake merz collected identifying advertisement page dexter text categorization dataset arcene cancer dataset feature guyon microarray colon cancer dataset alon work induction computational focused fast induction efficiently combined experimented naive bayes datasets training accuracyof classifier validation whole feature dataset work used induction gave highest cross validation eight feature scheme datasets induction feature serve baseline regularized linear package joachim datasets dataset arrhythmia internet dexter arcene theparametersandtheclassifierusedwiththecsaalgorithmforeachdataset induction used naive bayes feature forward phase feature eliminated backward elimination phase permutation size permutation sampled contribution explanation hyperparameters text binary classification filtering mutual classification binned continuous domain mutual filtering pearson correlation coefficient classification forest feature breiman classification feature forwardselection wrapper wrapper greedily selects feature improves classifier validation forward classification feature forward parameter parameter time feature sampled contribution threshold stopping termination feature fixed choosing contributionvalue threshold hyperparameter classification feature backward elimination parameter parameter time feature sampled contribution threshold stopping elimination hyperparameter avoid overfitting validation used calculating payoff used cross validation feature classification summarizes classifier test feature fraction classified test dataset feature forest best yielding feature behind backward elimination feature koller sahami markov blanket yield approximately feature dataset backward elimination best yielding feature koller sahami markov blanket yield approximately feature dataset thearrhythmiadataset dataset difficult backward elimination best yielding feature forward wrapper implying manyfeatures concomitantlyto performgoodfeatureselection dataset grafting perkins yield approximately dataset theinternetadsdataset approximately leading slightly outperforming interestingly wrapper feature phase neighbor feature checked leading arbitrary classifier phase yielding zero contribution selecting boosted outperform classifier thedexterdataset dexter dataset used tree process feature linear prediction feature done give satisfying feature impractical datasets overcome classifier feature classifier used classification optimization phase forward stopped phase dataset used optimize filter mutual feature best followed closely backward elimination forest dexter contributionof feature significantly outweigh contribution feature task classification forward well linearsvmwithout feature significantly feature thearcenedataset case dexter process feature linearsvmto prediction feature dataset wrapper arrhythmia internet dexter arcene corr comparisonofaccuracylevelsandnumberoffeaturesselectedinthedifferentdatasets wrapper forward elimination parameter bottom feature linear feature corr feature pearson correlation feature mutual feature forest calculated counting misclassified percentage feature bracket backward elimination rest backward elimination feature mutual yielded best poor performanceof forward explained poverty data comparing feature phase feature well training data coincidence avoided selecting feature truly contribution task classification phenomenon explained portrait datasets backward elimination achieved best case achieved best closer inspection intent capturing contribution task enables examine distribution contribution feature depicts plot distribution contribution phase arrhythmia dexter feature distribution implying contribution absolute rare justifying quantitatively need feature datasets also posse process feature displayed forward identifies feature phase sharp decrease contribution feature phase loglog plot distribution contribution absolute phase arrhythmia dexter feature demonstrates plot datasets show identical slope eliminated sake clarity backwardelimination gradual stable contribution eliminated feature peak graph contribution demonstrate contribution iterates case feature considerably increased contribution feature pointing intricate dependency feature also assist explaining backward elimination outperforms feature forward high dimensionality datasets feature assist prediction merely coincidence truly informative feature forward penalized severely case feature backward elimination maintains feature eliminated feature truly enhances classifier validation well eliminated lead stable backward elimination test progress final note view task feature coalitional game combined novel ranking shapley contribution feature classification work time selecting feature eliminating taking feature eliminated wrapper restricted induction used evaluating feature time limitation parallelizing eliminated feature predictionaccuracyandfeaturecontributionduringforwardselection andbackwardelimination forthearrhythmia dataset show classifier improves validation selects eliminates feature contribution feature decrease backward elimination generalizes test progress datasets wrapper hill climbing phase permutation parallel upon combined contribution progress feature forward elimination backward elimination decrease consequently permutation sampled speeding significantly restriction selecting learning prediction feature feature used induction demonstrated dexterand arcenedatasets verified feature significantly filter forest turn feature iteration filter entirely feature fact contributionvalues feature modified sometimes drastically feature tested datasets show classifier successfully compete feature case feature interact case feature permutation size namely greedy wrapper enhance classifier significantly successfully demonstrate applying game feature forward competitive feature show backward elimination superior feature used highly classifier
