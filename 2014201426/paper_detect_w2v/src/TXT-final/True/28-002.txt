 
in recent years  there has been much interest in learning bayesian networks from data. learning such models is desirable simply because there is a wide array of off-the-shelf tools that can apply the learned models as expert systems  diagnosis engines  and decision support systems. practitioners also claim that adaptive bayesian networks have advantages in their own right as a non-parametric method for density estimation  data analysis  pattern classification  and modeling. among the reasons cited we find: their semantic clarity and understandability by humans  the ease of acquisition and incorporation of prior knowledge  the ease of integration with optimal decision-making methods  the possibility of causal interpretation of learned models  and the automatic handling of noisy and missing data. 
in spite of these claims  and the initial success reported recently  methods that learn bayesian networks have yet to make the impact that other techniques such as neural networks and hidden markov models have made in applications such as pattern and speech recognition. in this paper  we challenge the research community to identify and characterize domains where induction of bayesian networks makes the critical difference  and to quantify the factors that are responsible for that difference. in addition to formalizing the challenge  we identify research problems whose solution is  in our view  crucial for meeting this challenge. 
1 introduction 
a bayesian network is a graphical representation of the joint probability distribution for a set of variables. the representation was originally designed to encode the uncertain knowledge of an expert  wright  1; 
1 	ai challenges 
howard and matheson  1; pearl  1   and indeed today  they play a crucial role in modern expert systems  diagnosis engines  and decision support systems  heckerman et al.  1 . they also have become the representation of choice among researchers interested in uncertainty in ai. one often-cited merit of bayesian networks is that they have formal probabilistic semantics and yet can serve as a natural mirror of knowledge structures in the human mind  spirtes et al.  1; heckerman et al.  1; pearl  1 . this facilitates the encoding and interpretation of knowledge in terms of a probability distribution  enabling inference and optimal decision making. 
　a bayesian network consists of two components. the first is a directed acyclic graph in which each vertex corresponds to a random variable. this graph represents a set of conditional independence properties of the represented distribution: each variable is probabilistically independent of its non-descendants in the graph given the state of its parents. this graph captures the qualitative structure of the probability distribution  and is exploited for efficient inference and decision making. thus  while bayesian networks can represent arbitrary probability distributions  they provide computational advantage for those distributions that can be represented with a simple structure. the second component is a collection of local interaction models that describe the conditional probability p xi psli  of each variable x  given its parents pai   see figure 1 . together  these two components represent a unique joint probability distribution over the complete set of variables x  pearl  1 . the joint distribution is given by the following equation: 
		 i  
it can be shown that this equation implies the conditional independence semantics of the graphical structure given earlier. 
　equation 1 shows that the joint distribution specified by a bayesian network has a factored representation 
as the product of individual local interaction models. sparse bayesian networks therefore correspond to concise representations of joint distributions. if the number of parents of any variable is bounded by a constant k  then  for most reasonable representations of the local interaction models  including all discrete models  the bayesian network requires a number of parameters that is linear in the number of variables  instead of exponential for an unstructured representation. this observation is  of course  directly relevant to the learning problem  since concise parameterizations lead to statistically efficient learning-provided that the problem domain admits of a sparse structure of conditional dependencies. the latter assumption is of course directly related to the usefulness of bayesian networks as models of human knowledge structures. 

figure 1:  a  a simple probabilistic network showing a proposed causal model   b  a node with associated conditional probability table. the table gives the conditional probability of each possible value of the variable emphysema  given each possible combination of values of the parent nodes smoker and coalminer. 
　the characterization given by equation 1 is a strictly mathematical characterization in terms of probabilities and conditional independence statements. an informal connection can be made between this characterization and the intuitive notion of direct causal influence. it has been noted that if the edges in the network structure correspond to causal relationships  where a variable's parents represent the direct causal influences on that variable  then resulting networks are often very concise and accurate descriptions of the domain. thus it appears that in many practical situations  a bayesian network provides a natural way to encode causal information. we can state this more precisely as the causal markov assumption  cma : if a network is constructed simply by connecting variables to other variables that they directly causally influence  then the resulting network interpreted according to equation 1 will correctly reflect the conditional independence statements that hold in the domain. 
　the naturalness of using causal information directly in constructing formally characterizable knowledge structures has made it possible to encode the knowledge of many experts. as a result  bayesian networks have been incorporated into many expert systems  diagnosis engines  and decision-support systems  heckerman et al.  1 . nonetheless  it is often difficult and timeconsuming to construct bayesian networks from expert knowledge alone. 
　this observation  together with the fact that data is becoming increasingly available and cheaper to acquire has led to a growing interest in using data to learn both the structure and probabilities of a bayesian network. several groups have worked on learning structure from scratch  spirtes et al.  1; pearl  1; friedman et al.  1  or with weak constraints such as variable ordering  cooper and herskovits  1  for example   while others have worked on learning structure by refining an initial model  heckerman et al.  1 . learning probabilities  which is non-trivial when the network contains hidden variables or the data set has missing values  can be done by a variety of methods including em  lauritzen  1; lauritzen  1; spiegelhalter et al.  1; olesen et al.  1; spiegelhalter and cowell  1; 
heckerman  1  gradient-based methods  laskey  1; 
golmard and mallet  1; neal  1; russell et al.  1   and monte-carlo techniques  neal  1 . 
　these researchers have cited several benefits of using the bayesian-network representation  with its causal interpretation  as a tool for learning: 
1. incorporation of prior knowledge. bayesian networks facilitate the translation of human knowledge into probabilistic form  making it suitable for refinement by data. 
1. validation and insight. in many cases  a learned bayesian network can be given a causal interpretation. consequently  a bayesian network is more easily understood than  black box  representations such as neural networks. as an immediate byproduct  people will more readily accept the recommendations of a bayesian network than those of a model justified only by its raw predictive performance. in addition  users are more likely to gain insights from bayesian networks. 
1. learning causal interactions. unlike purely probabilistic relationships  causal relationships allow us to make predictions given direct interventions or manipulations of the world. therefore  by learning with bayesian networks  there is a hope that we can make better predictions in the face of intervention. learning causal relationships is crucial in scientific discovery  where interventional studies are often expensive or impossible. similarly  the ability to learn 
	friedman  et al. 	1 
causal relationships is crucial for intelligent agents that must act in their environment on the basis of acquired knowledge. 
　other benefits of using bayesian networks for learning are derived from their probabilistic semantics. because sophisticated yet efficient methods have been developed for using a bayesian network to answer probabilistic queries  they can be used both for predictive inference and diagnostic  or abductive  inference. this is in contrast to standard regression and classification methods  e.g.  feed forward neural networks and decision trees  that encode only the probability distribution of a target variable given several input variables. whereas the bayesian-network representation can describe the casual ordering in the domain  there are no restrictions as to the directions of the queries. thus  there is no inherent notion of inputs and outputs of the network. this property also allows bayesian networks to reason efficiently with missing values  by computing the marginal probability of the query given the observed values. one other cited benefit of the bayesian-network representation  which derives from its probabilistic nature  is that it can be used to determine optimal decisions. 
　even though these claims are compelling  it is yet to be demonstrated that these issues provide tangible advantages in applications. the purpose of this paper is therefore to challenge researchers to characterize and quantify these claims  including the specification of domains where they made a difference in the efficiency of learning  e.g.  through the use of prior knowledge   in the quality of the resulting model  e.g.  a new causal theory that is accepted by the experts   or in the deployment the system  e.g.  through combination with utility estimation . 
　we hope that this challenge will focus the research community on a high-impact research agenda. we believe that in order to meet the challenge  at least three kinds of activities will take place: 
1. we believe that experience in applications provide valuable lessons. thus  we are interested in  success stories   that is papers that describe applications where learning bayesian networks has led to significant advantages over other methods. these papers should attempt to distill the characteristics of the problem that made bayesian networks the preferred solution. 
1. we propose a series of  bake-offs  to experimentally evaluate how bayesian networks and alternative approaches can exploit prior knowledge  deal with missing data  and learn causal models. these bake-offs will allow for controlled study and evaluation of the impact of the various alternatives. section 1 describes our proposal for organizing these 
1 	ai challenges 
bake-offs and evaluating the results. 
1. we identify specific technical research problems whose solution is  in our view  crucial for meeting the challenge. in section 1  we outline these problems. 
for a comprehensive assessment of the state of the art of the field  we refer the reader to heckerman  as well as the papers cited earlier. 
1 experimental bake-offs 
as mentioned in the introduction  we propose a series of bake-off competitions with the objective to evaluate the extent to which the special features of bayesian networks benefit the learning task and the resulting models. to this end  we will maintain a web site  where data sets  background information about them  and evaluation criterion will be made available. 
　we are currently assembling several collections of data sets  both syntactic and real  for the bake-offs described below. in order to preserve the validity of these experiments  some of them will be done using  blind  evaluation. that is  participants in the bake-off will have access to a portion of the training data and will have to register the learned models by a certain date. the learned models will be tested on unseen data by the central web 
server.1 
　our hope is that these data sets will provide appropriate test beds for testing theories and new algorithms. we encourage practitioners and researchers interested in other induction methods to participate in these bake-offs and to use these datasets.1 
　the exact evaluation criteria will be decided based on inputs from participants and the discussions that will follow the presentation of this challenge. these criteria will include various error measures such as log-loss  cross-entropy or kl distance  and classification accuracy  and will depend on the different learning strategies  e.g.  batch learning and incremental learning . 
　we propose to focus these bake-offs on three issues: incorporation of background knowledge  handling of missing data  and learning causal interactions. 
background knowledge. the main problem with experiments testing the influence of background knowledge in the learning process is to make the expertise readily available to all participants in a way that does not provide advantages to any particular learning method. 
   1  this stricture is intended to get around the irresistible tendency  noted during the statlog project  for researchers to  peek'1 at test data and report  best  results selected from runs with different knob settings. 
   1 toward this end  we plan to submit these data sets to both the ucl machine learning repository and the xxx repository at toronto. 
 a similar problem has arisen with experimental studies of inductive logic programming methods; we expect to compare notes.  we are considering two strategies. the first one is to provide summary of background expert knowledge in the form of free-form text and tables. the second strategy is to provide data about a domain familiar enough that anybody can be regarded as an expert  and define a prediction task in that domain. one such domain is that of tv shows. data could be provided about shows  viewers characteristics etc.  and the task would be to predict the shows that new subjects will like based on other shows they like. 
missing data. the basic problem of coping with missing values and hidden variables in the data set is addressed very simply in bayesian networks  because likelihoods can be computed no matter what subset of variables are available as evidence. the tricky problem arises when the data is missing due to specific values that other variables take. in this case  the failure to observe a variable may in itself be informative about the true state of 
the world  rubin  1 . in principle  a successful induction algorithm would be able to take advantage of a good model about the relationship between the state of the world and what variables are missing. 
　for this challenge we will provide both synthetic data and real-life data. the former allows controlled experiments that account for the number of missing values and the dependence of omissions on the true state of the world. we also plan to provide data sets where the target task involves a large amount of incomplete information. 
causal interactions. in this study  we will attempt to learn cause and effect from observational studies. ideally  we will also have interventional data to verify the real causal structure of the domain. we are currently investigating data sets in social sciences and epidemiology; the university of michigan survey data archive contains thousands of data sets  some running into the gigabytes  that might be very suitable. we will also try to provide synthetic data as follows. we will contact experts that will provide us with causal models for their domain  e.g.  epidemiology   from which we will create synthetic data. since prior knowledge plays a significant role in the induction of causal theories  these experts would also provide summary of the prior knowledge they consider reasonable for the domain they created  e.g.  known temporal ordering relations and possible latent causes . 
　we plan to evaluate the learned causal models as follows. first  we will measure how well they predict the effects of interventions  using standard statistical measures . second  we will measure how many causal interactions were correctly and incorrectly identified. 
1 technical challenges 
many researchers are now concentrating on learning in more expressive probabilistic models  including hybrid  discrete and continuous  models  lauritzen and wermuth  1   mixed  undirected and directed  models  buntine  1; cooper  1; spirtes et al.  1   dynamic bayesian network models representing stochastic processes  russell et al.  1   and stochastic grammars  stolcke and omohundro  1 . another important problem is the specification of prior distributions over parameters-most current work makes strong assumptions such as parameter independence and likelihood equivalence. mackay  and others are working on hierarchical models that relax the assumption of parameter independence. a third area of active research is the development of efficient approximation algorithms for probabilistic inference-a key component of learning-including monte-carlo  thomas et al.  1  and variational methods  saul et al.  1 . 
　there are two technical challenges that we believe are critical to the success of bayesian networks and for which much work needs to be done. one challenge is the efficient handling of incomplete data. one important subcomponent of the first task is the creation of search methods for bayesian networks with hidden variables. clever search strategies are needed to constrain the infinite search space. in addition  as mentioned in the previous section  learning with incomplete data is particularly difficult when the mere failure to observe some variable is informative about the true state of the world. for example  the fact that a patient drops out of a drug study may suggest that the he or she could not tolerate the effects of the drug. several researchers have developed basic principles and methods for dealing with such situations  including rubin   robins   cooper   spirtes et al.   and chickering and pearl   but more work needs to be done to connect these basic principles with graphical models and to make these methods more efficient. 
　a second challenge is the creation of simple but expressive probability distributions for the local interaction models in a bayesian network. most work on learning with bayesian networks concentrates on discrete variables where each variable is associated with a set of multinomial distributions  one distribution for each configuration of its parents. thiesson  discusses a class of local likelihoods for discrete variables that use fewer parameters. geiger and heckerman  and buntine  discuss simple linear local likelihoods for continuous variables that have continuous and discrete variables. buntine  also discusses a general class of local likelihoods from the exponential family. nonetheless  alternative likelihoods for discrete and continuous variables are desired. local likelihoods with fewer parameters might 
	friedman  etal. 	1 

allow for the selection of correct models with less data as demonstrated empirically by friedman and goldszmidt . in addition  local likelihoods that express more accurately the data generating process would allow for easier interpretation of the resulting models. 
1 concluding remarks 
there are of course a number of other promising research avenues  challenges  and needs that directly concern learning bayesian networks and their impact on real world applications for data analysis  pattern recognition and classification  knowledge discovery  and others. there is for example the belief that bayesian networks will play a significant role in issues of feature engineering  selection and abstraction  as well as in the formulation of the learning problem and the various tradeoff evaluations of knowledge representation versus quality of prediction of the induced models. our particular choice of problems and challenges were based not only on immediate relevance  but more important  on the fact that progress can be accomplished in the next two years. 
　needless to say  our hope is that researchers and practitioners of all induction methods will participate in our challenge so that a fair comparison can be made and true scientific knowledge can be distilled. 
acknowledgments 
the participation of nir friedman and stuart russell was funded by aro through muri daah1-1  and by nsf through fd1. 
