 
richard e. korf 
computer science department 
university of california  los angeles 
los angeles  ca 1 korf cs.ucla.edu 

abstract 
best-first search is limited by the memory needed to store nodes in order to detect duplicates. disks can greatly expand the amount of storage available  but randomly accessing a disk is impractical. rather than checking newlygenerated nodes as soon as they are generated  we append them to a disk file  then sort the file  and finally scan the sorted file in one pass to detect and remove duplicate nodes. this also speeds up such searches that fit entirely in memory  by improving cache performance. we implement this idea for breadth-first search  performing the first complete searches of the 1 x 1 sliding-tile puzzle  and the 1-disk  1-peg towers of hanoi puzzle. 
1 introduction: the problem 
best-first search algorithms  such as breadth-first search  bfs   dijkstra's algorithm  dijkstra  1   and a*  hart  nilsson  k raphael  1   store every node that is generated  in either the open list or the closed list. one reason for doing this is to detect duplicate nodes  and avoid expanding a state more than once. as a result  these algorithms are limited by the available memory. 
　in some problems  this memory limitation can be avoided by depth-first searches  dfs  such as depthfirst iterative-deepening  dfid  or iterative-deepeninga*  ida*   korf  1   but dfs can generate exponentially more nodes than bfs. for example  in a rectangular-grid problem space  bfs will generate 1 r1  nodes within a radius of r  while dfid will generate 1r  nodes. while there are ways of detecting some duplicate nodes in a dfs taylor k korf  1   they do not apply to all problem spaces. 
1 frontier search 
an algorithm called frontier searc/i korf  1; korf k zhang  1  saves only the open list of nodes at the frontier of the search  and not the closed list of nodes that have been expanded  thus saving some memory. 
with each node  it stores a used-operator bit for each operator  to indicate whether the neighboring state reached by that operator has already been generated. for example  the sliding-tile puzzles require four such bits for each state  one for each direction in which a tile could move. when a parent node is expanded  only children that are reached via unused operators are generated  and the parent node is deleted from memory. in each child node  the operator that generates the parent is marked as used. when duplicate states are found in the open list  only one copy is kept  and any operator that is marked as used in any copy is marked as used in the retained copy. since the closed list is not stored  reconstructing the solution path requires additional work. see  korf  1; korf k zhang  1  for two ways to do this. 
　the memory required by frontier search is proportional to the maximum size of the open list  or the width of the problem space  rather than the size of the space. for example  in the grid space mentioned above  the width of the space grows only linearly with the search radius  while the entire space grows quadratically. as another example  the width of the n-disk  1-peg towers of hanoi space is only 1n states  while the entire space contains 1n states. frontier search is still limited by the memory required to store the open list  however. 
1 delayed duplicate detection  ddd  
hard disks with hundreds of gigabytes of storage are available for less than a dollar per gigabyte  which is over a hundred times cheaper than memory. because of high latency  however  a disk behaves more like a sequential device  such as a magnetic-tape drive  with large capacity and high bandwidth  but only if it is accessed sequentially. to quickly detect duplicate states in a search algorithm  however  nodes are usually stored in a hash table  which is designed to be accessed randomly. 
poster papers 	1 　our solution to this problem is rather than checking each newly-generated node for duplicates as soon as it is generated  we append each node to a disk file containing previously generated nodes. at some point later we sort the file  thereby bringing together nodes representing the same state. then we scan the sorted list of nodes in one pass  merging any duplicate nodes. 
1 breadth-first frontier search 
as a simple example  we describe breadth-first frontier search with delayed duplicate detection. bfs is normally implemented with a fifo queue  which we implement with two disks files  an input file and an output file. initially  the input file contains the initial state. as we read each node in the input file  we expand it  and write its children to the output file  with no duplicate checking. when the input file is exhausted  we delete it. at that point  the output file contains all the nodes at the next depth  including any duplicate nodes. we then sort the nodes in the output file by their state representations  which brings together any duplicate nodes representing the same state. next  we linearly scan the output file  merging duplicate nodes and oring their used operator bits  and write one copy of each state to a new input file  deleting the output file when we're done. this completes one level of the breadth-first search. the algorithm continues until expanding the nodes in the input file doesn't generate any more nodes in the output file. 
1 sorting the disk files 
algorithms for sorting disk files are well-known. see  garcia-molina  ullman  & widom  1   pp. 1. the basic algorithm is to read as much of the unsorted file as will fit into memory  sort it in memory using quicksort  for example  and write the sorted portion of the file to a new subfile. continue until the entire original file has been read and written into a set of sorted subfiles. then  all the sorted subfiles are merged in one pass  storing the head of each file in memory  and writing the lowest record to a final sorted output file. 
1 d d d in memory 
surprisingly  delayed duplicate detection is useful even when all nodes fit in memory  resulting in reduced running time due to improved cache performance. in the standard implementation of breadth-first search in memory  the open list is stored in a hash table. as each new node is generated  it is looked up in the hash table  which often results in a cache miss  since the hash function is designed to randomly scatter the nodes. a ddd implementation doesn't use a hash table  but a single fifo queue in memory  reading nodes off the head of the queue  and appending them to the tail. once a level of the search is completed  the queue is sorted in memory using an algorithm such as quicksort  and the sorted queue is scanned  merging duplicate nodes. the advantage of this approach is that the queue is only accessed at the head and tail  or at two points in between during quicksort  and hence most memory references will reside in cache  reducing the running time. 
1 experiments 
we implemented a breadth-first search on sliding-tile puzzles  and the 1-peg towers of hanoi problem. 
1 sliding-tile puzzles 
 schofield  1  published a complete breadth-first search of the 1 x 1 eight puzzle. we completed a bfs for all sliding-tile puzzles up to the 1 x 1 thirteen puzzle. table 1 below shows the results. the first column gives the x and y dimensions of the puzzle  and the second column gives the number of moves needed to reach all solvable states  starting with the blank in a corner position. this is also the worst-case optimal solution length  for a goal with the blank in a corner. the third column gives the number of solvable states  which is  xy  /1  and the fourth column gives the width of the problem space  which is the maximum number of nodes at any depth. 
size moves total states max width 1 x 1 1 1  1 1 x 1 1 1 1 1 x 1 1 1 1 1 x 1 1 1 1 1 x 1 1 1 1 1 1 x 1 1 1 1 1 1 1 x 1 1 1 1 1 1 1 x 1 1 1 1 1 1 table 1: sliding-tile puzzle results 
　the 1 x 1 and 1 x 1 eleven puzzles were the largest that we could solve in memory. we implemented both a standard bfs algorithm  using one bit of memory per state  and also breadth-first frontier search with delayed duplicate detection. the standard bfs required 1 minutes  while the frontier search with ddd required only 1 minutes  on a 1 megahertz sun ultra 1 workstation. this demonstrates that ddd frontier search is useful even for problems that fit in memory. 
　the 1 x 1 fourteen puzzle was the largest we could search exhaustively with our 1 gigabyte disk. at 1 bytes per state  this problem required about 1 gigabytes of disk storage  and ran for 1 hours  1 minutes. the ratio of the problem size to the problem width is about 1  illustrating the advantage of frontier search. 
1 towers of hanoi 
for the standard 1-peg towers of hanoi problem  there is a simple algorithm that guarantees the shortest path between any pair of states. the 1-peg towers of hanoi puzzle  known as reve's puzzle  is much more interesting. there exists a algorithm for finding a solution  and a conjecture that it generates optimal solutions  but the conjecture remains unproven.  van de liefvoort  1  contains a good bibliography for this problem.  szegedy  1  gives bounds for the a:-peg version  but they include an unspecified constant in the exponent. 
1 	poster papers 　for the sliding-tile puzzles  all paths between any pair of states have the same even-odd parity  and the algorithm described in section 1 works correctly. for the towers of hanoi  however  two states can have both even and odd length paths between them. in that case  frontier search with delayed duplicate detection can fail by leaking into the interior of the search. one solution to this problem is to store two complete levels of the search at a time. see our full paper for the details  korf  1 . 
　using this algorithm  we were able to exhaustively search all problems through 1 disks  starting with all disks on one peg. table 1 shows the results. the first column gives the number of disks  and the second column shows the minimum number of moves required to move all the disks from one peg to another. the third column gives the total number of states of the problem  which is 1n  where n is the number of disks. the fourth column shows the maximum width of the graph  starting from a state with all disks on one peg. in all cases  the optimal number of moves equals the conjectured minimal number of moves. the 1-disk problem took almost 1 days to run  and at 1 bytes per state required about 1 gigabytes of disk space. note that the ratio of the total number of states to the problem width for the 1-disk problem is almost 1. 
disks moves total states max width r 
1 1 
1 1 
1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 table 1: 1-peg towers of hanoi results 
1 conclusions and further work 
we showed that delaying the detection of duplicate nodes in a breadth-first search can effectively make use of very large capacity disk-storage systems. we also showed how to combine this idea with frontier search. in addition  we demonstrated that these ideas can also speed up a search that occurs entirely in memory. using these algorithms  we completed breadth-first searches of sliding-tile puzzles with up to 1 billion nodes  and 1-peg towers of hanoi problems with up to 1 billion nodes. 
for the 1-peg towers of hanoi  we verified a conjecture regarding optimal solution lengths to 1 disks. 
　current work is focussed on implementing these techniques for more complex best-first search algorithms such as dijkstra's algorithm  dijkstra  1  and a*  hart  nilsson  & raphael  1 . 
1 acknowledgments 
larry taylor brought the 1-peg towers of hanoi problem to my attention  and also used disk storage in a breadthfirst search to find duplicate operator strings  taylor & korf  1 . thanks to jianming he for verifying the towers of hanoi results. this research was supported by nsf under grant no. eia-1  by nasa and jpl under contract no. 1  and by the state of california micro grant no. 1. 
