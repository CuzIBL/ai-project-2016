 
this paper presents the expected solution quality  esq  method for statistically characterizing scheduling problems and the performance of schedulers the esq method is 
demonstrated by applying it to a practical telescope scheduling problem the method addresses the important and difficult issue of how to meaningfully evaluate the performance of a scheduler on a constrained optimization problem for which an optimal solution is not known at the heart of esq is a monte carlo algorithm that estimates a problem's probability density function with respect to solution quality this  quality density function  provides a useful characterization of a scheduling problem  and it also provides a background against which  scheduler performance can be meaningfully evaluated esq provides a unitless measure that combines both schedule quality and the amount of time to generate a schedule 
1 	i n t r o d u c t i o n 
this paper presents a method for statistically characterizing both scheduling problems and the performance of scheduling techniques the method provides a measure of the expected distribution of schedule scores in a given search space we refer to it as the expected solution quality  esq  method this paper provides an overview of the esq method and demonstrates its application to a practical telescope scheduling problem 
　we are concerned with a scheduling problem which involves sequencing and executing command packets that control the behavior of ground based remotely located fully automatic telescopes such telescopes have been in operation for almost a decade  and some time ago the astronomy community developed a technique for automatically scheduling command packets their technique is a form of heuristic dispatch at any point in time  some command packet is dispatched for immediate execution  the selection is determined purely locally  without lookahead  by the application of domainspecific heuristics this heuristic dispatch scheme has been used with reasonable results  however  we thought that it should be possible to achieve better performance by using lookahead search in order to compare schedulers  we required a mathematical statement of a  good schedule  in collaboration with astronomers  we defined some attributes of good schedules and encoded them into a multi-attribute objective function 
　as a starting point  we used simple greed} search with one-step lookahead  more on this below  in so doing  our goal was not to design a new way of doing search  but simply to establish a starting point for comparison our first experiment was the obvious one we ran the heuristic dispatch scheduler and the one-step lookahead scheduler on a real problem instance and scored the schedules they found according to the objective function  for this objective function  lower scores indicate better schedules   the dispatch scheduler obtained a score of +1 and the lookahead scheduler obtained a score of - 1 while the scores clearly indicate that the lookahead scheduler performed better than the dispatcher  we did not have a way to evaluate the significance of the difference also  we did not know how well the lookahead algorithm was performing in absolute terms specifically  we wanted to know how close to optimal the lookahead scheduler performed the problem was not a  benchmark   so we did not have a catalog of scores obtained by different algorithms we looked in detail at the objec-
tive function  but it was difficult to say anything precise about the range of values it could return 
　we developed the esq method as a solution to this evaluation problem in this paper  we present an application of the esq method to the above-mentioned telescope scheduling domain though this scheduler comparison served as motivation for the work presented here  this paper does not make any specific claims regarding the superiority of lookahead scheduling over dispatch scheduling rather  the comparison serves as an illustration of the method itsell  which is the focus of the paper 
　in the esq method  random feasible solutions are generated via a monte carlo algorithm and are used to estimate a probability density function with respect to solution quality this quality density junction provides a background against which scheduler performance on a given problem can be meaningfully evaluated 
　the rest of the paper is organized as follows first  we present necessary background regarding the telescope domain and the esq method we then apply esq to charbresina  drummond and swanson 1 

actenie a problem's search space  to evaluate scheduler performance  and to support the construction of search heuristics finally  we discuss applying esq in more general contexts and make some concluding remarks 
1 	b a c k g r o u n d 
in this section  we briefly describe a particular telescope scheduling domain  see  bresina et al  1  for more details   describe our multi-attribute objective function  define our formulation of the search space  define a statistical sampling technique  and define our two schedulers this background is employed in section 1  where we demonstrate the application of the esq method 
1 	scheduling for a u t o m a t i c telescopes 
our domain involves the management and scheduling of ground-based  remotely located  fully automatic telescopes with fully automatic telescopes  the astronomer does not have to be at the observatory and  furthermore  does not have to engage in teleoperation fully automatic telescopes  see  genet & hayes  1   can operate unattended for weeks or months 
the automatic telescope instruction set  or atis  
 boyd tt al  1  is used to define observation requests in atis  a group is a command packet containing a sequence of telescope movement commands and instrument commands a group is the primitive unit to be scheduled and executed a group can be thought of as similar to a strips macro operator  fikes  hart  k. hayes  1  groups specify  hard  constraints  defined by basic physics  and  soft  preferences the primary hard constraint is that each group can be executed only within a specific time window  typically between one and eight hours wide  an example of a soft preference is the relative priority that an astronomer assigns to each submitted group 
　a scheduler's task is to find a sequence of groups that achieves a good score according to a domain-specific multi-attribute objective function the scheduling problem does not involve assigning an amount of execution time to each group  since each group executes until it aborts or successfully completes each group typically takes on the order of ten minutes to execute 
1 	an o b j e c t i v e f u n c t i o n 
the experiments presented in this paper used the objective function  mentioned in the introduction  that was derived in collaboration with astronomers the objective function is a weighted summation of three attributes priority  fairness  and atrmasa when constructing such a multi-attribute objective function  the scores of the different attributes need to be bcaled bo that they are comparable we return to this topic below  first  we define the three objective function attributes 
　for a given schedule  the priority attribute is computed as the average priority of the groups in that schedule in atis  a higher priority is indicated by a lower number  thus  a schedule that has a lower average priority includes more high priority observations 
　the second attribute attempts to measure how fair a schedule is in terms of the time allocated to each 
1 	planning 
astronomer the fairness measure for a particular astronomer is the difference between the fraction of the total requested time in the atis input file that the astronomer requested and the fraction of the total allocated time in a given schedule that was allocated to the astronomer the fairness measure for a given schedule is then the sum of the fairness measures for each of the astronomers smaller fairness scores are better 
　the third attribute attempts to improve observation quality by reducing the airmass  i e   amount of atmosphere  through which observations are made for a celestial object of a given declination  airmass is maximal when the telescope is pointing at the object on the horizon  and minimal when pointing at that object on the meridian1 we approximate the airmass measure as the average deviation from the meridian thus smaller airmass scores are better 
1 	search space f o r m u l a t i o n 
we have formulated the search space as a tree where each node corresponds to a world model state  the most important element of which is the  clock  time at the telescope the alternative arcs out of a given node represent the groups that are  enabled  in the node's state we say that a group is tnabled in a state if and only if all of its hard constraints  r e   preconditions  are satisfied in that state an arc connecting two nodes represents the simulated execution of an atis group 
　the search tree is organized chronologically  where the root node of the tree contains the state describing the time at which the observing night begins schedules that are identical up to a given branching point share a common prefix each path through the tree defines a unique feasible schedule and every feasible schedule is represented by a path in the tree since groups cannot be executed after the observation night ends  each schedule has finite length the number of schedules is finite and is exponential in the number of axis groups 
1 	i t e r a t i v e s a m p l i n g 
the construction of the quality density function is carried out by an algorithm called iterative sampling  chen  1  langley  1  minton  bresina  k  drummond  1  iterative sampling is a type of monte carlo algorithm that generates random paths in a search tree the algorithm starts at the root node and randomly chooses one of the arcs leading from that node the arc is followed and the process of random selection continues un til a leaf node is reached  i c   until no more groups are enabled in that path  
　we mentioned above that the various attributes in the multi-attribute objective function must be scaled so that they are comparable this scaling was achieved via the iterative sampling algorithm we scored each randomly selected path  or schedule  according to each of the three individual attributes  and we experimentally determined that the distribution of scores for each attribute was approximately normal the mean and standard deviation were calculated for each attribute and used to transform 
   'the mendian is an imaginary line running north-south through a point directly overhead 

each of these normal distributions into a standard normal distribution  i e a norma  distribution with a mean of zero and a standard deviation of one  ae a result of this transformation  all the attributes in the composite objective function were directly comparable  it is worth noting that if the attribute distributions are not normal  then some other transformation is required to appropriately scale the attribute scores   for these experiments  we wanted an objective function that placed equal importance on each attribute  so each transformed attribute was simply added  without weights  to form the composite objective function score hence  the com posite objective function has a normal distribution  but not a standard normal  
1 	t w o schedulers 
as mentioned previously  the existing telescope control software selects groups for execution via heuristic dispatch the heuristics used are the group selection rales defined by the atis standard  boyd tt al  1  the atis group selection rules reduce the set of currently enabled groups to a single group to be executed next 
　there are four heuristic group selection rules specified in the atis standard priority  number-of-observationsremaining  nearest- to-end-window  and file-position the rules are applied in the sequence given  and each rule is used to break ties that remain from the application of those that preceded it if the result of applying any rule is that there is only one group remaining  that group is selected for execution and no further rules are applied since there can be no file-position ties  application of the group selection rules deterministically makes a unique selection at every choice point  i c   the dispatch scheduler admits a single solution 
　our second search technique performs greedy search with one-step lookahead at each node visited  all the enabled groups are applied to generate a set of new nodes  each of which is scored by a heuristic evalua tion function the heuristic evaluation function is applied to the partial schedule that starts in the root node and terminates in the node undergoing evaluation the best-scoring node is then selected  and the process repeats from that node our greedy search implementation breaks ties randomly and  hence  is nondetermimstic our algorithm performs the greedy search ten times and returns the best-scoring of the ten resulting  not necessarily unique  schedules in our experiments comparing greedy lookahead and heuristic dispatch  the greedy search uses the composite objective function as its evaluation function  i t   local search heuristic  
1 	apphcation of the esq method 
this section demonstrates an application of the esq method for a real problem instance from our telescope scheduling domain the problem's input consists of 1 atis groups which represent the combined observation requests of three astronomers we show how the esq method can be employed to characterize a problem's search space in terms of size  shape  and solution quality  to evaluate scheduler performance  and to construct local search heuristics for effective greedy search 

1 	search space size a n d shape 
one of the primary determinants of problem difficulty is the size of the search space while it is not practical to enumerate all states in the space  the overall size can be estimated using iterative sampling knuth  was the first to use an iterative sampling approach to estimate the size of a search space knuth's algorithm was later extended by purdom  and chen  in knuth's original algorithm  each sample produced an estimate of the tree size under the assumption that all sibling nodes had the same number of nodes in their subtrees the subtree size for a given node was estimated by multiplying the number of child nodes by the subtree size estimate for a randomly chosen child a number of iterations of this procedure were performed and the resulting size estimates were then averaged to yield the final size estimate though we too used iterative sampling  our estimation calculation differs slightly from knuth's based on the iterative samples  we derive the average branching factor for each depth  these average branching factors are then multiplied to produce the final size estimate 
　figure 1 shows the results of 1 samples with error bars representing the 1% confidence interval  except for the rightmost points  the error bars are too small to distinguish m the figure   the branching factor is time-dependent  where the number of enabled groups decreases through the night the primary reason for a decreasing branching factor is that as groups are selected for execution  the number of unscheduled groups decreases leaf nodes occur at approximately the same depth for a couple of reasons first  the estimated duration of all schedules is about the same second  the group durations in this particular scheduling problem do not vary much the 1 samples had a mean depth of 1 and a standard deviation of 1 the size of the search space is estimated by the product of the average branching factors  this data suggests that the number of schedules in the search space is on the order of 1 
	bresina  drummond  and swans1n 	1mb 

1 	search space q u a l i t y 
a schedule produced should not only satisfy all hard constraints but  ideally  should also achieve an optimal ob-
jective function score 1 hence  it is not solely the size of the search space that determines the difficulty of finding a good schedule  the dtnstty of quality schedules is also important the same sampling method used to estimate search space size and shape can also be used to characterize schedule quality density evaluating the schedules generated via iterative sampling yields a frequency distribution of scores which estimates the expected density of each score obtainable in the search space we refer 
to this statistical estimate as a quality density function  and it 1 the basis for our esq method 
　in determining the quality density function  it is important  yet often non-trivial  to obtain an unbiased sample from the solution space  i e   to sample the possible schedules uniformly if the tree has a constant branching factor at every  internal  node and if all paths have the same length  i t   if the search tree is balanced n-ary   then iterative sampling produces an unbiased  uniform sample however  constant branching is not a necessary condition for uniform sampling  and it can be weakened as follows if  for every depth  all nodes at that depth have the same branching factor  then iterative sampling will be uniform  assuming equal-length paths  as can be seen in figure 1  the branching factor changes from depth to depth  however  the minuscule 1% confidence intervals indicate that the branching factor is nearly constant for nodes at the same depth  and  as argued above  the paths have approximately the same length   
　we performed 1 iterative samples  and each schedule generated was scored in terms of the composite ob-
jective function as well as in terms of each individual attribute  priority  fairness  and airmass  from these scores we constructed a quality density function for the composite objective and for each individual attribute the resulting four density functions are shown in figures 1  the two dashed lines in each figure are dis-
1
　　another important consideration is schedule execution robustness  see  drummond  bresina  & swanson  1  
1 	planning 
cussed below   the scores have been quantized into 1  score buckets  of equal size for each solid line  the r-coordinate is the mid-point of a score interval and the line's height indicates the number of samples that obtained a score in that interval 
1 	e v a l u a t i n g scheduler p e r f o r m a n c e 
we next describe how to evaluate scheduler performance using the quality density function  and we compare out two scheduling techniques with respect to quality deh sity in each of the four plots  figures 1   in addi tion to the quality density functions  we also indicate the performance of the two scheduling techniques the single schedule generated by each technique was scorec in terms of the composite objective function and in term 
of each individual attribute in each of the figures  the score obtained by a scheduler is shown by a dashed line  the height of the line is immaterial  it simply points to an x axis value  note that the composite score  showr 
in figure 1  obtained by each scheduler is the sum of the three attribute scores  shown in figures 1  1  and 1  
　as shown in figure 1  greedy lookahead obtained composite score of - 1 and heuristic dispatch ob tained a composite score of +1 the difference be tween these scores is 1  without knowledge of the distribution of scores  we do not know how significant thi difference is however  the quality density function en ables this difference to be interpreted more meaningfull one such interpretation is in terms of the standard deviation from the mean the quality density function for the composite scores  figure 1  had an estimated mear of 1 and an estimated standard deviation of 1 basec on these estimated statistics  the lookahead score is 1c standard deviations better than the mean  while the dispatch score is 1 standard deviations worse than the mean interpreting the schedulers1 performance against the background of the quality density function provide much more insight into just how much better greed lookahead performed 
　it is interesting that  with respect to the objective function  heuristic dispatch was no better than the mear value obtained by iterative  random  sampling in con trast  the score obtained by greedy lookahead is better than all of the 1 scores obtained by iterative sampling  figure 1  notice that heuristic dispatch outperforms greedy lookahead with respect to the priority attribute  figure 1  this is to be expected since group priority is the primary determinant of which group gets selected by the dispatcher  whereas  in the greedy search  priority has the same importance as the other two attributes 
　when comparing scheduling techniques  in addition to schedule quality  it is also important to take into account the amount of time to generate a schedule a natural performance measure that combines both of these scheduler performance factors is schedule quality divided by generation time in order to make these two factors easier to combine  we can express schedule quality in units of time as well  hence  obtaining a unitless performance measure  this can be accomplished by computing the expected amount of time for iterative sampling to generate a schedule that scores at least as well as a given schedule let p be the probability that a randomly generated schedule scores at least as well as some given schedule this probability is determined by the  cumulative  distribution function for schedule quality1 the expected number of iterative samples to generate such a schedule is then 1/p multiplying this expected number of samples by the computation time to perform one sample yields the expected sampling time measure of schedule quality with this quality measure  a higher measure indicates better quality  and since lower schedule generation time is preferred  a higher quality/time measure indicates superior performance 
　this expected sampling time measure for schedule quality is based on more information about the quality density function than just its mean and standard deviation  i c   it also takes into account the shape of the density function  hence  if the distribution function can be estimated accurately enough  this quality measure not 
1
　　the distribution function for schedule quality  f x   equals f f p dy  where f v is the quality density function since smaller scores we better  the probability of randomly obtaining a score at least aa good as x equal to f x  
only makes it easier to combine with generation time  but also yields a more discriminatory schedule quality comparison than the previously described metric of standard deviations from the mean 
　we determined the schedule generation time for our two techniques  as well as for iterative sampling  this was done by averaging over 1 runs the following are the results we obtained1 dispatch took 1 seconds  greedy lookahead took 1 seconds for ten iterations  and iterative sampling took 1 seconds per sample 
　assuming a normal distribution function  we can compute the probability that a random sample will obtain a schedule score at least as good as some given score this probability is 1 for +1 standard deviations  and it is 1 x 1 for -1 standard deviations 1 for the score obtained by dispatch  the expected number of samples is 1 /1 or 1  and for the score obtained by lookahead  the expected number is 1 /1 x 1 - 1 or 1 x 1′ multiplying by the time to perform one sample yields expected computation times of 1 seconds for dispatch and 1 x 1 seconds for lookahead for dispatch  the quality/time measure is 1 secs/1 sees or 1  and for lookahead  it is 1 x 1sees/1sees or 1x 1 hence  greedy lookahead far outperformed heuristic dispatch on this problem  the ratio of the lookahead measure to the dispatch measure is 1 x 1 
　in summary  we started with the scores obtained by our two techniques  which had a difference of 1 this information told us that the lookahead scheduler achieved a better score  but there was no well-founded interpretation of how much better it was based on the results of statistical sampling  the quality/time measure yielded an interpretation of the lookahead score as being 1 orders of magnitude better than the dispatch score 
*the results are cpu time for non-gc  user tasks from the 
common lisp time macro 
   * there may be some error between our sample distribution and the true distribution furthermore  since the range of the objective function is bounded  within some unknown interval   the distribution is actually a truncated normal 
	bresina  drumm1nd  and swans1n 	1 
1 	search heuristics 
in traditional mathematical programming  the objective function is used to directly guide the construction of a solution certain assumptions are made about the form and behavior of the objective function that allow for the application of closed-form techniques to find optimal solutions ai approaches to constrained optimization typically assume that the objective function is so complex that closed-form solutions cannot be found typically  these approaches employ state-space search for a particular problem instance  or class   heuristics are designed that can help guide search toward a solution that scores well according to the objective function however  it is often very difficult to design efficient search heuristics that effectively capture the information in the objective function when the heuristic and the objective become too  decoupled   the heuristic can end up having a different bias than that encoded in the objective and  hence  can fail to find high quality solutions 
　one indicator of a bias discrepancy is when a significant portion of the quality density function is better than the heuristically selected solution's score this is the case with the dispatch heuristic - its solution scored no better than the quality density function's mean value  figure 1  though this information indicates a bias discrepancy  it does not reveal the discrepancy's source  i t   what information in the objective is not effectively encoded in the heuristic this can be revealed by examining the heuristic's performance with respect to the individual objective function attributes as seen in figures 1 and 1  it is obvious that the dispatch heuristic is 
not taking into account fairness and air mass since its solution scores worse  with respect to these two attributes  than almost all of the randomly found solutions 
　after identifying some aspect of the objective function that is being ignored by the heuristic  one still has to determine how to repair the heuristic in general  this is a non-trivial problem  its difficulty depends on the complexity of the objective function  i e   how the attributes are combined  and on the form of the heuristic 
　since the solution found with the greedy heuristic scored better than all of the randomly generated solutions  there is no obvious indication of a bias discrepancy - not surprising  since the greedy heuristic was identical to the objective function however  we can still find such a discrepancy by looking at the quality density functions of the individual attributes recall that the objective function was designed to give equal importance to all three attributes a solution which achieved a balanced tradeoff among the attributes should score equally well with respect to each attribute  that is  each attribute score of the solution should be the same number of standard deviations from the mean of the attribute's quality density function however  as can be seen in figures 1  1  and 1  the solution found with the greedy heuristic does not score the same with respect to the attributes  rather it scores best on airmass  second best on priority  and worst on fairness this imbalance can be corrected by adjusting the weighting factors on the attributes in the greedy search heuristic though the direction of the adjustment is obvious  finding appropriate weighting fac-
1 	planning 

tors may take some experimentation  either manually or using machine learning techniques  
　though we used the objective function as a greedy search heuristic this is not always best  e g the objective function may be too expensive to evaluate or some attributes may not be effective search heuristics  i e   an attribute's scores with respect to a partial solution may not be predictive of the scores of its completions  the esq method can support the decision of which subset of the attributes to include in the search heuristic using the scheduler comparative analysis method illustrated 
in the previous section  we earned out the following em pineal evaluation for each attribute  a greedy lookahead search was performed using a heuristic based only on that single attribute  which is equivalent to zeroing the weight* of the other two attributes in the composite search heuristic  for each single-attribute greedy search  the best schedule found was evaluated in terms of the  original  composite objective function 
   figure 1 shows the three composite scores obtained by each single-attribute search heuristic against the background of the same composite quality density function as in figure 1 these results indicate that airmass is the best single-attribute local heuristic  i e   partial schedules that score well with respect to airmass are likely to be prefixes of complete schedules that score well with respect to the composite objective function the results also indicate that fairness is the worst local predictor  which makes sense since it is the  most global  attribute in the objective function that is  there are schedules that  while rated highly  perhaps even optimally  with respect to fairness  have prefixes that score poorly for example  consider a schedule that assigns each user's groups fairly  but gives each user a contiguous interval of time while the final schedule scores well  its prefixes score poorly  and would not be found during search the fairness heuristic prefers schedules that frequently alternate between users priority turned out not to be a very good local heuristic either  which explains why atis dispatch did not perform well with respect to the composite objective function 

1 	genera  application of esq 
in this section we discuss how the esq method can be applied in more general contexts our esq characterization was done under the assumption that iterative sampling in the search tree produces a uniform sample from the space of possible solutions our formulation of the scheduling problem has two beneficial characteristics that make it easy to satisfy this assumption firstly  nodes at the same depth in the search tree have nearly the same branching factor and the lengths of paths are approximately the same length because of this  sampling uniformly at each branch point implies that the leaf nodes ire uniformly sampled secondly  the search tree includes only feasible schedules  i e   schedules that satisfy all the hard constraints hence  uniformly sam pling the search tree leaf nodes implies a uniform sampling of the solution space in our case  the search tree is chronologically organized  however  as long as there is a one-to-one correspondence with leaf nodes and solutions  then the esq method as described directly applies 
　due to the shape of the search tree for our telescope scheduling problem  it was easy to uniformly sample the leaf nodes  however  in general  this is not trivial to guarantee uniform sampling of the leaf nodes  the random selection at each branch point must be biased by the size of the subtree below each choice that is  the probability of selecting a particular child node must be equal to the proportion of leaf nodes in the child's subtree  rel ative to the total in the current node's subtree  within the field of randomized algorithms  theoretical and practical results have been obtained for randomized approximate counting and almost uniform generation of combinatorial structures  for example  see  sinclair  1  jerum  valiant  it vaziram  1  these results can help in the construction of algorithms that almost  i e   with small bias  uniformly sample a tree's leaf nodes 
　there is another potential problem with general application of the esq method in many search formulations  not all leaf nodes correspond to solutions - some are failure nodes these failures are incorporated into the esq method as follows as before  the leaf nodes are uni formly sampled  however  the quality density function for solutions is computed based only on the non-failure leaf nodes the probability of failure  pj  is estimated by the number of failure leaf nodes encountered during the sampling divided by the number of samples as before  we compute the probability  p*  that a randomly generated schedule would score at least as well as some given schedule the probability  p  that a random sample produces a schedule that scores at least as well as some given schedule is then the product pb  1 - pj  using p  the quality/time measure is computed as before 
　in our telescope domain  we have all day to schedule  hence  whenever the scheduling problem changes  due to modified or new observation requests   we can afford to carry out a new set of esq experiments in order to retune our search heuristics however  in other domains  this may not be feasible in such cases  it may be possible to select a representative suite of problem instances and base the esq statistical evaluation and search heuristic tuning on the combined samples of these problems 
1 	summary and conclusion 
this paper's main contributions are the esq method for characterizing scheduling problems and evaluating scheduler performance and the demonstration of this method in a practical telescope scheduling domain we demonstrated how the esq method can provide an estimate of the site and shape of the search tree and can provide a quality/time measure of scheduler performance in addition  we illustrated how the method can support the construction of more effective search heuristics while we have been concerned with scheduling techniques and scheduling problems  the esq method should apply more generally within the larger class of constrained optimization problems 
　the esq method is statistical  employing a monte carlo algorithm called iterative sampling in summary  the complete esq method involves the following steps 
1 based on the shape and size of the search tree  uniformly sample the tree  from this sample  compute the frequency distribution of schedule quality and the failure probability  pj 
1 from the frequency distribution  characterize the quality density function that is  compute the sample mean and standard deviation of the schedule scores  and determine the distribution type  in the example presented  the distribution was normal   
1 express the scheduler's score in terms of standard deviations from the quality density function's mean 
1 using the distribution function  determine the probability  pi  that a randomly generated schedule will score at least as good as the scheduler's score then compute the probability  p  that a random sample will produce such a schedule as pi  1 - pj  
1 compute the expected time for iterative sampling lo obtain a score at least as good as the given score by multiplying the expected number of samples required  1/p  by the time to perform one sample 
1 compute the quality/time measure as the expected iterative sampling time  computed in previous btep  divided by the scheduler's computation time 
　in some sense  our quality/time measure is a  common currency  for expressing scheduler performance on a given problem in addition to comparing the performance of different schedulers  the esq method can also be employed for the following purposes  i  to compare the difficulty of different problems with respect to a given scheduler   n  to evaluate the impact of different search formulations with respect to scheduler performance   in  to evaluate the impact of different objective functions with respect to problem difficulty for a given scheduler 
　recall that our nondetermimstic greedy lookahead chose the best schedule after ten iterations  this number of iterations was chosen arbitrarily with more itera tions  the algorithm might find a better schedule - although at a higher computational cost using our qual ity/time measure  we could empirically determine the number of iterations that yields the most cost-effective performance this type of algorithm tuning could be applied to any scheduler whose performance behavior 
	bresina drummond andswanson 	1 

varies depending on the amount of computation time allocated similarly  we could empirically determine the amount of lookahead that is moat cost-effective  recall that our greedy algorithm used one-step lookahead  we 
intend to carry out such experiments in the future 
　in addition to the reported uses of the quality density function  we speculate that  in some cases  it can be useful in characterizing the difficulty of a problem for 
example  consider the hypothetical quality density function in figure 1 one might conclude that this problem is intrinsically easy since there are so many high quality solutions and not many low quality ones even an uninformed search  like iterative sampling  can quickly find a high quality solution for such a problem in cases that are not so extreme  it is less obvious how to relate shape of quality density and problem difficulty this topic requires further research 
　the experiments reported in this paper used a lispbased scheduling engine however  in order to make the system useful to astronomers  we have had to reimplement it in c so that they themselves can extend and support it this new system will provide a self eval uation facility which will automatically perform the esq characterization experiments upon request the final version of the system will be accessible to users via the internet and will accept new atis groups on a daily basis thus  the definition of the scheduling problem will change frequently we expect that a telescope manager will be able to use the belf evaluation facility to track the changing characterization of the search space based on the current characterization  a telescope manager could choose the best scheduling method and search heuristic for the current mix of atis groups it would be useful if the system itself were able to make these choices work along these lines is reported by greenwald and dean  1   where monte carlo simulation builds a picture of the search space that can be used to detect and avoid potential schedule bottlenecks exploring the use of esq in this context is a topic for future work 
acknowledgments 
we would like to thank will edgington and ellen drascher for their significant contributions towards making the automatic telescope software a reality for their helpful feedback on this paper  we would like to thank lise getoor  rich levinson  steve minton  nicola muscettola  and barney pell 
1 	planning 
