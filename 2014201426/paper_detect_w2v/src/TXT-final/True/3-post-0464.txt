
we present the problem of learning to communicate in decentralized and stochastic environments  analyzing it formally in a decision-theoretic context and illustrating the concept experimentally. our approach allows agents to converge upon coordinated communication and action over time.
1 introduction
learning to communicate in multi-agent systems is an emerging challenge ai research. autonomous systems  developed separately  interact more and more often in contexts like distributed computing  information gathering over the internet  and wide-spread networks of machines using distinct protocols. as a result  we foresee the need for autonomous systems that can learn to communicate with one another in order to achieve cooperative goals. we make some first steps towards solving the attendant problems.
¡¡coordination among agents acting in the same environment while sharing resources has been studied extensively  particularly by the multi-agent systems community. while such coordination may involve communication  typically there is no deliberation about the value of communication  resulting in systems with no communication or ones allowing free communication of well-understood messages. in contrast  we study decentralized systems that require agents to adapt their communication language when new situations arise or when mis-coordination occurs.
1 the decentralized learning framework
we study the problem in the context of decentralized markov decision processes  bernstein et al.  1  with communication  dec-mdp-com . such a process is a multi-agent extension of a common mdp in which each agent ¦Ái observes only its own local portion of the state-space  and can attempt to communicate with others using the set of messages ¦²i. decentralization makes dec-mdps  with communication or not  significantly harder to solve than regular mdps; for their complexity properties  see  goldman and zilberstein  1 .
¡¡if agents in a system share the same language  optimal linguistic action is a matter of deciding what and when to communicate  given its cost relative to the projected benefit of sharing information. however  where agents utilize different sets of messages  and do not fully understand one another  message-passing alone is not enough. rather  agents need to learn how to respond to the messages that are passed between them-in a sense  learning what those messages mean.
definition 1  translation . let ¦² and ¦²1 be sets of messages. a translation  ¦Ó  between ¦² and ¦²1 is a probability function over message-pairs: for any messages ¦Ò  ¦Ò1  ¦Ó ¦Ò  ¦Ò1  is the probability that ¦Ò and ¦Ò1 have the same meaning. ¦Ó¦²+ ¦²1 is the set of all translations between ¦² and ¦²1.
agents may need to consider multiple possible translations between messages; that is  agents possess beliefs as to which translation is correct given their present situation.
definition 1  belief-state . let agents ¦Á1  ¦Á1 use sets of messages ¦²1  ¦²1. a belief-state for ¦Ái is a probabilityfunction ¦Âi over translation-set ¦Ó¦²+i ¦²j  i =1 j . for translation  is the probability that ¦Ó is correct.
¡¡updating beliefs about translations is thus an important part of the overall process of learning to communicate. agents act based upon local observations  messages received  and current beliefs about how to translate those messages. their actions lead to new observations  causing them to update beliefs and translations. the procedure governing these updates comprises the agent's language-model  a function from actions  messages  and observations to distributions over translations. such models may be highly complex  or difficult to compute  especially where languages are complicated  or the environment is only partially observable. here we concentrate upon special-but interesting-cases for which generating these probabilities is much more straightforward.
1 formal properties of the problem
our main formal results isolate conditions under which decmdp-coms reduce to simpler problems  and present a protocol for learning to communicate in such reduced problems.
reduction to mmdps
boutilier  defines multiagent mdps  mmdps   consisting of a set of agents operating in a fully- and commonlyobserved environment; transitions between states in that environment arise from joint actions of all agents  and a common reward is shared by the system as a whole. while we can

figure 1: reward accumulated as language is learned.
calculate an optimal joint policy for such a process offline  this is not the same thing as implementing it. unless agents can coordinate their actions  there is no guarantee of a jointly optimal policy  since communication is not allowed  or is unreliable. boutilier thus defines coordination problems  which arise when agents may each take an individual action that is potentially optimal  but which combine in sub-optimal fashion. we show that certain  putatively more complex  decmdp-coms in fact reduce to mmdps for which such problems do not arise. this is notable  as dec-mdps are generally intractable  while mmdps can be solved efficiently.
definition 1  fully-describable . a dec-mdp-com is fullydescribable if and only if each agent ¦Ái possesses a language ¦²i that is sufficient to communicate both:  a  any observation it makes  and  b  any action it takes.
definition 1  freely-describable . a dec-mdp-com is freely-describable if and only if for any agent ¦Ái and message ¦Ò ¡Ê ¦²i  the cost of communicating that message is 1.
claim 1. a dec-mdp-com is equivalent to an mmdp without coordination problems if  a  it is both fully- and freelydescribable; and  b  agents share a common language. 
suitability and convergence
for any freely- and fully-describable dec-mdp-com  agents can calculate an optimal joint policy  under the working assumption that all agents share a common language and that all relevant information is shared. where agents must in fact learn to communicate  however  implementation of such policies requires cooperation from the environment  so that agents can update translations appropriately over time. the full definition of a suitable dec-mdp-com cannot be included here; we simply note that in such problems  the probability that each agent assigns to the actual prior observations and actions of others following some state-transition is strictly greater than that of the observations and actions considered most likely before that transition  unless those entries were actually correct . suitable dec-mdp-coms provide enough information to ensure that others' actual actions and observations are more likely than mistaken ones.
¡¡we extend work of goldman et al.   where agents communicate states but not actions   to give an elementary action protocol. using such a protocol for action and beliefupdate  agents move towards optimality  based upon the observed consequences of action in a suitable problem-domain.
claim 1. given an infinite time-horizon  agents acting according to the elementary action protocol in a suitable decmdp-com will eventually converge upon a joint policy that is optimal for the states they encounter from then on. 
1 empirical results and conclusions
to explore the viability of our approach  we implemented our language-learning protocol for a reasonably complex  but still suitable  dec-mdp-com  involving two agents in joint control of a set of pumps and flow-valves in a factory setting.
¡¡our results show the elementary protocol converging on optimal policies in a wide range of problem-instances. figure 1 gives an example  for a problem-instance featuring 1 vocabulary-items for each agent  showing the percentage of total accumulated reward  and total shared vocabulary  at each time-step in the process of learning and acting. as can be seen  the learning process  top  dotted line  proceeds quite steadily. reward-accumulation  on the other hand  grows with time before finally stabilizing. initially  language learning outpaces reward gain given that knowledge  as agents still find many of the other's actions and observations hard to determine. as time goes on  the rate of accumulated reward narrows this gap considerably; agents now know much of what they need to communicate  and spend more time accumulating reward in familiar circumstances  without necessarily learning anything new about the language of others.
¡¡these experimental results conform with intuition  showing that while a small amount of language learning does little to help agents in choosing their actions  they are capable of very nearly optimal action even in the presence of an understanding that is still less than perfect. this opens the door for further study into approximation in these contexts. we continue to investigate and compare other approaches to the problem  including analysis of the differences between possible optimal offline techniques and online learning methods.
acknowledgments
this work was supported in part by the national science foundation under grant iis-1 and by the air force office of scientific research under grant f1-1.
