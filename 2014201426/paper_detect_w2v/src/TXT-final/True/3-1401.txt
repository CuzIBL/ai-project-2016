
we study the effects of feature selection and human feedback on features in active learning settings. our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting  beyond that achieved via selective sampling alone  standard active learning  if we have access to an oracle that can point to the important  most predictive features. consistent with previous findings  we find that feature selection based on the labeled training set has little effect. but our experimentson human subjects indicate that human feedback on feature relevance can identify a sufficient proportion  1%  of the most relevant features. furthermore  these experiments show that feature labeling takes much less  about 1th  time than document labeling. we propose an algorithm that interleaves labeling features and documents which significantly accelerates active learning.
1 introduction
a major bottleneck in machine learning applications is the lack of sufficient labeled data for adequate classifier performance  as manual labeling is often tedious and costly. techniques such as active learning  semi-supervised learning  and transduction have been pursued with considerable success in reducing labeling requirements. in the standard active learning paradigm  learning proceeds sequentially  with the learning algorithm actively asking for the labels of instances from a teacher. the objective is to ask the teacher to label the most informative instances in order to reduce labeling costs and accelerate the learning. there has been very little work in supervised learning in which the user  teacher  is queried on aspects other than class assignment of instances. in experiments in this paper we study the benefits and costs of feature feedback via humans on active learning. to this end we pick document classification  sebastiani  1  as the learning problem of choice because it represents a case of supervised learning which traditionally relies on example documents as input for training and where users have sufficient prior knowledge on features which can be used to accelerate learning. for example  to find documents on the topic cars in traditional supervised learning the user would be required to provide sufficient examples of cars and non-cars documents. however  this is not the only way in which the information need of a user looking for documents on cars can be satisfied. in the information retrieval setting the user would be asked to issue a query  that is  state a few words  features  indicating her information need. thereafter  feedback which may be at a term or at a document level may be incorporated. in fact  even in document classification  a user may use a keyword based search to locate the initial training examples. however  traditional supervised learning tends to ignore the prior knowledge that the user has  once a set of training examples have been obtained. in this work we try to find a marriage between approaches to incorporating user feedback from machine learning and information retrieval and show that active learning should be a dual process - at the term and at the document-level. this has applications in email filtering and news filtering where the user has some prior knowledge and a willingness to label some  as few as possible  documents in order to build a system that suits her needs. we show that humans have good intuition for important features in text classification tasks since features are typically words that are perceptible to the human and that this human prior knowledge can indeed accelerate learning.
　in summary  our contributions are:  1  we demonstrate that access to a feature importance oracle can improve performance  f1  significantly over uncertainty sampling.  1  we show that even naive users can provide feedback on features with about 1% accuracy of the oracle.  1  we show that the relative manual costs of labeling features is about 1th that of document feedback.  1  we show a method of simultaneously soliciting class labels and feature feedback that improves classifier performance significantly.
　we describe the experimental setup in sec. 1 and show how feature selection using an oracle is useful to active learning in sec. 1. in sec. 1 we show that humans can indeed identify useful features and show how human-chosen features can be used to accelerate learning in sec. 1. we relate our work to past work in sec. 1 and outline directions for the future in section sec. 1.
1 experimental setup
our test bed for this paper comes from three domains:
 1  the 1 most frequent classes from the reuters-1 corpus  1 documents .  1  the 1-newsgroups corpus
 1 documents from 1 usenet newsgroups .  1  the first 1 topics from the tdt-1 corpus  1 documents in 1 languages from broadcast and news-wire sources . for all three corpora we consider each topic as a one versus all classification problem. we also pick two binary classification problems viz.  baseball vs hockey and automobiles vs motorcycles from the 1-newsgroups corpus. in all we have 1 classification problems.1 all the non-english stories in the tdt corpus were machine translated into english. as features we use words  bigrams and trigrams obtained after stopping and stemming with the porter stemmer in the rainbow toolkit  mccallum  1 
　we use linear support vector machines  svms  and uncertainty sampling for active learning  scholkopf and smola  1; lewis and catlett  1 . svms are the state of art in text categorization  and have been found to be fairly robust even in the presence of many redundant and irrelevant features  brank et al.  1; rose et al.  1 . uncertainty sampling  lewis and catlett  1  is a type of active learning in which the example that the user  teacher  is queried on is the unlabeled instance that the classifier is most uncertain about. when the classifier is an svm  unlabeled instances closest to the margin are chosen as queries  tong and koller  1 . the active learner may have access to all or a subset of the unlabeled instances. this subset is called the pool and we use a pool size of 1 in this paper. the newly labeled instance is added to the set of labeled instances and the classifier is retrained. the user is queried a total of t times.
　the deficiency metric  baram et al.  1  quantifies the performanceof the querying function for a given active learning algorithm. originally deficiency was defined in terms of accuracy. accuracy is a reasonable measure of performance when the positive class is a sizeable portion of the total. since this is not the case for all the classification problems we have chosen  we modify the definition of deficiency  and define it in terms of the f1 measure  harmonic mean of precision and recall  rose et al.  1  . using notation similar to the original paper  baram et al.  1   let u be a random set of p labeled instances  f1t rand  be the average f1 achieved by an algorithm when it is trained on t randomly picked examples and f1t act  be the average f1 obtained using t actively picked examples. deficiency d is defined as:

f1m rand  is the f1 obtained with a large number  m  of randomly picked examples. for this paper we take m = 1 and t = 1...1. when t = 1 we have one positive and one negative example. f1t    is the average f1 computed over 1 trials. in addition to deficiency we report f1t for some values of t. intuitively  if cact is the curve obtained by plotting f1t act   crand is the corresponding curve using random sampling and cm is the straight line f1t = f1m then deficiency is the ratio of the area between
cact and cm and the area betweencrand and cm. the lower the deficiency the better the active learning algorithm. we aim to minimize deficiency and maximize f1.
1 oracle feature selection experiments
the oracle in our experiments has access to the labels of all p documents in u and uses this information to return a list of the k most important features. we assume that the parameter k is input to the oracle. the oracle orders the k features in decreasing information gain order. given a set of k features we can perform active learning as discussed in the previous section and plot cact for each value of k.

figure 1: average f1t act  for different values of k. k is the number of features and t is the number of documents.
　figure 1 shows a plot of f1t act  against number of features k and number of labeled training examples t  for the earnings category in reuters. the x  y and z axes denote k  t and f1 respectively. the number of labeled training examples t ranges from 1...1 in increments of 1. the number of features used for classification k has values from 1 to 1  all features . the dark dots represent the maximum ft for each value of t  while the dark band represents the case when all features are used. this method of learning in one dimension is representative of traditional active learning. clearly when the number of documents is few  performance is better when there is a smaller number of features. as the number of documents increases the number of features needed to achieve best accuracy increases. from the figure it is obvious that we can get a big boost in accuracy by starting with fewer features and then increasing the complexity of the model  the number of relevent features  as the number of labeled documents increase.
　all 1 of our classification problems exhibit behavior like that in figure 1 raghavan et al.  1 . we report the average deficiency  f1  f1 score with 1 labeled examples  and f1 in fig. 1 to illustrate this point. the column labeled act shows performance using traditional active learning and all the features. the column labeled ora shows performance obtained using a reduced subset of features using the oracle.
class ◎d1f1f1f1actoracleactoraactoraactreuters11111111-news.1111111tdt1111111bas. vs hock1111111auto vs mot.1111111figure 1: improvements in deficiency  f1 and f1 using an oracle to select the most important features. remember that the objective is to minimize deficiency and maximize f1. for each of the three metrics  figures in bold are statistically significant improvements over uncertainty sampling using all features  the correspondingcolumns denotedby act . when 1documents are labeled  f1  using the entire feature set leads to better f1 scores.　intuitively  with limited labeled data  there is little evidence to prefer one feature over another. feature/dimension reduction  by the oracle  allows the learner to  focus  on dimensions that matter  rather than being  overwhelmed  with numerous dimensions right at the outset of learning. as the number of labeled examples increases  feature selection becomes less important  as the learning algorithm becomes more capable of finding the discriminating hyperplane   feature weights . we experimented with filter based methods for featureselection  which didnot work verywell  i.e. tiny or no improvements . this is expected given such limited training set sizes  see fig. 1   and is consistent with most previous findings  sebastiani  1 . next we determine if humans can identify these important features.
1 human labeling
consider our introductory example of a user who wants to find all documents that discuss cars. from a human perspective the words 'car'  'auto' etc may be important features in documents discussing this topic. given a large number of documents labeled as on-topic and off-topic  and given a classifier trained on these documents  the classifier may also find these features to be most relevant. with little labeled data  say 1 labeled examples  the classifier may not be able to determine the discriminating features. while in general in machine learning the source of labels is not important to us  in active learning scenarios in which we expect the labels to come from humans we have valid questions to pose:  1  can humans label features as well as documents   1  if the labels people provide are noisy through being inconsistent  can we learn well enough   1  are features that are important to the classifier perceptible to a human 
　our concern in this paper is asking people to give feedback on features  or word n-grams  as well as entire documents. we may expect this to be more efficient  since documents contain redundancy  and results from our oracle experiments indicate great potential. on the other hand  we also know that synthetic examples composed of a combination of real features can be difficult to label  baum and lang  1 .
1 experiments and results
in order to answer the above questions we conducted the following experiment. we picked 1 classification problems which we thought were perceptible to the average person on the street and also represented the broad spectrum of problems from our set of 1 classification problems. we took the two binary classification problems and from the remaining 1 one-versus-all problems we chose three  earnings  hurricane mitch and talk.politics.mideast . for a given classification problem we took the top 1 features as ranked by information gain on the entire labeled set. in this case we did not stem the data so that features remain as legitimate english words. we randomly mix these with features which are much lower in the ranked list. we show each user one feature at a time and give them two options - relevant and not-relevant/don't know. a feature is relevant if it helps discriminate the positive or the negative class. we measure the time it takes the user to label each feature. we do not show the user all the features as a list  though this may be easier  as lists provide some context and serve as a summary. hence our method provides an upper bound on the time it takes a user to judge a feature. we compare this with the time it takes a user to judge a document. we measure the precision and recall of the user's ability to label features. we ask the user to first label the features and then documents  so that the feature labeling process receives no benefit due to the fact that the user has viewed relevant documents. in the learning process we have proposed  though  the user would be labeling documents and features simultaneously  so the user would indeed be influenced by the documents he reads. hence our method is more stringent than the real case. we could in practice ask users to highlight terms as they read documents. experiments in this direction have been conducted in information retrieval  croft and das  1 .
　our users were six graduate students and two employees of a company  none of whom were authors of this paper. of the graduate students  five were in computer science and one from public health. all our users were familiar with the use of computers. five users understood the problem of document classification but none had worked with these corpora. one of our users was not a native speaker of english. the topics were distributed randomly  and without considering user expertise  so that each user got an average of 1 topics. there were overlapping topics between users such that each topic was labeled by 1 users on average. a feedback form asking the users some questions about the difficulty of the task was handed out at the end.
　we evaluated user feature labeling by calculating their average precision and recall at identifying the top 1 features as ranked by an oracle using information gain on the entire labeled set. fig. 1 shows these results. for comparison we have also provided the precision and recall  against the same oracle ranking of top 1 features  obtained using 1 labeled examples  picked using uncertainty sampling  denoted by  1.
class
problemprec.rec.avg. time  secs hum. 1hum. 1feat.docsbaseball..111111auto vs ...111111earnings111111...mideast111111...mitch111111average111111figure 1: ability of users to identify important features. precision and recall against an oracle  of users  hum.  and an active learner which has seen 1 documents  1 . average labeling times for features and documents are also shown. all numbers are averaged over users.
precision and recall of the humans is high  supporting our hypothesisthat features that a classifier finds to be relevant after seeing a large number of labeled instances are obvious to a human after seeing little or no labeled data  the latter case being true of our experiments . additionally the precision and recall  1 is significantly lower than that of humans  indicating that a classifier like an svm needs to see much more data before it can find the discriminatory features.
　the last column of fig. 1 shows time taken for labeling features and documents. on average humans require about 1 times longer to label documents than to label features. note that features may be even easier to label if they are shown in context - as lists  with relevant passages etc. there are several other metrics and points of discussion such as user expertise  time taken to label relevant and non-relevant features and so on  which we reserve for future work. one important consideration though  is that document length influences document labeling time. we found the two to be correlated by r = 1 which indicates a small increase in time for a large increase in length. the standard deviations for precision and recall are 1 and 1 respectively. different users vary significantly in precision  recall and the total number of features labeled relevant. based on feedback on a post-labeling survey we are inclined to believe that this is due to individual caution exercised during the labeling process.
　some of the highlights of the post-labeling survey are as follows. on average users found the ease of labeling features to be 1  where 1 is most difficult and 1 is very easy  and documents 1. in general users with poor prior knowledge found the feature labeling process very hard  although as we will show  their labels were extremely useful to the classifier. the average expertise  1=expert  was 1  indicating that most users felt they had little domain knowledge for the tasks they were assigned. we now proceed to see how to use features labeled as relevant by our naive users in active learning.
1 a human in the loop
we saw in sec. 1 that feature selection coupled with uncertainty sampling gives us big gains in performance when there are few labeled examples. in sec. 1 we saw that humans can discern discriminative features with reasonable accuracy. we now describe our approach of applying term and document level feedback simultaneously in active learning.
1 interactive learning algorithm
let documents be represented as vectors xi = xi1...xi|f|  where |f| is the total number of features. at each iteration the active learner not only queries the user on an uncertain document  but also presents a list of f features and asks the user to label features which she considers relevant. the features to be displayed to the user are the top f features obtained by ordering the features by information gain. to obtain the information gain values with t labeled instances we trained a classifier on these t labeled instances. then to compute informationgain  we used the 1 top ranked farthest from the margin  documents from the unlabeled set in addition to the t labeled documents. using the unlabeled data for term level feedback is very common in information retrieval and is called pseudo-relevance feedback  salton  1 .
　the user labels some of the f features which he considers discriminative features. let ~s = s1...s|f| be a vector containing weights of relevant features. if a feature number i that is presented to the user is labeled as relevant then we set si = a  otherwise si = b  where a and b are parameters of the system. the vector ~s is noisier than the real case because in addition to mistakes made by the user we lose out on those features that the user might have considered relevant  had he been presented that feature when we were collecting relevance judgments for features. in a real life scenario this might correspond to the lazy user who labels few features as relevant and leaves some features unlabeled in addition to making mistakes. if a user had labeled a feature as relevant in some past iteration we don't show the user that feature again. we incorporate the vector ~s as follows. for each xi in the labeled and unlabeled sets we multiply xij by sj to get xij1 . in other words we scale all relevant features by a and non-relevant features by b. we set a = 1 and b =1. 1
　by scaling the important features by a we are forcing the classifier to assign higher weights to these features. we demonstrate this with the following example. consider a linear svm  |f| = 1 and 1 data points x1 =  1  and x1 =  1  with labels +1 and  1 respectively. an svm trained on this input learns a classifier with w =   1 +1 . thus both features are equally discriminative. if feature 1 is considered more discriminative by a user  then by our method
 
thus assigning higher weight to f1. now  this is a  soft  version of the feature selection mechanism of sec. 1. but in that case the oracle knew the ideal set of features; we can view those set of experiments as a special case where b = 1. we expect that human labels are noisy and we do not want to zero-out potentially relevant features.
1 experiments and results
to make our experiments repeatable  to compute average performance and for convenience  we simulate user interaction as follows. for each classification problem we maintain a list of features that a user might have considered relevant had he been presented that feature. for these lists we used the judgments obtained in sec. 1. thus for each of the 1 classification problems we had 1 such lists  one per user who judged that topic. for the 1 tdt topics we have topic descriptions as provided by the ldc  and we simulated explicit human feedback on feature relevance as follows: the topic descriptions contain names of people  places and organizations that are key players in this topic in addition to other keywords. we used the words in these topic descriptions as the list of relevant features. now  given these lists we can perform the simulated hil  human in the loop  experiments for 1 classification problems. at each iteration f features are shown to the user. if the feature exists in the list of relevant features  we set the corresponding bit in ~s and proceed with the active learning as in sec. 1. fig. 1 shows the performance of the hil experiments. as before we report deficiency  f1 and f1. as a baseline we also report results for the case when the top 1 features as obtained by the information gain oracle are input to the simulated hil experiments  this represents what a user with 1% precision and recall would obtain by our method . the oracle is  as expected  much better than plain uncertainty sampling  on all 1 measures  reinforcing our faith in the algorithm of sec. 1. the performance of the hil experiments is almost as good as the oracle  indicating that user input  although noisy  can help improve performance significantly. the only relative poor performance for the hil simulation is on the average d1 measure for the tdt categories  where we used all the words from the topic descriptions as a proxy for explicit human feedback on features. the plot on the right is of f1t hil  for hurricane mitch. as a comparison f1t act  is shown. the hil values are much higher than for plain uncertainty sampling.
　we also observed that relevant features were usually spotted in very early iterations. for the auto vs motorcycles problem  the user has been asked to label 1%  averagedovermultiple iterations and multiple users  of the oracle features at some point or the other. the most informative words  as determined by the oracle  - car and bike are asked of the user in very early iterations. the label for car is always  1% of the times  asked  and 1% of the time the label for this word is asked of the user in the first iteration itself. this is closely followed by the word bike which the user is queried on within the first 1 iterations 1% of the time. most relevant features are queried within 1 iterations which makes us believe that we can stop feature level feedback in 1 iterations or so. when to stop asking questions on both features and documents and switch entirely to documents remains an area for future work.
1 related work
our work is related to a number of areas including query learning  active learning  use of  prior knowledgeand feature selection in machine learning  term-relevance feedback in information retrieval  and human-computer interaction  from which we can cite only a few.
　our proposed method is an instance of query-based learning and an extension of standard   pool-based   active learning which focuses on selective sampling of instances  from a pool of unlabeled data  alone  cohn et al.  1 . although query-based learning can be very powerful in theory  angluin  1   arbitrary queries may be difficult to answer in practice  baum and lang  1   hence the popularity of pool-based methods  and the motivation for studying the effectiveness and ease of predictive feature identification by humans in our application area. that human prior knowledge can accelerate learning has been investigated by  pazzani and kibler  1   but our work differs in techniques  they use prior knowledge to generate horn-clause rules  and applications.  beineke et al.  1  uses human prior knowledge of co-occurrence of words to improve classification of product reviews. none of this work  however  considers the use of prior knowledge in the active learning setting. our work is unique in the field of active learning as we extend the query model to include feature as well as document level feedback. our study of the human factors  such as quality of feedback and costs  is also a major differentiating theme between our work and previous work in incorporating prior knowledge which did not address this issue  or might have assumed experts in machine learning taking a role in training the system  schapire et al.  1; wu and srihari  1; godbole et al.  1 . we only assume knowledge about the topic of interest. our algorithmic techniques and the studied modes of interaction differ and are worth further comparison.
　in both  wu and srihari  1; schapire et al.  1   prior knowledge is given at the outset which leads to a  soft  labeling of the labeled or unlabeled data that is incorporated into training via modified boosting or svm training. however  in our scheme the user is labeling documents and features simultaneously. we expect that our proposed interactive mode has an advantage over requesting prior knowledge from the outset  as it may be easier for the user to identify/recall relevant features while labeling documents in the collection and being presented with candidate features. the work of  godbole et al.  1  puts more emphasis on system issues and focuses on multi-class training rather than a careful analysis of effects of feature selection and human efficacy. their proposed method is attractive in that it treats features as single term documents that can be labeled by humans  but they also study labeling features before documents  and only in an  oracle  setting  i.e.  not using actual human annotators   and do not observe much improvements using their particular method over standard active learning in the single domain  reuters  they test on.
1 conclusions and future work
we showed experimentally that for learning with few labeled examples  good feature selection is extremely useful. as the number of examples increases  the vocabulary  feature set size  of the system also needs to increase. a teacher  who is not knowledgeable in machine learning  can help accelerate training the system in this early stage  by pointing out potentially important words. we also conducteda user study to see how well naive users performed as compared to a feature oracle. we used our users' outputs in realistic human in the loop experiments and found significant increase in performance.

datasetd1f1f1actoraclehilactoraclehilactoraclehilbaseball111111111earnings111111111auto vs motor111111111hurr. mitch111111111talk.politics.mideast111111111avg tdt performance111111111figure 1: improvement in deficiency due to human feature selection. numbers for hil are averaged over users. the graph shows human feature selection for hurricane mitch with the x-axis being the number of labeled documents and y-axis f1 hil ; the difference between these two curves is summarized by the deficiency score. the f1 and f1 scores show the points on the two curves where 1 and 1 documents have been labeled with active learning. the difference between no feature feedback and human-labeled features is greatest with few documents labeled  but persists up to 1 documents labeled.this paper raises the question of what questions  other than questions about the labels of instances  an active learner can ask a user about the domain in order to learn as quickly as possible. in our case  the learner asked the teacher queries on the relevancy of words in addition to the labels of documents. both types of questions were easy for the teacher to understand. our subjects did indeed find marking words without context a little hard  and suggested that context might have helped. we intend to conduct a user study  to see what users can perceive easily  and to incorporate these into learning algorithms.
acknowledgments
this work was supported in part by the center for intelligent information retrieval and in part by spawarsyscen-sd grant number n1-1. any opinions  findings and conclusions or recommendations expressed in this material are the author s  and do not necessarily reflect those of the sponsor. we would also like to thank our users who voluntarily labeled data.
