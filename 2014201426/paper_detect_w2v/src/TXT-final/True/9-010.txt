 
     since 1 there has again been great interest in chess programming. this paper demonstrates that the structure of today's most successful programs cannot be extended to play master level chess. certain basic requirements of a master player's performance are shown to be outside the performance limits to which a program of this type could be extended. the paper also examines a basic weakness in the tree-searching model approach when applied to situations that cannot be searched to completion. this is the horizon 
effect  which causes unpredictable evaluation errors due to an interaction between the static evaluation function and the rules for search termination. the outline of a model of chess playing that avoids the horizon effect and appears extendable to play master level chess is presented  together with some results already achieved. 
keywords: game playing  chess programming  heuristic programming  horizon effect. 
introduction 
     since 1 chess programming has again been an interesting subject for artificial intelligence researchers. at that time greenblatt  developed a program which soon proved to play nearly as well as the mid-range of registered human players in america. 
     the basic structure of greenblatt's program can be historically traced through what shannon  in 
1 called a type b strategy  an initial effort by bernstein et. al.  in 1  another more successful program documented in kotok  1  and finally greenblatt. since then there have been several other programs of this type developed. of these only the northwestern university program  has achieved a human class c rating  which places it in a 
unique position along with the greenblatt program. 
     briefly  the type of program being discussed searches to a depth of five ply under tournament conditions  in endgames with very few legal moves they may search from one to three ply deeper . they do not investigate every legal move  but restrict themselves to a sub-set at each node. moves are selected for further search on the basis of a scoring function which attempts to rank order the legal moves according to their goodness. the number of moves selected for further examination is usually a function of the depth at which the node occurs. in special situations there are mechanisms which allow the search to be expanded beyond this pruned set  in order to attempt to meet unanticipated problems. 
* this work was supported by the advanced research projects agency  department of defence  under contract number f1-c-1. 
     programs of this type typically evaluate between 1 and 1 bottom nodes in the trees they generate. they do this by applying a static evaluation function  which  since it must be invoked so many times in the course of the three minutes allowed for a tournament move  must of necessity be restricted to a few milliseconds of computation. 
     because programs exist which can compete in human tournaments  and because of the annual computer competition et the acm meeting  there now exists considerable interest all over the world in the future of computer chess. in fact  there is a detectable expectancy in the al community that a master level chess program will exist before the end of the decade. 
     it is the purpose of this paper to show that a program with the structure of today's most successful programs cannot be extended to play master chess. when one considers that much effort by several groups has only succeeded in raising the 1 standard of performance of the greenblatt program by a hardly measurable amount  there is some reason to believe that the present design is already near the assymptote of its potential. we will show that the domain in which a master level program has to operate could never be subsumed in any domain to which the above programs could be extended. we will also show that certain features in the evaluation procedure used by current programs lead to basic errors that can not be tolerated in master play. finally  the outline of a model of chess that could perform as required is presented. 
     we will draw examples from both human and machine play. in order to minimize the chess knowledge required of the reader  the examples have been chosen to be as obvious as possible  and we have endeavored to remove all considerations from these  except those pertinent to the discussion. we consider it extremely likely that the phenomena being considered here  are also artifacts in other types of artificial intelligence programs. 
analysis of evidence 
     when branches in a tree search must be terminated prior to a legal termination point  according to the rules of the game   it is necessary to assign a value  an interim value other than win  lose  or draw  to the terminal node  which then allows comparison with other terminal nodes. this is usually done by invoking a static evaluation function. in games where a search to legal termination is not possible  no other recourse appears possible. an interesting phenomenon arises from the interaction of the artificial termination of the search and the fact that all the terms in the static evaluation function are evaluated at this terminal point. the result of this combination is that for the game playing program  reality exists in terms of the output of the static evaluation function  and anything that is not detectable at evaluation time does not exist as far as 
1 
the program is concerned. this interesting fact is present in all tree searches in any chess program that we know of  and causes interesting aberrations in program behavior. 
　　the class of aberrations defined above  we call the horizon effect. examples of the the horizon effect have been observed by several researchers  1 1  in game playing programs. however the complete phenomemon has never received a name in the literature nor has its causes and effects been properly cataloged. the regimen of insisting on evaluation at a prespecified point in the search causes the following effects which seem peculiar to human observers. when the horizon effect results in creating diversions which ineffectively delay an unavoidable consequence or make an unachievable one appear achievable  we call it an instance of the negative horizon effect. this is the phenomenon previously reported in the literature. it can best be shown by a typical example. 
　　in figure i it is white's turn to play  and for the sake of this example let us suppose the search is to be limited to three ply  we realize that the search usually goes deeper  however it is relatively easy to construct examples at any given depth  and we are choosing our examples for their expository simplicity . what will happen in the above position is that the program will try to play 1. b-n1 and after p-b1  1. anything  it is time to do a static evaluation. this usually consists of a material quiescence analysis  together with a calculation of the other coefficients of the evaluation function. the material quiescence analysis could consist of trying all capture sequences and accepting the minimax value if it is an improvement for the side starting the sequence. other quiescence procedures are also possible  but in essence they should yield the same value. now at the end of the above 1-ply sequence  the program will come to the conclusion that it will lose the bishop on n1  and will continue its search for something better. it will eventually come upon 1. p-k1 and recognize that if now pxb  then 1. pxn is good for white. therefore it will consider as best for black to play pxp  after which white plays 1. b-n1. since we are now at maximum depth  this position will be evaluated using the standard procedure. the quiescence analysis will show that white has saved his bishop since there is no sequence of captures which will win the bishop. alas  it is only after the next move that the program finds out that the pon-caoture threat of p-b1 has not been met by this diversion  and it then looks for other ways of parting with material worth less than a bishop in order to postpone the inevitable day when the bishop will finally be trapped and captured. in this case 1. rxb would no doubt be tried next since after nxb  1. b-n1   saving  the bishop by giving up the rook for the black bishop is preferred to losing it. we have seen programs indulge in this type of folly for five to six successive moves  resulting in going from a position in which they are well ahead to one in which they are hopelessly behind. 
　　a clever device to prevent this behavior was invented by greeenblatt and is also used by the northwestern university group  1 . this consists essentially of extending a new principal variation another two ply  to see whether the reason it was considered superior  will continue to obtain. in the above example  this will result in finding that the 

figure 1 
white to play 

threat of p-b1 does not go away  and thus a potential sequence of blunders is averted. however extending a principal variation two ply can only discover whether a one move threat has or has not been dissapated. threats requiring two or more moves can not be dealt with effectively in this manner. this is usually not noticed  since today's best programs perform at a 
level of skill where two move threats are rare and far from the major cause of  concern for their developers. 
　　the positive horizon effect is different in that instead of trying to avert some unavoidable consequences  the program grabs much too soon at a consequence that can be imposed on an opponent at leisure  frequently in a more effective form. this phenomenon has been largely overlooked in the literature  but is reported in a previous paper . figure ii shows a flagrant example of the positive horizon effect. 
　　in this position it is white to play and the search is again to a depth of three ply. the program notices that it can play 1. p-q1 and if black does not now play nxp  1. nxn  then it would get a new queen. it sees that in this way it can increase it's material superiority. it may or may not notice that it will then have to face the formidable task of mating with a bishop and knight. the interesting thing about this position is that the maneuver i. b-k1 followed by 1. b-q1 cannot be prevented and results in forcing the pawn through to a queen without letting black give up the knight for it  thus simplifying the win greatly. here the important point is that there is a consequence on the horizon  and the program insists on realizing it within the horizon of the search as otherwise it  does not exist. as a result  a 
　　consequence which could have turned out to be very beneficial  turns out to only have a small benefit. 
it is possible to find examples where wins are  thrown away  by such a premature cashing in. in fact  the main reason for the demonstrated lack of tolerance of complexity of today's chess programs is that their evaluation function insists on maximizing  in terms of a preconceived set of evaluation terms  anything that it detects within the search horizon  and thus all too frequently destroys an advantageous situation before it really has a chance to bloom. 
     an example of the positive horizon effect illustrating a throwing away of a positional advantage is shown in figure iii. here  if the evaluation function is aware of the beneficial effect of controlling an open file  and if the search is again being conducted to three ply  the most likely continuation will be 1. pxp ch  pxp  1. r-kr1 with control of the open file and  some advantage . the fact that on the next move black can answer r-kr1  after which white's advantage has largely evaporated is not recognized. neither is the key fact that black can do absolutely nothing to prevent white from opening the file whenever he likes   for human players there is the dictum  do not open a file until you are ready to use it  . however today's programs would almost certainly reject the correct 1. r-kr1 since after black plays r-kr1 and white plays 1. pxpch  it is time to invoke the quiescence procedure which produces pxp. now in contrast to the earlier variation  white does not control the open file. nor would he if any other 1nd move were played. clearly  a program could recognize the value of playing 1. r-kr  before j. pxp ch  only if it were secure in the knowledge that the file can be opened at a later time by pxp and that if black plays pxp  he will merely incur an equally difficult problem in defence of the krp as he has now in defence of the kr-file. in fact having once played i. r-kr1 and getting the answer r-kr1  a program that has reasoned thus far should have little difficulty in now playing 1. r-r1 since opening the file at the present moment is not advantageous and making room for the other rook could help. it should be noted that incorporating the human players' dictum appears extremely difficult as the issue of  ready to use it  is one requiring dynamic judgements  in which even good human players make mistakes at times. however  we feel that the construction of lemmas as explained in a later section  will go a long way toward providing better evaluation mechanisms at the search horizon. 
     another basic problem  the need for a global strategy  is shown in figure !v. here everyone of today's programs would conduct a 1-ply search and then play 1. k-k1. a summary of its findings during this tree search might run a follows: it decided that p-b1 would lose the pawn to k-k1  and therefore decided to move the king to the most 'central location available  this is a quantity recognized by the evaluation function . on the next move  having already achieved its  optimum  position  the program would be faced with a problem that all hill climbers face when they reach the top*. how to back down as little as possible  accordingly there would occur either k-q1 or k-b1. the point of this whole example is to show the hopeless hill-climbing characteristics of the present program design. in the given position  even a poor human player would recognize that there is nothing to be gained by the above maneuvers. the real problem is that today's programs mix their strategical and tactical objectives during the search. thus the above position could be handled effectively if a tactical 

search were first done and this came to the conclusion that p-b1 only resulted in losing the pawn. there being no other tactical tries  control would then revert to a strategical module which would try to improve the position of any and all pieces. since  in this simplified situation  we only have the king as a candidate  the next step would be to try to find an optimum or near optimum position for the king and determine if it could get there. here we must not rely solely on a static  preconceived notion of centrality  although that certainly is a part of the picture  but more importantly we seek a functional optimum. this can be found by noting that the black knp and kp are not defended by pawns and could possibly be attacked by the king  and also that our own kbp could possibly benefit from having our king near it. next  a null move analysis could be carried out  consisting of moving the white king around without looking at intervening moves  to see if we can find the optimum path to any of the desired squares. this will then eventually yield the correct idea of infiltrating with the white king via qr1  which wins easily. admittedly the control structure that could evoke such behavior would present some problems. most of the problems in chess are tactical  immediate  problems and for this reason  the lack of global ideas is frequently obscured in today's programs. however  it is absolutely necessary to be able to generate global goals in order to avoid hill climbing behavior. 
     we have above touched only on the relatively simple problem of finding the correct way to proceed. a far more difficult problem  which would also have to be faced by the master strength program  is to judge whether the position can be won or is a draw. a simple  pawn ahead  judgement is not enough. there may be other endgames from which to choose  in which the program is also a pawn ahead. in the position being discussed  for instance  if a further white pawn were at qn1  and a black pawn at its qn1  the position would be a draw. clearly dynamic judgements of this type are absolutely necessary. 
     in figure v  we see a much better understood problem than any of the above. it is the problem of calculating in depth. here white can execute a mating combination requiring an initial queen sacrifice and nine further moves  a total o  1 ply as follows: 
1. 	q-r1ch  	nxq  	1. 	pxpch  	k-n1  	1. 	b-b1ch  	k-n1  
1. r-b1ch  k-n1  1. r-b1ch  k-n1  1. r-n1ch  k-r1  1. r-k1ch  n-b1  1. rxnch  k-r1  1. p-n1  any  1. r-r1 mate. this combination was played by a former world champion while playing a total of 1 games simultaneously. the reason no program that looks at 1 to 1 alternatives at every node can play the correct move is that the principal variation to justify the initial queen sacrifice extends much  much further than the 1-ply depth that is about all that is possible with a program that gets buried in the exponential explosion of investigating 1 sprouts from every node. now it is quite possible to play master level chess without playing such long combinations. however  in the author's experience one must at least once a game be able to took 1 or more ply ahead. as for as the above example goes  we believe that 1 of all masters would solve it as well as a high percentage of experts and class a players. what is really difficult about the example is not the simple unravelling of the main line  which having few branches is fairly linear  but the conception of the position  and that such a solution involving chasing the king up the board might exist in it. 
     one could argue that just because good players can solve such problems  this does not show the requirement for the program to see to such depths in order to play at the master level. what this would mean is that the program would have to rely almost exclusively on static  non-tree-search computations for its moves. but we have already shown in examples ii and iii that static notions must be combined with dynamic tests in depth in order to yield correct results. so a program that could not look 1 ply ahead would be subject to any five move threat that comes along. even though the main thrust of most such threats could no doubt be muted  it would be inevitable that some concession would have to be made. this type of thrust and parry is at the heart of 
master play. even more importantly  a program that cannot look 1 ply ahead could never conceive a five move threat of its own which is dependent on adverse action. the evidence is quite overwhelming. 
     another interesting phenomenon  that of reality or illusion  that afflicts all of today's best programs can be seen in figure vi. here it is white's turn to play. the first thing that the evaluation function will discover is that white has both of his rooks  en prise  {captureble by the opponent under f avor able conditions . if this position has occured at some node which is eligible for sprouting  then moves that move either of the rooks to a  safer  place will receive good recommendations. if the node is a terminal node  then it will be considered as not satisfactory for white  as it is presumed that at least one of the rooks will be lost. in actuality  neither of the rooks is in danger. if black plays qxr then r-b1 mate  and if pxr  then qxpch  k-nl  q-r1ch  k-b1  p-n1ch followed by p-n1-qch wins quickly- even stranger is the fact that if this position occurs 

somewhere in the tree below the top node  and if  say  two ply earlier white had played rxp kb1  as a sacrifice which it turns out could not have been accepted  then in today's programs there would now be no knowledge of the sacrifice at kb1 when the position is tendered for evaluation two ply later. rather the rook would be considered en prise. indirect defences of this type are seen all the time in master chess. clearly  if a program aspires to this level it must be able to handle such problems. part of the solution consists in noting the functional overloading of the pieces that are thought of as doing the   capturing. here the black queen is guarding a check on the back rank apart from attacking the white rook. also the black knp is guarding a pawn and a check  while attacking  the white rook. however this is not enough  since it is quite possible that the checks that are being defended against are quite harmless  and it would be folly to try to determine  without further searching  the exact potency of every check on the board. 
     another problem  that of dynamic evaluation of material  is depicted in figure vii. here with either side to play  white's pawn cannot be stopped from queening  while black's pawns are going nowhere fast. yet there is no doubt that every one of today's programs  if playing black would refuse a draw in this position  and it is also very clear that only a very weak human player would offer a draw with white. the programs' rationale is that three passed pawns are better than one. the problem here is one of recognizing the dynamic potential of the white passed pawn which cannot be caught. it is true that in this case the job can be done statically be merely noting the distances of the white pawn and the black king from the queening square  however  if the black pawns were ell advanced three squares  the computation would have to be done dynamically  since there is a possibility they may arrive first. similar dynamic ideas  which no program can at present handle well  ere the notion of a defenceless king by reason of no surrounding men of his own to help defend him  and the notion of cooperation among various men rather than only assessing the goodness of their individual positions. such notions require dynamic exploration to determine the degree of their applicability in a given position. however  in a program where terminal evaluation must be done very quickly because of the large number of nodes that must be evaluated  such luxuries are not possible. we are here directly confronted with a basic limitation of the generate and test approach  when it does not allow enough time to do a detailed evaluation of the nodes visited. 
     our last two examples deal with situations that present-day programs can handle. however  the method by which they do this is terribly inefficient and could not be used if one wanted to do tree searches which could extend even a little deeper than the current five ply. the first of these problems is the problem of defence. it is relatively easy to recognize attacks and develop criteria for judging the value of most attacks. however  this is not so with defence. the problem is that in order for a defence to exist  a threat must first be known. all threats are not of the simple type such as threatening a capture  and it is precisely this other type of threat  which shows up in the backed up value of the current variation  that is not easy to counter because we only know the magnitude of its effect. figure viii shows a position of this type. here it is black's turn to play and the search is being conducted to a depth of five ply. if black plays a normal aggressive move such as 1.- p-r1  he will find that after 1. q-k1 ch  rxq  1. rxr he is mated. the search will then eventually revert to the point where black played p-r1. now in most of today's programs we would be armed with the killer heuristic  which says that against any new proposed move try the  killer  q-k1ch first . this would indeed result in the efficient dismissal of the next 1 or so moves likely to be tested. however the fact remains that each of these alternatives is being served up in a generate and test mode  and the program can consider itself fortunate if it discovers the only defence  q-k1  before it has 
exhausted half the legal moves. 
     our final example in figure ix shows ' another subtle consideration. in this position  programs that look five ply deep have an excellent chance of finding the mate in three moves; 1. bxpch  k-r1  1. b-n1ch  k - n l   1  q-r1 mate. if such a program  due to the fact that white is behind in material  were only to look at captures of pieces of greater or equal value to the current deficit  and checks  an assumption which requires some preprocessing  and to stop at five ply depth  for which it would be difficult to establish a logical reason   there would still be about 1 bottom positions to examine before the mate is found. here any tournament caliber human player would recognize the situation immediately as one of a set of queen and bishop mates. he would only have to determine the functional need to guard the king escape square at kb1  to determine what the correct sequence is and that it does lead to mate. the critical thing here is not that a program couldn't find the mate once the diagrammed position is reached  but that in advanced chess play such situations occur frequently 

figure vii 

figure viii 
black to play 

figure ix 
white to play 
in deep parts of a tree as a reason why some other move fads. if a program spends 1 nodes investigating such a well known pattern  then there is a definite limit on the amount of work the program can be expected to do. the answer here quite obviously is to have a repetoire of frequently occurring patterns available to the program together with some guidance to determine the exact applicability of any particular pattern. in the above case  recognition of the queen and bishop functionally bearing on the undefended kr1 square  together with the position of the black king hemmed in by some of its own pieces is the basic pattern. the dynamic analysis reveals that the king could escape over kb1 if this were not kept under continued guard. with these constraints  the number of variations to be examined are very few. 
gome conclusions 
　　let us examine some potential models of computer chess. all the complete models are clearly too time 

or space consuming. therefore  the most reasonable of 1-ply and possibly more. this in no way course appears to be to rely upon models that means that every move should be calculated to 
construct   trees of possibilities but with some this depth nor that when a move is  that every limitations imposed upon the growth of the trees. now branch would go to this depth also. however  the depending upon how we define these limits  we have a basic facility to allow probing to at least this tractable problem. the real question  and that depth must exist. addressed by this paper  is how these limits can be 
defined and implemented in order to include the range 1  from example vi we see the need to diagnose of performance exhibited by chess masters while still certain dynamic properties of positions  and the keeping the problem tractable  requirement to communicate such data to other 
let us summarize the requirements noted earlier: nodes in the tree. this need exists in order to avoid faulty interpretation and the necessity of 
1  in examples l-lll we have seen the horizon effect in operation. we have also seen that the two-ply extension of each new principal variation is only a stop-gap measure  which prevents one move 
debacles  anyone who doesn't believe this is invited to try figure xi out on his program . otherwise again. 
when 
statically 
case 	that discovers  discovering 	america  	over 	and 	over 
conditions are detected that make a calculated decision incorrect  in this a rook is en prise   the variation that this fact must during tree back-up what can be done about the horizon effect  assemble the conditions which are necessary for clearly the problem is due to the fact that some this contradiction to remain true  this new term in the static evaluation function is truth should then be stored as a lemma applicable evaluated  prematurely . prematurely here means to all nodes in the tree below any node for which that a noticable' change in the value ot the term it is discovered to be true. the lemma states can be forced  without any compensatory change in the conditions under which it remains true  and any other term s . from this  one can deduce at each succeeding node it is assumed true unless that there can be no arbitrary depth limit something that counters one or more of the imposed on the search. the decision as to conditions of the lemma has occurred. in that whether to terminate the search at a node or case the validity of the lemma would have to be continue  has to be a function of the information re-examined. it is important to express the that exists at that node and how this relates to lemma in a language which is neither too detailed the quiescence of each and every term in the nor too fuzzy. the former case would result in evaluation function. for instance  if we have an continuous re-examination as changes in single evaluation function that would consider it bad to conditions  which by themselves do not upset the have a bishop blocked in by its own pawns  then validity of the lemma  have to be looked into. some effort must be expended to determine the the latter case could result in being unable to permanency of such situations. in general what define or detect a critical change in the truth is required is a procedure to determine the value of the lemma. it should of course be 
quiescence of every term in the evaluation apparent that lemmas can concern themselves with function and in cases of non-quiescence  a other things than the material issues presented procedure for generating moves or applying some in the example. for instance  it should be static means of reaching a quiescence decision. possible to posit a lemma about the conditions this should not be construed as meaning that under which control of an open file is retained. 
perfect knowledge of the future status of each it is interesting to note that a key to parameter in the evaluation function is required  detecting that something may not be as it appears 
in fact some practical maximum depth or time statically  is the use of a functional analysis. limits must exist. thus only a finite set of in example vi  the initial indication that resourses can be expended to determine the true neither of the rooks is capturable is that each 
future status  and some controlled error will no of their attackers is also defending something of doubt neve to be tolerated. however  the importance. sometimes it is possible to resolve resulting error by this method should be orders such function conflicts statically by noting that 
of magnitude smaller  a -so-called judgement another piece can assume the required functional error  than the errors produced currently by the role without itself becoming overburdened. when horizon effect. in present day programs  this is not possible  the validity of a potential quiescence is pursued only for the material function conflict must be established dynamically parameter. and even this frequently does not by tree searching. a general discussion of the work out satisfactorily  since usually only use of functional relations in chess perception captures are considered  while forks  mate and reasoning can be found in newell and simon threats  etc. are ignored. . a good discussion of paths and their 
obstruction can be found in botwinnik. 
1  from example iv we see the need for having global goals and being able to determine 1  from the defence problem in example viii  we see something about the feasibility of such proposed a need for some similar communication within a goals. this may involve procedures of search tree. a proper description of a set of considerable complexity in order to answer basic undesirable consequences can save tremendous questions about the value of any node. all of effort in finding problem solutions if such which adds to the potential evaluation time exist  or moving on to more fruitful endeavors if required at a node. not. again  the adequacy of the language is important as it must be used to test whether the 
1  from example v  the program must on occasion be set of consequences were caused by the latest able to calculate precise variations to a depth move  and to provide an input to move generators 
1 

that could find an appropriate answer to the problem. for this purpose  functional relations which describe attacks that occured  and path information which describes paths traversed by moving pieces and paths over which threats occurred  appear to be required elements of the language. 
1  the functional relations mentioned in the previous examples are in a sense patterns involving two pieces or a piece and a square. certain clues can be gained by searching these patterns when they focus about a common square or piece. however  from example ix we can see the need for a still higher level of pattern abstraction. here we are looking for groups of pieces which form a pattern around some interesting focus. in the example cited  the kr1 square with the white queen and bishop attacking it  and the black king are the focal points which should suffice to index into the correct pattern  which will then produce a pointer to a routine for deciding if we are confronted with an exploitable instance of the pattern in question. 
     above  we have assembled the beginning of a set of requirements for a program that could have the power to play master level chess. it does not take long to dismiss the possibility of extending the current generation of chess program to meet the above requirements. it is quite enough to realize that such a program requires about a factor of 1 of additional time for each additional two ply of depth that it searches. 
     in 1 newell  simon  and shaw   argued that  as analysis deepens  greater computing effort per position soon pays tor itself  since it slows the growth in number of positions to be considered . this is well substantiated in the acm tournaments which have convincingly shown the superiority of programs that search a subset of legal moves and evaluate a moderate amount  over programs that search all legal moves and evaluate little. clearly it is time to move again  and more substantially in the direction of more evaluation and less search. the requirements demonstrated in this paper show a need to do possibly ten or more times as much processing at a node than is currently done. this means that  for equivalent computing power  we are faced with generating trees of at most 1 nodes distributed throughout the search space. the greenblatt and northwestern university programs have an effective branching factor  where number of bottom nodes - bf kr h  in excess of 1. if it is assumed that the search is limited to 1-ply  then the branching factor must be less than 1  if we are to stay within 1 nodes. 
     actually this is a meaningful measure only for trees which have a maximum depth. in order for a tree of no maximum depth to converge  a necessary and sufficient condition is that for any arbitrary node 1   i p  i     1  where p  i  is the probability of i sprouts . clearly the less i   i p  ij  is  the more rapidly the tree will converge. one can achieve such a decrease  with increasing depth of the tree  by being able to compare the state at the present node with the states of earlier nodes in the tree branch being investigated. comparisons involve how earlier expectations are holding up  and whether moves that are eligible for testing have appropriate thematic relationships to what has gone before. the number of such comparisons grows linearly with depth thus providing ever more conditions for stopping the search or not investigating an arbitrary move. 
     to guide the search we need mechanisms which can at linear cost provide analysis at a node so that the exponential cost of discovery and/or verification due to tree searching is drastically reduced. it appears reasonable that the more powerful  in the sense of greater depth  the prediction mechanism  the better the effect on program performance. here the functional analysis and pattern recognition mentioned earlier clearly are destined to play a part  with the former being an essential element of the latter. also the communication of defensive requirements appears vitally necessary. in fact since dissatisfaction with a result is a relative matter  one could consider using backed-up descriptions to discover ways of heightening the success of whatever is being attempted at present. 
     lastly  one can see the overriding importance of quiescence of concepts being used in the evaluation procedure. the evidence is quite overwhelming that the attempt to drive all evaluations into a quiescent state should be the major force that determines the shape of the tree. thus  while today's programs use up nearly all their time trying to assure tactical quiescence  this will now have to be done by less complete methods in order to make way for the additional facilities required. it is interesting to compare this derived role of quiescence as the main guiding force  with the control structure of the 1 newell  simon and shaw program  which was apparently derived from a concern with human behavior. 
     the conditions and the model we have set forth appear to be necessary for master chess. however  they are almost certainly not sufficient. masters know a great deal of chess knowledge which has as yet not been encoded in any program  and would probably have to be placed in long-term memory for occasional reference  we have avoided discussing what a minimum quantity of such data might look like  since until the necessary mechanisms for its use are in place  so that it would be possible to do some experimentation  there would be little scientific validity in such speculation. there is also the problem of doing at least some learning in order to avoid repeating obvious errors in identical situations. however  an organization ' which takes account of the conditions noted here is almost certainly necessary to make significant progress beyond the present state of the art  and the model appears extendable to the problems of learning and further pattern encodings  as these prove necessary. in the immediate future  the major problem appears to be how to produce a search of the economy of that proposed while retaining at least the same reliability as evidenced by today's programs which use a more complete search strategy. 
     for those who feel that our simple examples would not challenge their program design  we include figures x-xii which are moderately more difficult. in figure 
x it is white to play and win. in figure xi it is black to play and not do something foolish; and we are not thinking of the obviously foolish 1. qxr  but of other foolishness derived from this by the horizon effect. in figure xii it is white to play and win. we expect that a human class b player would have no difficulty with any of these. 
progress to date 
　　since july 1 the author has had a program  whose general objectives are those outlined above  running at carnegie-mellon university. in this section we wish to report the progress that has been made. as yet the program only concerns itself with tactical {material  issues; however  it is felt that the techniques being developed are general and applicable to the other dimensions of chess evaluation. the program notes  among others  the relations of attack  defence  blocking functions and paths along which such activities occur. thus  it has a language which is considerably more powerful than the notions of a legal move  which is the analytical element of most of today's programs. each chessboard is described in this language. this characterization greatly facilitates finding good  sacrificial  moves and moves which disturb the defence relations among pieces  while not hindering the evaluation of other types of moves. the program has an expectation level associated with every search it conducts  and there are mechanisms for raising and lowering this when results outside a range of acceptability from the expectation can be forced. there is a general causality facility which is used both for defence and improving attacking ideas. this facility can always detect whether a given set of consequences were not caused by the last move tried and thus constitute a problem inherited from higher in the tree. it does this by comparing a description of the consequences with a description of the move tried. however  it is not nearly as adept at detecting causality relations as an experienced chess player. still  the facility never deduces causality when causality does not exist  end deduces non-causality about 1. of the time that this is appropriate  putting it well ahead of contemporary programs. no dynamic re-evaluations or lemmas have been implemented as yet  but these are planned for the near future. higher level patterns are planned for a stilt later stage. 
　　the program is at present able to investigate an average position for tactical quiescence to a depth of 1-ply  generating from 1 to 1 nodes. the tactical reliability of such searches is somewhat better than that of the average program that participates in the annual acm tournaments. most of our effort in tree searching has been in an attempt to improve the likely correctness of a proposed move. this is achieved not only through the representation we use  but also through some 1 stopping  pruning and reordering rules. thus the effective branching factors of our trees vary between 1 for positions where a clear-cut result is obtainable  and 1 in positions where there is tactical complexity  but no way of gaining an advantage exists. although this compares very favorably with the effective branching factors of about 1 found in today's programs  we consider branching factors of 1 to be still much too high for our purposes. thus  we occasionally grow a tree of 1 nodes because the program gets side-tracked in an inconsequential part of the tree. however  there are still several tree control algorithms we plan to implement  which should prevent 
such behavior by selective jumping around in the tree. we expect in the not too distant future to extend the depth limit of the program significantly beyond its present 1-ply. even at its present depth  it is less affected by the horizon effect than programs with less 

figure x 
white to play 

figure xi 
black to play 

figure xii 
white to play 
deep searches. for instance  it has no trouble at all in figure xi  deciding that there are no diversions which can lead to an ultimate capture of the white rook. 
　　when the program is confronted with the defensive problem of figure vlll  after the initial variation starting with 1.- p-r1 is tried  a description of what happened is backed up along with the value of the position at branch termination. at each successive node during backup  if the result is unsatisfactory  the backed up description is compared with the description of the move made. this is done to see if what happened could have been caused by the latest move tried at that node. if not  a defensive move generator then generates the moves that can do something about the description. the description contains such information as the names of the pieces involved in the action  the path  if any  that they traversed  targets attacked when they arrived at their 
destinations  etc. if no defensive moves are generated and if no worthwhile aggressive moves remain to be investigated  it is assumed that the problem cannot be solved at this node of the tree. when the search reverts to the point where 1.- p-r1 was tried initially  an examination of the description reveals that the threat could possibly be countered by moving the king  guarding the k1 square  blocking the path of the rook from k1 to k1  capturing the rook or the queen  or blocking or getting ready to block the kb1 square across which the check passes. from this description six moves are generated: k-rl  n-q1  r-kb1  q-k1  q-k1ch  and q-k1. it then does not take the program very long to determine that q-k1 is the only defence  which in fact leaves black in a winning position . 
     the program has been tested extensively on positions which one finds in chess books which teach how to play good tactical chess. in such positions a more or less  hidden  series of moves allows the player on move to achieve a decisive advantage. in working on the first 1 examples in thr book  win at chess    the program solves about 1- of the middle game problems in cpu times ranging from a fraction of a second to eight minutes  with an average of about 1 seconds. this is despite the fact that the principal variation sometimes extends to nine ply. occasionally the program finds forced mates in the absolute minimum number of nodes. it also has found improvements on the analysis contained in the book  which the experimenter himself did not notice. the main reason that the program is not able to solve 1% of these problems is that several functional relations and many analytical routines are not yet implemented. 
     on the basis of tests to date  we find that the language of functions  paths  etc. is very useful for passing messages of the kind described above. we expect that it will also serve for the characterization of thematic relationships which would mitigate against a move being tried when it is  out of context  with previous moves. however  there seems to be little doubt that more powerful languages are possible. these may be necessary to effectively implement the lemma ideas  since the function language may be so detailed that investigating lemmas expressed in it will prove uneconomical. further  we conjecture that continuing progress in chess will be dependent on the invention of ever higher level languages in which chess concepts can be expressed. each such language level would then have an assymptote  defined by the power of the language  beyond which it would not be possible to improve the strength of the program  given that only a certain amount of time was available to compute a move. also as concepts are agglomerated into ever higher level concepts  we expect that they would get to be more fuzzy and would require a more complex control structure than used a present in order to produce the same level of reliability as can be obtained with less fuzzy concepts. we feel that this increased conceptualization is evident in the history of chess  and should make it possible ultimately to equal and exceed the performance of the best human players. 
acknowledge ment 
     the author wishes to acknowledge the roles of professors a. newell and h. a. simon in stimulating his interest in functional analyses of chess positions. the work reported herein was supervised by dr. newell. 
