 
in this paper we describe a case study in which we applied an approach to qualitative machine learning to induce  from system's behaviour data  a qualitative model of a complex  industrially relevant mechanical system  a car wheel suspension system . the induced qualitative model enables nice causal interpretation of the relations in the modelled system. moreover  we also show that the qualitative model can be used to guide the quantitative modelling process leading to numerical predictions that may be considerably more accurate than those obtained by state-of-the-art numerical modelling methods. this idea of combining qualitative and quantitative machine learning for system identification is in this paper carried out in two stages:  1  induction of qualitative constraints from system's behaviour data  and  1  induction of a numerical regression function that both respects the qualitative constraints and fits the training data numerically. we call this approach q1 learning  which stands for qualitatively faithful quantitative learning. 
1 introduction 
it is generally accepted that qualitative models are easier to understand and reason about than quantitative models. qualitative models thus provide a better basis for the explanation of phenomena in a modelled system than numerical models. in this paper we describe a case study in which we applied an approach to qualitative machine learning to induce  from system's behaviour data  a qualitative model of a complex  industrially relevant mechanical system  a car wheel suspension system . the induced qualitative model enables nice causal interpretation of the relations among the variables in the system. this is precisely as one would expect from a qualitative model. more surprisingly  however  we also show in this case study that the qualitative model can be used to guide the quantitative modelling process that may lead to numerical predictions that are considerably more accurate than those provided by state-of-the-art numerical modelling methods. 
　thus the main message of this paper is that a combination of methods for qualitative and quantitative system identification has good chances to attain significant improvements over numerical system identification techniques  including techniques of numerical machine learning methods  such as regression trees  breiman et ai  1  and model trees  quinlan  1 . the potential improvements are in two respects: first  the predictions are qualitatively consistent with the properties of the modelled system  and in addition they are also numerically more accurate. 
　this idea of combining qualitative and quantitative machine learning for system identification is in this paper carried out in two stages. first  induce qualitative constraints from system's behaviour data  training data  with program quin  overviewed in section 1 . second  induce a numerical regression function that both respects the qualitative constraints and fits well the training data numerically  called qualitative to quantitative transformation or q1q for short  described in section 1 . we call this approach q1 learning  which stands for qualitatively faithful quantitative learning. to underline the importance of qualitative fidelity  we illustrate in section 1 some problems that numerical learners typically have in respect of qualitative consistency. in section 1 we present the case study in applying q1 to the chosen problem of modelling car suspension. 
　there are several approaches to learning qualitative models from numerical data that may support alternative approaches 
to q1 learning. these include the program qmn  dzeroski and todorovski  1   qsi  say and kuru  1   squid  kay et al  1   and qoph  coghill et al  1 . 
1 qualitative difficulties of numerical learning 
1 	qualitative reasoning consider a simple container of cylindrical shape and a drain at the bottom. if we fill the container with water  the water drains out. water level monotonically decreases  until it reaches zero. suppose we fill the container with water  and measure initial outflow and the time behaviour of water level . since this is a rather simple behaviour  one would naturally expect that machine learning techniques should be able to fairly well predict time behaviour of water level if enough learning examples are given. quite surprisingly  even in such simple cases  the usual numerical predictors can give strange and qualitatively unacceptable predictions. we illustrate the problems with a simple experiment  using well-known techniques for numerical prediction: model trees and locally weighted regression. 

figure 1: m1 predictions of water outflow: empty circles are 
m1 predictions of level on a test set with = 1. other dots are the the learning examples. note that m1 predicts that water level increases at time 
　in our experiment we used container outflow simulation data to evaluate how different numerical predictors learn the time behaviour of water level. the outflow from a container is where c is a parameter depending on the 
area ot the drain. for the simulation we used euler integration where seconds and 
　　　we used six example sets  generated with different initial water levels and initial outflows 1. an example set has 1 examples corresponding to 1 seconds of simulation. 
　we used the weka  witten and frank  1j implementations of locally weighted regression latkeson et al  1   lwr  for short   and m1 regression and model trees  quinlan  1  to learn the time behaviour of level given the initial outflow  i.e.  one example set was used as a 
test set and other five sets  1 examples  for learning. when the test set was the set with m1 with the default parameters induced a model tree with 1 leaves. figure 1 shows the learning examples and the m1 prediction of level on a test set. note that m1 predicts that water level increases at time  the same happens if we change the pruning parameter. this is of course qualitatively unacceptable as water level can never increase. also  there are no learning examples where water level increases. 
　one might think that this is an isolated weird case or m1 bug. but it is not. lwr makes a similar qualitative error on the test set with = 1  when it predicts that water level increases at of course  lwr predictions depend on its parameter i.e. the number of neighbors used  but often the appropriate that doesn't give qualitative errors on one container  gives qualitative errors when learning from similar data but with different area of the drain. often  the errors are even more obvious if we make predictions at the edges of the space covered by learning examples  i.e. using as test set 
 = 1 or  = 1. as one would expect  regression trees make similar qualitative errors. 
　we believe that other numerical predictors make similar qualitative errors  at least in more complex domains. this might be quite acceptable in applications where we just want to minimize numerical prediction errors. but often it is also important to respect qualitative relations that are either given figure 1: a qualitative tree induced from a set of examples for the function z = x1 - y1. the rightmost leaf  applying when attributes a' and y are positive  says that z is strictly increasing in its dependence on a' and strictly decreasing in its dependence on}'. 
in advance or hidden in the data. ignoring them results in clearly unrealistic predictions that a domain expert would not approve. the idea of this paper is to use such qualitative relations  either given or induced from data  not only to avoid qualitatively incorrect predictions  but also to improve the accuracy of numerical prediction. 
1 qualitative data mining with quin 
qu1n  qualitative induction  is a learning program that looks for qualitative patterns in numerical data  sue  1; sue and bratko  1; 1 . induction of the so-called qualitative trees is similar to induction of decision trees. the difference is that in decision trees the leaves are labelled with class values  whereas in qualitative trees the leaves are labelled with what we call qualitatively constrained functions. 
　qualitatively constrained functions  qcfs for short  are a kind of monotonicity constraints that are widely used in the field of qualitative reasoning. a simple example of qcf is: y = a/ +  a  . this says that y is a monotonically increasing function of a. in general  qcfs can have more than one argument. for example  z = m +   -   a   y  says that z monotonically increases in a and decreases in y. we say that z is positively related to x and negatively related to y if both a and y increase  then according to this constraint  z may increase  decrease or stay unchanged. in such a case  a qcf cannot make an unambiguous prediction of the qualitative change in z. 
　quin takes as input a set of numerical examples and looks for qualitative patterns in the data. more precisely  quin looks for regions in the data space where monotonicity constraints hold. such a set of qualitative patterns are represented in terms of a qualitative tree. as in decision trees  the internal nodes in a qualitative tree specify conditions that split the attribute space into subspaces. in a qualitative tree  however  each leaf specifies a qcf that holds among the input data that fall into that leaf. for example  consider a set of data points  xy z  where z = x1 - y1 possibly with some noise added. when quin is asked to find in these data qualitative constraints on z as a function of x and y  quin generates the qualitative tree shown in figure 1. this tree partitions the data space into four regions that correspond to the four leaves of the tree. a different qcf applies in each of the leaves. the tree describes how z qualitatively depends on a and y. 
quin constructs a tree in the top-down greedy fashion  

qualitative reasoning 	1 

similarly to decision tree induction algorithms. a more elaborate description of the quin algorithm and its evaluation on a set of artificial domains is given elsewhere  sue  1; sue and bratko  1  . quin has been applied to qualitative reconstruction of human control strategies  sue  1   and to reverse engineer a complex industrial controller for gantry cranes  sue and bratko  1 . 
1 q1q transformation 
in this section we describe the qualitative-to-quantitative transformation  q1q for short . given a set of numerical data and a qualitative tree  q1q attempts to find a regression function that fits the data well numerically  and also respects the qualitative constraints in the tree. we say that such a regression function is qualitatively consistent. in fact  q1q finds a qualitatively consistent regression function with good fit for each leaf in the tree separately. the overall regression function is then obtained by gluing together the regression functions for the leaves. 
　the q1q procedure is as follows. first  we partition the learning examples according to the leaves of the qualitative tree. these subsets are then used for learning qualitatively consistent functions of the corresponding leaves. let us focus on learning a qualitatively consistent function for some particular leaf. suppose we have a leaf with the qualitative constraint  we then have to find some monotonically increasing function that fits the data well. one straightforward solution  used in q1q  is to divide the range of variable  with a number of equidistant points  i.e. in which we learn from the given data the function values y. 
the result is a set of pairs that defines 
a piece-wise linear function which can be easily checked for compliance with the given qualitative constraint. this procedure can be generalized to qualitative constraints of any dimension. 
　in our implementation the function values yi were determined with a standard version of locally weighted regression  lwr   atkeson et ai  1   which used gaussian weighting function. therefore  the two parameters of the transformation were the number of equidistant points per dimension and the kernel size of the gaussian weighting function all possible combinations of these two parameters  1=1  define the space of all candidate piece-wise linear functions for each leaf. these sets of candidate functions are exhaustively searched by q1q to find functions that satisfy given qualitative constraints. for each leaf  q1q selects among these qualitatively consistent piece-wise linear functions one that has best fit with the data that fall into this leaf. quality of the fit is measured with root mean squared error  rmse for short. it is theoretically possible that none of the candidate functions is qualitatively consistent with the qcf in the leaf. in such a case the q1q procedure would fail to find a qualitatively consistent regression function. although in our experiments this never happened  we are working on an improved version of q1q that is guaranteed to find a qualitatively consistent regression function. 

figure 1: intec wheel model: wheel position is given by and 	coordinates of the wheel center  and rotation angles 
about axes and called enforced wheel-spin angle camber and toe angle a  respectively. these are affected by horizontal forces and elevation of the road r and rotational moment that act upon the tyre. 
1 intec case study 
1 intec wheel model 
in this section we present an application of q1 learning to the modelling of car wheel suspension system. this is a complex mechanical system of industrial relevance. the model and simulation software used in this experiment were provided by a german car simulation company intec. the main role of the application in this paper is to provide a controlled experiment to assess the potentials of q1 learning on a modelling problem of industrial complexity. however  although the target model was already known and developing such a model was not an issue of practical relevance  initially this case study was nevertheless motivated by a practical objective. namely  the complexity of intec's model is so high that it cannot be simulated on the present simulation platform in real time. therefore the practical objective of the application of q1 learning was to speed up the wheel simulation. the goal thus was to obtain a simplified wheel model that would still be sufficiently accurate and at the same time significantly simpler than the original model to allow real-time simulation. indeed  the simplified model obtained with q1 is computationally trivial compared with the original model. 
　the intec wheel model  shown in figure 1  is a multi-body model of a front wheel suspension built in compliance with the physical model assuming no car-body movement and no wheel-spin. in fact  the suspension system is modelled as if the car-body is fixed. wheel position is given by and coordinates of the wheel center. because the flexible joints in multi-body suspension system that links the wheel to the carbody allow several levels of displacements  rotation angles about axes and are also measured. these are called enforced wheel-spin angle camber and toe angle respectively. 
　the multi-body simulation software simpack  intec  1  was used to set up the model and to generate simulation traces. during simulation  a number of elements are acting upon the tyre: two horizontal forces and vertical movement  measured as elevation of the road r  and rota-

1 	qualitative reasoning 


figure 1: a typical simulation trace of the intec wheel model: the input variables are on the upper graph  the output variables  except that changes the same as road r  are on the lower graph  axis is time in steps dt=1 seconds. note the complex behaviour of the output variables resulting from changes in fx and road r. 
tional moment . for example  is acting upon the tyre when braking  when driving through corners  centripetal force  and rotational moment  when parking the car. 
　during the simulation  input and output variables are logged to a file called simulation trace. we used traces of wheel simulation with different trajectories of input variables. each trace lasted for 1 seconds  and was sampled with seconds. in this way a trace gives 1 examples  each example contains 1 values  corresponding to the values of four input and six output variables at a given time. figure 1 shows a typical simulation trace. it should be noted that all these traces correspond to very slow changes of input variables  and as a result the traces are illustrative mainly of the kinematics of the mechanism  but not also of its dynamics. 
1 	details of experiments 
the experiments reported in this paper were done using a black-box approach. we did not use any knowledge of the model. the simulation traces were provided by our partners from czech technical university in the european project clockwork. 
　in all the experiments we used 1 traces for learning with the same road profile as in the trace of figure 1. in the first learning trace all other three input variables were zero. in the next three traces two of the other three input variables were zero and one other variable  was changing. figure 
1 shows one such trace. the remaining three traces were similar  but the trajectory of the changing variable was different  i.e. it first increased  stayed unchanged for 1 seconds  and than slowly decreased to zero. each trace gives 1 examples  giving altogether 1 learning examples with 1 variables. 
　the task was to learn each of the six output variables as a function of input variables. in this way we have six learning problems  where an output variable is the class and the input variables are the attributes. for example  angle a was learned 

the prediction accuracy was tested on 1 test traces. all the 

figure 1: induced qualitative tree for wheel center position. 
test traces have the same road profile as the traces used for learning  but different profiles of other three input variables  i.e. in the first trace all of the three input variables change similar as in the trace in figure 1. this trace was recommended as the most critical test trace by the domain expert  who considered it far more difficult  all 1 input variables change  than other 1 test traces where one or two input variables were always zero. 
1 inducing a qualitative wheel model with quin as described above  quin was used to induce a qualitative tree for each of the six output variables  where the input variables were the attributes. all of the induced qualitative trees had over 1 % consistency on the learning set of examples. we say that a qcf is consistent with an example if the qcf's qualitative prediction of the dependent variable does not contradict the direction of change in the example. the level of consistency of a qualitative tree with the examples is the percentage of the examples with which the tree is consistent. consistency of 1% indicates that the induced qualitative model fits the data nearly perfectly. 
　the simplest qualitative tree was induced for translation in the axis. this tree only has one leaf with qcf this tree has a simple and obvious explanation. it says that changes in the direction of the road change. if road increases then z increases  i.e. the wheel center moves upwards. 
　qualitative trees for translations in and axes are a bit more complicated. since they have similar explanations we will present just the qualitative tree for translations  given in figure 1. note that is measured in the opposite direction as usual  i.e. positive means wheel center moving in the direction of car driving backwards. both leaves of the tree have the same qualitative dependence on and but differ in qualitative dependence on road r. the qualitative tree says that x is positively related to force that acts in the direction of  obviously  wheel center position changes  wheel moves backward or forward  in the direction of force in direction. second  is negatively related to force this means that if we push the wheels together  we apply force in the direction   the wheels will move forward position decreasing . this is not so obvious  but can be understood if we consider the usual mechanics of wheel suspension. the qualitative dependence on road r is a bit more complicated. the qualitative tree of figure 1 says that is negatively related to 
r when . otherwiseis positively related to r  when r increases from its minimum to its maximum   will first decrease and than increase  i.e. the wheel center will first move forward and than backward. 
qualitative reasoning 	1 　rotations about axes and are measured by enforced wheel-spin camber   and toe angle respectively. for enforced wheel-spin quin induced a simple one-leaf tree that says note that changes in the 

direction of the tyre rotation when driving forward. consider for example the dependence of on force that is positive during braking. since is negatively related to increasing  causes to decrease  i.e. during braking enforced wheel spin angle changes in the direction of the tyre rotation. for camber angle qu1n induced a qualitative tree that is similar to qualitative trees for and translations. 
　the toe angle i.e. the rotation about -axis is effected by all input variables and is the most complicated. the induced tree is given in figure 1. we will omit explanation of this qualitative tree  since it requires understanding of the flexible nature of the multi-body suspension system that links the wheel to the car-body. 
　these qualitative trees give a good explanation of wheel suspension system behaviour. moreover  they provide a qualitative model of wheel suspension system that enables qualitative simulation. in this way  they enable to predict all possible qualitative changes of output variables over an arbitrary time interval given qualitative changes of all or some input variables. this qualitative model also enables to improve numerical predictions. 
 1 qualitative correctness of numerical predictors in this section we illustrate why q1 learning may have an advantage over the usual numerical predictors. figure 1 shows predicted with m1 model tree  lwr and lwr with opti-
mized parameters  on the most critical test trace  where all the input variables are changing simultaneously. the figure shows that both m1 and lwr sometimes make large errors. moreover these errors are not only numerical  but also qualitative. consider for example the lwr predictions at the beginning of the trace. here the predicted is decreasing  but the correct  is increasing. this error could be avoided by considering the induced qualitative tree for given in figure 1. since at the beginning of the test trace road r is near zero and increasing  and all other input variables are zero  the second leftmost leaf of the qualitative tree applies. its qcf requires increasing a since road r is 
increasing. 
　as can be observed in figure 1  m1 and lwr often make qualitative errors. q1 predictions are qualitatively correct. the use of  induced  qualitative model enables q1 to better generalize in the areas sparsely covered by the training examples  resulting in better numerical accuracy. 
1 numerical accuracy of the induced models here we compare the numerical accuracy of the weka implementations of lwr  m1 model trees and q1 learning. all the 


figure 1: comparing accuracy  measured with rmse  of q1 and lwr with optimized parameters: the left graph compares rmse on the most difficult test trace and the right graph shows rmse on the remaining test traces. 
methods learned from 1 learning traces  also used for learning of qualitative trees  and were tested against 1 test traces described in section 1. the test results are divided in two groups. the first group consists of results on a single test trace. this trace was recommended as the most critical test trace by the domain expert  who considered it far more difficult  all 1 input variables are changing simultaneously  than the 1 test traces in the second group. 
　learning of qualitatively consistent functions in the leaves was performed as described in section 1. the best fitting regression functions were then taken and glued together into the overall induced numerical model. we compared the accuracy of our q1 method with lwr and m1. the parameters of lwr were optimized for each output variable ...  according to the rmse criterion  through internal crossvalidation on the training set. when experimenting with m1  we noticed that it was grossly inferior both in terms of qualitative acceptability as well as numerical error. attempts at optimizing m1's parameters did not help noticeably. figure 1 gives the prediction accuracy for variables and   the predictions of the remaining variables and 
1 	qualitative reasoning were not much affected by qualitative constraints. the results on the most difficult test trace  left graph in figure 1  show that even our simple q1q method improves the numerical prediction on all the variables  compared to lwr . results on the second test trace group are given on the right graph in figure 1. as these traces were more similar to the learning traces  the improvement of q1 over lwr is smaller. 
1 discussion 
in this paper we introduced a new approach to machine learning in numerical domains  which we call q1 learning  qualitatively faithful quantitative learning . this combines the induction of qualitative properties from numerical data and numerical regression that respects the induced qualitative properties. we showed by an experimental case study that q1 learning may lead to the following advantages compared to the usual numerical learning: 
　 1  induced models tend to be qualitatively consistent with the data and therefore have better chances to correspond to the qualitative mechanisms in the domain of modeling. for example  if the amount of water in a container is decreasing  the level of water cannot be increasing. this is important with respect to the interpretation of induced models and explanation of phenomena in the domain based on these models. 
　 1  qualitative consistency of induced models with learning data also affects the accuracy of the model's numerical predictions: numerical accuracy may be considerably improved. this is illustrated by the experimental results. 
　in respect of numerical prediction accuracy  in our case study q1 overall outperformed all competing numerical learners. among these  locally weighted regression  lwr  with optimized parameters  through internal cross validation on the training set  performed best in terms of mean squared error. however its performance may sharply degrade under more difficult circumstances. consider lwr-optimized performance on the the most difficult test set  figure 1 . it achieves excellent accuracy on the first part of this trace which is similar to data in the training sets. lwr-optimized accuracy there is actually better than that of q1. however  problems begin for lwr in the second part of this trace where the input variables start to deviate considerably from the training data  and lwr's predictive error increases sharply. in this part of the trace  q1 manages to largely preserve qualitative consistency with the true results  and maintains the numerical accuracy at a comparable level as in the area densely populated by training examples. 
　lwr-optimized was the best among standard numerical learners  and therefore our presentation of experimental results largely concentrated on comparison between q1 and lwr. the performance of m1 was grossly inferior both in terms of qualitative acceptability as well as numerical error. optimizing m1's parameters did not help noticeably. 
　it should be noted that qualitatively faithful regression as carried out by the q1q program is actually inferior to lwr as a regression method. struggling to satisfy qualitative consistency  q1q is limited to piece-wise linear regression with a small number of linear segments. this numerical inferiority of q1q usually turns out to be more than compensated by 
preserving qualitative consistency. 
　in this paper  qualitative constraints for q1q were induced from training data with quin. alternatively  such constraints can be defined directly by a domain expert. in such a case  the q'1 learning can be viewed as an approach that enables the use of expert's qualitative knowledge in system identification. 
　among the limitations of our realization of q1  the primitive numerical regression method in q1q should be noted. 
this method allows sharp changes in variable values  discontinuities in the variables' derivatives  at the borders between leaves of a qualitative tree. future work should include a method for smoothing such discontinuities. 
acknowledgments 
the work reported in this paper was partially supported by the european fifth framework project clockwork and the slovenian ministry of education  science and sport. we thank a. eichberger of intec  for providing the wheel suspension model and the simpack simulation software for this study  and for helpful notes on the model. m.valasck and rsteinbauer from czech technical university also helped in the intec case study. 
