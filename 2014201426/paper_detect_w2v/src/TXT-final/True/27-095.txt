 
one of the challenges in process control is providing reliable control of poorly understood systems. before such a system can be controlled we must first be able to predict its future behavior-so that we know what control action is necessary. this paper presents two approaches to this prediction task  both using qualitative models augmented by records of historical system behavior. our hypothesis is that qualitative information about a system is more easily available than quantitative equations; moreover  the information need not be complete or totally correct. we restructure the historical information into a case-base suitable for the prediction task  and use the qualitative model to identify the attributes to use as case-indices. the case-base then provides the quantitative information needed for the prediction task. our techniques are extensively evaluated on data taken from a real-world system. 
1 	introduction 
understanding physical systems well enough to predict and control their behavior has long been a goal in engineering and science. when systems are simple  numerical equations can exactly reproduce the system's behavior. for complex systems  however  developing an accurate numerical model is rarely feasible. 
　qualitative modeling alleviates this problem by modeling systems at a higher level of abstraction. a qualitative model seeks to identify and model only the most important aspects of a system. in the classic example of a ball thrown into the air  an abstract model will not attempt to predict how high the ball rises-but only that it will rise to some height  reverse direction  and fall to the floor. 
　qualitative modeling techniques such as qsim  and qualitative process theory  have been success-
ful at modeling complex physical systems at this level. however  predictions at the qualitative level are insufficiently precise for tasks that do require numerical results  such as diagnosis and process control. techniques such as q1  sode  and simgen allow combining the qualitative models with numerical equations to obtain precise results. however  such techniques are applicable only when these numerical equations are known  again restricting them to well-understood processes. 
　for many practical systems  this is not the case. for example  the tests in this paper were carried out on a coffee roaster  used in various plants of nestle  for which it has so far been impossible to construct an accurate numerical model. the absence of accurate numerical predictions has led to many critical situations  such as fires  which destroy the entire load of coffee beans and require expensive shutdowns. since prevention of these critical situations depends on the numerical prediction of key parameters  purely qualitative models are insufficient. 
　however  an enormous amount of information about the roaster is available in the form of records of past behavior. an alternative to generating a numerical model is to use case-based reasoning  where predictions are based on previous experiences. marc goodman  has reported promising results using this paradigm to make predictions about the behavior of a complex video game. 
　the major problem for such an approach is indexing: which of the wealth of previous observations are in fact relevant to the current situation  in this paper  we describe two ways in which a qualitative model can be used to provide such indices  allowing us to combine past experiences into a prediction for the current situation. 
　a first approach to locating relevant precedents is nearest-neighbor search. here  the problem is to find an appropriate similarity metric which assigns weight to those aspects which are important for the prediction. we have implemented a system where this metric is determined based on a qualitative model  and tests on actual coffee roaster data have shown satisfactory results. 
　in another approach  first mentioned by hellerstein   the qualitative model is used to determine which experiences can provide bounds on the current situation. we have also implemented this second approach and have 
obtained very promising results. 
	richards  faltings  and duxbury-smith 	1 
　when prediction concerns a commonly occurring situation  it is usually possible to locate a single almost identical past experience that gives the correct prediction. however process prediction is most important in critical situations for which past experience is  hopefully  sparse. a prediction may then have to be construed from several precedents  none of which is entirely similar to the current situation. more precisely  each parameter may be predicted from cases selected according to criteria specific to that parameter. our approach is capable of intelligently combining information from several cases into a single solution. 
　using records of historical behavior to provide quantitative information has the marked benefit that the information is  free . in other words  where development of a numeric model requires a great deal of effort  developing a library of cases requires only that we monitor the system and record what it does-something that is a normal part of most process-control systems. 
　our techniques offer two further benefits. first  when dealing with complex machines  each machine may well have its own individual characteristics within some range of  normal  behavior. by using cases recorded from each machine  we automatically account for these individual variations. second  although our techniques take as input qualitative information about the system  they do not require this information to be complete or correct; accuracy degrades gracefully as the qualitative model deteriorates. 
1 	background 
qualitative models are descriptions of a system stated in non-numeric terms. the formalism we use is that of qsim1  in which a model consists of a set of qualitative constraints. a qualitative constraint represents an unknown numeric function which holds amongst the referenced variables. for example  the constraint m+ a  1  states that a strictly monotonically increasing function holds between variables a and b  although the exact function is unknown. 
　the problem we seek to address is that of process prediction and control. performing process prediction is a precursor to addressing the process control problem. the problem of process prediction may be stated as: 
  g i v e n : the current state of a system  given by the numerical values of a set of parameters  and possibly recent historical information . 
  p r e d i c t : the state the system will be in at a spe-cified time in the future  again expressed as the numerical values of a set of parameters. 
　in this paper  we limit our attention to the problem of process prediction. process control requires taking 
   'but most other formulations are equivalent for the purpose of this application. 
1 	qualitative reasoning and diagnosis 
corrective action once prediction shows that an undesirable state is about to be reached. determining appropriate control actions will require additional knowledge about the effects of control parameters on system variables. the idea of augmenting a qualitative model with case-based reasoning may be more difficult to apply to control actions  since historical data about corrective actions is scarce. 
　without loss of generality  we consider predicting only a single parameter at a time; we call it the query parameter. predicting the entire state can be carried out by repeatedly applying the procedure to each parameter in the state description. 
　the application we are using is a continuous-roast coffee roaster used by nestle. the problem underlying coffee roasting is that it is exothermic  and thus inherently unstable. the continuous-roast devices are a new technology  and the theoretical specifications of the machines  stated as numerical equations  etc.  have proven inadequate as a basis for process control. the result is that the machines must often be shut down  because the process is leaving its normal operating region and cannot be brought back by the current control system. 
　in this application  the process monitoring logs contain snapshots taken of the system every thirty seconds. each snapshot records the values of 1 parameters. historical cases are provided in the form of one or more such process logs  each containing approximately 1 hours of data  about 1 snapshots . in the results presented in this paper  we are trying to predict system behavior 1 minutes  or 1 samples  ahead. 
　the qualitative model represents the known relations between these roaster parameters. an important advantage of the qualitative model is that it does not have to be complete  and some relations may be missing without rendering it incorrect. our hand-built model contains known simplifications and approximations of the processes taking place within the roaster. 
1 	adaptation of historical cases 
the techniques we use for prediction were originally designed to predict missing feature values from static cases  the traditional task performed by many case-basedreasoning and machine-learning programs . in order to perform prediction  we first must adapt the cases to the prediction task. 
　our approach is to take the process monitoring logs and create cases which combine information from different times in the process log. so  for example  if we are predicting 1 samples into the future  each case consists of two time points from the process log: one from time x and one from time  x + 1   see figure 1 . 
　the system's current state provides us with the lefthalf of a case  and we use our constructed library to estimate the values that should appear in the right-half. the prediction problem is thus reduced to the better-
　

understood problem of predicting missing values  albeit continuous values  which is not possible using many machine learning techniques . 
1 	direct prediction 
our first approach  described originally in   l l     performs direct prediction. we take as input a qs1m model of the system  and use this to determine  for each parameter  which other parameters should be used as a basis for 
prediction. 
   in   l l   we wanted to solve a qualitative model as set of equations; the end result was to be stated in terms of a few distinguished variables. in the prediction task  however  all variables which appear in the logs are equally available  so none are distinguished. this makes the equation-solving trivial  and results in useless case indices  the indices devolve to the variable appearing in the first constraint to reference the target parameter . 
   however  equation solving can be viewed as graph traversal  where we are tracing paths to some set of distinguished nodes. in the prediction problem  where most or all nodes are distinguished  we need an alternate heuristic. the heuristic we use here is the following: the case indices for a parameter is the set of all variables in the model-graph which are distance d from the parameter  and d is a user-selectable integer greater than zero. 
   the intuition behind this selection is that causality must flow through the system in some direction. while the qualitative model does not provide any information on causality  one or more of the parameters nearby in the model-graph should be predictive. and we have eliminated many  distractor  parameters  which would be irrelevant or possibly damaging to the prediction accuracy. the algorithm for model-based prediction is: 
for each parameter we are to predict 
determine indices 
	for 	each timepoint 
select the nearest case or cases 
　take the predicted value from the selected cases end for 
end for 
given the indices  we use a simple nearest neighbor  inn  matching method; while more sophisticated methods are available  using a simple method provides with 

figure 1: a prediction obtained using the nearestneighbor approach. the solid line shows the actual measurements  the dotted line is the prediction. 
the best feedback on the efficacy of our indexing technique. a sample prediction graph appears in figure 1. here  the dotted line represents the predicted values  whereas the black line shows the actual system behavior. tics on the x-axis represent fifty-minute intervals. 
1 	bounding behaviors 
the second approach we present is based on . this approach determines experiences that provide upper and lower bounds on the value of a query parameter p based on the following consideration. assume that p is related to other parameters q and r by the constraints m+ p q  and m- p r . these constraints can be used to partition the set of previous experiences into three sets: 

if the qualitative model were perfectly accurate  system behavior completely consistent  and measurement noise negligible  then the greatest lower and smallest upper bound would provide a precise interval within which the actual value of p must fall. in practice  various inaccuracies mean that the measured behavior is typically normally distributed over an interval. hence  we ask the use to provide a desired confidence level  and then use statistical techniques to determine upper and lower 
bounds for the confidence interval  as described in .. 
　the set of related variables and their qualitative relations is obtained by taking the transitive closure of all m+ and m- constraints present in the qualitative model. since the parameter being predicted also appears in the historical portion of the case  we automatically add a positive monotonicity relating the parameter to itself. if the qualitative model is reasonably complete  there will typically be several related variables  thus providing a strong index where only a small fraction of the previous experiences will be relevant to an actual prediction problem. the algorithm for this approach is: 
	richards  faltings  and duxbury-smith 	1 
　

figure 1: a prediction made using hellerstein's technique. the solid line shows the actual measurement  the dotted lines give the predicted bounds. 
sort a l l cases by nearness to the current case 
for each parameter we are to predict determine the upper bound: 
find the m nearest cases which are   above  the current case 
sort the n values taken from these cases 
　select the upper bound  based on the desired confidence level determine the lower bound: 
find the m nearest cases which are  'below'* the current case 
sort the n values taken from these cases 
select the lower bound  based on the desired confidence level 
end for 
one limitation is that the confidence level depends on cases being randomly selected from a normally distributed population. since our cases are taken from temporal sequences  they are not independent  although we do use multiple  independent sequences . this means that the true statistical confidence may not correspond to that 
selected by the user; however  this has not proven to be a problem in practice. figure 1 shows a sample prediction made using this approach. the dotted lines show the bounds of the confidence intervals  and the black line shows the actual values. 
1 	empirical results 
this section presents our empirical results in four sections  followed by a fifth section discussing the results of the model-based algorithms. the empirical tests were: 
  tests showing how accuracy is affected by removing  redundant  data from the case base 
  standard learning curves  showing the prediction ac-curacy of various methods 
  model dependence tests  showing how accuracy is affected as the qualitative model degrades 
  prediction period tests  showing how accuracy changes as we predict farther ahead in time 
1 	qualitative reasoning and diagnosis 
　we selected five data files to use in all tests; logs from one roaster taken from five days in january 1. in all cases  the roaster was producing the same recipe  which means that the external settings were identical. all logs contain one or more anomalous events; normally emergency shut-downs followed by some period of inactivity and a process restart. depending on the test  we use between one and four logs as the source of historical cases  and one as a test set. each data point in the graphs that follow represents the average of five test runs  one using each of the logs as a test set. 
　the confidence for the bounding approach was set to 1%  meaning that on average 1% of the predicted values fall within the predicted bounds. as a basis for comparison  we also include results using the i b l algorithm  l   which indexes on all parameters. except for line-crossings  all results are statistically significant  p   1 . however  since we are drawing data from temporal sequences  case selection is not truly random; this means that the value calculated for statistical significance should be taken only as a general indication. 
1 	  f o r g e t t i n g   r e d u n d a n t d a t a 
in practice  a systemlearns as more experiences are added to its database. however  since memory and indexing capabilities are limited  we must provide a method for removing redundant data. in practice  much of the roaster data simply represents normal operation  with all values varying over relatively small ranges. having hundreds of hours of such data on-line is not really very useful. hence  we have implemented a filtering algorithm that deletes redundant cases. 
　to identify redundant cases  we first determine the range over which each parameter varies. we say that some second case is redundant  and can be eliminated  if every parameter is within r% of the range of the corresponding parameter in the first case. for example  suppose we have cases containing only the single parameter x  which varies from -1 to +1. if the user specifies a redundancy level of 1%  then two cases would be considered redundant if: abs x1 - x1    1. 
　to our surprise  even relatively harsh filtering  with r over 1%  did not have a severe impact on predictive accuracy. when using four days of data  we began with about 1 constructed cases; after filtering at 1%  we were left with about 1 cases  which were then evidently sufficient to cover the range of roaster behavior with reasonable accuracy. we do not yet know if this relative insensitivity is application specific; our future work will certainly include testing this on other real data sources. figure 1 shows the reduction in case size realized by various values of r  along with the accuracy impact when using the i b l algorithm; the accuracy of model-based prediction behaves almost identically. 
　
1 	learning curves 
figure 1 shows a standard learning curve for prediction using the ibi algorithm and using the model-based algorithms. these learning curves were produced using a similarity reduction r of 1%  except that no reduction was done on data provided to the bounding approach. the reason for this is that the statistical derivation process depends on the presence of redundant information in the database. 
　as can be seen  the model-based prediction algorithm did not perform as well as simple ib1. given the good results seen in   l l     we were surprised at this result  and it led to the analysis discussed in section 1. however  it is worth noting that the accuracy is competitive  and that the smaller indices used in the model-based approach yield much faster execution times. 
　to allow direct comparison of all approaches  we have also graphed the hellerstein approach in figure 1. however  interpretation of this requires some caution  as this algorithm is not attempting to do direct prediction. the  accuracy  in this case is the average distance of the upper and lower bounds from the actual value  hence  if the upper and lower bounds were simply the highest and lowest possible values  the  accuracy  would be 1 
　this means that the  accuracy  of the bounding approach actually represents the tightness of the confidence interval. thus  the graphs show that the bounds are as tight as the error of the directly predictive approachesthis in spite of the fact that these are outer bounds on possible behavior. however  the projected bounds are  in some sense  more useful. whereas a projected value 
1 	m o d e l dependence 
to verify our hypothesis that our approach degrades gracefully with the quality of the input model  we ran a test in which we gradually removed constraints from the model  see figure 1 . this test was done using the bounding approach. since the presence of large amounts of data tends to improve performance the performance of poor indexing schemes  given enough examples  even a poor indexing scheme will find an applicable example   we emphasized the model's importance by only using a single historical data set in each test run. 
　as constraints were removed from the model  the width of the bounds  graphed as  error   does indeed increase  but it does so gradually. ultimately  when all constraints are removed  the only remaining index is the variable itself; prediction of the query parameter's future value is based solely on its past value  resulting in a confidence interval with an average width of more than 1% of the parameter's range  each bound is  on average  more than 1% distant from the center point . 
1 	prediction period 
finally  we were curious to see how far into the future we could predict system behavior. using the ib1 approach with the reduction parameter set to 1%  we measured predictive accuracy out to 1 minutes  1 samples   see figure 1 . as expected  predictive accuracy is highest when predicting only a very short distance into the furichards  faltings  and duxbury-smith 1 
　

ture. however  accuracy remains quite good through 1 minutes  1 samples . 
1 	analysis of model-based results 
the disappointing results of direct model-based prediction led us to analyze the predictions being made. one of the major problems appears to be the lack of timescales in the model. for example  if the burner setting in the roaster increases  we can expect an increase in the furnace temperature after a few seconds-and this relationship is reflected in the model. however  there is no such direct relationship when predicting 1 minutes in the future. 
　to improve the model-based results  we need to develop a qualitative mode  for the timescale at which we are doing predictions. however  since the system we are working with is  in fact  very poorly understood  it is not clear what such a model should look like. hence  we are now working on methods to derive a partial model directly from this historical data  using techniques drawn from  and other sources. 
　the bounding approach is evidently less sensitive to these problems in the model  probably because it collects a statistical sample of cases from which it derives its bounds. however  we would expect its performance to improve as well  given a model suitable for the timescale at which we are doing prediction. 
1 	related work 
in this paper  we presented two approaches for combining qualitative models with historical behaviors  for the purpose of doing prediction. the first of these approaches  model-based indexing  is based on   l l   ; however  in   l l     this technique was used to develop process settings in terms of desired process outputs and ambient conditions  a well-defined subset of the process variables. in prediction there are no such distinguished parameters  so the approach was altered to use all parameters within a certain distance of the query parameter. in our test domain  this turned out not to work as well as we hoped. since the results in   l l   are quite good  we believe this is due to the mo del-timescale problems discussed above. 
　our second approach was based on   which uses historical information to develop confidence intervals for the 
1 	qualitative reasoning and diagnosis 
values of unknown parameters. we were able to adapt this approach directly to the task of process prediction  and the tightness of the confidence intervals is competitive with the accuracy of purely predictive approaches  which we consider an very positive result. 
　an alternate approach to combining model-based and instance-based learning can be found in . quinlan uses a quantitative model to correct the values retrieved by a normal case-based system  by applying the model both to the target instance and to the retrieved instance. his idea is that one can use the model to calculate a corrective factor  which is applied to the retrieved instance. this approach works well with a good quantitative model  but  in domains with weak models  the corrective factors proved counterproductive. our approach  on the other hand  requires only a qualitative model  and uses the model  not to correct the cases  but to develop indices indicating which cases are most relevant to the current situations. 
　a number of people in the qualitative modeling community have added quantitative information to qualitative models. in   forbus and falkenhainer use qualitative envisionment combined with numerical modeling information; this approach can work well  but requires that one provide both the qualitative and numerical models up-front. kay and kuipers  in  take the approach of quantitatively constraining function envelopes  but again this requires numerical modeling information as input. 
　a difterent approach to prediction is presented in . goodman's approach is a combination of inductive learning and case-based reasoning. he analyzes process logs offline to build clusters of similar behaviors  which are used as the basis for prediction. his approach makes provision for the addition of qualitative information  principally in the form of causal links   but no information is yet available on the accuracy of his predictions or on the effect of adding the qualitative information. 
　finally  where do we obtain the qualitative model used to guide the prediction process  in our application  the model was hand-built  and is known to be incomplete. in general  human experts can provide more complete models  but perfect models are generally not available. for this reason  plus the need for timescale-speciflc models  we are now looking at the possibility of deriving the needed model directly from the historical records of system behavior. the basic techniques needed have already been developed in misq   . 
　since some system knowledge is usually available  the model-building process could start from an initial handgenerated model  and refine it as needed to match the historical observations. this would result in a more directed and efficient model-generation process. in fact  subsequent errors in prediction could be fed back into the model-generation process  leading to an interesting synergy between machine learning and case-based reasoning in this framework. 
　
1 	conclusions 
for many physical devices of great economic importance  no accurate mathematical models exist. in some cases  it is in fact questionable whether numerical models of reasonable size could ever be constructed  since their complexity means that the number of state variables could be enormous. occasionally  techniques such as neural networks can be used to learn satisfactory prediction models  but cannot guarantee correct results in all cases. 
　for most systems  the designers can provide at least a partial qualitative model  which one could enhance using automated model-building techniques. combining this qualitative model with records of historical behavior  we are able to provide accurate numerical predictions. the fact that predictions are based on a qualitative model provides an advantage over pure statistical techniques  such as neural networks   in that the results can be guaranteed to be accurate within reasonable bounds. 
　furthermore  even when the qualitative model is incomplete  our experiments have shown that predictive accuracy degrades gracefully. this robustness stands in contrast to techniques based only on models  where an incomplete model results in an explosion of ambiguities  and model errors can lead to completely wrong conclusions. 
　we believe that our approch points out an interesting direction for resolving the knowledge engineering bottleneck. purely syntactic case-based reasoning is often insufficient to cover all situations in complex systems. model-based reasoning requires a complete and accurate model which is costly to build. by combining the two techniques  we no longer require a 1 % accurate and complete model  but can nevertheless adapt and combine previous cases to achieve good coverage of behaviors with relatively few cases. this disproves the popular myth that case combination and adaptation necessarily reintroduces the knowledge engineering bottleneck; on the contrary  model- and case-based reasoning complement each other very well. 
　many open issues for further work have become apparent. the first concerns the development of the qualitative models. preliminary work in this area  such as misq  has shown that it is possible to develop and refine models from records of process behavior. as the systems collects more and more experience  it should gradually improve its qualitative model through inductive learning techniques  thus improving its indexing accuracy and ability to handle large and larger databases. however  these automatic model-building techniques need further work before they can be used with real-world systems. 
　our results on  forgetting  redundant information also indicate a need for further work. by using a relatively simple heuristic for identifying redundant cases  we were able to eliminate more than 1% of the historical information without severely affecting predictive accuracy. if this proves not to be application specific  then more sophisticated heuristics should yield even better results. given the massive amounts of information which are collected through process monitoring   forgetting  techniques are essential to identify the useful information that should be retained for future reference. 
　the techniques we have described in this paper have been implemented in c + + under dos/windows. we are currently investigating new applications where prediction is important  such as load prediction in power distribution networks. 
