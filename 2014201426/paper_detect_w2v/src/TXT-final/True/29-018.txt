 
controlling a complex dynamic system  such as a plane or a crane  usually requires a skilled operator. such a control skill is typically hard to reconstruct through introspection. therefore an attractive approach to the reconstruction of control skill involves machine learning from operators' control traces  also known as behavioural cloning. in the most common approach to behavioural cloning  a controller is induced in the form of a rule set or a decision or regression tree that maps system states to actions. unfortunately  induced controllers usually suffer from lack of robustness and lack typical elements of human control strategies  such as subgoals and substages of the control plan. in this paper we present a new approach to behavioural cloning which involves the induction of a model of the controlled system and enables the identification of subgoals that the operator is pursuing at various stages of the execution trace. the underlying formal basis for the present approach to behavioural cloning is the theory of lq controllers. experimental results show that this approach greatly improves the robustness of the induced controllers and also offers a new way of understanding the operator's subcognitive skill. 
1 	introduction 
controllers can be designed by machine learning using different kinds of information available to the learning system. approaches like reinforcement learning  geneticalgorithms and neural networks typically don't use prior knowledge about the system to be controlled. 
　humans  however  rarely attempt to learn from scratch. they extract initial biases as well as strategies from their prior knowledge of the system or from demonstration of experienced operators. control theory makes use of the former  but it doesn't consider operator's skill. 
ivan bratko 
ivan.bratko fri.uni-lj.si 
faculty of computer and 
information sciences  
university of ljubljana  
trzaska 1  1 ljubljana  slovenia 
on the other hand  the idea of behavioural cloning 
 a term introduced by donald michie  michie  1   is to make use of the operator's skill in the development of an automatic controller. a skilled operator's control traces are used as examples for machine learning to reconstruct the underlying control strategy that the operator executes subconsciously. the goal of behavioural cloning is not only to induce a successful controller  but also to achieve better understanding of the human operator's subconscious skill  urbancic and bratko  1 . behavioural cloning was successfully used in problem domains as pole balancing  production line scheduling  piloting  sammut et al.  1  and operating cranes. these 
experiments are reviewed in  bratko et al.  1 . controllers were usually induced in the form of decision or regression trees. 
　although successful clones have been induced in the form of trees or rule sets  the following problems have generally been observed with this approach: 
  typically  induced clones are brittle with respect to small changes in the control task. 
  the clone induction process typically has low yield: the proportion of successful controllers among all the induced clones is low  typically well below 1%. 
  resulting clones are purely reactive and inade-quately structured as conceptualisations of the human skill. they lack typical elements of human control strategies: goals  subgoals  phases and causality. 
in this paper we propose a different approach to behavioural cloning which exploits some results from control theory. in particular  our clones take the form of lq controllers. the approach also involves induction of approximate models of controlled systems. it is experimentally demonstrated that this approach dramatically improves both the clones' robustness with respect to the changes of the control task  and the yield of the cloning process. also  the approach provides a way of interpreting the induced clones in terms of the operator's goals 


	suc & bratko 	1 

optimal control theory gives the optimal action u  which minimizes this cost-to-go function  by a simple gain matrix a : 

　linear system dynamics is assumed. however empirical results often show good robustness of lq-controllers even if the actual system is non-linear and approximated by a linear model. 
1 	approximating the operator's value function 
our aproach to behavioural cloning in the lqr framework relies on the following observations: 
1. an approximate model of the controlled system  in the form of a linear state transition function t  can be induced by locally weighted regression  schaal and atkeson  1  from some available execution traces. 
1. generally  when  a  cost matrices q and r are given and the execution cost is defined as lq cost  function of q and r   and  b  the system is linear  formulas from control theory can be used to compute the state value function v x  and corresponding optimal actions u x . 
1. we assume that the operator's actions in the available execution traces are intended to optimise the lq cost. so in the case of behavioral cloning we have to solve the inverse to the optimal control problem: given the actions  find the matrices q and r for which the actions are  approximately  optimal. 
1. the operator may be executing a more elaborate plan when he pursues different subgoals at different stages of the execution trace. in such cases  the trace can be broken into stages whose subgoals are reflected in different q and r matrices. 
in the following we develop the details of this idea. 
　a continous trace is sampled so that we have a sequence of pairs  xk uk   k=l 1 ... n  ordered according to time. goal state xdes is known in advance. the state transition function t x u  is estimated using locally weighted regression. assuming the linear dynamics of the system we can compute lq cost-to-go function as 

where p is computed as the solution of the riccatti equation as explained erlier. 
　define lq-optimal action as an action which minimizes the lq-cost for particular q* and r*: 

i.e  by finding  least-squares fit  of operator's cost-to-go function and lq-controller cost-to-go function. when q* and r* are found  the corresponding lq-controller can be constructed. matrices q* and r* can also be used to explain how the operator is performing the task. local optimization optimizes matrices of q and r  so the number of optimization parameters grows as the square of the number of state variables and control variables. 
　a simplification is to assume q and r are diagonal. in this case the number of optimization parameters grows linearly with the number of variables. this reduces the complexity of local optimization problem  and sometimes improves the comprehensibility of the induced controller. 
1 	learning subgoals 
in complex control tasks  like flying a plane for example  the operator's policy usually changes in time. to achieve the final goal  the operator must first achieve some subgoals. those subgoals define stages of a policy. each stage ends when its subgoal is reached. one possible approach is to break the execution trace into stages by hand  and learn a controller for each stage separately. however  it is not always possible to define stages by hand. an operator learns his skills by practice. the rules underlying these skills are typically opaque to the operator  and he is not able to describe them completely. the obvious alternative is to induce the subgoals 

from execution trace and construct a controller for each stage. we define a subgoal as a point in a state space approached by the operator and after which the operator's policy changes in the sense of lq-cost. accordingly we define a subgoal candidate xg j  at time step j as the point in the state space with coordinates computed as mean of the most recent 1 states. 
　the change of a policy is detected as minimization of weighted lq-fit error at a given subgoal candidate: 

first the whole execution trace is searched for a subgoal 
xg
 {j  which minimizes weighted lq-fit error. given the subgoal xg{j   the execution trace is divided into two stages  one before subgoal is reached and the other after the subgoal is reached. both stages are then recursively searched for subgoals. this is done until the error of the divided stage is greater than the error of the undivided stage. for every stage an lq-controller is computed. when controller's subgoal is approached  the next controller is activated. 
1 experimental results 
1 	container crane 
in this section we describe experiments in lqr cloning in the domain of container cranes  urbancic and bratko  1   systems with nonlinear dynamics. 
　to transport a container from a shore to a target position two operations are to be performed: positioning of the trolley  bringing it above the target load position  and rope operation  bringing the load to the desired height. the performance requirements include basic safety  stop-gap accuracy and as high capacity as possible. the last requirement means that the time for transportation is to be minimized. the most difficult aspect of the task is to control the swing of the rope. when the load is close to the goal position  the swing should ideally be zero. 
　a crane simulator was used in our experiments. the state of the system is specified by six variables: trolley position x and its velocity x  rope inclination angle and its angular velocity  rope length l and its velocity l. two control forces are applied to the system: force xf to the trolley in the horizontal direction and force yf in the direction of the rope. 
　we used experimental data from manually controlling the crane from a previous study  urbancic and bratko  1 . in that study  six students volunteered to learn to control the simulator. all of them succeeded to accomplish the task. however  remarkable individual differences were observed regarding the speed of controlling and the characteristics of the strategy they used. some operators tended towards fast and less reliable operation  others were more conservative and slower  in order to avoid large rope oscillations. our goal of cloning was also to reconstruct these individual differences between the operators in the style of driving the crane. 
　previous experiments in this domain involve behavioural cloning with regression trees and combining skills from several operators. the main problem for regression trees was the swing control  i.e large rope oscillations when the trolley approached its goal position. 
1 	experimental details 
to complete the task successfully  the operator had to reach the goal position within 1 s. each trace was sampled with frequency 1 hz. a successful trace typically lasts between approximately 1 and 1 s  so such a trace gives about 1 to 1 state-action pairs. for local optimization powell's method with discarding the direction of largest decrease was used. matrices q and ft were restricted to diagonal. 
　since state is a 1 dimensional and action is a 1 dimensional vector  1 parameters were approximated by local optimization. it took about 1 sec. on pentium 1 hz to find matrices q and r and construct the corresponding controller. when learning subgoals s was set to 1. to minimize time needed to find the best subgoal  a subgoal candidate was tested every 1 time steps. it took about  1 minutes to find the best one among those candidates. 
　to learn a linear model locally weighted regression at the mean point of the stage was used. to learn a model we used several operators' execution traces: first  the trace used to approximate the value function  another two traces of the same operator  and typically one more trace from another operator. there was no particular reason for this choice of traces and we feel that other choices would produce similar results. 
1 	experimental results 
experiments with clones based on lqr showed that the swing control is not as difficult as it seems to be for regression tree clones. even when a controller accelerates and this causes a huge swing of the rope  the controller usually manages to stabilize the rope angle within a few seconds. most experiments were done with controllers where matrices q and r were restricted to diagonal matrices. the reason for this restriction was simply in smaller complexity of local optimization. also  so restricted controllers are easier to comprehend. 
　controllers without subgoals  i.e single lq controllers usually failed to accomplish the task. 
　this observation lead us to conclusion that lq controllers with subgoals should be sought. there is also a 
	suc & bratko 	1 
figure 1: trace 1 of subject s and trace of its clone 


figure 1: trace 1 of subject v and trace of its clone 

against changes in the start state  the test where the controllers in the form of decision or regression trees usually fail. since lq controllers are goal directed  the change of the start state didn't affect their performance significantly. starting horizontal position was modified by 1% and 1%  and starting rope angle by -1%o  1% and 1%. this amounts to six combinations - six different tasks. 
nineteen of twenty controllers tested accomplished all six different tasks. 
　the results show that induced controllers perform very well and often accomplish the task faster than the original operator. 
1 	conclusion 
a new approach to behavioural cloning was presented in the paper. in previous work  the cloning problem was usually formulated as one of inducing a straightforward mapping from system states to actions. our approach on the other hand also involves the induction of a model of the dynamics of the controlled system  and takes into account the order of the example state-action pairs. it in a way enables the identification of subgoals that the operator is pursuing at various stages of the execution trace. experiments in the crane domain show that the  automatic  breakdown of the control traces into stages with their corresponding subgoals is essential for inducing robust controllers. 
　the underlying formal basis for the present approach to behavioural cloning is the theory of lq controllers  and the cloning problem is here done as the inverse of the usual problem of designing an optimal controller. as experimental results show  this approach in comparison with most of the previous experiments greatly improves the robustness of induced controllers and the yield of the cloning process. 
