 over the inherent syntactic and semantic variability in natural language  dagan and glickman  1 . this challenge is at the heart of many high level natural language processing tasks including question answering  information retrieval and extraction  machine translation and others that attempt to reason about and capture the meaning of linguistic expressions.
　research in natural language processing in the last few years has concentrated on developing resources that provide multiple levels of text analysis  both syntactic and semantic   resolve various context sensitive ambiguities  and identify abstractions  from syntactic categories like pos tags to semantic ones like named entities   and the text relational structures.
　indeed  several decades of research in natural language processing and related fields have made clear that the use of deep structural  relational and semantic properties of text is a necessary step towards supporting higher level tasks. however  beyond these resources  in order to support fundamental tasks such as inferring semantic entailment between two texts snippets  there needs to be a unified knowledge representation of the text that  1  provides a hierarchical encoding of the structural  relational and semantic properties of the given text   1  is integrated with learning mechanisms that can be used to induce such information from raw text  and  1  is equipped with an inferential mechanism that can be used to support inferences with respect to such representations.
just resorting to general purpose knowledge representations - fol based representations  probabilistic representations or hybrids - along with their corresponding general purpose inference algorithms is not sufficient.
　we have developed an integrated approach that provides solutions to all challenges mentioned above. we formally define the problem of semantic entailment for natural language and present a computational approach to solving it  that consists of a hierarchical knowledge representation language into which we induce appropriate representations of the given text and required background knowledge  along with a sound inferential mechanism that makes use of the induced representation to determine entailment. our inference approach is formalized as an optimization algorithm that we model as an integer linear programming problem. the preliminary evaluation of our approach is very encouraging and illustrates the significance of some of the key contributions of this approach.
1 general description of our approach
given two text snippets s  source  and t  target   typically  but not necessarily  s consists of a short paragraph and t  a sentence  we want to determine if s|=t  which we read as  s entails t  and  informally  understand to mean that most people would agree that the meaning of s implies that of t. somewhat more formally  we say that s entails t when some representation of t can be  matched   modulo some meaning-preserving transformations to be defined below  with some  or part of a  representation of s  at some level of granularity and abstraction.
the approach consists of the following components:
kr: a description logic based hierarchical knowledge representation  efdl  into which we re-represent the surface level text representations  augmented with induced syntactic and semantic parses and word and phrase level abstractions. kb: a knowledge base consisting of syntactic and semantic rewrite rules  written in efdl.
subsumption: an extended subsumption algorithm which determines subsumption between efdl expressions  representing text snippets or rewrite rules .  extended  here means that the basic unification operator is extended to support several word level and phrase level abstractions.
　first a set of machine learning based resources are used to induce the representation for s and t. the entailment algorithm then proceeds in two phases:  1  it incrementally gen-

figure 1: example of re-represented source & target pairs as concept graphs. the original source sentence s generated several alternatives includingand the sentence in the figure   . our algorithm was not able to determine entailment of the first alternative  as it fails to match in the extended subsumption phase   but it succeeded for. the dotted nodes represent phrase level abstractions.is generated in the first phase by applying the following chain of inference rules: #1  genitives :  z's w ★ w of z ; #1:  x put end to y's life ★ y die of x . in the extended subsumption  the system makes use of wordnet hypernymy relation   lung cancer  is-a  carcinoma   and np-subsumption rule   jazz singer marion montgomery'  is-a  singer  . the rectangles encode the hierarchical levels  h1 h1 h1  at which we applied the extended subsumption. also note that in the current experiments we don't consider noun plurals and verb tenses.erates re-representations of the original representation of the source text s and  1  it makes use of an  extended  subsumption algorithm to check whether any of the alternative representations of the source entails the representation of the target t. the subsumption algorithm mentioned above is used in both phases in slightly different ways.
　figure 1 provides a graphical example of the representation of two text snippets  along with an sketch of the extended subsumption approach to decide the entailment.
　along with the formal definition developed here of semantic entailment  our knowledge representation and algorithmic approach provide a novel solution that addresses some of the key issues the natural language research community needs to address in order to move forward towards higher level tasks of this sort. namely  we provide ways to represent knowledge  either external or induced  at multiple levels of abstractions and granularity  and reason with it at the appropriate level.
1 experimental evaluation
data. we tested our approach on the pascal challenge data set  http://www.pascal-network.org/challenges/rte/ . as the system was designed to test for semantic entailment  the pascal data set is ideally suited  being composed of 1 source - target sentence pairs  indicating whether the source logically entails the target. the set is split into various tasks: cd  comparable documents   ie  information extraction   mt  machine translation   pp  prepositional paraphrases   qa  question answering   and rc  reading comprehension .
in table 1 we show the system's performance. the baseline is a lexical-level matching based on bag-of-words representation with lemmatization and normalization  llm .
perform.overall  % task  % cdieirmtppqarcsystem11111111llm11111111table 1: system's performance obtained for each experiment on the pascal corpora and its subtasks.
acknowledgement
we thank dash optimization for the free academic use of their xpress-mp software. this work was supported by the advanced research and development activity  arda 's advanced question answering for intelligence  aquaint  program  nsf grant itr-iis-1  and onr's trecc and ncassr programs.
