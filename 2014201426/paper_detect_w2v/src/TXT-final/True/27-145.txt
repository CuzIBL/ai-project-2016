modeling interactive agents in alive 
pattie maes  bruce blumberg  trevor darrell  alex pentland  alan wexelblat 
mit media laboratory 
1 ames street 
cambridge  ma 1 
pattie/bruce/trevor/sandy/wex media.mit.edu 
   
   in this video we discuss the design and implementation of a novel system which allows wireless full-body interaction between a human participant and a graphical world inhabited by autonomous agents. the system is called  alive   an acronym for artificial life interactive video environment . one of the goals of the a l i v e project is to demonstrate that virtual environments can offer a more emotional and evocative experience by allowing the participant to interact with animated characters which have complex behaviors and which react to the user and the user's actions in the virtual world. 
   the alive system has been demonstrated and tested in several public forums. it was demonstrated for 1 days at the siggraph-1 tomorrow's realities show in anaheim  california and for 1 days at the aaai-1 art show in seattle  washington. the system is installed permanently at the m i t media laboratory in cambridge  massachusetts. it will feature in the ars electronica museum  currently under construction in 
linz  austria and the arctec electronic arts bienale in tokyo  japan  may 1. 
   in the style of myron krueger's videoplace system  the a l i v e system offers an unencombered  full-body interface to a virtual world . the alive user moves around in a space of approximately 1 by 1 feet. a video camera captures the user's image and removes the background environment; thus  no blue-screens or other special walls are needed. the separated outline is then composited into a 1d graphical world. the resulting scene is projected onto a large  approximately 1'xl1'  screen which faces the user ana acts as a  magic mirror:  the user sees him/herself in the environment  surrounded by objects and agents. no goggles  gloves  or tethering wires are needed for interaction with the virtual world. 
   computer vision techniques are used to extract information about the person  such as where in the space the person stands and the position of various body parts. a pattern-matching technique called dynamic time-warping is used to recognize simple gestures as they are performed. alive combines active vision and domain knowledge to achieve robust real-time performance . 
   the user's position as well as hand and body gestures are used as input to affect the behavior of agents in the virtual world. agents have vision sensors which allow them to  see  the user and react to gestures such as pointing or throwing a virtual ball. the user receives visual  on the big screen  and auditory  prerecorded sound  feedback about the agents' internal state and reactions. agents have a set of needs and motivations  a set of sensors to perceive their environment  a repertoire of activities which they can perform and a physicallybased motor system that allows them to move in and act on the environment. the behavior system decides in real-time which activity the agents should engage in so as to meet their internal needs and to take advantage of opportunities presented by the current state of the environment. 
   the system allows a direct-manipulation style of interaction in which users interact directly with the environment - such as pushing buttons or moving ob-
jects - and also an indirect style of interaction in which users give agents commands and the agents carry out the commands based on the user's input  the current environment  and their internal state. for example  the meaning of a gesture is interpreted by the agents based on the situation the agent and user find themselves in. when the user points away from herself  and thereby  gives the command  to send a character away  the character responding to the command will go to a different place in the virtual environment depending on where the user is standing and which direction she is pointing. in thib manner  a relatively small set of gestures can be employed to mean many different things in many different situations. 
   the alive system incorporates a tool  called  hamsterdam     for modeling semi-intelligent autonomous agents that can interact with one another and with the user. hamsterdam produces agents that respond with a relevant activity on every time step  given their internal needs and motivations  their past history and the environment they perceive with its attendant opportunities  challenges and changes. moreover  the pattern and rhythm of the chosen activities is such that the agents neither dither between multiple activities  nor persist too long in a single activity. they are capable of interrupting a given activity if a more pressing need or an unforeseen opportunity arises. 
   the hamsterdam activity model is based on elements taken from animal behavior models initially proposed by ethologists. in particular  several ethological concepts such as behavior hierarchies  releasers  fatigue  and so on have proven to be crucial in guaranteeing the robust and flexible behavior required by autonomous 
	maes  et al. 	1 
   
interacting agents  . 
   when using hamsterdam to build a creature  the designer specifies the sensors of the agent  its motivations or internal needs  and its activities  behaviors  and actions  motor system movements necessary to fulfill a behavior . given that information  the hamsterdam software automatically infers which of the activities is most relevant to the agent at a particular moment in time according to the state of the agent  the situation it finds itself in  relevant input from the environment  and its recent behavior history. the observed actions of the agent are the final result of numerous potentially executable behaviors competing for control of the agent. the activities compete on the basis of the value of a given activity to the agent at that instant  given the above factors. the details of the behavior model and a 
   discussion of its features are reported in  . 
   the most sophisticated creature built so far is a dog called silas. silas's behavioral repertoire currently includes following the user  sitting when asked by the user  going away when ordered by the user to do so  and performing other tricks such as standing on his hind legs  fetching a ball  lying down and shaking paws. silas also will chase the hamster if the latter creature is introduced into the same virtual environment as the dog. along with visual sensors and feedback  the alive environment also uses sound. silas provides auditory output in the form of a variety of prerecorded samples. 
   the alive system demonstrates that entertainment and the effort to model believable creatures in simple virtual environments can be a challenging and interesting application area for autonomous agents research. a l i v e provides a novel environment for studying architectures for intelligent autonomous agents. as a testbed for agent architectures  it avoids the problems associated with physical hardware agents or robots  but at the same time forces us to face non-trivial problems  such as noisy sensors and an unpredictable  fast-changing environment. it makes possible our study of agents with higher levels of cognition  without oversimplifying the world in which these agents live. 
   a l i v e represents only the beginning of a whole range of novel applications that could be explored with this kind of system. we are currently investigating a l i v e for interactive storytelling applications in which the user plays one of the characters in the story and all other characters are artificial agents which collaborate to make the story move forwards  for more exposition on this topic  see the three short papers on a l i v e in fll . another obvious entertainment application of a l i v e is video games. we have hooked up the a l i v e vision-based interface to existing video came software  so as to let the user control a game with his full body. in addition  we are investigating how autonomous video game characters can learn and improve their competence over time  so as to keep challenging a video game player. finally  we are modeling animated characters that teach a user a physical skill in a personalized way. the agent is modeled as a personal trainer that demonstrates to the user how to perform an action and provides personalized and timely feedback to the user  on the basis of the sensory information about the user's gestures and body positions. the alive system shows that animated characters that are based on ar-
1 	videos 
tificiai life models can not only look convincing - that is allow suspension of disbelief on viewing - but can act and interact in a realistic enough manner to maintain this suspension of disbelief during unpredictable realtime interaction with users. 
acknowledgements 
alan wexelblat spent many long hours shooting and editing the video. thanks alan! 
