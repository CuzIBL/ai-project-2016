
enforcing arc consistency  ac  during search has proven to be a very effective method in solving constraint satisfaction problems and it has been widely-used in many constraint programming systems. although much effort has been made to design efficient standalone ac algorithms  there is no systematic study on how to efficiently enforce ac during search  as far as we know. the significance of the latter is clear given the fact that ac will be enforced millions of times in solving hard problems. in this paper  we propose a framework for enforcing ac during search  acs  and complexity measurements of acs algorithms. based on this framework  several acs algorithms are designed to take advantage of the residual data left in the data structures by the previous invocation s  of acs. the algorithms vary in the worst-case time and space complexity and other complexity measurements. empirical study shows that some of the new acs algorithms perform better than the conventional implementation of ac algorithms in a search procedure.
1	introduction and background
enforcing arc consistency  ac  on constraint satisfaction problems  csp  during search has been proven very successful in the last decade  sabin and freuder  1; mackworth  1  as ac can be enforced millions of times in solving hard instances  the need for efficient ac algorithms is obvious. given the numerous attempts to optimize standalone ac algorithms  further improvement on their performance becomes very challenging. in this paper  in order to improve the overall efficiency of a search procedure employing arc consistency  we focus on how to efficiently enforce ac during search  acs   rather than on standalone ac algorithms.
모in this paper  we abstract acs into a separate module that maintains ac on a changing csp problem p with four methods. several complexity measurements are then proposed to evaluate the theoretical efficiency of acs algorithms in term of these methods. a key method is acs.try x = a   where x = a is an assignment. it checks whether p 뫋 {x = a} can be made arc consistent. whenever a search procedure makes an assignment  it will call acs.try   with that assignment as the argument and make further decision based on the return value of try  . with the explicit abstraction of acs  we notice that after one invocation of an acs method  say acs.try    there are residual data left in the structures of acs. we will explore how to make use of these residual data to design new acs algorithms with the new measurements in mind. empirical study is also carried out to benchmark the new acs algorithms and those designed using conventional techniques. one of the new acs algorithms is very simple but shows a clear performance advantage  clock time  over the rest. necessary background is reviewed below.
모a binary constraint satisfaction problem  csp  is a triple  v d c  where v is a finite set of variables  d ={dx | x 뫍 v and dx is the finite domain of x}  and c is a finite set of binary constraints over the variables of v . as usual  we assume there is at most one constraint on a pair of variables. we use n  e  and d to denote the number of variables  the number of constraints  and the maximum domain size of a csp problem.
모given a constraint cxy  a value b 뫍 dy is a support of a 뫍 dx if  a b  뫍 cxy  and a constraint check involves determining whether  u v  뫍 cxy for some u 뫍 dx and v 뫍 dy. a constraint cxy is arc consistent if each value of dx has a support in dy and every value of dy has a support in dx. a csp problem is arc consistent  ac  if all its constraints are arc consistent. to enforce arc consistency on a csp problem is to remove from the domains the values that have no support. a csp is arc inconsistent if a domain becomes empty when ac is enforced on the problem.
모we use dx1 to denote the initial domain of x before the search starts while dx the current domain at a momentduring ac or search. a value u is present in  or absent from  respectively  dx if u 뫍 dx  u /뫍 dx respectively . for each domain dx 뫍 d  we introduce two dummy values head and tail. we assume there is a total ordering on dx 뫋 {head  tail} where head is the first  i.e.  smallest  value  and tail the last  largest  value. for any a from dx1  succ a dx   pred a dx  respectively  is the first  last respectively  value of dx뫋 {head  tail} that is greater  smaller respectively  than a.
1	enforcing arc consistency during search
we take a csp solver as an iterative interaction between a search procedure and an acs algorithm. the acs algorithm can be abstracted into one data component and four methods: a csp problem p  init p1   try x = a   backjump x = a   and addinfer    where p1 is another csp problem  x 뫍 p.v   and a 뫍 p.dx. throughout the paper  p.v   p.dx x 뫍 p.v    and p.c denote the set of variables  the domain of x  and the set of constraints of p.
모acs.p is accessible  read only  to a caller. when the context is clear  we will simply use p  instead of acs.p.
모acs.init p1  sets p to be p1 and creates and initializes the internal data structures of acs. it returns false if p is arc inconsistent  and true otherwise. acs.try x = a  enforces arc consistency on p1 = p 뫋 {x = a}. if the new problem p1 is arc consistent  it sets p to be p1 and returns true. otherwise  x = a is discarded  the problem p remains unchanged  and try   returns false. in general  the method can accept any type of constraints  e.g.  x 뫟 a. acs.addinfer    enforces arc consistency on. if the new problem p1 is arc consistent  it sets p to be p1 and returns true. otherwise  addinfer   returns false. when mac infers that x can not take value a  it calls acs.addinfer  in general  any constraint can be added as long as it is inferred from the current assignments by the search procedure. acs.backjump x = a  discards from p all constraints added  by acs.try   or acs.addinfer    to p since  including  the addition of x = a. the consequences of those constraints caused by arc consistency processing are also retracted. this method does not return a value. we ignore the prefix acs of a method if it is clear from the context.
모a search procedure usually does not invoke the acs methods in an arbitrary order. the following concept characterizes a rather typical way for a search procedure to use acs methods. given a problem p  a canonical invocation sequence  cis  of acs methods is a sequence of methods m1  m1  ...  mk satisfying the following properties: 1  m1 is init p1  and for any i  1 뫞 i 뫞 k   mi 뫍 {try    addinfer    backjump  }; 1  m1 returns true if k 뫟 1; 1  for any try x = a  and addinfer 
acs.p.v and a 뫍 dx at the moment of invocation; 1  for any mi=backjump y = a  where 1 뫞 i 뫞 k  mi 1 must be an invocation of try   or addinfer   that returns false  and there exists mj such that 1 뫞 j   i   1 and mj=try y = a  and there is no backjump y = a  between mj and mi; 1  for any mi=addinfer   where 1 뫞 i 뫞 k  if it returns false  mi+1 must be backjump  . note that an arbitrary canonical invocation sequence might not be a sequence generated by any meaningful search procedure.
모example algorithm 1  line 1  illustrates how mac  sabin and freuder  1  can be designed using acs.
1	template implementation of acs methods
to facilitate the presentation of our acs algorithms  we list a template implementation for each acs method in algorithm 1. since try   could change the internal data structures and the domains of the problem p  it simply backups the current state of data structures with timestamp x a   line 1  before it enforces arc consistency  line 1 . an alternative is to  backup the changes  which is not discussed here because it does not affect any complexitymeasures of acs algorithms  except possibly the clock time . acs-x.propagate   follows

algorithm 1: mac and template acs methods

------------mac algorithm------------
mac p 
1 if  not acs.init p   then return no solution
1 create an empty stack s; // no assignments is made yet
1 freevariables 뫹p.v
1 while  freevariables do
1 select a variable xi from freevariables and a value a for xi
1 if  acs.try xi = a   then
1 s.push  xi a  
1 freevariables 뫹 freevariables  {xi}
1
	1	while  not acs.addinfer  do
1if  s is not empty  then  xi a 뫹 s.pop  
1else return no solution 1acs.backjump xi = a 
1freevariables 뫹 freevariables 뫋{xi};
1 return the assignments
------------template acs methods------------
acs-x.init p1 
1 p 뫹 p1  initialize the internal data structures of acs-x
1 return ac p  // ac   can be any standalone ac algorithm acs-x.try x = a 
1 backup timestamp  x a  
1 delete all values except a from dx
1 q 뫹{ y x |cyx 뫍 p.c }
1 if  propagate q   then return true
1 else acs-x.restore timestamp  x a    return false
acs-x.addinfer 
1 delete a from dx  q 뫹{ y x |cyx 뫍 p.c}  return propagate q  acs-x.backjump x = a 
1 restore timestamp  x a  
acs-x.backup timestamp  x a  
1 backup the internal data structures of acs-x  following timestamp  x a 
1 backup the current domains  following timestamp  x a 
acs-x.restore timestamp  x a  
1 restore the internal data structures of acs-x  following timestamp  x a 
1 restore the domains of p  following timestamp  x a 
acs-x.propagate q 
	1 while	do
1 select and delete an arc  x y  from q
1 if revise x y  then
1
	1	 	  | wx	=
1 return true
acs-x.revise x y  1 delete 뫹 false
1 foreach a 뫍 dx do
	1	if not hassupport x a y  then
1delete 뫹 true  delete a from dx
1 return delete
acs-x.hassupport x a y 
1 if   b 뫍 dy such that  a b  뫍 cxy  then return true 1 else return false

that of ac-1  mackworth  1 . acs-x.addinfer   line 1  does not call backup   becauseis an inference from the current assignments and thus no new backup is necessary.
1	complexity of acs algorithms
we present several types of the time and space complexities for acs algorithms. the node-forward time complexity of an acs algorithm is the worst-case time complexity of acs.try x = a  where x 뫍 p.v and a 뫍 dx. an incremental sequence is a consecutive invocations of acs.try   where each invocation returns true and no two invocations involve the the same variable  in the argument . the path-forward time complexity of an acs is the worst-case time complexity of any incremental sequence with any k 뫞 n  the size of p.v   invocations. node-forward space complexity of an acs algorithm is the worst case space complexity of the internal data structures  excluding those for the representation of the problem p  for acs.try x = a . path-forward space complexity of an acs algorithm is the worst case space complexity of the internal data structures for any incremental sequence with n invocations.
모in empirical studies  the number of constraint checks is a standard cost measurement for constraint processing. we define for acs two types of redundant checks. given a cis m1 m1 ... mk and two present values a 뫍 dx and b 뫍 dy at mt 1 뫞 t 뫞 k   a check cxy a b  at mt is a negative repeat  positive repeat respectively  iff 1   a b  뫍/ cxy   a b  뫍 cxy respectively   1  cxy a b  was performed at ms 1 뫞 s   t   and 1  b is present from ms to mt.
1	acs in folklore
traditionally  acs is simply taken as an implementation of standard ac algorithms in a search procedure. let us first consider an algorithm acs-1 employing ac-1. it is shown in algorithm 1 where only methods different from those in algorithm 1 are listed.

algorithm 1: acs-1 and acs-1record

-------------acs-1-----------
acs-1.init p1 
1 p 뫹 p1  initialize the internal data structures of acs-x
1 return ac-1 p 
acs-1.backup timestamp  x a  
1 backup the current domains  following timestamp  x a 
acs-1.restore p  timestamp  x a  
1 restore the domains  following timestamp  x a 
acs-1.hassupport x a y 
1 b뫹 head
1 while b뫹 succ b dy  and tail do  if  a b  뫍 cxy then return true 1 return false
-------------acs-1record-----------
acs-1record.init p1 
1 p 뫹 p1
1  cxy 뫍 p.c and  a 뫍 dx  initialize last  x a y 
1 return ac-1 p 
acs-1record.backup timestamp  x a  
1  cxy 뫍 p a 뫍 dx  backup last  x a y   following timestamp  x a 
1 backup the current domains  following timestamp  x a 
acs-1record.restore timestamp  x a  
1 restore the data structure last     following timestamp  x a 
1 restore the domains  following timestamp  x a 
acs-1record.hassupport x a y 
1 b뫹 last x a y  ; if b 뫍 dy then return true 1 while b뫹 succ b dy  andtail do 1		if  a b  뫍 cxy then { last x a y  뫹b ; return true } 1 return false

proposition 1 acs-1 is correct with respect to any cis. node-forward and path-forward complexity of acs-1 are both o ed1  while node-forward and path-forward space complexity are o ed . it can not avoid any positive or negative repeats.
모it is well known that variable-based ac-1 can be implemented with space o n . the same is also true for acs-1.
모we next introduce acs-1record  an algorithm that employs ac-1  bessiere et al.  1 . it is listed in algorithm 1 in which methods that are same as the template acs methods are omitted. ac-1 improves upon ac-1 simply by using a data structure last x a y  to remember the first support of a of x in dy in the latest revision of cxy. when cxy needs to be revised again  for each value a of x  ac-1 starts the search of support of a from last x a y . last x a y 
	..	..
	 a 	 b 
figure 1: example
satisfies the following two invariants: support invariant -  a  last x a y   뫍 cxy  and safety invariant - there exists no support of a in dy that comes before last x a y . the function hassupport    line 1  follows the same way as ac-1 to find a support. note that in restore    the removed values are restored in the original ordering of the domains  which is critical for the correctness of acs-1record.
theorem 1 acs-1record is correct with respect to any cis. node-forward and path-forward time complexity of acs-1record are both o ed1  while node-forward and path-forward space complexity are o ed  and o ned  respectively. it can neither fully avoid negative nor positive repeats.
모the node-forward space complexity of acs-1record can be improved to o edmin n d    van dongen  1 .
모example in this example we focus on how a support is found in a cis of acs-1record methods. consider the following cis: mi = try z = c   returning false  and mi+1 = try z = d . assume before mi  p is arc consistent and contains some constraints and domains shown in figure 1 a  where only the supports of a  c  and d are explicitly drawn. assume last x a y =b1 before mi. during mi  we need to find a new support for a 뫍 dx because b1 is deleted due to the propagation of z = c. assume last x a y  was updated by acs-1record to the support b1 before mi returns false. since p 뫋{z = c} is arc inconsistent  last x a y  is restored to be b1 by mi. in mi+1  a new support is needed for a 뫍 dx since b1 is deleted due to the propagation of z = d. acs-1record needs to check b1 and b1 before it finds the support b1. value b1 is present from mi to mi+1  and  a b1  뫍 cxy was checked in both mi and mi+1. the constraint check  a b1  뫍 cxy is a negative repeat at mi+1.
1	exploiting residual data
a key feature of acs-1 and acs-1record is that they are faithful to the respective ac algorithms. we will focus on acs and investigate new ways to make use of the fact that the methods of an acs algorithm are usually invoked many times  millions of times to solve a hard problem  by a search procedure.
1	acs-residue
in this section  we design a new algorithmacs-residue  listed in algorithm 1  that extends the ideas behind ac-1 and ac1. like acs-1record  acs-residue needs a data structure last x a y  for every cxy 뫍 c and a 뫍 dx. after acs-residue.init p1   last x a y  is initialized to be the first support of a with respect to cxy. at every invocation of acs-residue.try   or acs-residue.addinfer    when finding a support for a value a of dx with respect to cxy  acsresidue.hassupport x a y  first checks  line 1  if last x a y  is still present. if it is  a support is found. otherwise  it searches  line 1  the domain of y from scratch as ac1 does. if a new support is found  it will be used to update last x a y   line 1 . the method is called acs-residue because acs-residue.try   or acs-residue.addinfer   simply reuses the data left in the last   structure by the previous invocations of try   or addinfer  . unlike acs-1record  acsresidue does not maintain last   in backup   and restore  .

algorithm 1: acs-residue

-------------acs-residue-----------
acs-residue.init p1 {same as acs-1record.init p1 }
acs-residue.backup timestamp  x a  
1 backup the current domains  following timestamp  x a 
acs-residue.restore timestamp  x a  
1 restore the domains of p  following timestamp  x a 
acs-residue.hassupport x a y 
1 if last x a y  뫍 dy then return true else b뫹 head
1 whiletail do
1		if  a b  뫍 cxy then { last x a y  뫹b ; return true } 1 return false
-------------acs-resopt-----------
  initialize last  x a y  and stop  x a y 
1 return ac-1 p 
acs-resopt.backup timestamp  x a  
1 backup the current domains of p  following timestamp  x a 
acs-resopt.restore timestamp  x a  
1 restore the domains  following timestamp  x a 
acs-resopt.try x = a 
1  cxy 뫍 p.c and  a 뫍 dx  stop x a y  뫹 last x a y 
1 backup timestamp  x a  
1 delete all the values except a from dx  q 뫹{ y x |cyx 뫍 p.c }
1 if propagate q  then return true 1 else {restore timestamp  x a    return false }
acs-resopt.addinfer 
1  cxy 뫍 p.c and  a 뫍 dx  stop x a y  뫹 last x a y 
1 delete a from dx  q 뫹{ y x |cyx 뫍 p.c }
1 return propagate q 
acs-resopt.hassupport x a y 
1 b 뫹 last x a y  ; if b 뫍 dy then return true
1 while b 뫹cirsucc  do
1		if  a b  뫍 cxy then last x a y  뫹 b; return true 1 return false

theorem 1 acs-residue is correct with respect to any cis. node-forward and path-forward time complexity of acsresidue are o ed1  and o ed1  respectively while nodeforward and path-forward space complexity are both o ed . it fully avoids positive repeats but avoids only some negative repeats.
모compared with acs-1record  acs-residue has a better space complexity but a worse time complexity. acs-residue does not need to backup its internal data structures.
모example consider the example in the previous section. before mi  i.e.  acs-residue.try z = c   the problem is arc consistent and last x a y =b1. during mi  assume acsresidue updates last x a y  to be b1 before it returns false. after mi  only the deleted values are restored to the domains but nothing is done to last   structure and thus last x a y  is still b1  b1 is a residue in last   . in mi+1  i.e.  acsresidue.try z = d   when hassupport   tries to find a support for a of dx  it checks first last x a y . b1 is present and thus a support of a. in contrast  acs-1record.try z = d  looks for a support for a from b1 뫍 dy. through this example  it is clear that acs-residue can save some constraint checks that acs-1record can not save  the converse is also true obviously .
1	acs-resopt
acs-residue's node-forward complexity is not optimal. we propose another algorithm  acs-resopt  listed in algorithm 1   that has optimal node-forward complexity while using the residues in last  . the idea is to remember the residues in last x a y  by stop x a y   line 1 and line 1  at the beginning of try   and acs-resopt.addinfer  . then when hassupport x a y  looks for a support for a 뫍 dx and last x a y  =b  is not present  it looks for a new support after b  line 1   instead of the beginning of the domain. the search could go through the tail and back to the head and continue until encounter stop x a y . for simplicity  in line 1 of hassupport    the initial domain of the problem p is used: cirsucc a dy1  = succ head dy1  if succ a dy1  = tail; otherwise cirsucc a dy1  = succ a dy1 . in our experiment however  we implement hassupport   using the current domain.
theorem 1 acs-resopt is correct with respect to any cis. node-forward and path-forward time complexity are o ed1  and o ed1  respectively while node-forward and pathforward space complexity are both o ed . it fully avoids positive repeats but avoids only some negative repeats.
모example consider the constraint cxy in figure 1 b  before acs-resopt.try  . the supports of a are drawn explicitly in the graph. assume last x a y  = b1 before try  . acsresopt.try   will first set stop x a y =b1. assume in the following constraint propagation b1 is deleted due to other constraints on y. acs-resopt.hassupport x a y  will search a support for a after b1 and find the new support b1. assume b1 is later deleted by constraint propagation. acsresopt.hassupport x a y  will start from b1  go through the tail and back to the head  and finally stop at b1 because stop x a y =b1. no support is found for a and it will be deleted from the current domain.
1	acs with adaptive domain ordering
to explore the theoretical efficiency limits of acs  we propose the algorithmacs-ado with optimalnode-forwardand path-forward time complexity. acs-ado employs an adaptive domain ordering: a deleted value is simply restored to the end of its domain in restore    while in acs-1record  it is restored in regard of the total ordering on the initial domain. as a result  when finding a support by using last    it is sufficient for hassupport   to search to the end of the domain  rather than going back to the head of the domain as done by acs-resopt .
모acs-ado  listed in algorithm 1  needs the data structures lastp x a y   buf a y  for every cxy 뫍 p.c and a 뫍 dx. the content of lastp x a y  and buf a y  is a pointer p to a supporting node that has two components p뫺.bag and p뫺.for. if buf a y =p  p뫺.for is a and p뫺.bag is the set { b 뫍 dy | lastp y b x 뫺.for=a}. lastp x b y 뫺.for is a value of dy.

algorithm 1: acs-ado

acs-ado.init p1 
1  cxy 뫍 p.c and  a 뫍 dx  last  x a y  뫹 head
1  cxy 뫍 p.c and  a 뫍 dx 뫋 {tail} lastp  x a y  뫹null
1 flag 뫹 ac-1 p  // ac-1 will populate last
1 foreach cxy 뫍 p.c and each a 뫍 dx do
1b 뫹
1ifthen buf  b x  뫹createnode  b 
1add a to buf  b x 뫺.bag  lastp  x a y  뫹 buf  b x 
1 return flag
acs-ado.backup p  timestamp  x a  
1 backup the current domains  following timestamp  x a 
acs-ado.restore p  timestamp  x a  
1 foreach variable y 뫍 p.v do
1 restore the deleted values to the end of dy  following timestamp  x a 
1 let v be the first restored value
	1	foreach cxy 뫍 p.c do
1swap buf  v x  and buf  tail  x 
swap buf  v x 뫺.for and buf  v tail 뫺.for
acs-ado.remove b y 
1 c 뫹 succ  b dy 
1 if |buf  b x 뫺.bag| |buf  c x 뫺.bag| then 	swap buf  b x  and buf  c x   swap buf  b x 뫺.for and buf  b x 뫺.for
1 foreach v 뫍 buf  b x 뫺.bag do update  v c x y b 
1 delete b from dy
acs-ado.hassupport x a y 
1 b 뫹 lastp  x a y 뫺.for 1 if  a b  뫍 cxy then return true else b1 뫹 b 1 while b 뫹 succ b dy  and b= tail do 1		if  a b  뫍 cxy then {update  a b x y b1   return true } 1 update  a  tail  x y b1   return false
acs-ado.createnode b 
1 create a supporting node p such that p뫺.bag 뫹 {}  p뫺.for 뫹 b
1 return p
acs-ado.update a b x y b1 
1 delete a from buf  b1 x 뫺.bag
1 if buf  b x  = null then buf  b x  뫹createnode  b 
1 add a to buf  b x 뫺.bag  lastp  x a y  뫹 buf  b x 

acs-ado maintains on lastp x a y  the safety invariant  i.e.  there is no support of a before the value lastp x a y 뫺.for in dy  and the presence invariant  i.e.  lastp x a y 뫺.for is present in dy or the tail of dy. it also maintains the correspondence invariant  i.e.  lastp x a y 뫺.for=b if and only if a 뫍 buf b x 뫺.
모with the safety and presence invariants on lastp    to find a support of a 뫍 dx with respect to cxy  acs-
ado.hassupport x a y  starts from lastp x a y 뫺.for line 1  and stops by the tail of dy  line 1 . when a new support is found  lastp x a y  is updated  line 1  by update that guarantees the correspondence invariant  line 1 . acsado.hassupport assures the safety invariant on lastp. when removing a value  say b of dy  acs-ado.remove b y  finds the first present value c after b  line 1  and makes buf b x  point to the node with smaller bag  line 1 . it then updates  line 1  lastp x a y  for all a 뫍 buf b x 뫺.bag. in this way  we always update the lastp   structures for a smaller number of values. when restoring deleted values  for each variable y  acs-ado.restore timestamp x a  restores all the deleted values after timestamp x a  to the end of dy  line 1 . note that tail is greater than any values in dy. since there might be some values whose lastp x a y 뫺.for is tail of dy  we need to swap the supporting nodes in buf v x  and buf tail  x   line 1 .
모example figure 1 a  shows the data structures of the values of dx with respect to cxy. the nodes with 1 to 1 and a to d represent values of dy and dx. the value of a node disconnected from the linked list is not in the current domain. the nodes with area labelled by  bag  are supporting nodes.

figure 1: examples for remove   and restore  
the arrow from a supporting node  say p1  to the value node 1 means p1뫺.for=1  and the arrow from a value node  say 1  to the supporting node p1 implies buf 1 x =p1. an arrow from the lastp area of a value node  say a  to a supporting node p1 implies lastp x a y 뫺.for=p1. an arrow from the bag area of a supporting node  say p1  to a value node  say a  implies that a 뫍 p1뫺.bag. note that the details of lastp  buf respectively  structures of the values of dy  dx respectively  are omitted.
모assume value 1 is removed. since buf 1 x 뫺.bag is larger than that of value 1 that is the first present successor of 1  the method remove   swaps buf 1 x  and buf 1 x  and swap buf 1 x 뫺.for and buf 1 x 뫺.for. now a and b are pointing to value 1 through p1. then c of p1뫺.bag is made to point p1. figure 1 b  shows structures after the removal of 1.
모consider another data structures shown in figure 1 c . assume 1 needs to be restored to dy. since 1 is the first restored value  all values pointing to tail should now point to 1. the method restore   simply swaps buf 1 x  and buf tail  x  and swaps buf 1 x 뫺.for and buf tail x 뫺.for  figure 1 d  . in constant time  all the values previously pointing to tail are now pointing to 1 through the supporting node p1. 
proposition 1 consider any incremental sequence with n invocations  the cumulated worst-case time complexity of acs-ado.remove   in the sequence is o edlgd .
theorem 1 acs-ado is correct with respect to any cis. node-forward and path-forward time complexity are o ed1  while node-forward and path-forward space complexity are o ed . acs-ado fully avoids negative repeats but does not avoid any positive repeats.
1	experiments
the new acs algorithms are benchmarked on random binary constraint problems and radio link frequency assignment problems  rlfaps . the sample results on problems  n = 1 e = 1  at phase transition area are shown in figure 1 where the constraint checks are the average of 1 instances and the time is total amount for those instances. the experiments were carried out on a dell poweredge 1  two 1ghz intel xeon cpus  with linux 1.1. we use dom/deg variable ordering and lexicographical value ordering. the constraint check in our experiment is very cheap.

figure 1: experiments on random problems
모in terms of constraint checks  acs-1 and acs-ado are significantly worse than the rest. however  the difference among acs-1record  acs-resopt  and acs-residue is marginal. this shows the considerable savings led by the reuse of residual data given the fact that the node-forward complexity of acs-residue is not optimal. however  acsresopt is only slightly better than ac-residue although it improves the node-forward complexity of the latter to be optimal. acs-ado has the best theoretical time complexity among the new algorithms  but it has the worst experimental performance. one explanation is that it loses the benefit of residual support due to that fact that lastp x a y 뫺.for is guaranteed to be present but might not be a support of a. in summary  the use of residues bring much more savings than other complexity improvements of the new algorithms.
모since conducting roughly the same amount of constraint checks  the great simplicity of acs-residue makes it a clear winner over other algorithm in terms of clock time. the performance of acs1-resopt is very close to that of ac1record. since acs-ado uses a rather heavy data structure  its running time becomes the worst.
모the above observations also hold on rlfap problems  see the table below .  acs-resopt uses less number of checks than acs-1record but is the slowest because it conducts more domain checks than the others. 
rlfapacs-1acs-1recordacs-resoptacs-residueacs-adoscen1.1m1m1m1m1m1s1s1s1s1s1	related works and conclusions
we are not aware of any other work that has made a clear difference between standalone ac and ac during search. however  there does exist a number of works that began to look at specific algorithms to enforce ac in the context of a search procedure. lecoutre and hemery  1  confirmed the effectiveness of acs-residue in random problems and several other real life problems and extend it by multidirectionality of constraints. the work by regin  1  focuses specifically on reducing the space cost to embed an ac-1 based algorithm to mac while keeping its complexitythe same as that of standalone optimal ac algorithms on any branch of a search tree. however  experimental data has not been published for this algorithm.
모in this paper we propose to study acs as a whole rather than just an embedding of ac algorithm into a search procedure. some complexity measurements are also proposed to distinguishnew acs algorithms e.g.  acs-residue andacsresopt  although an implementation of acs using optimal ac algorithm with backup/restore mechanism  e.g.  acs1record  can always achieve the best node and path time complexity.
모a new perspective brought by acs is that we have more data to  re use and we do not have to follow exactly ac algorithms when designing acs algorithms. for example  acs-residue uses ideas from both ac-1 and ac-1 but also shows clear difference from either of them. the simplicity and efficiency of acs-residue vs. other theoretically more efficient algorithms reminds of a situation that occurred around 1 when it was found ac-1 empirically outperformed ac1  wallace  1  but finally the ideas behind them led to better ac algorithms. we expect that the effort on improving theoretical complexity of acs algorithms will eventually contribute to empirically more efficient algorithms.
모the success of acs-residue and our extensive empirical study suggest a few directions for the study of acs algorithms. 1  we need extensive theoretical and empirical study on possible combinations of the new acs algorithms and traditional filtering algorithms including ac-1. we have combined for example residue and ado  which significantly improved the performance of ado. 1  we notice in our experiments that the optimal complexity of acs-1record does not show significant gain over acs-residue even in terms of the number of constraint checks while efforts to improve acsresidue  e.g.  acs-resopt  gain very little. this phenomenon is worth of studying to boost the performance of acs algorithms. 1  the acs perspective provides fresh opportunities to reconsider the numerous existing filtering algorithms  including singleton arc consistency and global constraints  in the context of search.
1	acknowledgements
this material is based in part upon works supported by the science foundation ireland under grant 1/pi.1/c1 and by nasa under grant nasa-nng1gp1g.
