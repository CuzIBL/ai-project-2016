 
this paper presents an evaluation of two orthogonal schemes for improving the efficiency of solving constraint satisfaction problems  csps . the first scheme involves a class of pre-processing techniques designed to make the representation of the csp more explicit  including directional-arcconsistency  directional-path-consistency and adaptive-consistency. the second scheme aims at improving the order in which variables are chosen for evaluation during the search. in the first part of 
the experiment we tested the performance of backtracking  and its common enhancement -backjumping  with and without each of the preprocessings techniques above. the results show that directional arc-consistency  a scheme which embodies the simplest form of constraint recording  outperforms all other preprocessing techniques. the results of the second part of the experiment suggest that the best variable ordering is achieved by the fixed max-cardinality search order. 
1. introduction 
in this paper we report the results of two sets of experiments designed to evaluate several constraint-satisfaction algorithms. the first set is concerned with a class of preprocessing algorithms which transform a given constraint network into a more explicit representation before it is sub-
jected to a backtracking algorithm for solution. the three pre-processing algorithms tested were directional-arcconsistency  dac   directional-path-consistency  dpc   and adaptive-consistency  adapt   presented in  dechter 1 . these are variations of the well-known constraint-
*this work was supported in part by the national science foundation  grant #iri-1 and by the air force office of scientific research  grant #afosr-1. itay meiri 
cognitive systems laboratory 
computer science department 
university of california  los angeles  ca 1 
propagation  also called  relaxation  or  consistencyenforcing   techniques arc-consistency and path-
consistency  originally advocated by  montanari 1   
 waltz 1   and  mackworth 1   and their extension  /-consistency  freuder 1 . the preprocessing performed by each of these consistency-enforcing algorithms amounts to constraint recording  i.e.  the explicit recording of implicit constraints  with the aim of reducing the amount of post-processing required by the backtracking algorithm. they differ in the type and amount of implicit constraints they choose to record. the advantage of algorithms dac  dpc  and adapt is that they take into account the direction in which backtracking will eventually search the problem and  as a result  they avoid processing many constraints which are unnecessary for the search and which would have been processed and recorded by the earlier consistencyenforcing algorithms. 
　　worst-case analysis fails to reveal the merits and drawbacks of preprocessing techniques like those described above. with the exception of adapt  all the consistencyenforcing algorithms mentioned are of polynomial complexity and  therefore  negligible when compared to the exponential worst-case behavior of backtracking search. adapt records a set of constraints sufficient to guarantee a backtrack-free search   i.e.  linear post-processing complexity . its complexity is exponential in w*  a network parameter to be defined later  where w*  n   which is still bounded  in the worst case  by that of unprocessed backtracking. 
　　thus  worst-case analysis suggests that there is  nothing to lose  and ''everything to gain  by applying preprocessing before backtracking. unfortunately  worse-case analysis does not necessarily correlate with average case performance and  in practice  straightforward backtracking often performs best without the assistance of any consistency-enforcing algorithm. 
　　in order to shed some light on the practical utility of using algorithms dac  dpc  and adapt  we conducted experiments comparing their performance to that of backtracking and its advanced version  backjumping  dechter 1   on a set of randomly generated csps. the results  in general  show that the average complexity of solving 
	dechter and meiri 	1 
these problems by backtracking is far from exponential and  thus  the pre-processing performed by adapt and sometimes even by dpc  are too expensive  unless the graph is very sparse. algorithm dac  on the other hand  comes out as a clear winner. apparently  it performs the right amount of pre-processing for helping backtracking and  in most cases  it even outperforms backjumping  previously shown to be a very effective dialect of backtracking  dechter appear . 
　　the second set of experiments compares the performance of backtracking and backjumping under different ordcrings of variables. we tested three fixed heuristics for ordering: min-width  max-degree  and max-cardinality search  and one dynamic ordering named dynamic search rearangement  purdom 1 . the results suggest that the fixed max-cardinality ordering results in the smallest average time  while dynamic ordering is most effective in pruning the search space. 
　　section 1 presents the constraint network model and reviews algorithms dac  dpc and adapt. section 1 discusses different ordering heuristics. section 1 describes the methodology of the experiments and presents and analyzes results pertaining to the merits of pre-processing techniques and ordering heuristics. section 1 provides a summary and conclusions. 
1. directional pre-processing algorithms 
a constraint network  cn  involves a set of n variables  x1 ...  xn  their respective domains  r 1  ...  rn   and a set of constraints. a constraint ci xil        ixl  is a subset of the cartesian product rij-xrij that specifies which values of the variables are compatible with each other. a binary constraint network is one in which all the constraints are binary  i.e.  involving at most two variables. a cn may be associated with a constraint-graph in which nodes represent variables and arcs connect those pairs of variables which appear in the same constraints. for instance  the cn presented in figure la  describes the problem of coloring the nodes in a graph s.t. adjacent nodes have different colors. this is a binary cn whose variables are the nodes and whose values are the possible colors. a link represents the set of value-pairs permitted by the constraint between the variables it connects 
　　a solution of a constraint network is an assignment of values to all the variables such that all the constraints are satisfied. the classical constraint satisfaction tasks are to find one or all solutions. 
　　the following paragraphs present the tested algorithms. we start with adaptive-consistency  then generalize its operation to describe a class of pre-processing algorithms which include dac and dpc as special cases. few more definitions are needed for this discussion. given an ordering of the variables in a cn  for each variable  x  parents x  is the set of all variables connected to it and preceding it in the graph. the width of a node is the 

number of predecessors linked to that node. the width of an ordering is the maximum width of nodes in that ordering  and the width of a graph is the minimal width of all its orderings. for instance  given the ordering  e d c a b  of the graph in figure la  the width of node b is 1 while the width of this ordering is 1 and so is the width of this graph  see figure lb . adaptive-consistency  i.e.  adapt  process the nodes in reverse order  i.e.  each node is processed before any of its parents. 

　　the procedure record-constraint v set  generates and records those tuples of variables in set that can be consistent with at least one value of v. for instance if  in our example  a has only the color green in its domain and c and d each have two possible colors   red green   then the call for record-constraint a  {c d}  will result in recording a constraint on the variables c d  allowing only the pair  red rcd  in their relation. adapt may tighten existing constraints as well as impose constraints over clusters of variables. it was shown  dechter 1   that when the procedure terminates backtrack can solve the problem  in the order prescribed  without encountering any dead-end. the topology of the new induced graph can be found prior to executing the procedure  by recursively connecting any two parents sharing a common successor. 
unary constraint on d's domain  eliminating any of its 
1 	search 　　consider our example of figure la in the ordering  e d c a b  shown in figure lb. b is chosen first and since it has only one parent  d  the algorithm records a values which doesn't have a consistent match in b. variable a is processed next and a binary constraint is enforced on its two parents d and c  eliminating all pairs which have no common consistent match in a. this operation may require that a constraint arc be added between c and d. the algorithm proceeds in the same manner through the rest of the variables  taking into account both old and the newly generated constraints. the resulting graph  called induced graph  contains the dashed arc in figure lb. 
　　let w d  be the width of the ordering d and w*  d  the width of the induced graph along this ordering. the complexity of record-constraint v  parent v    step 1  is exponential in the cardinality of v and its parents. since the maximal size of the parent-sets is equal to the width of the induced graph and since this step dominates the whole computation  solving the problem along the ordering d is 1  n-exp w*  d +l    dechter 1 . 
　　the directional algorithms  i.e.  dac  dpc  and directional-i-consistency   differ from adapt only in the amount and size of constraint recording performed in step 1. namely  instead of recording one constraint among all the parent set  they record a few  smaller constraints on subsets of the parents. let level be a parameter indicating the utmost cardinality of constraints which are recorded. the class of algorithms adaptive  level  is henceforth described: 

　　adaptive level =1  reduces to dac while for level = 1 it becomes dpc. the graph induced by all these algorithms  excluding the case of level =1 where the graph doesn't change  has the same structure as the one generated by adaptive-consistency  clearly  adaptive level = w*  d   is the same as adaptive-consistency  and thus  is guaranteed to generate a backtrack-free solution. 
　　the complexity of adaptive /eve/  is both time and space dominated by the procedure new-record leve/  which is  this bound can be lightened if the ordering d results in a smaller w*  d . however  finding the ordering which has the minimum induced width is an np-complete problem. 
　　since backtracking and backjumping are used as postprocessing algorithms they also deserve a word. backtracking consistently assigns values to variables until either a solution is found or there is a deadend  i.e.  a variable has no value consistent with previous values . in that case backtracking goes back to the most recent instantiation  changes it and continues. backjumping improves the  goback** phase of backtracking and whenever a dead-end occurs at variable x  it backs up to the most recent variable connected to x in the constraint graph. this is a graphbased variant of gaschnig*s backjumping  gaschnig 1   and it was shown  dechter appear  that it outperforms backtracking on an instance by instance basis. 
1  the effects of variable ordering 
it is well known that the ordering of variables  be it fixed throughout search  or dynamic  may have a tremendous influence on the size of the search space explored by backtracking algorithms. finding an ordering which would minimize the search space is a difficult problem and  consequently  researchers have concentrated on devising heuristics for variable ordering. the best known dynamic ordering is the dynamic search rearangement  which was investigated analytically via average case analysis in  purdom 1  haralick 1  nudel 1  and experimentally in  stone 1  rosiers 1 . this heuristic selects as the next variable to be instantiated a variable that has a minimal number of values which are consistent with the current partial solution. heuristically  the choice of such variable minimizes the remaining search. deeper estimates of the remaining search space were also considered  purdom 1  zabih 1 . 
　　we consider three heuristics for fixed ordering of variables: the minimum width  the maximum degree  and the maximum cardinality heuristics. the minimum width heuristic  freuder 1   orders the variables from last to first by selecting  at each stage  a node in the constraint graph which has a minimal degree in the graph remaining after deleting from the graph all nodes which have been selected already. as its name indicates  the heuristic results in a minimum width ordering. the max-degree heuristic orders the variables in a decreasing order of their degrees in the constraint graph. this heuristic also aims at  but does not guarantee  finding a minimum-width ordering. 
　　the third heuristic is the max-cardinality search ordering. this ordering selects the first variable arbitrarily  then  at each stage it appends to the selected variables one which is connected to the largest group among the variables already selected. this heuristic can be thought of as the 
	dechter and meiri 	1 
fixed version of dynamic search rearangement: the next variable to be selected is the one which constraints with the largest number of already instantiated variables  namely it is the most constrained variable. 
1. experimental results 
we compared 1 algorithm combinations on our test problems. algorithms backtracking  btk  and backjumping  bj  were executed on each problem without any preprocessing and after pre-processing them by either directional-arc-consistency  directional-path-consistency and adaptive-consistency  1 combinations . each such combination was tested with each one of the fixed ordering heuristics  max-degree  max-cardinality search and minwidth heuristic   yielding 1 combinations . two more runs of backtracking and backjumping were performed in conjunction with dynamic ordering. 
　　the test problems were selected from a randomly generated csps. the random problems were created by generating random graphs and associating with each arc in the graph a randomly generated binary constraint. we purposely concentrated on parameters  e.g.  probability of an arc  which result in more difficult problems for backtracking. we chose to restrict the set of test problems to binary csps because problems with constraints of higher order tend to have denser constraint graphs for which the preprocessing algorithms have higher overhead. it should be pointed out  however  that adaptive consistency will add to the network non-binary constraints so the implementation of the backtracking and backjumping algorithms had to accommodate general  non-binary  csps. 
　　we experimented with two sets of random problems: one including 1 problem instances having 1 variables and 1 values and the other  including 1 problem instances  with 1 variables and 1 values. this set was selected from a much larger set of instances from which all the easy problems were deleted  and therefore  the given set represent the more difficult among such randomly generated problems. problems of larger size took too much time and space for our machine to handle  especially for algorithm adaptiveconsistency. 
　　we recorded the number of consistency-checks and the number of dead-ends  number of backtrackings  in each run. the number of consistency-checks is considered a 
　　realistic measure of the overall performance  while the number of backtrackings is indicative of the size of the search space exposed. 
　　each algorithm combination were run twice on each problem instance  once for finding one solution and the other for finding all solutions. the results were clustered into 1 groups corresponding to the two problem sizes  either 1 or 1 variables  and the three cases for which statistics were recorded  namely  finding one solution  called  first    finding all solutions  called  all    and for the cases that no solutions exist  called  failure  . 
1 	search 
1 evaluation of pre-processing algorithms 
our first interest is to compare the effect of the three preprocessing algorithms dac  dpc and adapt with backtracking and backjumping. since their relative behaviour w.r.t. the three fixed ordering schemes was found to be quite similar  and in order to save space  we report  in this part  the results of running them using max-degree pre-ordering only. 
　　figure 1 presents graphs of the average number of consistency checks  classified according to the width of the induced graph w*  for four of the six groups of instances. the results of the  first  and  failure  cases for problems size of 1 are omitted  for the sake of saving space  because they exhibit the same behavior as their counter parts of size 1. each pair of graphs describes the results of one of the groups  where the one on the left contrasts the results for algorithms adapt  btk and bj and the one on the right shows  using a different scale  the results of algorithms bj  dac and dpc. the results reported for dac  dpc  and adapt are for the cases were these algorithms were complemented by backjumping. the results when backtracking was used were very similar to those with backjumping since after the pre-processing most of the dead-ends were eliminated. 
　　comparing  first  adapt to btk and bj  left column in figure 1  we see that even on the average  adaptiveconsistency has an exponential behavior as a function of w*. btk and bj  on the other end  exhibit a much more moderate  maybe even linear  behavior. 
　　the average performance of adapt is better than btk only for small values of w* and when the task is to find all solutions. evidently  the amount of preparation performed by adapt is too heavy to be justified by just one solution  figure 1e   but when it is divided among several solutions  it becomes worth-while  figure 1c . for the case of n=1  when looking for one solution  btk outperformed adapt even for w* = 1  not shown in figure 1 . 
　　when compared to bj  however  adapt appears as a complete looser. bj outperformed adapt in every instance. clearly  bj exploits the structure of the problem in a more efficient way than adapt and should be preferred  especially considering the fact that it doesn't need the additional space which is consumed by adapt.  although adapt does not seem to be a sensible choice for a one time solution of a csp  it still can be used in order to find a better representation of a network of constraints  for example  when the network represents some knowledge-base on which many queries are to be answered over time. in such cases the work for generating the new representation can be ignored  dechter 1 .   
　　the disappointing results of adapt can be explained by comparing it with the two other  less ambitious  preprocessing algorithms  dac and dpc. when we counted the number of dead-ends left after pre-processing  not shown here   we found that in almost all problem instances 




figure 1: the effect of dac on the search space  case n=1  
algorithm dpc alone eliminated all future dead-ends. it is clear  therefore  that for problem instances of this type  adapt is doing unnecessary pre-processing. moreover  the number of dead-ends left by dac alone  see figure 1  shows that most of the work is accomplished by this algorithm which performs the smallest amount of constraint recording. 
in summary  the two algorithms that stand out in the experiments are bj and dac. furthermore  the performance of dac followed by bj is better than executing just bj. among the other three  dpc comes next  and the relationship between btk and adapt is dependent on w*. 
1 evaluation of the effects of variable ordering 
 we now wish to focus on the effects of of the three fixed ordering and the dynamic ordering on various algorithm combinations  and in particular on backtracking and backjumping. figure 1 presents the results of comparing backtracking and backjumping on  because of space limitations  four of the six groups and for each of the four ordering schemes: 1. the max-degree  max   1. min-width ordering  min   1. max-cardinality ordering  card   and 1. dynamic ordering  dnmic . in contrast with dynamic search rearangement  fixed ordering schemes were never evaluated experimentally. to maintain continuity we averaged the same set of instances  and therefore w* indicates the induced width of max-degree's ordering. 


figure 1: effects of variable-ordering rules 
1 	search 

　　in our experiments  the card ordering scheme gave the best results in most of the cases. it seems to outperform other fixed orderings and even the dynamic ordering. it is particularly clear for the task of finding all solutions  while for first solution min-width comes quite as good. however  when we compared the number of dead-ends associated with each ordering  it becomes clear that dynamic ordering is expanding the smallest search space  i.e.  it has the least number of deadends on an instance by instance basis  almost  these results are not graphed here . therefore  had we better implemented this technique we may have a better overall performance. indeed in the current implementation no data-structure was used to alleviate redundant consistency checks as was done in other look-ahead schemes like forward-checking  haralick 1 . 
1. summary and conclusions 
we evaluated the performance of several backtracking techniques for solving csps. first  we tested the effect of various pre-processing algorithms on backtracking and backjumping  under fixed ordering  and concluded that directional arc-consistency followed by backjumping yields the best improvement second  we tested the effect of four variable ordering schemes and concluded  quite convincingly that the max-cardinality order yields the least amount of computation. it is expected  therefore  that directional are-consistency with backjumping on the max-cardinality ordering will yield the best results. indeed  when we compared all algorithms  i.e.  btk  bj  adapt  dac  dpc  on all the three fixed orderings  i.e.  max  min  card  the combination of dac-card gave the best performance on three of the six groups tested and in the rest it came as a close second best after dac-min. averaging over the 1 groups of instances  dac-card was best. this suggest that the best approach is to choose max-cardinality ordering and to perform directional arc-consistency followed by backjumping. 
　　these results  however  should be qualified in two ways. first  the conclusions are valid only relative to problem domains with statistics similar to those used in generating our test samples. second  the superior pruning power of dynamic ordering suggests that further performance improvements could be realized by a more sophisticated implementation of this technique. 
