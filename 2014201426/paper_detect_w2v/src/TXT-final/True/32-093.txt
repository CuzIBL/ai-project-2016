 
an issue that is critical for the application of markov decision processes  mdps  to realistic problems is how the complexity of planning scales with the size of the mdp. in stochastic environments with very large or even infinite state spaces  traditional planning and reinforcement learning algorithms are often inapplicable  since their running time typically scales linearly with the state space size in this paper we present a new algorithm that  given only a generative model  simulator  for an arbitrary mdp  performs near-optimal planning with a running time that has no dependence on the number of states. although the running time is exponential in the horizon time  which depends only on the discount factor 1 and the desired degree of approximation to the optimal policy   our results establish for the first time that there are no theoretical barriers to computing near-optimal policies in arbitrarily large  unstructured mdps. 
our algorithm is based on the idea of sparse sampling. we prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an mdp. practical implementations of the algorithm are discussed  and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable mdps  kmn1 . 
1 	introduction 
in the past decade  markov decision processes  mdps  and reinforcement learning have become a standard framework for planning and learning under uncertainty within the artificial intelligence literature. the desire to attack problems of increasing complexity with this formalism has recently led researchers to focus particular attention on the case of  exponentially or even infinitely  large state spaces. a number of interesting algorithmic and representational suggestions have been made for coping with such large mdps. function approximation  is a well-studied approach to learning value functions in large state spaces  and many authors have recently begun to study the properties of large mdps that enjoy compact representations  such as mdps in which the state transition probabilities factor into a small number of components  mhk+1 . 
　in this paper  we are interested in the problem of computing a near-optimal policy in a large or infinite mdp that is given - that is  we are interested in planning. it should be clear that as an mdp becomes very large  the classical planning assumption that the mdp is given explicitly by tables of rewards and transition probabilities becomes infeasible. one approach to this difficulty is to assume that the mdp has some special structure that permits compact representation  such as the factored transition probabilities mentioned above   and to design special-purpose planning algorithms that exploit this structure. 
　here we take a rather different approach. we consider a setting in which our planning algorithm is given access to a generative model  or simulator  of the mdp. informally  this is a  black box  to which we can give any state-action pair  and receive in return a randomly sampled next state and reward from the distributions associated with  generative models are a natural way in which a large mdp might be specified  and are more general than most structured representations  in the sense that many structured representations  such as factored models  usually provide an effi-
cient way of implementing a generative model. note also that since a generative model provides less information than explicit tables of probabilities  but more information than a single continuous trajectory of experience generated according to some exploration policy  results obtained via a generative model blur the distinction between what is typically called  planning  and  learning  in mdps. 
　our main result is a new algorithm that accesses the given generative model to perform near-optimal plansing in an  on-line  fashion from any given state s  the algorithm samples the generative model for many different state-action pairs  and uses these samples to 

1 	uncertainty and probabilistic reasoning 

compute a near-optimal action from s: the amount of time required to compute a near-optimal action from any particular state s has no dependence on the number of states in the mdp  even though the next-state distributions from s may be very diffuse  that is  have large support . the key to our analysis is in showing that appropriate sparse sampling suffices to construct enough information about the environment near s to compute a near-optimal action. the analysis relies on a combination of bellman equation calculations  which are standard in reinforcement learning  and uniform convergence arguments  which are standard in supervised learning; this combination of techniques was first applied in  ks1 . as mentioned  the running time required at each state does have an exponential dependence on the horizon time  which we show to be unavoidable without further assumptions. however  our results leave open the possibility of an algorithm that runs in time polynomial in the accuracy parameter  which remains an important open problem. 
　note that one can view our planning algorithm as simply implementing a  stochastic  policy - a policy that happens to use a generative model as a subroutine. in this sense  if we view the generative model as providing a  compact  representation of the mdp  our algorithm provides a correspondingly  compact  representation of a near-optimal policy. we view our result as complementary to work that proposes and exploits particular compact representations of mdps  with both lines of work beginning to demonstrate the potential feasibility of planning and learning in very large environments. 
　the remainder of this paper is structured as follows: in section 1  we give the formal definitions needed in this paper. section 1 then gives our main result  an algorithm for planning in large or infinite mdps  whose per-state running time does not depend on the size of the state space. finally  section 1 describes related results and open problems. 
1 	preliminaries 
we begin with the definition of a markov decision process on a set of  states  explicitly allowing the possibility of the number of states being  countably or uncountably  infinite. 
definition 1 a markov decision process m on a set of states  and with actions  consists of: 
  transition probabilities. for each state-action pair  a next-state distribution  that specifies the probability of transition to each state $' upon execution of action a from state  
  reward distributions; for each state-actton pair  a distribution  on real-valued rewards for executing action a from state s. we assume rewards are bounded in absolute value by  
　for simplicity  we shall assume in this paper that all rewards are in fact deterministic - that is  the reward distributions have zero variance  and thus the reward received for executing a from s is always exactly however  all of our results have easy generalizations for the case of stochastic rewards  with an appropriate and necessary dependence on the variance of the reward distributions. 
　throughout the paper  we will primarily be interested in mdps with a very large  or even infinite  number of states  thus precluding approaches that compute directly on the full next-state distributions. instead  we will assume that our planning algorithms are given m in the form of the ability to sample the behavior of m. thus  the model given is simulative rather than explicit. we call this ability to sample the behavior of m a generative model. 
definition 1 a generative model for a markov decision process m is a randomized algorithm that  on input of a state-action pair  outputs and a state  
where  is randomly drawn according to the transition 
probabilities  
　we think of a generative model as falling somewhere in between being given explicit next-state distributions  and being given only  irreversible  experience in the mdp  in which the agent follows a single  continuous trajectory  with no ability to reset to any desired state . on the one hand  a generative model may often be available when explicit next-state distributions are not; on the other  a generative model obviates the important issue of exploration that arises in a setting where we only have irreversible experience. in this sense  planning results using generative models blur the distinction between what is typically called  planning  and what is typically called  learning . 
　following standard terminology  we define a  stochastic  policy to be any mapping  
thus  may be a random variable  but depends only on the current state s. we will be primarily concerned with discounted reinforcement learning l  so we assume we are given a number  called the discount factor  with which we then define the value function  for any policy  
where step of exe-
cuting the policy  from state s  and the expectation is over the transition probabilities and any randomization 
in  note that for any s and any 
where we define 
	we also define the q-function for a given policy 	as 
		 1  

however  our results can be generalized to the undis-
counted finite-horizon case for any fixed horizon h  ms1a . 
	kearns  mansour  and ng 	1 

 where the notation means that is drawn according to the distribution we will later de-
scribe an algorithm a that takes as input any state s and  stochastically  outputs an action a  and which therefore implements a policy. when we have such an algorithm  we will also write  and  to denote the value function and function of the policy implemented by finally  we define the optimal value function and the optimal function as and 
and the optimal policy  for all  
1 	planning in large or infinite mdps 
usually one considers the planning problem in mdps to be that of computing a near-optimal policy  given as input the transition probabilities and the rewards 
  for instance  by solving the mdp for the optimal policy . thus  the input is a complete and exact model  and the output is a total mapping from states to actions. without additional assumptions about the structure of the mdp  such an approach is clearly infeasible in very large state spaces  where even reading all of the input can take n1 time  and even specifying a general policy requires space on the order of n. in such mdps  a more fruitful way of thinking about planning might be an online view  in which we examine the per-state complexity 
of planning. thus  the input to a planning algorithm would be a single state  and the output would be which single action to take from that state. in this on-line view  a planning algorithm is itself simply a policy  but one that may need to perform some nontrivial computation at each state . 
　our main result is the description and analysis of an algorithm a that  given access to a generative model for an arbitrary mdp m  takes any state of m as input and produces an action as output  and meets the following performance criteria: 
  the policy implemented by a is near-optimal in m; 
  the running time of a  that is  the time required to compute an action at any state  has no dependence on the number of states of m. 
　this result is obtained under the assumption that the input state to a requires only  space  a standard assumption known as the uniform cost model that is typically adopted to allow analysis of algorithms that operate on real numbers  such as we require to allow infinite state spaces . the uniform cost model essentially posits the availability of infinite-precision registers  and constant-size circuitry for performing the basic arithmetic operations on these registers . if one is unhappy with this model  then algorithm a will suffer a dependence on the number of states only equal to the space required to name the states  for 
n states . 
1 	a sparse sampling planner 
here is our main result: 
theorem 1 there is a randomized algorithm a that  given access to a generative model for any mdp m  takes as input any state  and any value  outputs an action  and satisfies the following two conditions: 
. efficiency  the running time of a is where 
 1  
 1  
 1  
in particular  the running time depends only on and does not depend on if we view as a constant  the running time bound can also be written 
		 1  
   near-optimality  the value function of the stochastic policy implemented by a satisfies 
 1  
simultaneously for all states  
　as we have already suggested  it will be helpful to think of algorithm a in two different ways. on the one hand  a is an algorithm that takes a state as input and has access to a generative model  and as such we shall be interested in its resource complexity - its running time  and the number of calls it needs to make to the generative model  both per state input . on the other hand  a produces an action as output in response to each state given as input  and thus implements a  possibly stochastic  policy. 
　the proof of theorem 1 is given in appendix a  and detailed pseudo-code for the algorithm is provided in figure 1. we now give some high-level intuition for the algorithm and its analysis. 
　given as input a state s the algorithm must use the generative model to find a near-optimal action to perform from state s. the basic idea of the algorithm is to sample the generative model from states in the  neighborhood'* of $. this allows us to construct a small  subof m such that the optimal action in  from s is a near-optimal action from s in  there will be no guarantee that  will contain enough information to compute a good action from any state other than s. however  in exchange for this limited applicability  the 
mdp  will have a number of states that does not depend on the number of states in  
　the graphical structure of will be given by a directed tree in which each node is labeled by a state  and each directed edge to a child is labeled by an action and 
	will not literally be a sub-mdp of 	m in the sense of 
being strictly embedded in m  due to the variations of random sampling. but it will be very  near  such an embedded mdp. 

1 	uncertainty and probabilistic reasoning 

function:  
a generative model g  state  
	output: a list 	of estimates of the  
1- if 
	1. for each 	samples from the next-state d i s t r i b u t i o n l e t 
	sa be a set containing these 	next-states. 	 
1
 
figure 1: algorithm a for planning in large or infinite state spaces. estimatev finds the described in the text  and estimateq finds analogously defined  algorithm a implements the policy. 

a reward. for the sake of simplicity  let us consider only the two-action case here  with actions  and  each node will have children in which the edge to the child is labeled and children in which the edge to the child is labeled  
　　the root node of is labeled by the state of interest 1  and we generate the children of s in the obvious way: we call the generative model  times on the stateaction pair to get the -children  and o n t i m e s  on to get the -children. the edges to these children are also labeled by the rewards returned by the generative model  and the child nodes themselves are labeled by the states returned. we will build this ary tree to some depth to be determined. note that is essentially a sparse look-ahead tree. 
　　we can also think of  as an mdp in which the start state is s  and in which taking an action from a node in the tree causes a transition to a  uniformly  random child of that node with the corresponding action label; the childless leaf nodes are considered absorbing states. under this interpretation  we can compute the optimal action to take from the root s in  figure 1 shows a conceptual picture of this tree for a run of the algorithm from an input state s1  for  c will typically be much larger.  from the root we try action  three times and action  three times. from each of the resulting states  we also try each action times  and so on down to depth h in the tree. zero values assigned to the leaves then correspond to our estimates of  which are  backed-up  to find estimates of for their parents  which are in turn backed-up to their parents  and so on  up to the root to find an estimate of  
　　the central claim we establish about is that its size can be independent of the number of states in m  yet still result in our choosing near-optimal actions at the root. we do this by establishing bounds on the required depth h of the tree and the required degree  
recall that the optimal policy at s is given by 
arg maxa  and therefore is completely determined by  and easily calculated from   estimating the  is a common way of planning in mdps. from the standard duality between functions and value functions  the task of estimating -functions is very similar to that of estimating value functions. so while the algorithm uses the -function  we will  purely for expository purposes  actually describe here how we 
estimate  
　there are two parts to the approximation we use. first  rather than estimating  we will actually estimate  for a value of h to be specified later  the /t-step 
	kearns  mansour  and ng 	1 

	 1  
where  is the reward received on the ith time step upon executing the optimal policy from  moreover  we see that the for are recursively given by 
 1  
where is the action taken by the optimal policy from state and the quality of the approximation in equation  1  becomes better for larger values of  and is controllably tight for the largest value 
we eventually choose. one of the main efforts in the proof is establishing that the error incurred by the recursive application of this approximation can be made controllably small by choosing h sufficiently large. 
　thus  if we are able to obtain an estimate of  for any  we can inductively define an algorithm for finding an estimate  by making 
use of equation  1 . our algorithm will approximate the expectation in equation  1  by a sample of c random next states from the generative model  where c is a parameter to be determined  and which  for reasons that will become clear later  we call the  width  . recur-
sively  given a way of finding the estimator 	for any  we find our estimate 	as follows: 
1. for each action a  use the generative model to get and to sample a set of c independently sam-
pled states from the next-state distribution  
1.use our procedure for 	finding 	to estimate 
 for each state  in any of the sets  
1. following equation  1   our estimate of is then given by 
		 1  
　to complete the description of the algorithm  all that remains is to choose the depth h  depth  and c  which controls the width of the tree. bounding the required depth h is the easy and standard part. it is not hard to see that if we choose depth  
 the so-called c-horizon time   then the discounted sum of the rewards that is obtained by considering rewards beyond this horizon is bounded by  
　the central claim we establish about c is that it can be chosen independent of the number of states in m  yet still result in choosing near-optimal actions at the root. the key to the argument is that even though small samples may give very poor approximations to the next-state distribution at each state in the tree  they will  nevertheless  give good estimates of the expectation 

figure 1: sparse look-ahead tree of states constructed by the algorithm.  shown with c = 1  actions a1  a1.  
terms of equation  1   and that is really all we need. for this we apply a careful combination of uniform convergence methods and inductive arguments on the tree depth. again  the technical details of the proof are in appendix a. 
　in general  the resulting tree may represent only a vanishing fraction of all of the h-step paths starting from so that have non-zero probability in the mdp - that is  the sparse look-ahead tree covers only a vanishing part of the full look-ahead tree. in this sense  our algorithm is clearly related to and inspired by classical look-ahead search techniques  rn1 . our main contribution is in showing that in very large stochastic environments  clever random sampling suffices to reconstruct nearly all of the information available in the  exponentially or infinitely  large full look-ahead tree. note that in the case of deterministic environments  where from each state-action pair we can reach only a single next state  the sparse and full trees coincide  assuming a memoization trick described below   and our algorithm reduces to classical deterministic look-ahead search. 
1 	practical issues and lower bounds 
even though the running time of algorithm a does not depend on the size of the mdp  it still runs in time exponential in the e-horizon time h  and therefore exponential in  it would seem that the algorithm would be practical only if  is not too close to 1. in a 
moment  we will give a lower bound showing it is not possible to do much better without further assumptions on the mdp. nevertheless  there are a couple of simple tricks that may help to reduce the running time in certain cases  and we describe these tricks first 
　the first idea is to allow different amounts of sampling at each level of the tree. the intuition is that the further we are from the root  the less influence our estimates will have on the q-values at the root  due to the discounting . thus  we can sample more sparsely at deeper levels of the tree without having too adverse an impact on our approximation. 
we have analyzed various schemes for letting the 

1 	uncertainty and probabilistic reasoning 

amount of sampling at a node depend on its depth. none of the methods we results in a running time which is polynomial in however  one specific scheme that reduces the running time significantly is to let the number of samples per action at depth i be where the parameter c now controls the amount of sampling done at the root. the error in the q-values using such a scheme does not increase by much  and the running time is the square root of our original running time. 
　another way in which significant savings might be achieved is through the use of memoization in our subroutines for calculating the  in figure 1  this means that whenever there are two nodes at the same level of the tree that correspond to the same state  we collapse them into one node  keeping just one of their subtrees . while it is straightforward to show the correctness of such memoization procedures for deterministic procedures  one must be careful when addressing randomized procedures. we can show that the important properties of our algorithm are maintained under this optimization. 
　in implementing the algorithm  one may wish not to specify a targeted accuracy  in advance  but rather to try to do as well as is possible with the computational resources available. in this case  an  iterative-deepening  approach may be taken. this would entail simultaneously increasing c and h by decreasing the target e. also  as studied in davies et. al.  dnm1   if we have access to an initial estimate of the value function  we can replace our estimates at the leaves with the estimated value function at those states. though we shall not do so here  it is again easy to make formal performance guarantees depending on c  h and the supremum error of the value function estimate we are using. 
　unfortunately  despite these tricks  it is not difficult to prove a lower bound that shows that any planning algorithm with access only to a generative model  and which implements a policy that is to optimal in a general mdp  must have running time at least exponential in the e-horizon time. we now describe this lower bound. 
theorem 1 let a be any algorithm that is given access only to a generative model for an mdp m  and inputs 
  a state in m  and  let the stochastic policy implemented by a satisfy 
 1  
simultaneously for all states then there exists an mdp m on which a makes at least  
calls to the generative model. 
	  	con-
sider a binary tree t of depth h. we use t to define an mdp in the following way. the states of the mdp are the nodes of the tree. the actions of the mdp are 
　　　　　when we are in state s and perform an action we reach  deterministically  state  where  is the child of s in t. if s is a leaf of t then we move to an absorbing state. we choose a random leaf in the tree. the reward function for and any action is . and the reward at any other state and action is zero. 
　algorithm a is given  the root of t. for algorithm a to compute a near optimal policy  it has to  find  the node  and therefore has to perform at least calls to the generative model. 
1 	summary and related work 
we have described an algorithm for near-optimal planning from a generative model  that has a per-state running time that does not depend on the size of the state space  but which is still exponential in the e-horizon time. an important open problem is to close the gap between our lower and upper bound. our lower bound shows that the number of steps has to grow polynomially in  while in the upper bound the number of steps grows sub-exponentially in  more precisely 
 closing this gap  either by giving an algorithm that would be polynomial in or by proving a better lower bound  is an interesting open problem. 
　two interesting directions for improvement are to allow partially observable mdps  pomdps   and to find more efficient algorithms that do not have exponential dependence on the horizon time. as a first step towards both of these goals  in a separate paper  kmn1  we investigate a framework in which the goal is to use a generative model to find a near-best strategy within a restricted class of strategies for a pomdp. typical examples of such restricted strategy classes include limited-memory strategies in pomdps  or policies in large mdps that implement a linear mapping from state vectors to actions. our main result in this framework says that as long as the restricted class of strategies is not too  complex   where this is formalized using appropriate generalizations of standard notions like vc dimension from supervised learning   then it is possible to find a near-best strategy from within the class  in time that again has no dependence on the size of the state space. if the restricted class of strategies is smoothly parameterized  then this further leads to a number of fast  practical algorithms for doing gradient descent to find the near-best strategy within the class  where the running time of each gradient descent step now has only linear rather than exponential dependence on the horizon time. 
　another approach to planning in pomdps that is based on the algorithm presented here is investigated by mcauester and singh  who show how the approximate belief-state tracking methods of boyen and roller  can be combined with our algorithm. 
acknowledgements 
we thank david mcauester  satinder singh and rich 
sutton for many enlightening discussions and numerous insights on the ideas presented here. 
	kearns  mansour  and n-g 	1 

