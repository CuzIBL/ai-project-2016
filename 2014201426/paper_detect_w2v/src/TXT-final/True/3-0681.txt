
many practical problems have random variables with a large number of values that can be hierarchically structured into an abstraction tree of classes. this paper considers how to represent and exploit hierarchicalstructureinprobabilisticreasoning. we representthedistributionforsuchvariablesbyspecifying  for each class  the probability distribution over its immediate subclasses. we represent the conditional probability distribution of any variable conditioned on hierarchical variables using inheritance. we present an approach for reasoning in bayesian networks with hierarchically structured variablesthatdynamicallyconstructsaflatbayesian network  given some evidence and a query  by collapsing the hierarchies to include only those values necessary to answer the query. this can be done with a single pass over the network. we can answer the query from the flat bayesian network using any standard probabilistic inference algorithm such as variable elimination or stochastic simulation. the domain size of the variables in the flat bayesian network is independent of the size of the hierarchies; it depends on how many of the classes in the hierarchies are directly associated with the evidence and query. thus  the representation is applicable even when the hierarchy is conceptually infinite.
1 introduction
many problem domains have discrete variables with a large numberofvaluesthatcanberepresentedaprioriasanabstraction hierarchy  or an is-a hierarchy or taxonomic hierarchy   pearl  1; mackworth et al.  1 . we call these variables hierarchical variables. taxonomic hierarchies allow us to manage effectively the large state space of a variable because they allow facts and regularities to be revealed both at high and low levels of abstraction. in taxonomic hierarchies theinformationfromhigh-levelabstractionsareautomatically inherited by more specific concepts. abstraction hierarchies also allow us to answer more abstract queries. as an example of a hierarchical variable  consider a random variable lt that describes the species of living things. the large number of values of lt  i.e.  millions of species  can be classified according to linnaean taxonomy hierarchy.1 living things are divided into kingdoms  e.g.  plantae  animalia   classes  e.g.  mammals  birds  fish   all the way down to species.
　in this paper  we look at two related problems with hierarchicalvariablesinbayesiannetworks. thefirstisthecompact representation of conditional probability distributions. the second is how to exploit that representation in probabilistic inference for computational gain. we assume that the tree hierarchy of the values is fixed and does not change with the context.
　to compute a posterior probability  given some evidence in a bayesian network that has both simple and hierarchical variables  we construct a flat bayesian network by collapsing the hierarchies and including only those values necessary to answer the query. we can answer the query from the flat bayesian network using any standard inference algorithm. thedomainsizeofthevariablesintheflatbayesiannetworkis independent of the size of the hierarchies; it depends on how many of the classes in the hierarchy are directly associated with the evidence and query.
1 hierarchical variables
we divide the discrete random variables into two categories: simple variables and hierarchical variables. we call bayesian networks that have both simple and hierarchical variableshierarchicalbayesiannetworks.1 weuseuppercase letters to denote simple random variables  e.g.  x1  x1  x  and the actual value of these variables by small letters  e.g.  a  b  x1 . the domain of x  written val  x   is the set of values that x can take on.
　a hierarchical variable is a variable in which subsets of the values of the variable are represented hierarchically in a tree  pearl  1 . we refer to a node in the tree as a class. the nodesinthetreerepresentsubsetsofthevaluesofthevariable. the root of the tree represents the set of all the values of the variable. the children of a node correspond to a partitioning of the set of values of the node. thus  the subsets represented by the children of a class must be mutually disjoint and any class represents the union of its children in the tree.

figure 1: part of a taxonomic hierarchy of living things
　if a class cj is a child of ci  we say that cj is an immediate subclass of ci. the subclass relation is the symmetric transitive closure of the immediate subclass relation. that is  cj is a subclass of ci  written cj ＋ ci  if cj is a descendant of ci in the abstraction hierarchy  or is ci . the inverse of subclass is superclass. the conjunction of two classes cj and ck  written as cj … ck  denotes the set intersection of cj and ck. this is empty unless one is a superclass of the other.
　we denote the hierarchical variables using boldface uppercaseletters e.g.  lt . forexample  apartofthetreehierarchy ofvaluesofvariableltisshowninfigure1. tree lt denotes the tree hierarchy of lt.
1 conditional probabilities of hierarchical bayesian networks
there are two main issues to be considered to represent hierarchical bayesian networks:
c1: specifying the conditional probability for hierarchical variables;
c1: specifying how variables are conditioned on hierarchical parents.
　the simplest case of c1 is how to specify the prior probability of a hierarchical variable. the simplest case of c1 is how to specify the conditional probability of a simple variable conditioned on a hierarchical variable. the more complicated cases are built from these simpler cases.
1 prior probability for hierarchical variables
the prior probability of a hierarchical variable can be specified in a top-down manner. for each class that has subclasses  we specify a probability distribution over its immediate subclasses. for example  suppose we want to specify the prior over the hierarchical variable lt as shown in figure 1. the root of the hierarchy is the class livingthing. we specify the probability distribution over its immediate subclasses:
p animal|livingthing  = 1
p plant|livingthing  = 1
we can specify the probability distribution over the immediate subclasses of animal. suppose we have as part of this distribution:
p mammal|animal  = 1 p bird|animal  = 1  etc.
we can specify the probability of an immediate subclass of mammal given mammal  with probabilities such as: p bat|mammal  = 1
we can compute the prior probability of any class in a recursive manner by multiplying the probability of class given its immediate superclass and the probability of its immediate superclass. the root has probability 1  as it represents the set of all values . for example  given the probabilities as above  p bat  can be computed as follows:
	p bat 	=	p bat|mammal  〜 p mammal|animal  〜
p animal|livingthing 
	=	1
in this representation  computing the probability of any class is linear in the depth of the class in the hierarchy and otherwise is not a function of the hierarchy's size.
1 hierarchical variable is the parent of a simple variable
suppose that h is a hierarchical variable  f is a simple variable  and h is the only parent of f. we can exploit any hierarchical structure by assuming that the influence on f is local in the hierarchy of h. to specify the conditional probability distribution  cpd  p f|h  we use the notion of a default probability distribution. the default probability distribution of f for class cj  written pd f|cj   is assumed to be inherited by all subclasses of cj  unless overridden by a more specific default probability distribution. we can represent p f|h  by specifying pd f|cj  for some classes cj in the hierarchy of h. we call such classes cj the exceptional classes of h. the idea of using  inheritance  with abstraction hierarchies is similar to the use of  context specific independence  for the compact representation of the cpts in a bayesian network  boutilier et al.  1 .
　to make this properly defined  we need to ensure that every terminal node in the tree hierarchy of h is covered by some default probability distribution. the union of all of the sets of values associated with the exceptional classes must equal val h . this condition holds trivially if the root class is an exceptional class for f. if the root class is not an exceptional class  theremustbeanexceptionalclassalongeverypathfrom the root.
　the conditional probability of f for any class ck of h  p f|ck   can be computed as follows:
  if ck does not have any exceptional subclasses  p f|ck  is inherited from the default probability distribution of f for the lowest exceptional superclass cj of ck. thus 
p f|ck  = pd f|cj 
  otherwise  p f|ck  can be derived from its children
	p f|ck  =		p f|ci  〜 p ci|ck 
 ci（children ck 
example: consider a bayesian network in which a hierarchical variable lt is the only parent of a simple variable flying with domain {flying  flying}. to represent p flying|lt   we need to define the default probability distributions over flying for the exceptional classes of lt. for example  we can state that livingthings have a low probability of flying by default  but bird and insect are exceptional because they have a high probability of flying. from the children of class mammal  a bat is exceptional because it has a high probability of flying. from the children of class bird  a penguin is exceptional because it has a very low probability of flying. thus  to represent p flying|lt  we could have:
pd flying|livingthing  = 1
pd flying|bird  = 1
pd flying|bat  = 1
pd flying|penguin  = 1
pd flying|insect  = 1
note that p flying|livingthing  = 1 as the class livingthing contains bird and bat that each have a much higher probability of flying.
1 the general case
the general case is when a random variable  hierarchical or simple  can have any number and any kind of variables as parents. to represent the cpd of a hierarchical variable h conditioned on its parents  we assume that each class in the hierarchyofhhasaprobabilitydistributionoveritsimmediate subclasses that is conditioned on  some of  the parents of h. we can treat each of these classes as a simple variable. thus  the problem of representing a  conditional  probability distribution over a hierarchical variable reduces to the problem of representing a  conditional  probability distribution over simple variables.
　let f be a simple variable that has both simple and hierarchical parents. suppose pas f  denotes the simple parents of f and suppose pah f  denotes the hierarchical parents of f. let x denote an assignment of a class to each hierarchical parent of f; we call x a parent context. x =   where cxi is a class in the tree hierarchy of the ith hierarchical parent of f.
　torepresentp f|pas wespecifythedefault probability distribution over f for different combinations of values for simple parents  pas f   and some parent contexts. the given parent contexts  parent contexts for which the default distribution over f has been defined  can be represented in a concept hierarchy cp of partial ordering cp =  zp ＋   where zp is the set of given parent contexts. a parent context x （ zp is a super parent context of parent context y  written y ＋ x  if  i  cyi ＋ cxi . the conditional distribution over f for a parent context x is inherited from the default probability distribution of f for the1 super parent context z of x in zp that is not overridden by a more specific default probability distribution of f for parent context y such that x ＋ y   z. example: consider the bayesian network shown in figure 1

	 a 	part of tree hierarchy of location on the earth
 b 
figure1:  a abayesiannetworkwithsimpleandhierarchical variables  b  a part of the tree hierarchy for loc
 a . the hierarchical variable loc represents the location on earth. the simple variable s represents the season. figure 1  b showsthepartofthehierarchyofthelocation. weconsider that s has only two values:  northern hemisphere  summer  may - october  and winter  november - april . the probability distribution of class animal of lt over its immediate subclasses can be conditioned on some of its parents. suppose the distribution of class animal in the equatorial region is independent of season  so for parent context x =  equatorial  we could have a default distribution such as:
pd reptile|animal … x  = 1 pd insect|animal … x  = 1
pd mammal|animal … x  = 1  etc.
　in the polar regions the distribution of class animal over its children depends on the season  so for parent context x =  n   polar  we could have a default distribution such as:
pd reptile|animal … s = summer … x  = 1
pd mammal|animal … s = summer … x  = 1 pd insect|animal … s = summer … x  = 1  etc.
1 construction of a flat bayesian network
an observation about a hierarchical variable h can be positive or negative. an observation about h is positive when we observe that h is cobs  where cobs is a class in the hierarchy of h. this means that h has a value vh such that vh （ cobs. an observation about h is negative when we observe that h is  cobs. this means that h has a value vh such that vh （ cobs. without loss of generality  we can assume that there is always one positive observation about h  denoted by cpos. for example  suppose we have two positive observations c1 and c1 about h. if classes c1 and c1 are disjoint then observations about h are not consistent. if c1 is a subclass of c1  c1…c1 equals c1. if there are only negative observations about h  we assume root class is the positive observation. there can be multiple negative observations about h that are descendants of cpos in the hierarchy of h.
　we assume a query asks either for a distribution over a simple variable or for the probability of a particular class in a
　hierarchical variable.
　given some evidence  a set of observations  and a query  we construct a flat bayesian network  bf   from a hierarchical bayesian network bh by replacing each hierarchical variable with a simple variable whose domain includes only those values necessary to answer the query. the state space of a hierarchical variable h can be abstracted because  for a given problem  only certain classes of h are supported directly by the evidence or relevant to the query. to construct bf from bh given some evidence and a query  we traverse bh from the leaves upwards  prune those variables irrelevant to answer the query  and abstract the hierarchical variables. we abstract the hierarchical variables as part of a pass to remove variables that are irrelevant to the query. from the bottomup  you can recursively prune any variable that has no children and is not observed or queried  geiger et al.  1; pearl  1  as well as abstract hierarchical variables when their children have been abstracted. we can then answer the query from the flat bayesian network using any standard probabilistic inference algorithm.
1 abstraction of hierarchical variables
let h be a hierarchical variable with values val h . the abstractionofthedomainofh  denotedbypart h   isapartition of val h . that is  part h  is a set of subsets of val h  such that all sets in part h  are mutually disjoint and the union of all the sets equals val h . we refer to a set in part h  as an abstract value of h. an abstraction of hierarchical variable h  denoted by ha  is a simple variable with domain val ha  = part h .
1 finding a safe abstraction
in this section we describe a simple algorithm flat bayes for constructing a flat bayesian network bf from a hierarchical bayesian network bh given some evidence and a query. a safe abstraction has the same posterior distribution over the query variable in bf as in bh. to develop a safe abstraction for efficient inference  the following constraints are followed in flat bayes: 1. for every evidence or query  there must be an abstractvaluethatdirectlyassociateswiththatevidence/query and conveys its effect. 1. all abstract values must be mutually disjoint and exclusive. the algorithm flat bayes consists of three phases:
phase1:
if a query asks for the probability of a particular class in hierarchical variable h  we create an extra binary child q of h  which is true exactly when the query is true  i.e.  pd q = true|croot  = 1 and pd q = true|cq  = 1  where croot is the root class and cq is the query class in the tree hierarchy of h . the variable q has the same probability as the query class. we thus reduce the problem to one of computing the probability of a simple variable.
phase1: abstract
in this phase flat bayes traverses bh from the leaves upwards. it prunes a variable that is not queried or observed and does not have any children  geiger et al.  1; pearl  1 . for each unpruned hierarchical variable h  the algorithm computes its abstraction ha as follows:
case1: h is not observed
the hierarchical variable h can be a parent of several simple variables.1 we need to find the exceptional classes ck of h

figure 1: the abstraction hierarchy of h is divided into three regions r1  r1  and r1 by positive  cpos  and negative  c1neg  c1neg  observations about h
with respect to each of these children  i.e.  classes that are associated with the evidence or the query . the domain of ha is the set of non-empty abstract values for each exceptional class. the algorithm computes the abstract value  va  for every exceptional class ck as follows:
  va = ck  if ck does not have any exceptional strict subclasses.
  otherwise  va = ck  c1 ... cm  where c1 ... cm are the highest exceptional strict subclasses of ck. this set difference represents the set of all of the values that are in ck and are not covered by other abstract values.
the abstract values that are empty are removed from the domain of ha. to recursively abstract the parents of h  the algorithm finds those strict super classes of the exceptional classes ck  which are affected by h's parents.
case1: h is observed
the observations about h divide its tree hierarchy into three regionsr1 r1 andr1. r1includestheclassesthataretruefor the observation  superclasses of cpos   r1 includes the classes that we know nothing about  and r1 includes the classes that weknowarefalse. forexample  supposewehaveonepositive  cpos  and two negative  c1neg  c1neg  observations about h as shown in figure 1. the regions r1  r1  and r1 are shown by the bold lines in figure 1. we do not need to distinguish between the values of h that we know are false. the values of h can be partitioned into two disjoint and exclusive subsets: one  vfalse  corresponds to all the values that are false and another  vnotknow  corresponds to all the values we know nothing about. thus 
vfalse  cpos “  negative observations  vnotknow = cpos    negative observations 
val ha  = vnotknow 
　if h also has children  we need to partition its abstract value vnotknow into subsets based on the evidence from its children. we need to find the exceptional classes ck of h with respect to its children that are in region r1 in the hierarchy of h. we can compute the abstract value voa that corresponds to each exceptional class ck in the same way as described in case1 
but we need to remove all the false values. thus  voa = va   {negative observations that are subclasses ofck} let vovoak be the non-empty abstract values that correspond to exceptional classes ck. let vr = vo.
then  val ha  = voa1 ... voak vnotknow   vr vfalse
　to recursively abstract the parents of h  the algorithm finds those strict super classes of exceptional classes ck and cpos that are affected by h's parents.
phase1: construct tables
the algorithm constructs the cpt for variables xa of a flat network. let paa xa  denote the abstracted parents of xa. we compute p xa = va|pas xa  = vs … paa xa  = v  for each value va of xa and for each assignment vs of pas xa  and v of paa xa  for the following two cases:
case1: xa is not abstracted but it has abstracted parents as discussed in section 1  p xa = va|pas a paa xa  = v  is inherited from the default distribution of x
for the parent context c  c （ zp  v ＋ c and zp such that v ＋ y   c  where zp is the set of parent contexts for which the default distribution of xa has been defined.
case1: xa is an abstracted variable
let  denote the assignment pas xa  = vs … paa xa  = v. as discussed in phase1  va = ck   c1   ...   cm. then 
p  xa  p c |
　as discussed in section 1  to compute p ck|  we need the probability distribution of all the super classes of ck over their immediate subclasses for parent context v. as discussed in section 1  we can treat each of these classes as a simple variable. thus  the probability distribution of a class over its immediate subclasses for parent context v can be computed in the same way as described in case1.
　the evidence for observed hierarchical variable h is translated into the corresponding abstract variable ha of bf . the evidence eh for ha in bf is the disjunction of all those abstract values of h that are not false for the observation.
　the domain size of the variables in bf is independent of the size of the hierarchies; it depends on how many classes in their hierarchy are exceptional with respect to their relevant1children in bh. the running time to construct bf depends on both the depth and number of the exceptional classes. after constructing bf we can answer the query from bf using any standard probabilistic inference algorithm  for example  variable elimination algorithm  zhang and poole  1  or stochastic simulation.
1 correctness of flat bayesian network
let bh be a hierarchical bayesian network that has n discrete random variables  x = {x1 ... xn}  and we want to answer some probabilistic query  p  q|e   where q e   x. q denotes the query variables. we observed that e is in the set e  e   domain e   e （ e is the evidence. from bh we can recursively prune any variable that has no children  and is not observed or queried  geiger et al.  1; pearl  1 .
	p  q	e	e 
	p qee  =	… （ （
	p  e	e 
　letx1 ... xs bethenon-querynon-observedrandomvariables of bh. pa xi  denotes the parents of variable xi. the marginal p q … e  can be computed as follows:
n
p
	xs	x1 i=1
lemma 1 the posterior probability p q|e  computed from a bayesian network after replacing its variables xk by their abstractions xka is the same as if it is computed without replacing xk  if the xka are constructed from the partitions part xk  that have the following property :
 xk   yj （ children xk    v （ part pa yj   
	v1 	 1 
proof sketch summingoverallthevaluesofxk isequivalent to summing over the partition of the values of xk. thus 
j
	 xk = v|pa yl|pa yl  	 1 
v（val xk 	l=1	j
 v|pa
	v（part xk  v（v	l=1
if  1  is true  we can distribute the product out of the innermost sum from the rhs of the above equality  leaving the term xk = v|pa xk   whichisequaltop v|pa xk  . thus  1  is equal to
j
		p v|pa
v（part xk l=1
it is straightforward to incorporate evidence e and query q into the proof.
lemma 1 the algorithm flat bayes partitions the values of the hierarchical variables xk of bh such that all its children satisfies the equation  1 .
proof sketch let part xk  denote the abstraction of the domain of hierarchical variable xk computed by flat bayes. xka denotes the abstraction of xk. if xk is observed  flat bayes removes a set v  v （ part xk   such that all values in v are false. flat bayesconstructstheconditionalprobabilitytables forvariablesxka ofaflatbayesiannetworkusing inheritance  for the following two cases:
case1: xka is not abstracted but some of its parents are
as discussed in phase1  p xka = va|pas vs … paa  is inherited from the default distribution of xk for the parent context c （ zp  v ＋ c zp such that v ＋ y   c. thus 
 v	（	v  p xka|pas xka 	=	vs
pd xk|pas xk  = vs … c . this implies equation  1 .
case1: xka is abstracted
as discussed in phase1  to compute p xka = va|pas vs … paa xv  we need the conditional distribution of classes cj  cj （ tree xk   over their immediate subclasses. we can treat a class cj  cj （ tree xk   as a simple variable. thus  the proof follows from case1.

figure 1:  a  bayesian network representation of conference academics domain  b a part of the tree hierarchy for location in the world
theorem 1 the posterior probability p q|e  computed from a flat bayesian network as constructed by flat bayes is equal to the posterior probability computed from a hierarchical bayesian network.
proof sketch thetheoremfollowsfromlemmas1and1.
1 experiments
weimplementourflat bayesalgorithmontopofthevariable elimination  ve  algorithm  we call it flat bayes + ve. to get an idea of the performance of flat bayes+ve compared to ve  we investigate how the performance of both algorithms vary with the number of hierarchical variables in the bayesian network  with the depth of the tree hierarchies of the hierarchical variables  and with the depth of the exceptional classes. to test the algorithms  we consider a simple conference domain. in this domain  the location of an academic is influenced by the location of the conference and the location of the spouse of the academic is influenced by the location of the academic. let there be k academics. we assume that the locations of the different academics are not independent; they depend on the conference location. we consider that all location variables  conference location or person locations  have the same domain and are hierarchical variables. the bayesian network representation of this domain is shown in figure 1  a . the part of the tree hierarchy for the location is shown in figure 1  b . here we have 1k + 1 hierarchical random variables. the hierarchical random variable conf loc denotes the conference location. the hierarchical random variables acam loc1 ... acam lock denote the locations of the academics  and spou loc1 ... spou lock denote the locations of their spouses respectively.
	to	specify	the	conditional	probability
p acam lock|conf loc   we need to specify the default distribution of each class in the hierarchy of acam lock over its immediate subclasses for some parent contexts  for some classes in the hierarchy of conf loc . we assume the conference location influences each academic's location very locally; we need to specify the default distribution of the classes over their immediate subclasses for very few parent contexts.
example: suppose we know that the academic is in the uk but don't know whether he is in scotland  wales or england. now  knowing that the conference is taking place in the uk but not where in the uk  doesn't provide us any information

figure 1: the average inference time of applying ve and flat bayes + ve for k = 1 as a function of the depth of the hierarchy  and depth of the observations
about the location of the academic in the uk. however  if we know that the conference is taking place somewhere in scotland  we can state that there is a high probability that the academic is in scotland. we assume that knowing the conference location in scotland  e.g.  in edinburgh   doesn't provide us any extra information about where they may be if not in scotland. thus  we could have a default probability distribution of the class uk in the hierarchy of acam lock over its immediate subclasses conditioned on conf loc such as:
pd scotland|uk … conf loc = scotland  = 1
pd scotland|uk … conf loc = england  = 1
pd scotland|uk … conf loc = world  = 1
　fortheexperiments weobservethelocationsofthespouses and we want to compute where the conference is taking place. todeterminehowtheinferencetimedependsonthenumberof hierarchical variables  we vary k from 1 to 1. we represent the abstraction hierarchy of all location variables by a binary tree ofdepthd whered isaparameter. todeterminehowinference time depends on the depth of the hierarchy  d  we vary d from 1 to 1 with a step of 1. to determine how the inference time depends on the depth of the exceptional classes  for each valueofd wevarythedepthoftheobservationfortheobserved variables from 1 to d. we compute the posterior distribution of conf loc by applying only ve and flat bayes + ve.
　figure 1 shows the average inference time of both ve and flat bayes + ve for the belief network that has only 1 academics as a function of the depth of the tree  and depth of the observations. figure 1 shows the average inference time of both ve and flat bayes + ve as a function of the number of academics and the depth of observations; here we consider the depth of the hierarchy is 1. figure 1 shows that the inference time for flat bayes + ve doesn't depend on the depth of the hierarchy but increases linearly as the depth of the observations increases. this increase is because the depth of the observations for spou lock variables changes the depth and the number of exceptional classes for acam lock and conf loc.

figure 1: the average inference time of applying ve and flat bayes + ve for d = 1 as a function of the number of academics and the depth of the observations
note that ve takes less time as the depth of observation increases. this is because the domain of spou lock variable reduces as the depth of the observation increases. the results shown in figures 1 and 1 give us some indication of the overhead of constructing a flat bayesian network and cases in which it may be more effective to use it rather than ve alone.
1 related work
there is very limited work on combining taxonomic hierarchies and probabilistic reasoning. the problem of evidential reasoning in a taxonomic hierarchy of hypotheses was first studied by gordon and shortliffe  using the dempstershafer  d-s  theory. shenoy and shafer  improved the algorithm proposed by gordon and shortliffe and showed that belief functions can be propagated using local computations. pearl  provides a method for the same problem using
bayesian formalism. in all of these  a very restricted naive bayes classifier network form was considered. in this paper we concentrate on general bayesian networks that have both hierarchical and simple variables. pearl's method involves traversing the whole hierarchy for each observation. we show in this paper that we do not need to do this. in more recent work  koller and pfeffer  describe a representation language for combining frame-representation systems and bayesian networks for representing complex structured domains. they are looking at different aspects of the problem  concentrating on multiple objects  combining first-order logic representation with probabilities.
1 conclusion
having the values of a variable hierarchically structured can make reasoning more efficient than reasoning over the set of all values. we show how  given a query and observations  we can dynamically construct a flat bayesian network from the given hierarchical bayesian network that can be used in any standard probabilistic inference algorithm. the size of the flat network is independent of the size of the hierarchies; it depends on how many of the classes in the hierarchy are exceptional with respect to children that are observed or have observed descendants in a bayesian network. thus it is possible to have hierarchical variables with unbounded or infinite domains. for example  with a spatial hierarchy  as long as we have a way to compute the probability distribution over subclasses  there is no reason not to have a hierarchy that is infinitely detailed. it is only when we ask a query of a class or make observations that we need to restrict the size of the hierarchy.
acknowledgments
this work was supported by nserc discovery grant ogpoo1. thanks to valerie mcrae  mark crowley and ben d'andrea for proofreading.
