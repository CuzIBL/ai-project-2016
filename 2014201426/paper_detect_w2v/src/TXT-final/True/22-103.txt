 
selective induction techniques perform poorly when the features are inappropriate for the target concept. one solution is to have the learning system construct new features automatically; unfortunately feature construction is a difficult and poorly understood problem. in this paper we present a definition of feature construction in concept learning  and offer a 
framework for its study based on four aspects: detection  selection  generalization  and evaluation. this framework is used in the analysis of existing learning systems and as the basis for the design of a new system  citre. citre performs feature construction using decision trees and simple domain knowledge as constructive biases. initial results on a set of spatial-dependent problems suggest the importance of domain knowledge and feature generalization  i.e.  constructive induction. 
1 	introduction 
good representations are often crucial for solving difficult problems in ai. finding suitable problem representations  however  can be difficult and time consuming. this is especially true in machine learning: learning can be relatively easy if the training examples are presented in a suitable form  but when the features used in describing examples are inappropriate for the target concept  learning can be difficult or impossible using selective induction methods. to overcome this problem a learning system needs to be capable of generating appropriate features for new situations. 
　this paper is concerned with the automated construction of new features to facilitate concept learning  an issue closely related to the  problem of new terms   dietterich et a/.  1  and constructive induction  michalski  1 . we begin by defining  feature construction  in the context of concept learning from examples  and then proceed to identify four inherent aspects: detection  selection  generalization  and evaluation. these aspects comprise an analytical framework for studying feature 
   *this research was funded by a university of illinois cognitive science/artificial intelligence fellowship and onr grant n1k-1. 
construction which we describe through examples drawn from the existing systems of bacon.1  bogart  duce  plso  and stagger. this framework serves as the basis for the design of citre  a system that performs feature construction on decision trees using simple domain knowledge. the results of our initial experiments demonstrate citre's ability to improve learning through feature construction in a tic-tac-toe classification problem. extensions of citre for this and other problems are also discussed. 
1 	the problem 
we state the following definition of feature construction1 in concept learning: 
feature construction: the application of a set of constructive operators {o 1   1  ...on} to a set of existing features {f1 f1 ....fm}  resulting in the construction of one or more new features {f1  f1  .fn} intended for use in describing the target concept. 
　this definition emphasizes the notion of a constructive operator  defined as a function mapping a tuple of existing features into a new feature. a constructive operand is a tuple of features to which a constructive operator is applied. for convenience  an operator/operand pair will be referred to as a constructor  up until the time it becomes a new feature. the constructor and big i  red i    for example  consists of the two place operator and binary  binary  and the operand tuple  big i  red i  . 
additional implications of this definition include: 1  
a feature is a function mapping instances into values. 1  all new features are defined in terms of existing features  such that no inherently new information is added through feature construction. 1  the definition can be applied iteratively: after a new feature is created  it may serve as part of an operand in a subsequent round of construction. 1  a separate  selective induction algorithm is assumed to make use of the constructed features in attempting to describe the target concept. 
     1  feature construction is often referred to as  constructive induction.  we  however  reserve the term constructive induction to refer to the prediction of unobserved  disjunctive regions in instance space  see section 1 . 
	matheus and rendell 	1 

1 	four inherent aspects 
representative examples of systems that perform feature construction include fringe  pagallo  1   bacon  langley et a/.  1   stagger  schlimmer  1   
duce  muggleton  1   plso  rendell  1  and bogart  newman and uhr  1 . these systems employ a variety of techniques to a wide range of learning problems  making it difficult to identify exactly when  where  and how feature construction is performed. through our effort to understand feature construction in such systems we have discovered some common issues and concerns. in particular  we have identified the following four aspects believed to be inherent to the problem of feature construction: 
1. detection of when construction is required 
1. selection of constructors 1. generalization of selected constructors 
1. evaluation of the new features. 
　these four aspects are not necessarily present in every system performing feature construction. nor do they necessarily delineate sharply defined phases of the feature construction process. instead  they represent four identifiable issues inherent to the general problem of feature construction that account for much of the perceived variability between systems. as a result  these aspects have proven useful as a framework for the analysis and comparison of systems  see  matheus  1    as well as in the development of citre. we now consider each aspect in detail. 
1 	detecting the need 
if the original feature set is sufficient for the selective induction algorithm to acquire the target concept  feature construction is unnecessary. because the constructive process can be computationally expensive  it is often desirable to determine when feature construction is needed. there are at least three general approaches to determining when to perform construction: 
  no detection  i.e.  always perform construction  
  analysis of the initial data set 
  analysis of a concept description. 
　some systems have no detection mechanism because they continually perform construction. the sole purpose of bacon.l  for example  is to construct new features. a system might base detection on an analysis of the initial training set  for example using cluster or factor analysis  although none of the observed systems fully develop this approach . more typically  systems perform detection by observing the results of the selective induction system  i.e.  the concept description it produces. feature construction may be deemed necessary if the learning system fails to produce a description  or if the description fails to satisfy some measurable quality  e.g.  accuracy  conciseness  comprehensibility . using failure to trig;er construction is employed  for example  by stagger schlimmer  1   the presence of excessive disjuncts in a concept description can also be used for detection  as suggested in  rendell  1 . we return to this last approach in section 1 to discuss its use in citre. 
1 	selecting constructors 
after determining the need for feature construction  a system must select one or more constructors to use in the creation of new feature s . the difficulty with this selection process is that the space of potential constructors is generally intractably large. even in the simple case of a problem represented by n boolean features the number of potential constructors is 1 . for more complex problems  such as those involving numerical features  the search space can be infinite. the task of selection is to pick out a small subset of these potential constructors satisfying the representational requirements of the current problem. this selection process can be analyzed in terms of two phases: initial and runtime selection. 
　the first step in reducing the constructor space is to reduce the set of potential operators to a more manageable size. this initial selection has been approached in two ways. either a small set of simple  domain-independent operators is chosen  e.g.  {and  or  not}   or a set of problem- or domain-specific operators is developed  e.g.   counting the number of wheels on a box car  . simple operators have the advantage of being applicable to a wide range of problems  but they require multiple iterations of the constructive process in order to build up complex new features  e.g.  stagger  plso . problem-specific operators can reduce the complexity of the constructive process and thereby decrease construction time  but at the expense of being limited in application and also requiring effort and knowledge for their development. 
　the reduced set of constructive operators obtained from initial selection represents a system's  candidate  operator set. in every system we have encountered  the  candidate  operator set is significantly smaller than the  potential  set. even so  this set is still intractably large in most cases  and further selection is required at runtime to choose which operators to apply  and which operands to apply them to. this runtime selection amounts to ordering the set of candidate constructors. we have observed four approaches to ordering candidate constructor sets related to the following biases: 
  algorithm biases 
  training set biases 
  concept-based biases 
  domain-based biases. 

figure 1: constructive compilation versus constructive i n d u c t i o n . if a system  upon observing instances from r l   r1  and r1  constructs the new feature or rl  r1  r1  it is performing constructive compilation. if a system generalizes this feature into or rl r1 r1 r1   thereby predicting the existence of an unobserved region of instances  the white pieces  in r1  it is performing constructive induction. 　algorithm biases occur when the implementation of a feature construction algorithm arbitrarily selects one constructor over another. these biases are typically undesirable  being unjustified by the data or the model. some systems use information in the training set instances to help guide selection. stagger takes this approach by using instances that fail to agree with the concept as the basis for suggesting new features. likewise  in b1gart individual instances are used as templates for new features. similarly  concept descriptions can be used to bias constructor selection; this approach can be especially useful if a concept description is close to the target concept  we show how this approach can be used on decision trees in section 1 . domain knowledge can also serve as a selection bias  either as a set of heuristics for selecting useful constructors  or as a set of filters for re-
jecting undesirable candidate features. our experiments with citre suggest that even simple domain knowledge can be a powerful selection bias  section 1 . 
1 	generalizing constructors 
the set of selected constructors may be highly specific to a set of instances or a concept depending upon the selection biases employed. specific constructors can be generalized in ways analogous to how descriptions are generalized in learning concepts  e.g.  changing constants to variables  dropping terms  see  michalski  1  . if a feature construction algorithm does not generalize beyond its input data  then it is only able to  compile  observed features and feature values into new terms. we refer to this form of feature construction as constructive compilation. if generalization is performed during feature construction  the features generated may  predict  regions of positive and negative instances that have not been observed. we call this form of feature construction constructive induction. 
   figure 1 depicts a situation in which observed pairs of pieces on a checkers board  the dark pieces  have been mapped into disjunctive regions in an instance space of two abstract features. a constructive algorithm that merges these three regions  rl  r1  and r1  into a new feature or rl  r1  r1   is performing constructive compilation. if on the other hand  the algorithm generalizes this new feature to or rl  r1  r1  r1   thereby predicting the unobserved pair of pieces  the white pieces   the algorithm is performing constructive induction. 
　for certain learning algorithms  constructive compilation alone can significantly improve learning. consider a system that describes concepts by selecting one feature at a time  e.g.  a decision tree inducer . problems can arise when the target concept depends on the correlation of two or more features  because a feature that splits the data poorly by itself may provide a very good split when paired with another feature. if a constructor is able to find this pair and compile it into a new feature  the construction of a more accurate and/or concise concept becomes possible. this result is observed in our experiments with citre  see also  pagallo  1  . 
   constructive induction is potentially more powerful than constructive compilation because it can produce new features with far fewer observed instances  an important quality when the training set is sparse . the problem with constructive induction is that it requires strong biases in order to produce valid generalizations. strong and appropriate generalization biases are most readily available in the form of domain knowledge. the strong bias for the generalization or rl  r1  r1  r1  in figure 1  for example  might have come from knowledge that in checkers a feature useful at one board location can often be translated to other board locations. as our preliminary results suggest  the use of relevant domain knowledge can significantly affect the quality of constructed features  section 1 . 
1 	evaluating features 
if the number of new features grows too large  it may be necessary to evaluate and discard some. the evaluation of new features can be approached in at least three ways: 
  ignore the problem  keep all features  
  request evaluation from the user 
  order the features and keep the best ones. 
　the first approach  e.g.  in bacon.1  plso  and bogart  is the simplest but most limited in that it is only appropriate if the number of new features remains relatively small. the second approach  e.g.  in duce  places 
	matheus and rendell 	1 
the burden on the user who must be sufficiently knowledgeable to judge the quality of new features. the third approach  e.g.  in stagger and citre  is autonomous  but it requires a measurement of feature  quality   i.e.  the credit assignment problem . one solution is to use the measure employed for selecting features during concept formation as the measure for new feature evaluation; we consider this technique further in the next section. other measures of feature evaluation have been considered  see  seshu et a/.  1  . 
1 	citre 
the framework defined by these four aspects was used in the development of citrb  a new system for performing constructive induction on decision trees using simple domain knowledge.1 citre is similar to fringe  pagallo  1  in its use of learned decision trees to suggest useful constructors. both systems iterate between tree learning and feature construction: a decision tree is learned  the tree is used to construct new features  the new features are used in the creation of a new tree  and the process is repreated until no new features are created. citre differs in its use of domain knowledge  feature generalization  and feature evaluation. 
   as suggested above  the key to tractable and effective feature construction is the use of strong biases to help the system converge to an appropriate set of new features. citre uses three primary biases: concept-based operand selection  domain-knowledge constructor pruning  and an information-theoretic evaluation measure. these biases  depicted in figure 1  will now be described in terms of how citre handles each of the four aspects in the context of a tic-tac-toe classification problem {i.e.  classifying boards as winning or losing instances . 
detection: feature construction is performed any time disjunctive regions are detected in a candidate decision tree  as evidenced by the presence of more than one positively labeled terminal node. 
selection: the initial selection of operators was based on two criteria: first  we wanted to minimize the amount of domain-specific information present in the constructive operators. second  we wanted to test the hypothesis that useful  higher-level features can be constructed incrementally through the iterative application of simple constructive operators. we consequently selected the simple  generic  binary operator and binary  binary . the and ~  ~  operator used in conjunction with the negation implicit in decision trees provides complete representational capability for new features. 
　the primitive features used in the tic-tac-toe problem are the contents of the nine board positions: f - {posll  posl1  posl1  pos1  pos1  pos1  pos1  pos1  pos1}. these are nominal features having values x  o  or blank. in order to accommodate the binary operand 
1
　　 citre currently operates in conjunction with cogent  matheus  1   a decision tree induction program functionally comparable to id1  quinlan  1 . both citre and cogent are implemented in quintus prolog running on sun1 workstations. 

slots of the and      operator these nominal features are converted into boolean expressions as necessary during runtime selection  e.g.  equal posll x  . the size of the class of potential operands in this case  1 features  1 values  is thus 1 * 1 = 1. if n new features are actually created in round one  then round two has  n + 1  * n + 1  = n1 + 1v + 1 potential new features; the number of new features will similarly increase for all subsequent rounds. 
   the main bias used by citre in selecting the appropriate operand is concept based  and is derived from the idea of  merging disjunctive regions   rendell  1 . disjunctive regions in the decision tree are described by the branches leading from the root to the positively labeled terminal nodes. citre collects the feature-value pairs  e.g.  posll=x  at nodes along these positive branches and proposes all pairs of these binary feature-value pairs  e.g.  equals pos 1 x   as candidate operands. in a complete tree of depth d  this method results in 1d-l candidate operands. in tictac-toe where d is typically less than 1  this bias results in fewer than 1 operands being proposed out of greater than 1 potential operands. this large reduction    1 to   1  gives a good indication of how strong this bias is  as implied by the heavy arrows in figure 1 . for each operand selected a new candidate feature is constructed by applying the and   operator  e.g.  and  equalf pos 1  x  equal pos 1  x   . 
　domain knowledge can serve as an additional bias to filter out less promising candidate features  see figure 1 . in citre  domain knowledge is limited to simple facts  ground literal clauses  defining permissible relationships between constructive operands. for the tictac-toe problem we have implemented a small amount of general knowledge about board games: knowledge about adjacency of pieces being important  i.e.  only consider new features having adjacent constituent features   and knowledge about the significance of piece type  i.e.  only consider new features composed of the same type pieces . when citre uses this domain knowledge  the constructor and  equal posll x   equal po$1  o    would be re-
jected on both grounds. 
generalisation: in the experiments described in this paper we used the single generalization operator  changing constants to variables.  if a constructor's operand consists of two features having the same value  a generalized feature is proposed with its values replaced by a variable. for example  the generalization of and  equal poall  x   equal po$1  x    becomes and  equal po1ll  variable   equal posl1  variable  . both the original candidate feature and its generalization are added to the set of features. when a candidate feature is generalized  the resulting new feature is nominal  rather than binary  and its domain is  x o true . for a particular instance  the value of a generalized feature is false if the logical and   relationship does not hold  or the variable's value otherwise  i.e.  x or o . 
evaluation: although citre may generate hundreds of constructors while working on a given problem  it only keeps a maximum of 1  1 primitive + 1 constructed  features at a time. its criterion for evaluation of features is the same as that used in deciding which feature to select during tree formation  i.e.  an information theoretic measure. the  utility  of each feature is measured in terms of the information gained by using the feature to split the entire training set into disjoint subsets. they are then ordered by utility  and those features  excluding the primitives  with the lowest utilities are deleted until the total feature count is again down to 1. as shown in figure 1  this evaluation is the final bias used in pruning the set of potential new features. 
   evaluating features by their ability to effectively split the current data set works well in citre because the feature with the highest utility necessarily becomes the first node in the subsequent decision tree. this greedy approach can fail  however  if a new feature having a poor utility on the entire data set exhibits a relatively high utility sometime after the first split. for this reason  other forms of  deeper  evaluation are being considered. 
1 	e x p e r i m e n t a l results 
we conducted four series of tests in which we varied the use of generalization and domain knowledge as shown in table 1. all four series were run on an identical collection of fifteen data sets. each data set consisted of 1 randomly-generated tic-tac-toe instances  labeled either as  win for x  or  win for o.  each run iterated between tree construction and feature construction until no new features could be constructed. all decision trees constructed during a test were analyzed for accuracy on classifying a test set of 1 randomly chosen boards. 
　table 1 summarizes the results for all four test series. columns two and three indicate the use of domain knowledge and generalization for each test. the column labeled  first  lists the accuracies for the original decision trees averaged over the 1 data sets. corresponding accuracies for the final decision trees are in the column labeled  last.  the difference between the these two columns appears in the  difference  column. the ＼ values indicate the 1 percent confidence intervals as determined by a t-test. under  new terms  are the average number of new features considered per data set  measured after selection and generalization but before evaluation   with the highest number in any single run shown in parentheses. in the last column is the average difference between the number of nodes in the original trees and the number in the final trees. 
　on average  feature construction resulted in improved classification accuracy. for the last three tests - those making use of domain knowledge and/or generalization this result is significant with 1 percent confidence  with an average improvement of around six percent. whereas there is no significant difference among the accuracy improvements of these three tests  the use of generalization resulted in a greater variance in accuracy. this observation reflects the fact that although generalization on average improves performance  it can lead to decreased performance on individual runs when invalid generalizations are made. 
　another difference between the use of generalization and domain knowledge is evident in the number of new features considered. without domain knowledge an average of over 1 new features were considered per data set  with a single worst case occurring in the generalization test with the production of 1 candidate features. with the addition of the domain knowledge described above  the average drops below 1  and in the worst case is still less then 1. this difference translates directly into a significant improvement in efficiency. 
　in terms of number of internal decision tree nodes  the average difference does not change significantly between tests. because the number of nodes in the trees remains constant while the accuracy is improving the difference can be attributed to the use of better features when domain knowledge or generalization is used. 
1 	extensions and other applications 
in addition to tic-tac-toe  citre has been applied to the problem of learning random boolean functions. even on the simple problems thus far tested  e.g.  1 term  1 features/term dnf functions   accuracy improvements of greater than ten percent have been observed. detailed experiments in this domain are proving useful in the analysis of several aspects of citre's algorithm  including variations on the concept-bias  enhanced operator sets  and the effect of various evaluation methods. 
　our preliminary work on tic-tac-toe is being extended in several ways: 1  larger training sets are being used to demonstrate that generalizations tend to be more valid when based on larger samples of the instance space  1  new operators are being explored  e.g.  or       and         1  additional domain knowledge regarding board games is being used to help focus the feature construction  e.g.  emphasizing corners  straight lines  
	matheus and rendell 	1 
etc.   and 1  more powerful generalization rules are being tested including reflection  translation  and rotation generalizes. 
　we intend to apply the results obtained from the tictac-toe problem to more difficult board game problems such as knight-fork and chess endgames. towards this end  domain knowledge and generalization rules are being developed specifically for the spatial properties of board games. our ultimate goal is to develop a more general approach appropriate for the larger class of spatial problems that includes  for example  led classification  the blocks world  protein folding  etc. 
1 	conclusion 
the four aspects of feature construction presented here have proven useful as a framework for analyzing existing systems and guiding the development of new constructive techniques. in particular  this framework guided the design of citre  a new system that performs feature construction on decision trees using limited domain knowledge and simple generalization. results on a board game classification problem suggest the appropriateness of citre's three strong biases: concept-based selection  domain-knowledge pruning  and an information theoretic evaluation measure. more specifically  our results suggest the potential importance of domain knowledge and generalization for effective feature construction two areas to which current feature construction systems have paid little attention  see  matheus  1  . 
acknowledgment 
we would like to thank gunnar blix  gregg gunsch  carl kadie  doug medin  and david wilkins for their helpful discussion and comments on the issues developed in this paper. 
