on biases in estimating multi-valued attributes 
	i g o r 	k o n o n e n k o 
	university of ljubljana  faculty of electr 	eng 	and computei sc 
tizaika 1 si-1 ljubljana slovenia 

a b s t r a c t 
we analyse the biases of eleven mtasures for estimating the quality of the multivalued attributes the values of information gain jmeasure  gini-index and relevance tend to lin early increase with the number of values of an attribute the values of gam-ratio dis tance measure  relief and the weight of evidence decrease for informative attributes and increase for irrelevant attributes the bias of the statistic tests based on the chi-square distribution is similar but these functions are not able to discriminate among the attributes of different quality we also introduce a new func tion based on the mdl principle whose value slightly decreases with the increasing number of attribute s values 
1 	i n t r o d u c t i o n 
in top down induction of decision trees various impurity functions are used to estimate the quality of attributes in order to select the  best one to split on however various heuristics tend to overestimate the multi valued attnbules one possible approach to this problem in top down induction of decision trees is the construction of binary decision trees the other approach is to introduce a kind of normalization into the selection criterion such as gam-ratio  quinlan  1  and distance measure  mantaras  1  
　recently white and liu  showed that  even with normalization information based heuristics still tend to overestimate the attributes with more values their experiments indicated that  1 and g statistics are superior estimation techniques to information gain gain ratio and distance measure they used the monte carlo simulation technique to generate artificial data sets with at tributes with various numbers of values however  their scenario included only random attributes with the uni form distribution over attributes' values generated independently of the class 
1 	learning 
　the purpose of our investigation is to verify the conclusions of white and liu in mort realistic situa tions where attributes art informative and/or ha   nonuniform distribution of altnbutt s values we adopted and extended their scenario m order to verify results of methods tested b  white and liu and to lest also some oilier well known measures gini-index  breiman et al 1  j measure  smyth and goodman 1  the weight of evidence  miclue 1   and relevance  baim 1  besides we developed and tested also one  new selection measure based on the minimum description length  mdl  principle and a meassure derived from the algorithm relief  kira and rendell 1  
　in the following we describe all selection measures the experimental scenario and results we analyse the  dis advanlagfs of various- selection measures 
1 	selection measures 
in this section we bneflv describe all selection measures and develop a lit w one based on the mdl principle we  ssumt that all attributes are discrete and that the prob lem is lo select the best attribute among the attributes with various numbers of possible values all selection measures are defined in a wav that the best attribute should maximize the measure let c a and 1 b  the number of classes th  number of attributes and the number of values of the given attribute  respectivelv let n denote the number of training instances  n  the number of training instances from class  the number of instances with the j-th value of the given attribute  and nnj the number of instances from class c  and with the the th value of the given attribute let furthe her p }   denote the 
approximation of the probabilities from the training set 
1 	information based measures 
let hc  ha  he a  and hc/a be the entropv of the classes of the values of the given attribute  of the joint events class - attribute value  and of the classes given 
the value of the attribute  respectively 



where all the logarithms are of the has.e two the information gain is defined as the transmitted information by a given attribute about the object s class 

in order to avoid the overestimalion of the multivalued attribute*  quinlan  introduced the gam-ratio 
		 1  
　manldras  defined a distance measure d tliat can be rewritten as  whitf and liu 1  
		 1; 
smyth and goodman  introduced the j measure for estimating the information content of  the rule which is appropriate for selecting a single attribut* value for rule generation 

a straightforward generalization gives the attribute selection measure 
		 1  
another selertion measure re lated to informal ion theor  is the average absolute weight of enidence  miche 1  it is based on plausibility which is an alterative to antropv from the information theon let odds :   the measure is defintd for only two class problems 

and it holds ii e1 =r n e a straightforward generalization can be used for multi class problems 
	  e = ep we1 	:s  
1 	gini-index and relief 
breiman et al  use gini-index as the  nonnegative  attribute selection measure 
		 1  
kira and rendell  defined the algorithm relief for estimating the quality of attributes relief efficiently deals with strongly dependent attributes the idea behind is the search for the nearest instances from the eame class and the nearest instances from different classes kononenko  showed that  if this  nearest  condition is omitted  the estimates of relief can be viewed as the approximation of the following difference 
of probabilities 
relief = p diff value of an att  different class  
　　　　　　- p diff value of an at i |same class  which can be reformulated into 
		1  
where 
		 1  
is highlv correlati d with the gmi-mdev the only differ ence to   equation  1  is tlial instead of the factor 

in our experiments besides gnn and relief we e alu ated gun as well in order to verify whether this differ enee to equation  1  is significant or not 
1 	relevance 
bairn  introduced a selection measure called the relevance of an atlnbute lt for a given attribute value 
j be 

th  relevance of the attribute is defined with 

1 	 1 and g statistics 
the measures based on the rln square distribution use the following formula 
		 1  
where p z o   the chi-square distribution with d degrees of freedom and  o is the value of the statistic for a given attribute press et al  give the algorithms for evaluating the above formula we have two statistics that are well approximated bv the chi-square distribu tion with    - 1  c - 1  degrees of freedom  white and liu  1   x and g 

	kononenko 	1 
figure 1  a  gain for uniform distribution of attribute val ue*.  b  dam for informative attributes 
1 	m d l 
according to the minimal description length principle 
 rissanen 1 li and vitanvi 1  the problem of selecting the best attribute can be stated as the problem of selecting the most compressive attribute let us have the following transmission problem both tin. sender and the receiver have tht description of  he number of attributes 1 the number of possible values for each attnhutt v the number of possible classes c and the description of the training examples in terms of attributevalues but only the sender knows the correcl classifi cation of examples this information should be transmitted bv minimizing the length  the number of bits  of the message the sender may either explicitly code the class for each training example or may select the best attribute and encode  for each value of the selected attribute the classes of the examples having that value of the attribute therefore either we have one coding scheme for the prior distribution of classes  or we have a separate coding scheme for each value of the attribute with the associated posterior distribution for each coding scheme a decoder has to be transmitted as well 
　the number of bits  that are needed to explicitly encode classes of examples for a given probability distribution  can be approximated with entropy he times the number of training examples n plus the number of bits needed to encode the decoder for any coding rule the sufficient information to reconstruct the decoder is the probability distribution of events  i e classes therefore  if n is known  to reconstruct the decoder the receiver needs to reconstruct only  can be 
then uniquely determined  there is n + c - 1 over c - 1 possible distributions therefore  the approxima-
1 	learning 

figure 1  a  gini for uniform distribution of attribute val ues  b  gini  or informative attributes 
tion of the total number of bits that we need to encode the classes of n examples is 

and the approximation of the number of bits to encode the classes of examples in all subsets corresponding to all values of the selected attribute is 

the last term  log .a  is needed to encode the selection of an attribute among 1 attributes however this term is constant for a given selection problem and can be ignored the first term is equal to n hc a therefore  the mdl measure that evaluates the average compression 
 per instance  of the message bv an attribute is defined with the following difference pnor.mdl - post-mdl normalized with n mdl' = g a i n + 

 1  
however  entropy he can be used lo derive mdl if the messages are of arbitrary length if the length of the message is known the more optimal coding uses the logarithm of all possible combinations of class labels for given probability distribution 

1 	experimental scenario 
we adopted and extended the scenario of white and liu 
 their scenario included the following variations of settings the number of classes was 1 and 1 the distribution of classes was uniform  except in out. experiment when they used a non-uniform distribution  and there were three attributes with 1  and 1 possible values which wtre randomly generated from the uniform distribution independently of the class they performed the monte carlo simulation by 1 times randomly generating 1 training instances with the above properties the quailtv of all attributes was  eslimated using all measures described in section 1 and the results of each measure were averaged over 1 trials 
we extended the scenario m the following directions 
　1 we tried the following numbers of classes 1 1 and the following numbers of attribute values 1  1  1 
1  1 
　1 we used also informative attributes the attributes with different number of values are made equally infor mative by joining the values of the attribute into two subsets corresponding to two values of the binary attribute the probability that the value is from the subset depends on the class while the selection of one particular value in side the subset is random from the uniform distribution the probability that the value is in one subset is defined with 
we tried various values for k termines how informative is the attribute for example for 1 possible classes with uniform distribution  the information gain of the two-valued attribute is 1 bits if k = 1 and 1 bits if  and for 1 possible classes i  is 1 bits if and 1 bits if   however the biases of all measures were not very sensitive to the value of l in the next section we give the results for 

　1 we trn d also various non uniform distributions of attribute values for uninformalive attributes and also various non-uniform distributions of classes for each possible distribution of attribute values  uniform  nonuniform  informative  the biases of all measures were independent of the distribution of classes and for unmfonnative attributes also independent of the distribution of attribute values the graphs m the next section are for uniform distribution of classes 
1 	results 
we will show here two different results for irrelevant  unmformative  and informative attributes we will present results jointly for the measures with the similar behavior 
1 	linear bias in favor of multivalued attributes 
the values of measures increase linearly with the number of values of attributes  in all different scenarios and for all different numbers of classes for the following measures 
gam  1  j  1  gtm  1   and gini'  1  on figures 1  a  and  b  the values of gam are depicted ./-measure has similar graphs 
　note that the scale of the graph for gun is not comparable to the scale for gam gum and gum have prac tically identical graphs the difference to gam is that gini lends to decrease with the increasing number of classes which seems to be undesired feature for selection measures this is shown on figure 1  b  where the values of gini for higher number of classes  where attributes are even more informative  see eq  1    are lower than the 
	k1nenk1 	1 


figure 1  a  gamr for uniform distribution of attribute values  b  gauinr for informative attributes 

figure 1  a  mdl for uniform distribution of attribute val ues  b  mdl for informative attributes 
values  for lower number of classes this becomes even more obvious for more informative attributes  e g for l - 1 in eq  1    where the graph becomes similar to that on figure 1  b  except that for gim all curves are straight lines with higher slope 
   relrv  1  has similar behavior like gmi index  except that the estimates uicrea.se less  than linearly with the number of values of attributes figure 1 shows this note that relevance tends to decrease with the increasing number of classes even though the attributes tor problems with more classes are more informative 
1 exponential bias against the informative multivalued attributes 
the estimates of informative and highly informative attributes decrease exponentiallv with i he number of values of attributes for gainr  1  1 - d  1  and relief 
 1  	however 	for irrelevant attributes 	all three mea-
1 	irrning 

sures exhibit  he bias in favor of the multivalued attributes th e estimates of relief increase logarithm  callv with the number of values while for 1 - d and gainr the estimates increase hnearlv figure 1 shows both biases for relef and figure 1 shows both biases for gamr t h t performance of 1 - d is ver  similar to that of gaiur note the different scales for irrelevant and informative attributes this shows that tht bias in favor of the irrelevant multivalued attributes is not very strong and is the lowest for relief 
1 	slight bias against the multivalued attributes 
mdl  1    exhibits the bias against the multivalued at tributes mdl almost hnearlv decreases with the num her of values of attributes in all scenarios as is shown on figures b  a  and  b  a.s expected from the definition in eq  1  the. bias  the slope of the curve  is higher for the problems witli the higher number of classes namelv the number of classes influences the number of bits needed to encode the decoders 
   mdl is always negative for irrelevant attributes and therefore the irrelevant attributes are alwavs considered as non-compressive for informative attributes the compression decreases with the number of values of attributes 
   mdl   1  has similar behaviour as mdl for irrele vant attributes  however the slope of the curves is lower and all informative attributes are considered compressive this is shown on figure 1 
　the behavior of    e  1  is not stable 	the reason may be in the use of the non-differentiable function  absolute 
 alues  the behavior seems to be somewhere between relev  for irrelevant attributes  and mdl  for in forma live attributes  this is shown on figure 1 like mdl 
we decreases faster for the problems with more classes 
1 almost unbiased but also non-discriminating measures 
figure 1  a  shows that p  1  defined with eq  1  and 
 1  is unbiased le 	its valuer do not show an  tendenc  


figure 1  a  tlu weight of evidence for uniform distribution of at tribute values  b  the weight of evidence for informative attributes 
willi inereasing nurnber of values of attributes however  this measure is not able to distinguish between more and less informative attributes all informative attributes  for  and i m eq  1   have p{ 1  - 1   regardless of the number of values in faat when using floating point in  the cast when  we dittcted that  differs from 1  i m sometime*  ex otic decimal places  like j1lh decimal plact  of course  ins is the problem of computer precision and numerical evaluation of eq  1  but. the fact is that on most com pulers without special algorithms this measure will not be ahle to distinguish between attributes with different quahty which makes the measure impractical someone may argue that in the cases when * v    - . . the value of could be used this can be done onlv when comparing  lit attributes with t actly the same number of values  the same digre of freedom  which is not verv useful 
   figure 1  a  shows that p  *  defiiud with equations  1  and  1  overestimates the multivalued attributes which is not m agreement with the conclusions by white and liu  their conclusions seem valid if the figure is limited to c = 1 and 1 and  and 1 which was their original scenario besides  p g  has the same problems as p{ 1  with informative attributes its values for informative attributes are all equal to 1 
   we verified this for p{ 1  and p g  by varying the parameter in eq 1 1 as soon as  all attributes are too informative and both statistics get the value 1 the results in figures 1  b  and 1  b  for k = 1 show the biases for  and p{g  against the multivalued attributes for slight!} informa 
ti e attributes 

1 	conclusions 
while our results with the original scenario by w'hile and liu are the same  an increase of the numher of attribute values shows a slightly different picture and further variations of the scenario reveal that their conclusions should be considered with caution  shows clear bias in favor of irrelevant multivalued attributes  measures seem to be biasc d against the 
informative mullivaluid attributes however the prob lem of evaluating the correct value with the given computer precision makes this functions impractical as they are not able to discriminate between the attributes of different quality 
   from our results we can conclude that the worst mea sures are those whose values in all different scenarios tend to increase with the number of values of an attribute information gam /-measure gini-mdex and relevance some of the measures  gnu index and rel evanct  exhibit an undesired behavior their values decreet with tin increasing number of classes even though tht attributes for problems with more classes are more informative however the weight of evidence and mdl  1  show similar tendency but only with the increasing number of attributes values for mdl this behavior can be explained in terms of the number of bits required lo encode the decoders for uninformative attributes  the bias of we is similar to the bias of relevance  however its bias is not stable 
　the performance of gain-ratio  distance measure and fie he/  which all use a kind of normalization  is similar the values exponentjallv decrease for informative attributes and increase for irrelevant attributes for ir relevant attributes the performance of relief seems to be better  as the bias in favor of multivalued attributes is not linear but rather logarithmic however the exponen-
	k1nenk1 	1 
p g  	
v 
p g; 	
figure 1  a  p{g  for uniform distribution of attribute values  b  p g  for slightly informative attributes k = 1 in eq  1  
tial bias against the multivalued attributes can hardl  be 
justified and more conservative bias mav be more acceptable 
　the purpose of this investigation was to analvse the performance of various measures on multivalued at tributes independent to other attributes we used the name relief for the function  1  wihch is derived from the original function of relief bv omiting the search for the nearest instances among all the measures only relief  together with the search for the nearest instances  is non-myopic in the sense that it is able to appropriately deal with strongly dependent attributes besides relief can also efficiently  estimate continuous attributes  kira and rendell  1  the extensions introduced in the algorithm relieff  kononenko  1  enable it to efficiently deal with noisy data missing values  and multi-class problems all these important features together with the relatively acceptable bias de scribed in this paper  make relieff a promising measure 
　the values of two new selection measures based on the mdl principle slightly decrease with the increasing number of attribute's values this bias is stable and seems to be better than the bias of other selection measures the selection measures have natural interpretation and also show when the attribute is not useful at all if it is not compressive i e when the value of mdl  1  or mdl  1  is less than or equal to zero mdl seems more appropriate than mdl' it uses optimal coding and its graphs have lower  negative  slopes which indicates lower bias against the multivalued attributes 
1 	learning 
acknowledgments 
1 thank matevz kovacic  uros pompe  and marko robnik for discussions on the mdl principle and for comments on earlier drafts of the paper 
