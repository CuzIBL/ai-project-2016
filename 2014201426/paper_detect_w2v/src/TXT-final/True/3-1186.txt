
cognitive assistive technologies that aid people with dementia  such as alzheimer's disease  hold the promise to provide such people with an increased level of independence. however  to realize this promise  such systems must account for the specific needs and preferences of individuals. we argue that this form of customization requires a sequential  decision-theoretic model of interaction. we describe both fully and partially observable markov decision process  pomdp  models of a handwashing task  and show that  despite the potential computational complexity  these can be effectively solved and produce policies that are evaluated as useful by professional caregivers.
1 introduction
dementia is a clinical syndrome characterized by the deterioration of a person's memory and cognitive function. it is estimated that there are nearly 1 million older adults with dementia worldwide  with this number expected to reach 1 million by 1 . alzheimer's disease is the most common form of dementia and accounts for more than half of dementia diagnoses . home care offers tremendous advantages for such individuals  e.g.  economically  quality of life  over placement in a long-term healthcare facility. however  significant barriers often prevent home care from being a viable option  including the physical and emotional burden placed on family members and the cost of maintaining professional care in the home.
　a pressing need for people with advanced dementia arises from the difficulty they have completing activities of daily living  adls . a caregiver generally must guide them  step by step  through such routine activities as handwashing  toileting  dressing  moving about  taking medication  etc. cognitive assistive technologies that help guide users  i.e.  care recipients  through adls can relieve some of the stress of home care on family members and care professionals. however  current monitoring and reminding devices cannot accurately track user behavior at a step-by-step level  nor adapt to specific users. moreover  many devices require explicit feedback  e.g.  button presses   which cannot reasonably be expected of people with moderate-to-severe dementia.
　our objective is to design and develop systems that actively monitor a user attempting a task and offer assistance in the form of task guidance  e.g.  prompts or reminders  when it is most appropriate and in a form that will do the most good. such systems should tailor their guidance decisions  w.r.t. both form and timing  to specific individuals and circumstances  based on what will work best for them. in this work  we extend the prototype coach system  in which behavioral monitoring of a handwashing task is realized with a vision system. we extend coach by modeling the guidance decision process in a decision-theoretic fashion  more precisely  as a partially observable markov decision process  pomdp . there are several important factors that should influence guidance decisions that strongly suggest the use of this model  including perceptual noise  the stochastic nature of user behavior  the need to trade off various objective criteria  e.g.  task completion  caregiver burden  user frustration and independence   and the need to tailor guidance to specific individuals and circumstances.
　our key contributions are as follows. first  we develop a pomdp model for a specific adl  handwashing . while the model itself is of value for handwashing  the general form of the model is appropriate for almost any adl guidance or intervention module. second  we show that these models exhibit considerable structure  allowing them to be specified easily  and the underlying decision problem to be solved approximately by recent state-of-the-art algorithms. in particular  we obtain an exact solution for a fully observable mdp simplification of the pomdp; and an approximate solution is obtained for the pomdp despite its size and complexity. third  controlled evaluation of the mdp policy by professional caregivers demonstrates its significant value as an assistive device. while mdp performance is not as good as that of professional caregivers  it is more than adequate to serve as the basis for a supplemental device which can relieve the burden on caregivers or family members. finally  in simulation  we show that the approximate pomdp policy outperforms the exact mdp policy. this suggests that accounting for partial observability and user customization via the pomdp approach offers considerable benefits. these benefits are currently being quantified in clinical trials which will be reported in a longer version of this paper.1
1 cognitive assistive technologies & coach
assistive technology  at  is increasingly used to offset the impact of impairments resulting from injury  disease  and the aging process and related disorders. typically these technologies have focused on assisting users with mobility impairments. recent growth in the number of people with cognitive disabilities  such as dementia  has resulted in considerable research being conducted into developing cognitive assistive technologies  cats  to address the difficulties that this population faces. in broad terms  cats attempt to compensate for existing impairments by using devices  tools  or techniques that either compensate for a person's impaired cognitive ability  or translate a problem into one that matches the user's strengths. to date  computerized cats have been of limited scope  especially in the use of ai techniques. prototypes of cognitive aids include handheld devices that remind a person to complete basic adls  e.g.  taking medication  . however  most of these systems were not designed for older adults  particularly those with moderate-to-severe dementia  and would likely not be acceptable for this population.
　several intelligent systems that use ai and ubiquitous computing techniques are currently being developed for the older adult. these include the aware home project   the assisted cognition project  and the nursebot project . these new projects are similar to the work described in this paper in the sense that they attempt to incorporate ai and a decision-theoretic approach to overcome the shortcomings of current approaches. in particular  the autominder system   one aspect of the nursebot project  applies a pomdp in the development of the planning and scheduling aspect of the system . these systems do not incorporate advanced prompting techniques and algorithms  but rather are being developed as scheduling and memory aids.
　coach  our first prototype of an intelligent environment for older adults with dementia  monitors progress and provides assistance during handwashing. it uses computer vision and simple ai techniques to learn to associate hand positions  1d coordinates  with specific handwashing steps  e.g.  turning on the water  using the soap   and to adjust its parameters and cuing strategies . coach uses audio prompts for each of the possible plan steps  with each step associated with a general  a moderate  or a specific prompt. for example  a general prompt to turn on the water simply states  please turn on the water   while a specific prompt uses the subject's name and elaborates on the the process   ...by placing your hands on the tap in front of you ...  . clinical trials with 1 subjects with moderate-to-severe dementia showed that the number of handwashing steps that the subjects were able to complete without assistance from the caregiver increased overall by approximately 1% when the device was used .
　although this prototype and its algorithms showed some success  we have identified several remaining challenges. limitations include the assumption of full observability  i.e.  that the environment and user context are fully represented by the data provided   inability to tailor its prompting strategy to specific users  and the reliance on deterministic hand-coded rules to make decisions  w.r.t. when and how to provide assistance . the ability to plan appropriate courses of action automatically  deal with partial observability  and customize behavior to specific users has led to the development of the pomdp approach described here.
1 a pomdp approach
the decision problem faced by coach is fraught with uncertainty  both with respect to the observability of the environment  and the effects of system actions. furthermore  our goal is to satisfy a number of different objective criteria that often conflict and cannot be achieved with certainty. for this reason  a pomdp provides the best formal framework for modeling the decision process. pomdps allow one to model imprecise information about the current environment state  uncertainty in the effects of actions  and multiple  conflicting objectives. optimal policies will proffer courses of action that balance the importance of specific objectives with their odds of success; furthermore  they account for the long-term impact of decisions  including the value of information inherent in an action  thereby allowing to the system to actively learn about certain environment or user characteristics.
　many domain characteristics associated with adl guidance  handwashing being just one example  strongly suggest the use of a pomdp. these include:
  sensing  e.g.  vision in our system  is prone to noise  hence environment variables must be estimated probabilistically  e.g.  the location of a user's hands and body must be estimated due to sensor noise or occlusion . as a result  the task step the user is attempting to engage must also be estimated.
  successful completion of a task step given a specific form of guidance is generally stochastic  e.g.  a user may react appropriately to an audio prompt only some percentage of the time .
  conflicting objectives need to be traded off against one another and their odds of success  e.g.  maximizing the odds of task completion  maximizing the number of successful task steps without caregiver aid  minimizing caregiver intervention  and minimizing user frustration .   a given form of guidance may have different odds of success for specific individuals  hence requiring customization to specific individuals based on our current estimates  e.g.  a specific user may tend to get frustrated with prompting that is too frequent .
1 the pomdp model
we now describe a specific pomdp model for the handwashing adl that captures the desiderata described above. we also describe a fully observable counterpart of this model that illustrates how mdps can be applied. the core mdp addresses stochasticity and multiple  long-term  objectives  while the pomdp extension captures partial observability  noise  and the ability to customize behavior to specific individuals by estimation of hidden user characteristics. we take pains to describe several variants of each model. this is because the efficacy study conducted using human caregivers was based on an earlier version of the mdp. we wish to evaluate our results using this baseline; but also describe

figure 1: plan graph for handwashing adl.
improvements to the model which will be used in upcoming clinical trials.
　a discrete-time pomdp consists of: a finite set s of states; a finite set a of actions; a stochastic transition model pr : s〜a ★   s   with pr s a t  denoting the probability of moving from state s to t when action a is taken; a finite observation set z; a stochastic observation model with pr s z  denoting the probability of observation z in state s; and a reward function assigning reward r s a t  with state transition s to t induced by action a. given a specific pomdp  our goal is to find a policy that maximizes the expected discounted sum of rewards attained by the system. since the system state is not known with certainty  a policy maps either belief states  i.e.  distributions over s  or action-observation histories into choices of actions. we refer to  for a detailed overview of pomdp concepts and algorithms.
state variables
our state space is characterized by four classes of variables: those that capture the state of the environment; those that summarize the adl plan steps completed thus far; those summarizing system behavior; and those reflecting certain hidden aspects of the user's personality or mental state.
　environment variables represent the underlying physical state of the environment; these are hl  hand location: at tap  at soap  at towel  at sink  at water  away  and wo  water on: boolean .1
　activity status variables capture the adl steps that the user has completed. figure 1 shows the legitimate sequences of steps  any path from start to finish  that constitute successful handwashing. there are 1 activity status variables. ps  whose domain is the nodes  a-k  of the plan graph  l is shown only for convenience   denotes the most recent step the user has completed. mps denotes the maximum plan step completed  since a user may regress in the plan graph. we use this to reward progress in the graph without rewarding duplication of steps  see reward description below ; psr denotes that the current step has been repeated. finally  prg indicates whether  progress  is being made within the current plan step; for example  hands moving to sink position prior to step f is indicative of progress being made towards the completion of step f despite the fact that the plan step hasn't yet been completed.
　system behavior variables provide a summary of the system history relevant to prediction of user responses to a prompt. these are: np or number of prompts issued for the current plan step  with domain 1+ ; nw or number of time steps waited since last prompt  1+ ; lp  the type of last prompt  see description of prompting actions below ; pl  the specificity of the last prompt  see actions below ; and rgr  the number of times the user has regressed in the plan  1+   which is used as an  stochastic  indication of general responsiveness of the user.
　finally  user variables reflect aspects of the user's mental state that impact their response to prompts. our current prototype uses only one variable  resp  general responsiveness   taking values low and high  to provide a very crude characterization of user type. however  more sophisticated user modeling can be incorporated into the pomdp  using transition and observation models of precisely the same form as those described below.1
actions and dynamics
the system has 1 actions  1 of which comprise prompts for six different plan tasks  water on/off  use soap  wet hands  rinse hands  dry hands  at three levels of specificity  general  moderate  specific . general prompts gently cue the user  while specific prompts are designed to get the user's attention and provide a detailed description of the task. for example  a prompt to use the soap might be worded as  use the soap now  at a general specificity level  but might include references to the color or location of the soap at a specific level  such as  use the soap in the pink bottle on your left . using the patient's name is another strategy that is effective for specific prompting. the wording of the prompts was chosen based on prior experience  and was fixed for the duration of the experiments we describe here.1 the other two actions are the  null  action and  call caregiver.  the latter action ends the process and is presumed to result in successful task completion. this is an important aspect of our system  since our aim is to develop assistive technologies that supplement rather than replace human caregivers. our goal is to relieve stress and burden on caregivers by increasing the odds of successful  independent  task completion and reducing the need for constant  human  monitoring. however  in the efficacy study presented here  we remove this action temporarily to ensure the integrity of our evaluations  as described below . in the ongoing clinical trials  it plays a central role.
　transition probabilities describe the stochastic state changes induced by each action. these are specified by dynamic bayesian networks  dbns  over the state variables  with the conditional probability tables  cpts  represented by algebraic decision diagrams  adds  . the considerable structure in the domain leads to a very compact specification of the dynamics. the precise parameters of the model were produced using a handcrafted prior reflecting our experience with patients in earlier clinical trials using the hand-coded prompting system and our interactions with caregivers.1
　space prevents a complete description of the dynamics  but we mention some of the key intuitions underlying the model. the probability of the user taking a specific  user action  such as turning on the water-corresponding to a state change in an environment variable  i.e.  wo becoming true -tends to be higher at appropriate plan steps  but the probability depends on the precise prompting history. for example  the longer the system waits for a response  modeled by nw    the less likely it is that the user will complete the current plan step independently  i.e.  without prompting . furthermore  the more  unsuccessful  prompting has been for the current step  modeled by pl and np   the less likely it is that the patient will complete the step at all. such  response  probabilities vary formulaicly with resp and rgr. higher values of rgr are indicative of lower likelihood of eventual success  while success is more probable when resp is high than when it is low.1
　activity status variables are updated deterministically as a function of environment variables  note that environment variables are themselves only partially observable  in the obvious way. ps may move back and forth through the plan graph  since the user can regress  e.g.  by re-wetting their hands   but mps records the maximal step in the plan reached at any point  and never regresses. system variables are updated deterministically as well  e.g.  number of prompts np is updated with each prompt for the current step  but reset to zero when the plan step changes . finally  the user variable resp is static  and does not change value over time  though our estimates of its value does .1
rewards
rewards are associated with various state transitions and costs with specific actions. a large reward is given for full task completion  +1  and smaller rewards  +1  are associated with the  first  achievement of each plan step to encourage/reward even partial success  mps is used to ensure rewards are not repeated . action costs are also incorporated to ensure that prompting only occurs when needed. each prompt is given a small negative reward  with the more timeconsuming specific prompts penalized slightly more than simpler general prompts  the costs are -1  -1 and -1 . the cost of calling the caregiver should be set so that this action only occurs if the predicted odds of completion are too low 
water
figure 1: possible hand locations.
or predicted costs of completion too high.
　the design of the reward function is based on brief interactions with caregivers  earlier clinical trials with patients using hand-coded coach  and evaluation of numerous versions of mdp  rather than pomdp  policies. while the rewards seem reasonable  fine-tuning of the reward function is an ongoing research project  see section 1 .
observations
the handwashing task has two observables: the reported flow of water  on/off   and the reported location of the user's hand s . the pomdp is designed to be integrated with a computer vision system that estimates hand position  and water flow indirectly  based on skin color . the vision system reports estimated hand position  x y-coordinates  which is translated into one of six possible regions  see fig. 1 .
　we assume 1% noise in the observation function that detects hand position  i.e.  the correct hand position is detected with probability 1  while each incorrect position is detected with probability 1. similarly water flow is detected correctly with probability 1. these probabilities are estimated based on our prior experience  but a detailed empirical study is needed to assess these accurately.
1 a fully observable model
the pomdp model can be made fully observable with two simple changes. first  if we remove the hidden variable resp  a key source of partial observability disappears  though we sacrifice the ability to customize behavior to specific individuals.1 second  if we assume the tracking system computes hand position and water flow without error  the system becomes fully observable. the assumption of perfect observability is not unreasonable if we use a switch sensor on the tap and additional cameras to disambiguate obscured views and reduce the inherent noise in image processing.
1 computational results
to study the feasibility of the model  we solved both the mdp and pomdp formulations of the handwashing adl. the cost of calling the caregiver in both trials was set very high   1   to ensure that this action was never taken 
time  s 	1	1	1	1	1
key frame	
plan step	a	b	b	d	g	g	d	g	k
            time to wash	 put some	 rinse your	 pick up the	 john  rinse 	 use the towel	 thank you  john prompt	your hands  john 	soap on	hands under 	towel on	your hands 	on your right 	we're all 
	your hands 	the water 	your right 	done here 
figure 1: example sequence during a trial in which prompts were selected by the mdp policy. prompts were read by a human caregiver. the plan steps are those from figure 1. the user turns the water on and off independently  at 1 and 1 seconds  respectively   but must be prompted for all other steps. the user regresses at 1 seconds by putting more soap on his hands  which the system recognizes and thus givesa second prompt to rinse hands.
as required by the evaluation procedure we describe in section 1. the version of the fully observable mdp we solved has 1 variables  1 1 states  and 1 actions  rendering explicit state-based approaches impossible. however  due to the dbn structure  we were able to solve this problem exactly using the spudd algorithm . an optimal policy was produced in 1 minutes  and was represented using an add with 1 internal nodes and 1 leaves. the optimal value function was an add having 1 internal nodes and 1 leaves. we discuss evaluation of this policy below.
　the pomdp model has one additional variable  hence a state space of size 1 1  the same action space  and 1 observations. it is far beyond the reach of any exact solution techniques or model-based approximation methods. as a result  we developed a new approximation algorithm which exploits the structure in the system dynamics and rewards  as represented by our adds . perseus-add consists of the perseus algorithm   a randomized point-based version of value iteration  reconstructed to take advantage of add structure . the algorithm results in a policy of 1 state-value functions  α-vectors   computed in 1 hours.
　the relative quality of the mdp and pomdp policies demonstrates the importance of accounting for noise and user characteristics. while the pomdp policy is approximate  we evaluated it in simulation from the starting state  using 1 trials of 1 steps  with resp uniformly distributed in the initial belief state  and the specific value of resp drawn randomly for each trial. the average value obtained by the pomdp policy is 1 over 1 steps. the value of the optimal policy for the fully observable mdp  ignoring resp  is 1  which provides an upper bound on the value of the unknown optimal pomdp solution . however  this mdp value cannot be realized in the partially observable environment. so we implemented the mdp policy in this environment by computing the most likely state at each stage of the simulation and applying the corresponding mdp policy choice. in simulation  this attains an average value of 1 over 1 steps. we see that the mdp model provides a reasonable approximation for this pomdp despite its limitations. however  the full pomdp policy  despite only being solved approximately  outperforms the mdp policy by a significant margin. the value of pomdp modeling is apparent when one considers that the computational overhead in solving the policy is borne offline-once computed  the optimal policy is applied in real-time without any significant computation. all that is required online is simple belief-state updating  the critical component in allowing the policy to choose different prompting strategies for different users as beliefs about these users evolve.
　it is also interesting to note the customization of the pomdp policy based on its estimate of the resp variable. when a patient tends to be slow to respond to prompts  it is better to wait several time steps before repeating the prompt. for instance  when a patient has reached plan step g and was prompted to dry her hands two time steps ago  the pomdp policy repeats the prompt when the probability of resp = high is sufficiently high  but waits when it is low.
similarly  when plan step j is reached and the patient was prompted to turn off the water two time steps ago  the prompt is repeated only when responsiveness is believed to be high. in contrast  the mdp policy doesn't have the ability to estimate the level of responsiveness since it is not directly observable. as a result  in both cases it repeats the prompt at the risk of annoying a slower  less responsive patient.
1 caregiver evaluation
while the evaluation of the mdp and pomdp policies in simulation is useful  the true value of the system can ultimately only be gauged in clinical trials. clinical trials with alzheimer's patients are currently underway to test our results. these trials are based on improved versions of the models  see section 1 . results of these trials will be reported in a longer version of this paper.
　before undertaking clinical trials  an efficacy study of the mdp policy was undertaken to confirm its plausibility before applying it to dementia patients. in this study  an actor with considerable experience with dementia patients simulated the behavior of a user in the handwashing adl during 1 trials  each of which was videotaped. this simulated patient was guided using either the prompting strategy given by the mdp policy  or by a professional caregiver with over 1 years experience. during the caregiver trials  the caregiver acted naturally  prompting using her own strategy. during the mdp trials  the same professional caregiver read  verbatim  the prompts provided by the mdp policy  in order to prevent verbal distinction between caregiver and mdp trials. the hl and wo variables  describing the location of the actor's hands and whether the water was on  were manually annotated by a researcher during the trials  thus fulfilling the perfect observability assumption of the mdp  note that using the vision system would not have allowed accurate implementation of the mdp policy . the videos were then viewed by 1 professional caregivers  different from the prompter   who evaluated the performance of the prompting in each trial. these evaluators were unaware that the prompts in some trials were selected by a computer  this is why the call caregiver action could not be used .
　fig. 1 depicts snapshots of one of the mdp-guided scenarios used in the efficacy study. in this case  the handwashing subject is able to complete the steps of turning on the water  step b  and turning the water off  step k  independently. the subject ignores the prompt given to him at t = 1s to dry his hands and regresses in the activity to step d by applying soap instead. the planning system copes with this by identifying the regression  and prompting him to rinse his hands a second time. the majority of the time the prompting screen remained blank  as portrayed at t = 1s and t = 1s  allowing the subject time to attempt each step on his own.
　six of the 1 scenarios  three mdp and three human  were chosen for evaluation by 1 professional caregivers.1 each evaluator rated the strategy employed in each of the six scenarios  using a five-point likert scale  on five criteria: identification  the prompt s  given appropriately identified the next task ; detail  level of prompt detail appropriate ; time  an appropriate amount of time to attempt task provided before being prompted ; repetitions  number of prompt repetitions was appropriate ; and overall effectiveness  patient was guided effectively . free-form comments were also provided.1 quantitative results  see fig. 1  clearly indicate  results are statistically significant  that the professional caregiver outperforms the mdp policy in all evaluated aspects. indeed  we had no expectation that the mdp policy would perform as well. of course  our goals are much more modest; we intend the system to supplement human caregivers  not replace them. furthermore  the standard of an experienced professional caregiver sets the bar quite high.
　despite this  we were quite encouraged by the performance of the mdp. the average rating of the mdp was higher than that of the human professional in 1 of 1  1%  evaluator-aspect pairs. furthermore  qualitative comments indicate that the evaluators viewed the mdp policy as adequate  and we believe the evaluators to be rather critical given the average rating of the human caregiver ; and none suspected the prompting to be computerized. these facts suggest the mdp prototype can serve as the basis for a supplemental assistive device. finally  many of the critical comments made of the mdp policy reflected issues that could easily be remedied  and do not reflect a weakness in the mdp approach per se . for instance  many evaluators felt that turning the water on before asking a patient to use soap  or turning the water off before drying hands  provides a natural cue as to what the next activity step is  different transition priors or further data for training the transition model could readily reflect this . suggestions were also made regarding the language construction of prompts  the use of positive feedback and a friendly voice  and checking water temperature before allowing the patient to immerse his/her hands. none of these require modification of the current mdp model.

figure 1: caregiver ratings of mdp effectiveness for 1 criteria.
　addressing certain evaluator criticisms would require some modifications of the model  or fine-tuning of parameters . these include: using prompts that are tailored to the abilities of the user; giving the patient enough time for step completion; and incorporating visual cues  something that we have developed and tested independently . finally  in some situations  the patient asked the caregiver a question  such as  where's the soap  . with no dialog model or speech sensors  the mdp cannot react to this  which evaluators viewed negatively . however  pomdp models of spoken dialog  could be used for this purpose in the future.
1 further model validation & improvement
the efficacy study supports the value of the pomdp approach. although the study was based on a handcrafted mdp model  the policy evaluated by caregivers showed promise as a valuable means of relieving caregiver burden. since the pomdp policy outperforms this mdp policy in simulation  we expect the pomdp model to offer additional value in a realistic settings. ultimately  the value of the pomdp approach must be verified in a clinical setting. clinical trials are currently underway in which both the mdp and pomdp approaches are being compared to human caregivers in guiding alzheimer's patients through handwashing. the pomdp policy being evaluated is  in fact  somewhat different than the one reported here; we focus on the direct comparison of the pomdp described above with the mdp since it is a direct extension of the mdp  and thus differences in performance are largely due only to the power of the model to handle noise and user characteristics . the model used in the current clinical trials is significantly enhanced in several ways: a more realistic reward/cost function; a more realistic model the influence of level of responsiveness and response delays on step completion; model parameters estimated from video data of human caregivers; and a simplified and improved plan graph reflecting suggestions of the mdp policy-evaluators.
　our experience to date suggests a number of important avenues for model improvement. a difficult task is construct-
ing reasonable reward models; while transition models can be constructed from data  rewards cannot. one future aim is to validate the reward model indirectly by having caregivers critique the policies directly. while caregivers often have difficulty quantifying the utility of  say  partial versus complete task success  or the cost of prompting  they often  recognize a good policy when they see it.  using precise caregiver policy critique  we will re-engineer the reward function so that the resulting optimal policy is consistent with the suggestions  reminiscent of revealed preference in economics or inverse reinforcement learning  .
　another important direction is learning model structure and user behaviors from sequence data  rather than imposing our own structure on tasks. preliminary work along these lines is reported in . finally  generalizing the model to more directly account for continuous nature of both time  states  e.g.  hand position  and observations in this domain is of critical importance for more accurate modeling and useful prompting. preliminary results dealing with continuous observations in a simplified version of our domain  using a new algorithm for solving continuous observation pomdps  are reported in .
1 concluding remarks
we have proposed a decision-theoretic view of adl assistance for people with dementia  arguing that pomdps provide an ideal model for such problems. while we developed this model for a specific adl  the modeling principles embodied in our approach are broadly applicable. despite the size and complexity of the model  we have demonstrated that the mdp and pomdp solution algorithms can successfully be used to solve these problems  thus producing  approximately  sequentially optimal policies for adl prompting.
　considerable future work is required to apply the system in practice. clinical trials are currently in progress to help evaluate the effectiveness of the model and to help improve it. of particular interest are ways to to improve the  admittedly simple  user model. sessions with caregivers are also planned to help refine our reward function. we are currently exploring many of the interesting technical issues associated with revising mdp and pomdp models  especially reward functions  based on policy critique. we are also working toward applying pomdp models to other adls  such as toileting  and general living patterns. modeling these different tasks in a consistent  decision-theoretic fashion will eventually lead to hierarchies of pomdp cat systems throughout a home or care facility  all working towards the same goal. the ability of these pomdps to monitor user variables  such as resp  means that changes in a patient's state of health can be assessed automatically over different time scales.
acknowledgements
the authors thank the bonnie fernie for help with the experiments  and the caregivers who participated in the evaluations. the authors gratefully acknowledge the support of the natural sciences and engineering research council  nserc   intel corp.  the american alzheimer association  and the institute for robotics and intelligent systems  iris .
