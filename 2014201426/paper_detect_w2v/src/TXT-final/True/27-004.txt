 
this paper presents a method to incorporate knowledge from possibly imperfect models and domain theories into inductive learning of decision trees for classification the approach assumes that a model or domain theory reflects useful prior knowledge of th  task thus the default bias should accept the model s predictions as accurate even in the face of somewhat contradictory data which may be unrepresenlative or noisy however our approach allows the svslem to abandon the model or domain theorv  or portions thereof in the fact of sufficientlv contradictory data in particular we use c1 to induce decision trees from data that ha t heen augmented b  model or domaintheory-denvcd features' we weakly bias the svslem to select model-derived features dur ing decision tree induction but this preference is not dogmatically applied our experiments vary imperfection in a model the representa tiveness of data and the veracitv with which modfl-demed feature are preferred 
1 	introduction 
when human expertise is nonexistent or very weak relative to a particular domain/task and when data is plentiful machine induction from data mav be the only reasonable approach to task automation in contrast  when expertise is strong  then encoding the expert s model or domain theory via traditional knowledge acquisition strategies ma  be the best approach in fact  this human expertise may stem from induction over a much larger data sample than is available at the time task automation is undertaken 
　in many cases  however  conditions are indeterminate as to whether sole reliance on machine induction or human expertise is most appropriate human expertise may not be 'perfect and/or data may not be as plentiful as desired in cases where some data is available and human expertise is less than perfect an advantageous strategy may be to exploit both in an appropriate way 
　there is a growing body of work that combines modelbased or domain-theory knowledge with empirical learning from data clark and matwin  assume that 
doug fisher 
computer science department 
vanderbilt university 
	nashville  tennessee 	1 
　　　　　ii s   dfisher vuse vanderbilt edu 
an analyst-specified model mediates empirical learning the rules derived from a machine-induction system are ace1pted as long as they do not contradict the biases found in the model evans and fisher  employ a similar strategy - a human analyst may specify weak rules  e g when printing-plant humidity is low  a certain kind of printing error known as banding is more likely  to occur  if inductively-derived rules indicate an opposite trend then the learning system s default strategy is to reject the rule derived through induction in the case of both these approaches tht model or domain theory is deemed correct in its characterization of the domain task though it may not be a very deep characterization inductive learning is used to flesh out rules that are con sistent with the model  e g by selecting the particular numeric thresholds that distinguish high from moderate and low   or discovering rules relevant to a part of the domain space that are not addressed by the model or weak domain theorv at all 
　in the approaches above if the data contradicts the model then the implicit assumption is that the data are noisy or unrepresentative drawn from a verv small subspace of the data other approaches known is theory revision methods  ourston l1   towell et al 1  may give more credence to the data in these systems contradictions result in a revision of the domain theory to bring it in line with the data drastal el al  rendell and seshu  and ortega  suggest an alternative strategy that loosely couples empirical learning and model-based reasoning tht data is augmented by features that are actually intermediate terms of the domain theorv and which are deemed true of a datum by deductive application of the domain theorv induction is then performed over this augmented data set if domain-theory-derived features are included in rules denvea inductively  then this suggests a rough consistency between the model and data model features m iv be viewed as somewhat better predictors than raw features because noise is mitigated if model features are not referenced m a resultant classifier ihis may speak to imperfections in the model and/or this behavior mav stem from an unrepresentative data sample in bolh cases model-derived features ma  not look as informative as raw' features relative to the available data 
　this paper describes a strategy that augments data with domain-theory derived 'features 	but unlike previ 
	ortega and fisher 	1 
ous work we bias an adaptation of c1 quinian 1  to select domain-theory based features even when this conflicts somewhat with c1 s original bias to select the most informative feature as computed over the data the intent is to guard against the possibility of unrepresentative data however the domain-theory preference bias mav be overridden if c1 s original bias is suffic i e n t opposed to the domain-theorv preference bias the intent here is to acknowledge that there mav be some imperfections in the domain theorv our evpenments van imperfection in a model the representativeness of data and the the veracitv with which modelderned features are preferred 
1 	i m p l e m e n t i n g a f l e x i b l e 
d o m a i n - t h e o r y p r e f e r e n c e b i a s 
the approach described in this paper was motivated by our attempts to inductively build classifiers of faults of the reaction control system  rc s  of the space shuttle a mixed qualitative/quantitative model for fault pr  diction was available  robinson 1  as well as simulalted data representing svstem faults and normal behavior for each available datum the model was used to pre diet the fault this prediction was added as a featurr to the datum as were various intermediate computations made b  the model for the data point the data points augmented in this way were then given to ca '  which constructed a classifier that predicted either a svstem s fault or normal operation if the model were perfect then we would expect that c1 would build a tret that only tested the model-based final prediction such a tree would indicate that if a new datum w  re encountered  represented bv readings of various pressures and temperatures and other obsrrvables  then one should simply simulate the model on this datum and use the model-based final prediction in the cas  of certain imperfections a decision tree that tested vinous raw features as well as various model-based features might be constructed 
　to our initial surprise c1 consistently constructed trees that never or rarely referenced any model-based features rather than taking this as evidence of significant model imperfection or that the model added little or no information  above and beyond that implieit in the raw features a nasa analvst familiar with this application indicated that the simulated data used for training was unrepresentative or skewed - it represented a verv small subspace of the rcs description space 
　this work motivated an approach that weakly biases our adaptation of c1 to select model-based features in particular for purposes of this paper we assume a propositional domain theory used for classification that is acyclic and directed from the observable propositions to a final classification a partial description of the perfect domain theory for the audiology domain used in our experiments is shown in figure 1 as a tree the domain theory is a set of rules  each one consisting of a set of conditions together with the classification predicted b  the rule in figure 1 the antecedents of a rule are listed at the leaves of the tree each condition is an attribute-value pair  e g air=ptofound  there may 

	figure 1 	levels in audiology theorv 
be several rules thai predicl a particular clasmhcal ion as illustrated bv thf several possibh rules hading to each classification  e g ol itis media  in figure i 
　we characterize model feitures extracted from a the one of this type according to their distance in the th orv hierarchy to the final model prediction 
  level 1 model prediction feature we generate d single model prediction feature its value is the fin l prediction mad' bv the rule interpreter  using the rules of the given theorv   
  level 1 int  rrnediate concept feature* one of these features is generated for each possible ela.s sification in the theory each inte rmedial  concept feature corresponds to a logical or of the rules that predict particular classifications 
  level 1 rule features features of this. tvpe are generad for each rule in the theory a rule feature follows from the logical a n d of its antecedents 
  level 1 raw features each rul* anti cedent is t binary test as to whether an attribute takes a par ticular valuf on an example raw features whether rule antecedents or not are observable and are mi tiallv the exclusive means of representing data 
　to bias c1 r  towards model features clost r in the hierarchy to the final model prediction  we order features according to their level number from the model predic tion feature through niternn tliate concept fiatures to rule featurrs  and raw features at each step during in duction our variation of   1 chooses a feature of smallest level number unless a statistically-significant better feature  in terms of c1 s information score  of larger level number is found hence  c1 will choose the model prediction feature unless sufficient evidence is present in the data to refute this choice 
　thus we bias our inductive algorithm toward the model prediction feature and other features closer to it  of small level number  in a situation where we have a reasonably accurate model  and the available data is unrepresentative we expect our model-biased method to work better than a default strategy of choosing the fea-

ture of highest information  value according to the available data  eg as in the standard c1 r   nonetheless  if the data sufficiently contradicts the model the modelbias can be abandoned and should we choose the model can be revised acrordinglv 
1 using domain-theory bias with hypothesis testing 
the major difference between the original and our variation of   a 1 is the manner in which a feature is selected for each nod*  of a decision tree c1 selects the feature with the highest information value according to th* information gam ratio measure rather than selecting i lie fealure with the highest informal ion value outright our variation of c   r  requires that this value be statistical  significantly higher than the information value uf all features preceding it in a feature prefer'nee ranking like that described in the previous section put in another wav we select the highest feature in a preference ranking that has an information score not significantly worse lhan an  feature lower in the preference ranking 
　the above procedure is implemented b  the function selert f a t u r e     udt  shown m figure 1 where fp is the feature preference ranking ' i  is the set of t r i m ing data associated with the current node uifo fj 1  is the vahii of   1 s information measure for feature r} when evaluated on the set of dala d and fj is the hsl of the features sorted in desce nding order according to  his measure selectfeature    ode  initiallv chooses th -feature v  ith highest information vilu   l   first feature m f    however this feature is not accepted unless it  information value is significantly higher than all features of higher preference according to the fp ranking if so the candidate fealure is stl et d otherwise the high* r preference feature found bee omes the new candi date 1 be procedure is repeated unlil a significant dif fereiin is found or the f1 list is   vhausted 
　tht rt is also a minor difference between the cla-ssifi cation procedure of our system and the standard c1 algorithm for the situations where thert is insufficient data to select a lest for a particular node of the tree as a purelj data driven s stem the best c1 can do is to predict the most common class present in the current node instead since we assume our mode 1 is better lhan no information we use the prediction of our prior model 
　the critical component of  he function selectfeat u r e is the significantlybetter / - l l l   1 f jyrtj d  function shown in figure j this function returns true if the information value of feature fran t is estimated to be significantly higher than that of fpref according to a given level of statistical significance  iglevel i ins is done by testing the null hypothesis that the difference between the information values of frand and fprtj is zero if  his null hypothesis can be rejected with 1 - sigltvel confidence s i g m f i c a n t l y b e t t e r concludes that fcand is 
significantly better than fprrj 
!
　　in the current implementation the ranking is a total or denng features are sorted in ascending according to level number the ranking of features within a level number is 
arbitrary 

　if the. form of the probability distribution associated with c 1 r  s information measure is known and its parameters can be calculated then traditional statistical thtorv can b  u&ed 1o test significance this could be done for ihe information gain measure since musick el al  musick e1 ai 1 qi  prove that this measure is nor nnllv distributed and provide exphent formulas for the parameters of this distribution however the form of the distribution for the default measure used in   1 mfoi mation gain ratio is not known fortunately bootstrap wethodt   lfron and giong 1  allow for estimates of significance levels of arbitrary statistics when the form and parameters of the underlving distribution art not known  moreen 1  i ins is the method implemented in the funrtion sigiuflcantlybetter 
　in lfron s bootstrap methods an unknown complete population p is estimated b  repeated uniform subsampling with replacement from an available sample d of 
p 	from 	we 	obtain a. set of bootstrap subsamples where  q is a prespicihed number of subsarnples 	each 	  	is very likely to contain some duplicate-' and be missing some observations from d with the result that th  values of mfo  for each feature 1 j willhkelv b  different on each bootstrap suhsarnple d  	under some additional assumptions we then proceed as if the bootstrap samples were obtained from the actual population p 
　significantly better uses two different bootstrap methods described by noreen  the normal ap proximation method  and the shift method figure 1 shows the compulation of some quantities used in the above methods d i f f o the difference in information value between feand and fpref computed on the set of 
	ortega and fisher 	1 

training data d upb  the mean of our statistic of interest  difference in the information value between fcand and fpref  over the the bootstrap samples and apb the standard deviation of our statistic of interest over the bootstrap samples 
　the normal approximation method operates under the assumption that the sampling distribution of the statistic of interest  difference in the information value of fcand and fpref  under the null hypothesis  no differ ence  is normally distributed with mean zero and same variance as in the bootstrap samples this assumption is used to calculate p v n   the probability under the .norma/assumption that a value of our statistic higher than 
or equal to    could have been obtained by chance to calculate pvn  we use  and the probability function/tables for a standard normal distribution 
　the shift method assumes that the sampling distribution of the statistic of interest on the complete population p has the same shape  but different mean  than the sampling distribution over the bootstrap samples pg to determine the corresponding pvs  the probability under the shift assumption that a value equal or higher than diffo could have been obtained by chance we count the number of times that the value of the statistic on bootstrap samples is higher than a shift criterion 
　　　　　　　　　　  and divide that count by the number of subsamples nb 
　we only decide that the feature feand is significantly} better than fpref if it is significantly better according to both the normal approximation method and the shift method significantlybetter is computationally quite expensive however during the selection of most features this needs to be done verv few times if the feature with the highest initial information value is the feature of highest preference significantlybetter never needs to be computed when other features are lnitally selected onh the features with higher preference are checked as soon as one significant difference is computed  no other significance computation is necessary for the sake of efficiency the case where two or more insignificant dif ferences could account for a single significant difference is not considered our interest is not in precise computations of significance  but rather the qualitative effect of significance testing on the selection of attributes m ca 1 while retaining a reasonable level of efficiency 
to test our approach we conducted experiments with the audiology dataset from the uci  university of california at irvine  machine learning repository this database contains 1 examples from 1 classes each example is described bv 1 discrete features 
　for each of the 1 learning trials of our experiments a test set of 1 examples and a disjoint training set of 1 examples were randomly and uniformly selected because we want to lest the robustness of various strategies in the face of unrepresentative data we sorted the train ing examples according to their euclidian distance fiom a randomly selected datum the training set was fur ther divided into subsets which contain the first 1  1  1  1  and 1  i e   all  examples of the sorted training set decision trees are learned for each of these subsets of training data 
　the dependent variables of interest are predictive accuracy and the level  as illustrated in figure 1  of the root feature of the decision trees learned under different conditions due to the recursive nature of decision tree induction we expect that the tendencies observed at the root can be extrapolated to other nodes of the tree 
　the independent variables are the size of the training set  the significance level used for hypothesis testing in our variation of   a 1 and the degree of model imperfection note that while varying the size of the training set we are also varying its degree of skewedness because training data are ordered based on euclidean distance  small samples lend to be drawn from a small portion of the data description space  the larger the training dataset  the higher the proportion of the complete data set it covers  and thus the more representative it becomes for the largest data set of 1 examples all skewedness disappears since all 1 examples were ran-


domlv chosen from the complete data set w  present results from skewed sampling as this tends to repp sent worst case conditions for our learning system we have also experimented with traditional sampling  for all training sizes  thus allowing us to better tease apart the influence of skew and training set size though this paper does not elaborate on this issue significance levels of 1% 1%   1% and l% are varied to indicate increasing confidence in the quality of our models 
　we follow mooney s approach  moonev 1  for generating theories of varying degrees of imperfection a perfect theory  1 e   one that correctlv  classifies 1% of all audiology examples was first constructed b  running c 1 on the complete data set of audiology examples with all pruning disabled this theory  named foo  contained 1 rules with an average of 1 antecedents per rule imperfect theories  named f1 flolo f1 and 
f1  were generated bv randomlv adding and then randomly deleting a percent of all conditions from the rules of the perfect theory  a corresponding 1% 1% 1% and 1%  this results in contaminated theories with errors both of omission and of commission the accuracies over the complete data set of the imperfect theories f1 f1  f1 and/1 are 1%  1% 1% and 1%  respectively for comparison  the accuracy obtained if we always predict the most frequent class in the dataset is 1% 
1 	experimental results 
figure 1 shows results from a baseline study the curve labeled sk-c1 shows the results of standard c1 on skewed trials with 'raw' features only the rest of the curves in this figure show the accuracv and root feature level for c1 when model features are added to the description of the data  but no preference ordering or hypothesis testing is done we can see that accuracv improves significantly  over sk-r1  when a domain theory is exploited even for a low quality theory such as f1 these results compare favorably  to other systems tested on this domain  mooney 1}   ourston 1  
　an interesting fact illustrated in figure 1 is that the number of training examples required tor c1 with a domain theory {sk-fxx curves  to produce an accuracy equivalent to the corresponding theory alone seems to be inversely proportional to the qualitv of the theory the two extremes being f1  1 training examples required to reach 1 % accuracy  and foo  all training examples required to reach 1% accuracy  ideally however  he accuracy of our system should be equal or better than the accuracv of the model alone  or the c1  learning algorithm alone this only occurs for large enough or representative enough training data sets this behavior is not unique to our system it can be observed in the published learning curves shown for some systems that combine analytical and empirical learning pazzani and kibler 1   ourston 1l  as we will see significance testing of ranked features appears to mitigate this undesirable behavior 
　figure 1 gives a good indication of the tvpe of features  selected with theories of varying quality. standard c1 can only access raw features  level 1  c1 with features from a perfect modf 1  1 e foo  chooses almost exclusively  hut not always  the model prediction feature  level 1  with lower quality theories c 1 gradually chooses features of greater level numbers however for the perfect model  there should be no reason to choose anv feature other than the model prediction feature this does not happen due in part to a known bias of the information gain ratio against features with many values  in the audiology domain the model prediction feature has 1 possible values  other model features are binary and raw features have few values  however as we will see next this problematic bias can be mitigated with the use of significance testing 
　figure 1 shows the effects of significance testing of ranked features when c1 is augmented with features from a perfect model rather than the 1 examples 
   1tables corresponding to  he graphs in this paper ron laming means and standard deviations ean 1m found at http //www vuse vanderbilt edu/ dfisher/iech reports/ijcai1-tables pa 
   1 here we use standard 1 for learning for testing wt use the classlfiration procedure described in section 1 l e predicting with the model rather than the most frequenl class at leaves of the decision tree where there is insuffieirnt data for further decomposition 
	ortega and fisher 	1 

figure 1 average acruracv and root feature level of decision trees learned with features from a perfect model under varying levels of significance 
figure b average accuracv and roof feature level of deci sion trees learned with features from the fl1lo imperfect model under varving levels of sign

needed before hypothesis testing of ranked features results in 1c accurarv with only 1 examples when using a 1c significance level or with just 1 examples when using more strict significance levels  1% 1% or 1%  this figure also shows how the average root featur  level is gradually} reduced with stricter significance levels 
　perfect domain theories are an interesting boundary  case  but most interesting theories are imperfect the graphs of figure b illustrate how stricter levels of sig nificance achieve our objective of biasing c1 toward features of smaller level number using the flolo domain theory this figure also shows that the accuracy obtained with the flolo imperfect theorv improves consistently with stricter levels of significance testing  1c 1% 1%  for any size of the training set  including the complete training set of 1 examples in addition  while with no significance testing  or the almost equivalent significance testing at the 1% level  at least 1 examples are needed to obtain better accuracies than the flolo theorv alone with stricter levels of significance testing only jo examples are required for the 1% sig-
nificance level  accuracy  is better than other significance levels when the training sets contains less than 1 examples  and worse when the training set contains more than 1 examples thus at least in this domain there is a breakeven point for significance levels that depends both on the quality of th  model and the size of the training set after which stricter significance levels are detelmental in our experiments the size of the training sets corresponds to increasing quality in t he available data in the sense that thev are better representatives of the complete population both due to the sheer amount of data and due to the fact that the skewedness effect we introduced in the training set tends to dimmish as tht size of the trainings sets are increased 
　for lower quahtv theories similar behavior is observed with increasingly strict significance levels  accuracy improves when the training stt contains few examples  and decreases with training sets that contain manv examples thus  for every combination of model quality  l e amount of contamination in the model  and data quahtv  level of skewedness and number of training examples  there is an optimal level of significance between the extremes of 1% and 1% in this domain however the choice of a beneficial but perhaps non-optimal sig 
nificance level is not difficult significance levels only seem to become detrimental for large data sets when we use significance levels stricter than 1% values between 

1% and 1% always seem to improve accuracy  perhaps non-optimally  for any size of the training set thus  expert intuition about the trustworthiness of an existing model with respect to the available can be incorporated into our learning algorithm to obtain additional perfor mance improvements 
1 	c o n c l u d i n g r e m a r k s 
in this paper we address a situation that we believe to lie of practical interest learning whenever we have a model believed to be of good quality but imperfect nevertheless together with a set of data of unknown rep rescntativeness and quality we present a method that attempts to take advantage of both the model and the data plus our prior knowledge about the quaht  of the model our mc thod biases empirical learning in a flexible manner such that model-based features  or more generally preferred features bised on some a prion determined preference ordering are selected unless sufficient refuting evidence appears in the data the amount of evidence required is determined bv statistical significance and is set by the user according to his/her confidence in the quality of the available model 
　our expenniental results show that when features gtn erated from a model are smply added d to the description of the data aecuracy is increased to a degree proportional to the quality of the model however sum*  problems with this simple approach are illustrated by the fact that perfect models only result in perfect accuracy with large or very representivetve srts of traning exam ples if significance testing with a preference  ordering is used with a pe rfect model our system heroines more robust in the presence of skewed data few examples are then needed 1o obtain perfed accuracy further with imperfect models of good quality we obtain additional 
increases in accuracy for any number of training exam pies 
　although significance testing lias been used previously m machine learning methods such as for the pruning of decision trees  qumlan  1t b  our use. of ibis concept for flexibly introducing prior knowledge bias in empirical learning seems to be novel 
a c k n o w l e d g e m e n t s 
this research was supported bv a grant from nasa 
ames research   enter  nag 1  to doug fisher we thank deepak kulkarrm and peter robinson for early dis cussion on initial strategies that led to those described here 
r e f e r e n c e s 
 clark and malwm 1  p   lark and s matw in using qualitative models to guide inductive learning in proceedings of the tenth international conference on 
machine learning pages 1b amherst  ma 1 
 draslal et al   1  g drastal  g czako and s raatz induction in an abstraction space a form of constructive induction in proceedings of the eleventh international joint conference on artificial intelligence  pages 1 detroit mi  1 
 efron and gong  1  bradley efron and gail gong a leisurely look at the bootstrap the jackkmfe and cross-validation the american statistician 1  1-
1 
 evans and fisher  1  r r evans and d fisher overcoming process delavs with decision trer induction ieee expert  1  1 1 
 mooney 1  r j mooney induction over the une  plained using overly-general theories to and concept learning machine learning 1-1 
 musick et al  1  ron musick jason   allell and stuart russell decision theoretic suhsampling for induction on large databases in proceedings of the tenth international  onference on machine learning pages 1 amherst  ma 1 
 noreen 1  eric w n oreen computer intensive methods for testing hypotheses an introduction 
john wiley k. sons new york ny 1 
 ortega 1  j ortega making the most of what you ve got using models and data to improye learning rate and prediction accuracy technical report   s-1 computer selence dept  anderbilt university abstract appears in proceedings of the twlfth national   onferece nce on artificial intelligence p 1 seattle   a 1 
 ortega in preparation  j ortega making the most of what you got using models and data to improve prediction tecuracy phd thesis.   anderbilt univer sity nashille tn in preparation 
 ourston 1  d ourston i stng erplanation-bastd and empincal methods in theory reaision phd the-
	sis iniversitv of texas austin  1   	1 
 pazzatn and kibler 1  m pazzani and d kibler the utility of knowledge in inductive learning ma chine learning 1 1 1 
 qninlan 1b  1 r quinlan 	induction of decision trees machine learning 1 1b 1f  
 quinlan 1i  i r quinlan   1 programs for ma 
 hine learning morgan kaufmaim san maleo c   
1 
 rendell and seshu 1  larry rendell and raj seshm learning hard concepis through constructive induction framework and rationale computational intelligence 1  1 1 
 robinson 1  peter robinson auitomated fault diagnosis algorithms for the reaction control system of the space shuttle technical report fla-1 n a s a . ames ai research center 1 
 towell et al 1  g g towell j    shavlik and m o nsoordewier refinement of approxmate domain theories by knowledge-hased in ural networks in proceedings of the eighth national   onfertice on  rtificial intelligenct pages1l-1cb boston  ma 1 
	ortega and fisher 	1 1 
1 	learning 











1 	learning 

1 	learning 

1 	learning 







1 	learning 

1 	learning 

1 	learning 







