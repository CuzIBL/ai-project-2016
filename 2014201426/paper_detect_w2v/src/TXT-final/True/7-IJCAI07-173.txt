
td-falcon is a self-organizing neural network that incorporates temporal difference  td  methods for reinforcement learning. despite the advantages of fast and stable learning  td-falcon still relies on an iterative process to evaluate each available action in a decision cycle. to remove this deficiency  this paper presents a direct code access procedure whereby td-falcon conducts instantaneous searches for cognitive nodes that match with the current states and at the same time providemaximal reward values. our comparative experiments show that td-falcon with direct code access produces comparable performance with the original td-falcon while improving significantly in computation efficiency and network complexity.
1 introduction
reinforcement learning  sutton and barto  1  is an interaction based paradigm wherein an autonomous agent learns to adjust its behaviour according to feedback from the environment. classical solutions to the reinforcement learning problem generally involve learning one or more of the following mappings  the first linking a given state to a desired action  action policy   and the second associating a pair of state and action to a utility value  value function   using temporal difference methods  such as sarsa  rummery and niranjan  1  and q-learning  watkins and dayan  1 . the problem of the original formulation is that mappings must be learned for each and every possible state or each and every possible pair of state and action. this causes a scalability issue for continuous and/or very large state and action spaces.
　neural networks and reinforcement learning have had an intertwining relationship  kaelbling et al.  1 . in particular  multi-layer feed-forward neural networks  also known as multi-layer perceptron  mlp   have been used extensively in many reinforcement learning system and applications  ackley and littman  1; sutton  1 . under the recent thread of research in approximate dynamic programming  adp   si et al.  1   mlp and gradient descent backpropagation  bp  learning algorithms are commonly used to learn an approximation of the value function from the state and action spaces  value policy  and/or an approximation of the action function from the state space  action policy . mlp and bp however are not designed for online incremental learning as they typically require an iterative learning process. in addition  there is an issue of instability in the sense that learning of new patterns may erode the previously learned knowledge. consequently  the resultant reinforcement learning systems may not be able to learn and operate in real time.
　instead of learning value functions and action policies  self-organizing neural networks  such as self-organizing map  som   are typically used for the representation and generalization of continuous state and action spaces  smith  1 . the state and action clusters are then used as the entries in a q-value table implemented separately. using a localized representation  som has the advantage of more stable learning  compared with backpropagation networks. however  som remains an iterative learning system  requiring many rounds to learn the compressed representation of the state and action patterns. in addition  as mentioned in smith  1   som is expected to scale badly if the dimensions of the state and actions spaces are significantly higher than the dimension of the map. as such  most applications of som are limited to low dimensional state and action spaces.
　a recent approach to reinforcement learning builds upon adaptive resonance theory  art   carpenter and grossberg  1   also a class of self-organizing neural networks  but with very distinct characteristics from som. ueda et. al  1  use art models to learn the clusters of state and action patterns. the clusters are in turns used as the compressed states and actions by a q-learning module. ninomiya  1  couples a supervised art system with a temporal difference reinforcement learning module in a hybrid architecture. whereas the states and actions in the reinforcement module are exported from the supervised art system  the two learning systems operate independently. the redundancy in representation unfortunately leads to instability and unnecessarily long processing time in action selection as well as learning of value functions. through a generalization of art from one input pattern field to multiple pattern channels  tan  1 presents a neural architecture called falcon  fusion architecture for learning  cognition  and navigation   that learns multi-channel mappings simultaneously across multimodal input patterns  involving states  actions  and rewards  in an online and incremental manner. for handling problems with delayed evaluative feedback  reward signal   a variant of falcon  known as td-falcon  tan and xiao  1   learns the value functions of the state-action space estimated through temporal difference  td  algorithms. compared with other art-based systems described by ueda et. al and ninomiya  td-falcon presents a truly integrated solution in the sense that there is no implementation of a separate reinforcement learning module or q-value table.
　although td-falcon provides a promising approach  its action selection procedure contains an inherent limitation. specifically  td-falcon selects an action by weighting the consequence of performing each and every possible action in a given state. besides that the action selection process is inefficient with a large number of actions  the numerating step also assumes a finite set of actions  rendering it inapplicable to continuous action space. in view of this deficiency  this paper presents a direct code access procedure by which td-falcon can perform instantaneous searches for cognitive nodes that match with the current states and at the same time provide the highest reward values. besides that the algorithm is much more natural and efficient  td-falcon can now operate with both continuous state and action spaces. our comparative experiments based on a minefield navigation task show that td-falcon with direct code access produces comparable performance with the original td-falcon system while improving vastly in terms of computation efficiency as well as network complexity.
　the rest of the paper is organizedas follows. for completeness  section 1 presents a summary of the falcon architecture and the associated learning and prediction algorithms. section 1 presents the new td-falcon algorithm with the direct code access procedure. section 1 introduces the minefield navigation simulation task and presents the experimental results. the final section concludes and provides a brief discussion of future work.
1 falcon dynamics
falcon employs a 1-channel architecture  figure 1   comprising a cognitive field and three input fields  namely a sensory field  for representing current states  an action field f1 for representing actions  and a reward field f1 for representing reinforcement values. the generic network dynamics of falcon  based on fuzzy art operations  carpenter et al.  1   is described below.

figure 1: the falcon architecture.
input vectors: let s =  s1 s1 ... sn  denote the state vector  where si （  1  indicates the sensory input i. let a =  a1 a1 ... am  denote the action vector  where ai （  1  indicates a possible action i. let r =  r r．  denote the reward vector  where r （  1  is the reward signal value and r．  the complement of r  is given by r． = 1   r. complement coding serves to normalize the magnitude of the input vectors and has been found effective in art systems in preventing the code proliferation problem. as all input values of falcon are assumed to be bounded between 1 and 1  normalization is necessary if the original values are not in the range of  1 .
activity vectors: let xck denote the activity vector for k = 1 ... 1. let yc denote the activity vector. weight vectors: let wjck denote the weight vector associated with the jth node in for learning the input patterns in. initially  contains only one uncommitted node and its weight vectors contain all 1's. when an uncommitted node is selected to learn an association  it becomes committed.
parameters: the falcon's dynamics is determined by choice parameters αck   1 for k = 1 ... 1; learning rate parameters βck （  1  for k = 1 ... 1; contribution parameters γck （  1  for k = 1 ... 1 where; and vigilance parameters ρck （  1  for k = 1 ... 1. code activation: a bottom-up propagation process first takes place in which the activities  known as choice function values  of the cognitive nodes in the field are computed. specifically  given the activity vectors xc1  xc1 and xc1  in the input fields respectively   for each node j  the choice function tjc is computed as follows:
	 	 1 
where the fuzzy and operation … is defined by  p … q i 《 min pi qi   and the norm |.| is defined by  for vectors p and q. in essence  the choice function tj computes the similarity of the activity vectors with their respective weight vectors of thenode j with respect to the norm of individual weight vectors.
code competition: a code competition process follows under which the f1c node with the highest choice function value is identified. the winner is indexed at j where
	: for allnode j}.	 1 
when a category choice is made at node; and yjc =
1 for all. this indicates a winner-take-all strategy.
template matching: before code j can be used for learning  a template matching process checks that the weight templates of code j are sufficiently close to their respective activity patterns. specifically  resonanceoccurs if for each channel k  the match function of the chosen code j meets its vigilance criterion:
	.	 1 
the match function computes the similarity of the activity and weight vectors with respect to the norm of the activity vectors. together  the choice and match functions work cooperatively to achieve stable coding and maximize code compression.
when resonance occurs  learning ensues  as defined below. if any of the vigilance constraints is violated  mismatch reset occurs in which the value of the choice function tjc is set to 1 for the duration of the input presentation. with a match tracking process  at the beginning of each input presentation  the vigilance parameter ρc1 equals a baseline vigilance ρ．c1. if a mismatch reset occurs  ρc1 is increased until it is slightly larger than the match function mcj1. the search process then selects another node j under the revised vigilance criterion until a resonance is achieved. this search and test process is guaranteed to end as falcon will either find a committed node that satisfies the vigilance criterion or activate an uncommitted node which would definitely satisfy the criterion due to its initial weight values of 1s.
templatelearning: once a nodej is selected  for each channel k  the weight vector wjck is modified by the following learning rule: wjck new  =  1   βck wjck old  + βck xck … wjck old  .  1 
the learning rule adjusts the weight values towards the fuzzy and of their original values and the respective weight values. the rationale is to learn by encoding the common attribute values of the input vectors and the weight vectors. for an uncommitted node j  the learning rates βck are typically set to 1. for committed nodes  βck can remain as 1 for fast learning or below 1 for slow learning in a noisy environment. when an uncommitted node is selecting for learning  it becomes committed and a new uncommitted node is added to thefield. falcon thus expands its network architecture dynamically in response to the input patterns.
1 td-falcon
td-falcon incorporates temporal difference  td  methods to estimate and learn value functions of action-state pairs q s a  that indicates the goodness for a learning system to take a certain action a in a given state s. such value functions are then used in the action selection mechanism  also known as the policy  to select an action with the maximal payoff. the original td-falcon algorithm proposed by tan and xiao  1  selects an action with the maximal q-value in a state s by enumerating and evaluating each available action a by presenting the corresponding state and action vectors s and a to falcon. the td-falcon presented in this paper replaces the action enumeration step with a direct code access procedure  as shown in table 1. given the current state s  td-falcon first decides between exploration and exploitation by following an action selection policy. for exploration  a random action is picked. for exploitation  td-falcon searches for optimal action through a direct code access procedure. upon receiving a feedback from the environment after performing the action  a td formula is used to compute a new estimate of the q value of performing the chosen action in the current state. the new q value is then used as the teaching signal for td-falcon to learn the association of the current state and the chosen action to the estimated q value. the details of the action selection policy  the direct code access procedure  and the temporal difference equation are elaborated below.
1 action selection policy
the simplest action selection policy is to pick the action with the highest value predicted by the td-falcon network. however  a key requirement of autonomous agents is to explore the environment. this is especially important for an agent to function in situations without immediate evaluative feedback. if an agent keeps selecting the optimal action that it believes  it will not be able to explore and discover better alternative actions. there is thus a fundamental tradeoff between exploitation  i.e.  sticking to the best actions believed  and exploration  i.e.  trying out other seemingly inferior and less familiar actions.
　the -greedy policy selects the action with the highest value with a probability of  and takes a random action with probability   pe＞rez-uribe  1 . with a constant  value  the agent will always explore the environment with a fixed level of randomness. in practice  it may be beneficial to have a higher  value to encourage the exploration of paths in the initial stage and a lower  value to optimize the performance by exploiting familiar paths in the later stage. a decay -greedy policy is thus adopted to gradually reduce the value of  over time. the rate of decay is typically inversely proportional to the complexity of the environment as a more complex environment with larger state and action spaces will take a longer time to explore.
1 direct code access
in an exploiting mode  an agent  to the best of its knowledge  selects an action with the maximal reward value given the current situation  state . through a direct code access procedure  td-falcon searches for the cognitive node which matches with the current state and has the maximal reward value. for direct code access  the activity vectors xc1  xc1  and xc1 are initialized by xc1 = s  xc1 =  1 ... 1   and xc1 =  1 . td-falcon then performscode activation and code competition according to equations  1  and  1  to select a cognitive node. the following lemma shows that if there is at least one cognitive node s  encoding the current state  td-falcon with the given activity vectors will enable the selection of the cognitive node with the maximum reward value.
direct access principle: during direct code access  given the activity vectors xc1 = s  xc1 =  1 ... 1   and xc1 =  1   the td-falcon code activation and competition process will select the cognitive node j with the weight vectors wjc1 closest to s and wjc1 representing the maximal reward value  if one exists.
 proof  by contradiction : suppose td-falcon selects a  node j and there exists another node k of which the weight vector wkc1 is more similar to s than wjc1 and the weight vector wkc1 encodes a higher reward value than wjc1. as wkc1 is more similar to s than wjc1  we derive that
	.	 1 
likewise  as wkc1 encodes a higher reward than wjc1  we have
	.	 1 
table 1: td falcon algorithm with direct code access.

1. initialize the td-falcon network.
1. sense the environment and formulate a state representation s.
1. following an action selection policy  first make a choice between exploration and exploitation. if exploring  take a random action.
if exploiting  identify the action a with the maximal q s a  value by presenting the state vector s  the action vector a= 1 ...1   and the reward vector r= 1  to td-falcon.
1. perform the action a  observe the next state s  and receive a reward r  if any  from the environment.
1. estimate the revised value function q s a  following a temporal difference formula such as Δq s a  = αtderr.
1. present the corresponding state  action  and reward  q-value  vectors  s  a  and r  to td-falcon for learning.
1. update the current state by s=s'.
1. repeat from step 1 until s is a terminal state.
by equation  1   the above conditions imply that  which means node k should be selected by td-falcon instead of node j  contradiction .
 end of proof 
　upon selecting a winning node j  the chosen node j performs a readout of its weight vector to the action field such that
	xc1 new  = xc1 old  … wjc1.	 1 
an action ai is then chosen  which has the highest activation value
	 : for all node i}.	 1 
1 learning value function
a typical temporal difference equation for iterative estimation of value functions q s a  is given by
　　　　　　　　Δq s a  = αtderr  1  where α （  1  is the learning parameter and tderr is a function of the current q-value predicted by td-falcon and the q-value newly computed by the td formula.
　td-falcon employs a bounded q-learning rule  wherein the temporal error term is computed by
                                       .	 1  where  of which r is the immediate reward value  γ （  1  is the discount parameter  and max denotes the maximum estimated value of the next state s. it is important to note that the q values involved in estimating maxa q	are computed by the same falcon network itself and not by a separate reinforcement learning system. the q-learning update rule is applied to all states that the agent traverses. with value iteration  the value function q s a  is expected to converge to  over time. by incorporating the scaling term 1   q s a   the adjustment of q values will be self-scaling so that they will not be increased beyond 1. the learning rule thus provides a smooth normalization of the q values. if the reward value r is constrained between 1 and 1  we can guarantee that the q values will remain to be bounded between 1 and 1.

figure 1: the minefield navigation simulator.
1 the minefield navigation task
the objective of the given task is to teach an autonomous vehicle  av  to navigate through a minefield to a randomly selected target position in a specified time frame without hitting a mine. in each trial  the av starts from a randomly chosen position in the field  and repeats the cycles of sense  act  and learn. a trial ends when the av reaches the target  success   hits a mine  failure   or exceeds 1 sense-act-learn cycles  out of time . the target and the mines remain stationary during the trial. all results reported in this paper are based on a 1 by 1 minefield containing 1 mines as illustrated in figure 1.
　the av has a rather coarse sensory capability with a 1 degree forward view based on five sonar sensors. for each direction i  the sonar signal is measured by  where di is the distance to an obstacle  that can be a mine or the boundary of the minefield  in the i direction. other input attributes of the sensory  state  vector include the bearing of the target from the current position. in each step  the av can choose one out of the five possible actions  namely move left  move diagonally left  move straight ahead  move diagonally right  and move right. at the end of a trial  a reward of 1 is given when the av reaches the target. a reward of 1 is given when the av hits a mine. for the delayed reward scheme  no immediate reward is given at each step of the trial. a reward of 1 is given when the system runs out of time.
　for learning the minefield task  we use a td-falcon network containing 1 nodes in the sensory field  representing 1 complement-coded sonar signals and 1 target bearing values   1 nodes in the action field  and 1 nodes in the reward field  representing the complement-coded function value . td-falcon with direct code access employed a set of parameter values obtained through empirical experiments: choice parameters αc1 = 1 αc1 = 1 αc1 = 1; learning rate βck = 1 for k = 1 1 for fast learning; contribution parameters; baseline vigilance parameters ρ．c1 = 1 and ρ．c1 = 1 for a marginal level of match requirement in the state and action spaces  and ρ．c1 = 1 for a stricter match criterion on reward values. for temporal difference learning  the learning rate α was fixed at 1 and the discount factor γ was set to 1. the initial q value was set to 1. for action selection policy   was initialized to 1 and decayed at a rate of 1 until it dropped to 1.

figure 1: the success rates of td-falcon using action enumeration  ae  and direct code access  da  compared with those of bp reinforcement learner.
　to put the performanceof falcon in perspective  we further conducted experiments to evaluate the performanceof an alternative reinforcement learning system  using the standard q-learning rule and a multi-layer perceptron network  trained with a gradient descent based backpropagation algorithm  as the function approximator. we have chosen the backpropagation  bp  algorithm as the reference for comparison as gradient descent is by far one of the most widely used universal function approximation techniques and has been applied in many contexts  including q-learning  sun et al.  1  and adaptive critic  werbos  1 . the bp learner employed a standard three-layer feedforward architecture to learn the value function with a learning rate of 1 and a momentum term of 1. the input layer consisted of 1 nodes repre-

figure 1: the average normalized steps taken by tdfalcon using action enumeration  ae  and direct code access  da  compared with those of bp reinforcement learner.
senting the 1 sonar signal values  1 possible target bearings  and 1 selectable actions. the output layer consisted of only one node representing the value of performing an action in a particular state. a key issue in using a bp network is the determination of the number of hidden nodes. we experimented with a varying number of nodes empirically and obtained the best results with 1 nodes. to enable a fair comparison  the bp learner also made use of the same action selection module based on the decay -greedy policy.
　figure 1 summarizes the performance of td-falcon with direct code access  falcon-da   the original tdfalcon with action enumeration  falcon-ae   and the bp reinforcement learner in terms of success rates averaged at 1-trial intervals over 1 trials across 1 sets of experiments. we observed that the success rates of both falconda and falcon-ae increased steadily right from the beginning. beyond 1 trials  both systems can achieve over 1 percent success rates. the backpropagation bp  based reinforcement learner on the other hand required a much longer exploration phase. in fact  the bp learner only managed to reach around 1% success rates at 1 trials with a lower  decay rate of 1. therefore  in the first 1 trials  its success rates remained under 1%.
　to evaluate how well the av traverses from its starting position to the target  we define a measure called normalized step given by stepn = stepsd   where step is the number of sense-move-learn cycles taken to reach the target and sd is the shortest distance between the starting and target positions. a normalized step of 1 means the system has taken the optimal  shortest  path to the target. as depicted in figure 1  after 1 trials  both falcon-ae and falcon-da were able to reach the target in optimal or close to optimal paths in a great majority of the cases. the bp learner as expected produced unsatisfactory performancein terms of path optimality. considering network complexity  falcon-da demonstrated a great improvement over falcon-ae by creating

figure 1: the average number of cognitive nodes created by td-falcon using action enumeration  ae  and direct code access  da .
a much smaller set of cognitive nodes. in fact  the falconda networks consisting of around 1 cognitive nodes were almost the same size as a bp network with 1 hidden nodes. with a much more compact network structure  the efficiency of falcon-da in terms of computation time was also much better than falcon-ae. the average reaction time taken to complete a sense-act-learn cycle clocked 1 millisecond  in contrast to 1 to 1 milliseconds as required by falcon-ae.
1 conclusion
we have presented an enhanced td-falcon model with a direct code access mechanism for selecting optimal actions in reinforcement learning. besides that the network dynamics is more direct and natural  our comparative experiments show that td-falcon with direct code access is much more efficient than its predecessor in terms of computation time as well as network complexity. with a highly efficient learning architecture  our future work will involve applying falcon to more complex and challenging domains. another interesting extension is to interpret the learned cognitive nodes for explanation of the system's behaviour in performing specific tasks.
