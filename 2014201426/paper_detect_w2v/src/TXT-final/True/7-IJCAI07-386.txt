
most nonlinear dimensionality reduction approaches such as isomap heavily depend on the neighborhood structure of manifold. they determine the neighborhood graph using euclidean distance so that they often fail to nicely deal with sparsely sampled or noise contaminated data. this paper applies the graph algebra to optimize the neighborhood structure for isomap. the improved isomap outperforms the classic isomap in visualization and time complexity  as it provides good neighborhoodstructure that can speed up the subsequent dimensionality reducing process. it also has stronger topological stability and less sensitive to parameters. this indicates that the more complicated or even time-consuming approaches can be applied to construct the better neighborhood structure whilst the whole time complexity will not raise .the conducted experiments on benchmark data sets have validated the proposed approach.
1	introduction
classic dimensionality reduction approaches can not be reliably applied to find the meaningful low-dimensional structures hidden in the high-dimensional observations for exploratory data analysis such as classification and visualization  so that two new approaches have recently been developed. one is locally linear embedding  lle  that is based on local approximation of the geometry of the manifold roweis and saul  1 . lle has many variants  such as laplacian eigenmaps  belkin and niyogi  1 and hessian eigenmaps  donoho and grimes  1   incremental lle  kouropteva and pietikainen  1   supervised lle  de ridder et al.  1; kouropteva and pietikainen  1   integrated lle with classic pca or lda  abusham et al.  1; chang et al.  1   integrated lle with som  xiao et al.  1  etc. the other is isomap that preserves the manifold geometry at all scales and has better ability to deal with nonlinear subspaces  tenenbaum et al.  1 . it also has many variants  such as landmark isomap  silva and tenenbaum  1   supervised isomap  geng et al.  1   spatio-temporal extension of isomap  jenkins and mataric  1   incremental extension of isomap  law and jain  1   integrated isomap with fuzzy lda  weng et al.  1  etc. the other related studies are also performed such as the selection of the optimal parameter value for lle and isomap  kouropteva et al.  1; shao and huang  1   integration of lle and isomap  saxena et al.  1  etc. lle and isomap have their own superiorities so that they have been being developed simultaneously for various context applications.
¡¡despite isomap performs well in many cases  it often fails to nicely deal with noisy data or sparsely sampled data  geng et al.  1 . this is because in these cases the local neighborhood structure on which isomap largely depends is critically distorted. this makes isomap vulnerable to short-circuit errors that some neighors of the current point come from other different folds so that these neighbors are not nearest ones on manifold  silva and tenenbaum  1 . this can in turn lead to drastically incorrect low-dimensionalembedding. accordingly some supervised isomap are proposed  vlachos et al.  1; geng et al.  1  which employ the class labels of the input data to guide the manifold learning. they improve the isomap in classification and visualization  but they can not work when the class labels of data are not available. due to that isomap can not works on the disconnected neighborhood graph  some approaches to constructing connected neighborhood graph are proposed such as the k-connected or k-edge-connected spanning subgraph of the complete euclidean graph of all data points yang  1; 1 . this paper deals with the neighborhood graph from another perspective. it considers the implicit correlation among data points using the path algebra on the neighborhood graph  which is further applied to improve isomap.
1	approach to optimize the neighborhood
buhmann  1a; 1b . this paper applies it to build the neighborhood graph for isomap. the intuitive picture that two objects should be considered as neighbors if they can be connected by a mediating path of intermediate objects.
¡¡let g =  v v ¡Á v   be the complete euclidean graph of all data points. it weights the edges using euclidean distance de . a path l from v1 to vn in g is defined as a sequence of vertex  v1 ¡¤¡¤¡¤ vi ¡¤¡¤¡¤ vn  such that  vi vi+1  ¡Ê v ¡Á v  
1 ¡Ü i   n.
definition let p v1 vn ={l|l be a path from v1 to vn ing}  we define path algebra as a set p with two binary operations ¡Å and ¡¤ which have the following properties  b.carre  1 :
1. for all x y z ¡Ê p  the ¡Å is idempotent  commutative  and associative: x¡Åx = x x¡Åy = y ¡Åx  x¡Åy ¡Åz = x ¡Å  y ¡Å z 
1. for all x y z ¡Ê p  the ¡¤ is associative  and distributive over ¡Å:  x ¡¤ y  ¡¤ z = x ¡¤  y ¡¤ z  x ¡¤  y ¡Å z  =  x ¡¤ y  ¡Å  x ¡¤ z   y ¡Å z  ¡¤ x =  y ¡¤ x  ¡Å  z ¡¤ x 
1. the set p contains a zero element ¦Õ and a unit element e such that: ¦Õ¡¤x = ¦Õ ¦Õ¡Åx = x = x¡Å¦Õ e¡¤x = x = x¡¤e
¡¡obviously  there are many ways to define the path algebra based on different intuitions. this paper focuses on a simple but importantway to define the path algebra as follows  where for all x y ¡Ê r  ¦Õ =1  and e = ¡Þ
def
1. x ¡Å y = min x y 
def
1. x ¡¤ y = max x y 
in this way a new distance dm can be defined as follows  fischer and buhmann  1b : dm v1 vn  = minl¡Êp v1 vn {max vi vi+1 ¡Êl{de vi vi+1 }}  1  the difference between dm and de can be illustrated in figure 1. this difference can significantly impact the construc-
 figure 1: the different neighborhoods using dm and de
in terms of categories. by contrast  if dm distance is utilized to determine the neighbors for x  the result is
nm x  = {circle circle circle circle circle}
this is a correct result that indicates all neighbors of x lie on the manifold. it seems that dm can pulls points belonging to the same class closer and propels those belonging to different classes further away. it ensures to preserve the intrinsic structure of the data set and therefore can be applied to improve the isomap.
1	proposed isomap with optimized
neighborhood
isomap can perform dimensionality reduction well when the input data are well sampled and have little noise. in the presence of the noise or when the data are sparsely sampled  short-circuit edges pose a threat to isomap by further deteriorating the local neighborhood structure of the data. subsequently isomap generates drastically incorrect low-dimensional embedding. to relieve this negative effect of the noise  dm can be applied to determine the true neighborhoods as opposed to using euclidean distance de. this can get better estimation of geodesic distances and in turn give lower residual variance and robustness. it also reduces the time complexity because the better neighborhood struc-ture can speed up the subsequent optimization process.
¡¡to avoid computational expense to calculate the dm between any two points  we find an approximate way to build the neighborhood graph using dm that can be computed efficiently. it first applies de to build the neighborhood graph  and then utilize the idea of dm to optimize this neighborhood graph. the algorithm is given as follows.in many data analysis and pattern recognition tasks  similarity between two objects can be established by direct comparison and induced by mediating objects. namely  two objects might be considered similar when they are connected by a chain of intermediate objects where all dissimilarities or distances between neighboring objects in the chain are small  fischer and buhmann  1b . this concept can be generalized by assuming that object similarity behaves transitive in many applications. based on this intuition  the path algebra-based clustering approach is proposed that can extract elongated structures from the data in a robust way which is particularly useful in perceptual organization  fischer and dm a b  = min{max{de a c  de c d  de d b } de a b   }	 
figure 1: the difference between dm and de
tion of neigborhood graph on data manifold. for example  the categories of 1-nearest neighbors of x using de distance circled as in figure 1  is
ne x  = {square square square circle circle}
generally points belonging to the same class are often closer to each other than those belonging to different classes. obviously the short circuiting problem happens here because points marked by square are not the most similar points to x algorithm 1: optimizeneighborhood x k m d 
¡¡input: x = {xi} be the high dimensional data set  k be the neighborhood size  m be the scope for optimization of neighborhood and d be the dimension of the projected space.
¡¡output: the optimized neighborhood set n = {n xi } for all points.
¡¡1. calculate the neighborhood n xi  for any point xi using de  where n xi  is sorted ascendingly.
¡¡1. compute n  which is the number of points in x.
¡¡1. for i=1 to n
¡¡1. for j= 1 to k
¡¡1. select j-th point from n xi   denoted as xij
¡¡1. for p= 1 to m
¡¡1. select p-th point from n xij   denoted as xijp
¡¡1. if de xij xijp    de xi xik  and xijp ¡Ê/ n xi  and parent xijp  ¡Ê n xi 
¡¡1. delete xik from n xi 
¡¡1. insert xijp to n xi  ascendingly
¡¡1. let j=1
¡¡1. break
¡¡1. endif
¡¡1. endfor
¡¡1. endfor
¡¡1. endfor
¡¡1. return n = {n xi } that is the optimized neighborhoods for x
¡¡in algorithm 1  step 1 calculates the neighborhood graph using euclidean distance  as the same as most dimensionality reduction approaches such as isomap do. its time complexity   o n1 . from step 1 to step 1  the neighborhood of the given point xi determined in step 1 is optimized using dm. it includes two cycles. although step 1 reset j  k and m are neighborhood sizes so that they can be regarded as small constants. it means that optimization of neighborhood of a point can be finished in o 1 . therefore the neighborhood graph can be efficiently optimized with additional time complexity o n . in step 1  condition parent xijp  ¡Ê n xi  means that xijp must be reachable from xi in the neighborhood graph determined in step 1.

	x1	x1	x1	x1	x1	x1
 
 figure 1: illustration of optimizing neighborhood of point x	 
¡¡in order to illustrate the idea behind the algorithm 1  we give an example to optimize the neighborhood of point x  shown as figure 1  where k=1 and m=1. figure 1  shows the neighborhoodgraph of x determined using euclidean distance  where only parts are described. it can be observed that x has neighbors n x  = {x1 x1 x1}. in algorithm 1  step 1 chooses its first neighbor x1 and then step 1 selects the neighbor x1 for x1. in step 1  x1 will be compared with the largest neighbor of x1  namely x1. because the condition is satisfied  all steps between step 1 and step 1 will be executed. this results in that x1 will replace x1. consequently the neighorhood of x is optimized as n x  = {x1 x1 x1}. it is shown as figure 1 . because the neighborhood of point x has changed  the optimization restarts from the smallest neighbor x1 of the point x and enter the next cycle. due to x1 has been in neighborhood of x  x1 will be explored  shown as figure 1 . because the condition in step 1 is satisfied  all steps between step 1 and step 1 will be executed. this results in that x1 will replace x1.consequently the neighorhood of x is optimized as n x  = {x1 x1 x1} shown as figure 1 . due to m=1  x1 will never be exploited. so far all neighbors of x1 have been tested. next cycle will explore the second neighbor of x in step 1  namely x1. it goes through all steps as exploring the first neighbor x1 of x  which is not explained more here.
¡¡now optimizeneighborhood can be applied to improve the basic isomap. for hereafter comparison  we denote this improved isomap as m1-isomap  which is described as algorithm 1.
algorithm 1: m1-isomap x k m d 
¡¡input: x = {xi} be the high dimensional data set  k be the neighborhood size  m be the scope for optimization of neighborhood and d be the dimension of the projected space. output: the dimensionally reduced dataset y = {yi}.
1. utilize optimizeneighborhood x k m d  to calculate the optimized neighborhoodsfor each point in input data set x  which will be applied to construct the neighbor graph in next step.
1. construct the weighted neighbor graph ge =  v e  for the dataset x  by connecting each point to all its knearest neighbors  where  vi vi+1  ¡Ê e if xj is a member of k neighbors of xi determined in step 1. this edge has weight de xi xj .
1. employ the ge to approximately estimate the geodesic distance dg xi xj  between any pair of points as the shortest path through graph that connects them.
1. construct d-dimensional embedding from ge using
mds
¡¡compared with the basic isomap  m1-isomap adds step 1 and then modifies the step 1. it should be noted that the edges of neighborhoodgraph are weighted by de instead of dm. the dm is only applied to optimize the neighborhood rather than applied to estimate the geodesic distance. the step 1 and step 1 remains the same as the basic isomap. therefore the time complexity increases from step 1 where the additional complexity is o n . however the optimized neighborhood will speed up the step 1 so as to decrease the running time of the whole algorithm. this will be proved empiricially in later experiment.
1	experimental results
several experiments are conducted to compare isomap with m1-isomap in visualization and time complexity. isomap uses the publishedmatlab code tenenbaumet al.  1 . the m1-isomap is also implemented in matlab 1. in experiments  there are two parameters involved. isomap has a parameterk to determinethe size of local neighborhoodwhereas m1-isomap introduces an additional parameter m that controls the scope of the neighborhood to be modified  where k=1 and m =1 will be explored.
a. swiss roll data set
¡¡swiss roll dataset is widely applied to compare many non-linear dimensionality reduction approaches  roweis and saul  1; tenenbaum et al.  1; balasubramanian and schwartz  1; geng et al.  1 . we take many random samples from the swiss roll surface shown as figure 1  and do visualization experiments on them to compare isomap with m1-isomap in the following cases:  1 well sampled data without noise  1  well sampled data with noise  1 sparsely sampled data.

figure 1: swiss roll surface sample
figure 1: the mapped results of two approaches on five well sampled noiseless data sets
¡¡to make comparison on well sampled noiseless data sets  five sample data sets with 1 points are sampled randomly from swiss roll surface. firstly on the first sampled data set  two approaches are tested to choose the optimal parameters in the given scope  and then these parameters are applied to run the remaining four data sets. the mapped results from left to right corresponding to five data sets are shown as figure 1  where the parameters and residual variance for each data set are added as caption. for example on the first data set in figure 1 a isomap  the caption  1.1  indicates that the parameter k takes 1 whereas the residual variance is 1. in figure 1 b m1-isomap  the caption  1/1  indicates that the parameter k takes 1 m takes 1  and the residual variance is 1.1. it can be observed that on well sampled data sets without noise  m1isomap outperforms isomap. it gets better mapping results and lower residual variances on any data set.
¡¡experiment 1 on well sampled but noise contaminated data sets
¡¡to compare the topologically stability of isomap and m1isomap on data sets with noise  we do experiments on the first data set with 1 points used in experiment 1  but here it is contaminated by adding random gaussian noise. the mean of the noise is 1 and the variance is 1. it can be seen from figure 1 that isomap gets the best result with residual variance 1 when k=1  while m1-isomap gets the best result with the lower residual variance 1 with parameters k=1 and m=1. obviously m1-isomap outperforms isomap in terms of visualization and residual variance. it can be also observed from figure 1 that as k increases  the results of isomap change drastically  but those of m1-isomap do not change much. this indicate that m1-isomap is also less sensitive to k than isomap.
experiment1 on sparsely sampled data sets
¡¡generally  in sparsely sampled data sets  the euclidean distance between points in neighborhood becomes larger as compared to the distance between different folds of the manifold. this easily makes two approaches faces the problem of short-circuiting. this experiment makes comparison between two approaches about robust to samples with different sampling density. five data sets with 1 1 and 1 points respectively are randomly sampled from swiss roll surface. two approaches are tuned to select the best visualization results on each data set. it can be observed from figure 1 that m1-isomap outperforms isomap on each datset in terms of visualization performance and residual variance.
b. face image data set
¡¡this data set consists of 1 images  each contains 1¡Á1 pixels  of a human face rendered with different poses and lighting directions  tenenbaum et al.  1 . to compare two approaches on sparsity data set  we choose 1  half of the data set images from this data set to form a new data set. the figure 1: the mapped results of two approaches on five sparsely sampled data sets mapped results are shown in figure 1 and figure 1. in fig-
 1
figure 1: isomap  k=1 
1
 1
figure 1: m1-isomap  k=1 m=1 
ures  the x-axis represents the left-right poses of the faces  and the y-axis represents the up-down poses of the faces. the corresponding points of the successive images from left to right in the middle are marked by circles and linked by lines. the nine critical face samples are marked by plus at the leftbottom corner of each image indicates the point representing the image. it can be observed from these figures that isomap can hardly reveal the different face poses. the middle leftright line is heavily curved  and the arrangement of the nine face samples is tangle some. by contrast  m1-isomap puts the middle left-right line better. the nine face samples are also mapped to the approximately right positions corresponding to the face poses. these indicate that m1-isomap performs better than isomap.
¡¡this also can be supported from their residual variances. it can be observed from figure 1 that m1-isomap gets lower  n=1 .
residual variances and the correct intrinsic dimensionality of data set  while isomap gives the higher estimate to intrinsic dimensionality than that of the data set. the intrinsic dimensionality of the data can be estimated by looking for the  elbow  at which this curve ceases to decrease significantly with added dimensions.
c. time complexity comparison
¡¡we generate 1 data sets with the same size randomly sampled from swiss roll surface. two approaches run on these ten data sets respectively and then average running time for two approaches are calculated. and then the experiments are performed on different sizes: 1 1 1 1. it can be observed from table 1 that m1-isomap also exceeds the isomap in time complexity. the larger the data set size is  the superiorities of m1-isomap is more obvious. this is because good neighborhood structure of data set is beneficial to speed up the subsequent optimization process.
1	conclusion
this paper applies the graph algebra to build the neighborhood graph for isomap. the improved isomap outperforms the classic isomap in visualization and time complexity. it also has stronger topological stability and less sensitive to parameters. this indicates that the more complicated or even time-consuming approaches can be applied to construct the better neighborhoodstructure whilst the whole time complexity will not raise. because the transitivity of similarity is not absolutely correct for any data set  in the future  we will apply the fuzzy theory and probability theory to define the graph algebra and then to build the neighborhood graph. and besides  the proposed approach should be combined with kedge-connectedspanning subgraph so as to guaranteeto build the connected neighborhood graph for any data set.
acknowledgments
this work is supported by natural science foundation of china 1  hubei science and technology project  1aa1  and uk engineering and physical sciences research council under grant number gr/n1.
table 1: comparison of time complexity  seconds 
datasize ¡ú1111isomap11111111m1-isomap11111111