  or semantic. an example of a specific memory is the memory of the ace food company acquiring the acme hardware company. a traditional episodic memory  tulving  1  is composed of specific memories. an example of an abstract memory is the generalization across specific experiences we might have heard about where companies have acquired other companies. abstract  or generalized episodic  memories are on the border between spe-
figure 1: structure of long-term memory 
cific memories and semantic memory. semantic memory is the memory or knowledge of what  companies  are and what  acquiring  is. semantic memory is used in understanding and making inferences about the input to the system. 
     this tripartite division of memory approximates a continuum  where the three divisions represent the most specific level  the most general level  and the levels in between. at the most specific level are things that happen in the world that are composed of particular  unique instantiations of concepts. specific memories may abstract through generalization by the generalization mechanism  and these abstractions may abstract. figure 1 illustrates the structure of long-term memory with associated examples. 
     in addition to this tripartite division  another level of organization is superimposed on memory. groups of related concepts in episodic or abstract memory are linked together through a common node  called a tag. the tag allows the system to detect quickly whenever multiple instantiated concepts appear in the same event or episode. this tag is attached to concepts by the event integrator. the integrator simply takes all the instantiated concepts passed to it and assigns them a new tag. each tag has a numerical threshold value  currently equal to a fraction of the number of concepts in the episode  currently onethird. long articles may consist of tags that have tags as components. figure 1 illustrates the kind of structure the integrator superimposes on memory. 
b. 	the retrieval mechanism 
retrieval in scisor is a two-stage process. the first stage is a coarse search that finds events in memory likely to be relevant. relevance is determined by the number of features present in an event in memory related to features in the input. after the most likely candidates have been isolated  a more computation-intensive matching process is performed. the operation of the retrieval mechanism  
1 	cognitive modeling 
and the scisor system in general  is duscussed in more detail in rau  rau  1 . 
     the first stage of the retrieval process is performed by a process of priming or constrained spreading activation. as concepts are instantiated in the system  instances of concepts that are related via category membership links are marked  i.e.  primed or activated . when a certain subset of the concepts in an event or episode is marked  the entire episode is put into the system's short-term memory buffer. this is the spontaneous retrieval phase. the events that have been spontaneously retrieved can then be run through the match filter to check the nature of the match between the input and the events retrieved. periodically  all marks are deleted from the system. marker-passing  waves  are propagated continuously as new concepts are instantiated. this mechanism retrieves answers to input questions  old unanswered questions to new input answers  and stories given related input stories in exactly the same manner. 
     in more detail  the retrieval mechanism operates in the following manner: 
1. at the time of concept instantiation  related concepts are marked according to the  priming rules   see next section . every concept that is marked causes its containing episode or episodes  its tags  to have increased activation* the current and threshold levels of activation are simple properties of the tag. 
1. tags that have had their values increased check themselves to see whether their current activation levels exceed their threshold. 
1. if the tag threshold has been exceeded  an intersection has been detected  and the entire episode is put into the short-term memory buffer. the match filter is then called to refine the potential matches. 
1. if a concept is instantiated that is included in an episode already in the memory buffer  an incremental match filter is called to match incrementally the new concepts. 
    actually  high-frequency  low-content concepts may not participate in this process. 

1. all concepts in the memory are unmarked after every one or two sentences and after every question are input. 
　　the rules that guide how levels of activation or marks are passed through the conceptual hierarchy are given below. these rules were formulated to decrease the possibility of retrieving memories that have limited predictive capacity or relevance to the current situation. 
	1. 	priming rules 
　　the effect of the following rules is that all instances of concepts that are components of episodes and are children of the incoming parent of the parent of the incoming instance in the conceptual hierarchy  are marked. also  all instances of concepts that are components of memories  and are direct instances of concepts that are parents of the incoming instance  are marked. this rule prevents everything in the hierarchy from being marked  and limits what is marked to a level of conceptual abstraction supported by the presence of a direct instance at that or a more general level. 
for example if a user asked  what happened to the 
ace company    the event being asked about is at a fairly general level of conceptual abstraction. any event involving the ace company and more specific than this general  event  instance would be a valid answer. if no event were known  the unanswered question would be stored. in the future  any input events involving the ace company  i.e.  offers  rumors  would cause the unanswered question to be activated. note that all concepts as they are instantiated cause any related instances to be marked  which allows any feature in the input to be a potentially useful index key into memory. 
rules: 
1. mark concepts that are components of specific or abstract events. these concepts are marked with tags. 
1. determine the categories to which the incoming instance belongs  categories-of a . 
1. determine the concepts in the reflexive  transitive closure along category membership links of categories-of  categories-of a . 
1. mark the direct instances of concepts in this closure. each marked concept increases the current activation value of each of its tags. 
　　additional refinements to this algorithm have been made to increase the system's efficiency. one such refinement has been to check the number of episodes containing a certain concept before passing markers to all those episodes. this check can be made easily by maintaining a count of the number of category members in each conceptual category. for example  if the system had read about events involving thousands of companies  this number would be stored at the parent company node. when this number is very large  the system suppresses the marker-passing operation. events that are retrieved through the activation of more unique concepts then incorporate the high-frequency concept information in just those events. this simple refinement suppresses waves of activation unlikely to cause significant differentiation among events in memory  while still taking all features of the input into consideration. 
　　after a set of events has been spontaneously retrieved  a match filtering operation is performed to ensure that the correct relationship exists between concepts within the events. 
	1. 	the match filter 
　　the match filter takes spontaneously retrieved events and computes a final degree of match  based on the results of matching relationships between the concepts in the input and the concepts in the retrieved events. in the following discussion  a  component  refers to a constituent concept in an event. 
　　input to the match filter consists of an ordered list of the most highly activated episodes in the short-term memory buffer. also present is information about what input components caused a retrieved episode's component to be activated. output of the filter is a potentially reordered list of matching episodes  with additional information that relates the components of the input to the components of the retrieved episodes with more precision and certainty. 
　　the match filter starts at one node in the retrieved episode and begins to construct a new graph that represents the matching elements of the input and the retrieved nodes. it performs this process by checking that values of the  slots   called aspectuals in kodiak  in the retrieved episode are also marked by values of the slots in the input. for example  consider the example illustrated in figures 1 and 1. 

figure 1: part of a story episode 
　　the story episode contains information about a kind of offer  a cash-offer  made by warnaco for a clothing company. the question episode requests the value of any 

offers made by a company in a takeover attempt on an apparel company. 
　　due to the marker-passing  the instantiation of offer in the question episode causes cash-offer to be marked. similarly  clothing marks apparel. the other features that contribute to the retrieval of the story episode are takeover  company and value-offer. the match filter begins at a node that is marked only by one node from the input. offer1 is such a node  but note that company1 and company1 are both marked by company1 and company1. 
　　the filter proceeds to check that the offer of cashoffer1  takeover1  is marked by the offer of offer1 
 takeover1 . given that this is true  the offer1/cashoffer1 match is added to a new graph that represents the matching elements of the input and the retrieved episodes. this process is repeated for every node in the input. in the case of answering a question  those nodes that do not correspond are added to a list of presuppositions to be expressed to the user. also  any node that was more general than the input may be expressed to the user. for example  if the user was interested in what pet-food companies were being taken over  and the system only knew about food companies  this generalization is pointed out. 
c. 	system status 
scisor is implemented in common lisp; it is used on vax computers and symbolics and sun workstations. the trump parser and semantic interpreter has not yet been tested with a large grammar or vocabulary but  in these early stages  it has been relatively easy to customize. on the sun-1 it processes input at the rate of a few seconds per sentence  including the selection of candidate parse and semantic interpretation. the king natural language generator was implemented in franz lisp  and at this writing has not yet been converted to common lisp to run with trump. 
　　the system has been tested with a dozen or so stories stored in the knowledge base. hundreds of semantic concepts and domain vocabulary are also present. about a dozen questions are answered by the system. 
	i v . 	r e l a t e d r e s e a r c h 
a . 	question answering 
in scisor  the processes that find the approximate location of an answer to a user's question and the processes that determine what the answer should be are separate. a great deal of work has been done on the second problem  most notably by lehnert  lehnert  1 . determining an appropriate answer to a user's question  given that the context in which the user's question was posed is already known  is a separate process from the initial retrieval of a context in which to search for an answer. this initial retrieval of a context is the spontaneous retrieval this paper describes. 
b. memory-based reasoning 
stanfill and waltz  stanfill and waltz  1  provide excellent motivation for using episodic memory to perform various reasoning tasks. while it is hoped that scisor will eventually be used to perform reasoning of the type they describe  the method of retrieval of episodes used in the two systems is antithetical. while stanfill and waltz assume that it is impossible to find a best match from an event-based memory without examining every item in the knowlege base  this is in fact exactly what scisor does. scisor accomplishes this by only activating events with features related to features in the input. thus search is focused entirely on best-match candidates. events with no features related to features in the input are never touched. 
　　also  scisor uses its semantic knowledge to find close matches  where closeness is a measure of semantic distance. stanfill and waltz's proposed lack of knowledge of the semantic domain of system operation will prevent their system from finding truly best matches. 
c. partial parsing 
recently  there have been some limited successes in the development of ai systems to parse partially and to understand short texts in constrained domains  dejong  1  lebowitz  1  kolodner  1  young and hayes  1 . 
however  work in effectively accessing these knowledge bases has not been as successful. for example in young and hayes  young and hayes  1   the information cannot be accessed after it has been understood except through a traditional database front-end  or by direct examination of the conceptual  frame-like representation. in cyrus  a natural language question-answering component allows queries of the knowledge base. however  the system is not guaranteed to answer correctly. as a cognitive model  its memory failures are understandable and interesting  but when accuracy and reliability of information are important  such a model is not viable. scisor demonstrates some of the uses that can be made with automatically extracted conceptual information from text when that conceptual information is combined with a powerful and robust method of spontaneous retrieval. 
d. cognitive effects 
previously known to the system is flagged as potentially 
1 	cognitive modeling in systems such as scisor that use the same parsing mechanism for both question parsing and story parsing  interesting effects occur. for example  in boris  dyer  1   the system would occasionally take as true information present in a user's question not previously known to the system. although this has been shown to be a cognitively valid effect  loftus  1   and does increase the system's opportunities to learn  it is not a desirable effect for an information retrieval system. in the scisor system  a strict difference is maintained between information read in articles  and information present in user's questions in that all information present in questions and not unreliable. thus the system believes nothing it is told  but everything it reads in the paper. 
　　the model of retrieval discussed has a certain cognitive appeal. it exhibits the same kind of spontaneous recall 
 m as people do  as is discussed in schank's work on reminding  schank  1 . also  the answer to a user's question may be retrieved before the user has finished asking the question  an interesting effect also first achieved in dyer's boris system. 
	v . 	s u m m a r y 
scisor performs retrieval in a two-stage process. the first step  a spontaneous and coarse search  can be summarized as follows: 
1. new inputs are instantiated: as the system reads new stories  or users ask the system questions  concepts are instantiated. 
1. marker passing occurs: markers are passed to other instances related to the input instances through semantic category   isa   links. 
1. items are spontaneously retrieved: when enough instances in an event have been marked  the event is retrieved from long-term memory. 
1. items are evaluated: the system must evaluate items retrieved for relevance in the processing at hand. 
the second stage is a kind of graph matching process that performs a syntactic matching function on the likely candidates retrieved by the first process. this spontaneous method of retrieval elegantly solves some retrieval problems found in other systems: 
1. scisor efficiently addresses a user's previously unanswered questions if an answer becomes known. instead of always looking for answers that may or may not be present in incoming stories  incoming stories that relate to unanswered questions activate only those questions  which may then be considered. 
1. scisor can locate relevant information in response to a user's questions  even when that question contains misleading or partial information. this capability is a by-product of the method of retrieval used. 
1. scisor retrieves previous events when updates of those events are read. this is done in the same efficient manner as retrieving an unanswered question  and with the same tolerance for partial or contradictory input as in the question-answering case. 
	v i . 	p r o b l e m s 
currently  matching of features can be relaxed only by the spreading of markers through the  isa   or abstraction hierarchy. an example of a more difficult problem is mentioned in a discussion of matching in krl  bobrow and winograd  1 . that is  it might be nice to retrieve answers to questions like  does john own a dog  given that we have in memory that john owns a dog license. this type of reasoning and deduction could be performed with the scisor system by an iterative spontaneous retrieval. john's owning a dog could potentially retrieve the information that dog owners have dog licenses  which could then activate john's owning of the dog license. the effects on processing time and efficiency of such iterative retrieval are unknown  they could be substantial. 
　　another limitation scisor has is in the kind of questions the system can answer. currently the scisor system is capable of answering only questions about information explicitly stored in the knowledge base. any information that potentially could be reconstructed or inferred from information stored in the knowledge base is not available. the line between what is explicit in a story and what can be deduced from that story is not sharp  because some amount of  figuring  must go on to obtain any reasonable understanding of the story. to obtain this understanding  scisor computes something similar to a maximally complete inference set  cullingford  1  as the set of information present explicitly in articles and inferred from the context and other world knowledge. anything in that understanding can be directly retrieved. 
　　for example  scisor is able to answer the question  what company was sold for $1 billion   without preindexing a story containing that information by amountof-sale. however the system cannot answer the question 
 which companies have been taken over more times than they have taken over other companies   for example  because an answer would require counting all the times a company has been taken over and has taken over other companies  comparing these two numbers  and repeating the process for every other company in the knowledge base. another problem that ultimately must be addressed is the speed and memory requirements necessary in a viable information system. the effects of such a large knowledge base on the speed and accuracy of the retrieval mechanism are unknown. 
	v i i . 	c o n c l u s i o n s 
scisor is an experiment in the usefulness of a spontaneous  marker-passing approach to conceptual information retrieval. in its current implementation  it has demonstrated some promising results. the marker-passing scheme  combined with the knowledge representation used  seems to produce an effective contents-addressable  distributed representation. retrieval occurs spontaneously when features in the input are related to features of events in memory. scisor can find answers to input questions even in fight of missing or misleading input information. 
　　the system has not yet been tested on a large number of documents. however so far  the tests that have been performed are quite promising. before any definitive claims can be made about the ultimate usefulness of this type of system  it must be tested with a large sample of documents in real ir tasks. the next stage of the project will include such tests. 
r e f e r e n c e s 
 besemer and jacobs  1  d. besemer and p. jacobs. flush: a flexible lexicon design. in proceedings of the 1th meeting of the association for computational linguistics  palo alto  california  1. forthcoming. 
 bobrow and winograd  1  d. bobrow and t. winograd. an overview of krl  a knowledge representation language. cognitive science  l l :1  1. 
 brachman et ai  1  r. brachman  r. fikes  and h. levesque. krypton: integrating terminology and assertion. in proceedings of the national conference on artificial intelligence  pages 1  kaufmann  los altos  ca  august 1. 
 charniak et ai  1  e. charniak  c. riesbeck  and d. mcdermott. artificial intelligence programming. lawrence erlbaum associates  hillsdale  nj  1. 
 cullingford  1  r. e. cullingford. natural language processing: a knowledge-engineering approach. rowman and littlefield  totowa  nj  1. 
 deering et ai  1  m. deering  j. faletti  and r. wilensky. pearl: an efficient language for artificial intelligence programming. in proceedings of the seventh international joint conference on artificial intelligence  vancouver  british columbia  1. 
 dejong  1  g. dejong. skimming stories in real time: an experiment in integrated understanding. research report 1  department of computer science  yale university  1. 
 dibenigno et al  1  m. dibenigno  g. cross  and c. debessonet. corel - a conceptual retrieval system. technical report cs-1  computer science department  washington state university  1. 
 dyer  1  m.g. dyer. in-depth understanding. mit press  cambridge  ma  1. 
 jacobs  1  p. jacobs. language analysis in not-solimited domains. in proceedings of the fall joint computer conference  pages 1  ieee computer society press  washington  dc  november 1. 
 jacobs  fall  1  p. jacobs. knowledge-intensive natural language generation. artificial intelligence  1  fall  1. forthcoming. 
 jacobs and rau  1  p. jacobs and l. rau. ace: associating language with meaning. in t. o'shea  editor  advances in artificial intelligence  pages 1  north holland  amsterdam  1. 
 kolodner  1  j. kolodner. retrieval and organizational strategies in conceptual memory: a computer model. lawrence erlbaum associates  hillsdale  nj  1. 
 lebowitz  1  m. lebowitz. generalization from natural language text. cognitive science  1 l :l-1  1. 
1 	cognitive modeling 
 lehnert  1  w. g. lehnert. the process of question answering: computer simulation of cognition. lawrence erlbaum associates  hillsdale  nj  1. 
 loftus  1  e. f. loftus. leading questions and the eyewitness report. cognitive psychology  1  1. 
 rau  1  l.f. rau. knowledge organization and access in a conceptual information system. information processing and management  special issue on artificial intelligence for information retrieval  forthcom-
ing summer   1. 
 salton and mcgill  1  g. salton and m. mcgill. an introduction to modern information retrieval. mcgraw-hill  new york  1. 
 schank  1  r.c. schank. dynamic memory: a theory of reminding and learning in computers and people. cambridge university press  cambridge  1. 
 schank and abelson  1  r. c. schank and r. p. abelson. scripts  plans  goals  and understanding. lawrence erlbaum associates  halsted  nj  1. 
 stanfill and waltz  1  c. stanfill and d. waltz. toward memory-based reasoning. communications of the association for computing machinery  1 :1  1. 
 tulving  1  e. tulving. episodic and semantic memory. in e. tulving and w. donaldson  editors  organization and memory  pages 1  academic press  new york  1. 
 wilensky  1  r. wilensky. knowledge representation - a critique and a proposal. in j. kolodner and c. riesbeck  editors  experience  memory  and reasoning  pages 1  lawrence erlbaum associates  hills-
dale  nj  1. 
 young and hayes  1  s. young and p. hayes. automatic classification and summarization of banking telexes. in the second conference on artificial intelligence applications  pages 1  ieee press  1. 
	rau 	1 

	rau 	1 

	rau 	1 









	rau 	1 

	rau 	1 

	rau 	1 

	rau 	1 

	rau 	1 



	rau 	1 



