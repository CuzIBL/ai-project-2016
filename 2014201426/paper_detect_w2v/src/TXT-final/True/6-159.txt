 
an agent with limited consumable execution resources needs policies that attempt to achieve good performance while respecting these limitations. otherwise  an agent  such as a plane  might fail catastrophically  crash  when it runs out of resources  fuel  at the wrong time  in midair . we present a new approach to constructing policies for agents with limited execution resources that builds on principles of real-time al  as well as research in constrained markov decision processes. specifically  we formulate  solve  and analyze the policy optimization problem where constraints are imposed on the probability of exceeding the resource limits. we describe and empirically evaluate our solution technique to show that it is computationally reasonable  and that it generates policies that sacrifice some potential reward in order to make the kinds of precise guarantees about the probability of resource overutilization that are crucial for missioncritical applications. 
1 introduction 
optimality is the gold standard in rational decision making  e.g.   russell and subramanian  1    and  consequently  the problem of constructing optimal policies for autonomous agents has received considerable attention over the years. traditionally  this problem has been viewed separately from the problem of actually carrying out the policies. however  real agents have limitations as to what they can execute  and  clearly  a policy is less useful if an agent might run out of resources while carrying out the policy. 
　in this paper  we present a new approach for constructing policies for agents that have limited consumable resources where running out of the resources can have negative consequences. whereas ai research has mostly focused on  po mdp  boutilier et ai  1; dean et ai 1; howard  1; puterman  1  methods for formulating policies for agents without emphasizing constraints on their execution resources  the operations research literature has developed constrained mdp  cmdp   altaian  1; puterman  1  techniques that can account for resource constraints. cmdp methods are particularly useful for domains where the current resource amounts are unobservable and cannot be easily estimated by the agent  or where modeling resource amounts in the state description is computationally infeasible. in an aircraft scenario  some resources and situations where such methods arc beneficial could include an airplane with a broken fuel gauge  fuel amount is unobservable   pilot fatigue  attention is a resource that cannot be easily estimated   or a combination of non-critical resources  ex. various refreshments  that should nevertheless not be exhausted  but explicit modeling of which unnecessarily complicates the optimization problem and increases policy size. in the rest of the paper  we will use the fuel example  simply because it is a very intuitive instance of a consumable resource. 
　however  as we will explain later  standard risk-neutral cmdp optimization techniques are not applicable to problems where violating the constraints can have negative or  in the limit  catastrophic1 consequences. the main contribution of this work is that it extends the standard cmdp techniques to handle the types of hard constraints that naturally arise in problems involving critical resources. in particular  we formulate an optimization problem where constraints are imposed on improbability of resource overutilization  and show how the problem can be solved using standard linear programming algorithms. our formulation yields sub-optimal solutions to the constrained problem because it sacrifices potential reward to make guarantees about the probability of violating the resource constraints. as we later show  when violating constraints incurs dire costs  these guarantees are worthwhile   musliner et al  1  and references therein . 
　we introduce our model in section 1  where we review markov models  introduce notation  and specify our assumptions about the problem domain. section 1 describes and compares several candidate solutions for addressing aspects of our problem. we then present in section 1 our new method  and empirically evaluate it in section 1. we conclude with a discussion about the strengths and limitations of our results  and about our future directions. 
1 the model 
the stochastic properties of the environments in the problems that we are addressing lead us to formulate our op-
　　　!the term catastrophic is  of course  relative. we assume that the system designer is willing to accept certain risks to receive payoff. 

resource-bounded reasoning 	1 

timization problem as a stationary  discrete-time  markov decision process. in this section  we review some wellknown facts from the theory of standard  puterman  1; boutilier et a/.  1  and constrained  altman  1  fullyobservable markov decision processes and also discuss the assumptions that are particular to the class of problems that we address in this work. this section provides the necessary background for the subsequent sections  where we discuss resource-constrained optimization problems. 
1 markov decision processes 
a standard markov decision process can be defined as a tuple where s is a finite set of states that the agent can be in  is a finite set of actions that the agent can execute  
r defines the transition function 
is the probability that the agent will go to state if it executes action a in state i   and is the reward function  agent receives a reward of ria for executing action in state 
　clearly  the total probability of transitioning out of a state  given a particular action  cannot be greater than 1  i.e. 
　　　　　　as we discuss below  we are actually interested in domains where there exist states for which 
　a policy is defined as a procedure for selecting an action in each state. a policy that makes its choices according to a probability distribution over the set of actions is called randomized and can be described as a mapping of states to probability distributions over actions: 
 a deterministic policy that always chooses the same action for a state is  of course  a special case of a randomized policy. it can be seen  similarly to the case of standard cmdps  as described in  kallenberg  1; puterman  1   that under our optimization criterion and constraints  section 1   deterministic policies can be suboptimal. therefore  in this work  we focus on approximating optimal policies in the class of randomized ones. 
   if  at time 1  the agent has an initial probability distribution  over the state space  a markov system follows the following trajectory: 
 1  
where 
1 	assumptions 
typically  markov decision processes are divided into two categories: finite-horizon problems  where the total number of steps that the agent spends in the system is finite and is known a priori  and infinite-horizon problems  where the agent is assumed to stay in the system forever  see  puterman  1  for a detailed discussion of both types of models . 
　in this work we concentrate on dynamic real-time domains  where agents have tasks to accomplish. for example  consider an agent flying a plane  whose goal is to safely get to its destination and land there. this example does not naturally correspond to a finite-horizon problem  because the duration of executing various policies is not predetermined  unless we artificially impose such a finite duration  which is not easily justifiable . on the other hand  the problem does not naturally fit the definition of the infinite-horizon model  because the plane obviously cannot keep on flying forever. 
this leads us to make a slightly different and less common 
 although  certainly  not novel  assumption about how much time the agent spends executing its policy. we assume that there is no predefined number of steps that the agent spends in the system  but that optimal policies always yield transient markov processes  decision problems of this type were extensively studied by kallenberg f 1  . a policy is said to yield a transient markov process if the agent executing that policy will eventually leave the corresponding markov chain  after spending a finite number of time steps in it. given a finite state space  this assumption implies that there has to be some  leakage  of probability out of the system  i.e. there have to exist some state-action pairs  for which one particular case where the above assumption holds is in a system in which all trajectories lead to absorbing states. once an agent enters an absorbing state  it has finished  or failed to finish  some task and has nowhere else to go  i.e. the probability of transitioning out of an absorbing state i is zero: 
in the plane-flying example  all trajectories 
lead to either a safe landing or a crash  and once the agent enters one of these states  the probability of transitioning to other states is zero. 
　the transient nature of our problems leads us to adopt the expected total reward as the policy evaluation criterion. given that an agent receives a reward whenever it executes an action  the total expected utility of a policy can be expressed as: 
		 1  
where t is the number of steps during which the agent accumulates utility. for a transient system with bounded rewards  the above sum converges for any t. 
1 	related work 
in this section  we briefly survey several approaches  based on well-known techniques  to solving the problem of finding optimal policies for agents with limited resources  and point out the assumptions  strengths  and limitations of these methods. this establishes a landscape of solution algorithms  to which we can compare our method  presented in the next section  in terms of complexity  efficiency  and solution quality. 
1 	fully observable mdps 
the most straightforward way of handling resource constraints in a mdp framework is to explicitly model the resources by making their current amounts a part of the state description. this yields a standard fomdp  which can be solved by a wide variety of efficient methods. the benefit 
　　an alternative way of handling these states is to treat them as infinitely-recurrent  i.e. once the agent gets there  it infinitely transitions back to itself. we do not adopt this model  because it is less natural for our domains and also leads to unnecessary complications in the optimization problems. 

1 	resource-bounded reasoning 

of this approach is that it allows one to make use of all relevant information to construct the best possible policy  as the agent is able to base its action choice on the current state and resource amounts. the downside of this approach is that it requires an a priori discretization of resource amounts to be made when the world model is constructed. also  there is an additional burden of specifying the rewards and state transitions as functions of current resource amounts. furthermore  in this model  the size of the state space and  consequently  the policy size  explodes exponentially in the number of resources  as compared to the state space where the resource amounts are not folded into the state description. 
　the fomdp approach relies on the fact that the agent can observe the exact amounts of all resources at runtime. however  this may not hold  especially in multiagent domains with shared resources  when an agent does not know what other agents have been doing and how much of the shared resources they have been consuming. 
1 	constrained mdps 
an alternative to the fomdp approach described above is to treat the problem as a constrained markov decision process  altman  1   where the resources are not explicitly modeled  but rather are treated as constraints that are imposed on the space of feasible solutions  policies . 
　a constrained mdp for a resource-limited agent differs from a standard mdp in that the agent  besides getting rewards for executing actions  also incurs resource costs. consequently  a constrained mdp  cmdp  can be described as a tuple  s  a  p  r  c  q   where c =   defines a vector of actions' costs units of resource are used when action is executed in state i   and q is the vector of amounts of available resources  there is units of resource initially available . 
　the benefit of the cmdp is that it does not require one to explicitly model how the resources affect the world. indeed  if all policies satisfy the resource constraints  one does not have to worry about what happens when the resources are overutilized. consequently  the state space and the resulting policies are exponentially smaller than the ones in the fomdp model. the standard cmdp formulation constrains the expected usage of all resources to be below a certain limit and can be formulated as the following linear program: 
 1  
number of times action a is executed in state i. 
　a weakness of this approach is that it yields policies that can be suboptimal  as compared to the ones constructed by the fomdp method  because the agent does not base its decision on the current resource amounts  but rather completely ignores that aspect of the state. however  as mentioned in the 
previous section  in domains where the resources are not observable  or if the policy size is of vital importance  for example if the agent's architecture imposes constraints on policies that it can store   this approach could prove fruitful. 
　however  the biggest problem with this approach  as pointed out  for example  by ross and chen in the telecom-
resource-bounded reasoning 
munication domain   is that a standard cmdp imposes constraints on the expected amounts. clearly  this method does not work for critical resources  whose overutilization has negative consequences. indeed  an agent that pilots aircraft would not be satisfied with a policy that on average uses an amount of fuel that does not exceed the tank capacity. 
1 	sample path mdps 
as just mentioned  ross and chen pointed out the weakness of the cmdp approach with constraints on the expected amounts. as a possible solution  ross and varadarajan  1; 1  propose an approach where constraints are placed on the actual sample-path costs. in their work  the space of feasible solutions is constrained to the set of policies whose probability of violating the constraints  overutilizing the resources  in the long run is zero. however  their work concentrates on the average per-step costs and rewards  whereas we are interested in the total amounts  whose distributions are not easily calculable. the approach of ross and varadarajan has the same benefits as the standard cmdp method  i.e. no explicit modeling of resources is required  and the state space and policies are small. in addition  unlike the standard cmdp  this method is suitable for problems with critical resources. however  a weakness of this approach is that for some problems it might be too restrictive  in that it allows no possibility of overutilizing the resources. indeed  policies produced by the sample path method might have significantly lower payoff  as compared to policies that have some probability of resource overutilization. furthermore  for some domains it might be desirable for the system designer to be able to control the probability of resource overutilization  as a means of balancing optimality and risk. 
1 mdps with constraints on variance 
another approach to handling deviations from the expectation in markov models is to impose additional constraints on  or to assign additional cost to  the variance. sobel  proposed to constrain the expected cost and to maximize the mean-variance ratio of the reward. huang and kallenberg  proposed a unified approach to handling variance via an algorithm based on parametric linear programming. these approaches have the same benefits as the standard cmdp and the sample-path methods  compared to the fomdp formulation  in terms of state space and policy size  as well as the complexity of constructing the initial world model. additionally  they allow one to somewhat balance payoff and the deviation from the expected. however  these methods do not allow one to make hard guarantees about the probability of overutilizing the resources. 
1 	linear approximation 
as hinted at in the previous sections  we would like to be able to constrain the feasible solution space to the set of policies whose probability of overutilizing the resources is below a user-specified threshold  in other words  we would like to be able to solve the following math program: 
  1  
1 

where is the total amount of resource that is used by the policy  and is the upper bound on that resource  as before . 
　the trouble is that the optimization is in the space of which can be interpreted as the expected number of times for executing the actions in the corresponding states. however  it is difficult to express as a simple function of the optimization variables because the latter contain no information about the probability distribution of the random variable of the number of times the action is actually executed in the corresponding state - only the expected values.1 in this section  we present a linear approximation to the above program  based on the markov inequality:1 
 1  
using this inequality and the fact that the expected resource 
usage can be expressed as the optimization problem can be formulated as a linear program: 
　a potential weakness of this approach is that the markov inequality gives a very rough upper bound  and the policies that correspond to solutions to this lp can be too conservative  in that their real probability of overutilizing the resources can be significantly lower than  however  if making hard guarantees about the probability of overutilizing the resources is of vital importance  this method might prove valuable  as it yields policies that  in general  would have higher payoff than the ones obtained by the sample-path method  as the latter is often too restrictive . on the other hand  unlike the standard cmdps that impose constraints on the expected resource usage  and the mdps that constrain the variance  this method allows one to explicitly bound the allowable probability of resource overutilization  and to make precise guarantees about the behavior of the system in that respect. 
　it is also worth noting that  unlike the sample-path method or the methods that constrain on variance  this method relies on solving a standard linear program  whereas the former require solving quadratic or parametric linear programs. therefore  the above formulation appears to be a reasonable approximation  because it should be no harder to solve than the standard cmdp  see section 1 for experimental results   while providing a means of balancing solution quality with the precisely quantifiable risk of resource overutilization. 
1 	evaluation 
to verify our hypotheses about the properties of the approximation described in the previous section  we have performed a set of numerical experiments that compare its behavior to a standard cmdp with constraints on the expected resource amounts  section 1   and to an unconstrained mdp. 
　to reduce the bias that might arise from using a small number of hand-crafted test cases  we have instead used a large number of randomly-generated constrained mdps. all of the generated problems shared some common properties  among which the most interesting ones are the following  the values for our main experiments are given in parentheses : 
 the total number of states  actions  and resources  respectively.  1  1  1  
 maximum reward. rewards are assigned from a uniform distribution on   1  
mc = max c  maximum action cost. resource costs are assigned to state-action pairs from  based on a distribution of r and  c  r   described below .  1  
           correlation between rewards and resource costs; better actions are typically more costly. 
upper bounds on resource 
amounts are assigned according to a uniform distribution from this range.   1  1   
     dissipation of probability - the probability that the agent exits the system at each time step.   1 1   the last parameter was used to ensure a transient chain. instead of providing a small number of sink states  we have chosen to use a uniform dissipation of probability  in order to avoid additional randomization in our experiments  as the latter choice provides a more stable domain. 
　our main concern was the behavior of our approximation  as a function of the probability threshold  therefore  we have run a number of experiments for various values of 
       to be more precise  we have gradually increased from 1 to 1 in increments of 1  and for each value  gen-
erated 1 random models and solved them using the three methods: 1  an unconstrained mdp without any regard for resource limitations  1  a standard cmdp with constraints on the expected usage of resources  and 1  our cmdp with constraints on the probability of overutilizing the resources  eq. 1 . we then evaluated  using a monte-carlo simulation  each of the three solutions  policies  in terms of expected reward and probability of overutilizing the resources. 
　figure 1 shows a plot of the actual probability of overutilizing the resources for the policies obtained via the three methods as a function of the probability threshold . the data points are averaged over the runs for a particular value of po. the curve that corresponds to the method that bounds the overutilization probability also shows the standard deviation for the runs. the other data have very similar variance  so we will use the plots of means  averaged over the runs for a given po  for our analyses. 
　obviously  po has no effect on the unconstrained and the standard cmdp  which maintain more or less a constant probability of overutilization   but it does affect to a large extent the solutions to the problem with constraints on overutilization probability. one can see that the overutilization probability for the solutions produced by our approach is always below  as it should be . also  it is worth noting that when approaches 1  our approximation yields the same results as the other methods  which is good  since setting  should not constrain the space of feasible solutions. 

1 	resource-bounded reasoning 


figure 1: probability of resource overutilization. 

figure 1: average rewards for solutions to the three problems. 
   the rewards obtained by these policies are shown in figure 1. these actual rewards do not necessarily equal the expected rewards  which are used during the optimization process . this is because only the runs that did not overutilize the resources were included in the average  and policies were not penalized for violating the resource constraints. this also explains why the total rewards received by solutions to the standard cmdps were sometimes greater than the ones obtained by solutions to the unconstrained problems. 
   however  realistically  an agent always incurs a penalty for overutilizing a critical resource  where the penalty amount is based on the consequences of overutilization. for example  if the agent is flying a plane  and the resource in question is fuel  the consequences of trying to use too much of that resource are catastrophic  so the penalty is very high. if we take this into account by assigning a fixed penalty to policies that overutilize the resources  we can update the graph in figure 1 to get a more realistic picture. figure 1 shows the average rewards for solutions obtained via the three methods  recalculated to reflect the penalties: new rewards are where is the overutilization 
probability  r is the average reward for successful runs of the given policy  as in figure 1   and w is the penalty for overutilization. we see that  if we take the penalty into account  there is an interval of po where the conservative policy produced by our linear approximation outperforms the other policies. 

figure 1: average rewards with penalties for overutilization. 
moreover  for large penalties  iv = -1   the conservative policy outperforms the others for all values of; . note that here  the policy is re-evaluated post-factum. section 1 briefly discusses an approach that explicitly models the penalties in the optimization program. 
   as we mentioned in section 1  the linear approximation should be no harder to solve than the standard constrained mdp  because both are formulated as linear programs with the same number of constraints. to experimentally verify this  we have timed the runs of all our experiments. figure 1 contains a plot of the times that it took to solve the problems in all our experiments. one can see that the running times for all three methods are not appreciably different.1 in particular  the average ratio of the running time for the standard cmdp to the running time of the unconstrained method is 1; the ratio of the running time of our linear approximation with constraints on overutilization probability to the running time of the unconstrained method is 1. the slight downward curvature of the plot of the running time of our approximation method appears to be a consequence of the specific implementation of the linear programming algorithm that we used in our experiments. 
1 	discussion and future work 
our experiments substantiate the claims that our approximation provides an effective and efficient method that agents can use to formulate policies that not only consider limitations on execution resources  but that also explicitly bound the probability of resource overutilization. our new approximation achieves the constraints on overutilization  and is essentially no more expensive to use than more standard cmdp and unconstrained mdp methods. because our method constructs policies that are more careful about avoiding resource overutilization  the rewards associated with its policies when resources are not overutilized tend to be less than the rewards for the other methods' policies when resources are not overutilized. however  as we illustrated in figure 1  when overutilization incurs penalties our new method can outper-

resource-bounded reasoning 	1 


figure 1: running time for the three methods 
form previous techniques. thus  our new method is particularly suited to agents engaged in mission-critical domains. 
　furthermore  if penalties for resource overutilization are known at design-time and can be expressed in the same units as the rewards  an interesting modification of our lp formulation is to include the penalties in the policy evaluation criterion  as opposed to modeling the constraints on the overutilization probability. this would yield an lp with the following objective function: 
		 1  
where wk is the penalty  in units of ria  incurred for overutilizing resource k. the maximization is subject to just the standard  conservation of probability  constraints as in  eq. 1 . a benefit of this formulation is that  for certain initial probability distributions  deterministic policies are optimal. a downside is that the formulation does not allow one to explicitly control the acceptable overutilization probabilities. 
　as can be seen in figure 1  our linear approximation is in general overly conservative. for example  given permission to overutilize the resource 1% of the time  p1 = 1   the method generates a policy that overutilizes the resource only about 1% of the time. since rewards and resource usage are typically correlated  we would expect that a policy that undershoots the permitted resource overutilization probability by a lower amount would also yield a higher expected reward. toward this end  we have formulated a quadratic programming approximation  based on the chebyshev inequality  which allows us to put a better upper bound on the probability of resource overutilization: 
 1  
where 
allowable regions for the amounts of used resources. 
　we are also investigating another formulation of the optimization program that should give a more accurate estimate of the resource overutilization probability. the method is based on a polynomial approximation of the pdf of the total resource-usage cost and uses the moments of the cost as the optimization variables. this approximation and the one based on the chebyshev inequality are more costly to compute than the linear one. our current efforts are to encode the approximations and evaluate their strengths and weaknesses. 
1 	acknowledgments 
this paper is based upon work supported by darpa/ito and the air force research laboratory under contract f1-c-1 as a subcontractor through honeywell laboratories. the authors thank kang shin  haksun li  and david musliner for their valuable contributions  as well as one of the anonymous reviewers whose comments inspired  eq. 1 . 
