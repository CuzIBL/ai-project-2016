 
   register vector grammar is a new kind of finite-state automaton that is sensitive to context-without  of course  being contextsensitive in the sense of chomsky hierarchy. traditional automata are functionally simple: symbols match by identity and change by replacement. rvg is functionally complex: ternary feature vectors  e.g. +-¡À--++  match and change by masking   + matches but does not change any value . functional complexity-as opposed to the computational complexity of non-finite memory-is well suited for modelling multiple and discontinuous constraints. rvg is thus very good at handling the permutations and dependencies of syntax  wh-questions are explored as example . because center-embedding in natural languages is in fact very shallow and constrained  context-free power is not needed. rvg can thus be guaranteed to run in a small linear time  because it is fs  and yet can capture generalizations and constraints that functionally simple fs grammars cannot. 
	i 	introduction 
   lately there has been considerable impetus among natural language researchers to restrict the computational complexity required by an adequate theory  cf. gazdar 1  church 1  langendoen 1 . whereas fs languages guarantee linear recognition time  those calling for more computational power give rise to a combinatorial explosion with respect to worst recognition time. certainly  were there no other factors  such as those mentioned by berwick and weinberg 1  perrault 1  pullum 1 and 1   grammars with less computational complexity would be preferred  because they are more easily parsed  and probably also learned  see berwick 1 . 
   until recently  however  it has been supposed that grammars with a more desirable recognition time are cursed by a proliferation of rule structures when it comes to representing generalities and particularities of natural languages. the dilemma of computational complexity vs. linguistic generality has resisted a satisfactory solution ever since chomsky 1 argued that competence for natural languages requires at least context-free power in order to handle embedding of clauses. moreover  to capture generalizations about categories that participate in variations of order  he introduced transformations of phrase structure-further increasing computational complexity  cf. peters and ritchie 1 . similarly  woods 1 justified recursion plus the manifold tests and registers of atns  which make his scheme  equivalent to a turing machine in power   because  the actions which it performs are 'natural1 ones for the analysis of language.  
   but natural languages can be modelled by a kind of finite-state device that is quite compact. we can avoid excessive duplication of categories  and having to approximate unbounded memory resources. this is possible- i¡ê we are willing to abandon the functional simplicity implied by symbols in rule patterns. 


1 g. blank 
the table-driven implementation consists of 1  a register  here called the current syntactic state register  or cssr   1  a table of syntactic productions  the synindex  and 1  a machine which initializes the cssr  makes transitions from state to state  and responds to a final state. the synindex is a list associating: i  a category symbol  ii  a condition state symbol and i i i   a result state symbol. a machine that recognizes sentences makes transitions by first matching a synindex category symbol with a word's lexical category and the production's condition symbol with the cssr; it then replaces the symbol in the cssr with the production's result symbol. 
following the example grammar  a transition from 
n1 to n1 is possible because the cssr has been initialized to n1  and the category of a is det. the cssr then gets that production's result symbol  n1. so long as the list of productions is finite  it is a fs grammar. such a machine has a finite number of possible states  though it can run on indefinitely. the category prep  for example  iterates back to n1 . 
   a fs grammar requires that every category be determined as possible by a function of the immediately preceding category. space for the register and index are pre-allocated; no external memory is available. this is as true of the equivalent rvg: 

in a rvg  the symbols become vectors of features  each capable of three possible values  + - +   or  on    off    mask   or henceforward 1 1 . the match function allows either identity or ambiguity  1 matches any value ; the change function allows either literal or masked replacement  1 doesn't change anything . for example  if the cssr is initialized to 1  the condition vector of det  1  matches  and the result of det  1  can be applied  yielding 1. this is what is meant by functional complexity: whereas in a simple fs automaton match is identity and change is wholesale replacement  in rvg match and change allow ambiguity and masking. nevertheless the ternary functions are quite simple and determinate. 
note that the rvg synindex is considerably more compact than the equivalent fs table. whereas the fs table has three productions for the category 
adj and six for n  the rvg synindex has just one production per category. the savings-not only of space in the table  but time trying alternativeswill multiply as more permutations of order are introduced. 
   how do rvgs achieve their efficiency  functional complexity confers two properties that functionally simple grammars do not have: multiplicity and masking. condition vectors can convey multiple constraints. for example  prep  condition 1  cannot occur until the first three features have been reversed in value. moreover  result vectors can produce multiple effects: prep  result 1  re-enables all np-opening productions. note also that disjoint categories  e.g. n and pr1n  share the same position in ordering vectors  so that they mutually exclude each other. that is  the result of n and pron both disable the same feature 
 governing the head position . 	 pron  result 
1  also rules out having a post-modifying prepositional phrase.  
   masking is a consequence of ternary values. the third possible value  1  matches any value and does not change any value. thus constraints may be passed through intervening states. for example  the category n is oblivious to whether or not the categories det or adj have already occurred  since its condition vector  1  matches the initial state  1  or the state after det  1  or after adj  1 . thus we can represent options as well as obligations.  note that the ordering of optional productions is enforced by having successive categories also disable the constraining features  e.g. the result of n is 1.  moreover  iteration may be treated as a special case of optionality. whereas 
a one-occurrence category disables itself  e.g. the result of det is 1   an iterative category does not  the result of adj is 1 . like an optional category  an iterative category is disabled by successive categories up to the next obligatory category  e.g. the result of n is 1 . finally  masking features allows constraints to be held through any number of intervening states-as will be demonstrated when 
we consider long-distance dependencies. 
   finite functional complexity can thus increase the expressiveness of a grammar considerably. it is not to be confused with  complex symbols  found elsewhere in linguistic theory. the subcategorization symbols of chomsky 1 allow for context-sensitive power; the features of gazdar and pullum 1 call for recursive elaboration of symbols. features attached to gp1g phrase structure nodes themselves take the form of open-ended trees or directed graphs. the feature vectors of rvg  on the other hand  are finite and do not expand. moreover  rvg is not an 
attribute-value system requiring piecemeal interpretation of individual features. whole vectors  a ternary vector can be implemented as a pair of 

bit vectors   rather than lists of symbols  are the operational unit of on-line processing. 
   in any case  other researchers have used oomplex features to govern categorization  but never to describe syntactic states. 
	i l l 	wh-questions 
   wh-questions are perhaps the long-distance dependency par excellence. a wh-word  who  what  etc.  provokes the grammar to watch and wait for a 'gap'-an expected but missing noun phrase. the gap may be filled wherever a noun phrase is expected in a clause: 
what is the robot seeking  
wh:what; aux:is; subj:np:det:the; n:robot; 
vtrans:seeking; obj:ngap:cclose: ; 
what is the wrench on  
wh:what; aux:is; subj:np:det:the; 
n:wrench; prepion; ngap:cclose: ; 
what did the robot find the wrench on  wh:what; aux:did; subj:np:det:the; 
n:robot; vtrans:find; obj:np:det:the; 
n:wrench; preppost:on; ngap:cclose: ; 
what is a robot  
wh:what; aux:is; subj:np:det:a; n:robot; 
prednp:ngap:cclose: ; 
what did the robot find the wrench  synindex search fails at:   
delaying  for the moment  discussion of embedded clauses  we can see that wh-questions can be 
modelled straightforwardly. 
synindex for wh-questions 

g. blank 1 
cclose requires that gap have been turned off. the last sentence above fails because gap is never disobligated. 
   it should be observed that in rvg the recurrence of noun phrases at various positions within a clause does not involve calling a subnetwork. eschewing recursion  we instead allocate separate sections of the ordering vector type to deal with clause matters and phrase matters. ternary masking allows the cssr to preserve its clause-level status while it traverses a phrase. a number of boundary productions-subj  obj  iobj  prep  etc.-set a feature which at once temporarily disables subsequent clause-level productions  and enables the boundary production np. if np is chosen- there are other possibilities-all of the features ordering a phrase will be reset. one alternative is ngap  whose condition requires that both the np and gap be on  and its result turns both of these features off. thus np and ngap are disjoint. 
	iv 	embedding 
   clause embedding was the primary evidence cited by chomsky 1 for claiming that natural languages are at least context-free. he discusses  for example  the following kinds of structures: 
if s1 then s1 if  either s1 or s1 s1- then s1 if  either  george said that s1 s1 
or s1 s1 then s1 
the argument goes: i  finite-state automata cannot handle anbn  or  mirror image   grammars because they do not have the memory to keep track of n; ii  if..then  either..or constructions suggest that natural languages are of this type; i i i   therefore natural languages cannot be finitestate. 
   there are  however  severe constraints on embedding. the most well-known case is that of object relative constructions: 
the mouse the cat chased squeaked. 
the mouse the cat the dog bit chased squeaked. 
embedding object relatives once is not unusual  but twice is boggling. the most common explanantion for this problem is that centerembedding causes an overload of short-term memory processing. but as kac 1 has pointed out  this cannot be the whole story. for if it were we would expect embedding to break down at a predictable depth. but it does not. for example  though one clause can embed another of a different type  there is no self-embedding: 
if if the pope is catholic then pigs have wings then napoleon loves josephine. 
that that dan likes sue annoys bob bothers me. 
these sort of constructs are plausible  of course  

1 g. blank 
in truly context-free languages such as pascal or lisp. 
   nor are relative clauses a uniform phenomena. first of all  subject relatives are possible indefinitely: 
the fellow who saw the dog that bit the cat that chased the mouse is willing to testify. 
object relatives appear to be limited to a depth of one  but combinations of an object relative and a noun complement are not: 
the statement that the election which he lost ended his career dismays him. 
the only one who the fact that george resigned pleased was tom. 
embedding also goes to deeper levels in order to attach suspended arguments or adverbial adjuncts: 
a teacher who wants students who persuade their classmates who don't know the material to help them to ask him instead must make himself available. 
do you see why i wanted to deny that grammar is recursive so vehemently now  
in the first sentence  each infinitive clause i.s projected as an argument of an earlier predicate  wants persuade . all of the relatives are subject relatives  so for each no gap is suspended  only an argument. in the second sentence  adverbs are attached to predicates after intervening complement clauses-so vehemently to deny and now all the way back to the main clause's predicate  see. 
   though short-term memory is related to the shallowness of center-embedding generally  it is not an adequate account for the variety of specific constraints. typically  memory limitations have been regarded as an aspect of performance rather than competence. the perfor-
mance processor  with limited memory  is said to 
 approximaten competence  which is said to  idealize  the unlimited memory of a cf automaton. thus when church 1 talks about finite-state processing  his aim is just  to design a parser that approximates competence with realistic resources.  but one wonders: since memory constraints are universal to the species  why aren't they of import to models of competence  the various constraints on center-embedding argue against the functionally simple notation for denoting clauses  e.g. s or s-bar   which seems to imply that all clauses are  almost  alike. the versatility possible with the finite functional complexity of rvg re-opens the question whether cf power is needed. 
   if..then and either..or are better treated as discontinuous constraints than as  mirror-image  syntax. a single feature allocated for each paired construct can give us the obligations and options we want  with the help of a production to 

this fragment effectively enforces these constraints: i  if obligates a then clause before dclose; ii  either obligates an eith-or clause before dclose; i i i   if forbids another if until a then re-options i t ; iv  either forbids another 
either until eith-or re-options i t ; v  either forbids if until eith-or re-options i t . the last case ensures a constraint that kac 1 notes but cannot explain: 
either if  clause  then  clause  or  clause  
 a possible explanation for this constraint is that it avoids many ambiguities that might otherwise be brought on by the conjunction or.  
   in general  embedding can be modelled by any fs automata so long as the maximum depth is finite. conceivably even a simple fs automaton can manage embedding  by duplicating clause syntax at every possible entry point. but such a grammar would be enormous  and is perhaps justifiably scorned as lacking  explanatory adequacy.  
   but ternary vectors can consolidate matters considerably. rather than having to respecify for every possible site of embedding  an rvg need only keep track of the current level of embedding. 
   the data reviewed in this paper can be managed by keeping track of up to three levels of embedding. the top  or main clause level  is treated differently from the bottom two. where the grammar does allow us to embed more deeply  e.g. right-embedded complements  subject relatives  etc.   it will begin to iterate in the space of the bottom two clauses. thus we can adjoin arguments or adverbials to the current clause  or one clause higher  or to the main clause  which is always kept. e.g.: 
do you see  why i wanted  to deny  that grammar is recursive  so vehemently   now  
the adverbial so vehemently is attached back over an intervening clause; now to the main clause. 
   there are two styles in which one might implement clause embedding in rvg. they are virtually equivalent in terms of computational efficiency  but reviewing both will perhaps elucidate rvg methodology. the first is in the same spirit as np embedding. just as ordering of phrasal constituents is managed by a segment of the complete ordering vector  so each clause level might be treated as a separate segment of a long ordering vector type. just as several boundary productions  subj  prep  etc.  suspend further clause-level productions while enabling np  so other boundary productions may shift attention from one clause level to another. all of the segments are part of the same vector type  so that there is no need for storage beyond the cssr  
which holds a complete vector. yet it is possible  because of ternary masking  for boundary productions to  as it were  open and close  windows  on relevant segments. the drawback of this technique is that  for each level  the grammar must replicate the features for clauses  the ordering vectors all get wider   and also replicate most of the clause-level productions  the synindex table gets longer . replicating productions such as subj  cadj  et al.  is necessary in order to apply different condition and result vectors at each clause level. increased size is not necessarily a clinching drawback  since it is by a factor somewhat less than three  phrasal and discourse-level features and productions need not be replicated . the alternative approach makes for a smaller synindex  to be manipulated by a slightly more complicated algorithm. 
   instead of widening the ordering vector  we add depth to the cssr. the new cssr is multi-leveled: 

the state register is s t i l l finite and fully visible; no external memory is needed. only now it has ordered levels as well as features. it is as if we broke up a long vector  masking inactive segments with an array subscript rather than ternary values. to embed a clause  the shifting facility increments clauselevel  and initializes the lower clause from the higher  so that features like gap can be passed down . to return  it simply decrements clauselevel. 
   this scheme captures generalizations about what clause levels hold in common. it also allows specialized productions to distinguish clause types-with different result vectors. e.g.  the complementizer that sets up a complete clause  but the infinitive particle to arranges for a clause starting with a non-finite verb. similarly  the complex np constraint  ross 1  is easily modelled by different result codes. after shifting clause level  change applies as usual. complement productions allow feature gap to be passed down  by masking   whereas relatives reset i t : 

g. blank 1 
who were you persuaded to find the wrench by  
wh:who; tense:-past be:be; subj:np:pron:you; 
npclose: passive:-past par t.vditrans: persuade; compinf:to; nonfin:-inf  vtrans:f ind; obj:np: det:the; number:-sing n:wrench; npclose: 
passgap: passiveby:by; ngap:cclose: ; 
ciause-closing productions may or may not insist that gap be off. relclose makes this requirement  but does not affect the higher clause  whereas filledgap does turn off gap in the higher clause  and passgap leaves gap alone. 
   after the shifting facility has embedded two clauses  it resorts to re-use. that is  the second embedded clause is shifted up to the first. thus right-embedding can iterate in two registers  but preserves the main clause. two embedded clause registers are enough to allow the parser to resume suspended arguments or adverbials  as in 
a teacher who wants students who persuade their classmates who don't know the material to help them to ask him instead must make 
himself available. 
but its center-embedding capacity has now has been reached. 	the parser is baffled by this sentence: 
pamela persuaded the robot who wanted to give the pyramid which was on the blocks which it found to her very much to find a wrench instead. 
by the time it encounters very much  the clause to which the adverbial might have been attached  wanted...  has been lost to re-use. but such a sentence is beyond the tolerance of many human speakers as well. 
   superficially this embedding scheme resembles a bounded stack. but there are some crucial differences. first of all  a bounded stack scheme  such as that of church 1  typically calls for a great deal more storage than just three registers  since it will have to keep track of the full gamut of embedding implied by ps rules-nps  vps  x-bars and the like. second  whereas functionally simple systems treat every level of embedding alike  in rvg each level of embedding is 
marked-main clause  first embedded  re-usable. thus in rvg there is no self-embedding. third  
cssr levels are not really organized like a stack  since the lower two registers are re-used iteratively  and the main clause register is always preserved. in fact there is nothing which prevents a rvg from accessing clause levels in another order. for example  we could model cross-serial dependencies in dutch  see bresnan et al. 1   by allowing boundary productions to access storage registers in a queue-like order.  assuming that cross-serial dependency  like center-embedding  is limited.  

1 g. blank 
	v 	toward a complete rvg system 
   rvo syntax may be thought of a general-purpose scheduler  with potential applications wherever such a device would be desirable. generally  non-syntactic procedures can be scheduled by association with synindex productions. indeed  syntactic productions are motivated by nonsyntactic requirements. for example  nouns  pronouns and names are very similar posltionally  but they are treated as distinct categories because of semantics. in this section i will briefly describe how an rvo oan  while maintaining 
modular autonomy to a considerable degree  support integration of processes. 
   categories. in earlier versions of rvg  categories  like syntactic ordering and semantic constraints  were represented in terms of ternary feature vectors. ternary values supported cross-categorization well-lexemes in more than one category could allow them with 1's. but since rvg holds down duplication of productions  it is feasable to associate  with each lexeme  a list of synindex production numbers. moreover it is possible  with some pre-processing  to let these production numbers be generated from labels  and also to infer the set of non-lexioal productions that could precede each lexical production. thus  for example  the lexeme kitten is categorized by the label n  which is converted to a production number; the non-lexioal production numbers for sub j  obj  np  etc.  are kept in a precedes set associated with the synindex entry for n. this approach is at once more convenient  because categories are kept functionally simple with respect to notation  and more efficient  because the lexicon is 'wired' directly to the synindex  
obviating any on-line processing of categories. 
   semantics. rvg syntax can in fact support just about any form of semantics. but i present our biases. since rvg permits a relatively flat  direct treatment of permutations of order  there is no reason to complicate matters-as trees do  since they imply recursion. rather than transform syntactic structures  or propose meta-rules or lexical-dependency subtrees which have similar effect  why not let categorized actions operate upon semantic structures directly  the problem of mapping constituent structures into lexical structures is simplified by just eliminating context-free constituent structures. there is no need to move or unify sub-trees in rvg; there are no trees at all. moreover  semantic and morphological agreement can be simplified: there is no need to 'percolate' or 'inherit' features up and down trees  nor to design special filters or powerful constraints to regulate such activity. functional complexity allows rvg to be sensitive to context without being context-sensitive in the sense of the chomsky hierarchy. 
   in rvg  we allocate a fixed configuration of registers  the current predication state registers  or cpsr  for managing grammatical relations. the cpsr and cssr together comprise the state of a clause  or a phrase in a clause   and as shown above  rvg keeps track of up to three clauses. permanent semantic representation is built up dynamically  it is here that we allow open-ended structures ; cpsr slots hold addresses of proposed semantic referents. 
   categorized actions  associated with syntactic productions  have the responsibility of mapping new lexical material into existing semantic structure. in the spirit of rvg  lexemes are viewed as standardized in structure. all semantic features are organized in a single vector type  the first-order semantic vector. every lexeme has an intrinsic first-order code. constraints on arguments of predicates are specified  as needed  in terms of additional first-order vectors. first order  pred  a generalized procedure called by many categorized actions  for all arguments  first matches vectors  thus checking selectional restrictions   then unifies them. e.g.  1 matches 1 and yields 1. thus in rvg features are not used merely to validate  but also to define semantic structures. 
   morphology. kunst and blank  1  show that morphology oan be efficiently implemented as a retrieval tree  with provision for morphological paradigms as nodes encountered during lexical lookup. currently  we represent morphemes as lexemes in their own right  each with its own list of synindex production numbers and semantic structure. thus  for example  lookup recognizes oats as two lexemes: -plur and cat. the lexeme -plur has an intrinsic vector  distinguishing it from-sing  . it also lists the subscript for the synindex production numberer  which maps the 
intrinsic of the lexeme  -.plur  to cpsr head   and then calls first  order pred: thus enforcing number 
agreement. another morphologically categorized production  tense  enforces 'subject-verb' agree-
ment. actually  the kittens is playing fails at tense  whereas the bricks are playing violates constraints at vtrans. but note that all agreement is managed by uniform operations upon a single vector type. 
letters must have been being written. 
subj:np:num1er:-plur n:letter; npclose: 
tense:-pres modal:must; nqnfin:-inf have:have; 
perf:-pastpart be:be; prog:-progpart be:be; passive:-pastpart vtrans:write; ccl1se:.; 
   discourse  etc. rvg does not require that 's' be the root of syntax. the examples in this paper suggest that a disco use-level syntax could be integrated as part of the synindex  or as a separate production table interfacing with the synindex. paired particles were modelled as disoourse-level discontinuity  and wh-questions are marked by a feature switched on by the production wh. generalizations about syntax  expressed by features in vectors rather than by complex rule patterns  are thus more readily available to other systems. 
	rvgs are highly reversible. 	a parser and a 

generator can be implemented  and have been  using the same synindex and basic table-driving algorithm. indeed  all of the major data structures of rvg work in either direction as is; only the procedures need actually be reversed. while parsing  discourse-level features are set by syntax for higher-level consideration. while generating  discourse-level features are set from above to choose particular sentence forms. 
   rvgs are computationally simple and compact. the basic algorithm is that of a table-driven finite-state device  modified to invoke ternary match and change functions. ternary vectors can be represented on a binary computer by paired bit vectors. ternary match and change are implemented by combining logical operations  exploiting the low-level parallelism of bits in computer words . the most recent version of rvg  in pascal  earlier versions were in sncbol and icon   does so  with great gains in speed. on a dec-1  the syntactic parser averages about 1 milliseconds per lexeme. further improvement is possible-on ternary circuitry. 
   expansion to fuller coverage  a substantial fragment of english syntax has been implemented  will have a minimal slowing effect  since each new category motivates one new production  rather than many new rules  or a meta-rule that generates many rules . each revision is on features already allocated in all productions. occasionally new features must be allocated  but most frequently early in grammar-making  and least frequently late. 
   ambiguous parsing to be sure slows processing down  though not unbearably. the combination of well-specified category vectors  semantic constraints and possibly a form of bounded parallelism  under investigation  can hold syntactic processing time to a small linear time. thus parsing of natural languages appears to be feasable by machines in real time. indeed  syntax should; be fast  if it is to facilitate the many other processes-from phonology to reasoning- which all go on in real time. 
acknowledgement 
   rvg originates in unpublished work by a. e. kunst  to whom i am also grateful for help with this paper. in an unpublished paper  
professor 	kunst 	compares 	rvgs 	to 	petri 	nets 
 rather than simple transition networks . petri nets also allow functional complexity. specifically  he compares of safe petri nets  which are known to be weakly equivalent to fs automata  and rvgs. 
g. blank 1 
