
recent work on compiling bayesian networks has reduced the problem to that of factoring cnf encodings of these networks  providing an expressive framework for exploiting local structure. for networks that have local structure  large cpts  yet no excessive determinism  the quality of the cnf encodings and the amount of local structure they capture can have a significant effect on both the offline compile time and online inference time. we examine the encoding of such bayesian networks in this paper and report on new findings that allow us to significantly scale this compilation approach. in particular  we obtain order-of-magnitude improvements in compile time  compile some networks successfully for the first time  and obtain orders- of-magnitude improvements in online inference for some networks with local structure  as compared to baseline jointree inference  which does not exploit local structure.
1 introduction
it was shown recently that compiling bayesian networks corresponds to factoring multi-linear functions  mlfs   darwiche  1 . in particular  each bayesian network can be characterized by an mlf of exponential size  whose evaluation and differentiation solves the exact inference problem. moreover  the mlf can be factored into an arithmetic circuit  ac  whose size is not necessarily exponential  allowing one to use acs as a compiled representation of bayesian networks. interestingly   park and darwiche  1  has shown that building a jointree  jensen et al.  1; shenoy and shafer  1  for a bayesian network corresponds in a precise sense to a process of factoring the mlf into an ac  which is embedded by the jointree structure.
¡¡these findings created new possibilities for performing exact inference  as they provided a new computational framework based on factoring mlfs. in fact  a specific mlf factoring method was proposed in  darwiche  1   which can exploit the structure inherent in network parameters. according to this approach  one encodes the mlf using a propositional theory in conjunctive normal form  cnf   factors the cnf  and then immediately extracts the ac from the cnf factorization. the benefit of this logical approach is twofold. first  it allows one to encode local structure in the form of determinism and context specific independence  csi   boutilier et al.  1 . for example  using this approach  it became practical to compile some bayesian networks with binary variables and excessive determinism  induced by relational models  and having treewidths in excess of 1 by compiling the ac in minutes and evaluating them in seconds  chavira et al.  1 . the second advantage of this approach is its ability to accommodate different representations of conditional probability tables  cpts   decision trees  rules  noisy-or  etc.   without the need for algorithmic change. in this paper  we consider only tabular representations.
¡¡the critical computational step in the above approach is clearly that of factoring/compiling the cnf  which is done using an exhaustive and refined version of the dpll algorithm  davis et al.  1; darwiche  1 . the key observation underlying this paper is that the efficiency of the factoring step-both factoring time and size of factorization-can be significantly improved through careful cnf encodings which capture as much local structure as possible  and by passing additional information about these cnfs to the factoring algorithm. this becomes especially true when handling networks that have local structure  large cpts  yet no excessive determinism. we do indeed propose a particular cnf encoding which appears quite effective on networks with such properties. we also identify a key semantic property of the resulting cnfs  which we exploit in the cnf factoring algorithm. by incorporating these findings  we show dramatic improvements in the both offline compile time and online inference. in the offline compilation phase  we get an order-of- magnitude improvement in some cases and an ability to compile some networks for the first time. in the online phase  we observe orders-of-magnitude improvements on some well known benchmarks  such as pathfinder  munin1  and water  over online inference by the baseline jointree algorithm.
1 factoring multi-linear functions
our investigation is based on three technical observations  darwiche  1 : that every bayesian network can be interpreted as an exponentially-sized mlf whose evaluation and differentiation solves the exact inference problem; that such an mlf can be factored into an ac whose size may not be ex-

rowabcpr c | a b 1a1b1c1¦Èc1|a1= 1a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 1a1b1c1¦Èc1|a1= 1a1b1c1¦Èc1|a1= 1a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 11a1b1c1¦Èc1|a1= 1figure 1: a small bayesian network with one of its cpts  showing local structure in the form of determinism and csi.
f = a+ad+
abd+abcd
figure 1: an mlf factored into an ac.
ponential; and that the mlf factoring process can be reduced to factoring a cnf encoding of the mlf.
¡¡the mlf for a network contains two types of variables. for each value x of each network variable x  there is an indicator variable ¦Ëx. for each network parameter pr x|u   there is a parameter variable ¦Èx|u. the mlf contains a term for each instantiation of the network variables  and the term is the product of all indicators and parameters that are consistent with the instantiation. for the network in figure 1  variables a and b have two values  and variable c has three values. the mlf corresponding to this network is as follows: ¦Ëa1¦Ëb1¦Ëc1¦Èa1¦Èb1¦Èc1|a1 b1 + ¦Ëa1¦Ëb1¦Ëc1¦Èa1¦Èb1¦Èc1|a1 b1+ ... ¦Ëa1¦Ëb1¦Ëc1¦Èa1¦Èb1¦Èc1|a1 b1 + ¦Ëa1¦Ëb1¦Ëc1¦Èa1¦Èb1¦Èc1|a1 b1
to compute the probability of evidence e  we evaluate the mlf after setting indicators that contradict e to 1 and other indicators to 1. for example  to compute pr a1 b1   we set indicators ¦Ëa1 and ¦Ëb1 to 1  set other indicators to 1  and evaluate the reduced mlf: pr a1 b1  =
¦Èa1¦Èb1|a1¦Èc1|a1 b1 + ¦Èa1¦Èb1|a1¦Èc1|a1 b1 + ¦Èa1¦Èb1|a1¦Èc1|a1 b1
¡¡as is obvious from the above example  the mlf has an exponential size. yet the mlf can be factored into an ac whose size may not be exponential  leading one to formulate the exact inference problem as a problem of factoring mlfs into acs. an ac is a dag with internal nodes labeled with multiplications/additions and leaves labeled with variables and constants; see figure 1
¡¡one way to factor an mlf into an ac is by encoding the mlf into cnf and then factoring the cnf. to illustrate the encoding scheme  consider again the mlf f in figure 1 over real-valued variables a b c d. the basic idea is to specify this mlf using a propositional theory that has exactly four models  one for each term in f. specifically  the propositional theory  f = va ¡Ä  vb   vd  ¡Ä  vc   vb  over boolean variables va vb vc vd has exactly four models and encodes f as follows:
modelvavbvcvdencoded term¦Ò1truefalsefalsefalsea¦Ò1truefalsefalsetruead¦Ò1truetruefalsetrueabd¦Ò1truetruetruetrueabcdthat is  model ¦Ò encodes term t since ¦Ò vj  = true precisely when term t contains the real-valued variable j.
¡¡by factoring/compiling the cnf  f as discussed in  darwiche  1   one can immediately extract an ac representation of the mlf f in time and space proportional to the factored cnf  darwiche  1 . we will discuss this process later  but we first provide more detail on the encoding step. we start here with the baseline encoding  darwiche  1   which we refer to as prev.
¡¡the cnf has one boolean variable v¦Ë for each indicator variable ¦Ë  and one boolean variable v¦È for each parameter variable ¦È. for brevity though  we will abuse notation and simply write ¦Ë and ¦È instead of v¦Ë and v¦È. cnf clauses fall into three sets. first  for each network variable x with domain x1 x1 ... xn  we have:
	indicator clauses :	¦Ëx ¡Å ¦Ëx ¡Å ... ¡Å ¦Ëxn
  for i   j
for example  variable c in figure 1 generates:
¦Ëc1 ¡Å ¦Ëc1 ¡Å ¦Ëc1   ¦Ëc1 ¡Å  ¦Ëc1   ¦Ëc1 ¡Å  ¦Ëc1   ¦Ëc1 ¡Å  ¦Ëc1
these clauses ensure that exactly one indicator variable for c appears in each term of the mlf. the remaining sets of clauses correspond to network parameters. in particular  for each parameter ¦Èxn|x1 x1 ... xn 1  we have:
ip clause : ¦Ëx1 ¡Ä ¦Ëx1 ¡Ä ... ¡Ä ¦Ëxn   ¦Èxn|x1 x1 ... xn 1 pi clauses : ¦Èxn|x1 x1 ... xn 1   ¦Ëxi  for each i
for example  parameter ¦Èc1|a1 b1 in figure 1 generates:

¡¡the models of this cnf are in one-to-one correspondence with the terms of the mlf. in particular  each model of the cnf will correspond to a unique network variable instantiation  and will set to true only those indicator and parameter variables which are compatible with that instantiation.
1 encoding techniques
the prev encoding as discussed does not encode information about parameter values  local structure . however  it is quite

stricted representations of mlfs  they can be unfolded into acs .
table 1: -: jointree ran out of memory.
maxac inferenceimprovementnetworkclustertime  s 
over jtbm-11.1 1stud-11.1 1mm-1-111mm-1-11-bm-11.1-stud-11.1-easy to encode information about determinism within this encoding. consider figure 1 and the parameter ¦Èc1|a1 b1 = 1  which generates the four clauses in  1 . these clauses ensure that the parameter ¦Èc1|a1 b1 appears in an mlf term iff that term contains the indicators ¦Ëa1  ¦Ëb1 and ¦Ëc1. however  given that this parameter is known to be 1  all terms that contain this parameter must vanish. therefore  we can suppress the generation of a boolean variable for this parameter  and then replace the above clauses by a single clause:
 ¦Ëa1 ¡Å ¦Ëb1 ¡Å ¦Ëc1. this clause has the effect of eliminating all cnf models which correspond to vanishing terms  those containing the parameter ¦Èc1|a1 b1.
¡¡armed with determinism  the prev encoding can produce impressive results when applied to networks with only binary variables  that contain small cpts  and that contain large numbers of 1 parameters. for example   chavira et al.  1  reports on networks with such properties  generated from relational models  and table 1 reviews some of these results. as is clear from the table  one gets exponential improvements over the standard jointree method  which does not take advantage of network determinism.1 similar results have also been reported in  darwiche  1  with respect to bayesian networks corresponding to digital circuits.
¡¡for bayesian networks with local structure  large cpts  yet no excessive determinism  the encoding of determinism alone may not be so effective. table 1 lists a set of benchmark networks  some having variables with large cardinalities  others having very large cpts  and where the amount of determinism is not necessarily excessive. table 1 provides statistics on the cnfs generated for some of these networks  according to the prev encoding  while also encoding determinism as discussed above. these cnfs are quite large  but the striking property they have is the large percentage  up to 1% in some cases  of boolean variables that represent parameters versus those representing indicators. some of these cnfs proved challenging to factor  some taking too long and others running out of memory. there are two key observations  however  that allowed us to handle these networks successfully  leading to significant improvements in both offline compile time and online inference time. we explain each of these in some detail next  but after providing an overview of the cnf factoring/compilation algorithm of  darwiche  1 .
table 1: cnfs generated by prev  determinism encoded .
networkvarsparm vars
clausesliteralspathfinder11water11mildew11munin111munin111diabetes111 how the cnf factoring/compilation process works
we provide in this section a sketch of the cnf factoring process. we note  however  that this section may be skipped on a first reading of the paper as it is not strictly needed for the following sections.
¡¡consider again the cnf  f = va ¡Ä  vb   vd  ¡Ä  vc   vb  from the previous section  and the mlf f = a + ad + abd + abcd that it encodes. we now briefly describe the cnf factoring process which allows us to produce the ac shown in figure 1. first  the output of the factoring process is shown on the left of figure 1: it is a logical form known as negation normal form  nnf  which satisfies decomposability  conjunctions do not share variables   determinism  disjuncts must be logically incompatible   and smoothness  disjuncts must mention the same sets of variables . such a factorization is generated using an exhaustive version of the dpll procedure  davis et al.  1 . in particular  the algorithm will pick a variable x in the cnf  will factor  |x and  | x separately  and then combine the results into x ¡Ä factor  |x  ¡Å  x ¡Ä factor  | x . to improve performance  the algorithm keeps a cache that stores cnfs that have been factored and their factorizations and checks this cache before trying to factor a cnf ¦£. finally  before picking a variable x to split ¦£ on  the algorithm checks the cnf to see if it can be broken into disconnected components  say ¦Á1 and ¦Á1. in that case  the algorithm factors the components separately and combines their results into factor ¦Á1  ¡Ä factor ¦Á1 . the factoring algorithm we use  darwiche  1   utilizes a decomposition tree  dtree  to manage this decomposition process. in particular  a dtree for a cnf is a binary tree whose leaves correspond to the cnf clauses. moreover  each node in the dtree is associated with a set of variables whose instantiation is guaranteed to decompose the cnf into two independent components.
¡¡the above procedure generates nnfs that are decomposable and deterministic. smoothness can be established easily by a postprocessing step. given an nnf that satisfies the required properties  we can extract an ac by simply replacing conjunctions with multiplications  disjunctions with additions  and negative literals with the constant 1. positive literals are replaced by the real-valued variables they encode. this decoding process is shown in figure 1; see  darwiche  1  for more details.
table 1: the networks with which we experimented.
networkmax clustvarscardave cardtotal parmsmax cpt parmsave cpt parms%det%dpalarm11-1.1111bm11-1.1111diabetes11-1.1111hailfinder11-1.1111mildew11-1.1111mm11-1.1111munin1.11111.1.1munin1.11111.1.1munin1.11111.1.1munin1.11111.1.1pathfinder11-1.1111pigs11-1.1111students11-1.1111tcc1f11-1.1111water11-1.1111
figure 1: an nnf  left   its encoded ac  middle  and a simplification of the ac  right .¡¡this factoring algorithm is indeed a logical version of the recursive conditioning  rc  algorithm  darwiche  1b .1one can in principle use rc to compile a bayesian network directly into an ac  bypassing the cnf encoding   but the approach we use allows us to capitalize on the state of the art in logical reasoning for handling determinism and csi  even though it may incur more overhead in the factoring process. in particular  our cnf factoring algorithm uses unit resolution to propagate logical constraints; conflict directed backtracking to recover more efficiently when conditioning on variable settings that lead to contradictions; in addition to clause learning as a means for avoiding such contradictions early on in future conditioning. it also provides a more flexible framework for exploiting csi through the use of two techniques. first  it can detect non-structural decompositions  that is  subproblems that become independent due to conditioning on specific variable values-such independence cannot be detected based only on structural considerations  network topology . this is done by removing clauses that become subsumed due to conditioning  therefore  disconnecting subsets of the cnf under specific variable values. second  it uses a non-structural caching scheme which allows one to prove the equivalence of subproblems under specific variable values  therefore  avoiding multiple factorings of the same subproblem . again  these equivalences cannot be proven if we were to only use structural considerations. finally  the re-

tic inference using #sat  bacchus et al.  1 .
duction to cnf allows one to more naturally accommodate other types of cpt structures  e.g.  decision trees and rules  without the need for algorithmic change. the reduction to cnf factoring and the corresponding techniques lead to quite a bit of overhead in some cases  but this is justified since it will only be done once when compiling the network into an ac. if the application of these techniques lead to smaller acs  the compile time can then be amortized over all online queries. we will illustrate this benefit more concretely in the experimental results section.
1 encoding parameter equality / csi
the cpt depicted in figure 1 has 1 parameters  yet only 1 of these are distinct. some of the equalities among parameters imply context-specific independence; others do not. for example  the equality between parameters in rows 1   1 with those in rows 1 imply that c is independent of a given b = b1. however  the equality between parameters in rows 1 and 1 do not imply a csi.
¡¡from a purely encoding viewpoint  one would clearly want to exploit parameter equality  at least to reduce the number of boolean variables one must generate. table 1 shows the extent to which parameter equality can help in this regard. in particular  the table reports as %dp the percentage of distinct parameters among non-extreme parameters. that is  the percentage of parameters that would remain if  for each cpt  we collapsed equal  non-extreme parameters into a single parameter. the dramatic example here is pathfinder: about half of its parameters are extreme  and among the other half  only about 1% are distinct within their cpts. in addition to generating smaller cnfs  encoding parameter equality allows the compiler to run with less overhead  and to generate smaller acs since parameter equality provides more opportunities for factoring  which immediately translates to gains in online inference.
¡¡a key observation here is that no two parameters in the same cpt can ever appear in the same mlf term  as they correspond to incompatible network instantiations. this observation suggests that we can use the same boolean variable to represent multiple parameters  assuming that such parameters have equal values and appear in the same cpt. however  the idea will not work when applied to prev. consider again the cpt in figure 1. if we use the same variable ¦È to represent parameters ¦Èc1|a1 b1 and ¦Èc1|a1 b1  which are both equal to .1  we would get the following two pi clauses in the cnf: ¦È   ¦Ëb1 and ¦È   ¦Ëb1  which is inconsistent with other clauses. more generally  pi clauses assert that a parameter implies the corresponding family instantiation. therefore  if we simply use the same boolean variable to represent equal parameters  we would be implying inconsistent family instan-
tiations.1
¡¡the solution we adopt is to drop pi clauses from the encoding! note that dropping pi clauses introduces additional  unintended  models into the cnf  allowing mlf terms which contain multiple parameters from the same cpt. these unintended models/terms  however  can be easily filtered during the decoding process given the following.
theorem1 consider a bayesian network with n variables. let   be a cnf encoding which includes the indicator  ip and pi clause sets  and let ¦£ be the cnf encoding which includes the indicator and ip clause sets only. then  's models have cardinality 1n and are a subset of ¦£'s models. moreover  if ¦Ò is a model of ¦£ but not    then ¦Ò's cardinality is   1n.
therefore  unintended models have a higher cardinality than original models  which all have the same cardinality . as it turns out  if ¦£ is an nnf which satisfies decomposability  determinism and smoothness  one can in linear time obtain another nnf ¦£1 whose models are exactly the minimum cardinality models of ¦£ and which satisfies the required properties  darwiche  1a . therefore  we can safely drop pi clauses as long we minimize the resulting nnf before we decode the ac.
¡¡by including only indicator and ip clauses  we can now safely represent all equal parameters within the same cpt by a single boolean variable in the cnf encoding. for the pathfinder network for example  this drops the number of boolean variables needed to represent non-extreme parameters from 1 to 1  a 1% reduction! similar reductions are obtained for many other networks; see table 1. as we show later  not only does this technique improve the compilation time  but can lead to significantly smaller acs.
1 a more informed factoring algorithm
the cnf factoring algorithm employs two key techniques as discussed earlier. the first is variable splitting  which can be thought of as doing case analysis. the second is caching  so that one can avoid factoring the same cnf subset multiple times. which variables the algorithm ends up splitting on can very much affect its running time  and the size of factorizations it generates. moreover  the complexity of the caching scheme is proportional to the number of variables appearing in the cached cnf subset  as the state of such variables are used to generate keys that uniquely define cnfs. the following observations state interesting properties of our cnf encodings  which if passed to the factoring algorithm can significantly improve both the splitting and caching processes.
¡¡first  if two clauses share a parameter variable  then they must also share indicators over the same network variable. this property  and the presence of indicator clauses  allow the cnf factoring algorithm to restrict its splitting to indicator variables  which would be sufficient to decompose the problem into independent components  hence  no splitting/case analysis is needed on parameter variables . second  given the structure of indicator and ip clauses  the state of indicator variables are sufficient to characterize the state of parameter variables. this property allows us to only involve indicator variables when generating cnf keys during the caching process. both of the above optimizations can be exploited by simply identifying parameter variables to the factoring algorithm.
¡¡another technique that we have used in some of the experiments involves the construction of a decomposition tree  dtree  for the given bayesian network  and then converting it into a dtree for its cnf encoding. a dtree for a bayesian network is simply a binary tree whose leaves correspond to the network cpts  darwiche  1b . a dtree for a cnf is also a binary tree  but its leaves correspond to the cnf clauses. since each clause in the cnf encoding is generated by a cpt  we can convert a network dtree into a cnf dtree by simply unfolding the dtree node corresponding to a cpt into a subtree whose leaves correspond to the clauses generated by that cpt. the main point of this technique is to more efficiently generate dtrees for very large cnf encodings that are generated by bayesian networks with a small number of cpts  this happens when the network contains very large cpts .
1 other optimizations
our cnf encodings utilize some additional enhancements  two of which are described next. first  we define a new type of clause  called an eclause  which has the same syntax as a regular clause but stronger semantics: it asserts that exactly one of it literals is true. we use eclauses for representing indicator clauses  therefore reducing the size of cnfs considerably in networks having multi-valued variables. moreover  we outfit the dpll procedure used in factoring the cnf to work directly with eclauses  without having to unfold them into regular clauses. for another optimization example  the indicators and parameters corresponding to the same state of a root variable are logically equivalent  making it possible to delete the parameter variables and the corresponding ip and pi clauses  which establish the equivalence.
1 experimental results
experiments ran on a 1ghz pentium m with 1gb of ram  using the networks in table 1. two important columns in table 1 are %det  which is the percent of parameters that are equal to 1 or to 1 and %dp  which is the percent of non- extreme parameters remaining after collapsing equal parameters within the same cpt. these two values give an idea of the amounts of local structure in the form of determinism and possibly csi. bm-1  mm-1-1  and stud-1 are networks on which prev was already shown to perform well by only encoding determinism  chavira et al.  1 . these networks contain only binary variables  are highly deterministic  and have small cpts  no more than two parents per node . in contrast  the other networks contain variables with higher cardinalities and lesser degrees of determinism and  sometimes  very large cpts. these networks came from various sources: http://www1.sis.pitt.edu/¡«genie; hughes research labs; and http://www.cs.huji.ac.il/labs/compbio/repository.
¡¡the experiments serve to demonstrate three points. first  the new cnf encoding and the additional information we pass to the cnf factoring algorithm lead to significant improvement  both in the factoring time and the size of resulting factorizations. table 1  1st three columns  illustrates the improvement in factoring time  showing order of magnitude improvements in some cases and allowing us to factor some networks for the first time under the given memory constraints.
¡¡the second point illustrated by our experiments concerns the quality of factorizations  acs  we obtain  compared to the ones embedded in jointrees. recall that every jointree embeds an ac  park and darwiche  1   whose size depends only on the jointree size and  hence  is not dependent on local structure. our experiments are therefore set to illustrate the extent to which local structure can help the factorization process. table 1 illustrates significant improvements in ac size  as we obtained one to two orders of magnitude improvement on networks such as water  pathfinder  munin1  and munin1. for a more direct measure of improvement in online inference 1 we generated sixteen random sets of evidence  and for each evidence set  we computed the marginal of each network variable using jointree propagation and then using ac evaluation/differentiation.1 table 1 shows the obtained improvements in online inference  which are similar to improvements in ac size.
¡¡note that since the ac is compiled independently of evidence  the improvements apply for computing all marginals regardless of evidence. this is especially useful for tasks that apply a massive number of queries to bayesian networks  including parameter estimation algorithms  e.g.  em  given known local structure in the form of determinism and csi  and map algorithms based on branch&bound search.
¡¡ poole and zhang  1  present another approach for exploiting local structure  by providing a refinement on variable elimination  ve . their approach does not involve a compitable 1: effect of local structure on ac size  edge count .
local structurepathfinderwatermunin1det/equal parms111 1det only111 1equal parms only11 1 1no local structure11 1-jointree11 1 1lation step  and their results are sensitive to the given queries  so a direct comparison between the two approaches is not too meaningful. yet  we mention here that on a re-parameterized version of water  to introduce local structure    poole and zhang  1  show a factor of 1 improvement over standard ve  total time over all queries; the speedup was more or less depending on the query . water is the only network from table 1 that  poole and zhang  1  report on. note that the exploitation of local structure can incur overhead that may not be justifiable unless the savings due to local structure are significant enough  not all of the cases in table 1 lead to significant savings . this appears to be less an of an issue in our approach  since the overhead is pushed into the compilation step. however  in  poole and zhang  1  which does not involve a compilation step  this overhead is incurred in every query which may or may not lead to overall savings depending on the query-being query specific  however  may sometimes pay off quite significantly  since work performed can sometimes be simplified given specific evidence and specific query variables.
¡¡our final point regards the effect of local structure on the quality of factorizations. consider table 1  which shows ac sizes under different encodings of local structure. first  it is obvious that encoding local structure is responsible for the significant improvements on these networks. second  in water  determinism appears to be the main responsible factor. however  in pathfinder and munin1  parameter equality alone  without determinism  is sufficient to bring about most of the reported improvements  even though determinism alone can have a similar effect too. note that there is some overlap between determinism and parameter equality since by encoding determinism  1 parameters   one is effectively collapsing all 1 parameters  applying implicit parameter equality . the results for pathfinder and munin1 are therefore not surprising  suggesting possibly that parameter equality is more fundamental than determinism for these networks.
1 conclusion
we considered the problem of compiling bayesian networks into acs. our aim was to efficiently compile networks having local structure  yet large cpts and no excessive determinism. we proposed a new encoding scheme that facilitates the rep-
resentation of local structure in the form of parameter equality  and identified some of its properties that improve compile time. our results demonstrate significant improvements in both offline/compile and online/inference time  leading to orders of magnitude improvement in online inference.
table 1: comparing the new and prev encodings with the jointree baseline.
networkoffline compile time  s 
ac edge count
online inference time  s 
prevnewimprov.jointreenewimprov.jointreenewimprov.alarm111111111bm-11111 1 1 1111.1diabetes-1.1-1 1 11111hailfinder111111111mildew-1.1-1 1 11111mm-1-1.1.1.1 111111.1munin1-1.1-1 11 1.1 111munin1 1111 1 11111munin1 1111 1 11111munin1 1111 1 11111pathfinder111111111pigs1111 1 11111students-11111 1 1 1111tcc1f.obfuscated111111111water1111 1 1.1.1.1.1acknowledgments
we thank the reviewers for commenting on an earlier version of this paper. this work has been partially supported by nsf grant iis-1 and muri grant n1-1.
