* 
we characterize and illustrate evidential probability - an interval-valued measure of uncertainty that applies to sentences. evidential probability is also taken to be relative to a body of knowledge or a database representing the data on which judgments of uncertainty are to be made. every statement is equivalent to some statement to which statistics in the database are relevant. the problem is to find the correct statement; this is the problem of finding the correct reference class for a statement. a set of rules for determining the correct reference class is offered. 
1 	kinds of probability 
most scientists feel most comfortable with some kind of frequency or empirical interpretation of probability  wilson  1. it is not merely that this way of looking at probability seems to have a comfortable history in casinos  but that it is what physicists employ in their theoretical work in quantum mechanics as well as in statistical mechanics. it is also what is natural to have in mind in dealing with  random error  in measurement. indeed  it is the basic idea that is called on when we need to go from the  random errors  in a number of individual measurements of different quantities  to the  inferred error  in a quantity that is taken as a function of those measured quantities. this is a relatively uncontroversial framework for  combining  probabilities. 
　　　　in designing expert systems  however  what we have to work with are often the mere opinions of experts. it is said that  we have no statistics.   mccarthy and hayes  1  every event that we need to concern ourselves with  after all  is  unique.   that's the nature of events!  and there is a well developed interpretation of probability as subjective: the view developed by l. j. savage 1   f. p. ramsey   and bruno de finetti . this approach to probability has been widely adopted in business schools  and is accepted by a significant minority of statisticians  raiffa and schlaifer  1 . 
prima facie  the problem with subjective 
　research contributing to this paper has been supported by the signals warfare center of the u. s. army. 
1 	qualitative reasoning 
probability is that t is subjective; the founding fathers just cited make crystal clear that that is exactly what they intended: one man's coherent opinion is  formally  as good as another's. ramsey was selfconsciously clear about this:  ia man's  original expectations may within the limits of consistency be any he likes...   p. 1  
　　　　in contrast to both of these views  evidential probability both applies to unique events  and is solidly based on statistical evidence. this is not a new interpretation; it is a philosophical interpretation that has been around for a number of years as  epistemological probability.  the basic idea appeared in  kyburg  1 . the following sections will  a  detail the difficulties of applying probabilities  construed as empirical frequencies  to specific situations   b  illustrate further difficulties with  strictly  subjective probability   c  illustrate the application of evidential probability in simple contexts  and  d  argue that the scope of evidential probability is in fact wider than one might at first suppose. we will  finally   e   describe the relation between evidential probability and belief functions  subjective probability  and frequency probability. 
1 	frequencies are no help. 
consider a circumstance in which we ordinarily take ourselves to have unbounded and dependable statistical information: a well run casino. we suppose ourselves to know  with very little error  the relative frequency with which a die yields six: 1. in placing a bet  however  we are concerned with a much more specific situation than  a roll of the die.  it is a roll at casino x. but we don't have any statistics about casino x - only vaguely about dice in general. it is a roll by person y; but we certainly don't have any statistical information about his frequency of sevens. and it is a roll made at 1 a m   on date d. if we knew the relative frequency of sevens in that set of rolls of cardinality one  we'd be in good shape; but then we would have no call to use probability. 
　　　　of course  what we do is to suppose that the general frequency can be applied to the specific instance. in this kind of case that makes good sense. there are only two circumstances in which one can 
 ordinarily  be with regard to the roll made at 1 am on date d. either one knows how it came out  six or not  or one knows nothing that would interfere 
　　　　
with the use of the long run general frequency. 
there are other situations one can consider. 
thus  the following day  i and my fellow croupiers  having access both to the summary statistics of the previous night's outcomes  and the detailed histories  may bet on various rolls. thus we may bet on whether or not the roll made at 1 am on date d came out six. knowing that only 1 of the rolls at that table on date d came out six  we will bet at odds of 1 rather than odds of 1. but this is clearly an abnormal situation. 
　　　　perhaps  as scientists  we would prefer to deal with something more important and less hypothetical  such as the distribution of errors of measurement. but even in such familiar contexts identifying probability and frequency is not quite right.  which frequency   it is one thing  in a general context  to identify the probability that a measurement will be in error by three standard deviations with the long-run frequency of an error of that magnitude. it is quite another to attempt to identify the probability that a specific measurement was in error by that amount. that measurement either was or was not in error by three standard deviations: the frequency is 1 or 1. 
　　　　if we impute to the instance the general frequency  we had better take into account what we know of the magnitude measured  the results of other measurements of that magnitude  the results of other applications of the measuring device  the results of calibration  perhaps even the  personal equation   discovered in 1'th century astronomy . if a measurement yields the number 1  then  under some circumstances the probability that the magnitude being measured lies between 1 and 1 is 1  but suppose that measurement is known to be one of eight measurements  the average of which is 1. then clearly we would not want to say that the probability that the magnitude being measured lies between 1 and 1 is 1. something more than known frequencies must be invoked. inference from the performance of measurements is non-monotonic. 
　　　　the defect of frequencies  considered by themselves  for dealing with uncertainty is even more clearly brought out by the circumstances of a hypothetical insurance company. insurance companies collect enormous amounts of data. suppose we want to set a fair premium for a term life insurance policy for mr. y. mr. y is a white american male of age 1  and we have statistics on the mortality of white american males of age 1. 
　　　　but we give mr. y a questionnaire  and discover that both of his parents  and one of his sisters  died of heart disease at a very early age. clearly this puts him in a special group: those who have a family history of heart disease; and we have statistics for mortality among that group. 
　　　　on the other hand  he is a professor  and our data regarding professors suggests that they tend to suffer less from heart disease than most people. 
　　　　and so on. the problem is not that  once having decided to base our uncertainties on statistics  there aren't enough statistics  but that there are too many. mr. y belongs to many classes concerning which we can reasonably assume ourselves to have statistical knowledge  and they all have differing and conflicting inferred relative frequencies of death. 
　　　　ultimately  of course  mr. y is unique: the intersection of the classes to which he belongs  about which we may even have historical data  and be in a position to make statistical inferences  is occupied by only one person  mr. y  and the incidence of mortality over the next year is either 1 or 1. 
　　　　the philosopher hans reichenbach explored this problem long ago  reichenbach  1. his recommendation was to adopt as a reference class determining one's assessment of likelihood the  narrowest  reference class about which one has  adequate  statistics. but this recommendation suffers from a number of defects. we may have statistical information about professors and about people with a 
　　　　certain kind of family history  without having any statistical information about professors who have that kind of family history. and there are clearly other considerations than  narrowness  that can be raised. 
1 	a judgment call  
in the final analysis  it is perhaps just a judgement call  we'll be told.  see  for example  cheeseman  .  why should this be unsatisfactory  because it leaves us with no way of adjudicating conflicting judgments. maybe there is no way of settling such differences  perhaps it is as ramsey suggested long ago. but if an agent's original expectations may be any he likes  so  even if he updates his beliefs by conditionalization  may his final expectations be anything at all. 
　　　　of course  people do tend to agree. they tend to agree about the likelihood of a seven on the roll of a die made at 1 am on date d. is this agreement just a reflection of good will and social skills  or is it a result of intimidation  or of a failure to reflect  the trouble with conventions is not that they are all wrong  but there is no basis for evaluating them  for weighing one against another. 
　　　　ah  but how about success rate  the winners must have been right! not so; perhaps the winners were merely lucky. we do not automatically say of a man who bets on the long shot for no reason  and wins  that he was  right.  the problem of evaluating uncertainty reemerges: we need to be able to assess the likelihood of the proposition that his success was reasonably to have been expected  so that we know 
	kyburg 	1 
whether to emulate him in the future. 
　　　　and then there are the related problems of consistency and updating. according to the standard subjectivist view  we are to consult our opinions  judgments  and then  as evidence comes in  modify them by conditionalization. but our initial judgments are not likely to satisfy the constraints of the probability calculus: we will have to adjust them to make them come out right. this adjustment does not take place by conditionalization. we find some judgments  about which we feel sure/' to use a phrase of savage's  1   and adjust the others to match. 
　　　　but suppose we begin with a set of judgments we think coherent  get some evidence  conditionalize  and arrive at a set of judgments we no longer find plausible. no problem. we need merely go back and  as before  on the same basis  alter our original judgments until everything comes out right. 
　　　　is this a parody  perhaps. but it is also  from the subjective point of view  a rationally permissible scenario. we look to probability to guide our degrees of belief  our assessments of uncertainty; and what happens is that we end up using our intuitive assessments of likelihood and plausibility to control our imputations of probability. 
1 	evidential probability. 
it is precisely this tension between what we know of statistics - a fair amount  by any non-skeptical standard - and the need to adjudicate among known statistics in order to achieve an assessment of the particular case that confronts us that constitutes our problem. 
　　　　as we have seen  the problem with making frequencies do the work is that there are generally too many. the problem is not a lack of frequency information  but rather a surfeit of information  not merely a surfeit of observed data  but a surfeit of the information we can infer from it! . this becomes even more apparent when we note that every probabilistic measure of uncertainty has the property that if two statements  s and t  are known even to have the same truth value  and a fortiori if they are logically equivalent   they must be assigned the same probability. 

　　　　thus the probability of heads on the next toss of this coin  if it is to be based on everything we know about this coin  must reflect the knowledge we have about coins in general  about physical systems of a certain character  about 1 coins  about 1 quar-
1 	qualitative reasoning 
ters  about 1 quarters being tossed in 1  etc. that is: we know that the next toss of a  spatio-temporally identified  1 quarter lands heads if and only if the next toss of a  spatio-temporally identified  coin lands heads if and only if the next toss of this coin i hold now in my hand lands heads. i have little direct statistical knowledge about this coin  but i have a lot of knowledge about coins in general  both folk knowledge and the results of experiments  and  on a theoretical basis  about physical systems in which a small perturbation can lead to a qualitative difference. 
it follows from this principle of equivalence 
 sentences known to have the same truth value should have the same probability  combined with the principle of total evidence  we should take account of all the evidence we have   that if we know  of a certain object  that it belongs to a collection c of classes  the probability that it has a certain property should reflect what we know about the incidence of that property in those classes. 
1 extended example. 
here is a table that might have resulted from a survey of beef cattle reporting whether they develop founder given  a  worming medicine   b  implant   c  louse powder   d  soybean supplement  or  e  corn supplement  or any combination of these. the table does not give information regarding the effect of the lack of these features: thus abcd represents both those falling in class abcd who are given corn supplement  and those falling in that class who are not given corn supplement. 
　　　　the last two columns give .1 confidence level intervals for founder  based on the evidence given in the table  using standard forms of statistical inference. 
n founder fl fu u 1 1 1 1 a 1 1 1 1 b 1 1 1 1 c 1 1 1 1 d 1 1 1 1 e 1 1 1 1 ab 1 1 1 1 ac 1 1 1 1 ad 1 1 1 1 ae 1 1 1 1 bc 1 1 1 1 bd 1 1 1 1 be 1 1 1 1 cd 1 1 1 1 ce 1 1 1 1 de 1 1 1 1 abc 1 1 1 1 　　　　
abd 1 1 1 1 abe 1 1 1 1 acd 1 1 1 1 ace 1 1 1 1 ade 1 1 1 1 bcd 1 1 1 1 bce 1 1 1 1 bde 1 1 1 1 cde 1 1 1 1 abcd 1 1 1 1 abce 1 1 1 1 bcde 1 1 1 1 aede 1 1 1 1 abde 1 1 1 1 abcde 1 1 1 1 1 c o m b i n i n g e v i d e n c e . 
suppose cow c1 got treatments b  c  d  e.. we have no data about such cows. but she also got all but one of the four treatments - i.e.  bed  bce  etc. in particular  she is cde and also bee. but these two possible reference classes disagree in the sense that neither simply overlaps the other. put otherwise  they disagree in the sense that there is a frequency or chance that is compatible with one and not compatible with the other. the frequency in cde is in  .1   and the frequency in bee is in  .1 -1 . nothing seems to indicate any way of reconciling or combining these two possible reference classes - in particular  we have no data on their intersection. should we give up and say that the interval  1  characterizes the uncertainty to be attached to this cow  
　　　　there are two natural alternatives. one corresponds to the dempster/shafer updating procedure 
 shafer  1 . this is to look at the set of pairs  such that x belongs to cde and y belongs to bce. now the cow in question corresponds also to a pair of the form we don't know the frequency of success in the diagonal  but we do know one thing about the pair  that it either belongs to the set of pairs both of which founder  or to the set of pairs both of which do not founder. this leads us to a subset of the set of pairs in general - namely those that agree with respect to success. and we can calculate bounds on the relative frequency of success in this subset: they are given exactly in accordance with the dempster/shafer formula for combining evidence: 
 this construction for combin-
ing evidence has been suggested by ron loui . 
　　　　but faced with this concrete example  it does not seem so plausible.1 suppose that we know that a 
1
  while we don't know the relevant frequency among instances of the form  in general we allow ourselves to use frequencies known to apply to broader is in r 1 and also in r1  and that ' and 
                 these intervals conflict. but consider as a member of  it must belong to that part of that is included in the union of t x t and . it belongs to  just in case a belongs to t; and the frequency of this kind of occurrence is in  by the preceding formula.1 in the particular case at hand  the resulting interval is  .1 .1 . but this seems counterintuitive  since it is less than the frequency in either of the two classes whose information we have combined. 
　　　　an alternative approach would be to argue thus: we find ourselves in a conflict situation because we have evidence that supports two intervals that disagree. but the same evidence supports  even more strongly  two intervals that do not disagree. namely: the intervals bounded by the lower of the two lower bounds  and the higher of the two upper bounds. 
　　　　the proportion of foundered cows in the class bce is known to be between .1 and .1  and therefore obviously between .1 and .1 and the proportion of foundered cows in the class cde is known to be between .1 and .1  and therefore also between .1 and .1. these limits seem intuitively to be reasonable limits for c1 - the probability that she will founder is  .1 . this is a broad limit; one that in the face of conflicting evidence does not seem unreasonable. 
　　　　how about the other classes that c1 is known to belong to: bc  bd  etc.  each of them conflicts with one or another of the more specific classes; but such a conflict is most reasonably resolved by ignoring the less specific class. 
1 	precision 
contrast cow c1- she is known to be an abe; therefore she is also known to be an a a b an e  an ab  a be  and an ae. these items of statistical knowledge yield: 
abc .1 .1 ab .1 .1 ac .1 .1 classes as the basis for the probability applicable to an instance in a narrower class  else we would be stuck with the  1  characteristic of a unit class. 
1
  it might be thought that the process could be iterated  and that it would lead to intervals that migrate toward the extremes of 1 and 1. this is not so  since the  higher dimensionality  constraint in the full system prevents the components from  disagreeing  with the cross product subset  kyburg  
1 1 . nevertheless  as bulent murtzaoglu  has pointed out  the results are counterintuitive even without iteration. 
	kyburg 	1 
　　　　
bc .1 .1 a .1 .1 b .1 .1 c .1 .1 u .1 .1 as the intervals in which the frequency of founder might lie. although abc is the  smallest  class in which c1 is known to be  our knowledge about that class does not conflict with knowledge about any of the superclasses in which it is known to be included; it is merely less precise. if we look at the superclasses  the one about which we have the most information is ab; this does not conflict with any of the others  and thus we use ab as the reference class and take the probability to be 1 .1 . 1 
　　　　consider c1 who is known to belong to ade  in which the relative frequency is in  .1 . the 
corresponding table is: 
ade .1 .1 ad .1 .1 ae .1 .1 de .1 .1 a .1 .1 d .1 .1 e .1 .1 u .1 .1 here  although ad contains more precise information  and does not conflict with ade  it does conflict with ae and de  who have an equal right to be heard. since the subsets ad  ae  and de also conflict with each other  and none of them conflicts with our knowledge about ade  neither does their cover conflict with our knowledge about ade. all three of these classes are known to exhibit a frequency of founder in the interval  .1 . this is more precise knowledge  and should be used to determine the probability. 
　　　　finally  suppose c1 is known to belong to abd the table is: 
abd .1 .1 ab .1 .1 ad .1 .1 bd .1 .1 1
  if there was conflict between ab and another superclass two cases could arise. first  there is conflict also with the original subclass. then that subclass would prevail. second  neither of the superclasses conflicts with the subclass. then the conflict between the superclasses can be resolved by a cover  and since the cover will not conflict with the subclass either  it can plausibly be taken to determine the probability. 
1 	qualitative reasoning 
a 	.1 	.1 b 	.1 	.1 .1 	.1 
u 	.1 	.1 
here  although again more is known about ab than about abd  and there is no conflict  the superset bd conflicts with the more specific set  and therefore also with ab   and so we take the probability to be determined by our knowledge about abd:  .1 -1 . 
　　　　the principle we have been following so far is to reject a candidate reference class whenever the relevant frequency in it  or in any other candidate reference class of equal specificity  differs from that in the most specific candidate reference class  where  differs  means that neither interval is included in the other. the result will be a set of intervals  none of which differ from that of the most specific class. we use the most informative interval for our proba-
bility. 1 
　　　　note that we are taking the probability to be the interval  not that it is  included i n   the interval. there may be no identifiable class that contains a more precisely specified proportion than the one we are working with. 
1 c o l l e c t i o n s of h e r d s : b a y e s i a n inference. suppose that we have obtained data on the relation between the five factors we are considering and founder  not merely from one herd  but from several. the following table gives relevant data concerning four herds. the herds quite clearly differ in the effect of the various factors. this might be due to genetic differences among the breeding lines that dominate the various herds  or it might be due to environmental factors. the upper and lower frequencies  as before  reflect conservative confidence limits at the 
1 confidence level. 
herd 1 	herd 1 	1 fl 	fu 	fl 	fa 
u 1 1 1 1 b 1 1 1 1 c 1 1 1 1 ab 1 1 1 1 bc 1 1 1 1 
herd 1 herd 1 all 1 1 fl fu fl fu fl fu 
u 	1 	1 	1 	1 	1 	1 b 	1 	1 	1 	1 	1 	1 c 	1 	1 	1 	1 	1 	1 
　　　　
ab 1 1 1 1 1 be 1 1.1 1 1 
　　　　what is the chance of cow c$   who belongs to one of these herds and has received treatments b and c coming down with founder  you may ask   which herd does she belong to   but we are assuming that we don't know: all we know is that she belongs to one of these four herds. like the preceding case  in which we did not know which treatments a cow did not get  we are here dealing with a situation of limited knowledge. 
the candidate classes are u  b  c  and bc: b 	.1 	.1 c 	.1 	.1 bc 	.1 	.1 u 	.1 	.1 
the intuitively most plausible reference class is bc  even though our knowledge about be is much less precise than our knowledge about b and about c. 
　　　　n o w let us suppose that cow c1 is selected from these herds by an agricultural agent who first chooses a herd at random to sample from  and then chooses a cow from the herd he has chosen. 
　　　　we have a new variable to take account of: herd number. the herd is chosen at random  which is to say that in the long run  each herd will be selected with equal frequency - i.e.  1. we know how to calculate the probability: it is the average of the frequencies in the herds. the probability that c1 suffers from founder is  1 -1 . 
　　　　what is the reference class that gives rise to this interval  it is perfectly straight-forward: it is the set of pairs  x y  such that x is a herd  and y is a cow from that herd; the target set  of course  is the set of such pairs of which the second member founders. let us call the reference set just described b  and the target set s. 
　　　　while it is easy to agree to that probability interval  it is less easy to see why the former answer is wrong. the cow c1 is  now as before  a member of the total collection of cows about which we have data. what prevents that from being a correct reference class  as it was in the previous case  
　　　　the principle required - from which the specificity principle follows as a special case - is what we shall call the bayesian principle. suppose you have a reference set of the form described  and that it leads to an interval that conflicts with an interval obtained from some other reference set for the same proposition  for example  the reference set consisting of all the cows in all four herds. we can get from this reference set to the correct one as follows: note that the correct reference set makes use of a set of predicates  forming a partition  that does not appear in the first reference set. form the cross product of the partition and the original set.  this is the set of all pairs consisting of a herd and a cow from the collection of four herds.  then note that the intuitively obvious reference set is a subset of that set of pairs - the subset in which the cow  second member of the pair  is a member of the herd  the first member of the pair . 
　　　　this principle implies the specificity principle because if we use a vacuous partition  we will obtain the simple subset rule. 
　　　　in general  when a bayesian principle is appropriate  it is because it is appropriate for the reference set to be construed as resulting from a two  or multi  stage process. what can defend the probability that results from that process is precisely the construction just described. 
1 	sizes of samples: statistical inference 
suppose we want to know how many cows in the class ab will founder in the long run. we have a lot of evidence. in particular  we have the evidence obtained from herd 1: of 1 cows known to belong to ab  1 came down with founder. it follows  at a confidence level of .1  that between .1 and .1 of cows belonging to ab will founder. 
　　　　we also have the evidence obtained from all four herds: of 1 animals having those treatments  1 foundered  yielding an observed frequency of .1  and a 1% confidence interval of 
1 1 
　　　　the first inference gives us a perfectly proper inference for inferring the proportion of cows falling in the class ab who founder. if that was the only herd we had investigated  we would indeed be entitled to take that as a general conclusion about cows in general. in the presence of the broader sample  of course  it is no longer appropriate. why not  
　　　　because it does not take into account all the evidence we have about cows and foundering. if what we are interested in is the proportion of cows in general that founder among those who fall into the class in question  then we should take into account all we know of cows falling into that class. 
　　　　but that may not be what we are interested in. though we have more information about cows in general than we have about cows from particular herds  it may be that we always know what herd a cow comes from. if that's the case  it will be readily seen from what we have said before that what is of interest to us will be the proportion of cows that founder within the smaller class: ab and in herd 1; ab and in herd 1  etc. thus the first inference may be perfectly all right  so long as the class about which we are making the inference is ab and herd 1 rather than ab in general. 
　　　　in short  what we need to have justified are all the inferences we can legitimately make. which 
	kyburg 	1 
　　　　
ones we should make may be a function of our particular situation. 
1 	c o n c l u s i o n s * 
if we identify probability with empirical frequency  the probabilities of specific eventualities are either unknown  or are 1 or 1  since those are the only frequencies that can occur in a single instance. the frequency approach can be construed less literally  but then we must devise a way of combining all the evidence we have bearing on a specific eventuality. frequency approaches to probability provide no machinery for combining evidence  or for choosing among reference classes. 
　　　　if we identify probability with mere subjective opinion the connection with empirical data is equally unanalyzed. conditionalization is a consistent way of updating opinions  but since our opinions are uncertain to start with  the results of conditionalization can be taken to cast doubt on what we took to be our initial opinions. 
　　　　the evidential approach to probability attempts to formalize the evidential relation between general knowledge of frequencies and specific individual events. to do this requires first that the description of the individual event be related to a class about which we have statistical knowledge. in general there will be more than one way of doing this. 
　　　　second  we need some way of adjudicating conflicts  if any  among the statistics corresponding to the various ways of relating the event in question  under any acceptable description  to various possible reference classes. we considered three kinds of conflict: theser are discussed in more detail in  kyburg  1  and in  kyburg  1 . 
　　　　third  there are two kinds of conflicting statistics: since in general our statistical knowledge related to an individual event will be in the form of intervals  we will encounter cases in which one interval is included in another. if there is no other conflict  it makes sense to use the tightest interval we can justify. but it is also possible that the two intervals will be disjoint  or overlap without inclusion. this is the difficult case. 
　　　　evidential probability provides machinery for dealing with the individual instance that concerns us  employing all the statistical evidence we have bearing on that instance. it thus serves where subjective views would use mere opinion. it also takes every probability to be based on general statistical knowledge  thus satisfying one of the intuitive desiderata for which the frequency theories were designed. and in the world of expert knowledge  we may often  always   assume that the opinions of the expert are  or should have been  or could have been  based ultimately on knowledge of frequencies. 
1 	qualitative reasoning 
 cheeseman  1  peter cheeseman. inquiry into computer understanding. computational intelligence 1 : 1  1. 
 de finetti  1  bruno de finett. la prevision: ses lois logiques  ses sources sujectives. annales de l'institute henri poincare  1. 
 kyburg  1  henry e. kyburg  jr. probability and the logic of rational belief. wesleyan university press  middletown  connecticut  1. 
 kyburg  1  henry e. kyburg  jr. the logical foundations of statistical inference. reidel  dordrecht  1. 
 kyburg 1  henry e. kyburg  jr. the reference class. philosophy of science 1  pp. 1  1. 
 kyburg  1   henry e. kyburg jr. uncertainty and the conditioning of beliefs. in george m. von furstenberg ed. acting under uncertainty: 
multidisciplinary 	conceptions  	kluwer  dordrecht  pages 1  1. 
 loui  1  ronald p. loui. computing reference classes proceedings of the 1 workshop on uncertainty in artificial intelligence  pages 1  1. 
 mccarthy and hayes  1  john mccarthy and pat hayes. some philosophical problems from the standpoint of artifical intelligence. in meltzer and michie  eds   machine intelligence 1  edinburgh  pp. 1. 
 murtezaoglu  1  biilent murtezaoglu. a modification to evidential probability. tr  university of rochester  1. 
 raiffa and schlaifer 1  	howard raiffa and 
robert schlaifer. applied statistical decision theory  mit press  cambridge  mass  1. 
 ramsey  1  f. p. ramsey.the foundations of mathematics and other essays  humanities press  new york  1. 
 reichenbach  1  hans reichenbach. the theory of probability university of california press  berkeley and los angles  1. 
 savage 1 l. j. savage. foundations of statistics  john wiley  new york  1. 
 shafer  1  glenn shafer. a mathematical theory of evidence  princeton university press  princeton  1. 
 wilson  1  e. bright wilson  jr.. an introduction to scientific research  mcgraw-hill  new york  1. 
1 the  most informative  interval may be an interval cover  as we have seen. 
---------------
　　　　
------------------------------------------------------------
　　　　
　　　　---------------
　　　　
　　　　------------------------------------------------------------
　　　　
