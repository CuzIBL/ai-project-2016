
the  talking robots  experiment  inspired by the  talking heads  experiment from sony  explores possibilities on how to ground symbols into perception using language  with two autonomous aibo robots in an unconstrained environment. we present here the first results of this experiment and outline in the conclusion a planned extension to social behaviors grounding.
1 introduction
the  talking robots  experiment  inspired by the  talking heads  experiment from sony  has been started for six months in our laboratory. the purpose of this experiment is double: first  to show that the original  talking heads  experiment can be reproduced in a more demanding context: autonomous robots  aibos   unconstrained vision and speech  and autonomous synchronization. second  to serve as a ground to build a more complex experiment. making a full use of the unconstrained context  we will in future work investigate how the robots could autonomously develop their own language games.
　we will present briefly in this poster the structure of the existing  talking robots  experiment and the underlying scientific questions that we address. we will also comment on our first results.
1 grounding symbols into perception
grounding symbolic representations into perception is a key and difficult issue for artificial intelligence  harnad  1; steels and baillie  1 . including social interactions and  more specifically  language acquisition and development in this context has proven to be a fruitful orientation. one of the recent and successful attempts in this direction is the  talking heads  experiment  steels and kaplan  1 . this experiment involves two cameras interacting in a simplified visual environment made of colored shapes on a white board. the agents inside the cameras are developing a shared grounded lexicon of words related to visual meanings like  red    large    above   etc.
　the  talking heads  experiment has proven that it was possible to design a computer-based mechanism for language acquisition that shares many properties in common with what can be observed in human language acquisition. it can be seen as an attempt in the direction of a computational model of language acquisition and symbol grounding  outlining possible key structural elements that might be essential for such a development.
　however  the  talking heads  had two main limitations  which were good and necessary simplifications to start with  but that we will consider carefully here:
  the environment was limited and simplified  only colored shapes on a white background  fixed cameras .
  the interaction protocol  language game  was predetermined and hardcoded.
　with the recent development of relatively cheap and powerful robotic platforms  see sony  honda or fujitsu  among others  research on symbol grounding can move from simulation or simple environments to complex natural environments and embodied systems. following this trend  we have started the  talking robots  experiment which reproduces the initial talking heads experiment with autonomous robots  aibo ers1  evolving in an unconstrained visual environment instead of simple cameras.
　the second point  regarding hardcoded interactions  is not handled in the talking robots  but constitutes our future research goal. the visual routines  robust perception mechanisms and the general experience we drew from the talking robots will be a key component and a basis for an extended experiment involving the dynamical creation of language games between two robots.
1 results
1 technical issues
the autonomous and unconstrained implementation of the language game called  guessing game  involves the following technical and practical issues:
  automatically determine the leader  the  speaker   and the follower  the  hearer   in the game: we use beep signals with specific tones to perform synchronization.
  have the two robots standing one next to the other  starting from a random position: we use movement detectors and a waving head motion to locate an aibo head in the image and pattern recognition based on a fmi-spomf transform sheng chen et al.  1  to identify the orientation of this head. together with a scale measure  this gives an accurate enough estimation of the position and orientation of the other robot.
  use a stable segmentation algorithm which will give comparable results from one robot to the other  if they look in approximately the same direction: the csc algorithm rehrmann and priese  1  has proven to be the most stable after a series of tests  the measure of stability is the entropy of a normalized correlation matrix between two segmentations of two slightly different images  which should ideally be zero .
  provide an accurate pointing device for the robots to designate the objects they are talking about: we use a blinking laser pointer fixed on the robot's head and a simple red spot detector which gives excellent results.
　putting all these elements together  we successfully obtained converging results for the  discrimination game   comparable to the original experiment  and we have run several successful  guessing games .
1 discrimination game
the discrimination game steels and kaplan  1  is played by one robot only and is designed to dynamically create a set of visual categories grounded in the surrounding visual environment of the robot: for example  with objects of different sizes in the image  a category for  big  and  small  will be developed but no category corresponding to  colors  will arise in a black and white environment. our games successfully converge with 1 iterations to a set of approximatively 1 to 1 categories in the unconstrained environment of the lab  and a success rate above 1%:

iterations
　categories are mainly developed for position and area. if we turn the vision of the robot to black and white  the discrimination tree corresponding to  saturation  shrinks in a few iterations  as expected.
1 guessing game
the  guessing game  has been run on a limited set of experiments. although we have still no statistical results yet  we have observed in detail several games to compare them to the original talking heads.
　the following figure shows the result of a guessing game played on a topic  in black  whose meaning has been misunderstood by the hearer but correctly identified after the speaker has pointed it  both robots succeed in sharing the same topic :

　this kind of failure is currently happening most of the time  and we have to improve the constancy of segmentation and context to increase the probability of having the speaker and the hearer both categorizing a topic in the same way. apart from this  the original mechanisms of the talking heads have been successfully reimplemented and run. further results will be presented in the poster.
1 conclusion
the talking robot experiment has started to successfully reproduce in a complex environment the results obtained in the simplified environment of the talking heads. building on our experience on this embedded version  we plan to start a new set of experiments involving not only a predetermined language game interaction protocol  but where the robots will dynamically create a protocol and use it to play a language game which will not be known in advance. this ambitious research project will require to state precisely the core structural elements of the talking robots and propose an architecture to generalize them and have a mechanism to let them evolve. on the practical side  we will make use of the robust vision and localization algorithms we developed for the talking robots.
