
word sense disambiguation  wsd  has been a long-standing research objective for natural language processing. in this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale wsd. under this framework  finding the right sense for a given word amounts to identifying the most  important  node among the set of graph nodes representing its senses. we propose a variety of measures that analyze the connectivity of graph structures  thereby identifying the most relevant word senses. we assess their performance on standard datasets  and show that the best measures perform comparably to state-of-the-art.
1 introduction
word sense disambiguation  wsd   the ability to identify the intended meanings of words  word senses  in context  is a central research topic in natural language processing. sense disambiguation is often characterized as an intermediate task  which is not an end in itself  but essential for many applications requiring broad-coverage language understanding. examples include machine translation  vickrey et al.  1   information retrieval  stokoe  1   question answering  ramakrishnan et al.  1   and summarisation  barzilay and elhadad  1 .
﹛recent advances in wsd have benefited greatly from the availability of corpora annotated with word senses. most accurate wsd systems to date exploit supervised methods which automatically learn cues useful for disambiguation from hand-labeled data. although supervised approaches outperform their unsupervised alternatives  see snyder and palmer  for an overview   they often require large amounts of training data to yield reliable results  yarowsky and florian  1   and their coverage is typically limited to the words for which sense labeled data exist. unfortunately  creating sense tagged corpora manually is an expensive and labor-intensive endeavor  ng  1  which must be repeated for new domains  languages  and sense inventories. given the data requirements for supervised wsd and the current paucity of suitable data for many languages and text genres  unsupervised approaches would seem to offer near-term hope for large scale sense disambiguation.
﹛most unsupervised methods can be broadly divided in two categories  namely graph-based ones and similarity-based ones. graph-based algorithms often consist of two stages  barzilay and elhadad  1; navigli and velardi  1; mihalcea  1 . first  a graph is built representing all possible interpretations of the word sequence being disambiguated. graph nodes correspond to word senses  whereas edges represent dependencies between senses  e.g.  synomymy  antonymy . next  the graph structure is assessed to determine the importance of each node. here  sense disambiguation amounts to finding the most  important  node for each word. similarity-based algorithms assign a sense to an ambiguous word by comparing each of its senses with those of the words in the surrounding context  lesk  1; mccarthy et al.  1; mohammad and hirst  1 . the sense whose definition has the highest similarity is assumed to be the correct one. the algorithms differ in the type of similarity measure they employ and the adopted definition of context which can vary from a few words to the entire corpus. in graph-based methods word senses are determined collectively by exploiting dependencies across senses  whereas in similarity-based approaches each sense is determined for each word individually without considering the senses assigned to neighboring words. experimental comparisons between the two algorithm types  mihalcea  1; brody et al.  1  indicate that graph-based algorithms outperform similarity-based ones  often by a significant margin.
﹛in this paper we focus on graph-based methods for unsupervised wsd and investigate in depth the role of graph structure in determining wsd performance. specifically  we compare and contrast various measures of graph connectivity that assess the relative importance of a node within the graph. graph theory is abundant with such measures and evaluations have been undertaken in the context of studying the structure of a hyperlinked environment  botafogo et al.  1  and within social network analysis  hage and harary  1 . our experiments attempt to establish whether some of these measures are particularly appropriate for graph-based wsd. such a comparative study is novel to our knowledge; previous work restricts itself to a single measure which is either devised specifically for wsd  barzilay and elhadad  1  or adopted from network analysis  mihalcea  1; navigli and velardi  1 . our contributionsare three-fold:a general framework for graph-based wsd; an empirical comparison of a broad range of graph connectivity measures using standard evaluation datasets; and an investigation of the influence of the sense inventory on the resulting graph structure and consequently on wsd.
﹛in the following section  we briefly introduce the graphbased wsd algorithm considered in this paper. then we present and motivate several measures of graph connectivity and explain how they are adapted to wsd. next  we describe our evaluation methodology and present our experimental results. we conclude the paper by discussing future work.
1 graph-based wsd
in order to isolate the impact of graph connectivity measures on wsd  we devised a fairly general disambiguation algorithm that has very few parameters and relies almost exclusively on graph structure for inferring word senses. in common with much current work in wsd  we are assuming that meaning distinctions are provided by a reference lexicon  which encodes for each word a discrete set of senses. although our experiments will use the wordnet sense inventory  fellbaum  1   neither our graph-based algorithm nor the proposed connectivity measures are limited to this particular lexicon. resources with alternative sense distinctions and structure could also serve as input to our method.
﹛we can view wordnet as a graph whose nodes are concepts  represented by synsets  i.e.  synonym sets   and whose edges are semantic relations between concepts  e.g.  hypernymy  meronymy . for each sentence we build a graph g =  v e   which is induced from the graph of the reference lexicon. more formally  given a sentence 考 = w1 w1 ... wn  where wi is a word  we perform the following steps to construct g:
1. initially   where senses wi  is
=1
the set of senses of wi in wordnet; in other words  v考 represents all possible interpretations of sentence 考. we set v := v考 and e :=  ;
1. for each node v ﹋ v考  we perform a depth-first search of the wordnet graph: every time we encounter a node
  along a path v ↙ v1 ↙ ﹞﹞﹞ ↙ vk ↙
	from	to	:	:=	k	and	:=	﹍
﹛﹛﹛﹛﹛﹛﹛﹛﹛. for efficiency reasons  we allow paths of limited length  ≒ 1 edges .
﹛we thus obtain a subgraph of the entire lexicon which includes vertices reasonably useful for disambiguation: each vertex is at distance ≒ 1 edges from some vertex in the original set v考 of word senses. given a sentence 考  our aim is to select for each word wi ﹋ 考 the most appropriate sense swi ﹋ senses wi . the latter is determined by ranking each vertex in the graph g accordingto its importance.in section 1 we discuss several measures that operationalize importance in graph-theoretic terms. here  we will briefly note that these measures can be either local or global. local measures capture the degree of connectivity conveyed by a single vertex in the graph towards all other vertices  whereas global measures estimate the overall degree of connectivity of the entire graph. the choice of connectivity measure influences the selection process for the highest-ranked sense. given a local measure l  and the set of vertices v考  we induce a ranking of the vertices rankl such that
. then  for each word wi ﹋ 考  we select the best-ranking sense in senses wi  according to rankl. a global measure g characterizes the overall graph structure g and is thus not particularly helpful in selecting a unique sense for ambiguous words - g collectively represents all interpretations of 考. we get around this problem  by applying g iteratively to each interpretation of 考 and selecting the highest scoring one. an interpretation is a subgraph such that g includes one and only one sense of each word in sentence 考 and all their corresponding intermediate nodes  see step  1  above . so if our sentence has five interpretations  we will measure the connectivity of the resulting subgraphs five times.

figure 1: an example of a graph.
1 connectivity measures
in this section we describe the measures of graph connectivity we consider for unsupervised wsd. although our measures can be applied to both directed and undirected graphs  for wsd purposes we are assuming that we are dealing with undirected graphs  we view an undirected edge as a pair of directed edges . this is motivated by the fact that semantic relations often have an inverse counterpart  e.g.  hypernymy is the inverse relation of hyponymy .
﹛we next introduce the distance function d u v   which is used by some of the measures discussed below:
d u v  =
where indicates the existence of a path from u to v  and k is a conversion constant  botafogo et al.  1   which replaces the ﹢ distance with an integer when v is not reachable from u  we choose k = |v |  as the length of any simple path is   |v | . as an example consider the graph in figure 1 where d a i  = 1  d c g  = 1  and so on.
1	local measures
local measures of graph connectivity determine the degree of relevance of a single vertex v in a graph g. they can thus be viewed as measures of the influence of a node over the spread of information through the network. formally  we define a local measure l as:
l : v ↙  1 
a value close to 1 indicates that a vertex is relatively important  whereas a value close to 1 indicates that the vertex is peripheral.
﹛several local measures of graph connectivity have been proposed in the literature  see wasserman and faust  for a comprehensive overview . a large number rely on the notion of centrality: a node is central if it is maximally connected to all other nodes. in the following  we consider three best-known measures of centrality  namely degree  closeness  and betweeness  freeman  1   and variations thereof. we also show how graph connectivity can be computed by solving a max-flow problem.
in-degree centrality the simplest way to measure vertex importance is by its degree  i.e.  the number of edges terminating in a given vertex:
indeg v  = |{ u v  ﹋ e : u ﹋ v }|
a vertex is central  if it has a high degree. in-degreecentrality is the degree of a vertex normalized by the maximum degree:

so  according to the graph in figure 1 
  and.
eigenvector centrality a more sophisticated version of degree centrality is eigenvector centrality. whereas the former gives a simple count of the number of connections a vertex has  the latter acknowledges that not all connections are equal. it assigns relative scores to all nodes in the graph based on the principle that connections to nodes having a high score contribute more to the score of the node in question  bonacich  1 . pagerank  brin and page  1  and hits  kleinberg  1  are popular variants of the eigenvector centrality measure and have been almost exclusively used in graph-based wsd  mihalcea  1; navigli and velardi  1 .
﹛pagerank determines the relevance of a node v recursively based on a markov chain model. all nodes that link to v contribute towards determining its relevance. each contribution is given by the page rank value of the respective node  pr u   divided by the number of its neighbors:

the overall contribution is weighted with a damping factor 汐  which implements the so-called random surfer model: with probability1 汐  the randomsurfer is expected to discontinue the chain and select a random node  i.e.  page   each with relevance .
﹛in contrast  hits  hypertext induced topic selection  determines two values for each node v  the authority  a v   and the hub value  h v  . these are defined in terms of one another in a mutual recursion:
	 	;	
intuitively  a good hub is a node that points to many good authorities  whereas a good authority is a node that is pointed to by many good hubs. a major difference between hits and pagerank is that the former is computed dynamically on a subgraph of relevant pages  whereas the latter takes the entire graph structure into account.
﹛if we apply hits to the graph in figure 1  we get the following authority values: a d  = 1 a e  = 1 a b  = 1 ... a a  = 1 a i  = 1. the pagerank values are pr d  = pr e  = pr b  = pr d  = 1 
pr f  = pr g  = pr c  = 1  and pr i  = pr a  =
1. while hits yields a fine-grained ranking  pagerank delivers only three different ranks  ranging from central to peripheral. notice that  since our graphs are undirected  the authority and hub values coincide.
key player problem  kpp  kpp is similar to the better known closeness centrality measure1  freeman  1 . here  a vertex is considered important if it is relatively close to all other vertices  borgatti  1 :

where the numerator is the sum of the inverse shortest distances between v and all other nodes and the denominator is the number of nodes in the graph  excluding v .

figure 1: the maximum flow between nodes e and g  edges are labeled with the pair flow/capacity .
for example  the kpp for nodes a and f in figure 1 is

  respectively.
betweenness centrality the betweenness of vertex v is calculated as the fraction of shortest paths between node pairs that pass through v  freeman  1 . formally  betweenness is defined as:

where 考st is the number of shortest paths from s to t  and 考st v  the number of shortest paths from s to t that pass through vertex v. we normalize by dividing betweenness v  by the maximum number of node pairs excluding v:

the intuition behind betweenness is that a node is important if it is involved in a large number of paths compared to the total set of paths. with reference to figure 1  the pairs of vertices  x g  and  g x   with x ﹋ {a b c d e}  are connected by two possible shortest paths  including either f or h as an intermediate vertex. thus  考xg = 考gx = 1 and 考xg f  = 考gx f  = 1. we can now calculate betweenness f  = 1 ﹞ 1 = 1 and cb f  = 1 = 1 .
maximum flow let g =  v e  be a connected graph  and let c : e ↙ r be a capacity function such that every edge  u v  ﹋ e is associated with capacity c u v . we further distinguish two vertices  the source s and the sink t. finally  let f : v ℅ v ↙ r be a function called flow.
﹛given an s-t-cut  s t   i.e.  a partition of v into two disjoint sets s and t  such that s ﹋ s and t ﹋ t  the s-t-flow of the cut represents the amount of information that can be conveyed from s to t through the cut while obeying all capacity constraints. it is defined as:

the maximum s-t-flow of a graph g has the highest value among all s-t-cuts. for example  if we fix e as the source and g as the sink  or viceversa  in the graph in figure 1  the maximum flow that can be conveyed equals to the sum of the maximum flows f f g  + f e h  = 1 + 1 = 1. this flow is determined by taking into account the paths e ↙ f ↙ g and e ↙ h ↙ g. in fact  menger's theorem states that the maximum s-t-flow in undirected graphs corresponds to the number of independent paths between a pair of vertices.
﹛in the context of wsd  maximum s-t-flows provide a relevance ranking on the set of vertices: the more flow is conveyed from s to t  the more relevant the sink is. initially  the capacity of each edge  u v  ﹋ e is set to 1 and its flow f u v  to 1. to compute an overall score for each vertex  we execute the following steps:
 v ﹋ v score v  := 1
  do score t  := score t  + max s-t-flow
 v ﹋ v   do
score v  :=	score v  maxscore u 
u﹋v
﹛the resulting score for each vertex v ﹋ v is the sum of the maximum flows having v as a sink normalized by the maximum score. if g is disconnected  we do not need to apply the algorithm separately to each connected component  since the maximum flow between s and t is 1 if t is not reachable from s. we calculate the maximum flow with the fordfulkerson  algorithm based on the notion of augmenting paths. we adopted edmonds and karp's  efficient implementation.
1	global measures
global connectivity measures are concerned with the structure of the graph as a whole rather than with individual nodes. here we discuss three well-known measures  namely compactness  graph entropy  and edge density.
compactness this measure represents the extent of cross referencing in a graph  botafogo et al.  1 : when compactness is high  each vertex can be easily reached from other vertices. the measure is defined as:

where max = k﹞|v | |v | 1  is the maximum compactness  i.e.  for a disconnected graph  and min = |v | |v |   1  is the minimum compactness  i.e.  for a fully connected graph .
the compactness of the graph in figure 1 is: co g  =   in this example  k = |v | = 1 .
graph entropy entropy measures the amount of information  or alternatively uncertainty  in a random variable. in graph-theoretic terms  high entropy indicates that many vertices are equally important  whereas low entropy indicates that only a few vertices are relevant. we define a simple measure of graph entropy as:

where the vertex probability p v  is determined by the degree distribution. to obtain a measure with a
 1  range  we divide h g  by the maximum entropy given by log|v |. for example  the distribution associated with the graph in figure 1 is:  leading to an overall graph entropy.
edge density finally  we propose the use of edge density as a simple global connectivity measure. edge density is calculated as the ratio of edges in a graph over the number of edges of a complete graph with |v | vertices  given by  . formally:

for example  the graph in figure 1 has edge density
.
1 experimental setup
sense inventory the graph connectivity measures just described were incorporated in the disambiguation algorithm introduced in section 1. as explained earlier  disambiguation proceeds on a sentence-by-sentence basis. each sentence is represented by a graph corresponding to meaning distinctions provided by a reference lexicon. in our experiments we employed two such lexicons. the first is wordnet 1  fellbaum  1   a resource commonly used in wsd research  see snyder and palmer  . we also used an extended version of wordnet created by navigli . the latter contains additional semantic relatedness edges  approximately 1  that relate associated concepts across parts of speech  e.g.  dog and bark  drink and glass . these were automatically extracted from collocation resources  e.g.  oxford collocations  longman language activator  and semiautomatically disambiguated.
data we selected two standard data sets for evaluating our connectivity measures  namely the semcor corpus  miller et al.  1  and the senseval-1 english all-words test set  snyder and palmer  1 . semcor is a subset of the brown corpus  and includes more than 1 content words manually tagged with wordnet senses. senseval-1 is a subset of the penn treebank corpus and contains 1 content words  again labeled with wordnet senses. we exhaustively tested our measures on the semcor dataset. the best performing one was also evaluated on senseval-1 and compared with state-of-the-art.
graph construction in order to speed up the graph construction process  all paths connecting pairs of senses in both versions of wordnet were exhaustively enumerated and stored in a database which was consulted at run-time during disambiguation. unfortunately  the use of global connectivity measures makes our wsd algorithm susceptible to combinatorial explosion  since all possible interpretations of a given sentence must be ranked  see section 1 . we used simulated annealing to heuristically explore the entire space of interpretations for a given sentence  cowie et al.  1 .
baseline and upper bound our graph-based algorithm was compared against a naive baseline that selects a sense for each word at random. as an upper bound  we used the first-sense heuristic which assigns all instances of an ambiguous word its most frequent sense according to the manually annotated semcor. it is important to note that current unsupervised wsd approaches-and also many supervised ones-rarely outperform this simple heuristic  mccarthy et al.  1 .
1 results
our results on semcor are summarized in table 1. we report performance solely on polysemous words  i.e.  words with more than one wordnet sense.
﹛let us first concentrate on the results we obtained with the standard wordnet inventory. as can be seen  almost all measures perform better than the random sense baseline. the
wordnetenwordnetmeasureprecrecf1precrecf1baseline111111indegree111111betweenness
kpp
hits
pagerank1 1 1
11 1 1
11 1 1
11 1 1
11 1 1
11 1 1
1maxflow111111compactness
graphentropy
edgedense1 1
11 1
11 1
11 1
11 1
11 1
1upperbnd111111table 1: performance of connectivity measures on semcor.
differences are significant both in terms of precision and recall  using a 聿1 test . hits and betweenness yield significantly better precision but worse recall. the best performing local measure is kpp  f1.1%   whereas the best performing global measure is graph entropy  f1.1% . kpp is significantly better than graph entropy both in terms of precision and recall  again using a 聿1 test . we conjecture that the inferior performance of the global measures is due to the use of a heuristic algorithm for searching the interpretation space. interestingly  pagerank yields significantly better recall and precision than hits. we attribute the difference in performance to the fact that pagerank implements the random surfer model. finally  note that a relatively simple measure like indegree performs as well as pagerank  f1 is 1% for both measures . this is not entirely surprising. the pagerank value of a node is proportional to its degree in undirected graphs. furthermore  research on directed graphs has experimentally shown that the two measures are broadly equivalent  upstill et al.  1 .
﹛we now turn to the performance of the different measures when the enriched wordnet  enwordnet  is used. here we also observe that all measures are significantly better than the baseline  in terms of precision and recall . the best performing global measure is compactness  f1.1% . the best local measures are indegree  kpp and pagerank  f1 is around 1% . kpp performs consistently well with wordnet and its enriched version. all three local measures achieve significantly better precision and recall than compactness. it seems that local measures benefit from a denser reference lexicon  with a large number of semantic relations  whereas global measures are disadvantaged due to the combinatorial explosion problem discussed above.to further substantiate this  we analyzed how kpp's performance varies when an increasing number of edges is considered for disambiguation. figure 1 shows that f1 increases when a sense has a large number of edges. in fact  when more than 1 edges are taken into account  kpp obtains an f1 of 1%. notice that we are excluding unambiguous words and that there are at least 1 occurrences of word senses in the semcor corpus for each interval in the graph.
﹛we next assess how kpp performs on the senseval-1 english all-words test set when using the enriched wordnet. we also compare our results with the best unsupervised system that took part in the senseval-1 competition1. the latter is

figure 1: performance of kpp by number of edges.
measurepart of speechprecrecf1kppnouns
adjectives1
11
11
1verbs111irst-dddnouns
adjectives1
11
11
1verbs111table 1: results on the senseval-1 all words task by part of speech.
a similarity-based algorithm. it was developed by strapparava et al.  and performs domain driven disambiguation  irst-ddd . the approach compares the domain of the context surrounding the target word with the domains of its senses and uses a version of wordnet augmented with domain labels  e.g.  economy  geography . table 1 shows how performance varies across parts of speech.1 kpp performs comparably to irst-ddd for nouns and adjectives  the differences in recall and precision are not statistically significant . irst-ddd yields significantly better results for verbs. this can be explained by the fact that the enriched wordnet contains a significantly smaller number of relatedness edges for verbs than for nouns or adjectives and this impacts the performance of kpp. also note that our experiments focused primarily on graph connectivity measures. consequently  we employed a relatively generic wsd algorithm see section 1  without additional tuning. for instance we could obtain improvedresults by consideringword sequences largerthan sentences or by weighting edges according to semantic importance  e.g.  hypernymy is more important than meronymy .
1 conclusions
in this paper we presented a study of graph connectivity measures for unsupervised wsd. we evaluated a wide range of local and global measures with the aim of isolating those that are particularly suited for this task. our results indicate that local measures yield better performancethan globalones. the best local measures are kpp  indegree  and pagerank. kpp has a slight advantage over the other two measures  since it performs consistently well across experimental conditions. our results are in agreement with borgatti  who shows in the context of social network analysis that kpp is better than other measures  e.g.  betweeness or in-degree centrality  at identifying which node in the graph is maximally connected to all other nodes. in linguistic terms this means that kpp selects maximally cohesive nodes which typically correspond to topical senses  thus indirectly enforcing the onesense per discourse constraint. we also find that the employed reference dictionary critically influences wsd performance. we obtain a large f1 improvement  1% for kpp  1% for indegree when adopting a version of wordnet enrichedwith thousands of relatedness edges. interestingly  we observe that indegree and pagerank yield performances comparable to kpp when the enriched wordnet is used. this is due to the increased number of relatedness edges which result in more densly connected graphs with more outgoing edges for every node. centrality-based measures are particularly suited at identifying such nodes.
﹛beyond the specific wsd algorithm presented in this paper  our results are relevant for other graph-based approaches to wsd  mihalcea  1; navigli and velardi  1 . our experiments indicate that performance could potentially increase when the right connectivity measure is chosen. the proposed measures are independent of the adopted reference lexicon and the graph construction algorithm. they induce a sense ranking solely by considering graph connectivity and can thus be easily ported across algorithms  languages  and sense inventories.
﹛an important future direction lies in combining the different measures in a unified framework. notice in table 1 that certain measures yield high precision  e.g.  betweeness  whereas others yield high recall  e.g.  kpp . we will also evaluate the impact of kpp in other applications such as graph-based summarization and the recognition of entailment relations.
