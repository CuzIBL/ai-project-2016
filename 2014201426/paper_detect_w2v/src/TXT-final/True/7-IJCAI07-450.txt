
traditional classification involves building a classifier using labeled training examples from a set of predefined classes and then applying the classifier to classify test instances into the same set of classes. in practice  this paradigm can be problematic because the test data may contain instances that do not belong to any of the previously defined classes. detecting such unexpected instances in the test set is an important issue in practice. the problem can be formulated as learning from positive and unlabeled examples  pu learning . however  current pu learning algorithms require a large proportion of negative instances in the unlabeled set to be effective. this paper proposes a novel technique to solve this problem in the text classification domain. the technique first generates a single artificial negative document an. the sets p and {an} are then used to build a na ve bayesian classifier. our experiment results show that this method is significantly better than existing techniques.
1 introduction
classification is a well-studied problem in machine learning. traditionally  to build a classifier  a user first collects a set of training examples that are labeled with predefined or known classes. a classification algorithm is then applied to the training data to build a classifier that is subsequently employed to assign the predefined classes to instances in a test set  for evaluation  or future instances  in practice .
　this paradigm can be problematic in practice because some of the test or future instances may not belong to any of the predefined classes of the original training set. the test set may contain additional unknown subclasses  or new subclasses may arise as the underlying domain evolves over time. for example  in cancer classification  the training set consists of data from currently known cancer subtypes. however  since cancer is a complex and heterogeneous disease  and still a perplexing one to-date  it is likely that the test data contain cancer subtypes that are not yet medically classified  they are therefore not covered in the training data . even if the training data do contain all the current cancer subtypes  new subtypes may be formed at a later stage as the disease evolves due to mutations or other cancer-causing agents. this phenomenon is not uncommon even in the seemingly simpler application domains. for example  in document classification  topics are often heterogeneous and new topics evolve over time. a document classifier built for classifying say  computer science papers  would face the similar problems as the cancer classifier described above. this is because computer science is a heterogeneous and increasingly cross-disciplinary domain; it is also a rapidly evolving one with new topics being created over time.
　thus  a classifier created based on the notion of a fixed set of predefined classes is bound to be inadequate in the complex and dynamic real-world in the long run  requiring the user to manually go through the classification results to remove the unexpected instances. in practice  a competent classifier should learn to identify unexpected instances in the test set so as to automatically set these unclassifiable instances apart. in some applications  this can be important in itself. for example  in the cancer example above  detection of the unexpected instances can alert the scientists that some new medical discovery  a new cancer subtype  may have occurred.
　in recent years  researchers have studied the problem of learning from positive and unlabeled examples  or pu learning . given a positive set p and an unlabelled set u  a pu learning algorithm learns a classifier that can identify hidden positive documents in the unlabeled set u. our problem of identifying unexpected instances in the test set can be modeled as a pu learning problem by treating all the training data as the positive set p and the test set as the unlabeled set u. a classifier can then be learned using pu learning algorithms to classify the test set to identify those unexpected  or negative  instances before applying a traditional classifier to classify the remaining instances into the original predefined classes.
　however  as the current pu techniques operate by trying to identify an adequate set of reliable negative data from the unlabeled set u to learn from  they require a large proportion of unexpected instances in the unlabeled set u to be effective. in practice  the number of unexpected instances in the test data can be very small since they are most likely to be arising from an emerging class. this means that the classifiers built with existing pu learning techniques will perform poorly due to the small number of unexpected
 negative  instances in u.
　in this paper  we propose a novel technique called lgn  pu learning by generating negative examples   and we study the problem using text classification. lgn uses an entropy-based method to generate a single artificial negative document an based on the information in p and u  in which the features' frequency distributions correspond to the degrees of  negativeness  in terms of their respective entropy values. a more accurate classifier  we use the na ve bayesian method  can be built to identify unexpected instances with the help of the artificial negative document an. experimental results on the benchmark 1 newsgroup data showed that lgn outperforms existing methods dramatically.
1 related work
pu learning was investigated by several researchers in recent years. a study of pac learning from positive and unlabeled examples under the statistical query model was given in  denis  1 .  liu et al.  1  reported sample complexity results and showed how the problem may be solved.
　subsequently  a number of practical algorithms  liu et al.  1; yu et al.  1; li and liu  1  were proposed. they all conformed to the theoretical results in  liu et al.  1  following a two-step strategy:  1  identifying a set of reliable negative documents from the unlabeled set; and  1  building a classifier using em or svm iteratively. their specific differences in the two steps are as follows. s-em proposed in  liu et al.  1  is based on na ve bayesian classification and the em algorithm  dempster  1 . the main idea was to first use a spying technique to identify some reliable negative documents from the unlabeled set  and then to run em to build the final classifier. pebl  yu et al.  1  uses a different method  1-dnf  to identify reliable negative examples and then runs svm iteratively to build a classifier.
　more recently   li and liu  1  reported a technique called roc-svm. in this technique  reliable negative documents are extracted by using the information retrieval technique rocchio  rocchio  1   and svm is used in the second step. in  fung et al.  1   a method called pn-svm is proposed to deal with the situation when the positive set is small. all these existing methods require that the unlabeled set have a large number of hidden negative instances. in this paper  we deal with the opposite problem  i.e. the number of hidden negative instances is very small.
　another line of related work is learning from only positive data. in  scholkopf  1   a one-class svm is proposed. it was also studied in  manevitz and yousef  1  and  crammer  1 . one-class svm builds a classifier by treating the training data as the positive set p. those instances in test set that are classified as negative by the classifier can be regarded as unexpected instances. however  our experiments show that its results are poorer than pu learning  which indicates that unlabeled data helps classification.
1 the proposed algorithm
given a training set {ci}  i = 1  1  ...  n  of multiple classes  our target is to automatically identify those unexpected instances in test set t that do not belong to any of the training classes ci. in the next subsection  section 1   we describe a baseline algorithm that directly applies pu learning techniques to identify unexpected instances. then  in section 1  we present our proposed lgn algorithm.
1 baseline algorithms: pu learning
to recapitulate  our problem of identifying unexpected instances in the test set can be formulated as a pu learning problem as follows. the training instances of all classes are first combined to form the positive set p. the test set t then forms the unlabeled set u  which contains both positive instances  i.e.  those belonging to training classes ci  and negative/unexpected instances in t  i.e.  those not belonging to any training class ci . then  pu learning techniques can be employed to build a classifier to classify the unlabeled set u  test set t  to identify negative instances in u  the unexpected instances . figure 1 gives the detailed framework for generating baseline algorithms based on pu learning techniques.
1. ue = ;
1. p = training examples from all classes  treated as positive ;
1. u = t  test set  ignore the class labels in t if present ;
1. run an existing pu learning algorithm with p and u to build a classifier q;
1. for each instance di （ u  which is the same as t 
1. use a classifier q to classify di
1. if di is classified as negative then
1. ue = ue “ {di};
1. output ue
figure 1. directly applying existing pu learning techniques
   in the baseline algorithm  we use a set ue to store the negative  unexpected  instances identified. step 1 initializes ue to the empty set  while steps 1 initialize the positive set p and unlabeled set u as described above. in step 1  we run an existing pu learning algorithm  various pu learning techniques can be applied to build different classifiers  to construct a classifier q. we then employ the classifier q to classify the test instances in u in steps 1 to 1. those instances that are classified by q as negative class are added to ue as unexpected instances. after we have iterated through all the test instances  step 1 outputs the unexpected set ue.
1 the proposed technique: lgn
in traditional classification  the training and test instances are drawn independently according to some fixed distribution d over x 〜 y  where x denotes the set of possible documents in our text classification application  and y = {c1  c1  ...  cn} denotes the known classes. theoretically  for each class ci  if its training and test instances follow the same distribution  a classifier learned from the training instances can be used to classify the test instances into the n known classes.
　in our problem  the training set tr with instances from classes c1  c1  ...  cn are still drawn from the distribution d. however  the test set t consists of two subsets  t.p  called positive instances in t  and t.n  called unexpected / negative instances in t . the instances in t.p are independently drawn from d  but the instances in t.n are drawn from an unknown and different distribution du. our objective is to identify all the instances drawn from this unknown distribution du  or in other words to identify all the hidden instances in t.n.
　let us now formally reformulate this problem as a two-class classification problem without labeled negative training examples. we first rename the training set tr as the positive set p by changing every class label ci （ y to  +   the positive class . we then rename the test set t as the unlabeled set u  which comprises both hidden positive instances and hidden unexpected instances. the unexpected instances in u  or t  are now called negative instances with the class label      bear in mind that there are many hidden positive instances in u . a learning algorithm will select a function f from a class of functions f: x ★ {+   } to be used as a classifier that can identify the unexpected  negative  instances from u. the problem here is that there are no labeled negative examples for learning. thus  it becomes a problem of learning from positive and unlabeled examples  pu learning . as discussed in the previous section  this problem has been studied by researchers in recent years  but existing pu techniques performed poorly when the number of negative  unexpected  instances in u is very small. to address this  we will propose a technique to generate artificial negative documents based on the given data.
   let us analyze the problem from a probabilistic point of view. in our text classification problem  documents are commonly represented by frequencies of words w1  w1  ...  w|v| that appear in the document collection  where v is called the vocabulary. let w+ represent a positive word feature that characterizes the instances in p and let w- represent a negative feature that characterizes negative  unexpected  instances in u. if u contains a large proportion of positive instances  then the feature w+ will have similar distribution in both p and u. however  for the negative feature w-   its probability distributions in the set p and u are very different. our strategy is to exploit this difference to generate an effective set of artificial negative documents n so that it can be used together with the positive set p for a classifier training to identify negative  unexpected  documents in u accurately.
   given that we use the na ve bayesian framework in this work  before going further  we now introduce na ve baye-
sian classifier for text classification.
na ve bayesian classification
na ve bayesian  nb  classification has been shown to be an effective technique for text classification  lewis  1; mccallum and nigam  1 . given a set of training documents d  each document is considered an ordered list of words. we use wdi k to denote the word in position k of document di  where each word is from the vocabulary v = {w1  w1  ...  w|v |}. the vocabulary is the set of all words we consider for classification. we also have a set of predefined classes  c = {c1  c1  ...  c|c|}. in order to perform classification  we need to compute the posterior probability  pr cj|di   where cj is a class and di is a document. based on the bayesian probability and the multinomial model  we have
 c j | di  
	| d |	 1 
and with laplacian smoothing 
	 Ρr 	 
	Ρr wt | c j   =	|v |	|d|
                     |v | +‘ ‘s=1 i=1 n ws   di  Ρr c j | di   where n wt di  is the count of the number of times that the word wt occurs in document di and pr cj|di （{1} depending on the class label of the document.
　finally  assuming that the probabilities of the words are independent given the class  we obtain the nb classifier:
 w  k | c j  
	Ρr c j | di    iwdi  k | cr  	 1 
　in the naive bayesian classifier  the class with the highest pr cj|di  is assigned as the class of the document.
generating negative data
in this subsection  we present our algorithm to generate the negative data. given that in a na ve bayesian framework  the conditional probabilities pr wt|-   equation  1   are computed based on the accumulative frequencies of all the documents in the negative class  a single artificial negative instance an would work equally well for bayesian learning. in other words  we need to generate the negative document an in such a way to ensure pr w+|+    pr w+|-    1 for a positive feature w+ and pr w-|+    pr w-|-    1 for a negative feature w-. we use an entropy-based method to estimate if a feature wi in u has significantly different conditional probabilities in p and in u  i.e   pr wi|+  and pr wi|-   . the entropy equation is:
	entropy wi   =  ‘pr wi | c *log pr wi | c  	 1 
c（{+  }
　the entropy values show the relative discriminatory power of the word features: the bigger a feature's entropy is  the more likely it has similar distributions in both p and u  i.e. less discriminatory . this means that for a negative feature w-  its entropy entropy w-  is small as pr w-|-   wmainly occurring in u  is significantly larger than pr w-|+   while entropy w+  is large as pr w+|+  and pr w+|-  are similar. the entropy  and its conditional probabilities  can therefore indicate whether a feature belongs to the positive or the negative class. we generate features for an based on the entropy information  weighted as follows:
	q wi   =1 	entropy wi  	 1 
max j=1... |v |  entropy wj   
　if q wi = 1  it means that wi uniformly occurs in both p and u and we therefore do not generate wi in an. if q wi  = 1  we can be almost certain that wi is a negative feature and we generate it for an  based on its distribution in u. in this way  those features that are deemed more discriminatory will be generated more frequently in an. for those features with q wi  between the two extremes  their frequencies in an are generated proportionally.
　we generate the artificial negative document an as follows. given the positive set p and the unlabeled set u  we compute each word feature's entropy value. the feature's frequency in the negative document an is then randomly generated following a gaussian distribution according to q wi  = 1-entropy wi /max entropy wj   wj（v . the detailed algorithm is shown in figure 1.
1. an =;
1. p = training documents from all classes  treated as positive ;
1. u = t  test set  ignore the class labels in t if present ;
1. for each feature wi （ u
1. compute the frequency of wi in each document dk freq wi  dk   dk （ u;
1. let mean	dk‘（dwifreq wi  dk  	where dwi is the set of μwi =	 
                       | dwi | documents in containing wi
1. let varianceσwi 1 =	1 ‘  freq wi dk    μwi  1 ;
 | dwi |  1  dk （dwi
1. for each feature wi （ v
1. compute pr wi|+   pr wi |-  using equation  1  assuming that all the documents in u are negative;
1.	let entropy wi  =  ‘pr wi | c *log pr wi | c   ;
c（{+  }
1. let m = max entropy wj    j =1  ...  |v|;
1. for each feature wi （ v
1. q wi  =1  entropy wi   ;
m
1. for j = 1 to |dwi| * q wi 
1. generate a frequency fnew wi   using the gaussian
 x μ  1
	distribution	 
i
1. an = an  { wi  fnew wi  }
1. output an
figure 1. generating the negative document an
   in the algorithm  step 1 initializes the negative document an  which consists of a set of feature-frequency pairs  to the empty set while steps 1 and step 1 initialize the positive set p and the unlabeled set u. from step 1 to step 1  for each feature wi that appeared in u  we compute its frequency in each document  and then calculate the frequency mean and variance in those documents dwi that contain wi. these information are used to generate an later. from step 1 to step 1  we compute the entropy of wi using pr wi|+  and pr wi|-   which are computed using equation  1  by assuming that all the documents in u are negative . after obtaining the maximal entropy value in step 1  we generate the negative document an in steps 1 to 1. in particular  step 1 computes q wi   which shows how  negative  a feature wi is in terms of how different the wi's distributions in u and in p are the bigger the difference  the higher the frequency with which we generate the feature. steps 1 to 1 is an inner loop and |dwi| * q wi  decides the number of times we generate a frequency for word wi. thus  if q wi  is small  it means that wi has occurred in both p and u with similar probabilities  and we generate fewer wi. otherwise  wi is quite likely to be a negative feature and we generate it with a distribution similar to the one in u. in each iteration  step 1 uses a gaussian distribution with corresponding wi and σwi to generate a frequency fnew wi  for wi. step 1 places the pair  wi  fnew wi   into the negative document an. finally  step 1 outputs our generated negative set an. note that the frequency for each feature wi in an may not of an integer value as it is generated by a gaussian distribution. an is essentially a randomly generated aggregated document that summarizes the unlabelled data set  but with the features indicative of positive class dramatically reduced.
building the final nb classifier
finally  we describe how to build an nb classifier with the positive set p and the generated single negative document an to identify unexpected document instances. the detailed algorithm is shown in figure 1.
1. ue =;
1. build a na ve bayesian classifier q with p and {an} using equations  1  and  1 ;
1. for each document di （ u
1. using q to classify di using equation  1 ;
1. if  pr -|di    pr +|di  
1. ue = ue  {di}; 1. output ue;
figure 1. building the final nb classifier
　ue stores the set of unexpected documents identified in u  or test set t   initialized to empty set in step 1. in step 1  we
use equations  1  and  1  to build a nb classifier by computing the prior probabilities pr +  and pr -   and the conditional probabilities of pr wi|+  and pr wi|- . clearly  pr wi|+  and pr wi|-  can be computed based on the positive set p and the single negative document an respectively  an can be regarded as the average document of a set of virtual negative documents . however  the problem is how to compute the prior probabilities of pr +  and pr - . it turns out that this is not a major issue  we can simply assume that we have generated a negative document set that has the same number of documents as the number of documents in the positive set p. we will report experimental results that support this in the next section. after building the nb classifier q  we use it to classify each test document in u  steps 1 . the final output is the ue set that stored all the identified unexpected documents in u.
1 empirical evaluation
in this section  we evaluate our proposed technique lgn. we compare it with both one-class svm  osvm  we use libsvm http://www.csie.ntu.edu.tw/~cjlin/libsvm/  and existing pu learning methods: s-em  liu et al.  1   pebl  yu et al.  1  and roc-svm  li and liu  1 . s-em and roc-svm are publicly available1. we implemented pebl as it is not available from its authors.
1 datasets
for evaluation  we used the benchmark 1 newsgroup collection  which consists of 1 documents from 1 different usenet discussion groups. the 1 groups were also categorized into 1 main categories   computer    recreation    science   and  talk . we first perform the following two sets of experiments:
1-classes: this set of experiments simulates the case in which the training data has two classes  i.e. our positive set p contains two classes. the two classes of data were chosen from two main categories   computer  and  science   in which the  computer  group has five subgroups  and the  science  group has four subgroups. every subgroup consists of 1 documents.
　each data set for training and testing is then constructed as follows: the positive documents for both training and testing consist of documents from one subgroup  or class  in  computer  and one subgroup  or class  in  science . this gives us 1 data sets. for each class  or subgroup   we partitioned its documents into two standard subsets: 1% for training and 1% for testing. that is  each positive set p for training contains 1 documents of two classes  and each test set u contains 1 positive documents of the same two classes. we then add negative  unexpected  documents to u  which are randomly selected from the remaining 1 groups.
　in order to create different experimental settings  we vary the number of unexpected documents  which is controlled by a parameter α  a percentage of |u|  i.e.  the number of unexpected documents added to u is α〜 |u|.
1-classes: this set of experiments simulates the case in which the training data has three different classes  i.e. our positive set p contains three classes of data. we used the same 1 data sets formed above and added another class to each for both p and u. the added third class was randomly selected from the remaining 1 groups. for each data set  the unexpected documents in u were then randomly selected from the remaining 1 newsgroups. all other settings were the same as for the 1-classes case.
1 experimental results
1-classes: we performed experiments using all possible c1 and c1 combinations  i.e.  1 data sets . for each technique  namely  osvm  s-em  roc-svm  pebl and lgn  we performed 1 random runs to obtain the average results. in each run  the training and test document sets from c1 and c1 as well as the unexpected document instances from the other 1 classes were selected randomly. we varied αfrom 1% to 1%. table 1 shows the classification results of various techniques in terms of f-score  for negative class  when α= 1%. the first column of table 1 lists the 1 different combinations of c1 and c1. columns 1 to 1 show the results of four techniques osvm  s-em  roc-svm and pebl respectively. column 1 gives the corresponding results of our technique lgn.
　we observe from table 1 that lgn produces the best results consistently for all data sets  achieving an f-score of
1% on average  which is 1%  1%  1% and 1% higher than the f-scores of existing four techniques  osvm  s-em  roc-svm and pebl  respectively in absolute terms. we also see that lgn is highly consistent across different data sets. in fact  we have checked the first step of the three existing pu learning techniques and found that most of the extracted negative documents were wrong. as a result  in their respective second steps  svm and em were unable to build accurate classifiers due to very noisy negative data. since the s-em algorithm has a parameter  we tried different values  but the results were similar.
table 1. experimental results for α= 1%.
data setosvms-emroc-svmpebllgngraphic-crypt11111graphic-electro11111graphic-med11111graphic-space11111os-crypt11111os-electronics11111os-med11111os-space11111mac.hardware-crypt11111mac.hardware-electro11111mac.hardware-med11111mac.hardware-space11111ibm.hardware-crypt11111ibm.hardware-electro11111ibm.hardware-med11111ibm.hardware-space11111windows-crypt11111windows-electro11111windows-med11111windows-space11111average11111　figure 1 shows the macro-average results of all αvalues  from 1% to 1%  for all five techniques in the 1-classes experiments. our method lgn outperformed all others significantly for α＋ 1%. when αwas increased to 1% and 1%  roc-svm achieved slightly better results than lgn. we also observe that osvm  s-em and roc-svm outperformed pebl since they were able to extract more reliable negatives than the 1-dnf method used in pebl. pebl needed a higher α 1%  to achieve similar good results.

figure. 1. the comparison results with different percentages of unexpected documents in u in the 1-classes experiments.
1-classes: figure 1 shows the 1-classes results where lgn still performed much better than the methods when the proportion of unexpected documents is small  α ＋ 1%  and comparably with s-em and roc-svm when the proportion is larger. osvm's results are much worse than s-em  roc-svm and lgn when α is larger  showing that pu learning is better than one-class svm in the problem. again  pebl required a much larger proportion of unexpected documents to produce comparable results.

figure. 1. the comparison results with different percentages of unexpected documents in u in the 1-classes experiments.
　in summary  we conclude that lgn is significantly better  with high f-scores  than the other techniques when α is small  α＋ 1%   which indicates that it can be used to effectively extract unexpected documents from the test set even in the challenging scenarios in which their presence in u is non-obvious. the other methods all failed badly when α is small. lgn also performed comparably in the event when the proportion of unexpected instances is large  α− 1% .
　finally  we also conducted 1-classes experiments in which ten different classes from both the 1 newsgroups and reuter collections  with same experimental setting for the 1-classes  were used. the behaviors of the algorithms for 1 classes were the same as for 1 classes and 1 classes. using the
reuter collection with 1 classes and αset to 1%  1%  1%  1% and 1%  our algorithm lgn achieved 1%  1%  1%  1%  1% higher f-scores respectively than the best results of the existing methods  osvm  s-em  roc-svm and pebl . similarly  using 1 classes from the 1 newsgroup collection  lgn achieved 1%  1%  1%  1%  and 1% higher f-scores for α=1%  1%  1%  1% and 1% of unexpected documents respectively than the best of the four other existing methods.
effect of priors: recall that in section 1 we have left the prior probabilities as a parameter since we only generate a single artificial negative document. to check the effect of priors  we also varied the prior in our experiments by changing the proportion of negative documents as a percentage of the number of positive documents in p. we tried 1%  1%  1% and 1%. the results were virtually the same  with average differences only within ＼1%. thus  we simply choose 1% as the default of our system  which gives us pr +  = pr -  = 1. all the experimental results reported here were obtained using this default setting.
1 conclusion
in real-world classification applications  the test data may differ from the training data because unexpected instances that do not belong to any of the predefined classes may be present  or emerge in the long run  and they cannot be identified by traditional classification techniques. we have shown here that the problem can be addressed by formulating it as a pu learning problem. however  directly applying existing pu learning algorithms performed poorly as they require a large proportion of unexpected instances to be present in the unlabeled test data  which is often not the case in practice.
　we then proposed a novel technique lgn to identify unexpected documents by generating a single artificial negative document to help train a classifier to better detect unexpected instances. our experimental results in document classification demonstrate that lgn performed significantly better than existing techniques when the proportion of unexpected instances is low. the method is also robust irrespective of the proportions of unexpected instances present in the test set. although our current experiments were performed in the text classification application using an nb classifier  we believe that the approach is also applicable to other domains. using a single artificial negative document  however  will not suitable for other learning algorithms. in our future work  we plan to generate a large set of artificial documents so that other learning methods may also be applied.
