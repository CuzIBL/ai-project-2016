
latent semantic indexing  lsi  has been shown to be effective in recovering from synonymy and polysemy in text retrieval applications. however since lsi ignores class labels of training documents  lsi generated representations are not as effective in classification tasks. to address this limitation  a process called 'sprinkling' is presented. sprinkling is a simple extension of lsi based on augmenting the set of features using additional terms that encode class knowledge. however  a limitation of sprinkling is that it treats all classes  and classifiers  in the same way. to overcome this  we propose a more principled extension called adaptive sprinkling  as . as leverages confusion matrices to emphasise the differences between those classes which are hard to separate. the method is tested on diverse classification tasks  including those where classes share ordinal or hierarchical relationships. these experiments reveal that as can significantly enhance the performance of instance-based techniques  knn  to make them competitive with the state-of-the-art svm classifier. the revised representations generated by as also have a favourable impact on svm performance.
1 introduction
support vector machines  svm  and k-nearest neighbours  knn  are two well-studied machine learning approaches to text classification. they are both applied on the vector space model defined over a bag of words  bow . in this model  documents are represented as vectors over a set of dimensions  each corresponding to a word in the bow. despite its wide usage  it has been argued that the bow approach fails to scale up for classification tasks  principally because of its inability to handle polysemy and synonymy  sebastiani  1 . existing research reveals two broad ways of overcoming this limitation. the first involves using background knowledge such as external thesauri or repositories  zelikovitz and hirsh  1; gabrilovich and markovitch  1 . the second approach is introspective  in that it does not rely on any external knowledge. typically complex features are extracted from the bow  using factor analysis  deerwester et al.  1   word clustering  bekkerman et al.  1  or rule induction  cohen and singer  1 .
　introspective techniques rely on exploiting the duality of term and document spaces. documents are regarded as similar when they have similar terms; but terms are  in turn  regarded as similar when they occur in similar documents. latent semantic indexing  lsi  is an introspective technique that uses factor analysis to resolve this circularity. words and documents are mapped to a lower-dimensional  concept  space. lsi has recently been applied to real world text classification problems. in  gee  1  lsi has been applied to spam classification  and performances competitive with naive bayes classifier reported. in a study by zelikovitz et al   lsi-based classifiers have been extended to accommodate background knowledge.
　however  an inherent limitation of lsi when applied to classification is that it fails to exploit class knowledge of training documents. if taken into account  class knowledge can lead lsi to promote inferred associations between words representative of the same class  and attenuate word associations otherwise. in this paper  we combine lsi with class knowledgeto producerevised documentrepresentations from the vector space model. we show that both knn and svm benefit from these revised representations.
　our work expands on and extends recent work by chakraborti et al   where a simple approach called  sprinkling  was proposed to integrate class knowledge into lsi. the basic idea involves encoding class labels as artificial terms which are appended to training documents. in the binary classification case  for example  two additional terms corresponding to classes c1 and c1 are appended to documents belonging to c1 and c1 respectively. lsi is performed on the augmented term-document matrix  resulting in class-specific word associations being promoted. thus  documentsand words belonging to the same class are pulled closer to each other. to further emphasise class knowledge  more than one term could be sprinkled per class.
　the basic sprinkling approach treats all classes equally. this is a limitation for the many multi-class problems with explicit relationships between classes. two examples are hierarchical classes  e.g. yahoo directory  and ordinal classes  e.g. ratings 1 to 1 in movie review  each rating treated as a class . also  sprinkling is blind to classifier needs. in reality  pairs of classes found to be easily separable by one classifier could be difficult to discriminate for another.
　in this paper  we present theoretical insights to explain why sprinkling works  and validate the same empirically. we then propose a principled extension  called adaptive sprinkling  as  that addresses the limitations mentioned above. the key idea behind as is to leverage confusion matrices reported by classifiers to emphasise differences between those classes which are hard to separate. we show that this approach implicitly takes into account explicit relationships between classes in multi-class problems. experimental results are reported on three differenttypes of classification problems involving disjoint  ordinal and hierarchical classes.
　the contributions of this paper are threefold. first  we show that sprinkled lsi can be used to significantly improve the performance of instance based learners like knn  to make them competitive with state-of-the-art classifiers like svm. this has practical implications in situations where fast incremental updates and lazy learning  which are strengths of knn  are desirable. secondly  our research highlights the wealth of information hidden in confusion matrices  which can be exploited to improve classification. the classifierspecific nature of this information is especially useful in this regard. thirdly  we show that as-generated lsi representations have a favourable influence on svm performance.
1 sprinkling
1 latent semantic indexing
the purpose of lsi is to extract a smaller number of dimensions that are more robust indicators of meaning than individual terms. lsi uses the singular value decomposition  svd  to arrive at these latent dimensions. in principle  svd achieves a two-mode factor analysis and positions both terms and documents in a single space defined over the extracted dimensions. svd breaks down the original term document matrix into three matrices  two of which show the revised representations of words and documents in terms of the new dimensions  and the third associates  weights  to these dimensions. the thesis behind lsi is that less important dimensions correspond to  noise  due to word-choice variability. a reduced rank approximation to the original matrix is constructed by dropping these noisy dimensions. this approximation is a smoothed  blurred  version of the original  and is expected to model associations between terms and documents in terms of the underlying concepts more accurately. an interesting property of svd is that the generated approximation is the closest matrix of its rank to the original in the least-squares sense.
　while lsi has been shown to be successful in retrieval applications  it has certain limitations when applied to classification tasks. as noted earlier  since lsi is blind to class labels of training documents  the extracted dimensions are not necessarily the best in terms of discriminating between classes. also  infrequent words with high discriminatory power may be filtered out by lsi. the idea of sprinkling was conceived as a simple and intuitive way of addressing these limitations.

figure 1: an example of sprinkling.
1 sprinkling
we illustrate the basic idea behind sprinkling with an example. figure 1 shows a trivial term-document matrix constructed from six documents belonging to two different classes. instead of performing lsi directly on this matrix  sprinkling augments the feature set with artificial terms corresponding to the class labels. these terms act as carriers of class knowledge. in figure 1  terms t1  and t1 are  sprinkled  to documents belonging to classes c1  and c1 respectively. svd is then performed on the augmented term document matrix. noisy dimensions corresponding to low singular values are dropped and a lower rank approximation constructed in the usual manner. to make training document representations compatible with test documents  the sprinkled dimensions are dropped. this is referred to as  unsprinkling  in figure 1. the test and training documents can now be compared in the usual manner using knn or svm. while in the example above we used one sprinkled term per class  in principle we can sprinkle more terms per class  to boost the contribution of class knowledge to the classification process.
1 adaptive sprinkling
adaptive sprinkling  as  is motivated by the need to address two main limitations of the basic sprinkling approach. first  in a multi-class situation  sprinkling treats all classes equally and disregards relationships between classes  as defined over ordinal and hierarchical classes. furthermore  even in cases where classes have no explicit relation between them  some classes are more easily separable than others  so the number of sprinkled terms should depend on the complexity of the class decision boundary. secondly  the classes found confusing by a knn classifier could be different from those found confusing by svm  and ideally the sprinkling process should adapt to classifier needs.
　the key to as is its exploitation of the confusion matrices generated by classifiers like knn and svm. a confusion matrix compares a classifier's predictions against expert judgements on a class-by-class basis. the non-diagonal values in this matrix are indicative of classes that the classifier finds hard to separate; the lower the values  the more easily separable the classes. referring to classification errors in the example confusion matrix of figure 1  we readily infer that classes 1 and 1 are easy to tell apart  while classes 1 and 1 are harder to discriminate. as is based on the intuition that relatively more sprinkled terms are to be allocated between hard-

1    1      1     1     1       1       1        1      1 1    1     1     1     1       1        1       1      1 1     1     1    1       1       1        1       1      1       1       1    1      1       1       1       1       1      1       1       1     1      1       1       1       1       1        1       1        1     1     1     1       1       1        1       1        1      1    1     1       1 1        1        1      1       1       1       1     1   1 1        1       1       1        1       1        1      1    11. comp.graphics
1. comp.os.ms-windows.misc
1. comp.sys.ibm.pc.hardware
1. comp.sys.mac.harware
1. comp.windows.xp
1. rec.autos
1. rec.motorcycles
1. rec.sport.baseball1. rec.sport.hockey
classifiers predictions
figure 1: a confusion matrix over the 1ng hierarchy. shaded regions correspond to the comp and rec subtrees.
for  i = 1  to  m-1  {      /* m is the number of classes. */       for  j = i+1  to  m  {
1. compute normalized mutual class complexity between classes c   ci	j as follows:
mcc  i   j  
mcc         i   j   =
	norm	mcc  i   j  
max
1. s =   msl     mcc          i   j   norm
1. sprinkle s terms in all documents belonging to class      and s othersci in all documents belonging to class cj
}
}figure 1: determining the number of sprinkled terms.
to-discriminate classes. interestingly  we found that confusion matrices also implicitly carry information about explicit class relationships as in ordinal and hierarchical classes. for example  in figure 1  we see that the two shaded regions correspond to confusion between classes within the comp and rec subtrees of 1 newsgroups  1ng   lang  1 . the confusion between classes from the two disjoint subtrees is smaller.
　as determinesthe numberof sprinkled terms for each class from the confusion matrix. let qij be a non-diagonal element of the confusion matrix q  showing the number of documents of class ci being misclassified as class cj. we define probabilities p i|j  and p j|i  as the probability of class ci being misclassified as class cj  and vice versa  respectively. these probabilities can be estimated from the confusion matrix as follows:   and .
we then define the  mutual complexity  between classes ci and cj as mcc i j  =  p i|j  + p j|i  /1. the asymmetric confusion matrix q is now transformed into a mutual complexity matrix m  which is symmetric. the pseudocode in figure 1 shows how sprinkled terms can be generated based on the matrix m. the maximum sprinkling length msl is empirically determined. in our experiments we used msl = 1. we note that the mutual class complexity values are normalised and used as weights to vary the number of sprinkled terms as a fraction of msl. thus the influence of class knowledge is greater for those classes that are more difficult to discriminate.
1	why does sprinkling work 
the improved performance of sprinkled lsi in classification tasks can be explained using empirical observations made in

figure 1: document and term clustering.
 kontostathis and pottenger  1 . their work reveals close correspondence between lsi and higher order associations between terms. a word w1 is said to have a first order association with another word w1  if they co-occur in at least one document. w1 and w1 share a second order association if there is at least one term w1 that co-occurs with w1 and w1 in distinct documents. similarly  we can extend this to orders higher than 1. kontostathis and pottenger provide experimental evidence to show that lsi boosts similarity between terms sharing higher order associations. in the light of this observation  it is interesting to note that sprinkled terms boost second-order associations between terms related to the same class  hence bringing them closer.
　we carried out an analysis of the effect of sprinkling on lsi  to verify the hypothesis that sprinkling leads to better term and document representations. starting with document representations generated by sprinkled lsi and treating each class as a cluster  we compute the  within-cluster  and  between-cluster  point scatters  hastie et al.  1  that measure cluster separability. the ratio of within- and between-cluster point scatters  referred to as the w/b measure  is used as a measure of cluster separability. the lower the value  the more separable the clusters. for analysis of term clustering  terms prototypical of classes were manually identified and the impact of sprinkling on their revised representations investigated.
　figure 1 shows that with increased number of sprinkled terms  the w/b measure falls conspicuously. however there is a second factor to be taken into account. sprinkled lsi distorts the original term document matrix d to a class-enriched lsi approximation ds. however  ds is no longer the best k-rank approximation to the d in the least-square sense. the vertical axes of the graphs in figure 1 show the mean square of errors between d and ds. we see that the reduction in w/b achieved by sprinkling is at the cost of losing information on d. thus very large number of sprinkled terms may be detrimental to classification performance  as it may overemphasise class-knowledge. ideally we would like a trade off between  under- and  over-sprinkling  that gives us the best of both worlds: improve class-discrimination while not overlooking specific patterns in d.
1 empirical evaluation
we evaluated adaptive sprinkling on three types of classification problems. the first involves hierarchical classes  which have an is-a taxonomy defined over them. the second type has an ordinal relationship defined between classes. for example  a textual review accompanied by a rating of 1  on a 1 point scale  is expected to be more similar to one rated at 1 than another at 1. if numeric ratings are treated as class labels  similarity between classes is a function of this ordering. finally  we consider orthogonal problems where classes bear no explicit relationship to each other.
1 experimental methodology
we used the following datasets in our experiments:
1. the hierarchical dataset: this dataset was formed from the 1 newsgroups collection  lang  1  which has seven sub-trees: comp  rec  talk  alt  misc  soc  and sci. we selected the comp and rec sub-trees which contain 1 and 1 classes  corresponding to leaf-nodes  respectively. we used 1 documents from each of these nine classes.
1. the ordinal dataset: classification between ordinal classes is an interesting problem in sentiment analysis literature  pang and lee  1 . however  due to the relative youth of the field  no suitable benchmark datasets was readily available. we therefore compiled a new dataset from reviews on the  actors and actresses  sub-topic of the rateitall.com opinion website. each review contained an integer rating  1 to 1 inclusive  assigned by the author. these ratings were used as the class labels. we removed all reviews having less than 1 words  and created 1 equally distributed classes  each with 1 reviews.
1. the orthogonal dataset: we used the acq  crude  and earn classes of the reuters-1 collection  reuters  1  to form this dataset. 1 documents were selected from each class  such that each document belongs to at most one class.
　all three datasets underwent similar pre-processing. after stop word removaland stemming  binary valued term-document matrices were constructed. for each of the datasets  information gain  ig   sebastiani  1 was used to select the top 1 discriminating words. for experiments using svm  we used the svmmulticlass implementation  joachims  1 . a linear kernel was used as this was found to be best for text classification problems  joachims  1 . we use accuracy as a measure of classifier effectiveness since this is known to be appropriate for single labelled documents in datasets with equal class distributions  gabrilovich and markovitch  1 . for all datasets we performed classification using 1 equally sized train-test pairs  and used the paired t-test to assess significance.
1 the effect of sprinkling on confusion matrices
as described in section 1  high off-diagonalvalues in a confusion matrix indicate classes that the classifier finds hard to separate. this forms the intuitive basis for using the confusion matrix to generate the sprinkling codes. in our experiments  a 1-fold cross-validation on the raw training data yields five confusion matrices  which are used to construct an average confusion matrix q. sprinkled terms are generated based on q  and lsi is performed on the sprinkled representation. the same classifier is then applied to the revised representations  yielding a new confusion matrix q. comparing q and q provides direct evidence of the quality of the revised representation.

figure 1: confusion matrices before  left column  and after  right column  sprinkling
　figure 1 is a qualitative illustration of the effects of as on the initial confusion matrices  which result from applying a knn classifier to three datasets. each element of the matrix is mapped onto a cell colour. light colours signify many entries in that cell  dark ones signify few. ideally all cells except those on the diagonal should be dark  as this indicates total agreement between the expert and the classifier.
　in all three datasets  we observe that as results in a reduction in inter-class confusion. the first column in the matrix of figure 1a and the second one of figure 1c  reveal pairs of classes that knn finds hard to classify. interestingly  as succeeded in reducing inter-class confusion  as is revealed by the near-diagonal patterns in matrices of figures 1b and 1d.
　a closer look at the confusion matrices obtained after sprinkling reveals patterns that are consistent with the relationship between classes. in the hierarchical dataset  the confusion is mainly between classes within the same sub-tree. there are two broad confusion zones  one between the five classes of the comp subtree  the other between four classes of rec. furthermore very closely related classes like those corresponding to pc and mac hardware  and those relating to autos and motorcycles are hard to discriminate  and this is reflected in the lighter shades in the corresponding cells of figure 1b. for ordinal classes  the confusion matrix of figure 1d shows that as has implicitly mined the similarity between rating classes and attenuated confusion between distant classes. this is evident from the broad pattern of light shades along the diagonal  and darker shades elsewhere. this is expected as adjacent classes of an ordinal dataset are the most similar. the orthogonal dataset has the least confusion between classes since there is no explicit re-
hierarchicalordinalorthogonalbaseline111knnclsi111lsi+as111baseline111knnelsi111lsi+as111svmbaseline111table 1: knn performance before and after sprinkling.
lationship between them. figures 1e and 1f show that sprinkling has a positive effect in reducing inter-class confusion. in particular  the confusionbetween classes acqand crude has been markedly reduced. we sought an empirical explanation for this by studying similarity between terms  kontostathis and pottenger  1  before and after as. it was observed that similarity between words were boosted if they related strongly to the same class  and attenuated otherwise. for example   opec  and  refinery   both relevantto the class crude  were drawn closer  while  dividend   from earn  and  crude   from crude  were moved apart.
1 the effect of sprinkling on knn and svm
to assess the impact of sprinkling we constructed three representations of each dataset: the raw term-document matrix  baseline   the lsi-generated reduced dimensional representation  lsi   and the approximation of the original matrix generated by sprinkled lsi  lsi+as .
　effects of sprinkling on knn: we used two variants of knn  the first based on the euclidean distance measure  knne  and the second on cosine similarity  knnc . both use a weighted majority vote from the 1 nearest neighbours.
　table 1 reports knn performances  before and after sprinkling  at the lsi dimension empirically found best. these are compared against baseline svm performance. for each dataset  the performances significantly better  p   1  than the rest  are shown in bold. firstly  we observe that as leads to sizable improvements in performance of both knne and knnc over the respective baselines. knne and knnc performances with lsi+as are significantly better than lsi on all datasets. secondly  lsi+as enhances knn performance to be competitive with  and occasionally outperform baseline svm. figure 1 shows knnc and knne performances over various lsi dimensions. we note that lsi+as consistently outperforms lsi at all dimensions  on both measures.
　the poor performance of all classifiers on the ordinal dataset can be attributed to classes that are not neatly separable. this is partly caused by subjective differences between reviewers  who use different ratings to express similar judgements. the positive impact of as on confusion matrices in figure 1d suggests that a regression-based technique can fare better than a classifier that attempts to predict a precise rating. furthermore  the ig measure used for feature selection assumes classes to be disjoint and needs to be reformulated to accommodate inter-class similarity.
　effects of sprinkling on svm: table 1 shows the impact of sprinkling on svm performance. it may be noted that the confusion matrix used to generate sprinkled terms re-
hierarchicalordinalorthogonalbaseline111svmlsi111lsi+as111table 1: svm performance before and after sprinkling.
flected weaknesses specific to svm  hence as should ideally emphasise differences between classes that svm on its own found hard to classify. the results are in line with our expectation  as lsi+as significantly  p   1  outperforms the baseline on all three datasets. there is some evidence to suggest that lsi alone improves svm performance  but the difference is not statistically significant except for the orthogonal dataset.
1 related work
we are aware of three other efforts that extend lsi to accommodate class knowledge. sun et al  presents a technique called slsi that is based on iteratively identifying discriminative eigenvectors from class-specific lsi representations. in their study  no significant improvement of slsi overbaseline svm was reported. additionally slsi involves km svd computations  corresponding to k iterations over m classes  making it computationally expensive. in another study  wiener et al  use a combination of what they refer to as local lsi and global lsi. local lsi representations are constructed for each class  in a similar spirit to slsi. test documents are compared against each local lsi representation independently. in addition to computation overheads  one shortcoming of the approach is that similarities between test documents across the local lsi representations cannot be compared easily. finally  wang et al  presents a theoretical model to accommodate class knowledge in lsi. no empirical studies were reported  but the authors note that their approach slows down when a document belongs to more than one class. in contrast  sprinkled terms can comfortably reflect affiliation of documents to more than one class  and this has no adverse implication on time performance.
　when compared to as  a general shortcoming of all of the above mentioned approaches is that they fail to take into account relationships between classes. a second relative strength of our approach is that it is simple and can easily be integrated into existing lsi implementations. unlike most of the approaches above  the time complexity of our algorithm is independent of the number of classes. in all our benchmark experiments  computing svd over an augmented term-document matrix takes less than 1% additional time compared to svd on the original matrix.
1 conclusion and future work

figure 1: knn performance at various lsi dimensions.the first contribution of our paper is extending lsi to supervised classification tasks and generating revised document representations that can be used by any technique founded on the vector space model. the experimental results verify that we have succeeded in enhancing the performance of instance-based learners like knn to make them comparable to state-of-the-art techniques like svm. this has strong practical implications for applications where lazy incremental updates are desirable. also while svm-like kernel methods suffer from the  black-box  syndrome  knn is well recognised to be suitable for explanation and visualisation  making expert-initiated refinement possible.
　experiments on hierarchical and ordinal datasets conclusively demonstrate that confusion matrices implicitly capture class structure  which can be exploited to reduce confusion between classes. to our knowledge  ours is the first work combining the strengths of lsi  like higher order cooccurrence modeling and ability to recover from word choice variability  with the knowledge of class relationships as inferred from confusion matrices. the result is a revised vector space representation that adapts itself to classifier needs.
　we have also shown that as-generated svm representations result in significant improvementsin svm performance. the results obtained are best-in-line for all three datasets.
　as part of future work  we plan to devise a more principled approach for estimating the msl parameter in the as algorithm. our current idea is to use cross-validation over training data to arrive at a good value. finally  we are also investigating an extension of as that sprinkles documents carrying background knowledge of term associations. this has the potential to generalise as to a comprehensive framework that unifies background and introspectively acquired knowledge  while addressing class-specific confusion.
