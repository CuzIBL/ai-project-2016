 
evolutionary computation is a useful technique for learning behaviors in multiagent systems. among the several types of evolutionary computation  one natural and popular method is to coevolve multiagent behaviors in multiple  cooperating populations. recenl research has suggested that revolutionary systems may favor stability rather than performance in some domains. in order to improve upon existing methods  this paper examines the idea of modifying traditional coevolution  biasing it to search for maximal rewards. we introduce a theoretical justification of the improved method and present experiments in three problem domains. we conclude that biasing can help coevolution find better results in some multiagent problem domains. 
1 	introduction 
multi-agent learning is an area of intense research  and is challenging because the problem dynamics are often complex and fraught with local optima. these difficulties have made evolutionary computation  ec  an attractive approach to learning multiagent behaviors  for example  flba  1; luke et a/.  1; wu et a/.  1; bull et a/.  1; bassett and de jong  1; bull  1  . this work has led to interesting research questions in applying ec in a multiagent setting  including communication  representation  generalization  teamwork  and collaboration strategies. 
　as it is very general  and relatively knowledge-poor   evolutionary computation is particularly useful in problems that are of high dimensionality  are non-markovian  or yield few heuristic clues about the search space that otherwise would make reinforcement learning or various supervised learning methods good choices. we believe that multiagent learning problem domains often exhibit such features. these problem domains are often complex and  correct  actions cannot be known beforehand in a given situation. further  even relatively simple problems can require large numbers of external and  even more challenging  internal state variables. last  many such problems exhibit changing environments  even ones that adapt to make the problem harder for the learner  due to the presence of co-learning opponents . 
　ec fits nicely with multiagent systems because it is already population-oriented: it searches over a set of multiple agents  the individuals . further  an ec population may be broken down into distinct subpopulations  each yielding agents to be tested together in a multiagent environment  with each subpopulation  evolving  in parallel. this notion of separately evolving  interacting populations of agents is known as coevolution. coevolution has proven a useful technique for multiagent problems where the quality of agent is typically assessed in the context of competing or cooperating peers. 
　but coevolution is no panacea. recent research has shown that a coevolutionary system does not necessarily search for better teams of agents  but can instead search for agent populations that represent stable equilibria in the cooperative search space  ficici and pollack  1; wiegand et a/.  1b . this paper will explore this problem  then introduce a method for biasing coevolution so that the search for stability coincides with optimization for improvement. 
　we continue this paper with a brief description of coevolution and present an experimental and a theoretical framework. we then suggest a method for biasing the coevolutionary process  describe a theoretical investigation on how biasing modifies the search space  and discuss experimental results on three problem domains. the paper ends with a set of conclusions and directions for future work. 
1 	evolutionary computation and coevolution 
evolutionary computation is a family of techniques  known as evolutionary algorithms  widely used for learning agent behaviors. in ec  abstract darwinian models of evolution are applied to refine populations of agents  known as individuals  representing candidate solutions to a given problem. an evolutionary algorithm begins with an initial population of randomly-generated agents. each member of this population is then evaluated and assigned a fitness  a quality assessment . the ea then uses a fitness-oriented procedure to select agents  breeds and mutates them to produce child agents  which are then added to the population  replacing older agents. one evaluation  selection  and breeding cycle is known as a generation. successive generations continue to refine the population until time is exhausted or a sufficiently fit agent is discovered. 
multiagent systems 	1 　coevolutionary algorithms  ceas  represent a natural approach to applying evolutionary computation to refine multiagent behaviors. in a cea  the fitness of an individual is based on its interaction with other individuals in the population: thus the fitness assessment is context-sensitive and sub-
jective. in competitive systems  agents benefit at the expense of other agents; but in cooperative systems  agents succeed or fail together in collaboration. the focus of this paper is in cooperative coevolutionary algorithms. interesting cea issues include communication  bull et al.  1   teamwork  and collaboration  bull  1 . 
　a standard approach ipotter  1  to applying cooperative coevolutionary algorithms  or cceas  to an optimization problem starts by identifying a static decomposition of the problem representation into subcomponents  each represented by a separate population of individuals. for example  if a task requires two agents whose collaboration must be optimized  one might choose to use two populations  one per agent in the task. the fitness of an individual in a population is then determined by testing the individual in collaboration with one or more individuals from the other population. aside from this collaborative assessment  each population follows its own independent evolution process in parallel with other populations. 
1 	formalizing the ccea 
an appealing abstract mathematical model for this system comes from the biology literature: evolutionary game theory  egt   maynard-smith  1; hofbauer and sigmund  1 . egt provides a formalism based on traditional game theory and dynamical systems techniques to analyze the limiting behaviors of interacting populations under long-term evolution. for specifics about applying egt to the analysis of multi-population cooperative coevolutionary algorithms  see  wiegand et a/.  1a . 
　in this paper  we consider only two-population models. in such a model  a common way of expressing the rewards from individual interactions is through a pair of payoff matrices we assume a symmetric model such that when individuals from the first population interact with individuals from the second  one payoff matrix a is used  while individuals from the second population receive rewards defined by the transpose of this matrix  at . in our theoretical exploration of egt in this paper  we will use an infinite population: thus a population can be thought of not as a set of individuals  but rather as a finite-length vector x of proportions  where each element in the vector is the proportion of a given individual configuration  popularly known as a genotype or  as we will term it  a strategy  in the population. as the proportions in a valid vector must sum to one  all legal vectors make up what is commonly known as the unit simplex  denoted a   where n here is the number of distinct strategies possible   
　formally we can model the effects of evaluation and proportional selection over time using a pair of difference equations  one for each population. the proportion vectors for the two populations are x and y respectively. neglecting the issue of mutation and breeding and concentrating only on the effects of selection  we can define the dynamical system of a two-population cooperative coevolutionary algorithm as: 
1 
 1  
 1  
 1  
 1  
...where x' and y' represent the new population distributions for the next generation. here it is assumed that an individual's fitness is assessed through pair-wise collaborations with every member of the cooperating population. we call this idea complete mixing. the equations above describe a twostep process. first  the vectors u and w are derived; these represent the fitness assessments of strategies in the generations x and y respectively. note that an infinite population model considers the fitness assessment for a strategy  and not for a particular instance of that strategy  an individual . then selection is performed by computing the proportion of the fitness of a specific strategy over the sum fitness of the entire population. 
1 	optimization versus balance 
ccea researchers apply these algorithms hoping to optimize the collaborations between the populations  but it isn't clear that this system is meant to do this. in fact  the system seeks a form of balance between strategies  which may not correspond with what we  as external viewers of the system  would consider optimal. in the context of a payoff matrix  an optimal position is the pair of strategies that yield the highest payoff for the cooperating agents. this position is a stable attracting fixed point of such a system; but it is also the case that there are other suboptimal points  which can also attract trajectories  wiegand et al.  1b . indeed  it is possible that most  if not all  trajectories can be pulled toward suboptimal spots. these points correspond to nash equilibria: suboptimal combinations of strategies where if any one strategy is changed  the net reward for both agents will decrease. 
　as a result  individuals in a ccea are not necessarily refined to be the optimal subcomponent of the optimal component; instead they are refined to be jacks-of-all-trades that dovetail nicely with the current individuals from the other population. what does this mean for practitioners wanting to coevolve  optimal   or perhaps  even  good   cooperative strategies using a coevolutionary algorithm  it means that ceas are not necessarily optimizers in the sense that one might intuitively expect them to be. something must be done to modify the existing algorithms or our expectations of what these algorithms really do. 
1 biasing for optimal cooperation 
one reason cceas tend toward  balance  is that an individual's fitness is commonly assessed based on how well it performs with immediate individuals from the other population. to find optimal cooperation  the search process may need to be more optimistic than this: assessing fitness based more on the highest-reward interactions between an individual and various members of the other population. a previous investigation in this direction is reported in  wiegand et al  1 : 
multiagent systems 


table 1: joint reward matrixes for the climb  left  and penalty  right  domains. 
assessing an individual's fitness based on its maximum performance with other agents in a collaborative domain was shown to yield better results than when using the mean or minimum performance. the idea presented in this paper is relatively simple: base an individual's fitness on a combination of its immediate reward while interacting with individuals in the population  and on an estimate for the reward it would have received had it interacted with its ideal collaborators. the fraction of reward due to the immediate  as opposed to the ideal  interaction changes during the course of the run. 
　we note that this notion of bias towards maximum possible reward has also been used in the reinforcement learning literature in subtly different ways than we use it here. for example  maximum reward was used by loaus and boutilier  1 to modify the exploration strategy of the agent  and by llauer and riedmiller  1  to modify the update rule for the q table. to some extent  the  hall of fame  method introduced by  rosin and belew  1  for competitive coevolution is also related to biased cooperative coevolution. 
we justify the use of such a bias in a ccea as follows. 
recall that if an individual's fitness is based on its immediate interaction with individuals from the other population  then u = ay and w - atx as described in equations 1 and 1. now  let us consider a function maxa that returns a column vector corresponding to the maximum value of each row in matrix a. now  if an individual's fitness is based on its maximum possible performance in conjunction with any individual from the other population  then we may modify equations 1 and 1 to be w = maxat and w = max r'. 
　in this modified system  the tendency to optimize performance is clear. at each iteration of the model  the fitness of each strategy will be its best possible fitness. if there is a unique maximum  that result will have the highest fitness and so the proportion of the corresponding strategy will increase in the next step. when the global maxima are not unique  the resulting fixed point is a mixed strategy  with weights split between those maxima. 
　the reason for this is straightforward: the problem has lost the dimensionality added due to the nature of the interactions between the agents. without this  the problem reduces to a 
　simple evolutionary algorithm: regardless of the content of the opposing population  the fitness measure for a given strategy is the same. as shown in  vose  1   an infinite population model of this reduced evolutionary algorithm will converge to a unique global maximum. 
　but it is difficult to imagine how a real ccea algorithm would know the maximum possible reward for a given individual a priori. one approach is to use historical information during the run to approximate the maximum possible collabo-

figure 1: probability of converging to the optimum as the bias parameter 1 is varied between 1 and 1. 
rative fitness for an individual. however  if the approximation is too large  or has too strong an effect on the overall fitness   and if it appears too early in the evolutionary run  then it can deform the search space to drive search trajectories into suboptimal parts of the space from which they cannot escape. on the other hand  if the approximation affects the fitness measurement very weakly  and too late in the run  then it may not be of much help  and the system will still gravitate towards  balance . 
　to better see this tradeoff  we again alter equations 1 and 1  this time adding a bias weight parameter 1. now  u =  1 -1 -i1y+1-max i1' andvt'=  1 -1 atx+1m'd*art. varying 1 between 1 and 1 will control the degree to which the model makes use of the bias. consider the climb payoff matrix on the left side of table 1. we select 1 initial points of the dynamical system uniformly at random from a  x am  and iterate the system until it converges. while convergence is virtually guaranteed in traditional two-matrix egt games lhofbauer and sigmund  1  it is not necessarily guaranteed in our modified system. in our experimental results  however  we obtained convergence in all cases to within some degree of machine precision. figure 1 shows the probability  for various levels of 1  of the dynamical system converging to the optimum when the penalty is set to -1  -1  -1 or -1. notice that  as the penalty worsens  the transition between optimal and suboptimal convergence becomes more severe. this suggests that for some problems  any benefits provided by this type of bias may be quite sensitive to the degree of bias. 
1 	experiments 
while this theoretical discussion helps justify our intuition for including a performance bias in fitness evaluation  it is not immediately applicable to real problems. in a more realistic setting  simplifying model assumptions such as infinite populations  lack of variational operators  complete mixing  and a priori knowledge of the maximum payoff are not possible. to convert theory into practice  we have adopted an approximation to the performance bias that is based on historical information gathered during the evolutionary run. we also decreased the bias through the course of a run to take advantage of the fact that initial partners are likely to be weak  while later partners are stronger. 
we performed several experiments to compare simple co-

multiagent systems 	1 


figure 1: joint reward in the continuous two peaks domain 
evolution  sc  with biased coevolution  bc  in three problem domains detailed later. both sc and bc base fitness on the immediate performance of an individual in the context of individuals from one other cooperating population. bc additionally includes a bias factor: part of the fitness is based on an approximation of what an individual's fitness would be were it to cooperate with its ideal partners. 
   we compared these two techniques in combination with two approaches to representing an individual. in the pure strategy representation  psr   an individual represented a single strategy. psr individuals stored a single integer representing the strategy in question. a psr individual bred children through mutation: a coin was repeatedly tossed  and the individual's integer was increased or decreased  the direction chosen at random beforehand  until the coin came up heads. in the mixed strategy representation  msr   an individual represented not a single strategy but a probability distribution over all possible strategies. when evaluating an msr individual with a partner agent  1 independent trials were performed  and each time each agent's strategy was chosen at random from the the agent's probability distribution. msr individuals used one-point crossover  followed by adding random gaussian noise  ju - 1  a = 1  to each of the distribution values  followed by renormalization of the distribution. observe that using msr creates a potentially more difficult problem domain than using psr  for reasons of search space size and stochasticity of the fitness result. 
   we chose a common approach to cooperative coevolution fitness assessment. an individual is assessed twice to determine fitness: once with a partner chosen at random  then once partnered with the individual in the other population that had received the highest fitness in the previous generation. an individual's fitness is set to the maximum of these two assessments. during a fitness assessment  an individual receives some number of rewards for trying certain strategies in the context of partners. for a psr individual  the assessment was simply the single reward it received for trying its strategy with its partners. as an msr individual tried fifty strategies  its assessment was the mean of the fifty rewards it received. 
   sc and bc differ in that bc adds into the reward a bias term  that is  reward  -  1 - 1    reward 1- 1   maxreward  where 1 is a decreasing bias rate that starts at 1 and linearly decreases until it reaches 1 when 1 of the maximal run length has passed. ideally  the maxreward bias factor would be the highest possible reward received for trying that partic-

table 1: proportion of runs that converged to global optimum and average best individual fitness  climbing domain 
penalty 

table 1: proportion of runs that converged to global optimum and average best individual fitness  penalty domain 
ular strategy  over all possible partner strategies. in the experiments in this paper  we chose to approximate maxreward by setting it to the maximum reward seen so far in the run for the given strategy. 
   in all experiments  the most fit individual survived automatically from one generation to the next. to select an individual for breeding  we chose two individuals at random with replacement from the population  then selected the fitter of the two. each experiment was repeated 1 times. the experiments used the ecj1 software package  luke  1 . 
1 	problem domains 
we experimented with three different single-stage game domains: two simpler ones  climb and penalty  introduced in  claus and boutilier  1   and a more complex artificial problem  two peaks . evolutionary runs in the climb and penalty problem domain lasted 1 generations and used 1 individuals per population. runs in the two peaks domain lasted 1 generations and used populations of 1 individuals each. 
   the joint reward matrices for the climb and the penalty domains are presented in table 1. the domains are difficult because of the penalties associated with miscoordinated actions and the presence of suboptimal collaborations that avoid penalties. figure 1 presents a continuous version of a the two peaks coordination game  where the x and y axes represent the continuous range of actions for the two agents  and the z axis shows the joint reward. the reward surface has two peaks  one lower but spread over a large surface  and the other one higher but covering a small area. because an agent's strategy space is continuous over  1   we discretized it into increasingly difficult sets of 1  1  1  1 or 1 strategies. the discretizations result in slightly different optimal values. 
1 	results 
tables 1 present the proportion  out of 1 runs  that converged to the global optimum  plus the mean fitness of the best individuals in the runs. msr individuals were considered optimal if and only if the optimal strategy held over 1 

1 	multiagent systems 


table 1: proportion of runs that converged to global optimum and average best individual fitness  two peaks domain 

figure 1: distance from best-of-generation individuals to optimal strategy for the 1 actions two peaks domain using sc  top  and bc  bottom . 
percent of the distribution  in fact  most optimal msr individuals had over 1 percent . 
　biased coevolution consistently found the global optima as often as  or more often than  standard coevolution. the only times where standard coevolution held its own was in the climbing and penalty domains  where psr individuals found the optimum 1% of the time  as well as in the harder two peaks domain  where no msr individuals found the optimum. for those problems when individuals found the optimum less than 1% of the time  we also compared differences in mean best fitness of a run  using a two-factor anova with repetitions  factored over the method used and the problem domain. 
　the anova results allow us to state with 1% confidence that biased coevolution is better than simple coevolution when msr is used in the climbing domain  and also in the two peaks domain when psr is used; the tests give only a 1% confidence for stating that bc+msr is better than sc+msr in the penalty domain. 
　in order to better understand what happens when using msr in the two peaks domains  we plotted the average euclidian distance from the best individual per generation to the known global optima  figures 1 and 1 . the graphs present the 1% confidence interval for the mean of the fitnesses. investigations showed that sc converged to suboptimal interactions  the lower  wider peak in figure 1  in all cases. on the other hand  the trajectories of the search process are radically different when using bc. let's take a closer look as to why this might be so. 
as we learned from our discussion surrounding figure 1  

generations 
figure 1: distance from best-of-generation individuals to optimal strategy for the 1 actions two peaks domain using sc  top  and bc  bottom . 
successful applications of this biasing method are tied to successfully determining the appropriate degree of bias to apply. due to msr's increased difficulty  it may be more challenging to find an appropriate balance for the bias. figure 1 suggests exactly this. notice that  in the early part of the run  when 1 is strong   the algorithm tends towards the optimal solution; however  as the bias is reduced  it becomes overwhelmed and the trajectories are eventually drawn toward the suboptimal local attractor. moreover  as the problem becomes larger  i.e.  figure 1  as well as others not shown   this failure occurs earlier in the run. this suggests more careful attention is needed to set the parameters and to adjust the bias rate when using msr versus psr. indeed  by running longer and allowing for more interactions during evaluation  we were able to obtain convergence to the global optimum when using msr  not shown . 
1 	conclusions and future work 
although cooperative coevolution has been successfully applied to the task of learning multiagent behaviors  as research about these algorithms advances  it becomes increasingly clear that these algorithms may favor stability over optimality for some problem domains. in this paper  we develop a very simple idea: improve coevolution through the use of a maximum reward bias. we introduce a theoretical justification for the idea  then present experimental evidence that confirms that biasing coevolution can yield significantly better results than standard coevolution when searching for optimal collaborations. our work further reveals that domain features greatly influence the levels of biasing necessary for convergence to optima: for some problems the performance changes slowly when the level of bias is modified  while for other domains there is a rapid degradation in results. this suggests that  while adding some kind of maximum reward bias can be helpful  there is still work to be done in understanding how best to apply this bias in different problem domains. 
　our initial experimental results in this paper suggest that it is effective to use a history as an approximation to the true maximal collaborative reward for a given strategy. for future 

multiagent systems 	1 

work we intend to extend these experiments to problem domains with search spaces much larger than the ones used in these experiments. in such domains  the number of strategies may be very large  even infinite. keeping an effective history of strategies may thus be infeasible in certain circumstances; we intend to explore ways to sample the space or cache the most significant strategy results. repeated games  such as the iterated prisoner's dilemma  or stochastic games  may also require different approaches to biasing coevolution. understanding these issues  we hope  can lead to significant improvements in cooperative revolution's effectiveness as a multi-agent optimization technique. 
1 	acknowledgements 
this research was partially supported through department of army grant daab1-1-l1  onr grant n1-1  and a gift from sra international. 
