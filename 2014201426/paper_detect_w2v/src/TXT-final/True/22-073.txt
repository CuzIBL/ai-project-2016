 
nilsson's probabilistic logic is a set theoretic mechanism for reasoning with uncertainty. we propose a new way of looking at the probability constraints enforced by the framework  which allows the expert to include conditional probabilities in the semantic tree  thus making probabilistic logic more expressive. an algorithm is presented which will find the maximum entropy point probability for a rule of entailment without resorting to solution by iterative approximation. the algorithm works for both the propositional and the predicate logic. also presented are a number of methods for employing the conditional probabilities. 
1. introduction 
   a recent trend in reasoning with uncertainty has been to move away from representing the uncertainty of a sentence with a point probability  towards more complex mechanisms. most notably with 
probabilistic 	logic 
 nilsson 1a  grosofl1a  guggcnheimer 1a   incidence calculus  bundy 1a  bundyl1b   and stochastic simulation 
 pearl 1a . all of these systems involve explicit knowledge of possible world scenarios. in incidence calculus  the probability of a 
sentence is based on a sample space of points. each of these points can be regarded as a possible world. in pearl's stochastic simulation probabilities of events are computed by recording the fraction of time that events occur in a random series of scenarios generated from some causal model. probabilistic logic is a generalisation of the ordinary true-false semantics for logical sentences to a semantics that allows sentences to be uncertain  and consequently to have more than one possible state. 
   the consequences of their set-theoretic nature leaves these systems prey to complexity problems in space and time. bundy's legal assignment finder  which finds all the legal specialisations of an initial probability assignment is at least exponential. the number of runs it takes to approximate correct probability values in the stochastic simulator is of the same order  as is entailment inside probabilistic logic. nilsson reports that implementation of the full procedure for probabilistic entailment would usually be impracticable. 
   the maximum entropy  levine 1a  bard 1a  principle also needs knowledge of all the possible states of uncertain information  and in this respect it is related to the possible world listed above  and shares the same complexity problems. 
   these methods form part of what appears to be a formidable family of conceptually compelling theories of reasoning with uncertainty which suffer from the same problem: intractibility. this paper addresses this problem for nilsson's probabilistic logic  and discusses it's use of the maximum entropy method. it is the coupling of this method to the semantic framework of probabilistic logic which is at the core of this paper. the system produced is very fast  and allows the expert to use conditional probabilities in designing the statistical distribution. 
1. entropy 
   entropy  harris 1a   is a statistical term which has evolved from a study of thermodynamics  1a . it is related to the probability of a thermodynamic system being in a given state as related to the number of different molecular configurations that the system can assume in that state. since in general a system changes spontaneously toward a more probable stale  the entropy increases accordingly. equilibrium  or maximum entropy  is the state in which the molecules can occupy the greatest number of configurations. 
   more formally  the entropy of the probability mass function px x  may be regarded as a descriptive quantity  just as the median  mode  variance and coefficient of skewness may be regarded as descriptive parameters. the entropy of a distribution is a measure of the extent to which the probability is concentrated on a few points or dispersed over many. it is an expression of the degree of disorder of a system. 

   in the examples here we will use 1 as the logbase; although any base can actually be used  harrisl1a . 
example 
   we are provided with four coins and told that one of the coins is counterfeit below there are four probability distributions given  where the coins are labelled 1 to 1  and pn represents the probability that coin n is the counterfeit coin. the distributions are labelled dl 

to d1  and the entropy is labelled h. 
1 	automated deduction 


   in this case  the distribution with maximum entropy is dl. the reduction in entropy from dl to d1 demonstrates the effect of having more information about the change in probabilistic likelihood of one of the coins over the others. d1 represents the case where there is no uncertainty as to which coin is counterfeit. 
   information is embodied in each of the distributions  and we can see that the distribution which says least about the identity of the counterfeit coin is dl. this equation of information with entropy leads to the maximum entropy principle: of all probability distributions which satisfy the constraints imposed by the known aggregate probabilities  choose the one which has the maximum entropy or  equivalently  contains the least information. 
1. probabilistic entailment and context 
   nilsson defines probabilistic entailment as an analogue of logical entailment in classical logic. the rule of modus ponens allows us to use the set  a1 a1 = b  to deduce  b . when we have uncertainty about whether or not a1 or a1= b is true  the real world  which has the real value of b  becomes a random variable  and can be one of a number of possible states. these states  possible worlds  can be produced mechanically by an exhaustive theorem prover 
 changl1a   and the collected group represented called a semantic tree. in conventional set theoretic terms  this set of all possibilities is the universal set. in statistical terms  this set is called the sample space or possibility space. 
   as an example  nilsson uses the set  a1  a1= b  to estimate the probability of logically entailed sentence b. a complete interpretation table for the worlds which form the base set for the inference is: 

the possible worlds are labelled with small letters a  b and c. 
each possible world must eventually be assigned a non-zero probability such that if the probability of a sentence s is s  and s is true in worlds a and b  then p a  + p b  = s. the tautology t is true in all possible worlds and is included in the set to ensure that all the probabilities sum to 1. 
   structurally  world c in this example is the world which causes concern. nilsson presented the states for the semantic tree as: 

   the reasoning behind this being that in worlds a and b  b can only assume one logical value  1 and 1 respectively. but in world c  b can logically assume either of the values 1 or 1. hence  in figure 
1  c represents the world where b is false  and d represents the world where b is true. 
   however  in the semantic tree for figure 1 there is no way of distinguishing between the worlds c and d  because they are the same world: i.e. where a 1 is false and the rule a 1= b is true. figure 1 also imposes an unnecessary condition on the relationship between the possible worlds  -a1 a1= b  b  and  -ia1= b  -.b   namely that they have the same probability. in this sense  figure 1 incorporates information into our reasoning process which is not necessarily true. 
   in appendix b we show that 1 +l possible worlds are created from the tree in figure 1  where n is the number of propositions in the antecedent list of the rule. effectively  we are left with n equations and 1n possible worlds to solve for. one way to remove the additional degrees of freedom is to maximise the entropy of the system. bard  bardl1a  bardl1a  presents examples which employs the notion of a semantic tree  and which illustrates the following solution methods with clear examples. 
   each possible world is rewritten in terms of a multiplication of factors  bardl1a  cheeseman 1a  nilsson 1a   where an unknown factor is associated with each of the sentences in the database. we shall use the following notation  with a1 representing the factor for the tautology; and the factors aj being associated with proposition j; and factor ar being associated with the rule. we include the factor in the multiplication list for a possible world only if the world has a one in the corresponding row of the semantic tree. so  in figure 1  we have: 

	kane 	1 


1 	automated deduction 


figure 1: algorithm for rapid calculations of aggregate factors. 
   once these aggregate factors are found for any consistent probability problem  the possible worlds can be rebuilt from the appropriate multiplication of factors. not only will we have the probability of a conclusion but also a detailed breakdown of the probabilities of the contributing possible worlds. 
   the proof that this algorithm is correct is inductive  and can be found in appendix a. for the case where the probability of the rule is one  we use the same reasoning on a simplified version of the semantic tree. 
1. using context splits 
we are now in a position to use conditional probabilities in the 
probabilistic logic in the fonn of the context splits. obviously  if the expert provides all of the context splits  probabilistic logic is now able to produce point probabilities from the probabilistic entailment. if the expert wishes to specify all of these for a rule of the form   the system requires: 
1. the probabilistic strength of the rule 
1. 1n-l context splits. 
figure 1: information the expert can provide. 
   as the number n increases  providing reliable context splits will become impracticable. this section suggests some mechanisms for dealing with this problem. 
1/. if we assume that the probability of b and ~b is the same in each split context  then the probability of the conclusion is simply: 
 equation 1 
this method of assigning probability gives a very quick result. 
1/. we can assign a contextual weight of 1/n to each of the sentences in the rule. e.g. for the case of n = 1  the contextual weight to each sentence is 1  and for the tree: 

1/. another method is to get the expert to assign contextual weights to each of the propositions aj  to am as an indicator of how much the entailment of the conclusion depends on each of the individual ai . 
so  lor example  in the rule given  the expert may assign weights: 

   this system of probabilistic logic gives the expert all the necessary tools to fully design a subjective probability distribution to describe their level of expertise. all three of the above methods have been implemented in prolog and may conveniently be used in meta-interprctcd expert systems  sterling 1a  for deducing maximum entropy point probabilities from uncertain information and uncertain rules of inference. a fully operational expert system using these mechanisms is described in  kane 1a . 
1. combination problem 
   we have presented a completely sound method of providing the maximum entropy result from a probabilistic rule of inference  within the constraints of consistency. all of these rules may exist in a database independently of each other and be called on only when needed. a problem which arises is how best to combine the results of two reasoning processes  both of which it is consistent to fire  and both of which entail the same conclusion  
   one solution would be to join the two rules together using the logical or-operator  since the conclusion can be entailed from either of the rules   and join the two probabilities together using the maximum entropy principle. 
example 
the rules are: 

1. conclusion 
   what we have presented is a manner of getting a point estimate from a probabilistic rule of entailment using the maximum entropy distribution. the equations are not solved iteratively  and consequently  the result is achieved very quickly. this result has more mathematical significance than a point produced by the mycin inference mechanism  and can be computed just as quickly. 
	kane 	1 

the system as presented might be thought of as a probabilistic prolog  and is fully operational. 
   we have implemented these ideas in an expert system for vision recognition of 1-dimensional objects  and the system works succesfully. 
acknowledgement 
dedicated to my dad  who showed me perseverance. with thanks to 
heriot-watt university's computer science department for help and encouragement. particular thanks to steven salvini  patrick mcandrew  andrew wallace  hunter davis; and the reviewers for pertinent suggestions. 
appendix a: proof of algorithm by induction 
the proof proceeds in four stages. first  to derive the expression for the world where the rule is false. second  to show that for each of the terms  there is a direct match of terms on the numerator and denominator of the expression: 
i.e. all the unknown worlds where sentence aj  is true divided by all the worlds where aj  is false. the third stage is related to the first and allows us to solve for at. the fourth stage is for the final factor ar and is based on the worlds in which the rule is true  and a recursive expression for describing the contribution of each of the possible worlds to this probability: 
base case 
1/. in each row there are now 1n+1 possible worlds  where there used to be 1n the difference between the tree for n+1 propositions and n propositions  being that in row n+1 there are now 1  l's and 1  1 s  and the rule is pushed down to position n+1. 
for the half of the tree with 1's in row n+1 we proved that there is a direct match to give each of the previous ai 's. for the other half  we use the same enumeration  and find that the factor for proposition n+1 cancels out on top and bottom. furthermore the numerator still only holds the worlds where sentence aj is true  and the denominator the worlds where a} is false. therefore the equation still holds. 
is the formula true for new row n+1   
the new tree was made up of two identical copies of the old tree  one of which has a 1 in row n+1  the other of which has a zero in row n+1. consequently  again it is possible to cancel the terms of the true worlds divided by the false worlds so that there is only a 
factor of a  n+1  left. 
1/. the expression for all the worlds in terms of the factors for n 

when we include the new row  we have a new multiplicative factor: we have two copies: one with an an+1 in row n+1  and one with a 1. 
so the new expression for all the worlds is: 
the proof is a generalisation 


and the above equations satisfy the algorithm with n = 1. 
step 
algorithm is true for n  prove true for n+1. 
the new premise a n+1  is added to the antecedent arm of the rule  and placed after premise an in the premise list. we now have aggregate factors 
1 	automated deduction 

inclusion of b in the set  would produce a contradiction. 

bundy 1a. a. bundy  incidence calculus: a mechanism for probabilistic reasoning  1  pp. 1   apr 1 . 

case 1: at least one of aa1  an false  and rule true. the literals in the rule cannot all be resolved away from the premises  and so no statement can be made about b from the rule. consequently  either b on ~b will be consistent with the set. the number of worlds produced is 1 ~1.  i.e. only removing the all true world from the list of posiblitics.  
there is no analogous case for case 1 where the rule is false. this is because  the rule will split into n+1 clauses  with a1  an all true; and b false. consequently  if any of the a1  an premises are false  a contradiction is immediately produced. 

