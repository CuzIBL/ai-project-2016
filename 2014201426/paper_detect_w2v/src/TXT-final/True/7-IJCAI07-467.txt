
topic-focused multi-document summarization aims to produce a summary biased to a given topic or user profile. this paper presents a novel extractive approach based on manifold-ranking of sentences to this summarization task. the manifold-ranking process can naturally make full use of both the relationships among all the sentences in the documents and the relationships between the given topic and the sentences. the ranking score is obtained for each sentence in the manifold-ranking process to denote the biased information richness of the sentence. then the greedy algorithm is employed to impose diversity penalty on each sentence. the summary is produced by choosing the sentences with both high biased information richness and high information novelty. experiments on duc1 and duc1 are performed and the rouge evaluation results show that the proposed approach can significantly outperform existing approaches of the top performing systems in duc tasks and baseline approaches.
1 introduction
multi-document summarization aims to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. topic-focused multi-document summarization is a particular kind of multi-document summarization. given a specified topic description  i.e. user profile  user query   topic-focused multi-document summarization  i.e. query-based multi-document summarization  is to create from the documents a summary which either answers the need for information expressed in the topic or explains the topic.
　automatic multi-document summarization has drawn much attention in recent years and it exhibits the practicability in document management and search systems. multi-document summary can be used to concisely describe the information contained in a cluster of documents and facilitate the users to understand the document cluster. for example  a number of news services  such as google news1  newsblaster1  have been developed to group news articles into news topics  and then produce a short summary for each news topic. the users can easily understand the topic they have interest in by taking a look at the short summary. topic-focused summary can be used to provide personalized services for users after the user profiles are created manually or automatically. the above news services can be personalized by collecting users' interests  and both the retrieved related news articles and the news summary biased to the user profile are delivered to the specified user. other examples include question/answer systems  where a question-focused summary is usually required to answer the information need in the issued question.
　the challenges for topic-focused multi-document summarization are as follows: the first one is a common problem for general multi-document summarization  that the information stored in different documents inevitably overlaps with each other  and hence we need effective summarization methods to merge information stored in different documents  and if possible  contrast their differences; the second one is a particular challenge for topic-focused multi-document summarization that the information in the summary must be biased to the given topic  so we need effective summarization methods to take into account this topic-biased characteristic during the summarization process. in brief  a good topic-focused summary is expected to preserve the information contained in the documents as much as possible  and at the same time keep the information as novel as possible  and moreover  the information must be biased to the given topic. in recent years  a series of workshops and conferences on automatic text summarization  e.g. ntcir1  duc1   special topic sessions in acl  coling  and sigir have advanced the technology and produced a couple of experimental online systems.
　in this study  we propose a novel extractive approach based on manifold-ranking  zhou et al.  1a; zhou et al.  1b  of sentences to topic-focused multi-document summarization. the proposed approach first employs the manifold-ranking process to compute the manifold-raking score for each sentence that denotes the biased information richness of the sentence  and then uses the greedy algorithm to penalize the sentences highly overlapping with other informative sentences. the summary is produced by choosing the sentences with highest overall scores  which are deemed both informative and novel  and highly biased to the given topic. in the manifold-ranking algorithm  the intra-document and inter-document links between sentences are differentiated with different weights. experimental results on two duc tasks show that the proposed approach significantly outperforms the top performing approaches in duc tasks and baseline approaches.
　in the rest of this paper: section 1 discusses previous work. the proposed summarization approach is proposed in section 1. section 1 describes the evaluation results. section 1 presents our conclusion and future work.
1 previous work
a variety of multi-document summarization methods have been developed recently. generally speaking  the methods can be either extractive summarization or abstractive summarization. extractive summarization involves assigning salience scores to some units  e.g. sentences  paragraphs  of the documents and extracting the sentences with highest scores  while abstraction summarization  e.g. newsblaster  usually needs information fusion  sentence compression and reformulation. in this study  we focus on extractive summarization.
　the centroid-based method  radev et al.  1  is one of the most popular extractive summarization methods. mead is an implementation of the centroid-based method that scores sentences based on such features  as cluster centroids  position  tf*idf. neats  lin and hovy  1  uses sentence position  term frequency  topic signature and term clustering to select important content  and use mmr  goldstein et al.  1  to remove redundancy. xdox  hardy et al.  1  first identifies the most salient themes within the document set by passage clustering and then composes an extraction summary  which reflects these main themes. harabagiu and lacatusu  investigate five different topic representations and introduce a novel representation of topics based on topic themes. recently  graph-based methods have been proposed to rank sentences or passages. websumm  mani and bloedorn  1   lexpagerank  erkan and radev  1  and mihalcea and tarau  are three such systems using algorithms similar to pagerank and hits to compute sentence importance.
　most topic-focused document summarization methods incorporate the information of the given topic or query into generic summarizers and extracts sentences suiting the user's declared information need. in  saggion et al.  1   a simple query-based scorer by computing the similarity value between each sentence and the query is incorporated into a generic summarizer to produce the query-based summary. the query words and named entities in the topic description are investigated in  ge et al.  1  and classy  conroy and schlesinger  1  for event-focused/query-based multi-document summarization. in  hovy et al.  1   the important sentences are selected based on the scores of basic elements  be . cats  farzindar et al.  1  is a topic-oriented multi-document summarizer which first performs a thematic analysis of the documents  and then matches these themes with the ones identified in the topic. more related work can be found on duc 1 and duc 1 publications.
　to the best of our knowledge  the above systems are usually simple extensions of generic summarizers and do not uniformly fuse the information in the topic and the documents. while our approach can naturally and simultaneously take into account that information in the manifold-ranking process and select the sentences with both high biased information richness and information novelty.
1 the manifold-ranking based approach
1 overview
the manifold-ranking based summarization approach consists of two steps:  1  the manifold-ranking score is computed for each sentence in the manifold-ranking process where the score denotes the biased information richness of a sentence;  1  based on the manifold-ranking scores  the diversity penalty is imposed on each sentence and the overall ranking score of each sentence is obtained to reflect both the biased information richness and the information novelty of the sentence. the sentences with high overall ranking scores are chosen for the summary. the definitions of biased information richness and information novelty are given as below:
　biased information richness: given a sentence collection χ={xi | 1in} and a topic t  the biased information richness of sentence xi is used to denote the information degree of the sentence xi with respect to both the sentence collection and t  i.e. the richness of information contained in the sentence xi biased towards t.
　information novelty: given a set of sentences in the summary r={xi | 1im}  the information novelty of sentence xi is used to measure the novelty degree of information contained in the sentence xi  with respect to all other sentences in the set r..
　the underlying idea of the proposed approach is that a good summary is expected to include the sentences with both high biased information richness and high information novelty.
1 manifold-ranking process
the manifold-ranking method  zhou et al.  1a; zhou et al.  1b  is a universal ranking algorithm and it is initially used to rank data points along their underlying manifold structure. the prior assumption of manifold-ranking is:  1  nearby points are likely to have the same ranking scores;  1  points on the same structure  typically referred to as a cluster or a manifold  are likely to have the same ranking scores. an intuitive description of manifold-ranking is as follows: a weighted network is formed on the data  and a positive rank score is assigned to each known relevant point and zero to the remaining points which are to be ranked. all points then spread their ranking score to their nearby neighbors via the weighted network. the spread process is repeated until a global stable state is achieved  and all points obtain their final ranking scores.
　in our context  the data points are denoted by the topic description and all the sentences in the documents. the manifold-ranking process in our context can be formalized as follows:
　given a set of data points χ={x1 x1 ... xn} rm   the first point x1 is the topic description and the rest n points are the sentences in the documents. note that because the topic description is usually short in our experiments and we treat it as a pseudo-sentence1  and then it can be processed in the same way as other sentences. let f :χ★ r denote a ranking function which assigns to each point xi  1in  a ranking value fi. we can view f as a vector f= f1 ... fn t. we also define a vector y= y1 ... yn t  in which y1 because x1 is the topic sentence and yi=1  1in  for all the sentences in the documents. the manifold ranking algorithm goes as follows:

1. compute the pair-wise similarity values between sentences  points  using the standard cosine measure. the weight associated with term t is calculated with the tft*isft formula  where tft is the frequency of term t in the sentence and isft is the inverse sentence frequency of term t  i.e. 1+log n/nt   where n is the total number of sentences and nt is the number of the sentences containing term t. given two sentences  data points  xi and xj  the cosine similarity is denoted as sim xi xj   computed as the normalized inner product of the corresponding term vectors.
1. connect any two points with an edge if their similarity value exceeds 1. we define the affinity matrix w by wij=sim xi xj  if there is an edge linking xi and xj. note that we let wii=1 to avoid loops in the graph built in next step.
1. symmetrically normalize w by s=d-1wd-1 in which d is the diagonal matrix with
 i i -element equal to the sum of the i-th row of w.
1. iterate f t+1 = sf t + 1- y. until convergence  where  is a parameter in  1 .
1. let fi* denote the limit of the sequence {fi t }. each sentences xi  1in  gets its ranking score fi*.

1
 the topic can also be represented by more than one sentence  and in this case only the vector y needs to be modified to represent all the topic sentences in the manifold-ranking algorithm.
　in the above iterative algorithm  the normalization in the third step is necessary to prove the algorithm's convergence. the fourth step is the key step of the algorithm  where all points spread their ranking score to their neighbors via the weighted network. the parameter of manifold-ranking weight  specifies the relative contributions to the ranking scores from neighbors and the initial ranking scores. note that self-reinforcement is avoided since the diagonal elements of the affinity matrix are set to zero.
　the theorem in  zhou et al.  1b  guarantees that the sequence {f t } converges to
	f * =β i  αs  1 y	 1 
where =1-. although f* can be expressed in a closed form  for large scale problems  the iteration algorithm is preferable due to computational efficiency. usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any point falls below a given threshold  1 in this study .
　note that in our context  the links  edges  between sentences in the documents can be categorized into two classes: intra-document link and inter-document link. given a link between a sentence pair of xi and xj  if xi and xj come from the same document  the link is an intra-document link; and if xi and xj come from different documents  the link is an inter-document link. the links between the topic sentence and any other sentences are all inter-document links. we believe that intra-document links and inter-document links have unequal contributions in the above iterative algorithm. in order to investigate this intuition  distinct weights are assigned to the intra-document links and the inter-document links respectively. in the second step of the above algorithm  the affinity matrix w can be decomposed as
	w =wintra +winter	 1 
where wintra is the affinity matrix containing only the intra-document links  the entries of inter-document links are set to 1  and winter is the affinity matrix containing only the inter-document links  the entries of intra-document links are set to 1 .
　we differentiate the intra-document links and inter-document links as follows:
	~	 1 
w =λ1wintra +λ1winter
　we let   1  in the experiments. if 1  the inter-document links are more important than the intra-document links in the algorithm and vice versa. note
that if 1=1  equation  1  reduces to equation  1 . in
　　　　　　　　　　　　　　　~ the manifold-ranking algorithm  w is normalized into ~
s in the third step and the fourth step uses the follow-
                         ~ ing iteration form: f t+1 =  s f t + 1- y.
1 diversity penalty imposition
the original affinity matrix w is normalized by s =d-1w to make the sum of each row equal to 1. based on s   the greedy algorithm similar to  zhang et al.  1  is applied to impose the diversity penalty and compute the final overall ranking scores  reflecting both the biased information richness and the information novelty of the sentences. the algorithm goes as follows:

1. initialize two sets a=   b={xi | i=1 ... n}  and each sentence's overall ranking score is initialized to its manifold-ranking score  i.e.
rankscore xi  = fi*  i=1 ...n.
1. sort the sentences in b by their current overall ranking scores in descending order.
1. suppose xi is the highest ranked sentence  i.e. the first sentence in the ranked list. move sentence xi from b to a  and then the diversity penalty is imposed to the overall ranking score of each sentence linked with xi in b as follows: for each sentence xj （b  rankscore xj   = rankscore xj   fi* where  1 is the penalty degree factor. the larger  is  the greater penalty is imposed to the overall ranking score. if =1  no diversity penalty is imposed at all.
1. go to step 1 and iterate until b=   or the iteration count reaches a predefined maximum number.

figure 1: the algorithm of diversity penalty imposition.
　in the above algorithm  the third step is crucial and its basic idea is to decrease the overall ranking score of less informative sentences by the part conveyed from the most informative one. after the overall ranking scores are obtained for all sentences  several sentences with highest ranking scores are chosen to produce the summary according to the summary length limit.
1 experiments
1 data set
topic-focused multi-document summarization has been evaluated on tasks 1 and 1 of duc 1 and the only task of duc 1  each task having a gold standard data set consisting of document clusters and reference summaries. in our experiments  task 1 of duc 1 was used for training and parameter tuning and the other two tasks were used for testing. note that the topic representations of the three topic-focused summarization tasks are different: task 1 of duc 1 is to produce summaries focused by events; task 1 of duc 1 is to produce summaries focused by viewpoints; the task of duc 1 is to produce summaries focused by duc topics. in the experiments  the above topic representations were treated uniformly because they were deemed to have no substantial differences from each other. table 1 gives a short summary of the three data sets.
　as a preprocessing step  dialog sentences  sentences in quotation marks  were removed from each document. the stop words in each sentence were removed and the remaining	words	were	stemmed	using	the	porter's stemmer1.
duc 1duc 1duc 1tasktask 1task 1the only tasknumber of clusters11data sourcetdttrectrecsummary length1 words1 words1 wordstable 1: summary of data sets used in the experiments.
1 evaluation metric
we used the rouge  lin and hovy  1  toolkit1 for evaluation  which was adopted by duc for automatically summarization evaluation. it measures summary quality by counting overlapping units such as the n-gram  word sequences and word pairs between the candidate summary and the reference summary. rouge-n is an n-gram recall measure computed as follows:
‘ ‘countmatch n  gram 
rouge   n = s（{ re f sum} n-gram s（	 1  ‘ ‘count n   gram 
s（{ re f sum} n-gram s（
where n stands for the length of the n-gram  and countmatch n-gram  is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries. count n-gram  is the number of n-grams in the reference summaries.
　the rouge toolkit reports separate scores for 1  1  1 and 1-gram  and also for longest common subsequence co-occurrences. among these different scores  unigram-based rouge score  rouge-1  has been shown to agree with human judgment most  lin and hovy  1 . we show three of the rouge metrics in the experimental results  at a confidence level of 1%: rouge-1  unigram-based   rouge-1  bigram-based   and rouge-w  based on weighted longest common subsequence  weight=1 .
　in order to truncate summaries longer than length limit  we used the  -l  option in the rouge toolkit and we also used the  -m  option for word stemming.
1 experimental results
1.1 system comparison
in the experiments  the proposed approach was compared with top three systems and four baseline systems on task 1 of duc 1 and the only task of duc 1 respectively. the top three systems are the systems with highest rouge scores  chosen from the performing systems on each task respectively. the lead baseline and coverage baseline are two baselines employed in the topic-focused multi-document summarization tasks of duc 1 and 1. the lead baseline takes the first sentences one by one in the last document in the collection  where documents are assumed to be ordered chronologically. and

1 http://www.tartarus.org/martin/porterstemmer/
1
   we use rougeeval-1.1 downloaded from http://haydn.isi.edu/ rouge/
the coverage baseline takes the first sentence one by one from the first document to the last document.
　in addition to the two standard baseline systems  we have implemented two other baseline systems  i.e. similarity-ranking1 and similarity-ranking1. the similarity-ranking1 first computes the similarity between the topic description and each sentence in the documents  and then the greedy algorithm proposed in section 1 is employed to impose the diversity penalty on each sentence  with the normalized similarity value as the initial overall ranking score. the sentences with highest overall ranking scores are chosen to produce the summary. in essence  the similarity-ranking1 can be considered as a simplified version of the proposed manifold-ranking based system by ignoring the relationships between the sentences in the documents. and the similarity-ranking1 does not employ the diversity penalty imposition process and simply ranks the sentences by their similarity value with the topic description  which can be considered as a simplified version of similarity-ranking1 without the step of imposing diversity penalty.
　tables 1 and 1 show the system comparison results on the two tasks respectively. in the tables  s1-s1 are the system ids of the top performing systems  whose details are described in duc publications. the manifold-ranking adopts the proposed approach described in section 1. the parameters of the manifold-ranking are set as follows: =1  1.1 and 1  =1. and the only parameter of the similarity-ranking1 is set as =1.
systemrouge-1rouge-1rouge-wmanifold-ranking111similarity-ranking1.1.1.1s1.1.1.1similarity-ranking1.1.1.1s1.1.1.1s1.1.1.1coverage baseline111lead baseline111table 1: system comparison ontask 1 of duc 1.systemrouge-1rouge-1rouge-wmanifold-ranking111s1.1.1.1s1.1.1.1similarity-ranking1.1.1.1s1.1.1.1similarity-ranking1.1.1.1coverage baseline111lead baseline111table 1: system comparison on the task of duc 1.
　seen from tables 1 and 1  the proposed system  i.e. manifold-ranking  outperforms the top performing systems and all baseline systems on all three tasks over all rouge scores1. the high performance achieved by the manifold-ranking benefits from the following factors:
　1  manifold-ranking process: the manifold-ranking process in the proposed approach makes full use of the inter-relationships between sentences by spreading the rank scores. in comparison with the similarity-ranking1  the rouge-1 scores of the proposed approach increase by 1 and 1 on the two tasks  respectively.
　1  diversity penalty imposition: if the proposed approach does not impose diversity penalty on sentences  i.e. =1   the rouge-1 scores will decrease by 1 and 1 on the two tasks  respectively. we can also see for the tables that the similarity-ranking1 much outperforms the similaity-ranking1 because of imposing diversity penalty on sentences.
　1  intra-document/inter-document link differentiation: if the proposed approach does not differentiate the intra-document and inter-document links between sentences  i.e. 1=1   the rouge-1 scores will slightly decrease by 1 and 1 on the two tasks  respectively.
　in next sections we will mainly show rouge-1 performance due to page limit.
1.1 parameter tuning
figure 1 demonstrates the influence of the penalty factor  in the proposed approach  i.e. manifold-ranking  and the baseline approach  i.e. similarity-ranking1  when 1:
1.1 and =1. we can see that when  varies from 1 to 1  the performances of the manifold-ranking are always better than the corresponding performances of the similarity-ranking1 on the two tasks  respectively. this verifies that the use of the relationships between the sentences of the documents in the proposed approach can benefit the summarization task. it is also clear that no diversity penalty and too much diversity penalty will deteriorate the performances.
　figure 1 demonstrates the influence of the intra-document/inter-document link differentiating weight 1 in the proposed approach when =1 and =1. 1 and 1 range from 1 to 1 and 1: 1 denotes the real values 1 and 1 are set to. different 1: 1 gives different contribution weights to the intra-document links and the inter-document links. it is observed that when more importance is attached to the intra-document links  i.e. 1 =1 and 1.1   the performances decrease evidently. it is the worst case when inter-document links are not taken into account  i.e. 1: 1:1   however  when intra-document links are not taken into account  i.e. 1: 1:1   the performances are still very well  which demonstrates that inter-document links are more important than intra-document links for the summarization task. figure 1 demonstrates the influence of the manifold weight  in the manifold-ranking algorithm of the proposed approach when =1  1=1:1.
