 
data mining can be understood as a process of extraction of knowledge hidden in very large data sets. often data mining techniques  e.g. discretization or decision tree  are based on searching for an optimal partition of data with respect to some optimization criterion. in this paper  we investigate the problem of optimal binary partition of continuous attribute domain for large data sets stored in relational data bases  rdb . the critical for time complexity of algorithms solving this problem is the number of simple sql queries like select count from ... where attribute between ... 
 related to some interval of attribute values  necessary to construct such partitions. we assume that the answer time for such queries does not depend on the interval length. using straightforward approach to optimal partition selection  with respect to a given measure   the number of necessary queries is of order o n   where n is the number of preassumed partitions of the searching space. we show some properties of considered optimization measures  that allow to reduce the size of searching space. moreover  we prove that using only o logiv  simple queries  one can construct the partition very close to optimal. 
1 	introduction 
the problem of searching for optimal partitions of real value attributes  features   defined by so called cuts  has been studied by many authors  see e.g. {catlett  1; chmielewski and grzymala-busse  1; dougherty at of.  1; fayyad and irani  1; liu and setiono  1; quinlan  1; nguyen and skowron  1  . the main goal is to discover knowledge in the form of cuts which can be used to synthesize decision trees or decision rules of high quality with respect to some quality measures  e g. quality of classification of new unseen objects  quality defined by the decision tree height  support and confidence of decision rules . in general  all those problems are hard from computational point of view  e.g. 
1 	machine learning 
it has been shown in  nguyen and skowron  1  that the searching problem for minimal and consistent set of cuts is np-hard . hence numerous heuristics have been developed searching for approximate solutions of these problems. these heuristics are based on some approximate measures estimating the quality of extracted cuts. in section 1 we present a short overview of some of these measures. in our approach we use so called discernibuity measures used extensively in rough set approach  polkowski and skowron 1 . all those methods are very efficient if data sets are stored in executive memory because  after sorting of data  the number of steps to check distribution of objects in intervals defined by consecutive cuts is of order o n . we consider the problem of searching for optimal partition of real value attributes assuming that the large data table is represented in relational data base. in this case even the linear complexity is not acceptable because of the time for one step. the critical factor for time complexity of algorithms solving the discussed problem is the number of simple sql queries like select count from ... where attribute between...  related to some interval of attribute values  necessary to construct such partitions. we assume that the answer time for such queries does not depend on the interval length  this assumption is satisfied in some existing data base servers . using straightforward approach to optimal partition selection  with respect to a given measure   the number of necessary queries is of order o iv   where n is the number of preassumed partitions of the searching space. we show some properties of considered optimization measures allowing to reduce the size of searching space. moreover  we prove that using only o logiv  simple queries  one can construct the partition very close to optimal. 
1 	basic notions 
an information system  pawlak  1  is a pair a =  u  a   where u is a non-empty  finite set called the universe and a is a non-empty finite set of attributes  or features   i.e.  where  is called the value set of a. elements of u are called objects or records. two objects x y  u are said to be discernible by attributes from a if there exists an attribute  a such that . any information system of the form 

　any pair  a  c   where is an attribute and c is a real value  is called cut  if the attribute a a has been uniquely specified  then cut can be denoted simply by any c r we say that  the cut  a  c  discerns a pair of objects x  y   if either 
any pair of objects x y exists a cut 
optimal iff card  p  
set of cuts p'. 
1 	t h e q u a l i t y measures 
developing some decision tree induction methods  see 
 fayyad and irani  1; quinlan  1   and some supervised discretization methods  see  catlett  1; dougherty at a/.  1; nguyen and skowron  1; 
liu and setiono  1; nguyen  1    we should often solve the following problem: 
f o r a given real value attribute a and 
set of candidate cuts   find a 
cut  belonging to the set of optimal cuts with high probability. 
　for example  the algorithm for decision tree induction can be described as follows: 
1. for a given set of objects u  select a cut  a  cbest  of high quality among all possible cuts and all attributes; 
1. induce a partition  by  a.cseat  ; 
1. recursively apply step 1 to both sets  of objects until some stopping condition is satisfied. 
　usually  we use some measure  or quality functions   to estimate the quality of cuts. for a given measure f  the straightforward searching algorithm for the best cut should compute the values of f for all cuts: . the cut cbest which optimizes  i.e. maximizes or minimizes  the value of function f is selected as the result of searching process. 
　in next sections we recall the most frequently used measures for decision tree induction and discretization like test    entropy function  and  discernibitity 
measure   respectively. first we fix some notations. let 
and the set of all relevant 
statistical test methods 
statistical tests allow to check the probabilistic independence between the object partition defined by decision attribute and by the cut c. the independence degree is estimated by the 
intuitively  if the partition defined by c does not depend on the partition defined by the decision attribute dec then we have  = 1. in opposite case if there exists a cut c which properly separates objects from different decision classes the value of test for c is very high. 
　discretization methods based on test are choosing only cuts with large value of this test  and delete the cuts with small value of  test . there are different versions of this method  see e.g. chimerge  kerber  1  and chi1  liu and setiono  1   
entropy methods 
a number of methods based on entropy measure formed the strong group of works in the domain of decision tree induction and discretization. this concept uses classentropy as a criterion to evaluate the list of best cuts 
	nguyen 	1 

where where {u   u } is a partition of u defined by c. 
　for a given feature a  the cut cmin which minimizes the entropy function over all possible cuts is selected. there is a number of methods based on information entropy theory reported in  catlett  1; fayyad and irani  1; chmielewski and grzymala-busse  1; quinlan  1 . 
boolean reasoning method 
in boolean reasoning methods  cuts are treated as boolean variables and the problem of searching for optimal set of cuts can be characterized by a boolean function   where a is a given decision table . any set of cuts is arconsistent if and only if the corresponding evaluation of variables in returns the value true  see  nguyen and skowron  1  . nguyen and skowron shown that the quality of cuts can be measured by 
their discernibuity properties. intuitively  energy of the set of objects x  u can be defined by the number of pairs of objects from x to be discerned called conflict x . let  be a class distribution of x  then conflict x  can be computed by 

the cut c which divides the set of objects u into u1  and 
u1 is evaluated by 

i.e. the more is number of pairs of objects discerned by the cut  a c   the larger is chance that c can be chosen to the optimal set of cut. hence  in the discretization and decision tree induction algorithms based on boolean reasoning approach  the quality of a given cut c is defined by number of pairs of objects discerned by c i.e. 
	 1  
1 	algorithm acceleration methods 
in this section we present some properties discovered using the boolean reasoning approach. these properties allow to induce decision trees and make discretization of real value attributes directly from large data base. 
1 maximal-discernibility heuristics 
1 	machine learning 
　now we are ready to show how to compute the difference between the discernibility measures of and using information about class distribution in intervals defined by these cuts. the exact formula is given in the following lemma. 
lemma 1 	the following equation holds: 

our goal is to find cuts maximizing the function w c . 
let us recall the notion of boundary cuts and the well known notion in statistics called median using the notations presented in section 1 : 

definition 1 any is called the boundary cut if there exist two objects such that and dec u   
definition 1 for given set of cuts on by median of the k i h decision class we mean the cut which minimizes the value the median of the k t h decision class will be denoted by median k . 
　we will show that it is enough to restrict the search to the set of boundary cuts. 
theorem 1 the cut cbest which maximizes the function w c  can be found among boundary cuts. 
　theorem 1 allows to look for the optimal cuts among boundary cuts only. this property also holds for entropy measures  see  fayyad and irani  1  . although it is interesting but can not be used to construct efficient heuristic for the investigated in the paper problem because of complexity of the algorithm detecting the boundary cuts  in case of data tables stored in rdb . however  it was possible to find another property allowing to eliminate the large number of cuts. let 
be the set of candidate cuts  and let 
the best cut may belong with the highest probability. we will use some approximating measures to predict the interval which probably contains the best cut with respect to disceraibility measure. this process is repeated until the considered interval consists of one cut. then the best cut can be chosen between all visited cuts. 
　the problem arises how to define the measure evaluating the quality of the interval  cl;cr  having class distributions: 	in and 	 see figure 1 . this measure should estimate the quality of the best cut among those belonging to the interval 
　let us consider an arbitrary cut c lying between cl and cr. we have the following theorem  the proof of this theorem is included as an appendix  
　one can use formulas  1  and  1  to construct the measure estimating quality of the best cut in  cl;ck  
		 1  
where the real parameter from  1; 1  can be tuned in learning process. the details of our algorithm can be described as follows: 
	nguyen 	1 

one can see that to determine the value eval we need only 1 d  simple sql queries of the form: select count from ... 
tween .... hence the number of queries necessary for running our algorithm is of order tice we set because the function over positive integers is taking minimum for k = 1. for instead choosing the best interval 
algorithm can select the best union secutive intervals in every step for a predefined parameter  the modified algorithm needs more - but still of order 1{log n  - simple questions only. 
1 	examples 
we consider a data table consisting of 1 records. objects are classified into 1 decision classes with the distribution  1 1   respectively. one real value attribute has been selected and n = 1 cuts on its domain has generated class distributions as shown in figure 1. 
figure 1: distributions for decision classes 1  1  1. 
　the medians of classes are ci1  c1 and cig1  respectively. the median of every decision class has been determined by binary search algorithm using logiv = 1 simple queries. applying theorem 1 we conclude that it is enough to consider only cuts from {c1  - c1}in this way 1 cuts have been eliminated by using 1 simple queries only. 
　in figure 1 we show the graph of w ci  for and we illustrated the outcome of applica-
tion of our algorithm to the reduce set of cuts for 
and 
　first the cut c1 is chosen and it is necessary to determine to which of the intervals the best cut belongs. the values of function eval 
tm 	machine learning 
figure 1: graph of 
on these intervals is computed: 	= 
1  	hence  the and the . the 
we show that one 
 catlett  1  catlett  j.: on changing continuous attributes into ordered discrete attributes. in. y. kodratoff  ed.   machine learning-ewsl-1  proc. of the european working session on learning  porto  portugal  lecture notes in artificial intelligence  springer-verlag  berlin  1 
 chmielewski and grzymala-busse  1  chmielewski  m. r.  grzymala-busse  j. w.: global discretization of attributes as preprocessing for machine learning. in. t.y. lin  a.m. wildberger  eds. . soft computing. rough sets  fuzzy logic neural networks  uncertainty management  knowledge discovery  simulation councils  inc.  san diego  ca 1 
 dougherty at a/.  1  dougherty j.  kohavi r.  
　　　m.: supervised and unsupervised discretization of continuous features. in. proceedings of the twelfth 

international conference on machine learning  morgan kaufmann  san fvancisco  ca 
 fayyad and irani  1  fayyad  u. m.  irani  k.b.: on the handling of continuous-valued attributes in decision tree generation. machine learning 1  1 
 fayyad and irani  1  fayyad  u. m.  irani  k.b.: 
the attribute selection problem in decision tree gener- figure 1: the random cut c and random class distribu ation. in. proc. of aaai-1  san jose  ca. mit press tion  induced by c 
 kerber  1  kerber  r.: chimerge. discretization of numeric attributes. in. proc. of the tenth national 
conference on artificial intelligence  mit press  1 
 liu and setiono  1  liu  h.  setiono  r: chi1. feature selection and discretization of numeric aattributes. in. proc. of the seventh ieee international conference on tools with artificial intelligence  tai1   washington dc 1 
 nguyen and skowron  1  nguyen  h. son  skowron  a.: quantization of real value attributes. in. p.p. wang  ed.   second annual joint conference on information sciences  jcis'1   wrightsville beach  nc  1. 
 nguyen  1  nguyen  h. son: discretization methods in data mining. in. in l. polkowski  a. skowron 
 eds. : rough sets in knowledge discovery 1  springer physica-verlag  heidelberg  1. 
 pawlak  1  pawlak z.: rough sets: theoretical aspects of reasoning about data  kluwer dordrecht. 
 polkowski and skowron 1  polkowski  l.  skowron  a.  eds. : rough sets in knowledge discovery vol. 1  springer physica-verlag  heidelberg. 
 quinlan  1  quinlan  j. r. c1. programs for machine learning. morgan kaufmann  san mateo ca 
 skowron and rauszer  1  skowron  a.  rauszer  c: the discernibility matrices and functions in information systems. in. r. slowinski  ed. . intelligent decision support - handbook of applications and advances of the rough sets theory  kluwer academic publishers  dordrecht 1 
appendix a 
in the consequence we have 

	nguyen 	w1 
