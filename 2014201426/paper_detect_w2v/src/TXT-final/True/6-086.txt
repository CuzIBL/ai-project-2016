 
a new approach to the text categorization problem is here presented. it is called gaussian weighting and it is a supervised learning algorithm that  during the training phase  estimates two very simple and easily computable statistics which are: the presence p  how much a term / is present in a category c  the expressiveness e  how much / is present outside c in the rest of the domain. once the system has learned this information  a gaussian function is shaped for each term of a category  in order to assign the term a weight that estimates the level of its importance for that particular category. we tested our learning method on the task of single-label classification using the reuters-1 benchmark. the outcome of the result was quite impressive: in different experimental setups  we reached a microaveraged fl-measure of 1  with a peak of 1. moreover  a macro-averaged recall and precision was calculated: the former reported a 1  the latter a 1. these results reach most of the stateof-the-art techniques of machine learning applied to text categorization  demonstrating that this new weighting scheme does perform well on this particular task. 
1 introduction 
consider the problem of automated classification of text documents. this problem is of great importance as the accessible textual information increases and the volume of online texts available through the internet expands rapidly. one solution is to categorize documents according to their topics beforehand or in real time. 
　a number of different machine learning methods have been applied and to text categorization  tc   including probabilistic classifiers  decision trees  regression methods  neural networks and support vector machines  see  sebastiani  1  . most of these methodologies share a common factor of high level complexity  that  often  makes a classifier design difficult to understand. 
　the question addressed in this paper is if it is possible to find a simple learning algorithm that uses naive  human comprehensible parameters  in such a way as to act like a non-
learning 
expert human being in front of the problem of classifying new unknown documents. 
　starting from this idea of simplicity  we decided to design a training algorithm that corresponds to the action of finding two information parameters: given a set of categories c of documents  for each term t of a category c  c we calculate its presence p  that is  the percent of documents of the category c in which the term / appears at least once; and its expressiveness e   that is  how much the term / is absent in the other categories of the domain. then  we model a gaussian function with the two parameters above  whose value in the abscissae equal to 1 is used as the weight of the term /. we called this learning approach gaussian weighting. it is important to stress the fact that we don't use any stemming algorithm or feature selection function  see  yang and pedersen  1  . an eventual reduction of the number of features per category is done using only two thresholds named thresp and threse  relative to the parameters p and e. 
　to make the evaluation of gaussian weighting comparable to most of the published results on tc  we chose the reuters1 corpus as benchmark for the single-label classification task. three different tests have been performed with a complete automated learning approach: the first run has been done without the use of a gaussian function  but simply giving a weight proportional to p and e: the second and the third test runs have been done with the gaussian weighting approach proposed here. 
　the outcome of the results was quite surprising: throughout the different experimental setups  we repeatedly reached a micro-averaged fl -measure around the 1  with a peak of 1. then  since the micro-average is known to be highly influenced by frequent categories  see  yang and liu  1    we decided to compute the macro-averaged recall and precision. again  the results reached a satisfactory macrorecall around 1 and a macro-precision of 1. these results reach most of the state-of-the-art techniques of machine learning applied to text categorization  see  dumais et al.  1    demonstrating that this new weighting scheme does perform well on this particular task. moreover  the low computational cost of the algorithm we propose  makes this approach particularly preferable when the computing power is low. 

www.daviddlewis.com/resources/testcollcction/reuters1 
1 

1 the algorithm 
in this section we present our supervised algorithm. first  we will analytically define the two parameters: presence p and expressiveness e. then  we will explain how the training algorithm works and how the system classifies new documents at run-time. 
1 presence p and expressiveness e 
a central key of this naive approach is the computation of two easy human understandable parameters that we call: presence p  and expressiveness e. the former captures how much a term t appears at least once in the documents belonging to a category c  the latter estimates how much the same term t does not appear in the documents of the other categories. given a set of categories  
  each category having a number of documents m   e.g. we give the following 
definitions: 
the number of documents d of the i-th category is: 
 1  
 the number of documents of category / in which the term / is present: 
		 1  
note that we use the symbol *|  to denote the presence of a term inside a category or a document. this is not to be confused with the same symbol when used for the conditional probability  p x|y  . 
　then  the presence p of a term t in the i-th category is  using  1  and  1 : 

while  the expressiveness e of term t in the i-th category is calculated using 1  1 : 

it is important to stress that the same term in different categories has different values of expressiveness. this fact is better explained in table 1. a term t is present in all the three categories with presence shown in the first row. the second row is the expressiveness of the same term for each category. note how / of category c1 has a greater expressiveness given the relatively smaller presence of the term in the rest of the domain. 

table 1: a numerical example of presence and expressiveness. 
1 
locality of presence and expressiveness 
the approach uses a weighting method for terms per each class  which is in a sense a local type of weighting/modeling scheme of classes. this idea is similar to the local lsi representation  hull  1  where  in order to characterize the term space  the singular value decomposition is applied to a matrix consisting only of the known relevant documents  in our case the documents belonging to a class . 
　there are substantial differences with the  weighting scheme. counts the number of occurrences of a term / in a document  while p counts the documents in which / appears at least once. computes the number of documents in which t appears over the whole collection  meanwhile e calculates  according to which class we are computing the weights  the partial averaged presence of/ in the domain. 
1 learning the gaussian weights  gw  
in this paragraph we explain how we shape a gaussian function using the values p and e. starting from the definition of a generic gaussian function: 
 1  
we estimate the parameters 
 1  
 1  
　then  the gaussian weighting gw for a term t of a category i is defined  substituting  1  and  1  in  1   as: 
		 1  
the gw calculated for x = 1  returns a real value belonging to the  1  interval. in particular  the maximum weight can be reached only when the presence of a term is equal to 1  that is to say  when it appears in each document of the category of interest. figure 1 shows an example of a gw with p = 1 and e = 1  continuous   and another one with p = 1 and e 
= 1  dashed ; when two terms have the same presence  the one with a higher expressiveness has a higher weight too. 
computing the parameter p the algorithm to compute p is the following: 
1. 1. 
1. 1. 1. 1. 1. 
1. end for 
1. end for 
1. for each term in 
learning 


figure 1: example of a gw. the continuous line is a gw with p = 1 and ♀ = 1. the dashed line is a gw with p = 1 and e = 1. for the same p  the higher expressiveness a term possesses  the bigger the output gw gives. 
1. 	
1. end for 
1. end for 
if we assume we can use a hashmap hm to store the results of  such that any update or search in the hm has a constant cost k  in the worst case the algorithm to compute the parameter p has a computational cost of  
computing the parameter e 
the algorithm to compute e is the following: 
1. for each category 
1. sum- 1 
1. for each term t in 
1. for each category 
1. if tin  
1. 
	1. 	end for 
	1. 	
1. end for 
1. end for 
again  if we assume that  is a hashmap with a constant cost k of access and search  the computational cost of this second algorithm is   which is quadratic cost with respect to the number of categories c. 
1 categorizing a new document 
once the system has been trained and the parameters p and e have been found for each term of the domain  it is possible to feed the system with an unknown document and  according to the value of a couple of optional thresholds thresp and threse  classify it with the following algorithm: 
learning 
1. for each  
1. output  
1. for each term in  
1. if t is in test and  
1. then output 
nterm = nterm + 1 
1. end for 
1. 
1. end for 
1. assign the document to the with the highest output  
for each category  the algorithm calculates  the mean of the activated gaussian functions of the terms whose are greater than two fixed thresholds  respectively thresp and threse. the computational cost of this algorithm is  

1 experimental setup 
1 test collection 
to make the evaluation of gw comparable to most of the published results on tc  we chose the reuters-1 corpus as benchmark. during the last few years this corpus has been used as a standard benchmark on which many tc methods have been evaluated  although the results are sometimes difficult to compare as slightly different version have been used. for this paper we used the modapte split of reuters-1 in which 1% of the stories  1  are used as training documents to build the classifier and the remaining 1%  1  to test the accuracy of our single-label classifier. now  the reuters-1 is known to be quite unbalanced on the distribution of stories per category. therefore  of the 1 potential topics categories only the 1 most frequent are here used; these 1 categories account for almost the 1% of the training instances  while the remainder is distributed among the other 1. table 1 shows the number of training and test samples for each category. 

table 1: number of training and test documents for the ten most frequent categories of reuters-1 modapte split 
1 

1 parameters setting 
we performed three different test runs on our system: 
1. the 'what if ' test run: what if we didn't use a gaussian function  we tried to run our system with the most simple function we could use:  
1. the gw test run: uses the gw function defined in equation  1  
1. the simple gw test run: we simplified the gaussian function with a variance equal to  
all the tests maintained threse equal to 1  that is to say  a term in a category should have an expressivity value greater than 1. this value permitted to achieve the highest performance during the test phase. thresp has been varied in the range between 1  all the terms of the categories were used  and 1  only the terms present in the 1% of the training documents were used . 
 yang and pedersen  1    we have decided to calculate the macro-recall  mre  and the macro-precision  mpr    as well: 
1 results 
1 the 'what if ' test run 
in this test  the weight of each term / of a category i is computed as: 

　the number of terms per category according to the value p is reported in table 1. the 'what if ' test run results are shown in figure 1. 

　a stoplist of 1 terms has been used to remove the most frequent words of the english language  but no stemming or feature selection have been performed. 
　in order to evaluate the accuracy of the classifier we have computed the standard 1r measures  such as recall and precision  and the following averaging measures: micro-recall micro-precision and micro-averaged fl measure  micro-fl : 

in particular we have reported the values of the fl-measure in order to directly compare our results with the ones in the literature. but  since the micro-averaged measure is known to be highly influenced by the most frequent category  see 
1 
table 1: the 'what if ' test run. 
　in this particular test we did not expect so remarkable a performance. considering the weight of a term proportional to the parameters p and e seems to be another possible solution to investigate. the performance of the system seems not to be influenced significantly when the threshold thresp is set either to 1 or 1. it is worth stressing that this test performed better when all the terms  except those belonging to the stoplist  were used. 
1 	the gw test run 
the results of this run are shown in figure 1  while numerical values are reported in table 1. 
　this test has been performed using equation  1  to calculate the weight of each term of the category. the best results have been obtained with thres macro-recall and 
macro-precision are quite high  and they indicate that the system works well for every category. the performance of the system starts to decrease sensibly when thres  
learning 


figure 1: the gw test run. in this test the function 1 is used to weight each term. 
1.1 1 1   1 1 1 1 micro f1 1 1 1 1 1 1  macro re 1 1 1 1 1 11 macro pr 1 1 1 1 1 1table 1: the gw test run. each column reports the values of micro f1  macro recall and macro precision for each thresp. 
1 	the simple gw test run 
the results of this second run are shown in figure 1. numerical values are reported in table 1. 

figure 1: the simple gw test run. in this test the variance of the gw is equal to  
　this test has been performed using a simplified version of equation  1  to calculate the weight of each term of the category. the variance of the gaussian function is set to  
 the best results have been obtained with thresp= there does not seem to be any particular difference with the gw test run  except in cases when thresp equals 1  where the macro precision is sensibly lower. 
1 	discussion 
the results reported above are satisfactory. in many situations the system reached an accuracy comparable to the ones in the state-of-the-art systems. some aspects need to be discussed here: the performance of the system in all the test runs decades when thresp exceeds the 1  that is to say  looking back to table  1   when the number of features per 
learning 

table 1: the simple gw test run. 
category decreases from almost 1 to almost 1. this sharp decay influences the macro-averaged performance of the system. in fact  while the macro-recall retains the same values  the macro-precision is cut-off by almost 1%. this fact indicates that the system is incorrectly assigning the stories to all the categories of the domain rather that to the biggest ones only. as the thresp gets bigger  the performance of the system shows the classic behavior in which the recall tends to 1 and the precision to 1. 
　for the 'what if ' test run we didn't expect such good results. the simple proportional weight assignment performed as well as the other approaches  nevertheless the performance decays slightly more rapidly. this test was very important for us to confirm the idea of using a simplified learning approach to solve the problem of tc. 
　the second and the third test runs show almost the same values. the one-per-thousand differences are not to be considered relevant. at the moment  we cannot draw any conclusion about which of the three learning approaches will perform better in general. we plan to investigate this matter testing on the complete reuters-1 and on other test collections. 
comparative results 
dumais et al. tested a number of inductive learning algorithms on the same reuters-1 modapte split we used   dumais et ai  1  . these results arc briefly summirized in table 1 for the 1 most frequent categories. the 

table 1: microavcraged fl-measure of the 1 most frequent categories of reuters-1  reported by dumais 
findsim method is a variant of rocchio's method for relevance feedback  rocchio  1  . the naive bayes classifier is constructed by using the training data to estimate the probability of each category given the document feature values of a new instance. a discussion of the indipendence assumption of naive bayes classifier can be found in   lewis  1  . bayesnets is a bayesian network  that uses 1-dependence bayesian  see  sahami  1  for another example of bayes nets for classification . tree is a decision tree approach described in   chickering et al  1  . meanwhile  linear svm is a linear hyperplane that separates a set of positive examples from a set of negative ones   joachims  1  . 

　both svms and decision trees produce very high overall classification accuracy. as the results of table 1 are directly comparable with ours  our gaussian weighting approach is placed just behind the svms  which are known to be the best machine learning method in the field of text classification  with a micro-averaged fl-measure of 1 of the first two test runs. this result is important for two reasons: it demonstrates that this new learning method does perform well on this particular task  and encourage us to further investigate other fields of application  in which gw's simplicity could perform as well as other state-of-the-art machine learning methods. 
1 future work 
two are the most compelling issues that we are going to complete in the future work are two: 
  incorporate a wrapper phase  kohavi and john  1  to run systematically varying experiments with  i  using expressiveness or not   ii  varying values of thresp and threse   iii  varying values of gw variance  and  iv  using a gaussian function or not; 
  test the full reuters benchmark data rather than restrict-ing ourselves to only the 1 most frequent categories  and on other collections like 1 newsgroups1 or medline1. 
1 conclusions 
in this paper we addressed the problem of finding a simple learning algorithm that uses naive  human comprehensible parameters. a very accurate text classifier can be learned automatically by using only two information parameters: presence p and expressivity e. the algorithm proposed here has  in the worst case  a quadratic computational cost  with respect to the number of categories of documents. we tested gw on the reuters-1 benchmark and calculated the microaveraged fl-measure to directly compare our results with the ones of the literature  and the macro-recall and macroprecision. the results achieved are satisfactory and can be compared to most state-of-the-art machine learning methods applied to tc. 
acknowledgments 
we are grateful to the anonymous reviewers for their insightful and helpful comments. the authors would also like to thank prof. maristella agosti for her valuable comments and suggestions to improve the paper. 
