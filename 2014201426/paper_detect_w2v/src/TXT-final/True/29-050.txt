 
max and min operations have interesting properties that facilitate the exchange of information between the symbolic and real-valued domains. as such  neural networks that employ max-min activation functions have been a sub-
ject of interest in recent years. since max-min functions are not strictly differentiate  we propose a mathematically sound learning method based on using fourier convergence analysis of side-derivatives to derive a gradient descent technique for max-min error functions. this method is applied to a  typical  fuzzy-neural network model employing max-rnin activation functions. we show how this network can be trained to perform function approximation; its performance was found to be better than that of a conventional feedforward neural network. 
1 	introduction 
max and min operators are widely used in systems which performs fuzzy logic  multi-valued logic or other forms of uncertainty reasoning. besides generalizing the binary or and and operators to the real-valued domain  max and min operators also have attractive properties such as commutativity  monotonicity and associativity. in addition  the max and min operators are the only realvalued logical operators that are continuous and idempotent  klir and folger  1 . from a theoretical point of view  they form a bridge between the symbolic and real-valued domains. hence  they facilitate the encoding and extraction of symbolic knowledge in hybrid systems which combine logic and neural networks. 
　however  the problem with neural networks which employ max and min operators has always been with learning using gradient descent techniques such as backpropagation  rumelhart et al.  1g . this is because gradient descent requires the activation functions to be fully differentiable. activation functions with max or min operations  unfortunately  do not satisfy this requirement. 
1 	neural networks 
one of the goals of this paper is to analyze the calculus of max-min functions  especially with regard to the side-differentiability of such functions. in particular  we show how fourier convergence analysis can be used to obtain the pseudo-derivatives. this in turn leads to a gradient descent technique for neural networks that employ max-min activation functions. such an approach is obviously mathematically sound compared with ad hoc methods found in the literature. 

　side-differentiability implies quasi-differentiability  but not conversely. higher-order derivatives may also exist. interested readers can learn more about sidedifferentiability and quasi-differentiability in reference  fulks  1 . in this paper  we shall only be concerned with functions which are side-differentiable in the first order and quasi-differentiable in the second order. 


	teow & loe 	1 

problem by replacing a max-min function with a differentiable one  for example by replacing the max and min operators with sum and product operators respectively. 
　a more interesting approach would be to use a parameterized function which limits to the max or min operator. one such function is the softmax operation commonly used in winner-take-all neural architectures. the softmax operation is defined as follows: 

　in fact  this inconsistency arises whenever pseudoderivatives of max and min operations are used directly when  differentiating  a nested max-rnin function. 
　the method proposed in this paper  on the other hand  does not have this problem. in our method  each of the side-derivatives of the entire function must be evaluated before they are combined. hence  we apply theorem 1 
and theorem 1 to give 
1 	neural networks 

1 	a fuzzy-neural network model 
the synthesis of fuzzy logic and neural networks has been a popular theme in research in the past decade 
 buckley and hayashi  1; gupta and rao  1; ishibuchi et a/.  1; keller et a/.  1; mitra and pal  1; pedrycz  1; simpson  1; simpson  1 . this is not surprising considering that neural networks and fuzzy logic complement each other. neural networks are well known for their learning capabilities  which allow them to model accurately almost any input-output relationship. fuzzy logic  on the other hand  facilitates the encoding of experts' knowledge in linguistic terms and inferencing from such knowledge using mathematical techniques. fuzzy-neural networks  by combining these two technologies  can  among other things  allow the fine-tuning of of experts' knowledge as well as a more natural interpretation of the knowledge learnt. 
　max and min are the standard logical operations used in fuzzy set theory  klir and folger  1 . in addition  they have many attractive properties as described in the introduction of this paper. hence  we employ maxmin activation functions in a  typical  fuzzy-neural network architecture commonly found in the fuzzy-neural research literature. since max-min functions are nondifferentiable  we show how such a network can be trained with gradient descent based on using fourier convergence analysis. 
1 	description of the model 
 the fuzzy-neural network model we use has five layers: one input  one output and three hidden  figure 1 . each unit in the input layer corresponds to an input variable  and is connected to several antecedent fuzzy set units in the first hidden layer. the units in the antecedent layer can be divided into sub-groups  each corresponding to a fuzzy variable. from every sub-group  only one fuzzy set unit may be connected to a rule unit in the second hidden layer  depending on the fuzzy rule associated with that rule unit. the rules comprises of all possible combinations of antecedent fuzzy sets; hence  if there are 1 input variables with each having 1 fuzzy sets  then there will be a total of 1x1 rules. each rule unit is connected to the consequent fuzzy set units in the third hidden layer. like the units in the first hidden layer  consequent fuzzy set units are also grouped in a similar fashion. each sub-group has connections to the output unit corresponding to its fuzzy output variable. 


	teow & loe 	1 


1 	neural networks 


figure 1: mean squared error  m.s.e.  curves on the training data for the fuzzy-neural network and the sigmoidal neural network  averaged over 1 trials  with 1 epochs in each trial. 
　table 1 gives the comparison between the fuzzyneural network and the sigmoidal neural network in terms of the mean squared error  m.s.e.   averaged over 1 trials  on both the training data and the testing data after being trained. as expected  the fuzzyneural network has a much lower mean squared error on the training data than the sigmoidal neural network. its generalization power  as seen by the performances on the testing data  is also better than that of the sigmoidal neural network. 
　both figure 1 and table 1 demonstrate how the fuzzy-neural network can effectively model a highly nonlinear function as compared to a sigmoidal neural network with nearly the same number of network parameters. the authors did not make an exhaustive search of all possible network architectures and learning parameters in either model  so this comparison cannot be a universal one. 
1 	conclusions 
a learning method that utilizes gradient descent based on using fourier convergence analysis for max-min functions was presented. it has been applied to effectively train a feedforward fuzzy-neural network model. this model employs max and min as its logical operations and gaussian functions as its input fuzzy sets. we have shown how the network can be trained to approximate a highly non-linear function. it learns and generalizes better than a conventional feedforward neural network. 
　in conclusion  we have shown that our proposed gradient descent technique does allow max-min neural networks to learn effectively. our approach should be extensible to other neural networks that have nondifferentiable activations functions. this remains a topic of further research. 
