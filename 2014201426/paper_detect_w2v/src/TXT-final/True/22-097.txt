 
a learning problem solver consists of three components:  1  a problem solver   1  a memory of problem-solving knowledge  and  1  a learning component for deriving new problemsolving knowledge from experience. previous learning problem solvers have acquired knowledge as macros  control knowledge  or cases. steppingstone is a general learning problem solver that improves its performance by learning subgoal sequences. the underlying problem solver for steppingstone is a combination of means-ends analysis and brute-force search. it depends primarily upon means-ends analysis and its problem-solving knowledge to solve the subgoals of the problem. learning occurs only when this approach fails. upon failure  search is applied and a new subgoal sequence is derived and added to its problem-solving knowledge. before steppingstone attempts to solve any of the problem subgoals  it first orders them with a domain independent heuristic which we call openness. openness is used to order the subgoals to minimize interactions. steppingstone's ability to improve its performance and scale to difficult problems is demonstrated with an implemented system. we show that a small memory of appropriate subgoals yields multiple orders of magnitude savings in problem-solving cost. 
1 	introduction 
early planners were capable of solving only the simplest problems. in response  researchers sought ways of improving problem-solving performance by augmenting the planner with knowledge gleaned from experience. one approach acquires new operator sequences or macros  fikes et a/.  1  korf  1b  minton  1  iba  1 . a second approach learns control knowledge  either represented as heuristics  kibler  1  pearl  1   or production rules  mitchell et a/.  1  langley  1  porter and kibler  1  minton  1 . 
   *this work was partially supported by a grant from the hughes artificial intelligence center. 
both of these approaches are encompassed by a general model of human learning called chunking  laird et a/.  1 . a third approach to learning from experi-
ence  learning problem-solving cases  operates through storage and reuse of previous problem-solving sessions. these previous sessions are incorporated into a global knowledge structure along with any other knowledge about the world and planning  and are indexed by their goals solved  impasses encountered  failures encountered or similarity to the current problem  hammond  
1  alterman  1  carbonell  1  kolodner  1  bradtke and lehnert  1  ruby and kibler  1 . 
¡¡we present a new representation for problem-solving knowledge  sequences of intermediate subgoals  stepping stones  between problem and goal states. learning sequences of subgoals has the effect of decreasing the distance between problem states and goals without increasing the branching factor. learning macros also decreases the distance between problem states and goals  but it increases the branching factor. learning control knowledge decreases the branching factor of the problem  but leaves the distance between problem states and goals unchanged. 
¡¡the value of subgoals has been analytically demonstrated by korf . for example  if k appropriate intermediate subgoals are learned for a problem and a brute-force search technique is used  then the amount of search required to solve the problem decreases from bd to kbdlk  where b is the branching factor of the domain and d is solution depth. in addition  if a problem can be broken into k independent subgoals  the branching factor b of the problem can be decreased by a factor of k. clearly subgoals have immense value. the problem is how to learn thern. 
¡¡steppingstone is a general problem solver which learns to improve its performance by learning sequences of subgoals. it learns these new subgoal sequences when normal problem solving fails. search is applied during these situations and the results are used to generate a new sequence of subgoals for memory. by restricting learning to those situations where normal problem solving fails  the utility of the learned knowledge is kept high. 
1 	learning subgoal sequences 
a learning problem solver consists of three basic components:  1  a problem solver   1  a memory of problem-

solving knowledge used to assist the problem solver  and  1  a learning component that is able to compile experience into additional problem-solving knowledge. p r o d i g y  minton  1j and soar  laird et a/.  
1  both use weak methods as their basic problem solvers. prodigy learns control rules to assist its meansends problem solver. soar learns production rules to record the solution to impasses. steppingstone combines means-ends analysis and brute-force search for its problem solver. its means-ends system does not allow undoing any previously solved subgoals unless the current subgoal explicitly directs it. initially  steppingstone depends upon its means-ends system and knowledge of subgoals sequences to solve the problem  but resorts to search when this fails. the results of the search are generalized  not into a macro or control rule  but into a sequence of subgoals that can be reused by the means-end system. 
¡¡steppingstone  like soar and prodigy  uses a statespace representation of a problem  consisting of a set of operators  a start state  and a goal state. the goal state is represented as a conjunction of subgoals. the following pseudo-code outlines the steppingstone approach: 
order problem subgoals 
loop through ordered problem subgoals 
apply means-ends to current subgoal 
if means-ends fails but memory has solution then 
apply steppingstone to subgoals from memory 
elseif means-ends and memory fails then 
apply search to failure 
   learn new subgoal sequence and store in memory end loop 
the following sections will further elaborate this description  with particular attention to the method for ordering the problem subgoals and learning new subgoal sequences. 
1 	goal o r d e r i n g based on openness 
steppingstone begins by ordering the subgoals of the problem so few impasses are encountered. this ordering is found using a domain independent heuristic we call openness. given a set of goals  some of which have al-
ready been solved  openness measures the likelihood of solving the remaining goals while preserving the solved goals. 
   more formally  openness is defined first for a single unsolved goal  and a set of solved goals. let g  denote a set of solved goals  g1 a set of unsolved goals  and g a single goal in g1 the openness of g with respect to g1 is defined as: 
openness gi  g  = number of operators solving g whose preconditions are not contradicted by the assertion of goals g1. 
intuitively  this measures the likelihood of moving from a state where the goals g  are solved but g is not solved  to a state where both g1 and g are solved. the openness of the set g1 of unsolved goals with respect to the set g  of solved goals is now defined as: 
¡¡given a goal set g consisting of g   g1 ...  gn  the desired ordering maximizes the sum: 

since there are n! factorial orderings of these goals  an exhaustive search is not usually possible. instead  an ordering is built up iteratively by beginning with those goals g ¡ê g that maximize openness g  g - g . here g - g includes those goals in the set g except the goal g. this selection generates initial partial orderings consisting of an ordered set g i   and unordered set g1. these partial orderings are then iteratively expanded by adding goals g € g1 that maximize openness g1 + g g1 - g   the set g1 + g includes goal g along with all those goals in g . for each iteration k  only those partial orderings with maximum sums 

are further expanded. although this search can be expensive  experimentation has proved it tractable  see table 1 . the final ordering of the goals minimizes the liklihood of generating an impasse during problem solving by maximizing the probability of solving the problem subgoals. 
1 	m e m o r y of n e w subgoal sequences 
given a goal-ordering  steppingstone attempts to solve the ordered subgoals of the problem by means-ends analysis without undoing them once solved. although this approach will not usually work for the entire set of subgoals  subsequences of subgoals are often independent and can be solved in this manner. we say that an impasse arises if problem solving requires the undoing of some previously solved subgoal. there are two ways of resolving an impasse - by memory or by search. 
   memory consists of impasse characterizations and new sequences of subgoals for resolving the impasses. an impasse is characterized by the subgoal being attempted and those solved subgoals that must be undone to solve it. when reaching an impasse  steppingstone indexes memory by testing whether a characterization in memory is contained within the current context defined by the subgoal being attempted and previously solved subgoals. after successfully indexing memory  steppingstone is recursively applied to the ordered sequence of subgoals to resolve the impasse. 
   figure 1 illustrates an impasse from the blocks world and its resolution. it is a classic example of a nonlinear subgoal interaction problem that a linear meansends problem solver cannot solve. given the initial and goal states listed  and goal ordering of achieving on b c  first  the impasse shown would be encountered. this impasse arises because the final subgoal of on a b  cannot be achieved without undoing the previously solved subgoal on b  c . at the bottom of figure 1 is a sequence of subgoals for resolving this impasse. this sequence would 

be stored in memory and indexed by the subgoal being 
1 	machine learning 

solved  on a b   and the subgoal that must be undone  on b c . 
¡¡note that block d is not mentioned in the impasse solution. this occurs because it is not involved in the impasse. this allows the impasse solution in figure 1 to work regardless of the number of blocks between block c and block a. this type of generalization is normally very difficult to achieve  but falls out naturally from the use of a subgoal sequence to represent the solution. the impasse characterizations are further generalized by changing domain objects into variables. 
1 	deriving n e w subgoal sequences 
if memory does not contain an entry to resolve an impasse  steppingstone resorts to search. during the search for the solution to a problem  the irrelevant aspects of the state greatly increase its difficulty. for example  adding more blocks to a blocks world problem makes the problem more difficult even if the blocks are independent of the problem solution. the search procedure used by steppingstone mitigates this problem by beginning the search within the clearly relevant portions of the problem state. initially  only those state predicates defining the impasse can be undone during search. if the solution cannot be found in this space  it is opened by allowing additional state predicates to be undone during the search until the search space includes a solution. the predicates chosen are those that are least independent of the ones currently allowed to be undone  as measured by the openness heuristic. 
   once a sequence of moves for resolving an impasse is found  it is analyzed to derive a new subgoal sequence. the sequence of moves for resolving the impasse is equivalent to the sequence of states generated by the moves. this sequence of states can be regarded as a sequence of very specific subgoals. since these subgoals solve an impasse  only those portions of the state involved in the impasse are relevant. this allows the subgoals to be generalized by including in each only those portions of the state involved in the impasse  which is defined by the subgoal being solved and the previously solved subgoals that needed to be undone to solve the impasse. from this sequences repeating subgoals are then removed  yielding the desired sequence of subgoals to be stored in memory. 
steppingstone was implemented and applied to the tile sliding domain. because of its complexity  this puzzle has been a standard benchmark for search-based techniques. for techniques based on goals  the degree of subgoal interaction also makes the problem difficult. each problem of size nxn  consists of n1-l subgoals defining the final location of the tiles. since the blank was not treated as a subgoal  the problem was not decomposable. 
¡¡steppingstone was tested on different sized puzzles ranging from the 1  1-puzzle  to the 1  1-puzzle . the number of reachable problem states in an n x n puzzle is n1//1. for the 1 case the number of states is 1  and for the 1 case the number of states is approximately 1. general problem solvers are rarely applied to this puzzle because of the huge search space. solutions to random problems from the 1 sized puzzle have yet to be published using any system. 
¡¡we explored stepping stone's ability to learn new subgoal sequences  and the effects these sequences had on problem-solving performance. we tested the system's ability to scale to difficult problems. the independence of steppingstone's learning method from the subgoal ordering  as well as the effects of different subgoal orderings on performance and learning speed were also examined. 
1 	system performance 
experiment one was designed to test the ability of steppingstone to learn to improve its performance  and to scale to difficult problems. to do this  it was applied to the tile sliding domains described. ten orderings of the subgoals were chosen based upon the openness heuristic for each problem size. the system was trained on twenty problems  during which new subgoal sequences were derived and stored in memory. after training  the number and length of the learned subgoal sequences was recorded. the system was then tested on 1 random solvable problems. during testing  learning was turned off  and no new subgoal sequences were added to memory. the length of the solutions found were recorded. optimal solutions were acquired  when possible  to evaluate the solution quality.1 averaged results of these experiments are presented in table 1. 
¡¡somewhat surprisingly  after learning  steppingstone solved every problem in the test set without resorting 
1
¡¡¡¡the optimal data for the 1 case were compiled using a*. the data and 1 test problems for the 1 sized puzzle were taken from korf   1a . 


to search  i.e.  it never reached an impasse that it could not overcome with its current small memory of subgoal sequences. similar results occurred for each of the four different sized puzzles. 
¡¡steppingstone learned few new subgoal sequences because it encountered few impasses. this experiment showed that with the orderings derived using the openness heuristic  most subgoals were solved without ever having to undo previously solved subgoals. in addition  when the size of the problem increased  the number of impasses grew at a constant rate  while the size of the problem grew at an exponential rate. finally  the experiment showed that the solutions derived were about two to three times longer than optimal  which is not surprising since the approach made no attempt at optimization. 
   derived subgoal sequences were short  the result of the generalization from domain moves to subgoals. these new subgoals greatly decreased the amount of work required to solve the problems. figure 1 illustrates the difference in the amount of work required by the system before and after the new subgoal sequences were learned. 
¡¡to show the steppingstone approach reduces the amount of work required to solve problems  it is compared to a simple search based approach. iterative deepening depth-first search was applied to the same ordered sequence of subgoals used by steppingstone and was tested on the same set of random problems. the work required by this search method to solve the problems is plotted with the steppingstone data in figure 1. because of the huge amount of search required by the search-based approach on the 1 and 1 sized puzzles  they were conservatively estimated using the growth in the computed branching factor  and assuming an approximate constant growth in the average maximum distance between subgoals. the results are plotted on a log scale to illustrate the exponential decrease in the amount of work required. 
¡¡the results indicate that learning subgoals significantly reduced the work required for problem solving. this clearly demonstrates the power of the subgoals being learned. the work required before learning was dom-
1 	machine learning 

inated by the search for impasse solutions. 
¡¡the amount of work required before learning was still much less than that required to exhaustively search for each subgoal solution. this resulted from two factors. first  steppingstone did not resort to search for every subgoal  but only for those that generated impasses. second  the openness heuristic was surprisingly effective at curbing the growth in the branching factor by keeping the search for the impasse solution within a small portion of the entire search space. 
¡¡a classic problem with a learning problem solver is that it can become swamped with its own learning and perform slower  resulting in the problem-solving faneffect  minton  1 . steppingstone avoids this problem by learning only when impasses are encountered  and using its knowledge only when needed. impasses provide sufficient focus to ensure that what is learned is useful  while the indexing assures that what is learned is tried only when it is likely to be useful. this experiment has shown that this type of learning is sufficient to provide a great deal of improvement in problem-solving performance. 
1 	goal ordering effects 
steppingstone was tested in the 1 tile sliding domain with four different subgoal orderings to evaluate the ability of the openness heuristic to order the subgoals to both decrease the number of impasses encountered and work required to solve them. the first ordering  a good ordering  was derived using the openness heuristic. the second ordering  a bad ordering  was derived by ordering the subgoals according to the inverse of the openness heuristic. the third ordering was the standard numeric ordering. the fourth ordering was a randomly generated ordering. 
for each subgoal ordering  the system was trained on 
1 problems. after solving each problem  the system was tested on a separate set of 1 problems. figure 1 illustrates the amount of work required to solve the problems in the test set as more problems in the training set are solved when using the good and bad orderings. in addition  the amount of work required by iterative deepening a* using the manhattan distance metric on the same test set of 1 problems is also plotted. this algo-


rithm is a good basis for comparison since it is an optimal knowledge-free  search-based algorithm  korf  1a . 
¡¡in both cases steppingstone reduced the amount of work required to solve the problem to less than 1 search tree nodes. as expected  the bad subgoal ordering encountered many more impasses than the good ordering. using the bad subgoal ordering steppingstone required thirty new subgoal sequences to solve impasses encountered  while with the good ordering it required only two. initially  the system using the bad ordering required more work than iterative deepening a*  but after solving fourteen problems required less. the bad ordering required more than an order of magnitude more search to solve the initial training problem than the good ordering. it also required many more training examples to learn the needed new subgoal sequences. similar results occurred with the linear and random orderings  with the performance curve falling between that for the bad and good ordering. the results imply that the openness heuristic is able to order the subgoals so that fewer impasses are encountered  and the ones encountered are easier to solve. 
¡¡interestingly  after learning was completed  the bad ordering of the subgoals required slightly less search to solve the problems than did the good ordering. with the bad ordering  the system almost always had to rely upon the more efficient memory of subgoal sequences. this small amount of improved performance came at the cost of a larger memory  and slower  more costly learning. nonetheless  as long as solutions to all impasses can be generated  the system performance will always reach a level where the memory of subgoal sequences suffice to solve all encountered impasses  establishing the independence of the learning method from the subgoal orderings. 
¡¡the good subgoal orderings were derived through a search of the goal space using the openness heuristic. the search began with orderings of length one and then expanded only those nodes corresponding to the best orderings that include an additional subgoal  as measured by the openness heuristic. the amount of search required for each of the domains is recorded in table 1. the search required was relatively small since there were usually few best partial orderings. this search assisted in reducing the number of impasses encountered by deriving a good subgoal ordering. the search was conducted separately for each domain  although using a good representation of the problem offers the opportunity for reuse of subgoal orderings from smaller problems when working on larger ones. 
1 	discussion 
the results show how learning new subgoal sequences can greatly improve problem-solving performance. by learning only when an impasse was encountered and indexing the knowledge with the impasse context  the utility of the knowledge learned was much higher than its cost to apply. by first ordering the subgoals of the problem with the openness heuristic the likelihood of encountering impasses was decreased  along with the amount of learning required. the openness heuristic also proved useful in guiding the search for a solution to the impasse by limiting the portion of the space that needed to be searched. the independence of the learning method from the goal orderings was demonstrated by its success with good  bad  numeric  and random orderings. the generality of the subgoal sequences learned was illustrated by the small number of them required to solve random problems selected from domains with large numbers of difficult problems. 
¡¡steppingstone derives its power through the interaction of a number of different methods. first  the ordered subgoals of the problem are initially treated as independent. independent subgoals greatly decrease the branching factor of a problem  korf  1 . second  means-ends analysis decreases the search required by focusing on relevant subgoals and operators. third  learning new subgoal sequences greatly simplifies the difficult portions of the problem by deriving intermediate goals  or stepping stones  to bridge these problem gaps. this  too  has been shown to greatly reduce the difficulty of a problem  korf  1 . finally  brute-force search provides the power needed to initially solve those difficult problem gaps. 
¡¡steppingstone depends upon some regularity in the impasses that are encountered. if it continued to encounter new impasses  it would be forced to continually fall back on costly search. in addition  solutions to the impasses encountered must be accessible. steppingstone reduces the amount of search required to find impasse solutions by using the openness heuristic to constrain the 
search. 
¡¡finally  since steppingstone combines two different weak methods  it has opportunities for learning that are unavailable to approaches using a single weak method. steppingstone takes a sequence of moves generated by a search procedure and translates it back into a goal-based representation that the means-ends problem solver can use. 
   we intend to test the generality of the approach by applying it to additional domains and by comparing it with other problem solvers  such as prodigy  minton  1  and soar  laird et a/.  1 . work has also begun on deriving new subgoal sequences for avoiding impasses rather than solving them once encountered. finally  comparison on specific problem domains is a weak means for illustrating the power of a problem solver. it is our hope that we can develop more principled techniques for evaluating and comparing learning problem solvers. 

acknowledgements 
we would like to thank david aha  randy jones  patrick young  and the rest of the uci machine learning group for their comments and discussions concerning this work. 
