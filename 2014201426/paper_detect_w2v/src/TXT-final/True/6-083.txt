 	1 the  interested reader  model 

we present a simple  easily implemented spectral learning algorithm which applies equally whether we have no supervisory information  pairwise link constraints  or labeled examples. in the unsupervised case  it performs consistently with other spectral clustering algorithms. in the supervised case  our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 1 newsgroups data set. furthermore  its classification accuracy increases with the addition of unlabeled documents  demonstrating effective use of unlabeled data. by using normalized affinity matrices which are both symmetric and stochastic  we also obtain both a probabilistic interpretation of our method and certain guarantees of performance. 
1 introduction 
spectral algorithms use information contained in the eigenvectors of a data affinity  i.e.  item-item similarity  matrix to detect structure. such an approach has proven effective on many tasks  including information retrieval  deerwester et al.   1   web search  page et al  1; kleinberg  1   image segmentation lmeila and shi  1   word class detection  brew and schulte im walde  1  and data clustering  ng et al.  1 . but while spectral algorithms have been very useful in unsupervised learning  clustering   little work has been done in developing spectral algorithms for supervised learning  classification . 
　in this work  we consider the adaptation of spectral clustering methods to classification. we first present a method for combining item similarities with supervision information to produce a markov transition process between data items. we call this markov process the  interested reader  model by appeal to the special case of text clustering/classification. our algorithm incorporates supervisory information whenever it is available  either in the form of pairwise constraints or labeled data  or both . empirically  our algorithm achieves high accuracy when supplied either small amounts of labeled data  section 1  or small numbers of pairwise constraints  section 1 . 
we propose a markov chain model similar in spirit to the  random surfer  model of  page et al.% 1 .1 this description is motivated in the context of text categorization  but the model depends only on notions of pairwise data similarity and is completely general. in the model  there is a collection of documents  each of which has some  possibly unknown  topic. a reader begins with some document of interest and continues to read successive documents. when she chooses the next document to read  she tries to read another document on the same topic  and hence will prefer other documents which are similar to her current document. some mapping between similarities and transition probabilities must be chosen; we describe a specific choice in section 1. 
　these transition probabilities define a markov chain among the documents in the collection. if there exist distinct topic areas in the document set  or  generally  if there are clusters in the data   this markov chain will be composed of subsets that have high intra-set transition probabilities  and low inter-set transition probabilities. we will refer to these subsets as cliques. each of the cliques corresponds to a topic in the text clustering problem. 
　of course  the natural clusters in the data need not be perfectly compatible with document labels  and we have said nothing about the use of supervision information. in section 1  we use supervision to override the similarity-based transition probilities. for example  we will disallow transition between two documents which are known to be differentlylabeled  regardless of their pairwise similarity. 
1 spectral clustering algorithms 
in this section  we discuss the process of turning an affinity matrix a of pairwise document similarities into a normalized markov transition process n. the eigenvectors of n are then used to detect blocks or or near-blocks in n  which will correspond to clusters of the data. 
　　1  note that there is an important difference between the way these two models are used; the random surfer model is used for the first left eigenvector of the transition matrix  which indicates the relative amount of time the process spends at each data item. on the other hand  we are interested in right eigenvectors of our transition matrix  which more straightforwardly relate to  near- block structure in the transition matrix. 

learning 	1 


for clustering: 
1. treating each row of x as a point in rk  cluster into k clusters using k-means or any other sensible clustering algorithm. 
1. assign the original point xi  to cluster j if and only if row i of x was assigned to cluster j. 
for classification: 
1. represent each data point i by the row xi of x. 
1. classify these rows as points in rk using any reasonable classifier  trained on the labeled points. 
1. assign the data point i the class c that x-% was assigned. 
figure 1: spectral learning algorithm. 
algorithm 	normalization 	v a  d  
mncut 	divisive 	n = di1 a 
njw 	symmetric divisive 	n = d 	/1ad~ /1 
lsa 	none 	n = a 
sl 	normalized additive 	n -  a 1- dmaxl - d /dmax 
table 1: normalizations used by spectral methods. 
1 	calculating the transition matrix 
in order to fully specify the data-to-data markov transition matrix  we must map document similarities to transition probabilities. let a be the affinity matrix over documents whose elements aij are the similarities between documents / and j. when we are given documents / as points xi - and a distance function   a common definition is aij -where a is a free scale parameter. in lsa  deerwester et al.t 1   we are given a row-normalized term-document matrix b  and a is defined to be b1 b  the cosine similarity matrix  salton  1  . 
　we may map document similarities to transition probabilities in several of ways. we can define   meila and 
shi  1   where d is the diagonal matrix whose elements 
 this corresponds to transitioning with proba-
bility proportional to relative similarity values. alternatively  we can define   fiedler  1; 
chung  1   where dmax is the maximum rowsum of a. 
here  transition probabilities are sensitive to the absolute similarity values. for example  if a given document is similar to very few others  the interested reader may keep reading that document repeatedly  rather than move on to another document. while either of these normalizations are plausible  we chose the latter  since it had slight empirical performance benefits for our data. 
　in  meila and shi  1   it is shown that a probability transition matrix n for a markov chain with k strong cliques will have k piecewise constant eigenvectors  and they suggest clustering by finding approximately equal segments in the top k eigenvectors. our algorithm uses this general method as spectral learning 	k-means 
1 news 1 1 1 news 1 1 lymphoma 1 1 
	soybean 	1 	1 
table 1: a comparison of spectral learning and k-mcans. 
well  but some of the details differ; our algorithm is shown in figure 1. this algorithm is most similar to the algorithm presented in  ng et al  1   which we call njw after its authors. in fact  the only difference is the type of normalization used. there are two differences between our algorithm and mncut from  meila and shi  1 ; the normalization of a is again different  and  additionally  mncut docs not row normalize x  step 1 . table 1 describes the different types of normalizations and mentions some algorithms that use them. 
　it should be noted that for data sets where there are distant outliers  additive normalization can lead to very poor performance. this is because  with additive normalization  the outliers become their own clique. therefore  the clusters will represent outliers rather than true clusters. in a dataset where there are distant outliers  divisive normalization is likely to lead to better performance. 
1 	parameter selection 
the importance of parameter selection is often overlooked in the presentation of standard spectral clustering methods. with different values of a  the results of spectral clustering can be vastly different. in  ng et al.  1   the parameter a is chosen based on that value of a that gives the least distorted clusters. 
　in our text experiments  the data b was a term-document matrix  and the similarity function f gave the pairwise cosine similarities  with an entry aij set to zero if neither i was one of the top k nearest-neighbors of j nor the reverse. thresholding the affinity matrix in this manner is very useful  as spectral methods empirically work much better when there are zeros in the affinity matrix for pairs of items that are not in the same class. for our experiments  we chose k = 1; however  one may learn the optimal k in the same manner that  ng et al.  
1  learn the optimal scale factor 
1 	empirical results 
we compared the spectral learning algorithm in figure 1 to k-means on 1 data sets: 
  1 newsgroups a collection of approximately 1 postings from each of 1 usenet newsgroups.1 
  1 newsgroups 1 of the 1 newsgroups: sci.crypt  talk.politics.mideast  and soc.religion.christian. 
  lymphoma gene expression profiles of 1 normal and malignant lymphocyte samples. there are 1 classes: diffuse large b-cell lymphoma  1 samples   and non-dlcbl  1 samples   alizadeh  1 . 
　　1 from http://www.ai.mit.edu/~jrennie/1newsgroups/; a total of 1 documents. documents were stripped of headers  stopwords  and converted to lowercase. all numbers were discarded. all words that occur in more than 1 or less than 1 documents were removed. 

1 	learning 

  soybean is the soybean-largh data set from the uci repository. 1 classes.1 
the results are shown in table 1. the numbers reported are adjusted rand index values  hubert and arabie  1  for the clusters output by the algorithms. the rand index is frequently used for evaluating clusters  and is based on whether pairs are placed in the same or different clusters in two partitionings. the adjusted rand index ranges from - 1 to 1  and its key property is that the expected value for a random clustering is 1. the result that spectral methods generally perform better than k-means is consistent with the results in  ng et ai  1; brew and schulte im walde  1 . in some cases  the poor performance of k-means reflects its inability to cope with noise dimensions  especially in the case of the text data  and highly non-spherical clusters  in the case of the composite negative cluster for lymphoma .1 however  spectral learning outperforms k-means on the soybean datasct as well  which is a low-dimensional  multi-class data set. 
1 	spectral classification 
in the previous section  we described clustering a data set by creating a markov chain based on the similarities of the data items with one another  and analyzing the dominant eigenvectors of the resulting markov matrix. in this section  we show how to classify a data set by making two changes. first  we modify the markov chain itself by using class labels  when known  to override the underlying similarities. second  we use a classification algorithm in the spectral space rather than a clustering algorithm. 
1 	modifying the  interested reader  model 
the model described in section 1 can be modified to incorporate labeled data in the following simple manner. if the interested reader happens to be at a labeled document  the probability that she will choose another labeled document of the same category is high  while the probability that she will choose a labeled document of a different category is low  or zero . transition probabilities to unlabeled documents are still proportional to their similarity to the current source document  whether the current document is labeled or not. 
　wc wish to create a markov matrix that reflects this modified model. we propose doing this in the following manner  using the normalization introduced in section 1. for most similarity functions  the maximum pairwise similarity value is 1  and the minimum similarity is 1. therefore  we would like to say that two points in the same class are maximally similar  and two points in different classes are minimally similar: 
   1 thcrc arc has 1 instances  1 features  and 1 different classes. it is nominal; hamming distance was used. 
   1 for some of these sets  the k-means numbers are low. this is partially illusory  due to the zeroed expectation of the adjusted rand index  and partially a real consequence of the sparse highdimensionality of the text data. better k-means results on text typically require some kind of aggressive dimensionality reduction   usually lsa  another spectral method  or careful feature selection  or both . 
1. define the affinity matrix a as in the previous algorithms. 
1. first  for each pair of points  /  j  that are in the same class  assign the values atj = aj  - 1. 
1. likewise  for each pair of points  /'  j  that are in different classes  assign the values aij - aji - 1. 
1. 
　this gives us a symmetric markov matrix describing the  interested reader  process which uses supervisory information when present  and data similarities otherwise. a strength of this model lies in the fact that it incorporates unlabeled data  whereas the majority of classification models deal strictly with the labeled data. a benefit of additive normalization is that  after the affinities are adjusted  same-labeled pairs will always have a higher  or equal  mutual transition probability than unlabeled pairs. this will not necessarily be the case with other normalization schemes. 
1 a spectral classification algorithm 
again  if natural classes occur in the data  the markov chain described above should have cliques. furthermore  the cliques will become stronger as the number of labeled documents increases. given this model  we wish to categorize documents by assigning them to the appropriate clique in the markov chain. the spectral clustering methods given in section 1 can be adapted to do classification by replacing the final few steps  clustering in spectral space  with the steps shown in figure 1  which classify in spectral space . 
　the key differences between the spectral classifier and the clustering algorithm are  a  that our transition matrix a incorporates labeling information  and  b  wc use a classifier in the spectral space rather than a clustering method. what is novel here is that this algorithm is able to classify documents by the similarity of their transition probabilities to known subsets of b. because the model incorporates both labeled and unlabeled data  it should improve not only with the addition of labeled data  but also with the addition of unlabeled data. we observe this empirically in section 1. 
1 empirical results 
cliques 
it was suggested in section 1 that the markov chain described above will have cliques  that is  subsets of nodes in the graph that are internally fast-mixing  but are not mutually fast-mixing. figure 1 shows the thresholded sparsity pattern for the affinity matrices for the 1 newgroups data set  as labeled data is added. the left matrix is the affinity matrix for 1% labeled data. even the underlying similarities show block-like behavior  if weakly. to the extent that the unlabeled data gives a block-like affinity matrix  clusters naturally exist in the data; this is the basis for spectral clustering. the subsequent matrices have increasing fractions of data labeled. the effect of adding labeled data is to sharpen and coerce the natural clusters into the desired classes. as more labels are added  the blocks become clearer  the cliques become stronger  and  in the limit of 1% labeled data  the interested reader will never accidently jump from a document of one topic to one of another. 

learning 	1 


figure 1: categorization accuracy on the 1 newsgroups task as the number of  a  unlabeled and  b  labeled points increases. in  a   1 labeled documents and the given number of unlabeled docments were used as a training set. in  b   the training set is all of 1 newsgroups  with the given fraction labeled. in both cases  the test set for a given run consisted of all documents in 1 newsgroups whose labels were not known during training for that run. 

figure 1: categorization accuracy on the 1 newsgroups task as the  a  amount of unlabeled data and  b  fraction of labeled data increases. 
accuracies 
to validate the utility of spectral classification  we performed the following experiments on the 1 newsgroups data set. 
　we built two batch classifiers. the first was a standard multinomial naive bayes  nb  classifier with standard addone smoothing. the second was a spectral classifier as described above  which used a single-nearest neighbor classifier to perform the final classification in the spectral space. the affinity matrix a was the thresholded cosine similarity between documents.1 we note here that the thresholding is important  since it weakens the effect of outliers. furthermore  it saves space and computation time  since the resulting affinity matrix is sparse. 
　we split the data into a labeled training set and an unlabeled test set. for classification  the spectral classifier processed both the training and test set  but was evaluated on the test set only. 
　figure 1 a  shows the effect of using a sample of 1 documents from the 1 newsgroups data as a labeled training set  with an increasing number of unlabeled documents as a test set. the accuracy of the nb classifier is  of course  constant up to sampling variation  since it discards unlabeled data. the spectral classifier is more accurate than the nb classifier when given sufficiently many additional unlabeled 
　　1 for each document  we take the most similar 1 documents  and put those similarities in the appropriate row and column. all other entries are 1. 
documents to incorporate. 
　figure 1 b  shows the effect of supplying increasingly large fractions of the 1 newsgroups data set as labeled training instances  and using the remainder of the data set as test instances. the spectral classifier outperforms naive bayes  more substantially so when there is little labeled data. 
figures 1 a  and  b  show the same graphs for the 1 newsgroups data set. again  spectral classification performs well  especially when less than 1% of the data is labeled. it should be noticed that  for this data set  naive bayes outperforms the spectral classifier in the strongly supervised case   1% labeled data . the strength of spectral learning lies in incorporating unlabeled data  and  for the strongly supervised case  this is of less value. 
spectral space 
to further investigate the behavior of the spectral classifier  wc performed the following experiment. we took the 1 newsgroups data and labeled various fractions of each of the 1 classes. we then plotted each document's position in the resulting 1-dimensional spectral space  the space of the rows of the matrix x as defined by our spectral learning algorithm . figure 1 shows dimensions 1 and 1 of these plots. with no labels  the data does not tightly cluster. as we add labels  circled points   two things happen. first  the labeled points move close to same-labeled points  away from different-labeled points  and generally towards the outside  since they are  hubs  of the markov process. second  they pull the unlabeled points out radially along with them. this is effective in that it seems to pull the classes apart  even though the classes were not naturally very strong clusters in the unlabeled data. 
1 	related work 
in  yu and shi  1   a spectral grouping method  which they call  grouping with bias   is presented that allows for top-level bias  as in labeled data. they formulate the problem as a constrained optimization problem  where the optimal partition is sought  subject to the constraint that the normalized cut values of any two nodes that are preassigned to the same class should be the same. 
the main drawback with the algorithm in  yu and shi  
1  is that it only constrains same-labeled data points. the algorithm we present here benefits from the zeros in the sparsity pattern introduced by differently-labeled pairs. furthermore  it should be noted that  in the multi-class case  labeled sets combinatorialy tend to embody more differently-labeled pairs than same-labeled pairs. the other drawback to not using the information given by differently-labeled points is that the trivial partition  all points in one cluster  will satisfy the constraints  even when many points are labeled. in fact  when all the data is labeled  it is likely that the partition found by the grouping with bias algorithm will be the trivial partition. 
figure 1 a  shows that our spectral classifier outperforms the 
grouping with bias algorithm for the 1 newsgroups data set. in fact  grouping with bias started performing slightly worse when a large fraction of the data was labeled. 

1 	learning 


figure 1: three classes of the 1 newsgroups data set in spectral space with increasing mounts of labeled data. the classes are sci.crypt  pluses   talk.politics.midcast  dots   and soc.religion.christian  squares . the labeled points are circled. the bottom graphs show the sparsity patterns of the associated affinity matrices. 

1 	constrained spectral clustering 	1. must-links: two items are known to be in the same class. 
1. cannot-links: two items are in different classes. 
recently  there has been interest in constrained clustering constrained clustering allows one to do exploratory data  wagstaff and cardie  1; klein et al  1   which in- analysis when one has some prior knowledge  but not class lavolves clustering with two types of pairwise constraints: bels. the classifier presented in section 1 can be easily modi-
learning 	1 


figure 1:  a  classification accuracy vs. fraction of pairs constrained for spectral classifier vs. grouping with bias   b  rand index of spec-
tral constrained clustering vs. fraction of pairs constrained  shown up to 1% constrained . both are on 1 newsgroups data. 
fied for this kind of prior knowledge: we have been reducing labeling information to pairwise information during affinity modification all along. 
specifically  the affinity matrix is now defined as follows: 
1. define the affinity matrix a as before. 
1. for each pair of must-linked points  /  j  assign the val-

   this is equivalent to the imposing constraints step in  klein et al  1 . in this step  must-linked points are made more similar than any other pair of points in the data set  and cannot-linked points are made more dissimilar than any pair of points in the data set. 
　the spectral constrained clustering algorithm proceeds just as the other spectral clustering algorithms presented here; the only difference is that it uses the modified normalized affinity matrix presented in this section. 
   figure 1 b  shows a plot of accuracy vs. number of constraints for the 1 newsgroups data set using spectral constrained clustering. the accuracy is measured by the constrained rand index  klein et al  1; wagstaffand cardie  1 . the accuracy increases with number of constraints  showing that the spectral constrained clustering can effectively use constraints to better cluster data. 
1 	conclusion 
we present here a probabilistic model and an associated spectral learning algorithm that is able to work for the unsupervised  semi-supervised  and fully-supervised learning problems. we show that this algorithm is able to cluster well  and further is able to effectively utilize prior knowledge  either given by pairwise constraints or by labeled examples. 
acknowledgments 
this paper is based on research supported in part by the national science foundation under grant no. 1s-1  and by the research collaboration between ntt communication science laboratories  nippon telegraph and telephone corporation and csli  stanford university  research project on concept bases for lexical acquisition and intelligently reasoning with meaning . 
