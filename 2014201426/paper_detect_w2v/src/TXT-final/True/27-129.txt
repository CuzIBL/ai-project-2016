very large knowledge bases - architecture vs 
engineering 
	james hendler 	jaime carbonell 	douglas lenat 
university of maryland 	carnegie mellon university 	cycorp 
riichiro mizoguchi paul rosenbloom osaka university university of southern california 
panel topic 
in the past decade  al research has created important technologies. the annual investment in  ant1 return from  the several thousand existing systems employing al technology is in the hundreds of millions of dollars. ... one feature these successful programs have in common is that they work in well-defined domains in which the systems1 information  or knowledge base  kb   is not extremely large. typically  al systems produce their answers based on no more than several hundred facts concerning the area of their expertise. although this is enough for many interesting problems  algorithmic difficulties have prevented the scaling of al technology to much larger problems which require rapid access to many thousands or even millions of facts. such very large knowledge bases  vlkbs  are necessary to many applications however  particularly those motivated by the exponential growth of the information resources  and needs  of our society. 
   dealing with extremely large amounts of information has long been a challenge to some researchers in the field of al. for many people  the very phrase  artificial intelligence   con-
jures up a vision of an intelligent computer which can provide immediate access to vast amounts of information. such systems  like the hal 1 computer from arthur clarke's 1: 
a space odyssey or the super-human android  
lieutenant commander data  of the television program star trek: the next generation remain squarely in the realm of science fiction  but they are never far from the hearts of many al researchers. in fact  many early researchers in the field set out to create such programs. their failures led to the realization that to provide intelligent help in dealing with large amounts of information  an al system must itself have access to large amounts of knowledge. al scientists call this the  knowledge-is-power  hypothesis or  more simply   the knowledge principle  
 lenat and feigenbaum  1 .l 
¡¡the particular topic of this panel is to explore where such very large knowledge bases  vlkbs  are to come from. the panel will focus on comparing the imperatives for collecting knowledge  particularly broad knowledge across a wide swath of domains   as is being done in the cyc project  as opposed to developing architectures that are intended to learn the knowledge or to glean the knowledge bases fromn existing data repositories. contrasted with lenat's cyc project will be approaches including focused knowledge engineering  mizoguchi   integrated architectures such as the soar  project  rosenbloom   learning  carbonell   and large  hybrid knowledge and data bases  hendler . 
relevance 
there are several reasons why we believe this topic is particularly relevant at the current time: 
  the media attention  a/k/a hype  over the cyc project has caused massive speculation about the possibility of creation of vlkbs. this panel will include doug lenat  who can talk about the current status of cyc and current plans for its use. 
  despite the fact that cyc has become al-most synonymous with vlkb efforts  there are currently many other efforts to build and use large knowledge bases. this panel 
   'these first two paragraph  are taken  nearly verbatim  from .1. hendler   high performance artificial intelligence 1' science  1  aug 1  1  p. 1. they are used here with permission of the author. 
	hendler etal 	1 
will familiarize the audience with some of the other  perhaps less controversial  approaches being taken including large lexicons  hybrid knowledge/data bases  and the scaling of rule-based approaches. 
  information and knowledge technology have recently been the focus of major articles in the context of the american  national information infrastructure   nil  information superhighway  and the new  global information infrastucture   gii  infobahn . this panel will use the vlkbs to help expose the audience to some of the issues resulting from the scaling and use of ai  in the large.  
   currently  vlkbs are being pursued in numerous subfields of ai. among these areas are the following  represented by the members of this panel: 
  the best-known  and most talked about  effort in vlkbs is the cyc project. dr. lenat is the principle architect of this project. 
  in machine translation and nl projects  large lexicons are becoming both necessary and available. building  and learning  such lexicons has been a focus of dr. carbonell's work. 
  in the  cognitive architecture  area  ap-proaches are being explored to scale up to much larger systems. the soar  project  represented by dr. rosenbloom  is the most advanced of these architectures and has been examining the issues of scaling to very large rule-based systems. 
* work in japan resulted in the development of the electronic dictionary  the largest machine translation dictionary built to date. current work is attempting to scale this work into a practical and usable  comrnonsense  knowledge base. dr. riichiro mizoguchi is an active participant in the japanese project. 
  the use of high performance computing systems to support artificial intelligence research has been gaining significant interest in recent years. one of the most advanced projects in this area is dr. hendler's parka system  which uses parallel supercomputers in the support of massive knowledge bases and hybrid know ledge/data bases. 
1 	panels 
   we believe that this panel  therefore  visits several of the largest current projects in the area of building very large knowledge bases. as such  it should provide a broad background on which to discuss the critical question of the panel  where will these very large knowledge bases come from.  
position statements jaime carbonell  carnegie mellon university 
which comes first  the knowledge or the architecture in building large-scale ai systems  the question is not a chicken-and-egg conundrum  but a crucial  if unresolved  scientific issue. the extreme positions might be taken on the one hand by cyc believers to whom knowledge is virtually everything  and other hand by statisticians in tasks such as speech recognition where the holy grail is a very limited form of architecture  often a kind of markoff or basyan model  plus limitless training data  not to be confused with knowledge  they tell us . more moderate views stress the importance of both architecture and knowledge  where either may be general or task-specific. 
   my philosophy derives from machine learning. the architecture is - or should be the generator of the vast bulk of the knowledge in any very large-scale knowledge-based system. why  it is far easier to build learning architectures than to build truly massive but useful knowledge bases. soar  for instance  can build a million chunks automatically. prodigy builds thousand-case libraries from problemsolving experience. and  both systems actually use their large knowledge bases to solve new difficult problems efficiently. both architectures are capable of building new knowledge bases fully automatically in new domains. in contrast  the hand-crafted v l k b approach  has no such generative capabilities. however  learning architectures address the problem of the utility and organization of the knowledge they aquire  but fail to address the problem of cross-task generality of that knowlege. this remains one of the greatest challenges of the architecture-first approach  and a reason why there is still room for hand-built knowledge bases. 
j a m e s h e n d l e r   u n i v e r s i t y o f m a r y l a n d 
too much of the focus in knowledge base development  to date has been on the form of 

the data as it is input  without enough concentration on how to get it back out and how it. can be used. so-called knowledge engineering approaches  like cyc'  have focused on development of vlkbs with many different retrieval strategies  few of which scale well and all of which take a long time to retrieve complex facts. so-called architectural approaches  including soar and prodigy  have focused on learning of large amounts of data  and using them in fairly specific ways. unlike our counterparts in the database community  we've never put too much thought into how humans will access the facts in the vlkbs or how ai systems may use it. if inferences take minutes or hours  then memory becomes a major bottleneck. i therefore will argue that architecture is a primary concern  but not the  knowledge  architecture per se  but rather the computational architectures that will allow rapid access to vlkbs. 
d o u g l a s l e n a t   c y c o r p 
everyone else on this panel has clearly articulated their disagreement with my approach to getting v l k b s   which might be caricatured as  build it carefully and slowly  by hand.  but i find 1 must strongly disagree with them  in the sense that  my message to them is:  no  i actually agree with all of you!  namely  the cyc approach is merely to carry on manual knowledge entering so long as it's needed  to prime the pump  as it were  with knowledge so fundamental that it's easier to just tell the machine those things than to have it induce or deduce them. this is the knowledge which is a prerequisite for  real  automated learning  guided by plausible theories rather than dissociated statistics. it is also the knowledge which is a prerequisite for  real  automated understanding of natural language  and for that matter speech and images as well  and which therefore is required in order to break the chicken-and-egg codependency that carbonnel refers to. and even if we do our job well  hendler's sort of efficient reasoning machinery will be a must. the real points of disagreement among our group  apparently  are:  1  bow much knowledge needs to be manually represented  before automated methods can really take off  1 think it's quite a bit - several person-centuries' worth of effort and others think it may be drastically less.  1  is it important to get the architecture  right   the others think that the answer is yes that it's critically important in fact but i think the answer is no  that we can merely pick one and get started encoding the knowledge  and it will evolve as that kb-building enterprise unfolds. we have invested well over a person-century of time since the cyc project began in 1  following through on this philosophy  and its architecture has evolved quite dramatically in that time  in directions i neither expected nor welcomed. i'll try to convey some of the feel for those changes  and give some arguments for the two contrarian points of view i've listed above. 
r i i c h i r o m i z o g u c h i   osaka u n i v e r s i t y 
preparing for the coming advanced information society  japan is trying to set  up a national project called  human media  which aims at building a seamless information space. the future information technology has to cope with huge amount of knowledge represented in multimedia in a unified manner and to provide humans with sophisticated support for traveling around in a huge information space. through this project  we challenge some innovative research topics such as sharing and reuse of multimedia knowledge  ontology design for bridging the gap between computer media and human media and for integration of multimedia information  building very large knowledge bases based on multi-agent systems  etc  in this panel  1 would like to talk about the philosophy behind the project and the major research plans towards so-called  content-directed a l   . 
p a u l r o s e n b l o o m   u n i v e r s i t y o f s o u t h e r n c a l i f o r n i a 
the most effective way for an agent - either human or synthetic to learn large amounts of knowledge has to be for it to make use of whatever information the world provides to it. whether information is available in the form of theoretical statements  facts  data bases  experiences  guidance  lectures  books  examples  stories  images  or facial expressions  a failure to extract what  lessons the information has to offer will result in slower growth of the internal knowledge base. 'the problem though is that no existing synthetic agents can actually make use of all of these forms of information  or even a significant fraction of them . the pure knowledgeengineering position responds to this problem by reformulating information by hand so as to make it more easily usable by an agent  while the pure learning position responds by developing automatic reformulation mechanisms  also often called learning  understanding or comprehension mechanisms  that allow agents to directly accept a broader range of information. however  
	hendleretal 	1 

there is no essential reason why purity of either sort should actually be useful  especially given that some knowledge  such as the laws of physics  has taken centuries to extract from the raw data  while other information is quite easily extracted from everyday experience. our experience in working with soar agents that learn aligns with this mixed view  in that it is still much easier to spoon feed most conceptualizations of the world into such agents than it is for the agents to learn them autonomously  i.e.  the cyc position  ; while conversely  it can be easier for the agents to learn the many variations on a conceptual theme through experience than it is to hand code them  i.e.  the prodigy position  .  in addition  between these two extremes  learning from guided experience can sometimes do a 
reasonable job on both conceptualizations and variations.  since there tend to be more variations then distinct conceptualizations  the quantity of information learned can far outstrip the quantity hand coded - in fact  in our experience  by factors of thousands - even when most of the crucial information is hand coded. as carbonell points out in his note  considerable research is still necessary before agents will be able to autonomously acquire information that is very broad  e.g.  involving conceptualizations across multiple domains ; however  the same is also true for information that is very deep  e.g.  non-obvious scientific theories . in addition  as hendler points out in his note  research is needed on the efficient and effective retrieval of knowledge  as a function of an agent's goals and situation  from very large knowledge bases; although doorenbos has at least shown in soar that it is possible to acquire over a million rules  while still allowing their effective and efficient use. 
1 	panels 
