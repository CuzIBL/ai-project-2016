 
in this paper we present an average-case analysis of the nearest neighbor algorithm  a simple induction method that has been studied by many researchers. our analysis assumes a conjunctive target concept  noise-free boolean attributes  and a uniform distribution over the instance space. we calculate the probability that the algorithm will encounter a test instance that is distance d from the prototype of the concept  along with the probability that the nearest stored training case is distance e from this test instance. from this we compute the probability of correct classification as a function of the number of observed training cases  the number of relevant attributes  and the number of irrelevant attributes. we also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains  and give experimental results on these domains as a check on our reasoning. 
1. nearest neighbor algorithms 
　most learning methods form some abstraction from experience and store this structure in memory. the field has explored a wide range of such structures  including decision trees  quinlan  1   multilayer networks  rumelhart & mcclelland  1   and probabilistic summaries  fisher  1 . however  in recent years there has been growing interest in methods that store instances or cases in memory  and that apply this specific knowledge directly to new situations. this approach goes by many names  including instance-based learning and case-based reasoning  and one can apply it to many different tasks. 
　the simplest and most widely studied class of techniques  often called nearest neighbor algorithms  originated in the field of pattern recognition  cover & hart  
1; dasarathy  1  and applies to classification tasks. in the basic method  learning appears almost trivial one simply stores each training instance in memory. the power of the method comes from the retrieval process. given a new test instance  one finds the stored training case that is nearest according to some distance measure  notes the class of the retrieved case  and predicts the new instance will have the same class. 
　many variants exist on this basic algorithm. for instance  stanfill and waltz  1  have studied a version that retrieves the k closest instances and bases predic-
nearest neighbor algorithm 
wayne iba 
ai research branch 
mail stop 1 
nasa ames research center 
moffett field  ca 1 usa 
tions on a weighted vote  incorporating the distance of each stored instance from the test case; such techniques are often referred to as k-nearest neighbor algorithms. others  cover & hart  1; aha  kibler  & albert  
1  have studied an alternative approach that stores cases in memory only upon making an error  thus reducing memory load and retrieval time with little reduction in accuracy. 
　we would like to understand the learning behavior of this intriguing class of methods under various conditions. aha et al. present a pac analysis of one such algorithm  but our aim is to obtain tighter bounds that we can directly relate to experimental results. to this end  we decided to pursue an average-case analysis along the lines developed by hirschberg and pazzani  1  for logical induction methods and by langley  iba  and thompson  1  for probabilistic ones. for the sake of tractability  we focused our efforts on the most basic of the instance-based techniques  which stores all training cases and bases its prediction on the single nearest neighbor. 
　however  the simplicity of this method does not mean it lacks power. aha et al.  1  report the results of an experimental study that compared the algorithm  which they called ibl  to quinlan's  1  more sophisticated c1 algorithm for inducing decision trees. table 1 contains the results on four natural domains  two of them   cleveland  and  hungarian   involving prediction of heart disease from symptoms  another concerning the diagnosis of primary tumors  and a fourth involving prediction of party affiliations for members of congress from their voting records. for each domain  
aha et al. trained the algorithms on approximately 1% of the cases and tested them on the remaining instances  averaging over 1 different partitions. on the cleveland data  the two algorithms' performance was indistinguishable  and ibl's behavior on the tumor and voting records nearly reached c1's level. although the basic nearest neighbor algorithm fared much worse on the hungarian data set  simple modifications produce accuracy comparable to that for c1  aha  1   and its performance on the other domains argues that it deserves closer inspection in any case. 
　in the remainder of this paper  we report the initial results of our average-case analysis of the simple nearest neighbor method. we begin by presenting the as-
	langley and iba 	1 


1 	machine learning 


figure i. regions of interest for computing the probability of correctly classifying a negative test case /d  distance d from prototype p  using the stored training case je  which is distance e from id- the inner region depicts the probability that e  d - i  the outer region shows the probability that e   d + i  and the central region indicates the probability that d - t   e   d + i. 
　the second term from this equation  t jd-i d+i n  the contribution to the accuracy when the distance to the nearest stored training instance je is between the distances to the nearest possible positive instance and the farthest one  i.e.  d-i   e   d+i . this corresponds to the central region in the figure. in this situation  conflicts can occur during the classification process  in that the algorithm may retrieve both positive and negative training cases at distance e from the test case. 
　ties are possible in this region because  given t irrelevant attributes  there are 1* positive instances that can be located i steps or less away from the prototype. for any given test case id that is distance d from the prototype  the nearest stored positive case may be t steps away from the prototype in the direction toward id  i steps away from the prototype in the direction away from id  or somewhere between these two extremes. negative instances can also occur anywhere within this region  making the entire middle band in the figure open to the possibility of ties. 
　to handle all possible ties  we must sum over all distances e between d - * and d + i  then sum over the possible numbers k of nearest instances  positive or negative  that have been stored at each such distance. in each case  we must multiply the probability m je nk of that number occurring by the accuracy e je   that results from such a tie. we can state this formally as 
		 1  
we can further decompose the first term in the product into the probability that exactly k of the n training cases are distance e from the test case and that the remaining n - a: are at some greater distance  since the k cases are the nearest ones   giving 

as the formal expression. to compute the accuracy given a tie among a: stored cases  we must sum over the possible numbers j of negative instances  in each case multiplying the subaccuracy by the probability of that occurrence. this gives 

where j/k is the expected accuracy when one selects a training case at random from a set that contains j out of k negative instances. the term v c -d represents the probability of a positive instance je given that the instance is e steps away from negative test case / *. which is in turn d steps away from the prototype. we caexpand this term to 

which is the probability that my given training case will fall at distance e or greater fom the test instance  taken to power n to generate the probability that every training instance seen so far satisfies this condition. 
　now we can turn to a c n  the accuracy on positive test cases after n training instances. the situation here is simpler than for negative test cases  but still nontrivial. the algorithm is guaranteed to classify a positive test case id correctly only when the nearest stored training instance is itself the test case  i.e.  e = 1 . ties can occur anywhere in the range 1   e   i  giving the expression 
	langley and iba 	1 
note that here we must use the numerator k - j rather than k  in that we are dealing with positive test cases. moreover  we must take a different approach to computing v c +  the probability of a positive instance je given that the instance is e steps away from positive test case id- in this case  we have 
		 1  
when i   e  but zero in other situations. taken together  the definitions for a c  n   a  c  n   and their component terms let us predict the overall accuracy an for the nearest neighbor algorithm as a function of the number of training instances n  the number of relevant attributes r  and the number of irrelevant attributes i. 
1. behavioral implications of the analysis 
　although the equations in the previous sections provide a formal characterization of the nearest neighbor algorithm's behavior  their implications are not obvious. to better understand the effects of domain characteristics  we systematically varied certain domain parameters and examined the predicted results. in addition to computing theoretical predictions  we also collected experimental learning curves that summarized the algorithm's actual behavior. each datum on these curves reports the classification accuracy averaged over 1 runs on randomly generated training sets  measured over the entire space of uniformly distributed noise-free instances. in each case  we bound the mean accuracy with 1% confidence intervals to show the degree to which our predicted learning curves fit the observed ones. these experimental results provide an important check on our reasoning  and they identified a number of problems during development of the analysis. 
　figure 1 shows the effects of the number of relevant attributes in the conjunctive target concept. for this study  we held the number of irrelevant attributes i constant at one  and we varied both the number of training instances and the number of relevant attributes r. as typical with learning curves  the accuracy starts low and gradually improves as the algorithm encounters more training instances. the effects of target complexity also make sense. increasing the number of relevant features 
1 	machine learning 
figure 1. predictive accuracy of the nearest neighbor algorithm on a conjunctive concept  assuming the presence of one irrelevant attribute  as a function of training instances and the number of relevant attributes. the lines represent theoretical learning curves  whereas the error bars indicate experimental results. 
should increase the overall number of negative instances  giving higher accuracy early in the induction process; however  this factor also increases the total number of possible instances  requiring more training cases to reach asymptote and producing a crossover effect. the learning rate seems to degrade gracefully with increasing complexity  and the theoretical and actual learning curves are in close agreement  which lends confidence to the analysis. 
　the sensitivity of the nearest neighbor algorithm to irrelevant attributes is more dramatic  as shown in figure 1. this graph summarizes the results of a similar study of the interaction between the number of training instances n and the number of irrelevant attributes. here we held the number of relevant attributes constant at two  and we examined three levels of the i parameter. as with the previous study  the degradation in learning rate is graceful  but the effect is somewhat greater. the difference between the two results appears more significant when one realizes that increasing i does not reduce the proportion of positive instances  as does increasing the number of relevant attributes. these observations are consistent with aha's  1  reports on the sensitivity of nearest neighbor methods to the number of irrelevant attributes. 
　we can also compare the behavior of the nearest neighbor algorithm to that of other induction methods for which average-case analyses exist. in particular  pazzani and sarrett  1  have studied the wholist algorithm  which initializes its concept description to the conjunction of features in the first positive training instance  then removes any feature that fails to occur in later positive instances. similarly  langley  iba  and thompson  1  have analyzed the behavior of the bayesian classifier  a simple probabilistic method that stores observed 

figure 1. predictive accuracy of the nearest neighbor algorithm on a conjunctive concept  assuming the presence of two relevant attributes  as a function of training instances and the number of irrelevant attributes. the lines represent theoretical learning curves  whereas the error bars indicate experimental results. 
base rates and conditional probabilities. as pazzani and sarrett note  whollst's learning rate is unaffected by the number of relevant attributes  so their algorithm clearly scales up better on this dimension than does the nearest neighbor technique. comparison to the bayesian classifier on this factor is more difficult  in that langley et al.'s study examined equal probabilities for the two classes  whereas the current analysis assumes that they differ. 
　in some domains  effective learning relies more on the ability to handle many irrelevant features than many relevant ones. in this vein  we have shown analytically that the number of training instances required for wholist to achieve a given level of accuracy increases only with the logarithm of the number of irrelevant attributes. although we have not yet derived similar analytic relations for the nearest neighbor or probabilistic methods  we can use the existing analyses to estimate ability to scale on this dimension. 
　figure 1 graphs the predicted number of training instances needed to achieve 1% accuracy for each algorithm as a function of the number of irrelevant attributes  assuming a target concept involving only one relevant feature and a uniform distribution of instances. the analyses do not provide these quantities directly  but one can interpolate them from the theoretical learning curves. the figure reveals that the bayesian classifier scales well to increasing numbers of irrelevant attributes  with the dependent measure growing as an approximate linear function of this factor. in contrast  the number of training instances required by the nearest neighbor method grows much faster  although we cannot yet determine the precise superlinear relation. these results are also consistent with aha's  1  conclusions about the response of standard instance-based methods to many irrelevant features. 
figure 1. theoretical number of training instances  interpolated  required to reach 1% accuracy by a nearest neighbor algorithm and a simple bayesian classifier on a conjunctive concept  assuming the presence of one relevant attribute  as a function of the number of irrelevant attributes. 
　however  the above comparisons are not entirely fair. neither the w h o l i s t algorithm nor the bayesian classifier are designed to handle disjunctive concepts  which present no obstacles to even the simplest nearest neighbor algorithm. our focus on conjunctive concepts in the current analysis has obscured this strength. also  aha  1  has developed a variant of the nearest neighbor algorithm that retains statistics on the usefulness of each attribute  and he has shown that this approach fares better in domains with many irrelevant terms. nevertheless  the ability to make comparisons of the above type is one advantage of careful formal analyses  and they have provided insights about the relative strengths of the different learning algorithms. 
1. general discussion 
　in this paper we presented an average-case analysis of the most basic nearest neighbor algorithm. our treatment assumes that the target concept is conjunctive  that instances are free of noise  that attributes are boolean  and that instances are uniformly distributed. given information about the number of relevant and irrelevant attributes  our equations let us compute the expected classification accuracy after a given number of training instances. 
　to explore the implications of the analysis  we plotted the predicted behavior of the algorithm as a function of these three factors  finding graceful degradation as the number of relevants r and irrelevants i increased  but finding a stronger effect for the second. as a check on our analysis  we ran the algorithm on artificial domains with the same characteristics. the predicted behavior closely fit that found in the experiments  but only after correcting several errors in our reasoning that the empirical studies revealed. 
	langley and iba 	1 

　these results begin to account for the wide range of performance observed for the algorithm by aha and others on natural domains. however  a full explanation will require several extensions to the analysis. in particular  we must incorporate the influence of both class and attribute noise  as we have done in earlier analyses  iba & langley  1; langley et al.  1 . we must also handle situations in which each attribute follows a separate probability distribution  following the approach taken by hirschberg and pazzani  1 . 
　even more important  we must extend the framework to handle broader classes of target concepts. nearest neighbor methods are well suited for m of n concepts  in which any m of the n features in the prototype are sufficient for membership in the class. since distance from the prototype plays a central role in the current analysis  we believe extending it to handle such concepts will be quite feasible. similarly  because the algorithm stores many training instances in memory  it can easily acquire disjunctive concepts that require multiple prototypes. again  we hope that simple extensions to the existing framework will handle this situation. we should also generalize the analysis to include fc-nearest neighbor methods  following the lead recently provided by turney  in press . 
　another direction for future work would attempt to map the extended analysis onto natural domains in which there already exist experimental results with the method. given information about the distributions of attributes 
 which are available in the data   along with estimates of the noise levels and target concepts  which require informed guesses   we can compare learning curves predicted by the theory with those observed in experimental runs. this approach would extend the applicability of our average-case model beyond the artificial domains to which we have limited our tests to date. 
　in summary  we believe that our initial analysis has provided some useful insights about the behavior of the basic nearest neighbor algorithm. these begin to explain why the algorithm compares favorably with more complex induction methods on some domains but not others  and our results are consistent with intuitions about the algorithm's sensitivity to irrelevant attributes. we also believe the existing theoretical framework can be extended to handle more challenging target concepts and other factors that complicate the learning task  thus providing a solid base on which to carry out further studies of instance-based learning. 
acknowledgements 
we would like to thank stephanie sage and david aha for discussions that helped clarify our ideas  david aha and three reviewers for useful comments  and nib nilsson for his support and encouragement. the first author 
1 	machine learning 
contributed to this work while he was at the center for the study of language and information  stanford university  and the institute for the study of learning and expertise. 
