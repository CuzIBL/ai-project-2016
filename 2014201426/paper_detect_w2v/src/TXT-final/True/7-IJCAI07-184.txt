
in recent years  metric learning in the semisupervised setting has aroused a lot of research interests. one type of semi-supervised metric learning utilizes supervisory information in the form of pairwise similarity or dissimilarity constraints. however  most methods proposed so far are either limited to linear metric learning or unable to scale up well with the data set size. in this paper  we propose a nonlinear metric learning method based on the kernel approach. by applying low-rank approximation to the kernel matrix  our method can handle significantly larger data sets. moreover  our low-rank approximation scheme can naturally lead to out-of-sample generalization. experiments performed on both artificial and real-world data show very promising results.
1 introduction
1 semi-supervised learning
in supervised learning  we are given a training sample in the form of input-outputpairs. the learning task is to find a functional relationship that maps any input to an output such that disagreement with future input-output observations is minimized. classification and regression problems are the most common supervised learning problems for discrete-valued and continuous-valued outputs  respectively. in unsupervised learning  we are given a training sample of objects with no output values  with the aim of extracting some structure from them to obtain a concise representation or to gain some understanding of the process that generated the data. clustering  density estimation  and novelty detection problems are common unsupervised learning problems.
¡¡over the past decade or so  there has been growing interest in exploring new learning problems between the supervised and unsupervised learning extremes. these methods are generally referred to as semi-supervised learning methods  although there exist large variations in both the problem

¡¡¡¡  this research has been supported by competitive earmarked research grant 1 from the research grants council of the hong kong special administrative region  china.formulationand the approach to solve the problem. the semisupervised learning literature is too large to do a comprehensive review here. interested readers are referred to some good surveys  e.g.   zhu  1 .
¡¡one way to categorize many  though not all  semisupervised learning methods is to consider the type of supervisory information available for learning. unlike unsupervised learning tasks  supervisory information is available in semi-supervised learning tasks. however  the information is in a form that is weaker than that available in typical supervised learning tasks. one type of  weak  supervisory information assumes that only part  usually a limited part  of the training data are labeled. this scenario is commonly encountered in many real-world applications. one example is the automatic classification of web pages into semantic categories. since labeling web pages is very labor-intensive and hence costly  unlabeled web pages are far more plentiful on the web. it would be desirable if a classification algorithm can take advantage of the unlabeled data in increasing the classification accuracy. many semi-supervised classification methods  zhu  1  belong to this category.
¡¡another type of supervisory information is even weaker in that it only assumes the existence of some pairwise constraints indicating similarity or dissimilarity relationships between training examples. in video indexing applications  for example  temporalcontinuity in the data can be naturally used to impose pairwise similarity constraints between successive frames in video sequences. another example is in proteomic analysis  where protein-protein interactions can naturally be represented as pairwise constraints for the study of proteins encoded by the genes  e.g.  database of interacting proteins  dip   http://dip.doe-mbi.ucla.edu/ . yet another example is the anti-spam problem. recent studies show that more than half of the e-mail in the internet today is spam  or unsolicited commercial e-mail. one recent approach to spam detection is based on the trustworthiness of social networks  boykin and roychowdhury  1 . such social networks can naturally be represented as graphs with edges between nodes representing pairwise relationships in the social networks.
while supervisory information in the form of limited la-
beled data can be transformed into pairwise similarity and dissimilarity constraints  inverse transformation is in general not possible except for the special case of two-class problems.
in this sense  the second type of supervisory information is of a weaker form and hence the corresponding learning problem is more difficult to solve. the focus of our paper is on this category of semi-supervised learning problems.
1 semi-supervised metric learning based on pairwise constraints
while many semi-supervised learning methods assume the existence of limited labeled data  zhu  1   there are much fewer methods that can work with pairwise constraints only. we survey the most representativemethods in this subsection.
¡¡very often  the pairwise constraints simply state whether two examples belong to the same class or different classes.  wagstaff and cardie  1  first used such pairwise information for semi-supervised clustering tasks by modifying the standard k-means clustering algorithm to take into account the pairwise similarity and dissimilarity constraints. extensions have also been made to model-based clustering based on the expectation-maximization  em  algorithm for gaussian mixture models  shental et al.  1; lu and leen  1 . however  these methods do not explicitly learn a distance function but seek to satisfy the constraints  typically for clustering tasks. hence  they are sometimes referred to as constraint-based clustering methods.
¡¡as a different category  some methods have been proposed to learn a mahalanobis metric or some other distance function based on pairwise constraints.  xing et al.  1  formulated a convex optimization problem with inequality constraints to learn a mahalanobis metric and demonstrated performance improvement in a subsequent clustering task. solving a similar problem to learn a mahalanobis metric  the relevant component analysis  rca  algorithm  bar-hillel et al.  1; 1  was proposed as a simpler and more efficient algorithm than that of  xing et al.  1 . however  rca can make use of similarity constraints only.  hertz et al.  1  proposed a distance function learning method called distboost. however  there is no guarantee that the distance function learned is a metric.  bilenko et al.  1  explored the possibility of integrating constraint-based clustering and semi-supervised clustering based on distance function learning. the distance function learning methods reviewed above either learn a mahalanobis metric that corresponds to linear transformation only  or learn a distance function that is not a metric. in our previous work  chang and yeung  1   we proposed a metric learning method that corresponds to nonlinear transformation. while this method is more powerful than the linear methods  the optimization problem is non-convex and hence is more complicated to solve.
¡¡in this paper  we focus on the distance function learning approach because distance functions are central to many learning models and algorithms. this also makes it easier to achieve out-of-sample generalization. moreover  we focus on learning metrics because this allows us to formulate the metric learning problembased on the kernel approach scho¡§lkopf and smola  1   which provides a disciplined  computationally appealing approach to nonlinear metric learning.
1 organization of paper
in section 1  we propose a simple and efficient kernel-based metric learning method based on pairwise similarity constraints. like many kernel methods  a limitation of this method is that it does not scale up well with the sample size. in section 1  we address the scalability issue by applying low-rank approximation to the kernel matrix. this extended method can also naturally give rise to out-of-sample generalization  which is addressed in section 1. we present some experimental results in section 1 to demonstrate the effectiveness of our metric learning algorithm. finally  section 1 concludes the paper.
1 kernel-based metric learning
1 problem setup
let be a set of n data points in some input space x. suppose we have a mercer kernel k  which induces a nonlinear feature map ¦Õ  from x to some reproducing kernel hilbert space h  scho¡§lkopfand smola  1 . the correspondingset of feature vectors in and the kernel matrix is
k  . choices for k  include the gaussian rbf kernel and the polynomial kernel. we apply a centering transform such that the feature vectors in h have zero mean. the resulting kernel matrix k can be computed as k 
hkh    where the centering matrix h = i    1/n 1t with i being the n¡Án identity matrix and 1 being the n¡Á1 vector of ones.
¡¡we consider a type of semi-supervised metric learning in which the supervisory information is given in the form of pairwise similarity constraints.1 specifically  we are given a set of point pairs  which is a subset of x ¡Á x  as s = { xi xj |xi and xj belong to the same class}. our goal is to make use of s to learn a better metric through modifying the kernel so that the performance of some subsequent task  e.g.  clustering  classification  based on the metric is improved after kernel learning.
1 kernel learning
since the kernel matrix k is symmetric and positive semidefinite  we can express it as k 
  where ¦Ë1 ¡Ý ¡¤¡¤¡¤ ¡Ý ¦Ëp   1 are the p ¡Ü n positive eigenvalues of k  v1 ... vp are the corresponding normalized eigenvectors  and kr = vrvrt.
¡¡we consider a restricted form of kernel matrix learning by modifying k through changing the ¦Ër's while keeping all kr's fixed. to ensure that the eigenvalues are nonnegative  we rewrite k as k¦Â =  k¦Â xi xj  n¡Án =
  which represents
a	family of	kernel matrices parameterized by ¦Â =
 ¦Â1 ... ¦Âp t.
¡¡we perform kernel learning such that the mean squared euclidean distance induced by k¦Â between feature vectors in h corresponding to point pairs in s is reduced. thus the criterion function for optimization is

where bi is the ith column of the p ¡Á p identity matrix1 and ds is a p ¡Á p diagonal matrix with diagonal entries
kr bi   bj 

¡¡to prevent ¦Â from degenerating to the zero vector 1 and to eliminate the scaling factor  we minimize the convex function js ¦Â  subject to the linear constraint 1t¦Â = c for some constant c   1. this is a simple convex optimization problem with a quadratic objective function and a linear equality constraint. we introduce a lagrange multiplier ¦Ñ to minimize the following lagrangian:
	js ¦Â ¦Ñ  = js ¦Â  + ¦Ñ c   1t¦Â .	 1 
¡¡the optimization problem can be solved easily to give the optimal value of ¦Â as the following closed-form solution:
	.	 1 
note that d 1 exists as long as all the diagonal entries of are positive  which is usually true.s 1 we set the constant
r.
1 scalable kernel learning
the kernel learning method described above requires performing eigendecomposition on k. in case n is very large and hence k is a large matrix  operations such as eigendecomposition on k are computationally demanding. in this section  we apply low-rank approximation to extend the kernel learning method so that it scales up well with n.
1 low-rank approximation
we apply low-rank approximation to approximate k by another n ¡Á n symmetric and positive semi-definite matrix
                 k wlwt 	 1  where w ¡Ê rn¡Ám is an n ¡Á m matrix and l ¡Ê rm¡Ám is symmetric and positive semi-definite matrix  with
¡¡there are different ways to construct l for low-rank approximation. we consider one way which constructs l using a subset of the n data points. we refer to these points as landmarks  de silva and tenenbaum 1; weinbergeret al.  1; silva et al.  1 . without loss of generality  we assume that the n points are ordered in such a way that the first m points form the set of landmarks. we should ensure that all points involved in s are chosen as landmarks. other landmarks are randomly sampled from all the data points. similar to k in the previous section  l is obtained here by applying the centering transform to l   as l = hlh    where l   is the upper-left m ¡Á m submatrix of k  .
we apply eigendecomposition on l and express it as
                 l  	 1 
where ¦Ì1 ¡Ý ¡¤¡¤¡¤ ¡Ý ¦Ìq   1 are the q ¡Ü m positive eigenvalues of l  ¦Á1 ... ¦Áq are the corresponding normalized eigenvectors  d¦Ì = diag ¦Ì1 ... ¦Ìq   and v . substituting  1  into  1   we can rewrite   where.
1 kernel learning
we apply low-rank approximation to devise a scalable kernel learning algorithm which can be seen as an extension of the k and define the following parameterized family of kernel algorithm described in section 1. we use k to approximate
matrices: . note  however  that ¦Â is now a q ¡Á 1p ¡Á 1 vector.
¡¡the optimal value of ¦Â has the same form as  1   except that the constant c is set to and ds is now a q¡Áq diagonal matrix with diagonal entries

1 computing the embedding weights
a question that remains to be answered is how to obtain w for low-rank approximation. we use a method that is similar to locally linear embedding  lle   roweis and saul  1; saul and roweis  1   with two differences. first  we only use the first part of lle to obtain the weights forlocally linear fitting. second  we performlocally linear fitting in the kernelinduced feature space h rather than the input space x.
¡¡let w =  wij n¡Ám and wi =  wi1 ... wim t. if xi is a landmark  i.e.  1 ¡Ü i ¡Ü m  then
	1	i = j
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ 1  1	otherwise.
if xi is not a landmark  then we minimize the following function to obtain wi:
	 	 1 
where ni is the set of k nearest landmarks of ¦Õ xi  in h  subject to the constraints and
wij = 1 for all. we can rewrite e wi  as
 1 
where
gi =  k xi xi  + k xj xk    k xi xj    k xi xk  k¡Ák
 1 
is the local gram matrix of xi in h.
¡¡to preventwi from degeneratingto 1  we minimize e wi  subject to the constraintswi = 1 and wij = 1 for all. as above for the kernel learning problem  we solve a convex optimization problem with a quadratic objective function and a linear equality constraint. the lagrangian with a lagrange multiplier ¦Á is as follows:
	l wi ¦Á  = witgiwi + ¦Á 1   1twi .	 1 
the closed-form solution for this optimization problem is given by wi =  g i 1 / 1tg i 1  if g i 1 exists.1
¡¡instead of performing matrix inversion  a more efficient way of finding the solution is to solve the linear system of equations giw i = 1 for w i and then compute wi as wi = w i/ 1tw i  to ensure that the equality constraint 1twi = 1 is satisfied.
¡¡we assume above that the neighborhood relationships between points in h and the local gram matrices remain fixed during the kernel learning process. a simple extension of this basic algorithm is to repeat the above procedure iteratively using the learned kernel at each iteration. thus the basic algorithm is just a special case of this iterative extension when the number of iterations is equal to one.
1 out-of-sample generalization
the exact form of out-of-sample generalization depends on the operation we want to perform. for example  the n given points are first clustered into c ¡Ý 1 classes after kernel learning  and then a new data point x is classified into one of the c classes. we are interested in the case where both the clustering of the n points and the classification of new points are based on the same euclidean metric in h.
¡¡the key idea of our out-of-sample generalization scheme rests on the observation that kernel principal component analysis  kpca   scho¡§lkopf et al.  1  can be performed on  to obtain an embedding in a q-dimensional subspace y of h  so that the non-landmark points and any out-of-sample point x can be embedded into y in the same way.
¡¡let {u1 ... uq} be an orthonormal basis with each ur ¡Ê h being a unit vector along the direction of the rth principal component. we define u =  u1 ... uq . then each landmark xi  i = 1 ... m  can be embedded into y to give a q-dimensional vector yi = ut¦Õ xi . since u  we can express yi as yi =
d. let ym =  y1 ... ym . so
y.
for the non-landmark points  from  1   we use
to approximate ¦Õ xi  and yi
. thus we have
yi = ut¦Õ xi  = ymwtb
   similarly  for any out-of-sample example	x  we use denote the embedding of ¦Õ x  in y. the embedding weights  to approximate ¦Õ x  and y to
w =  w1 ... wm t can be determined as in section 1. similar to the non-landmark points  we can ob-
tain
	y.	 1 
based on  1  and  1   the squared euclidean distance be-
tween	and	in y as d1 xi 
	=	 bti w	 .
the squared euclidean distance after kernel learning is
 	 
where d¦Â = diag ¦Â1 ... ¦Âq1 .
1 experimental results
in this section  we present some experiments we have performed based on both artificial and real-world data.
1 experimental setup
we compare two versions of our kernel-based metric learning method described in sections 1 and 1 with rca  bar-hillel et al.  1; 1   which is a promising linear metric learning method that usually performs equally well as other computationally more demanding methods. for baseline comparison  we also include two metrics without metric learning. they are the euclidean metric in the input space and the euclidean metric in the feature space induced by a gaussian rbf kernel.
¡¡for each data set  we randomlygenerate1 differents sets. for small data sets  we can learn k¦Â without low-rank approximation. for large data sets  in addition to the data points involved in s  we also randomly select some other points as landmarks for learning . we use the iterative extension of the scalable kernel learning algorithm with the number of iterations equal to 1. we also measure the change in metric learning performance as the number of landmarks increases.

	 a 	 b 	 c 

	 d 	 e 	 f 
figure 1: xor illustration.	 a  input data and similarity constraints;  b  new data points;  c  rca;  d  rbf;  e  k¦Â;.
1 an illustrative example
figure 1 uses the xor data set to compare the performance of different metrics  some with metric learning. figure 1 a  shows 1 data points in the input space. the points with the same color and mark belong to the same class. the randomly generated point pairs corresponding to the similarity constraints in s are shown in solid lines. figure 1 b  shows 1 new data points in the input space. the results obtained by rca  rbf kernel and two versions of our method are shown in figure 1 c - f . rca performs metric learning directly in the input space  which can be generalized to new data using the learned linear transformation. for the kernel methods  we apply kpca using the  learned  kernel matrix to embed the data points to a 1-dimensional space  as shown in figure 1 d - f . new data points can be embedded using the generalization method described in section 1. as expected  rca does not perform satisfactorily for the xor data set since it can only perform linear transformation. on the other hand  our kernel-based metric learning method can group the points according to their class membership. the result of the scalable kernel learning method with low-rank approximation based on 1 landmarks is almost as good as that of the basic algorithm based on all 1 data points. moreover  the result on embedding of new data points verifies the effectiveness of our out-of-sample generalization method.
1 quantitative performance comparison
let be the set of true class labels of the n data points. we define the following performance measure:
	 	 1 
where is the mean betweenclass distance with nb being the number of point pairs with different class labels  andxj is the mean within-class distance with nw being the number of point pairs with the same class label. note that j is closely related to the optimization criterion js in  1   except that js is defined for the labeled data  i.e.  data involved in the pairwise constraints  only while j is for all data assuming the existence of true class labels. for kernel methods  we use ¦Õ xi  the mean distances and hence j. a larger value of j correor ¦Õ xi  in place of xi and apply the kernel trick to compute
sponds to a better metric due to its higher class separability.
¡¡we first perform some experiments on a much larger xor data set with 1 data points. we randomly select 1 similarity constraints to form s and measure the metric learning performance in terms of the j value for an increasing number of landmarks. table 1 shows the results for different metrics. for the metric learning methods  i.e.  rca and our method   we show for each trial the mean  upper  and standard deviation  lower  over 1 random runs corresponding to different s sets. from the results  we can see that our method outperforms the other methods significantly. moreover  using more landmarks generally gives better results.
table 1: performance comparison in terms of j value for xor data set  |s| = 1  m = 1:1 .
input datarbfrcaour methodm =1m =1m =1m =1m =1m =1m =1m =1.1.1.1.1.1.1.1.1.1.1.1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1table 1: performance comparison in terms of j value for isolet and mnist data sets  |s| = 1  m = 1 .
isoletmnist{1}{1}{1}{1}{1}{1 1}{1 1}{1 1}{1 1}input data1111111111rbf1111111111rca1111111111¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1ourmethod1111111111¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡À1¡¡we further perform some experiments on real-world data sets. one of them is the isolet data set from the uci machine learning repository which contains 1 isolated spoken english letters belonging to 1 classes  with each letter represented as a 1-dimensional vector. other data sets are handwritten digits from the mnist database.1 the digits in the database have been size-normalized and centered to 1¡Á1 gray-level images. hence the dimensionality of the input space is 1. in our experiments  we randomly choose 1 images for each digit from a total of 1 digit images in the mnist training set. we use 1 similarity constraints and 1 landmarks in the experiments. the results for isolet and different digit data sets are shown in table 1. from the results  we can again see that the metric learned by our method is the best in terms of the j measure.
1 concluding remarks
we have presented a simple and efficient kernel-based semisupervised metric learning method based on supervisory information in the form of pairwise similarity constraints. not only does it scale up well with the data set size  it can also naturally lead to out-of-sample generalization. although previous studies by other researchers showed that pairwise dissimilarity constraints usually cannot help much in many realworld applications  there are situations when incorporating them may still be helpful and hence we plan to extend our method to incorporate dissimilarity constraints as well. in our low-rank approximation scheme  besides including those points involved in s  we also randomly sample some other points as landmarks. a recent study  silva et al.  1  shows that non-uniform sampling of landmarks for manifold learning can give parsimonious approximations using only very few landmarks. we will pursue research along this direction in our future work.
