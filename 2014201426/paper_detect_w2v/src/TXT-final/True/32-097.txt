 
the success of evolutionary methods on standard control learning tasks has created a need for new benchmarks. the classic pole balancing problem is no longer difficult enough to serve as a viable yardstick for measuring the learning efficiency of these systems. the double pole case  where two poles connected to the cart must be balanced simultaneously is much more difficult  especially when velocity information is not available. in this article  we demonstrate a neuroevolution system  enforced sub-populations  esp   that is used to evolve a controller for the standard double pole task and a much harder  non-markovian version. in both cases  our results show that esp is faster than other neuroevolution methods. in addition  we introduce an incremental method that evolves on a sequence of tasks  and utilizes a local search technique  deltacoding  to sustain diversity. this method enables the system to solve even more difficult versions of the task where direct evolution cannot. 
1 	introduction 
the pole-balancing or inverted pendulum problem has been established as a standard benchmark for artificial learning systems. for over 1 years researchers in fields ranging from control engineering to reinforcement learning have tested their systems on this task  schaffer and cannon  1; michie and chambers  1; anderson  1   there are two primary reasons for this longevity:  1  pole balancing has intuitive appeal. it is a real-world task that is easy to understand and visualize. it can be performed manually by humans and implemented on a physical robot.  1  it embodies many essential aspects of a whole class of learning tasks that involve temporal credit assignment  sutton  1 . in short  it is an elegant environment that is a good surrogate for more general problems. 
　despite this long history  the relatively recent success of modern reinforcement learning methods on control learning tasks has rendered the basic pole balancing problem obsolete. it can now be solved so easily that it provides little or no insight about a system's ability. neuroevolution  ne  systems  i.e. systems that evolve neural networks using genetic algorithms   for example  often find solutions in the initial random population  moriarty and miikkulainen  1; gomez and miikkulainen  1 . in response to this need for a new benchmark  the basic pole-balancing task has been extended in a variety of ways. 
　wieland presented several variations to the standard single pole task that can be grouped into two categories:  1  modifications to the mechanical system itself such as adding a second pole either next to or on top of the other.  1  restricting the amount of state information that is given to the controller; for example  only providing the cart position and the pole angle. the most challenging of these is a double pole configuration where two poles of unequal length must be balanced simultaneously. even with complete state information this problem is very difficult requiring extremely precise control to solve. in this paper  we demonstrate a neuroevolution method  enforced sub-populations  esp;gomez and miikkulainen 1   on an even harder version of this task in which the two poles must be balanced without velocity information. this task represents a significant leap in terms of difficulty. 
we show that esp can solve this task  and can do so more efficiently than other methods have been able to solve it even with velocity information. 
　an interesting aspect of the double pole system is that it is more difficult to control as the poles assume similar lengths. when the poles are very close in length  solutions to this system cannot be evolved directly by current methods. in order to control the system under these conditions  shaping  or incremental learning  techniques can be employed that increase the length of the shorter pole very gradually  wieland  1; saravanan and fogel  1 . this kind of approach is effective but can be extremely slow due to the limitations of the underlying evolutionary search method-many generations are required to recover from minute changes to the environment using an incremental approach in conjunction with a local search technique  delta-coding; whitley et al. 1  to sustain diversity  we demonstrate that esp can cope with more significant changes to the environment. instead of evolving on the goal task directly  esp evolves on a sequence of increasingly difficult tasks. 
the paper is organized as follows. section 1 describes the 
esp and delta-coding algorithms. in section 1 we describe incremental evolution in detail. in section 1 we show the 

1 	uncertainty and probabilistic reasoning 


figure 1: symbiotic  adaptive neuro-evolution  sane . 
the population consists of hidden neurons  each with its own input and output connections. the networks are formed by randomly choosing u neurons for the hidden layer. networks are evaluated in the task  and the fitness is distributed among all the neurons that participated in the network. after all neurons are evaluated this way  recombination is performed in the neuron population. 
results for three different tasks:  1  the double pole with velocities   1  double pole without velocities  and  1  double pole without velocities demonstrating incremental evolution to almost equal pole lengths. the last two sections contain a discussion of the results and the conclusion. 
1 neuro-evolution method: enforced sub-populations + delta-coding. 
the neuroevolution method used is based on symbiotic  adaptive neuro-evolution  sane; moriarty  1; moriarty and miikkulainen  1 . sane has been shown to be a powerful reinforcement learning method for tasks with sparse reinforcement. 
1 sane 
sane differs from other ne systems in that it evolves a population of neurons instead of complete networks  figure 1 . these neurons are combined to form hidden layers of feedforward networks that are then evaluated on a given problem. evolution in sane proceeds as follows: 
1. initialization. the number of hidden units u in the networks that will be formed is specified and a population of neuron chromosomes is created. each chromosome encodes the input and output connection weights of a neuron with a random string of binary numbers. 
1. evaluation. a set of u neurons is selected randomly from the population to form a hidden layer of a feedforward network. the network is submitted to a trial in which it is evaluated on the task and awarded a fitness score. the score is added to the cumulative fitness of each neuron that participated in the network. this process is repeated until each neuron has participated in an average of e.g. 1 trials  
1. recombination. the average fitness of each neuron is calculated by dividing its cumulative fitness by the number of trials in which it participated. neurons are then ranked by average fitness. each neuron in the top quartile is recombined with a higher-ranking neuron using 
1 -point crossover and mutation at low levels to create 

figure 1: the enforced sub-populations method  esp . the population of neurons is segregated into sub-populations shown here as clusters of circles. the network is formed by randomly selecting one neuron from each sub-population. 
the offspring to replace the lowest-ranking half of the population. 
1. the evaluation-recombination cycle is repeated until a network that performs sufficiently well in the task is found. 
　in sane  neurons compete on the basis of how well  on average  the networks in which they participate perform. a high average fitness means that the neuron contributes to forming successful networks and  consequently  suggests a good ability to cooperate with other neurons. over time  neurons will evolve that result in good networks. 
　the sane approach has proven faster and more efficient than other reinforcement learning methods such as adaptive heuristic critic  q-learning  and standard neuroevolution  in  for example  the basic pole balancing task and in the robot arm control task  moriarty  1; moriarty and miikkulainen  1 . 
1 enforced sub-populations  esp  
in enforced sub-populations  as in sane  the population consists of individual neurons instead of full networks  and a subset of neurons are put together to form a complete network. however  esp allocates a separate population for each of the u units in the network  and a neuron can only be recombined with members of its own sub-population  figure 1 . 
　esp speeds up sane evolution for two reasons: the subpopulations that gradually form in sane are already circumscribed by design in esp. the  species  do not have to organize themselves out of a single large population  and their progressive specialization is not hindered by recombination across specializations that usually fulfill relatively orthogonal roles in the network. second  because the networks formed by esp always consist of a representative from each evolving specialization  a neuron is always evaluated on how well it performs its role in the context of all the other players. in sane  networks can contain multiple members of some specializations and omit members of others  and its evaluations are therefore less consistent. 
　the main contribution of esp  however  is that it allows evolution of recurrent networks. since sane forms networks by randomly selecting neurons from a single population  a neuron cannot rely on being combined with similar neurons in any two trials. a neuron that behaves one way in one trial may behave very differently in another  resulting in evaluations of neuron fitness that are very noisy. the sup-population 
	gomez and miikkulainen 	1 

architecture of esp makes the evaluation of the neurons more consistent. a neuron's recurrent connection weight ri will always be associated with neurons from sub-population si. as the sub-populations specialize  neurons evolve to expect  with increasing certainty  the kinds of neurons to which they will be connected. therefore  the recurrent connections to those neurons can be adapted reliably. 
　as evolution progresses  each sub-population will decline in diversity. this is a problem  especially in incremental evolution  because a converged population cannot easily adapt to a new task. to accomplish task transfer despite convergence  
esp is combined with an iterative search technique known as delta-coding. 
1 	delta-coding 
the idea of delta-coding  whitley et al.  1  is to search for optimal modifications of the current best solution. in a conventional single-population ga  when the population of candidate solutions has converged  delta-coding is invoked by first saving the best solution and then initializing a population of new individuals called -chromosomes. the chromosomes have the same length  number of genes  as the best solution and they consist of values  that represent differences from the best solution. the new population is evolved by selecting -chromosomes  adding their values to the best solution  and evaluating the result. those 
-chromosomes that improve the solution are selected for reproduction. therefore  delta-coding explores the hyperspace in a  neighborhood'* around the best previous solution. 
delta-coding can be applied multiple times  with successive 
-populations representing differences to the previous best solution. 
　in the experiments presented in this paper  delta-coding is implemented with the esp sub-population architecture. once the neuron sub-populations have reached minimal diversity  the best solution  i.e. the best network specification  is saved. new sub-populations are then initialized with -chromosomes so that each neuron in the best solution has a dedicated sub-population of -chromosomes that will be evolved to improve it specifically. esp selects a 
-chromosome from each sub-population and adds the values to the connection weights of the neurons in the bestsolution. when these sub-populations converge the best chromosomes are added to the best solution to form the new best solution for the next iteration of the delta phase. 
　delta-coding was originally developed to enhance the fine local tuning capability of genetic algorithms for numerical optimization by whitley et al. . gomez and miikkulainen   showed how delta-coding can be used to facilitate incremental evolution. when a task was completed the best solution was saved and -populations initialized before evolution was begun on the next task. a more general approach has been taken in the experiments described in this paper. delta-coding is activated whenever the system's performance ceases to improve over a predefined number of generations. this strategy limits the disruption of genetic building blocks when the population is still adjusting well to task changes by only introducing additional variation when necessary. 

figure 1: the double pole system. snapshot of 1d real-time display available at 
http://www.cs.utexas.edu/users/inaki/esp/twopole-demo. 
1 	incremental evolution 
evolutionary search methods can be ineffective if the task is too demanding to exert significant selective pressure on the population during the early stages of evolution. in such a case  all individuals perform poorly and the oa gets trapped in an unfruitful region of the solution space. one remedy is to enlarge the population size so that a more diverse set of phenotypes is sampled. however  prohibitively large populations may be required to discover individuals with sufficient competence to direct the search. another approach is to view the goal task as one of many possible instances of a more general parameterized task. the system then learns by evolving on a 
sequence of increasingly difficult evaluation tasks culminating in the intended goal task. 
　a number of researchers have applied task decomposition  or shaping  to make learning complex tasks tractable 
 colombetti and dorigo  1; perkins and hayes  1; singh  1 . typically  in these approaches the complex task is broken into simpler components or subtasks that are each learned by separate systems  e.g. gas or rule-bases  and then combined to achieve the goal task. in contrast  in incremental evolution as proposed in this paper  and also used by wieland  and saravanan and fogel    a single system learns a succession of tasks. such an adaptation process is similar to continual  or lifelong  learning  elman  1; ring  1   and motivated by staged learning in real life. 
1 pole balancing experiments 
the starting point for our experiments is the more challenging double pole problem in which a second pole is placed next to the first  figure 1 . the objective is to apply force to the cart at regular time intervals such that the poles are balanced indefinitely and the cart stays within the track boundaries. the state of this system is defined by six state variables: the angle of each pole from vertical the angular velocity of each 
pole the position of the cart on the track x  and the velocity of the cart  where   see wieland for the equations of motion and parameters used in this task . we adopt the notation to denote the evaluation-task where is the length of the short pole in meters. the long pole is always set to 1 meter. three different experiments were conducted using this configuration with the following three goal tasks: 

1 	uncertainty and probabilistic reasoning 

1. e1 with velocity information. 
1. e1 without velocity information. 1. e1 without velocity information. 
　　all of the pole balancing experiments were implemented using the runge-kutta fourth-order method with a step size of 1s. the state variables were scaled to  before being input to the network. during simulation the networks output a force value every 1 seconds  i.e. time step  in the range for tasks 1 and 1  the initial angle for the long pole was set to  so that the networks could not control the system by simply outputting values close to zero   and fitness was determined by the number of time steps a network could keep both poles within  degrees from vertical and keep the cart between ends of a 1 meter track. a task was considered solved if a network could balance the poles for 1 time steps  which is equal to over 1 minutes in simulated time. for task 1  the fitness function and starting state are described in section 1. neuron chromosomes were encoded as strings of floating point numbers. arithmetic crossover was used to generate new neurons. each chromosome was mutated with probability 1  replacing a randomly chosen weight value with a random value within the range  1 1 . the techniques and parameters were found effective experimentally; small deviations from them produce roughly equivalent results. 
1 related work 
we compared the performance of esp with sane and the published results of three other evolutionary methods. the first two  wieland  1; saravanan and fogel  1  have been applied to the double pole problem with velocities. wieland used an ne approach which we have termed conventional ne. this is a single population method for evolving neural networks in which each individual represents a complete network. fogel and saravanan use evolutionary programming  a general mutation-based approach that generates offspring by perturbing the best individuals with gaussian noise. for the case without velocities  gruau et al.  is the only study we know that has addressed this problem with some success. therefore  for this task  we compare esp only to the cellular encoding  ce  method. ce uses a graph transformation language to evolve the network topology as well as its weights. we did not compare esp with conventional reinforcement learning methods  e.g. q-leaming  td a   in this study  because ne methods have already been shown more efficient on easier versions of these tasks  moriarty and miikkulainen  1 . 
1 1 poles with velocities e1  
table 1 shows the results for the 1 pole configuration with velocities. as in wieland  and saravanan and fogel   the networks were composed of 1 hidden units. fogel and saravanan used feed-forward networks  and wieland used a fully recurrent architecture. it is clear that ne methods based on partial solutions are superior to other neuroevolution methods in terms of learning speed. although cpu time was unavailable for the other methods  it can be estimated from the number of evaluations required  
method generations no. nets conventional ne 1 ev. programming 1 1 sane 1 1 1 esp 1 1 1 cpu 
table 1: comparison of results for the double pole simulations with velocities  long pole = 1m; short pole = 1m . evolutionary programming data is taken from saravanan and fogel 1   and conventional ne from wieland . sane and esp data are average of 1 simulations. 
method noise cpu generations failures sane 1 1 1 esp 1 1 1 sane 1 esp 1 1 1 table 1: comparison between sane and esp on the double pole problem with evaluation noise. the starting angle of the long pole was chosen randomly from a uniform distribution within the specified range of degrees from vertical  long pole = lm; short pole = 1m . each entry is the average of 1 simulations. 
that they are considerably slower. also  esp is faster than sane by a factor of 1. 
　to verify the robustness of sane and esp we also performed experiments in which the long pole was started with an angle chosen randomly from a fixed range. table 1 shows the results for two ranges  one of degrees and the other  degrees. this has the effect of varying the difficulty 
of the task from trial to trial thereby introducing noise into the fitness evaluation. evaluation noise is a real problem in non-deterministic domains because it limits a gas ability to determine the underlying fitness of a population. even though sane has been shown robust against noisy evaluations in general  moriarty  1   it could not handle it in this more difficult task  most of the time failing to find a solution at all. in contrast  esp was largely unaffected by such variation solving this task every time even when the range was extended degrees. these results demonstrate that esp is highly resistant to evaluation noise. 
1 	1 poles without velocities  
this task is identical to the one in the previous section except that the networks do not receive any velocity information. therefore  the networks need to be recurrent so that the velocities can be computed internally using feedback connections. this makes the task significantly harder in two ways:  1  it is simply more difficult to control such a delicate system when the concomitant problem of velocity calculation must also be solved.  1  the number of connections in the networks is necessarily larger  thereby expanding the size of the search space. 
　for these simulations we compare esp to ce using the same fitness function as gruau et al . the function is the weighted sum of two separate fitness measurements 
	gomez and miikkulainen 	1 

taken over a simulation of 1 time steps: 
 1  
 1  
where t is the number of time steps the pole was balanced  k is a constant  set to 1   and the denominator in  1  is the sum of the absolute values of the cart and the large pole state variables  summed over the last 1 time steps of the run. this complex fitness is intended to force the network to compute the pole velocities by penalizing swinging  and thereby making the ga favor controllers with the ability to return the poles to the upright position and damp oscillations. this kind of fitness measure is necessary because otherwise networks can balance the poles by merely swinging them back and forth  i.e. without calculating the velocities   gruau et al.  1. 
　gruau et at. claimed that the decidedly large number of evaluations required by ce to solve this task  compared to direct encoding  table 1   is offset by the number of evaluations saved by not having to search for an effective network architecture to solve the problem. also  they were unable to solve this problem using direct encoding. ce does not assume an a priori network topology  and is therefore able to optimize it to suit a particular problem. while such methods are an important area of research  we found that they are not necessary nor advantageous for this problem. to demonstrate this we designed an e x t e n t that minimizes the amount of human intervention in the determination of network topology that esp evolves. in this experiment  the number of hidden units h is still fixed for each evolution  but instead of being prescribed by the user it is chosen at random by the system in a range from  for each simulation  the system begins evolving with the randomly selected h. if it does not solve the task  for whatever reason   it will restart with a new h. this occurs repeatedly until the task is solved. the total number of evaluations over all of the restarts is then counted. 
　table 1 compares the performance of esp and ce in this task. the esp experiments are the aforementioned where h is selected randomly. the initial angle for the long pole was set to 1＜ for all simulations. to determine if the task had been solved  we tested the most fit individual from each generation to see if it could balance the pole for 1 time steps and score at least 1 on the generalization test describe below. the latter was necessary because we found that fitness did not correlate well with the ability to generalize to novel 
initial states. 
　　in addition to learning speed  the robustness of the solutions was also tested. the column labeled  generalization  refers to each method's average score on a test where a successful controller is awarded a point for each of 1 different initial states from which it is able to control the system for 1 time steps. the test cases were generated by allowing the state variables and to take on the values:  
　　　　　　　　　　　　　　this test  first introduced in  dominic et al.  1   has become a standard for evaluating 
method evaluations generalization no. nets ce 1 1 1esp 1' 1 1  1 
table 1: results for double pole without velocities. long pole = 1m; short pole = 1m. average of 1 simulations. results for ce taken from gruau et al  1. 
the generality of solutions in pole balancing. a high score indicates that a solution has competence in a wide area of the state space. 
　the main result is that esp is roughly 1 times faster than ce without significantly compromising generalization  showing that the search for an appropriate architecture for this task can be automated by a simple stochastic mechanism. the esp simulations had a restart rate of 1. that means that  on average  the system had to start over with a new random h about 1 times per run. 
1 	incremental evolution of 1 poles without velocities  
this section compares the results of incremental versus direct evolution. for the incremental experiments  the following method was used the determine the sequence of tasks: the evolution begins with the pole balancing system described in the previous section  as the initial evaluation task when this task is solved  the shorter pole is lengthened by a predefined increment  p . p can change from task to task according to a simple rule. if esp is unable to solve the next task after two delta phases  p is halved and esp then tries to solve the problem where the shorter pole has a length halfway between  and the new   unachieved  task  once this intermediate task is solved  esp will move on to the next task 
 so instead going from to in a single step  the system does it in two steps if after completing  
task can still not be achieved  p is halved again and added to yielding which is halfway between  and  tasks 
are therefore repeatedly simplified  with p decreasing monotonically  until either a transition occurs or a lower bound on p is reached. 
　the incremental evolution can be illustrated best with an example. normally with this method the task differences are quite large at first. as the networks move on to harder tasks  p tends to shrink  and more task transitions are required for a given increase in the length of the short pole. for the initial value of p used in these experiments  1   a typical evolution schedule might look like: 

　in each of the direct evolution simulations  the evaluation task was fixed. four different tasks were chosen to test this approach:  each task was attempted 1 
times evaluating 1 networks per generation. note that the direct simulations were given a population size over two times larger than that of the incremental  1 vs.1 . this was done to see if the harder tasks could be solved by simply increasing the number of search points. 
　table 1 compares the two methods on different tasks  was started with a value of 1. this means that after solv-

1 	uncertainty and probabilistic reasoning 

pole length direct incremental 1 1 1 1 1 1 1 1 1 1 1 1 table 1: incremental vs. direct evolution on the double pole problem without velocities. the table shows the percentage of simulations that were able to achieve each task for the two approaches. the tasks are denoted by the length of the shorter pole  first column . 
ing the initial task  the short pole will be increased by 1% to  this is a significant change to the environment. other approaches that have applied shaping to the easier double pole task  with velocities  have incremented the short pole by only 1%  wieland  1; saravanan and fogcl  1 . esp was almost always able to complete the first three tasks  without having to decrement p  and the first two tasks were always achieved in less than 1 generations. 
　even with a larger population  direct evolution was incapable of solving when the task is this hard esp cannot discover a good region of the search space before converging. no individual does well enough to guide the search. esp selects genotypes that are slightly better than others in terms of the fitness scalar but are not necessarily any closer to the goal. the incremental approach  on the other hand  was able to solve the hardest task  within 1 generations 1% of the time  making an average of 1 task transitions per run. 
1 	conclusions 
the results show that esp with delta-coding can be an efficient method for controlling unstable systems. it was able to solve a markovian version of the double pole balancing problem much faster than other methods  and also a much more difficult non-markovian version. incremental evolution was found to be an effective way to scale up the approach to even more difficult tasks. the non-markovian control task is an important benchmark not only because is it presents nonlinear dynamical environment  but also because it requires memory. many tasks in varied domains from game-playing to robotics require memory to overcome perceptual aliasing. 
　in the future  we plan to apply esp to real-world robot navigation tasks. tasks of this kind are often naturally decomposable into a hierarchy of subtasks amenable to the incremental paradigm. they also pose the challenge of changing environments  which we believe can also be solved effectively by the same approach. 
acknowledgments 
special thanks to oliver gomez for help in preparing the illustrations. this research was supported in part by national science foundation under grant  
