 
this paper describes a jam session system that enables a human player to interplay with virtual players which can imitate the player personality models of various human players. previous systems have parameters that allow some alteration in the way virtual players react  but these systems cannot imitate human personalities. our system can obtain three kinds of player personality models from a midi recording of a session in which that player participated - a reaction model  a phrase model  and a groove model. the reaction model is the characteristic way that a player reacts to other players  and it can be statistically learned from the relationship between the midi data of music the player listens to and the midi data of music improvised by that player. the phrase model is a set of player's characteristic phrases; it can be acquired through musical segmentation of a midi session recording by using voronoi diagrams on a piano-roll. the groove model is a model that generates onset time deviation; it can be acquired by using a hidden markov model. experimental results show that the personality models of any player participating in a guitar trio session can be derived from a midi recording of that session. 
1 introduction 
our goal is to create a jam session system in which virtual players react as if they were actual human players. we want to make it possible for a human player to interact  whenever they like  with a virtual player that can imitate whoever the human player wishes to perform  for example  with a familiar  professional  or deceased player  or even with themselves. what is most important in imitating players is to acquire the player's personality models of a target human player. 
　previous session systems have not been able to imitate a 
　human player's personality. some systems  aono et al.  1  have been designed to follow the performance of a human soloist  but without considering the individual character of the virtual player. although jasper  wake et al.  1  has a set of rules that determine the system's reactions and virja session  goto et al.  1  has parameters for altering how it reacts  these systems cannot develop player personality models of an actual human player. 
　to realistically imitate a human player  a system must be able to acquire player personality models of that player. the imitating virtual player can then improvise according to the models. the neuro-musician  nishijima and kijima  1; nishijima and watanabe  1  can learn the relationship between 1 sets of an 1-bar-length input pattern and an output pattern by using neural networks. however  it is only capable of dealing with the limited style of a jam session where a solo part must be changed in 1-bar rotation. in other words  a virtual player and a human player cannot both play a solo part in the same time. moreover  the neuro-musician must prepare a training set of 1-bar-length input-output data to enable neural network learning. in an actual jam session  a player does not always play an 1-bar solo to the 1-bar solo of the other players. therefore  we cannot acquire the player models from a 
midi session recording by using the neuro-musician method. 
　on the other hand  the band-out-of-a-box  bob   which deals with a problem similar to ours  thorn  1a; thorn  1b   indicates that machine learning techniques provide a useful approach to acquire a player's models. however  bob can only react to a human performance of an immediately previous four bars. it has a fixed relationship in which the human player is the leader and the virtual player is the follower. 
　our jam system allows us to acquire player personality models of a target human player from the midi recording of a session in which that player participated. the main advantage of our approach is that we do not have to directly evaluate the target player: all we need to build the models is session recording data. 
1 a guitar trio session system 
our system deals with constant-tempo 1-bar blues performed by a guitar trio. figure 1 shows a jam session 
art and creativity 	1 midi stands for musical instrument digital interface. 

model in which either a human or the computer can be selected to perform the part of each player. we can imagine sessions in which all players will be human players just as we can imagine sessions in which all players are computer players. the three players take the solo part one after another without a fixed leader-follower relationship. 
 we obtain three kinds of player personality model from a midi session recording - a reaction model  a phrase model  and a groove model. 
　the system has two modes  a learning mode and a session mode. in the learning mode  discussed in sections 1  and 1   the system acquires player personality models in non-real time. these models are stored in a database and different personality models can be assigned to the two virtual players before session play  figure 1 . in the session mode  discussed in section 1   a human player can interact with the virtual players in real time. 

1 	learning a reaction model 
a reaction model is the characteristic way that a player reacts to other players. acquiring an actual player's individual reaction model is necessary to create a virtual player that reacts as that actual human player does. as shown in figure 1  each virtual player listens to the performances of all the players  including its own  and uses the reaction model to determine what its next reaction  output performance  will be. the main issue in deriving the reaction model is to learn the relationship between the input and the output of the target player in midi recordings. this can be formulated as a problem of obtaining a mapping from the input to the target player's output. however the direct midi-level learning of this mapping is too difficult because the same midi-level sequence rarely occurs more than once and the mapping itself is too sparse. we therefore have introduced two intermediate subjective spaces: an impression space and an intention space  figure 
1 . 
1 impression space 
the impression space represents the subjective impression derived from the midi input. by applying principal components analysis  pca  to the results of subjective evaluations of various midi performances  we determined three coordinate axes for the impression space. pca is a statistical method for reducing the number of dimensions while capturing the major variances within a large data set. while listening to a performance  a subject subjectively evaluated it by using ten impression words to rank the performance's impression on a scale of one to seven. the three selected axes of the impression space represent qualities that can be described as appealing  energetic  and heavy. to obtain a vector in this space  an impression vector corresponding to the midi input  we use canonical correlation analysis  cca . this analysis maximizes the correlation between various low-level features of the midi input  such as pitch  note counts  tensions  and pitch bend  and the corresponding subjective evaluation. since an impression vector is obtained from an individual player's performance  we have at every moment three impression vectors  figure 1 . 
　the impression space is necessary for learning the relationship between various input performances and the corresponding output performances. if we represent the input performances as short midi segments without using the impression space  the same midi segments will not be repeated in different sessions. the impression space enables the abstracting of subjective impressions from input midi data and those impressions can be repeated. even if two segments of the input midi data differ  they can be represented as a similar vector in the impression space as long as they give the same impression. 

1 	art and creativity 


art and creativity 	1 

the rbf networks have good generalization ability and can learn whichever nonlinear mapping function we are dealing with. 
1 learning a phrase model 
a phrase model is a set of player's characteristic phrases. to create a virtual player that performs using phrases as an actual human player does  acquiring the actual player's individual phrase model is necessary. this can be done through musical segmentation of a midi session recording  figure 1 . 

　two kinds of grouping appear in polyphony  one in the direction of pitch interval and the other in the direction of time. grouping in the pitch interval direction divides polyphony into multiple homophony  figure 1a . in the time direction  notes are grouped from time gap to time gap  figure 1b . 
　to segment a midi session recording into phrases  we need to automatically divide the polyphony notes into groups. the generative theory of tonal music  gttm   lerdahl and jackendoff  1  includes a grouping concept  and thus can be used to derive a set of rules for the division of notes into groups. we think that gttm is the most promising theory of music in terms of computer implementation; however  no strict order exists for applying the rules of gttm. this may lead to ambiguities in terms of analysis results. the implementation of gttm as a computer system has been attempted  ida et al.  1   but the resulting system was only capable of dealing with a limited polyphony made up of two monophonies. 
　in this paper  we propose a method of grouping based on applying the voronoi diagram. we have developed a method of grouping rather than naively implementing gttm so that a result obtained using our method is equifinal to one obtained with the gttm approach. we 
a: grouping in pilch interval direction. 

b: grouping in time direction. 

figure 1: examples of grouping. 
compare the results of grouping by our method with the results of grouping by a human according to the gttm. 
1 	generative theory of tonal music 
the generative theory of tonal music is composed of four modules  each of which is assigned to a separate part of the structural description of a listener's understanding of music. the four gttm modules are the grouping structure  the metrical structure  the time-span reduction  and the prolongational reduction. 
　the grouping structure is intended to formalize the intuition that tonal music is organized into groups  which are in turn composed of subgroups. there are two kinds of rules for gttm grouping: grouping well-formedness rules and grouping preference rules. grouping well-formedness rules are necessary conditions for the assignment of a grouping structure and restrictions on the generated structures. when more than one structure may satisfy the grouping well-formedness rules  grouping preference rules only suggest the superiority of one structure over another; they do not represent a deterministic procedure. this can lead to the problem of ambiguity mentioned above. 
1 	use of voronoi diagrams for grouping 
to overcome the ambiguity problem  we propose a method of grouping based on the use of voronoi diagrams. the gttm result is a binary tree that indicates the hierarchical structure of a piece of music. in our method  voronoi diagrams on a piano-roll represent the hierarchical structure of a piece of music. 

we can form the voronoi diagram for a given set of points in a plane as a connected set of segments of half-plane boundaries  where each of the half-planes is formed by partitioning the plane into two half-planes  one on either side of the bisector of the line between each adjacent pair/   and pj . 

1 	art and creativity 

voronoi diagram for two notes 
our method uses the piano-roll format as a score  and thus notes are expressed as horizontal line segments on a piano-roll. to construct a voronoi diagram on the score  we need to consider the voronoi diagram for multiple horizontal line segments  which will be constructed of linear segments and quadratic segments. 
　when two notes sound at the same time or no note sounds  the corresponding part of the voronoi diagram is a linear segment  figures. 1a and 1c . when a single note sounds  the voronoi diagram becomes a quadratic segment  figure 1b . 

voronoi diagram for more than two notes 
to construct a voronoi diagram for more than two notes  we construct the voronoi diagrams for all note pairs and delete the irrelevant segments. for example  to construct a voronoi diagram for notes a  b  and c  we construct three 
voronoi diagrams  figure 1 . the boundaries in the three diagrams then intersect at a point that is equidistant from each note. the voronoi diagram for notes a and b is divided into two half-lines at the intersection. we then delete the half-line that is closer to c than to a or b. 

1 	m a k i n g groups 
hierarchical groups of notes were constructed by converging adjacent notes iteratively. here  we introduce the following principle for making groups; the smallest voronoi cell is first merged to an adjacent group. 
　we have implemented our grouping method  figure 1 la  and have compared the results with correct data obtained by a human  figure 1b . we evaluated grouping performance in terms of a correctness rate defined as 
the number of notes grouped correctly 
	correctness rate = 	-- 	-  1  
                         the number of notes when wc ran the program  we calculated that the correctness rate was 1 percent. the tune used as midi data in this experiment was the turkish march. 
a. result obtained using our method. 

b. result obtained using gttm. 

figure 1: results obtained using our method and gttm. 
1 learning a groove model 
a groove model is a model generating the deviation of on set times. acquiring an actual player's individual groove model is necessary to create a virtual player that performs with musical expression as that human player does. a human player  even when repeating a given phrase on a 
midl-equipped instrument  rarely produces exactly the same sequence of onset notes because the onset times deviate according to performer's actions and expressions. we can model the process generating that deviation by using a probabilistic model. 
1 	formulation of the hidden markov models 
let a sequence of intended  normalized  onset times be 1 and a sequence of performed onset times  with deviation  be y. then  a model for generating the deviation of onset times can be expressed by a conditional probability p  y  1   figure 1 . using this conditional probability and the prior probability p  1  can be formulated as a hidden markov model  hmm   which is a probabilistic model that generates a transition sequence of hidden states as a markov chain. each hidden state in the state transition sequence then generates an observation value according to an observation probability. 
modeling of performance 
target in modeling 
we modeled the onset time of a musical note  i.e. the start time of the note  and introduced a new model of distribution of onset times. while the duration- time-based model 

art and creativity 	1 


used in  otsuki et al.  1  is limited  our onset-time-based model is suitable for treating polyphonic performances  such as those including two-hand piano voicing and guitar arpeggio. unit in modeling 
we use a quarter note  beat  as the unit of each hmm: the temporal length corresponding to each hmm is a quarter note. the reason we use the quarter-note unit is to distinguish between eighth triplets and sixteenth notes within the scope of quarter notes. the three notes of eighth triplets are located on three equi-divided positions in a quarter note  while the four sixteenth notes are located on four equi-divided positions in a quarter note. an actual performance consisting of a sequence of quarter notes can be modeled by concatenating the quarter-note-length hmms. this quarter-note modeling has the advantages of reducing calculation time and facilitating the preparation of the large data sets used for training the model. unit of quantization 
we introduce two different discrete temporal indices  k and i. the unit of k is a quantization unit to describe performed onset time and is 1 of a quarter note  which is often used in commercial sequencing software. the i unit is a quantization unit to describe the intended onset time and is one-twelfth of a quarter note. it can describe both eighth triplets and sixteenth notes. 
quarter-note hidden markov model 
figure 1 shows the hmm used in our study. we model a sequence of onset times within a quarter note  beat  by using the hmm. all the hidden states of the hmm correspond to possible positions of intended onset times  and an observed value that comes from a hidden state corresponds to a performed onset time with deviation. onset times in a beat are quantized into 1 positions for hidden states and into 1 positions for observation values. that is  each component of the hmm is interpreted as follows. hidden state /: intended onset time.  i= 1  ...  1  
observation k: performed onset time.  k= 1  ...  1  transition probability aij: probability that intended onset time j follows intended onset time i. 
observation probability b  k   probability that performed onset time is k and intended onset time is i. a state-transition sequence begins with a dummy state  start  and ends with a state  end   figure 1 . 


1 	art and creativity 


figure 1: simple example of state sequences. 
1 learning model parameters 
the hmm parameters aij and b  k  were derived from a midi session recording by using the baum-welch algorithm. figure 1 shows a b  k  distribution obtained from a human performance in a midi session recording. 

1 session mode 
using the personality models acquired in the learning mode  each virtual player improvises while reacting to the human player and the other virtual player. the processing flow of each virtual player can be summarized as follows: 1. the low-level features of the midi performances of all the players are calculated at every 1 bar. 
1. at every 1 bar  the three impression vectors are obtained from the low-level features. 
1. at the beginning of every bar  the intention vector is determined by feeding the reaction model the past impression vectors. 
1. the output performance is generated by connecting short phrases selected from a phrase-model database. each phrase is selected  according to the determined intention vector  by considering its fitness for the chord progression. a virtual player can start a solo performance at any bar. 
1. the deviation of the onset times is generated according to the groove model. 
note that the reaction model can predict the next intention vector from the impression vectors gathered during the past twelve bars in real time: a virtual player thus does not fall behind the other players. 
1 	experimental results 
we have implemented the proposed system on a personal computer  with a pentium iv 1ghz processor ; figure 1 shows a screen snapshot of the system output. as shown  there are three columns  called player panels   each corresponding to a different player. the toggle switch on the top of each panel indicates whether the panel is for a virtual player or a human player  and each panel contains two boxes representing three-dimensional spaces: the upper box is the impression space and the lower box is the intention space. the sphere in each box indicates the current value of the impression or intention vector. 
　in our experiments  after recording a session performance of three human guitarists playing midi guitars  we first made the system learn the reaction model  the phrase model and the groove model of each. we used a metronome sound to maintain the tempo  1 mm  when recording  and the total length of this recording session was 1 bars. we then let five human guitarists individually use the system in session mode. the system indeed enabled each human guitarist to interact with two virtual guitarists  each with a different reaction model. 
　to find out how well a virtual player could imitate a human player  we asked a human player to perform with virtual player a imitating him and with virtual player b imitating a different player. the human player and the virtual player imitating him tended to take a solo at almost the same time and to perform phrases that felt similar. figure 1 shows the transition of intention vectors of three players during 1 bars where the intention vectors of the virtual player a and the human player are particularly similar. examining all the values of the intention vectors during the session  we compared the distances between the intention vectors of the virtual players and the human player. over 1 bars the average distance between the intention vectors of the human player and the virtual player imitating him was significantly smaller than that between the intention vectors of the human player and the virtual player imitating a different player. we think that this means the virtual player's rbf networks actually predicted the human player's intention. 
　we also tested whether a virtual player could imitate the target human player by applying the turing test format. as subjects  we used three guitarists  a  b  and c  who had played in the same band for more than a year and so un-

figure 1: screen snapshot of system output. 

art and creativity 	1 

derstood each other's musical personalities. we prepared a reaction model  a phrase model  and a groove model for each of these subjects. we then used different personality models to prepare 1  = 1 players a 1 models  kinds of virtual player. the subjects evaluated each virtual player with regard to whose models it was based upon. subjects were told in advance that the player was a virtual player imitating either player a  b  or c. we found that a virtual player having the three personality models acquired from the same human player was correctly identified as such. we calculated that the success rate was 1 percent. the subjects could thus distinguish when the virtual player was based on the personality models from one human player or from multiple players. 
　furthermore  five guitarists who performed with the system remarked that each virtual player performed characteristically. in particular  a human player who participated in a jam session with a virtual player that was imitating him remarked that he was uncomfortable playing with that virtual player because he felt that he was being mimicked. these results show that our system successfully derived and applied personality models from midi session recordings. 
1 	conclusion 
we have described a jam session system in which a human player and virtual players interact with each other. the system is based on the learning of three types of personality model - the reaction  the phrase  and the groove models. our experimental results show that our system can imitate the musical personalities of human players. we plan to extend the system so that it can be applied to other musical instruments  such as piano and drums. 
acknowledgments 
we thank daisuke hashimoto  shinichi chida  and yasuhiro saita  who cooperated with the experiment as players. 
