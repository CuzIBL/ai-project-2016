 
this paper describes quem  a method for assessing the skill level of a knowledge-based system based on the quality of the solutions it produces. quem is demonstrated by using it to assess the performance of a particular knowledge-based system  p1. quem can be viewed as an achievement or job placement test 
given to know ledge-based systems to help system designers determine how the system should be used  and in what capacity by what level of users. in general  it is difficult to find useful metrics for assessing a system's overall performance. most literature on evaluation deals with validation  verification and testing in which the primary concern is the correctness and consistency in the databases and rulebases. however  these properties alone may not be sufficient to determine how well a system performs its task. quem allows software developers to assess their system's performance by constructing a skill function based on human performance data that relates experience and solution quality. quem can be used to gauge the experience level of an individual system  compare two systems  or compare a system to its intended users. this represents an important advance in quantitative measures of over-all system performance that can be applied to a broad range of systems. 
1 	introduction 
when evaluating know ledge-based systems  kbss  it is often difficult to find useful metrics for assessing a 
system's overall performance. most literature on kbs evaluation deals with validation  verification and testing  nazareth and kennedy  1  in which the primary concern is with the correctness and consistency in the databases and rule-bases. other systems address modifiability  ease of use and cost of the system. however  these properties alone may not be sufficient to determine how well a system performs its task. for example  
1 	cognitive modeling 
michael i. parzen university of chicago graduate school of business 1 east 1th street  
chicago  illinois  1 u.s.a. 
a complete and consistent kbs may not necessarily create high quality solutions. it would be useful to have a method to estimate how well a kbs's performs its task on some absolute scale that would allow comparisons between systems to be made. however  it is not immediately obvious what type of scale should be used. in this paper we present quem  quality and experience metric  pronounced  kwem    a method for evaluating the experience level of a know ledge-based system and the quality of its solutions. in other words  quem estimates the performance of a system in terms of the years of experience a human would require to generate solutions of the same quality; experience level is the scale on which quality is assessed. quem can be considered to be an achievement test for kbss since it estimates the level that a kbs has achieved. we use expert judges to assess the quality of solutions generated by human experts and kbs's. we then construct a  skill function  for the human data relating experience and solution quality. this skill function is used to estimate kbs experience level. 
　quem provides a quantitative way to estimate the experience level of a kbs  compare two kbs's  or compare the experience level of a kbs to that of its users. this type comparison is of particular importance if a kbs is to be used as an aid to human users. understanding the skill level of the kbs relative to its users is important in determining how the system should be used and in predicting whether users will accept it. for example  it is often necessary that the skill level of the kbs equal or exceed that of its users. if the kbs produces solutions of lower sophistication and quality than the user can produce on his or her own  the user may consider the system to be a hindrance. additionally  estimation of a kbs's experience level is one measure of how well developers have succeeded in capturing the domain expertise. 
　the development of quem arose from our desire to measure the quality of various knowledge-based systems which were under development. we were particularly interested in having a way to assess the quality of the problem solving abilities of prototype systems because it would be of great assistance in making development decisions. before continuing a large programming effort to improve a given system  we wanted to have some assurance that the approach used in the prototype was a reasonable one. if evaluation were to show that the best solutions produced by a given system were of low quality  then we would conclude that the approach was not a reasonable one and our efforts should be focused on re-structuring the approach and problem solving architecture. on the other hand  if the solutions produced proved to be of high quality then we could feel more confident that the approach was reasonable and that it would be worthwhile to put efforts into further development of the current system architecture. 
　in this paper we will outline the quem method developed for assessing average solution quality in terms of experience level  and demonstrate the use of quem to estimate the solution quality and experience level of a kbs for creating manufacturing plans. we used these results to assess the competence of our kbs problem solving approach  and to determine if we should continue on the same approach in future developments. 
1 	challenges 
our first challenge was to identify a method for assessing solution quality in complex domains. quality is the perceived utility that an artifact has to some set of people in a given context. a utility function is often used to provide a precise number to estimate quality  just as a watch provides a precise number to estimate the true time. solution quality is in general hard to measure because it can be hard to quantify. this is particularly true in domains that are very rich such as architectural design  military battle planning  and manufacturing. there are usually many  sometimes conflicting factors that determine solution quality such as cost and accuracy  and esthetics. even if one can generate a utility function to describe quality in a given domain  it may be hard to quantify the component factors that determine quality. 
　in our initial attempts to estimate solution quality for the manufacturing domain  we tried to construct a quality utility function composed of factors which our experts believed to be important. in the manufacturing domain  important factors included plan cost  feasibility and reliability. we attempted to generate a quality function for this domain based on these factors. however  we soon found this approach to be inadequate; it was not feasible to construct an accurate mathematical quality function because some important component factors  such as reliability  were very difficult to quantify. 
　to describe the example further  plan reliability is the likelihood that the operations within the plan will fail or will produce marginal results. plans can fail in catastrophic ways resulting in physical damage to agents or equipment executing the plans  or in subtler ways  such as when the resulting product does not meet requirements. predicting reliability requires knowledge of a wide variety of situations which are hard to capture without a large body of empirical data. because of these difficult to quantify component factors  the task of constructing a reasonably accurate mathematical quality metric for kbs solutions is very difficult in practice  for many  if not most  rich and complex domains. 
　however  after some initial disappointment on finding that a good mathematical quality function was not feasible in our domain  it occurred to us that we did not actually need a quality function because we had a number of fairly robust quality measuring devices readily available to us: human experts. human experts can succeed in assessing quality where a quality function may fail because experts are able to estimate hard-to-quantify quality factors  such as reliability  based on their broad empirical experience. additionally  we found that although there was some variation in how judges assessed quality  their quality assessments were usually fairly close. experts with similar experience tend to make similar quality assessments of a given solution. furthermore  those assessments correlate very strongly with the experience of the problem solver. thus  although there is a perception that humans are unreliable  we found that human experts were fairly consistent in assessments  and that their variability can be measured  for example  by having several experts independently rate the same solution  and taken into account. we decided look for a way to use the solution quality assessments produced by human 
experts to assess kbs solution quality. 
　our second challenge was to devise a scoring system in which human judges could report their quality assessments. the scoring system must allow the quality assessments of different judges to be compared. initially  we considered having the judges assign quality scores between 1 and 1  like olympic sports judges  indicating the absolute quality of each plan. however  we decided against such an approach because experts do not have a standard or agreed upon method for assigning quality measures to plans. we were concerned that it might be difficult to compare scores assigned by two judges; if 1 is the best quality score  an enthusiastic judge might give many 1's while a conservative judge may rarely give a score better than 1. however  the first judges's 1 may mean the same thing as the second judges's 1. we decided that it would be more appropriate to have the 
judges rank order solutions from best to worst  rather 
than to assign them scores. 
1 	related work 
as mentioned earlier  most literature on knowledgebased system  kbs  evaluation deals with validation  verification and testing  vvt   nazareth and kennedy  1  in which the primary concern is with correctness  circularity  inconsistency  redundancy  modifiability  ease of use and cost  lane  1    liebowitz  1 . however  these properties alone may not be enough to describe a system's competence in solving problems effectively.  clancey  1  describes four perspectives useful for evaluating a system's competence: performance  articulation  accuracy and completeness. other parameters important to system competence are: solution feasibility  solution quality  problem solving range  computer 
effort  and user effort. 
most competence evaluations provide relative mea-
	hayes & parzen 	1 

sures of system performance. these evaluations provide the information that system x works better than system y  or human z. for example  when aikins  aikins  1  evaluated her system  puff  a medical diagnostic system for cardio-pulmonary diseases  she compared the performance of her system against the diagnostic performance of three human doctors. she found that puff's diagnosis agreed with the average diagnosis more often than did any of the individual doctors. from this she concluded that not only could puff perform competently  but it was also more accurate on average than any of the individual experts in the study. dixon  et al. evaluated their system  dominic  dixon et a/.  1   by comparing its results against those of two other kbs's and a human expert. from this comparison they concluded that  dominic is a reasonably capable designer ... although the two domain specific programs produced slightly superior performance.  
　however  simply knowing that one kbs produces better quality solutions than another does not necessarily tell the kbs developers if either produces particularly good solutions. for this reason we also felt it was necessary to develop a quantitative measure of kbs experience level which would allow one to make statements such as  
 my kbs is estimated to have captured n years of experience.  such measures can better aid system developers in assessing whether their kbs is sophisticated enough for their purposes. 
1 	general method 
the quem procedure requires one or more knowledgebased systems for comparison  a set of problems  several subjects of various experience levels  and two or more expert judges. the expert subjects should have differing levels of experience. the expert judges should have 
experience equal to or greater than that of all subjects. the judges should not double as subjects in order for this test to produce meaningful results. additionally  the domain of experience for the kbs  judges and sub-
jects  must all be very similar  otherwise they may judge quality by very different criteria. the quem procedure for rating kbs experience level is: 
1. solve. have all subjects and all kbss each solve all problems in the problem set. 
1. sort. for each problem  put all solutions together in a group. if there are 1 problems  there will be 1 
solution groups. 
1. rank. have the expert judges independently rank order all of the solutions in each group from worst quality to best quality. label the worst solution in each group as number 1. successively number each solution  assigning the highest number to the best solution. 
1. adjust ranks. if a judge ranks several solutions as having equal quality  the ranks must be normalized so that they can be compared to other rankings. for example  suppose judge 1 is given 1 solutions 
	1 	cognitive modeling 
which he ranks 1 through 1  while judge 1 is given the same six solutions but she ranks two solutions as worst  three as intermediate  and one as best  producing the ranks of 1  1  1  1  and 1. the rankings of judge 1 must be adjusted if they are to be compared to judge l's rankings  lb adjust the rankings  they must be divided in to tied groups. judge 1's rankings would be divided into three groups:  1  1   1  1  1   1 . all data points must be renumbered starting from the lowest number  such that 
each has a separate consecutive rank:  1  1   1  1  1   1 . next  the average rank of each group is computed  and each member of a group is assigned the value of its group average. thus  judge 1's adjusted rankings would be: 1  1  1  1  1  and 1. 
1. compute subject averages. compute the average quality ranking for each subject and kbs across all problems using the adjusted rankings. 
1. plot subject averages. put the kbs data aside for a moment. plot each human subject's experience on the y axis and his or her average quality ranking on the x axis. 
1. fit a skill function to the data. fit a line or curve to these data using linear regression or other method appropriate to the data. call this the skill function. 
1. construct confidence bands to indicate the amount of variation one can expect in individual performances at any given experience level. a point estimate of experience is not useful without some idea how accurate the estimate is. to compute these bands let  denote the average quality rank of a kbs. using the linear regression model described above  our experience estimate of the kbs is  a 1% confidence interval for 
this estimate is given by where based on the t-distribution and the amount of variability in the relationship between experience level and average quality rank. all these quantities are standard output results from statistics packages. 
1. construct an experience estimate and interval. for each kbs in the study  
 a  plug the kbs's average quality ranking  x  into the skill function to obtain the experience estimate for the kbs. 
 b  plug the kbs's average quality ranking  x  into the equation for the upper confidence band. repeat for the lower confidence band. the two numbers produced represent the experience interval for the kbs. 
the results of this process are: 
  a skill function for humans  relating length of experience to solution quality. 
  confidence bands around the skill function showing the expected range of skill for practitioners having a given length of experience. 
  an experience estimate for the kbs. this value indicates the most likely value for the experience-level of the kbs. 
  an experience interval showing the range of human experience levels expected to achieve the same average quality as the kbs  with 1% confidence. 
1 	a p p l i c a t i o n s o f q u e m 
quem can be used in a variety of ways: 
1. estimate the experience level of a single k b s . when applied in this way  solutions created by a single kbs are ranked along with solutions created by a range of humans. 
1. identify a change in experience level between an old and a new version of a kbs. solutions of two or more versions of the same kbs are ranked along with solutions created by humans. 
1. compare two or more kbss in the same domain using the same method described in  1 . 
1. compare two unrelated kbss that operate in different domains. in order to compare two unrelated kbs's  two separate quem tests must be performed  and the resulting experience levels compared. a separate group of judges and subjects with appropriate domain knowledge must be selected for each test. 
1. estimate the amount by which a computer assistant raises the skill level of a user. run two problem solving sets of the same difficulty on the same user: one set without the aid of the kbs and one with the kbs. a separate problem set must be used for each trial to avoid learning effects. use the same analysis method as  1  - treat the user's two trials as one would treat two different versions 
of the same kbs. 
1 	selecting j u d g e s   subjects  problems 
in order to perform a test  the experimenter will need to take some care in selecting judges and a range of subjects. selection of problems turned out to be a less difficult issue. we found that in the domains which we studied  manufacturing and software development  even very simple problems were of sufficient complexity to show strong differences between practitioners ranging between 1 and 1 years of experience. this is probably true for most complex domains  although it may not be true for very simple or toy domains. 
　subject and judge selection. the judges should preferably have 1 or more years of experience.  macmillan  macmillan et al.  1  refers to such experts as 'super experts.   however  given the rarity of highly experienced experts  one may have to settle for what   one can get. the subjects' and the judges' experience area should closely match the domain of the kbs being evaluated. 
　range of subjects. ideally  one would like to select subjects so that the experience level of the kbs falls within the subjects' experience range. the method will still work even if the kbs falls slightly outside the range of the subjects' experience  however if it falls too far outside their range then the experience interval may become too broad to supply a useful experience estimate. for example  if the experience level of the kbs is 1 years  one may want to select subjects ranging from 1 to 1 years of experience. unfortunately  before applying quem  one does not know the experience level of the kbs  so one must make an initial educated guess as to what the range of experience levels should be for the subjects. it may be necessary to conduct 1 or more pilot studies to identify the appropriate experience range for the subjects. the first time we tested the manufacturing kbs  described in the later example   we did not guess the subject range correctly. we selected subjects having between 1 and 1 years of experience  but found that the kbs's experience level was above the range of these subjects. this provided useful information  but it did not allow us to put an upper bound on the system's experience level. after two additional years of development on the system  we conducted a second test in which we selected sub-
jects between 1 and 1 years of experience. this time we found that our kbs's experience level did fall inside the range of the subjects  approximately at the 1 year mark . these two previous studies enabled us to select the correct range of subjects  1 - 1 years  for the study in this paper. 
　graceful degradation. there are many sources of variation in the data. variations may arise from differences in the way judges make assessments  motivation levels of the subjects  and other factors. the total variation in the data from all such sources is reflected in the width of the confidence bands and experience interval. this representation of variability makes quem robust to noise to an extent  and provides quem with the prop-
erty of graceful degradation. thus  if the experimenter accidentally introduces additional variation by poor selection of one judge or subject  it may broaden the experience interval  reducing the precision of the answer  but it will not greatly change the result. 
1 	l i m i t a t i o n s of t h e m e t h o d 
quem can provide useful information for a domain only when practitioners in the field show a distinct improvement in solution quality over time. an example of a domain where this relationship is known to exist is management planning of software development projects fiebig  1 . however  experience may not bring quality improvements in all domains. the existence of such a relationship can easily be determined by testing if a simple function can be found which fits the data well. the converse  that no relation exists  is harder to determine. if 
	hayes & parzen 	1 


figure 1: quality rankings assigned to solutions 
uo clear relationship is found in the data  it does not necessarily mean that one does not exist. it could also mean that the subjects or judges were not chosen well  the range of experience levels was too narrow  or increased skill manifests itself in ways other than through increased solution quality  such as increased speed in producing a solution . 
1 	example: evaluation of a kbs 
quem was used to evaluate a manufacturing kbs  machinist  later called p1  hayes  1  . experts in this domain are highly skilled and require as much as 1 to 1 years of intensive practice to achieve master level status. seven subjects and two judges were selected. the subjects ranged between 1 and 1 years of experience. they had 1  1  1  1  1  1  and 1 years of experience respectively. the two expert judges had 1 and 1 years of 
experience respectively. we prepared 1 problems for the subjects to solve  p1  p1 and p1  all of approximately the same difficulty level. 
1 	evaluating a kbs with quem 
1. solve: we had each of the subjects and the kbs solve all three problems. we wrote up all solutions in a uniform format and handwriting  to disguise their source . 
1. sort: we sorted the solutions into 1 groups: each group contained all solutions to a specific problem. 
1. rank: we had two expert judges independently rank the plans in each group  from worst to best. the worst plan was given a score of 1. the ranks assigned to each plan are shown in figure 1. pi  p1 and p1 are problems 1  1 and 1. the missing data points resulted when subjects were unable to complete all three problems when they called away due to immediate job demands. 
1. adjust ranks. this step was not necessary for these data because each plan had a unique rank. 
	1 	cognitive modeling 

average solution quality  x  
figure 1: average quality rankings and skill function. 
1. compute subject averages. the average quality ranking received by each subject across all three problems was computed. these values are shown in the last column of figure 1. the lowest average score  1  was received by the subject 1 who had only two years of experience. the highest average score of 1 was received by subject 1 who had 1 years of experience. the kbs received a quality ranking of 1. a factorial analysis performed on the data showed experience to be statistically significant  but not judge nor part  which confirmed our expectations . 
1. plot. the average quality rankings or the human subjects were plotted on the graph shown in figure 
1. 
1. fit a skill function to the data. several types of curves were fit to these data  including a logarithmic function and several types of polynomials. however  a simple linear regression fit the data best. 
the regression yielded the following equation for the model: y = -1 + 1x. we use this as the skill function  shown in figure 1 as a heavy diagonal line. 
1. construct confidence bands. 1% confidence bands are shown in figure 1 as curved bands flanking the skill function. 
1. plot the k b s average quality rank. the average quality rank for the kbs  x m   was plotted on the quality  x  axis. 
1. construct an experience estimate and interval. the average plan quality rating of the kbs  1  is plugged into the skill function. this produces a value which estimates the kbs's experience level at-1 years of experience. this is the experience estimate. using the equations for the confidence bands  it was determined that the experience interval is 1 to 1 years of experience. this means that the true experience level of the kbs lies 


average solution quality  x  
figure 1: the kbs experience estimate and interval. 
somewhere between 1 and 1 years  with 1% confidence. 
　figure 1 shows the experience estimate and experience interval for the kbs on the y axis. from this evaluation we concluded that our system exhibits a very high experience level. on the basis of these results we confirmed that our problem solving approach was a reasonable and effective one. we decided that our basic approach was sound and that we should proceed with development along the same lines. information on how to change the system to improve it further was derived from further knowledge engineering and protocol analysis. 
1 	summary and conclusions 
in this paper we present quem  a general method for measuring the experience level of a kbs  and assessing the quality of its solutions relative to human practitioner. this method allows researchers to answer the question   how expert is my expert system   assessing the solution quality and experience level of a kbs system is important in helping developers decide if their approach is sufficient  how the system should be used  and with what level of user it should interact. 
　the method utilizes expert judges to rank order solutions produced by both kbss and human subjects. the rankings for the human subjects are used to construct a 
skill function describing the relationship between experience and solution quality. lastly this skill function  and its confidence bands can be used to estimate the kbss' 
experience level  and to bound the true value. 
　previous methods for evaluating a kbs performance involve qualitative comparisons  such as  system x performs better than system y   which does not say if either system performs well at all. the quem procedure allows a system developer to make a quantitative assessment of the solution quality and experience level of a 
1 figure 1 courtesy of m. c. p. dorneich. 
kbs. this measure allows system developers to answer the questions such as   how much better is system x than system y   or  how many years of experience does my kbs capture   
　quem can be used in any domain in which increased experience leads to measurably increased solution quality  which is presumably most complex domains . additionally  it can be used to assess a partially complete kbs which can construct solutions but which may not be complete or correct in all aspects. it can be used to measure the experience level of an individual kbs  to compare several kbss which operate in the same domain  or to compare the experience levels of several kbs's that operate in unrelated domains. quem represents an important advance in providing quantitative measures of system performance that can be applied to a broad range of complex domains in which solution quality may otherwise be hard to quantify. 
