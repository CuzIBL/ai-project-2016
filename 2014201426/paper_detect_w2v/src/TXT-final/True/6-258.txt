 
the ability to recognize when an agent abandons a plan is an open problem in the plan recognition literature and is a significant problem if these methods are to be applied in real systems. this paper presents an explicit  formal  and implemented solution to the problem of recognizing when an agent has abandoned one of its goals based on a theory of probabilistic model revision. 
1 introduction 
there is a large body of research on the topic of intent recognition or task tracking  focused on identifying the goals of a user from observations of their actions. however  none of this work has directly addressed the problem of recognizing when a user has abandoned a goal. 
　in general  the ability to infer abandoned goals is an operational requirement for any plan recognition system that is executing incrementally and continuously. abandoning goals is something that any real observed agent will do. if a plan recognition system is unable to recognize this fact  the system will build up an ever increasing set of active or open plans that the agent has no intention of completing. a system attempting to find completions for these open plans will wind up considering unreasonable situations such as the first step of a time critical two step plan simple plan are taken but the plan is not elder a two step plan with a required seconds or minutes duration but not attempting the second step of the plan until days or weeks later. unfortunately  existing plan recognition systems cannot draw such inferences. 
1 background 
recent work in execution based plan/intent recognition  bui etai  1; geib and goldman  1; goldman et al  1  has been based on a model of the execution of simple hierarchical task network  htn  plans  erol et al  1 . 
　the idea behind this approach is that initially the executing agent has a set of goals and chooses a set of plans to execute to achieve these goals. the set of plans chosen determines a 
   *this material is based upon work supported by darpa/ipto and the air force research laboratory under contract no. f1-c-1 

figure 1: a simple model of plan execution. 
pending set of primitive actions. the agent executes one of the pending actions  generating a new pending set from which the next action will be chosen  and so on. 
　this process is illustrated in figure 1. in this light  the observed actions are nothing more than the observations of a hidden markov model and the process of plan recognition is the inference of the underlying state of the model.space prohibits a full exposition of this approach. we refer readers to  geib and goldman  1; goldman et al.  1  for a more complete discussion. 
1 exact solutions 
given our model of plan execution  a formal and general model of probabilistic abandonment of goals  will be forced to deal with an exponentially larger model and will be required to obtain the prior probabilities that each of the goals is abandoned. for real world applications this approach is simply untenable. a complete discussion of these issues is provided in the full paper. 
1 model revision 
poster papers 	1 rather than explicitly considering all of the possible plans that could be abandoned  the problem can be looked at as a question of model revision. if we are using a model of plan execution that does not consider plan abandonment to recognize observation streams in which the agent is abandoning plans  we expect that the computed probabilities for the observation streams will be quite low. laskey   jensen  jensen et ai  1   and others have suggested that cases of an unexpectedly small p observations m odd  should be used as evidence of a model mismatch. 

figure 1: a very simple example plan library. 
　instead of the general p observations model  statistic we propose the probability that none of the observed actions in a subsequence  from say s to t  contribute to one of the goals  call it g   and we denote it 
 if this prob-
ability gets unexpectedly small  we consider this as evidence of a mismatch between the model and the real world. namely the model predicts that the agent is still working on the goal  while the agent may have abandoned it. 
1 	computing notcontrib 
consider the plan library shown in figure 1. the first plan is a very simple plan for achieving s by executing a  1  and c and the second plan for r has only the single step of g. next  assume the following sequence of observations: 

　in this case we know that at time 1 and 1 that the agent has as a goal achieving s. let us assume that all of the elements of the pending set are equally likely to be selected for execution next. note that this is an assumption that we will make for the rest of this paper. nothing about the algorithm hinges on this uniformity assumption. it is made solely for ease of computation and discussion. 
　given this assumption  the probability of seeing c at time 1 is given by:  where m is the number of elements in the pending set that have c as the next action. the probability that we don't see c  that is the probability that any other element of the pending set is chosen at time 1  is just: 

or more generally the probability that we have seen b at time  s - 1  and not seen c by time t: 

　to handle partially ordered plans  this formula must be generalized slightly. with partially ordered plans it is possible for more than a single next action to contribute to the specified root goal. thus  if mqi represents the number of elements  with any next action  in the pending set at time i that contribute to goal q   s-1  is the last time we saw an action contribute to q and t is the current time  

　thus  under the assumptions that we have made we can compute the probability of the subsequence of actions not 

figure 1: required evidence theoretical curves. 
contributing to a given plan or goal. by computing this value and setting a threshold  we can consider any drop in this probability below the threshold as sufficient evidence of a model mismatch and revise the model to reflect the goals abandonment. this requires removing all the elements from the current pending set that contribute to the abandoned goal. modeling the rest of the plans continues as before. 
1 	evidential requirements 
this approach creates an interesting linkage between the size of the pending set  the number of elements that contribute to the goal of interest  and the number of actions that don't contribute to the goal that must be observed before the goal is considered abandoned. 
　figure 1 shows three theoretical curves for the probability of notcontrib for different sets of values. the curves are labeled with the number of actions that contribute to a goal and the size of the pending set. thus the curve labeled   1 of 1  shows the drop in the probability given each observation if there is one action that contributes to the desired goal out of a pending set of size two. note that in these curves  we are again making the assumption that all of the actions in the pending set are equally likely to be chosen. 
　notice that as the ratio of the number of contributing actions to the size of the pending set drops the number of actions required to drive notcontrib down to a particular threshold value increases significantly. we will see the effects of this in the empirical results. 
1 	estimating 	p abandoned{g   obs  
if we compute  for each 
g and threshold our explanations as described in the previous section  we can now produce explanations of the observations in which goals have been abandoned. by considering the complete and covering set of such explanations for the observations we can estimate the probability of a specific goal's abandonment. it is given by: 

1 	poster papers where exp represents the set of all explanations for the observations  and  represents the set of explanations in which goal g is marked as abandoned. 

figure 1: empirical accuracy 
1 	accuracy 
to test this theory we have extended geib and goldman's probabilistic hostile agent task tracker  phatt  to estimate goal abandonment. we will not cover the details of the phatt algorithm here; instead we refer the interested reader to  geib and goldman  1 . we used a very simple plan library with three root goals each having eight unordered steps. to generate test cases  we chose an ordering for the actions for each of the three goals. these plans were then randomly interleaved preserving the intra-plan ordering. to simulate goal abandonment  at each time step one of the goals could be chosen for abandonment. if chosen  all remaining steps of the plan were removed from the observation stream. when given to phatt  each of these test sequences produced one of three possible results: 
abandoned where phatt believed with probability greater than 1 that the correct goal had been abandoned. 
not abandoned where phatt did not believe believe with probability greater than 1 that the correct goal had been abandoned 
not explained where phatt was unable to explain the set of observations. in all cases  this was a result of the system believing that a goal was abandoned before it actually had been. the system was therefore unable to account for the remaining actions in that test data point. 
　the results of one thousand such randomly generated data points at each of nine notcontrib threshold values between 1 and 1 can be seen in figure 1. to aid in understanding  the x-axis plots  l - the notcontrib threshold  which we will call the probability of abandonment threshold pat . the y-axis plots the percentage of test points for each of the possible results at that threshold value. the results confirm our intuitions. 
　the number of test sequences that are not explained drops to zero as the pat is raised. the pat is specifying how much evidence the algorithm needs to have before it can consider a goal abandoned. since the system's failure to explain a test sequence is a result of prematurely believing a goal has been abandoned  as the pat rises and more evidence is required this number should drop to zero. 
　the number of test sequences that are not abandoned rises as the pat is raised. as the threshold rises the system requires more and more evidence to be convinced that the goal has been abandoned. this allows more data points to reach the end of the observation sequence without being convinced of the goal's abandonment. 
　finally  abandoned rises and peaks giving the algorithm a 
　maximum accuracy of about seventy five percent at a pat of between 1 and 1. the algorithm's subsequent dip is again a result of the increasing confidence in abandonment required by the rising pat. as with not abandoned at this point the system's accuracy is falling prey to the limited length of the test sequences. since each test sequence is of limited length the test runs are ending before enough evidence can be observed for the higher pat values. figure 1 will show us why. 
　consider the curve in figure 1 labeled  l of 1.  this is approximately the ratio of contributing actions to the size of the pending set in this example. at a pat of 1 the system will require approximately ten actions in a row that do not contribute to the goal in order to convince itself of the goals abandonment. since the longest any of the tests can be is twenty three actions if the plan is abandoned more than half way through the observation stream it will have a hard time producing enough evidence to convince the system. 
1 	conclusions 
this paper presents a solution to the problem of recognizing when an agent has abandoned a goals based on probabilistic model revision. a number of issues are covered in more detail in the full version of this paper available from the author. 
