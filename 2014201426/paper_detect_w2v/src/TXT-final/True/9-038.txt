 
     a description of techniques used for semantic modeling in a deductive question-answering system is given. the system maintains a dialog and is able to understand situations which can be expressed as a series of sequential time-frames. specific relevant questions are asked by the system when it is unable to succeed in a given task. it can also provide reasons for its previous actions. 
	i. 	introduction 
　　　this paper describes semantic modeling techniques developed for a deductive question-answering system. the problems treated include handling dynamic information  engaging the user in a meaningful dialog and handling a multiplicity of interpretations and assumptions at one time. the data base used to demonstrate these general approaches deals with driving situations. the driver's world was chosen as a data base because it possesses many of the features which make it wellsuited as an experimental domain for a r t i f i c i a l intelligence research in computer understanding. the 
processes involved are of sufficient complexity  dynamic in nature  and amenable to codification. the types of facts encountered and deductions performed in the driver's world are representative of those in the real 
world and would not normally be considered t r i v i a l . since decisions are made based upon events in which the various objects are not stationary  a dynamic representation scheme is essential. in addition  the pertinent rules and regulations have been specified explicitly and relatively unambiguously in the form of laws. 
     the system described here has been limited to the solving of problems associated with deduction and man-
machine interaction about the driver's world. it has been implemented in micro-planner. information is input to the system as micro-planner assertions. at present a parser is being adapted to parse natural language input; it is not implemented and is therefore 
not discussed in the paper. 	it is based in part on 
winograd's programmar.1 
　　　research aimed at developing intelligent systems capable of communicating in natural language has been carried on for well over a decade. simmons1 has surveyed the earlier systems; of the more recent work  winograd   charniakl  and woods  et al.1 have made significant contributions. 
     winograd has shown how a model can be used in conjunction with procedures in a general way. his model has proved very effective in the blocks world. the blocks world does not  however  permit the ai researcher to experiment with a number of problems that can 
this work is an outgrowth of research sponsored by the joint services electronics program  u.s. army  u.s. navy  and u.s. air force  under contract no. daab-1-c-1. 
presently at dept. of electrical and computer engineering  university of michigan  ann arbor  michigan 
1 
be found in domains such as the driver's world. in particular  in the blocks world every object has an exact known location; every object is completely under the control of the model; every object has distinguishing characteristics; usually only unambiguous information is f i l l e d in by the model; and the model does not 
ask the user for specific information when it is needed. 
　　　charniak has relied on extensive use of demons  which are antecedent theorems  and has worked on a model for computer understanding of children's stories. with the demons he is able to f i l l in many assumptions that are not explicitly stated in the stories themselves. these are invoked whenever appropriate patterns occur in the predicate-like input. 
　　　woods  et a l . have demonstrated the practicality of natural language processing by computer in applying these techniques to a lunar science information system. 
i i . technical problems encountered 
　　　researchers dealing with natural language communication with computers have constantly tried to extend the limits of competence of their systems over previous systems. every system is limited to some extent in the discourse it can maintain with its user. particular difficulty comes about when the system must either 
maintain a coherent dialog or cope with information which is not completely specified; or make bubtle inferences on how conditions exist in the real world. 
on multiplicity of interpretations and assumptions 
     in an intelligent dialog it may be the case that a given input could generate many possible interpretations or models of the state of the world. 	as an 
example  consider the following statements which might be encountered in the driver's world. 
four cars arrive at an intersection at the same time 	 1  
and 
two cars arrive at an intersection at the same time. 	 1  
     in  1  it is possible to generate many interpretations  some of which are shown in figure 1. these 
are a l l valid interpretations which are physically as well as legally possible. however  it is felt that in a great majority of cases  the following picture w i l l be brought to mind: an intersection of unmarked two-lane highways with one car in the right hand lane in each part of the intersection  figure 1 . 
　　　in  1  above  excluding rotations  there are two valid interpretations  figure 1 . one does not appear to be more likely than the other. 
1 　　　it could be argued that when a statement is ambiguous or leads to more than one possible interpretation  that each interpretation should be considered. however  an intelligent approach would consider only the most plausible interpretations. so  while in  1  one interpretation is most plausible  in  1  both interpretations are equally possible. 
     sot only is it necessary to choose between various interpretations  but it is also necessary to decide which are the reasonable assumptions that should be made in a given situation. consider the case where a vehicle is approaching an intersection. in the absence of explicit information what should be assumed about the situation at the intersection  are there traffic signals  a police offier  other vehicles  etc. that w i l l affect any decisions concerning the car's progress through the intersection  what  if any  questions should be asked to clarify the situation  
     in  1  the user may have to be asked which interpretation is intended. many systems would report a standard insufficient information message  but a preferable response would be 
what are the relative directions of the vehicles  
this response is direct  to the point  and shows a true understanding of the meaning and ambiguities of the input. 
     it is not really important which assumptions or interpretations are actually made as long as the model that was used to arrive at these decisions corresponds to what we would consider as reasonable. the important point to be made is that any system should be capable of accepting a wide variety of reasonable 
models. 
on responding 
     the intelligence of any system is related to the naturalness of its responses. 	a system which responds with the correct answer inserted into a predefined format appears to be less intelligent than one in 
which responses are spontaneous and more to the point. consider the following inputs: 
a car and a truck approach an intersection from different streets. the car has a yield sign. 
	which one can proceed first  	 1  
a car is approaching an intersection. another car is approaching from a different street and has a yield sign. which car has the right-
	of-way  	 1  
examples  1  and  1  describe exactly the same situation except that where there is a truck in  1  there is a car in  a . the answer to  1  is  the truck  
while the answer to  1  is  the one without the yield sign . the answer  therefore  depends not only on the situation  but also on the way in which objects 
can be uniquely identified. sometimes a question may have to be answered by using its relationship to other things as in  1  rather than by naming it explicitly. 
on deciding what to ask 
　　　when trying to answer a question  the traffic laws usually have to be applied. there are two reasons for any given traffic law to be deemed inapplicable: one or more of the necessary conditions of the law contradict the known information or  alternatively  some of the necessary conditions of the law may be unspecified or unknown. consider the following: 
a car is on a twolane highway. it is behind a truck which is going below the speed limit. can the car pass the truck  
     any law which is not applicable cannot be considered. but what about the law which says you cannot pass on a curve  or the one which states that you cannot pass in a no-passing zone  it is not clear if these laws are appropriate. both of these laws would f a i l because of lack of information. if it is possible to isolate which portion of the law fails because of lack of information  then it would be possible to formulate a question which would ask the user to supply 
a specified piece of information  such as  does this take place in a no-passing zone or on a curve   this 
capability would allow the system to engage the user in an intelligent dialog. 
　　　in this paper we w i l l describe a system based upon the driver's world and the approaches taken to handle a number of technical problems such as those already mentioned above. the techniques used to overcome these problems involve solutions to time handling  system made assumptions  asking relevant questions and setting up relative scenes. 
i i i . sample dialog 
　　　the following is a sample dialog with the computer model for the driver's world. a more detailed discussion describing the performance will be given in a later section. actual input is presently in the form of micro-planner assertions rather than natural english  pseudo-input is in lower case and output is in upper case. 
three cars approach an intersection at the same time. the car in the middle has a yield sign. the car to the left of this car is going to go straight while the third car intends to make a left turn. who has the right-of-way  
*the car without the yield sign and which does not intend to turn left 
the car with the right-of-way proceeds. who has the right-of-way now  
*the car which intends to make a left turn 
suppose  when the three cars approached the intersection the yield sign is replaced by a flashing yellow light. 	which car may proceed  
*the car at the flashing yellow light. 
why  
*the other two directions have yield signs. 
if t h i s car proceeds  who has the right-of-way  
*the car which intends to go straight. 
if t h i s car proceeds  who has the right-of-way  
*the car which intends to make a left turn 
a truck in f r o n t of a car is t r a v e l l i n g below the speed l i m i t . they are on a two-lane h i g h -
	way. 	can the car pass the truck on the r i g h t   
*n1 
why  
*passing on the shoulder is prohibited. 
two cars are approaching an unmarked i n t e r section from d i f f e r e n t d i r e c t i o n s . who has the right-of-way  
*what are the relative directions or positions of the vehicles  
one car is proceeding n o r t h . the other car is t o i t s r i g h t . 
*the car proceeding west. 
thank you. 
*y1u're welcome. 
iv. a model for computer understanding structure 
     the model for the driver's world is implemented in micro-planner. as the input is being accepted  
antecedent theorems may be invoked in order to add information to the data base. 	after the data has been entered any question w i l l be transformed into a goal or series of goals. 	in attempting to satisfy these goals  theorems which represent the laws and other facts concerning the driver's world are applied. 	in a few cases the original goals w i l l not be satisfied due 
to the lack of sufficient information. in attempting to gain more complete information  routines called specialists are invoked in order to determine the most likely default conditions. the applicable specialist 
w i l l take the input data and make appropriate assumptions based on the available information  and generate 
a series of time-frames which includes a l l of the possible interpretations. another attempt is made to satisfy the original goals and the laws are reapplied. if the goals cannot be satisfied  then the user may be asked to supply additional information by use of the conditional failure mechanism. this added information is followed by repeated attempts to satisfy the goals. 
     after a question has been successfully answered  it is possible for the user to further interrogate the 
system  modify the situation  or add additional infor-
mation  or completely change the direction of the dialogue. the previous interpretations w i l l be updated to reflect the new situation. 
     a representation of the structure of the system is given in figure 1. the arrows indicate which routines can be invoked by other routines. since a l l information is expressed relative to time-frames  a description of this formalism w i l l be discussed f i r s t . 
time-frames 
     in this system a completely described state of the world is called an event. 	it consists of a l i s t of participants  e.g. vehicles  and a l i s t of assertions expressing relationships and attributes among these participants. 	every continuous sequence of events is considered to be a series of s t i l l pictures called time-frames. 	a complete time-frame consists of the following: 	an event  a pointer to the time-frame immediately preceding the present one and a l i s t of 
pointers to time-frames which the present time-frame precedes  which may be empty or have multiple entries. 
　　　there are primitives available to create and destroy time-frames. whenever a time-frame is created  a l l of the statements from the previous frame are carried over. it is possible to add or delete assertions from just the present time-frame as well as from the present and a l l following time-frames. it is also possible to move to time-frames with respect to each other. it is felt that this provides a natural way to handle dynamic data and sequences of events. 
　　　in conniver contexts are similar to the timeframes described. however  time-frames  which are embedded in micro-planner  have permitted us to use 
1 

figure 1. organization of the system the pertinent aspects of contexts and yet maintain the s a l i e n t features of micro-planner without excessive overhead as might be the case if implemented in some 
other manner. 
global facts 
　　　global facts are theorems which contain  among other t h i n g s   the properties of objects such as vehicles as w e l l as c e r t a i n laws of nature in the d r i v e r ' s world such as 
if a car is about to enter an i n t e r s e c t i o n then i t i s a t the i n t e r s e c t i o n . 
these facts are expressed in terms of antecedent and consequent theorems. 
　　　the rules of law for the d r i v e r ' s world are stored as consequent theorems. 	an example of one of these laws is given in figure 1. 	these lavs are called 
whenever a legal consequence  such as  who has the 
r i g h t - o f - w a y     has to be determined. 
       some goals have f i l t e r s associated w i t h them that r e s t r i c t the a p p l i c a t i o n to s p e c i f i c time-frames. in general  any theorem may be used to s a t i s f y a goal  but a l l data w i l l be r e s t r i c t e d w i t h respect to time-
frames. 	 see examples in figure 1.  
structure 	of assertions 
　　　the inputs to the system as w e l l as system generated statementa are stored as a series of predicates and arguments. for example  the input  a car a r r i v e s at an i n t e r s e c t i o n   w i l l be represented as 
 al arrive-at car intersection  
a l l of the input assertions are stored as statements in the present time-frame. 
　　　the statement  two cars a r r i v e at an i n t e r s e c t i o n   would be represented as 
 a1 arrive-at *gr1up1 intersection  
 a1 is *gr1up1 *car1 *car1  
　　　whenever an assertion is made  any antecedent theorem containing global information may be invoked to a l t e r or augment the a s s e r t i o n . 
       some of the arguments of the predicate may themselves be names of predicates. for example   two cars a r r i v e at an i n t e r s e c t i o n at the same time  is represented as 
 a1 arrive-at *gr1up1 intersection  
 a1 at a1 same-time  
 a1 is *gr1up1 *car1 *cara  
　　　each of the vehicles or other objects in the system w i l l have on i t s property l i s t the special t r a i t s and modifiers which d i s t i n g u i s h it from other objects. 
specialists 
　　　each of the specialists are routines which contain detailed knowledge of some small aspect of the d r i v e r ' s 
w o r l d . 	the s p e c i a l i s t s use t h e i r knowledge to f i l l in information  to set up r e l a t i v e scenes and to keep track of scene modifiers. 	a s p e c i a l i s t can be called 
upon at any time. unlike charniak's demons  they do not have to be pattern invoked. 
1 　　　as an example  consider the i n t e r s e c t i o n s p e c i a l i s t . this routine can examine scenes dealing w i t h intersections and t r i e s to place a l l vehicles in a standard l o c a t i o n of a standard i n t e r s e c t i o n . a standard i n t e r s e c t i o n is made up of four  arms  each of which is a type of street  e.g. two-lane . the positions which make a standard i n t e r s e c t i o n are shjqmtf in fig. 1. 
     the station position is immediately adjacent to the intersection. when approaching  the position can only contain one vehicle  which is the next to enter the intersection. the approach position can contain any number of vehicles. vehicles in the approach are heading toward the intersection but w i l l not be the first to enter. the leave position w i l l hold any num-
ber of vehicles which have left the intersection. the in position contains the one car which has entered the intersection. the sign position denotes the occurence of any traffic control devices. the specialist attempts to assign every vehicle a pair of values   arm position   which uniquely describes its position in the time-frame. 
     the intersection specialist has only one input  the name of the time-frame which contains the assertions for the scene. when the specialist is called  a series of antecedent theorems are asserted. each of the input assertions is then pseudo-asserted  i.e.  
any relevant theorem is used but the calling theorem is not asserted  it has been asserted previously . 
     the time-frame system described previously is used to create and save possible interpretations. as each new vehicle is encountered  the possible number interpretations is increased. as new information is evaluated  any interpretations which contradict this information is deleted. 
　　　for example  if it is known that two cars  a and s  are at an intersection  if the fact that a is to the left of b is input  then any interpretation which contradicts this  such as an interpretation which states that b is to the left of a  is deleted. 
　　　whenever an antecedent theorem has been successfully applied  the assertion which has been used is said to have been processed. if an input assertion has predicates for arguments the predicates must be processed f i r s t . the net effect of this is that before a vehicle position or relative position can be modified  the vehicle has to be placed in the intersection. there is a constant check to eliminate equal interpretations  i.e.   rotations . 
　　　the specialist returns a l i s t of time-frames which are a l l possible interpretatlon s . after this processing has been completed the most likely interpretation  or interpretations  is  are  selected. 
     among the various specialists are those which deal with two-lane highways  four-lane highways  ramps  alleys  intersections  passing  parking  etc. 
giving reasons 
     when some of the laws within the system are successfully applied  it is possible to assign a  reason  for success to some of the objects in the scene. 
     as an example consider law1  fig. 1 . 	every vehicle that is used when the law is satisfied can be placed in one of two classes. 	the vehicle which must yield has associated with it the  reason  intends to 
make a left turn while the vehicle which has the rightof-way is tagged with the  reason  does not intend to make a left turn. 
     in order for the question to be answered properly  it may be necessary to use the reasons to form an answer. 	if the objects do not have unique names  then the answer must be given in relative terms. 	in fact  the answer w i l l be the reasons associated with that object. 
english: 
a car which intends to make a left turn at an intersection must yield to a l l traffic approaching from the opposite direction. 
micro-planner: 

consider the following: 
two cars approach an intersection from opposite directions. one car intends to make a left turn. which car has the right-of-way  
　　　because the vehicles do not have unique names  a relation must be used to supply the answer. the proper response is the car that does not intend to make a left turn. while the question: 
a car and a truck approach an intersection from opposite directions. the truck intends to make a left turn. who must yield  
would be answered the car. 
generating questions 
　　　in attempting to answer a question or satisfy a 
　　　goal  it is possible to encounter failure in the traditional micro-planner sense. this may lead to the 
case where it is impossible to pursue the deduction further. we have implemented in micro-planner the notion of conditional failure to cope with a large number of cases of this type. 
     when a goal f a i l s   there are two possible cases. in the f i r s t case  there is no positive information to support the goal but there exists information which 
1 would prevent the goal from succeeding no matter what additional information is supplied. in the second case  additional information could be used to prove the goal. consider the following example: suppose the data base contains the assertion 


	lane 1 	lane 1 
arm 1 
figure 1. standard 
by 1-lane 
intersection 
and f u r t h e r suppose we wish to prove the goals 
 thgoal  a on b  st  	 1   thgoai.  b on c  st  	 1  
these goals w i l l o r d i n a r i l y always f a i l . 
　　　note  that a d i s t i n c t i o n can be made between these two f a i l u r e s . in ci  the goal could never succeed because we can prove 
          thgoal  not a on b  $t  but no such r e s u l t can be proved for 
 thgoal  not b on c  $t  
       in order to take advantage of t h i s d i s t i n c t i o n   consider the f o l l o w i n g procedure: 
 1  upon entering s p e c i a l l y marked theorems  if a marked subgoal f a i l s without having once succeeded and subsequently backed through  then record t h i s goal on a l i s t called gfail. 
 1  if the major goal f a i l s in the normal sense  then for each element of gfail construct a 
goal which is i t s converse. 
 1  if any goal in  1  succeeds remove the corresponding goal from gfail. 
 1  any remaining elements on gfail represent c o n d i t i o n a l f a i l u r e s  those goals which could conceivably be determined to be true  which are used to construct relevant questions. 
in the one example  1  w i t h the goal 
                thgoal  b on c  st  we proceed as f o l l o w s : 
       because  b on c  is not an assertion  theorem x-on-z is invoked. goall succeeds with  b on a . g1al1   a on c  f a i l s and is placed on glist. an a t tempt is made to f i n d another i n s t a n t i a t i o n for goall  
 b on s y . this fails but is not placed on glist because it has previously succeeded. x-on-z f a i l s . glist now contains only  a on c . 
     the converse   not a on c   is constructed  but this goal fails when using not-x-on-y. consequently  the glist remains unchanged. 
　　　it is now possible to easily construct a relevant question the answer to which could provide the necessary information to satisfy the original goal   b on c . in this case the question i s : is  a on c   if the response is positive   a on c  is asserted and the original goal when reattempted w i l l succeed. 
　　　it is possible to assign to each element of gfail a priority which will direct the order in which the questions are asked when gfail has more than one entry. 
v. performance of the model 
　　　the performance of the model can best be judged by describing a sample dialog. 	the dialog given earlier w i l l serve this purpose. 	consider the f i r s t statement. 
three cars approach an intersection at the same time. the car in the middle has a yield sign. the car to the left is going to go straight while the third car intends to make a left turn. who has the right-of-way  
the actual input to the system would be the following micro-planneh assertions. 

　　　the notation *car1  *car1 and *car1 for the vehicle indicates the vehicle names are  system assigned  rather than user assigned  e.g. a car and a truck . in order to answer the question the yielding laws w i l l be applied to determine which vehicles must 
yield. in this case a l l of the applicable laws w i l l f a i l because the data is incomplete. the input statements which state that some vehicle is approaching 
the intersection will lead to the activation of one of the intersection specialists. i n i t i a l l y the model w i l l 
be a two-lane by two-lane intersection. if the data is contradictory and cannot be accommodated in the 
model another specialist  e.g. intersection of highways  w i l l be called. 
     when the two-lane by two-lane intersection specialist is called  the following scene is constructed. 

and the time-frame is 

	where global is the global time-frame. 	the assumptions 
that are supplied by the specialist are that the inter-
1 
section is composed of a pair of two-lane streets  one with a yield with one empty arm and one car in each of the others. under these assumptions  the above is the only interpretation excluding rotations. using the yield laws  e.g.. law1   the cars are compared pairwise  and it is determined that *carl must yield to *car1 and *car1 because of the yield sign  and *car1 must yield to *car1 because it intends to turn left. it is then determined that the only logi-
cal choice for the answer is *car1. because this is a system supplied name  the reasons associated with *car1 w i l l be used to answer the question instead of the system generated name. thus the answer 
the car without the yield sign and which does 
not intend to turn left 
the next input 
 the car with the right-of-way proceeds. who has the right-of-way now  
the f i r s t input 
 1 proceed *car1  
 1 have-right-of-way *car1  
causes a new time-frame to be created. the vehicles in the time-frames w i l l be advanced  the f i n a l p o s i t i o n being determined by the applicable procedures. 
　　　the f o l l o w i n g fact is added to the present timeframe : 
　　　 a1 at *car1 arm1 leave  g i v i n g : 
	 *tf1  *carl *car1 *car1  a1 a1 a1 a1 a1  	 *tfl   
regarding the question of r i g h t - o f - w a y   the same approach as before is taken except that the new timeframe is now used. 
a p p l i c a t i o n s of the laws are straightforward and 
the car which intends to make a left turn is given as an answer. 
for the next input: 
　　　suppose  when the three cars approached the i n t e r s e c t i o n the y i e l d sign is replaced by a f l a s h i n g yellow l i g h t   which car may proceed  the f o l l o w i n g input assertions are used: 
 1 when arrive-at *group1 intersection  
 1 is *group1 *car1 *car1 *car1  
 1 replace yield-sign flashing-yellow  
　　　the f o l l o w i n g is added by an i n t e r s e c t i o n s p e c i a l i s t : 
	 a1 	arm1 flashing-yellow  
	 a1 	arm1 flashing-yellow  
　　　 a1 arm1 flashing-red   a1 arm1 flashing-red  and the time-frame 
 *tf1  *car1 *car1 *car1  
            al a1 a1 a1 a1 a1 a1 a1 a1  *tf1   is created. 
in this case the specialists provide the informa-
tion that flashing lights come in pairs at intersections. the opposite directions have flashing yellow and red lights  respectively. the flashing red lights w i l l be interpreted as yields. so the right answer is the car at the flashing yellow light. 
　　　inputing the question  why   does not change the time-frame. when goals succeed  reasons  are stored. these are returned as the answer. so  here the reason is the other two directions have yield signs. note that this default condition may not be true at a l l times  but the inserted assumptions should be true for 
a majority of the time. 
when asking: 
　　　if the car proceeds  who has the right-of-way  the f o l l o w i n g assertion and time-frame are formed. 
  a l l at *car1 arm1 leave  
 *tf1  *car1 *car1 *car1  
 a1 a1 a1 a1 a1 a1 a1 a1 a1  *tf1   
again  the goal succeeds and the answer is the car which intends to go straight. now asking 
if t h i s car proceeds who has the right-of-way  
adds 
        a1 at *car1 arm1 leave  and 
	 *tf1 	 *car1 *car1  
 a1 a1 a1 a1 a1 a1 a1 a1  *tf1   
to the database. 	there is only one car at the i n t e r section and it has the r i g h t - o f - w a y . 	so  the car 
which intends to make a left turn is the response. 
s t a r t i n g w i t h 
a truck in f r o n t of a car is t r a v e l l i n g below the speed l i m i t . they are on a two-lane highway. can the car pass the truck on the r i g h t   
a new scene is required. the f o l l o w i n g assertions are made: 
 a1 in car lane1  
 a1 in truck lane1  
 a 1 in-front truck car  
 a1 travel truck  
 a1 below a1 speed-limit  
 a1 *scene1 two-lane-highway  
 *tf1  car truck  a1 a1 a1 a1 a1 a1  *tf1   
notice that the model assumes a two-lane highway. using the lav dealing with passing on the right on two-lane highways we find that the answer is no. the next question   why   is answered by using the reason  passing on the shoulder is prohibited. 
the next question is 
two cars are approaching an unmarked intersection from different directions. who has the rightof-way  
because the law is straightforward  the problem is not who has the right-of-way but what are the relative 
directions of the two cars. 
　　　the intersection specialist is called and returns a l i s t of time-frames containing the possible inter-

1 
pretations. in this case there are two which would be considered equally likely  see fig. 1 . the user is asked which of the interpretations is intended. when the information is given  the question can be answered 
and the proper response  the car proceeding west  is given. 
v. concluding remarks 
     we have described techniques for semantic modeling in a deductive question-answering system. the system can maintain a dialog and is able to understand situations which can be expressed as a series of sequential time-frames. the system has the ability to f i l l - i n information which corresponds roughly to those assumptions that might normally be made by a person. the system can ask specific relevant questions of the user because it knows which subgoal failed for insufficient information. it can give reasons for answers based upon the tags left by successfully using the laws. the specialists can set up relative scenes  f i l l in detailed assumptions about situations  and set 
up scene modifiers. 
