 
　　one of the major unsolved problems in designing an autonomous agent |robot  that must function in a complex  moving environment is obtaining reliable  real-time depth information  preferably without the limitations of active scanners. stereo remains computationally intensive and prone to severe errors  the use of motion information is still quite experimental  and autofocus schemes can measure depth at only one point at a time. we examine a novel source of depth information: focal gradients resulting from the limited depth of field inherent in most optical sy.stems. we prove that this source of information can be used to make reliable depth maps of useful accuracy with relatively minimal computation. experiments with realistic imagery show that measurement of these optical gradients can potentially provide depth information roughly comparable to stereo disparity or motion parallax  while avoiding image-to-image matching problems. a potentially real-time version of this algorithm is described. 
i. introduction 
   our subjective impression is that we view our surroundings in sharp  clear focus. this impression is reinforced by the virtually universal photographic tradition** to make images that are everywhere in focus  i.e.  that have infinite depth of field unfortunately  both this photographic tradition and our feeling of a sharply focused world seems to have lead vision researchers in both human and machine vision to largely ignore the fact that in biological systems the images that fall on the retina are typically quite badly focused everywhere except within the central fovea  1 . there is a gradient of focus  ranging from nearly perfect focus at the point of regard to almost complete blur at points on distant objects. 
　　it is puzzling that biological visual systems first employ an optical system that produces a degraded image  and then go to great lengths to undo this blurring and present us with a subjective impression of sharp focus this is especially peculiar because it is just as easy to start out with everything in perfect focus. why  then  does nature prefer to employ a lens system in which most of the image is blurred  
this research was made possible in part by a grant from the systems 
development foundation  and by a grant from the national science 
foundation  grant no. dcr-1  and by defense advanced research projects agency contract no. mda 1-c-1 
　a practice established in large part by ansel adams and others in the famous ''f/1 club  
   in this paper we report the finding that this gradient of focus inherent in biological and most other optical systems is a useful source of depth information  prove that these focal gradients may be used to recover a depth map  i.e.  distances between viewer and points in the scene  by means of a few  simple transformations of the image  and that with additional computation the reliability of this depth information may be internally checked. this source of depth information  which differs markedly from that used in automatic focusing methods  has not previously been described in the human vision literature  and we have been unable to find any investigation of it in the somewhat more scattered machine vision literature. the performance of a practical technique has been demonstrated on realistic imagery  and an inexpensive  real-time version of the algorithm is described. finally  we report experiments showing that people make significant use of this depth information. 
   this novel method of obtaining a depth map is important because there is currently no passive sensing method for obtaining depth information that is simultaneously fast enough  reliable enough  and produces a sufficiently dense depth map to support the requirements of a robot moving in a complex environment. stercopsis  despite huge investment  remains computationally intensive and prone to severe errors  the use of motion information is still in an experimental stage  and autofocus schemes can measure depth at only one point at a time. we believe that this research  therefore  will prove a significant advance in solving the problem of real-time acquisition of reliable depth maps without the limitations inherent in active scanners  e.g.  laser rangefinders . 
ii. the focal gradient 
   most biological lens systems are exactly focused* at only one distance along each radius from the lens into the scene. the locus of exactly focused points forms a doubly curved  approximately spherical surface in three-dimensional space only when objects in the scene intersect this surface is their image exactly in focus; objects distant from this surface of exact focus are blurred  an effect familiar to photographers as depth of field. 
   the amount of defocus or blurring depends solely on the distance to the surface of exact focus and the characteristics of the lens system; as the distance between the imaged point and the surface of exact focus increases  the imaged objects become progressively more defocused. if we could measure the amount of blurring at a given point in the image  therefore  it seems possible that we could use our knowledge of the parameters of the lens system to compute the distance to the corresponding point in the scene. 
  exact foe us  is taken here to mean  has the minimum variance point spread function   the phrase  measurement of focus  is taken to mean 
 characterize the point spread function.  



1 a. pentland 
and thus obtain a. the solution of this linear regression is 
equation  1  we can obtain the following estimate of the value of the spatial constant 
having estimated we can now use equation  1  to find the distance to the imaged point; note that there are two solutions  one corresponding to a point in front of the locus of exact focus  the other corresponding to a point behind it this ambiguity is generally unimportant because we can usually arrange things so that the surface of exact focus is nearer to the sensor than any of the objects in the field of view. 
b. comparison across differing apertures 
   the limiting factor in the previous method is the requirement that we must know the scene characteristics before we can measure the focus; this restricts the applicability of the method to special points such as step discontinuities. if  however  we had two images of exactly the same scene  but with different depth of field  we could factor out the contribution of the scene to the two images  as the contribution is the same   and measure the focus directly. 
   figure 1 shows one method of taking a single view of the scene and producing two images that are identical except for aperture size and therefore depth of field. this len1 system uses a half-silvered mirror  or comparable contrivance  to split the original image into two identical images  which are then directed through lens systems with different aperture size. because change in aperture does not affect the position of image features  the result is two images that are identical except* for their focal gradient  amount of depth of field   and so there is no difficulty in matching points in one image to points in the other. figures 1  b  and  c  show a pair of such images. alternatively  one could rig a video or cod camera so that alternate frames employ a different aperture; as long as no significant motion occurs between frames the result will again be two images identical except for depth of field. 
   because differing aperture size causes differing focal gradients  the same point will be focused differently in the two images; for our purposes the critical fact is that the magnitude of this difference is a simple function of the distance between the viewer and the imaged point. to obtain an estimate of depth  therefore  we need only compare corresponding points in the two images and measure this change in focus. because the two images are identical except for aperture size they may be compared directly; i.e.  there is no matching problem as there is with stereo or motion algorithms. thus we can then recover the absolute distance d by simple point-by-point comparison of the two images  as described below. 
   mathematical details. we start by taking a patch centered at within the first image 
and calculate its two-dimensional fourier transform is done for a patch at the corresponding point in the second image  giving us . again  note that there is no matching problem  as the images are identical except for depth of field. 
   now consider the relation of . both cover the same region in the image  so that if there were no blurring both would be equal to the same intensity function . however  because there is blurring 
their overall brightness might also differ. 


   we may solve either of the two equations in  1  for d  the distance to the imaged surface patch. thus the solution is overconstrained; both 
solutions must produce the same estimate of distance -otherwise the estimates of ax and a-z must be in error. this can occur  for instance  when there is insufficient high-frequency information in the image patch to enable the change in focus to be calculated. the important point is that this overconstraint allows us to check our answer  if the equations disagree  then we know not to trust our answer. if  on the other hand  both equations agree then we can know  to within measurement error  that our answer must be correct. 
e. human perception 
   we have recently reported evidence demonstrating that people make use of the depth information contained in focal gradients |1|; interestingly  the ecological salience of this optical gradient does not appear to have been previously reported in the scientific literature. the hypothesis that the human visual system makes significant use of this cue to depth has been investigated in two experiments. 
   in the first experiment  pictures of naturalistic scenes were presented with various magnitude of focal gradient information. it was found that increasing the magnitude of the focal gradient results in increasing subjective depth. in the second experiment  subjects were shown a rightward rotating wireframe  nekker  cube displayed in perspective on a cut. such a display may be perceived as either as a rigid object rotating to the right  or  surprisingly  as wobbling  nonrigid object rotating to the left. normally subjects see the rigid interpretations most of the time  but when we introduced a focal gradient that favored the non-rigid interpretations  the non-rigid interpretations was seen almost as often as the rigid one. 
   an experiment demonstrating the importance of depth of field in human perception can be easily performed by the reader. first make a pinhole camera by poking a small  clean hole through a piece of stiff paper or metal. imposition of a pinhole in the line of sight causes the depth of field to be very large  thus effectively removing this depth cue from the image. close one eye and view the world through the pinhole  holding it as close as possible to the surface of your eye  and note your impression of depth  for those of you with glasses  things will look sharper if you are doing it correctly . now quickly remove the pinhole and view the world normally  still using only one eye . the change in the sense of depth is remarkable  many observers report that the change is nearly comparable to the difference between monocular and binocular viewing  or the change which occurs when a stationary object begins to move 
iii. implementation and evaluation 

d. accuracy 
   possibly the major question concerning the usefulness of focal gradient information is whether such information can be sufficiently accurate there are two major issues to be addressed first  can we 
estimate the variance a of the point spread function with sufficient accuracy  and second  does this translate into a reasonable degree of accuracy in the estimation of depth. 
   recent research aimed at estimating the point spread function has shown that it may be accurately recovered from unfamiliar images despite the presence of normal image noise  1 . further  it appears that humans can estimate the width of the point spread function to within a few percent |1|. these findings  together with the results of estimating reported in the next section  show that accurate estimation of c is practical given sufficient image resolution. 
   the second issue is whether the available accuracy at estimating σ translates into a reasonable accuracy in estimating depth. figure 1  a  show the theoretical error curve for the human eye  assuming the accuracy at estimating σ reported in  1|. it can be seen that reasonable accuracy is available out to several meters. this curve should be compared to the accuracy curve for stereopsis  shown in figure 1  b   again assuming human parameters. it can be seen that the accuracies are comparable. 
a. using sharp edges 
   the first method of deriving depth from the focal gradient  by measuring apparent blur near sharp discontinuities  was implemented in a straightforward manner  convolution values near zero-crossings were employed in equations  1  -   1   and evaluated on the image shown in figure 1. in this image the optical system had a smaller depth of field than is currently typical in vision research; this was done because the algorithm requires that the digitization adequately resolve the point spread function. 
   figure 1 also shows the depth estimates which were obtained when the algorithm was applied to this image. part  a  of this figure 1 shows all the sharp discontinuities identified  it was found that there was considerable variability in the depth estimates obtained along these contours  perhaps resulting from the substantial noise  1 of 1 bits  which was present in the digitized image values to minimize this variability the zero-crossing contours were segmented at points of high curvature  and the depth values were averaged within the zero-crossing segments figures 1  b    c   and  d  show the zero-crossing segments that have large  medium  and small depth values  respectively. it can be seen that the image is properly segmented with respect to depth  with the exception of one small segment near the top of  c . this example demonstrates that this depth estimation technique - which requires little computation beyond the calculation of zero-crossings - can be employed to order sharp edges by their depth values. 

1 a. pentland 
areas of 1  c  that are black have insufficient high-frequency energy in the sharp-focus image to make an estimate of depth. 
it can be seen that this disparity map is fairly accurate. note 

figure 1. an indoor image of a sand castle  refrigerator  and door  
together with depth estimates for its zero-crossing segments. part  a  of this figure shows all the sharp discontinuities found. parts  b    c   and  d  show the zero crossing segments that have large  medium  and small depth values  respectively. it can be seen that the image is properly segmented with respect to depth  with the exception of one small segment near the top 
of  c . 
b. comparison of different apertures 
   the second technique  comparing two images identical except for aperture  can be implemented in many different ways. we will report a very simple version of the algorithm that is amenable to an inexpensive real-time implementation. 
   in this algorithm two images are acquired as shown in figure 1  a ; they are identical except for their depth of field and thus the amount of focal gradient present  as shown in figures 1  b  and  c . these images are then convolved with a small laplacian filter  providing an estimate of their local high-frequency content. the output of the laplacian filters are then summed over a small area and normalized by dividing them by the mean local image brightness  obtained by convolving the original images with a gaussian filter. it appears that a region as small as 1 x 1 pixels is sufficient to obtain stable estimates of high-frequency content  figures 1  a  and  b  show the normalized high-frequency content of figures 1  b  and  c  
   finally  the estimated high-frequency content of the blurry  largeaperture image is divided by that of the sharp  small-aperture image  i.e.  each point of figure 1  a  is divided by the corresponding point in figure 1 b . this produces a  focal disparity  map  analogous to a stereo disparity map  that measures the change in focus between the two images and whose values are monotonically related to depth by 
equation  i . figure 1  c  shows the disparity map produced from figures 1  b  and 1  c ; intensity in this figure is proportional to depth. that points reflected in the bottle are estimated as further than points along the edge of the bottle; this is not a mistake  for these points the distance traveled by the light is further than for those along the edge of the bottle. this algorithm  in common with stereo and motion algorithms  does not  know  about mirrored surfaces. 
c. design for a real-time implementation 
　　a minimum of one convolution per image is required for this technique  together with a left shift and four subtractions for the laplacian  and three divides for the normalization and comparison. if special convolution hardware is available  one can use two convolutions one laplacian and one gaussian per image  leaving only three divides* for the normalization and comparison. frame buffers that can convolve image data in parallel with image acquisition are now available at a reasonable price  leaving as few as 1 operations per pixel to calculate the disparity map. for a 1f  x 1 image  this can be accomplished in as little as 1 seconds with currently available microcomputers. 
iv. discussion 
　　the most striking aspect of this source of depth information is that absolute range can be estimated from a single view with no image-toirnage matching problem  perhaps the major source of error in stereo and motion algorithms. furthermore  no special scene characteristics need be assumed  so that the techniques utilizing this cue to depth can be generally applicable the second most striking fact is the simplicity of these algorithms: it appears that a real-time implementation can be accomplished relatively cheaply. 
   measurement of the focal gradients associated with limited depth of field appears to be capable of producing depth estimates that are at least roughly comparable to edge- or feature-based stereo and motion which can be done by table lookup. 

algorithms. the mathematics of the aperture-comparison technique shows it to be potentially more reliable than stereo or motion i.e.  there is no correspondence problem  and one can obtain an internal check on the answer although  as discussed above  it has somewhat less accuracy. 
   the sharp-edge algorithm appears to have potential for useful depth-plane segmentation  although it is probably not accurate enough to produce a depth map. i believe that this algorithm will be of some interest because most of the work finding and measuring the slope of zero-crossings - is often already being done for other purposes. thus this type of depth-plane segmentation can be done almost as a side 
effect of edge finding or other operations. 
　　the aperture-comparison algorithm provides considerably stronger information about the scene because it overconstrains scene depth  allowing an internal check on the algorithm's answer thus it provides depth information with a reliability comparable to the best that is theoretically available from three-or-more image stereo and motion algorithms  although it has son's what less depth resolution. the major limitation in measuring focal gradient depth information in this manner appears to be insuring sufficient high-frequency information to measure the change between images; this requires having both adequate image resolution and high-frequency scene content. 
　　summary. in summary  we have described a new source of depth informal ion - the focal gradient that can provide depth information at least roughly comparable to stereo disparity or motion parallax  while avoiding the image-to-image matching problems that have made stereo and motion algorithms unreliable we have shown that the limited depth of field inherent in most optical systems can be used to make depth maps of useful accuracy with relatively minimal computation  and have successfully demonstrated a potentially real-time technique for recovering depth maps from realistic imagery. it is our hope  therefore  that this research will prove to be a substantial advance towards building a robot that can function in complex  moving natural environments 
