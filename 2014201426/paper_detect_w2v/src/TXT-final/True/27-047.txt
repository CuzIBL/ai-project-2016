 
we present an approach to syntax-based machine translation that combines unification-style inter pretation with statistical processing this approach enables us to translate any japanese newspaper article into english with quality far better than a word for-word translation novel ideas in clude the use of feature structures to encode word lattices and the use of unification to compose and manipulate lattices unification also allows us to specify abstract features that delay target-language synthesis untd enough source language information is assembled our statistical component enables us to search efficiently among competing translations and locate those with high fnglish fluency 
1 	background 
japangloss  knight et al  1  1  is a project whose goals are to scale up knowledge-based machine translation   k b m t   techniques to handle japancstfnghsh newspaper m t   to achieve higher quality output than is currently available and to develop techniques for rapidl} constructing mt systems we built the first ver sion of japangloss in nine months and recently participated in an arpa evaluation of mt quality  white and o'connell  1  japangloss is an effort within the larger pangloss  nmsu/crl et al 1  mt 
project 
　our approach is to use a k b m t framework  but to fall back on statistical methods when knowledge gaps arise  as they inevitably will  we syntactically analyze japanese text  map it to a semantic representation  then 

   the semantic representation contains conceptual to kens drawn from the 1-term sensus ontolog}  knight and luk  1  semantic analysis proceeds as a bottom-up walk of the parse tree  in the style of montague and moore  dowty et al 1  moore 1  se mantics is compositional  with each parse tret node assigned a meaning based on the meanings of its children leaf node meanings are retrieved from a semantic lex icon  while meaning composition rules handle internal nodes semantic rules and lexical entries are sensitive to syntactic structure  e g 

generate english 
1 

1   which includes a large systemic grammar of english gaps in the generator's knowledge are filled with statistical techniques  knight and hatzivassiloglou  1 knight and chander  1   including a model that can rank potential generator outputs the english lexicon includes 1 roots  comparable in size to the 1 roots used in japanese syntactic analysis 
　all of these kbs  however  are still not enough to drive full semantic throughput major missing pieces include a large japanese semantic lexicon and a set of ontological constraints we are attacking these problems with a combination of manual and automatic techniques  oku mura and hovy  1  knight and luk  1  meanwhile  we want to test our current lexicons  rules  and analyzers in an end-to-end mt system 
　we have therefore modified our kbmt system to include a short-cut path from japanese to english which we describe in this paper this path skips semantic analysis and know ledge-based generation  but it uses the same syntactic analyses  lexicons  etc   as the full system we call this short-cut glossing  and it features a new component called the glosser whose job is to transform a japanese parse tree into english using easily obtamable resources our glosser achieves 1% throughput  even when the parser fails to fully analyze the input sentence and only produces a fragment ary parst tree 
1 	bottom-up glossing 
in thinking about the glossing problem- turning japanese parse trees into english-we had the follow ing goals and insights 
  quality glossing necessarily involves guessing  at  is most obvious from an ambiguous word like ben which may be glossed as either rice or american without a semantic analysis improved guessing is the road to improved quality 
1. all potential translation guesses can be packed into an english word lattice of the sort used in speech recognition systems 
1. guesses can be ranked with a statistical ian guage model and the most promising ones can be identified with a search procedure 
  component re-use it is possible to build a glosser very quickly if we re-use representations and modules from a full kbmt system 
1. word lattices can be stored and manipulated as feature structures 
1. the compositional semantic interpreter can serve as a glosser if we provide new knowledge bases 
1. the statistical model we built for ranking gen erator outputs  knight and hatzivassiloglou  
1  can also be used for glossing 
　this section describes how we put together an mt system based on these ideas we concentrate here on the components and knowledge bases  deferring linguistic and statistical aspects to following sections 
　word lattices model ambiguities from three sourcesjapanese syntactic analysis  lexical glossing  and english synthesis here is a small sample lattice 

this lattice encodes 1 possible transitions the two mam pathways correspond to two different parses 1 he star symbol  *  stands for an empty transition 
　our original bottom-up semantic analyzer transforms parse trees into semantic feature structures however we can make it produce word lattices if we encode lattices with disjunctive feature structures e g 

　in the above representation   1r* marks mutually disjoint components of the gloss while the features op1  op1 etc represent sequentially ordered portions of the gloss 
　this structure can be transformed automatically into a format suitable for statistical processing as part of that transformation  we also do a hit of english morphol 
ogy  to simplify the analyzer s work the analyzers still runs as a bottom-up walk of the parse tret  using unification to implement montague-sty it composition now ever  we replace the conventional semantic lexicon with a gloss lexicon  easily obtainable from an online japanese 
english dictionary 

we also replace semantic rules with glossing rules  e g 
	hatzivassiloglou and knight 	1 


figure 1 semantic analysis versus glossing both convert parse trees into feature structures using unification-based compositional techniques semantics computes a conceptual representation while glossing computes a target language 
word lattice 
1 


　this rule says to gloss a japanese noun phrase  hp  created from a relative clause  s  combining with another noun phrase  hp   glue together the following-an english gloss of the child ip  a relative pronoun  either which or that   and an english gloss of the s the rule also propagates abstract features  tmp  from the child hp to the parent we return to these features in the next section 
　we built a set of 1 complex rules to match the structure* in our syntactic grammar our new semantic analyzer composes glosses  word laltices  rather than meanings  so we call it the glosser 
　figurt 1 compares glossing and semantic interpretation each parse tree node js annotated with its analysis sentence level analyses appear at the top these analyses are then fed to subsequent japan gloss modules- to the generator  in the case of semantic analysis  or 
directly to the statistical model  in the case of glossing  
1 	linguistic aspects 

　how to efficiently turn bundles of abstract features into english is a difficult general problem lving at the heart of natural language generation our glosser tackles only simple instances of this problem  involving at most three of four features three binary features can require up to eight rules to 'spell out   and we sometimes must specify all cases often  however we see a decomposition in which one abstract feature spells itself out independently of the others in these cases  there is no exponential blowup in thp required number of glosser rules 
　here is a fragment of rules dealing with the above example 
	hatzivassiloglou and knight 	1 

in this notation  =c  a symbol borrowed from lexical-
functional grammar  kaplan and bresnan  1   means the feature sequence must already exist in the incoming child constituent  and *xdr* sets up a disjunction of feature constraints  only one of which is allowed to be satisfied 
1 	statistical language modeling 
our glosser module proposes a number of possible translations for each japanese word or sequence of words that have been matched to a syntactic constituent by the bottom-up parser each such translation unit represents a lexical island according to the knowledge the glosser hat  1 e   a. piece of text for which no other constraints are available at the same time  the various renditions of each translation unit can combine together  leading to many possible translations at the sentence level in order to select among the many combinations of these possibilities we need an objective function that will score them  hopefully ranking the correct translation near the top to accomplish this task  we approximate correctness by fluency  and further approximate fluency by like lihood  selecting the combination of words and phrases that seems most likely to occur in the target language 
this approach offers two advantages 
  because we measure likelihood at the sentence level  we take into account interactions between words and phrases that are produced from different parts of the japanese input for example bei in japanese may mean either american or rice and sha may mean either shrine or company if both possibilities survive for both words after the glosser processes beisha  the likelihood criterion will select american company which is almost always the correct translation in addition ranking potential translations b  their probability in the target language indirectly handles collocational constraints and allows the correct choice of function words which may not appear in the source text at all  e g articles in japanese  or are subject to non-compositional lexical constraints  e g prepositions in english  as in afraid of or on monday versus in february  
  in the absence of additional lexical constraints orig-inating from neighboring target language words and phrases individual translations containing more common and widely used words are preferred over translations that contain more rare and obscure words in this way the japanese word kuruma will he translated as car and not as motorcar this tactic is optimal when no disambiguating information is available  since it selects the most likely transla tion avoiding rare and very specialized alternatives 
　in the remainder of this section we discuss how we measure the probability of an english sentence from the probabilities of short sequences of words  n-grams  bahl et al  1    how we estimate these basic probabilities of n-grams  how we handle problems of sparse data by smoothing our estimates  and how we search the space of translation possibilities efficiently during translation so as to select the best scoring translations 
1 
1 	the sentence likelihood model 
as we discussed in the previous paragraph  we want to associate with each english sentence a a likelihood measure pt a  since the number of such sequences is very large and our training text is not unlimited  we cannot expect to count the occurrences of 1 in a eorpus and then use a classic estimation technique such as maximum likelihood estimation 1 instead  we adopt a markov assumption  according to which the probability of seeing a given word depends only on the short history of words appearing just before it in the sentence using a history of one or two previous words  the stochastic process that generates sequences of english words is approx1 mated by a first or second order markov chain  bigram or ingram model  respectively for reasons of numerical accuracy with finite precision computations  we. convert probabilities to log likelihoods then  the log-likelihood of a sequence of words s = w1 wn is 

　unfortunate!}  this likelihood model will assign smaller and smaller probabilities as the sequence becomes longer since we need to compare alternative translations of different lengths we alleviate this problem by adding a heuristic corrective bonus which is an increasing function of sentence length after experimenting with several such functions we have found that the function f n  = 1n where n is the length of the word sequence gives satisfactory results when added to the log-likchhood measure this is equivalent to adding an 
exponential function of length to the original probabili-
ties 
1 	estimating n-gram probabilities 
to estimate the conditional bigram and ingram proba bilities used in our model  we processed a large corpus of carefully written english texts and we measured the frequencies of one two- and three-word sequences in it since we aim at translation of unrestricted japanese newspaper articles  we selected the wall street journal  wsj  corpus1 as the most representative available collection of fnglish texts that our output should imitate we processed the 1 and 1 years from the wsj corpus  giving us 1 million words of training text  containing approximately 1 different word types 
　the large number of different word tvpes makes our modeling task significantly more complicated than previous similar language models these models were usually designed for speech recognition tasks  where the vocabulary was limited to at most a few thousand frequent english words with our vocabulary of 1 words  we have 1 different bigrame  and 1 1 differ ent trigrams handling such large numbers of n-grams 
1  even with unlimited text  such an approach is not feasi ble because of practical limitations in terms of memory and hardware speed 
1 available from the acl data collection initiative as cd 
rom 1 


	hatzivassiloglou and knight 	1 

lattice at each state  we extend all word sequences ending at the predecessors the the current state  recompute their scores  and prune the search space by keeping only a prespecified number of sequences  specified by the width of the global search beam in practice  we have found that a beam of 1 hypotheses per node gives accurate results at reasonable search speed the sequences are stored compactly via pointers to the preceding states  along with information about the specific arc taken at each step   and maintained in a fast priority queue to avoid sorting this allows us to simulate an hmm of anv order  as well as trace any number of final sentences  up to the beam width  when the final state of the lattice is reached the complexity of the search algorithm is slightly superhnear in terms of the beam width  the number of slates  the n-gram length used in the model  and the average fan-out in the lattice  number of arcs 
leaving each state  
1 	results 
the glosser is currently being used in our machine translation system as a fall-back component in cases of parsing or semantic transfer failures we participated in the most recent  september 1  arpa evaluation of machine translation systems  see  white and o'connell  1  for a discussion of the evaluation methodology employed  with promising results a sample of translations produced by the glossing module is given below  in the form of japanese input followed by the correct translation and the translation given by the glosser due to space limitations  we are showing output on small example sentences  although japangloss typically operates on much longer sentences characteristic of newspaper text 

he has the contrary to violence 
　all the above translations have been obtained with the bigram language model  as heavy computational and storage demands have delayed the deployment of the more precise trigram model we expect higher quality output when the ingram model becomes fully operational 
1 	related work and discussion 
the glosser described in this paper is a type of transfer 
mt  and it follows in the tradition of syntax-based mt systems like systran however  our use of statistics allowed us to avoid much of the traditional hand-coding and to produce a competitive mt system in nine months 
other statistical approaches to mt include candidf  brown el ai 1   which docs not do a syntactic analysis of the source text  and lingstat  yamron tt al  
1   which does probabilistic parsing both ling stat and japangloss require syntax because they translate between languages with radically different word orders 
　our use of features in syntax  glossing  and semantics gives us the flexibility to correct translation errors capture generalizations  and rapidly build up a complete mt system as the features become more abstract the analysis deepens  and our translations improve-this is knowledge-based work improvement will also come from better statistical modeling our future work will be directed at finding these improvements  and at studying the interaction between knowledge bases and statistics 
a c k n o w l e d g m e n t s 
vve would like to thank  volanda gil  kenji yamada  and 

1 	natural language 

the 1jca1 reviewers for helpful comments on a draft of this paper 
r e f e r e n c e s 
 bahl et at  1  l r bahl  f jelinek  and r l mer cer a maximum likelihood approach to continuous speech recognition ieee transactions on pattern analysis and machine intelligence pami 1   1 
 brown et at  1  p f b rown  s a della-pietra v j delia-pietra  and r l mercer the mathematics of statistical machine translation parameter estimation computational linguistics  1   june 
1 
 chow and schwartz 1  y chow and r schwartz the n-best algorithm an efficient search procedure for finding top n sentence hypotheses in proc darpa speech and natural language work shop  pages 1  1 
 church and gale  1  k w church and w a gale a comparison of the enhanced good-turing and deleted estimation methods for estimating probabilities of english bigrams computer speech and language  1  1 

 dowty tt al  1  d r dowty  r wall  and s peters introduction to montague bemanhcs reidel dordrecht  1 
 good  1  i j good the population frequencies of species and the estimation of population parameters biometnka  1  1 
 jelinek and mercer  1  f jehnek and r l mercer interpolated estimation of markov source parameters from sparse data in pattern recognition in practice north-holland  amsterdam  1 
 jelinek et al  1  f jelinek  l r bahl and r l mercer design of a linguistic statistical decoder for the recognition of continuous speech ieee transactions on information theory 1 j   1 
 kaplan and bresnan  l1  h m kaplan and j bresnan lexical-functional grammar a formal system for grammatical representation in the mental representation of grammatical relation1  mit press  cambridge  ma  1 
 knight jid chandcr  1  k knight  and 1 chander automated postediting of documents in proc aa al 
1 
 knight and hatzivassiloglou  1  k knight and v halzivassiloglou two-level  many-paths genera lion in proc acl 1 
 knight and luk  1  k knighl and s k i uk building a urge-scale knowledge base for machine translation in proc aaaf 1 
 knight et al 	1  k knight i chandu- m haines 
v halzivdisiloglou h iio y m hda  s k luk  a okumura r whitnej and k  amada integrating knowledge bases and statistics in mt in proc conference of the association for machine jransla hon in the americas  am1a  1 
 knight et al   1  k knight  i   handtr m ii aines  v hatzivassiloglou  e how  m iida  s k luk  
r whitney  and k yamada filling knowledge gaps m a broad-co erage mt system in proc ijcai  1 
 moore  1  r m oore umhcation-baicd semantic interpretation in proc acl  1 
 nguyen et al  1  l nguyen  r schwartz  y zhao and g zavahagkos is n-best dead1 in proc arpa human language technology workshop advanced research projects agenc   1 
 nmsu/crl et at  1j 
nmsu/crl  cmu/cmt  and usg/isi ihe pangloss mark iii machine translation system technical report cmu-cmt-1  carnegie mellon university  1 jointly issued by computing research laboratory  new mexico state university   center for machine translation  carnegie mellon university  information sciences institute  university of southern california  edited by s nirenburg 
 okumura and hovy  1  a okumura and e hovy ontology concept association using a bilingual dictionary in proc of the arpa human language technology workshop 1 
 penman  1  penman the penman documentation 
technical report  usc/lnformation sciences institute  1 
 shieber  1  s shieber an introduction to unification-based approaches to grammar university of chicago  1 also csti lecture notes series 
 viterbi  1  a j viterbi error bounds for convolution codes and an asymptotically optimal decoding algorithm ieee transactions on information theory 1-1 
 white and o connell 1  
j white and i o connell evaluation in the arpa machine translation program 1 methodology in 
proc arpa human language technology workshop 1 
  yamron et al   1  j yaniron j cant a dtnudls t dietzel  and   ito iht automatic componenl of the lincistal machine-aided translation system in proc arpa workshop on human language technology 1 
	hatzivassiloglou and knight 	1 
natural language 

natural language 

natural language 







natural language 

natural language 

natural language 









natural language 

natural language 







