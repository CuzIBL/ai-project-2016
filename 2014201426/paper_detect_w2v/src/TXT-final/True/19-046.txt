 
previous work in explanation-based learning has primarily focused on developing problem solvers that learn by observing solutions. however  learning from solutions is only one strategy for improving performance. this paper describes how the prodigy system uses explanation-based specialization to learn from a variety of phenomena  including solutions  failures  and goal-interactions. explicit target concepts describe these phenomena  and each target concept is associated with a strategy for dynamically improving the performance of the problem solver. explanations are formulated using a theory describing the domain and the prodigy problem solver. both the target concepts and the theory are declaratively specified and extensible.1 
1. introduction 
　recent research has demonstrated that explanation-based learning  ebl  is a viable method for acquiring search control knowledge  1 1 1 . almost all ebl problem solvers learn by analyzing why a solution succeeds in solving a problem. the result is a knowledge structure  such as a macro-operator or schema  that describes how to solve similar problems should they arise in the future. 
　however  explanation-based learning can also be used to learn why a problem-solving method failed. by explaining why a method failed to solve a problem or sub-problem  a system can learn when to avoid that method in the future  1 1 . in fact  ebl is a very general technique  and in principle an ebl system can learn from any phenomenon that it can explain. to do so  however  a system must know what phenomena - or target concepts - to explain and be able to formulate appropriate explanations . furthermore  the system must be able to modify its behavior appropriately on the basis of the learned information. 
　this paper describes how the prodigy system learns from multiple target concepts in order to increase the utility of the learning process. specifically  unlike previous ebl problem-solvers  prodigy learns from a variety of phenomena  including success  failure  and goal-interactions. each of these meta-level target concepts corresponds to a strategy for optimizing prodigy's effectiveness as a problem solver. explanations are formulated using an explicit theory describing the domain and the prodigy problem solver. both the set of target concepts and the theory are declarative 
     lthis research was sponsored in part by the defense advanced research prefects agency  dod   arpa order no. 1  monitored by the air force avionics laboratory under contract f1-k-1  in part by the office of naval research under contract n1-k-1. and in part by a gift from the hughes corporation. in addition  the first author was supported by a bell laboratories ph.d scholarship. the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies  either expressed or implied  of darpa  the air force office of scientific research or the us government. and extensible  therefore additional optimization strategies can be incorporated into the system when necessary. this paper also introduces an new ebl algorithm  explanation-based specialization  ebs   which constructs explanations by specializing proof schemas using information gained from a training example. ebs is particularly appropriate for problem solvers that learn from their own problem solving performance. 
1. the terminology of explanation-based learning 
　several recent papers  1 1  have contributed greatly to the emergence of a relatively standard terminology for describing explanation-based learning.  we use the generic term  explanationbased learning  to refer to deductive learning from single examples  as exemplified by  1 1 1 .  
　the target concept is a high-level description of a concept  i.e.  a class of instances .1 a training example is an instance of the target concept. using the domain theory  a set of axioms describing the domain  one can explain why the training example is an instance of the target concept. the explanation constitutes a proof that the training example satisfies the target concept1. by finding the weakest conditions under which the explanation holds  ebl will produce a learned description that is both a generalization of the training example  and a specialization of the target concept. typically  the purpose of the learning process is to produce a 
　description that can serve as an efficient recognizer for the target concept. 
　as an example  adapted from  1  consider the target concept  safe-to-stack x y   that is  object x is can be safely placed on object y without object y collapsing. a training example could be a demonstration that a particular book  'principles of ai   can be safely placed upon a particular table  coffee-table-1. let us suppose that our domain theory  shown below  contains assertions enabling us to prove that  principles of ai  is safe to stack on coffee-table-1 because all books are lighter than tables. from this proof we can conclude that that any book can be safely stacked on any table. thus the learned description is  and  is-book x   is-table y  . 
domain theory. 
 is-book principlesof-ai  
 safe-to-stack x y  if  or  lighter x y   not-fragile y   
 less-than w 1-lbs  if  and  is-book x   weight x w   
1. the prodigy system 
　prodigy is a learning apprentice  that acquires problem solving expertise by interacting with an expert  by carrying out experiments  and  as described in this paper  by analyzing problem solving traces  prodigy can be divided into four subsystems: 
 an advanced strips-like problem solver that provides a uniform control structure for searching with both inference rules and operators. the problem solver includes a simple reason-maintenance system  allows operators with conditional effects to be specified  and is capable of interleaving goals. the problem solver's search can be guided by domain-independent or domain-specific control rules. 

  an explanation-based learning facility  1  that can propose explanations about why a control decision was appropriate  and transform them into search control rules. 
  a learning-by-experimentation module  for refining incompletely or incorrectly specified domain knowledge. experimentation is triggered when plan execution monitoring detects a divergence between internal expectations and external expectations. 
  a user-interface that can participate in an apprentice-like dialogue  enabling the user to evaluate and modify the system's behavior. 
　except for the experimentation module  which is currently under development   prodigy has been fully implemented  and tested on several task domains including a machine shop scheduling domain and a 1-d robotics construction domain. for brevity of exposition the examples described in this paper will be taken from an extremely simple task domain  the familiar  blocks world   whose standard specification  1 is shown in figure 1. while the blocks world may seem contrived  solving apparently simple problems may involve searching hundreds of nodes. in other words  the domain is simple but as we will see  the formalization  i.e.  axiomization  of the domain is far from ideal. by acquiring search control knowledge  prodigy effectively modifies its behavior so that its performance reflects the inherent simplicity of the domain  rather than the deficiencies of the formalization. learning search control knowledge thus has the same effect as reformalization ~ important knowledge is brought to bear early  so that correct decisions can be made efficiently. 
pickup b 
precondition:  and  clear b   ontable b   armempty   
add:  holding b  
delete:  ontable b   clear b   armempty  
putdownb 
precondition:  holding b  
add:  ontable b   clear b   armempty  delete:  holding b  
stack b1 b1 
precondition:  and  holding b1   clear b1   
add:  on bl bl   clear bl   armempty  delete:  holding bl   clear b1  
unstack bl bl 
precondition:  and  on bl bl   clear bl   armempty   
add:  holding m   clear bl  
delete:  on bl bl   clear m   armempty  
figure 1: blocks world specification 
1. an example problem 
　a problem consists of a description of a start-state  and a set of goal states  as shown in figure 1. prodigy's description language  called pdl  is a logic-based language that includes conjunction  disjunction  negation  existential quantification  and universal quantification over sets.  variables are shown in lower case.  pdl is also used for describing the preconditions of operators and search control rules. 

by default  prodigy searches depth-first  attacking goals from left to right  unless search control rules indicates a more appropriate ordering. to solve a goal  the system selects an operator with a postcondition  a formula in the add or delete list  that matches the goal. if the preconditions of the operator are satisfied  then the operator can be applied  otherwise the system subgoals on any unsatisfied preconditions.  for simplicity  this paper describes only the basic capabilities of the system necessary to illustrate the examples. more sophisticated features  such as conditional effects  inference rules  functions  and non-linear planning are discussed in . readers unfamiliar with the basic operation of a strips-style system should see  1.  

figure 1: search tree 
　to solve the example problem  prodigy considers applying either pickup or unstack to achieve  holding b   as illustrated in figure 1. pickup is inappropriate  but this is only discovered during the course of solving the problem. in attempting to pickup b  prodigy is forced to subgoal on  clear b  and  ontable b   the unsatisfied preconditions of pickup. these are added to the goal-stack  and then prodigy attempts to solve  clear b . unstacking a from b achieves  clear b   but then a goal-stack cycle occurs in attempting to achieve  ontable b . prodigy then attempts other ways to  clear b  at node1  but these also result in failure.  the problem solver has no way of knowing that unstacking a from b did not cause the failure of node1. but as we will see  when learning is interleaved with problem-solving  prodigy backtracks more intelligently.  eventually backtracking to nodel  prodigy attempts unstacking b from c  which leads to a solution. 
　prodigy's performance in a given domain can be improved by the addition of search control rules. control rules constrain the search by mediating the four decisions prodigy makes during each problem solving cycle. first  the system must choose a node in the search tree to expand next - the default is depth-first expansion. each node consists of a goal-stack  a set of subgoals and a state describing the world. once a node has been chosen  one of the subgoals must be selected  and then an operator relevant to reducing this subgoal. finally  a set of bindings for the variables in that operator must be decided upon. 
　each of these four decisions can be affected by control rules  which indicate when to select  reject  or prefer a particular candidate  node  goal  operator or bindings . given a default set of 
	minton and carbonell 	1 
candidates  prodigy first applies the applicable selection rules to select a subset of the candidates. if no selection rules are applicable  all the candidates are selected. then rejection rules are executed to filter this set  and finally preference rules are used to find the 
heuristically best alternative .  if backtracking is necessary  the next most preferred is attempted  and so on.  
　consider the first control rule shown in figure 1. this rule is a operator selection rule; it is used in deciding which candidate operator to choose. notice that the description language pdl allows the use of meta-level predicates such as known. the formula operator op fails to solve goal g at node n. as with all concepts in the 
system  target concepts are represented by atomic formulas. each target concept is declaratively specified to the system  as illustrated by figure 1. a target concept specification includes a template for building search control rules  and may include other information such as heuristics for selecting training examples  as described in  1. 
 known node exp  is true if the expression exp matches the state at 	 current-goal node goal  target concept:  operator-fails op goal node  rule template:  reject operator op  if  and  current-node node  

node. therefore the rule asserts that if the current goal matches  holding x  and x is not on the table  then unstack is the appropriate operator. normally  since both pickup and unstack can achieve  holding x   both would be candidate operators. if this rule had been learned prior to our example problem  figure 1   prodigy would have solved the problem without having to waste time exploring node1 and its descendents. 
 select operator  unstack x y   if  and  current-node node  
 current-goal node  holding x   
 candidate-operator node  unstack x y    
 known node  not  ontable x       
 prefer goal  on x y  over  on w x   if  and  current-node node  
 candidate-goal node  on x y    
 candidate-goal node  on w x      
figure 1: two search control rules 
　the second control rule in figure 1  a goal preference rule  is used to decide which of the candidate goals at a node to attempt first. the rule states that if  on w x  and  on x y  are candidate goals  then it is preferred that  on x y  should be attempted before  on w x . this piece of control knowledge directs the problem solver to build stacks of blocks from the bottom up. since prodigy finds the most preferred goal from a set of candidates  this rule correctly handles stacks regardless of height  i.e.  the number of goals in the on chain . 
1. learning control rules: the explanation-based 
specialization method 
　prodigy's explanation-based learning module can either be invoked after the problem solver has finished  or learning can be interleaved with problem solving. in either case  the learning process begins by examining the a trace of the explored search tree  in a pre-order traversal  in order to pick out examples of prodigy's target concepts. search control rules are learned by explaining why a training example satisfies a target concept. there are currently four types of target concepts implemented in prodigy: 
1. succeeds: a control choice succeeds if it results in a solution. learning about successes results in preference rules. 
1. fails: a choice fails if there is no solution consistent with that choice. learning about failures results in rejection rules. 
1. sole-alternative: a choice is a sole-alternative if all other candidates fail. learning about sole alternatives results in selection rules. 
1. goal-interaction: a choice causes in a goal-interaction if it results in a situation where a goal must be re-achieved. learning about goal-interactions results in preference rules. 
　each type of target concept has four variants  one for each of the four control decisions  picking a node  goal  operator  and bindings . for example  the target concept  operator-fails op g n  is true if 
　1 preferences are transitive. if there is a cycle in the preference graph  then those preferences in the cyde are disregarded. a candidate is best  or  most preferred   if there is no candidate that is preferred over it. 
1 	knowledge acquisition 
 candidate-operator op node  
 operator-fails op goal node   
figure 1: target concept specification for operator-fails 
　an explanation describes why an example is a valid instance of a target concept  and corresponds to a proof. to construct such proofs  we require a theory describing the problem-solver  as well as a 
　theory that describes the task domain  such as the blocks world . therefore  prodigy contains a set of architecture-level axioms which serve as its theory of the problem solver. the domain theory is given by a set of domain-level axioms extracted from the domain operators by a pre-processor. together  these sets of axioms are referred to as proof-schemas. each proof-schema is a conditional which describes 
when a concept is true. appendix 1 illustrates the architecturelevel proof schemas relevant to the concept operatorsucceeds. these schemas state that an operator succeeds in solving a goal at a node if: 
  the operator is applicable and directly solves the goal  or 
  subgoaling occurs  creating a child node where the operator succeeds  or 
  another operator is applied to achieve a subgoal  creating a child node where the operator succeeds. 
domain-level schemas  appendix 1  indicate the available operators  and their effects and preconditions. 
to prove that a target concept is satisfied by an example  
prodigy specializes the target concept in accordance with the example. this process is equivalent to creating a proof tree by starting at the root  i.e.  the target concept  and incrementally expanding the leaves of the tree. to specialize a concept prodigy retrieves a proof schema that implies the concept and recursively specializes all the subconcepts in the schema  as described in figure 1. the process terminates when primitive concepts are 
encountered. the result of the specialization process is a description of the weakest premises of the explanation. 
　if a concept is described by more than one proof schema  as is operator-succeeds  then it is disjunctive. when specializing disjunctive concepts  prodigy uses the example to determine which schema gives the appropriate specialization1 specifically  we allow each concept to be associated with a discriminator function which examines the problem solving trace and selects a schema consistent with the training example. in this way  discriminator functions provide a mapping between the example and the explanation. 
　after the ebs process terminates  the resulting description is a specialization of the target concept. a search control rule is formed by retrieving the rule construction template  found in the target concept specification  and substituting the specialized description 
　1 there can be multiple training examples for a subconcept if the subconcept is within the scope of a universal quantifier. universally quantified statements take the form forall x such-that  p x    q x . the formula  p x  acts as a generator for values of x.  q x  can be any expression. the implementation treats universally quantified statements as if they were written in the logically equivalent normal form: 
forall x  or  not  p x   q x   . because  p x  is negated  it is not specialized.  however  if p generates a fixed set of values  it can typically be simplified out of the learned description using simplification axioms! .  ebs specializes q separately for each example of x  and returns the disjunction of these descriptions as the specialization of q 
a concept it represented by an atomic formula. to specialize a concept: 
  if the concept is primitive - there is no schema that implies the concept - then return the concept unchanged  otherwise  
  call the discriminator function associated with the concept to retrieve a schema consistent with the training example. each non-negated atomic formula in the schema is a subconcept. while there exists a subconcept in the schema that has not been specialized  do the following: 
  specialize the subconcept 
 uniquely rename variables in the specialized description to avoid name conflicts  
  substitute the specialized description for the subconcept in the schema and simplify. return the schema  now a fully specialized description of the concept . 
figure 1: the ebs algorithm 
for the target concept in the template. during subsequent problemsolving episodes  prodigy will maintain a utility estimate for the new rule  where utility is defined by the cumulative time cost of matching the rule versus the cumulative savings in search time provided by the rule. only rules that prove useful are retained in the set of active control rules. 
　let us return to our example problem  figure 1  to illustrate how the ebs algorithm works. as stated previously  the selection of unstack at nodel provides a training example for the target concept operator-succeeds. 
target concept:  operator-succeeds op goal node    
example*:  operat1r-1dccxed1  unstack b   holding b  nodal  
because operator-succeeds is a recursive definition  the success of unstack depends on its success at nodes 1 and 1 and eventual application at nodel 1. we shall illustrate the specialization process by  unrolling  the recursion from the bottom up  starting with the success of unstack at nodell. 
subconcept:  operator-soccxxdfl op goal node   
example: operator-succeeds  unstack b  holding b  nodell  
because unstack. directly solves  holding b  at nodell  
operator-succeeds is specialized by schema-si in appendix 1 - the schema consistent with the example. the system then recursively specializes the subconcepts as shown below: 
specialize  operator-succeeds op goal node  using scheme-1: 
 and  added-by-operator goal op  
 applicable op node  
specialize  added-by-operator goal op  using scheme-dl: 
 and  and  matches op  unstack x y  
 matches goal  holding x   
 applicable op node  
spaoialisa  applicable op node  using schema-d1: 
 and  and  matches op  unstack x y  
 matches goal  holding x   
 and  matches of  unstack u v   
 known node  and  clear u   on u v   armempty    
after some trivial simplifications  our result can be re-expressed as follows: 
 operator-succeeds op goal node  if 
 and  matches op  unstack x y   
 matches goal  holding x   
 known node  and  clear x   on x y   armempty    
this simply states that an operator succeeds in solving a goal at a node if the operator is unstack  the goal is to be holding an object  and the preconditions of unstack are known to be true at the 
node. though this result is not particularly useful by itself  it serves as a lemma in explaining why the application putdown at node1 enabled the application of unstack: 
sub-concept:  operator-succeeds op goal node    
example: operator-succeeds  unstack b  holding b  nodel1  
this time  operator-succeeds is specialized by schema-s1  the recursive 	definition 	of 	operator-succeeds  	because putdown was applied as a precursor to unstack  rather than directly solving the problem. then applicable is specialized as before  and the recursive specialization of operator-succeeds is accomplished using the result from nodell. finally  the concept  child-node-after-applying-op child-node pre-op node  is specialized and then simplified; doing so effectively regresses the constraints on child-node across the definition of putdown.  the relevant proof-schemas used for regression are shown in appendix 1 . the result states that unstack will succeed whenever the goal is to hold an object  and the preconditions of the sequence putdown w  unstack x y are satisfied: 
 operator-succeeds op goal node  if 
 and  matches op  unstack x y   
 matches goal  holding x   
 known node  and  clear x   on xy   holding w    
the specialization process continues in this manner  until finally the successful selection of unstack at nodel is explained. the resulting expression indicates that unstack is appropriate if the goal is to hold an object and the preconditions of the sequence unstack w x  putdown w  unstack x y are satisfied.1 that is  unstack is appropriate when the goal is to hold a block  there is a single block on top of the desired block  and the robot's arm is empty: 
 operator-succeed1 op goal node  if 
 and  matches op  unstack x y   
 matches goal  holding x   
 known node  and  on x y   on w x  
 clear w   armempty      
at this point  a control rule can be built from the rule construction template for operator-succeeds. however  the reader may have noticed that the learned description seems very specific. although we will not discuss here the process by which prodigy evaluates the utility of the rules it learns  it should be clear that the learned rule will be a relatively weak control rule. in the next section  we will see that a much better explanation of why unstack was appropriate can be found by by analyzing why the alternative choice  putdown  failed1. 
　this example was chosen for several reasons. first  it is similar to the useful-op example in lex1  as described in 1   and thus can be directly compared to previous work. furthermore  the example illustrates that learning from successful operator applications is not guaranteed to produce strong control rules. as we will see  the utility of explanation-based learning in a given task domain depends greatly on the choice of target concept and explanation. b notice that the arguments of the first unstack must be w and x in order to achieve the preconditions of the remainder of the sequence. otherwise unique variables would take the place of w and x. 
　1 a third possibility  not discussed in this paper  is to formulate an inductive proof explaining thai unstack succeeds no matter how many blocks are on top  1  1 . 
1. learning from failure 
　to learn from the failure of pickup at nodel in our example  figure 1   prodigy uses the target concept operator-fails. proof-schemas for specializing operator-fails are shown in appendix 1. these schemas state that an operator fails to achieve a goal at a node if: 
  the operator is rejected by a control rule  or 
  the operator is not relevant to the goal  or 
  the operator is not applicable and subgoaling fails  or 
  the operator is applicable  and all operator applications result in failure. 
the reader is cautioned that for expository clarity these schemas are a simplified subset of those used in the actual implementation. 
to explain why the selection of pickup at nodel failed  
prodigy must explain why the subtree rooted at node1 failed. as in the previous section  the analysis is recursive and corresponds to a 
pre-order traversal of the tree. we will describe the results of the ebs process in a bottom-up manner. first  attempting to putdown b at node1 failed because the goal  holding b  was already on the goal-stack  resulting in a goal-stack cycle. therefore  specialization of operator-fails at node1 proceeds as follows: 
target concept:  operator-fails op goal node  example:  operator-fails  putdown b   ontable b  node1  specialize  operator-fails op goal node  by schema-r1: 
 amd  added-by-operator goal op  
 is-precondition p op  
 known node  mot p   
 subgoaling-fails p node   
specialize  added-by-operator goal op  by schama-d1  specialize  ib-precondition p op  by 1ohana-d1  specialize  subgoaling-rails p node   by schema-r1: 
 amd  amd  matches op  putdown x   
 matches goal  omtable x    
 amd  matches op  putdown y   
 matches p  holding y     
 known node  mot p   
 on-goal-stack node p    
prodigy simplifies this result to the following expression  which states that putdown fails if  ontable x  is the goal and  holding x  is not true  but is on the goal-stack. 
 operator-fails op goal node  if 
 amd  matches op  putdown x   
 matches goal  ontable x   
 known node  mot  holding x    
 on-goal-1tack node  holding x   
this expression describes why the operator putdown failed at 
node1. because putdown was the only operator relevant to 
 ontable b  the goal at node1  the node failed. therefore the concept  node-fails node  specializes to the following  given  node-fails node1  as the training instance  and using the result shown above as a lemma: 
 mode-fails node  
 and  is-goal node  ontable x   
 known node  not  holdxmg x    
 on-goal-stack node  holdxmg x   
moving up the tree  we find that the description above also explains why node1 failed. in fact had the user asked prodigy to interleave learning with problem-solving  the inevitable failure of node1 would have been recognized without the necessity of exploring the rest of the subtree below node1. in effect  because prodigy reasons about its failures in the process of learning  dependency-directed backtracking  results when learning and problem solving are interleaved. 
1 	knowledge acquisition 
　at this point  prodigy can explain why the selection of pickup failed at nodel. because the precondition  on-table b  was not true  it became a goal at node1 and  holding b  was pushed down on the goal-stack  resulting in the failure described above. thus  after the appropriate specializations  ebs yields the following learned-description for operator-fails: 
 operator-fails op goal node  if 
 amd  matches op  pickup x   
 matches goal  holding x  
 known node  not  ontable x    
this description can then be converted into a control rule asserting that if the current goal is  holding x  and x is not on the table  then pickup should be rejected. however  in addition  an even higher-level rule can be learned by specializing the target concept op-is-sole-alternative  whose definition is shown in 
appendix 1   which says that an operator should be selected if all other operators will fail. the description shown above  stating why pick-up failed  is used as a lemma while specializing op-is-solealternative. thus  prodigy learns the first control rule shown in figure 1  stating that  unstack x y  should be selected if the goal is to be holding x  and x is not on the table.  selecting unstack is slightly more efficient than rejecting pickup  because  as described in section 1  prodigy first selects the appropriate alternatives  and then uses rejection rules to filter this set. if a single alternative is selected  it is not necessary to check the rejection rules.  
notice that this rule provides a much more general and more efficient description of why unstack was appropriate than the rule learned in the previous section. 
　as this example illustrated  learning why an alternative choice failed can be more useful than learning why a candidate succeeded . this is especially true in the blocks world  where one can succinctly state the reason why a choice was  stupid . learning from success and from failure are complementary optimization techniques whose relative utility varies from domain to domain. 
1. learning from goal interactions 
　an example of a goal interaction  occurs when prodigy attempts to solve  and  on a b  on b c   by first stacking a on b before stacking b on c  see figure 1 . of course  stacking a on b deletes  clear b   a prerequisite for stacking b on c. consequently  there is no way to achieve  on b c  without unstacking a from b. 
　in the general case  we say that a plan exhibits a goal interaction if there is a goal in the plan that has been negated by a previous step in the plan. there are two complementary forms in which a goal interaction may manifest itself. a protection violation occurs when an action undoes a previously achieved goal  requiring the goal to be re-achieved. a prerequisite xnolation occurs when an action negates a goal that arises later in the planning process. in our example  a prerequisite violation occurs when stacking a on b deletes  clear b .  furthermore  a protection violation will follow when picking up b deletes the goal  on a b . the two phenomenon can also occur independently.  
　goal interactions typically result in sub-optimal plans. some previous planning systems have included built-in mechanisms for avoiding goal interactions  1 1 ; prodigy improves on this by reasoning about interactions in order to learn control rules  in a 
　manner reminiscent of sussman's hacker .  since goal interactions can be unavoidable in some problems  and may even occasionally result in better plans  prodigy learns rules that express preferences  thereby enabling solutions to be found even when interactions are unavoidable. learning to avoid goal interactions is an optimization technique specifically designed for planning domains; in other types of domains  e.g.  theorem-proving  this technique may be irrelevant. 
　to explain why a decision caused a goal-interaction  prodigy must show that all paths in the search tree subsequent to that decision resulted in a protection violation  a prerequisite violation or a failure. shown below is an informal definition of the target concept goal-causes-interaction which describes how selecting a goal before another goal at a node1 can result in goal interaction: 
 goal-causes-interaction goall goal1 node  if 
forall paths in which goall is selected before goal1  either: 
- the path terminates in a failure  or 
- while achieving goal1  goall is negated  a protection violation   or 
- while achieving goal1 a subgoal arises that was negated in the process of achieving goall  a prerequisite violation . 
　figure 1 illustrates the relevant section of the search tree from our example. nodel and node1 represent the two goal orderings that prodigy attempts. as can be seen  it is not necessary to expand the entire tree to prove that a goal interaction occurs if  on a b  is attacked before  on b c . next to nodes 1 through 1 are shown the intermediate descriptions produced by ebs in explaining why the goal interaction occurred. the target concept and training example are: 
target concept: 
  goal-causes-interaction goall goal1 node  example: 
 goal-causes-interaction  on a b   on b c  nodel  
figure 1: search tree illustrating goal interaction 
the final result of the learning process is the goal preference rule shown in figure 1 indicating that  on x y  should be attacked before  on w x  when both are candidate goals. after learning this rule  prodigy will always work from the bottom up whenever the goal is to produce an on chain. 
　the key point in the learning process occurs at node1  where ebs reveals that attacking  on vl v1  before  on v1 v1  results in an interaction only if v1 equals v1. this equivalence is a necessary part of the proof because achieving  on vl v1  must delete  clear v1  for the prerequisite violation to occur. the fact that v1 and v1 were incidently bound to the same constant in the example is not the reason for the equivalence. 
1. discussion 
1. explanation-based specialization 
　the most obvious difference between prodigy's ebs method and previous explanation-based learning algorithms  such as cbg  ebg   eggs  and the strips macrops 
algorithm   is that ebs is a specialization-based method rather than a generalization-based method. although this is primarily an algorithmic distinction  it demonstrates how the appropriateness of an ebl algorithm depends on the environment in which learning takes place. as an example  consider the ebg method described by mitchell  keller  and kedar-cabelli. the first step of the ebg method is to create a fully instantiated explanation  in the form of a proof tree . in the second step  ebg takes the explanation and computes a general set of sufficient conditions under which the explanation structure holds. similarly  the cbg  eggs and strips methods are given an instantiated explanation  often a sequence of operators produced by a problem solver  and they compute sufficient conditions under which the explanation structure holds. thus these generalization-based methods all take a ground-level explanation and compute a generalization that is based on the structure of the explanation. there are two basic approaches to generating the explanation. either it is constructed by a black box as in ebg 
 perhaps using a theorem prover as demonstrated by mostow and bhatnagar  1   or more typically  an observed solution sequence serves as the explanation  as in strips. in the latter case the problem solving operators are the domain theory1 
　ebs  on the other hand  never bothers to create a ground-level explanation. instead  a generalized proof tree is expanded from the top down1. there are two factors that make this method appropriate. first  there is no ground-level explanation that is immediately observable  as in strips. although the problem solving trace is available  it does not constitute a useful explanation; to explain failures  goal-interactions  etc.  an explanation must be constructed from the appropriate architecture-level and domainlevel axioms. secondly  mapping between from the problem solving trace to the explanation can be efficiently accomplished in a top down manner. the discrimination functions that augment the theory control the explanation process; by specifying the appropriate schema  i.e.  inference rules  with which to specialize each concept 
　1 these two schemes have been implemented in a variety of ways. for example  dejong and mooney  discuss a hybrid scheme in which the observed operator sequence is optional. if there is no observed operator sequence  it is constructed from the domain theory. in either case  a fully instantiated explanation is eventually produced. in the soar system  1  an observed sequence of production firings is the basis for learning. soar learns whenever a sequence of productions produces a result for a goal.  because learning is implemented on the production level and a result is obtained independent of the goal's failure or success  soar is able to learn from certain forms of failure as well as success.  

 the proof schemas for goal-interactions are similar to the proof schemas for 	1
failures. a goal interaction can be regarded as a  soft  failure. as with failure  the 	alao  note that in ebs  unlike ebg  the learned description represents the weakest wrong choice of a node  goal  operator  or bindings can result in a goal interaction. sufficient conditions under which the proof holds  1 . 

they provide all the information necessary to directly find the preconditions of the generalized proof. therefore prodigy simply specializes the target concept top-down to produce the learned description  instead of first constructing the instantiated proof and then regressing the general conditions through the proof structure. in fact  the learned description produced by ebs is more general than the training example only because ebs terminates before producing the ground-level explanation. the bias in ebs  i.e.  the factor determining the generality of the resulting description  1 1  comes from the proof schemas  and in particular  is determined by the disjunctive and primitive concepts used. 
1. the utility of multiple optimization strategies 
　in this paper  we have described how explanation-based learning can serve as a general method for implementing optimization strategies to improve problem solving performance. in particular  we have illustrated four strategies for dynamically improving the performance of the prodigy system; each strategy corresponds to a target concept that can be explained  and therefore  be learned. in the future  more target concepts can be added to prodigy by augmenting the theory as necessary. 
　the four optimization strategies illustrated in this paper are completely general  as they are applicable in any domain specified to prodigy. a better question concerns the utility of the techniques. obviously  prodigy can always learn from observing solutions  as do other ebl systems. however  using additional target concepts gives the system a range of options that can result in better performance  as illustrated by our examples. in general  the utility of a particular target concept depends greatly on the task domain and its formalization. in a sense  any optimization technique can be regarded as a strategy for recovering from suboptimal formalizations - those that are epistemologically adequate but procedurally inadequate . the need for multiple strategies is suggested by both theoretical and practical work on program optimization  1 . 
1. acknowledgements 
we gratefully acknowledge the assistance of craig knoblock  dan 
kuokka and henrik nordin in designing and implementing the 
prodigy system. ideas and suggestions by jerry dejong  oren 
etzioni  yolanda gil  smadar kedar-cabelli  rich keller  sridhar 
mahadevan  tom mitchell  ray mooney  jack mostow  and prasad tadepalli greatly influenced our work. 
i. proof schemas 
　the following proof schemas represent a very simplified subset of the actual schemas used in the prodigy implementation. we have assumed  for instance  that the preconditions of operators are simple existentially quantified conjunctions  that there can be not negated goals  and that an operator can have at most one formula in its addlist relevant to a goal. a more complete description of the actual schemas used in prodigy can be found in . 
1. architecture-level schemas for operator-
succeeds 
schema-si: an operator succeeds if it directly solves the problem. 
 operator-succeeds op god node  if 
 amd  added-by-operator goal op   applicable op node   
1 	knowledge acquisition 
schema-s1: for an operator to succeed  another operator may first be required. 
 operator-socceeds op goal node  if 
 amd  applicable pre-op node    
 operator-succeeds op goal child-node  
 child-mode-after-applying-op chad-node pre-op node    
schema-s1: for an operator to succeed  subgoaling may be necessary.  sub 
goaling creates the node where pre-op applies. see schema-s1.  
 operator-succeeds op goal node  if 
 amd  child-node-after-subgoaling child-node pre-op node     operator-succeeds op goal child-node  
1. some domain-level schemas for the blocks world 
schema-dl: a postcondition of  unstack bl b1  is  holding b1  
 added-by-operator goal op  if 
 amd  matches op  unstack bl bl   
 hatches goal  holding bl    
schema-d1: a postcondition of  putdown b  is  ontable b  
 added-by-operator goal op  if 
 amd  matches op  putdown b   
 matches goal  ontable b     
schema-d1: a precondition of  putdown b  is  holding b  
 is-precondition subgoal op  if 
 amd  matches op  putdown b   
 matches subgoal  holding b    
schema-d1: unstack is applicable if its preconditions are established. 
 applicable op node  if 
 amd  matches op  unstack bl b1   
'  known node  amd  clear bl   on bl b1   armempty     
1. architecture-level axioms for computing regressions 
schema-rl: computes the constraints on node when child-node results from applying op. note that child-node and op must be bound when this schema is used  otherwise the result may be incorrect. 
 child-node-after-applyino-op chad-node op node  if 
 forall x such-that  known x child-node  
 regres1-across-op-application node child-node x op    
schema-r1: the following two schemas represent frame axioms. all literals that hold at the child node were either added by the operator  or hold at the parent node  and don't match any member of the operator's delete list . 
 regress-across-op-application parent-node child-node lit op  if 
 amd  known child-node lit  
 in-add-list add op  
 matches add lit   
schema-r1: 
 regress-across-op-applicati1n parent-node child-node lit op  if 
 amd  known x parent-node   
 forall del 1uch-that  in-delete-li1t del op  
 mot  matches dd lit    
schema-r1: these 1 schemas regress facts when subgoaling. called by schema-f1. 
 regress-across-subgoal-action node subgoal formula  if 
 known node formula  
schema-r1: 
 regres1-across-subooal-action node subgoal formula  if 
 amd  matches formula  1n-goal-stack child-node god  
 i1-goal node god    
schema-r1: 
 regre1s-across-subgoal-action node subgod formula  if 
 matches formula  is-goal child-node subgoal   


