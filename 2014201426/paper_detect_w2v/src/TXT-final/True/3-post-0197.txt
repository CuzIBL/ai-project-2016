 
the textual entailment problem is to determine if a given text entails a given hypothesis. this paper describes first a general generative probabilistic setting for textual entailment. we then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. this problem is recast as one of text categorization in which the classes are the vocabulary words. we make novel use of na ve bayes to model the problem in an entirely unsupervised fashion. empirical tests suggest that the method is effective and compares favorably with state-of-theart heuristic scoring approaches.  
textual entailment 
many natural language processing  nlp  applications need to recognize when the meaning of one text can be expressed by  or inferred from  another text. information retrieval  ir   question answering  qa   information extraction  ie  and text summarization are examples of applications that need to assess such semantic overlap between text segments. textual entailment recognition has recently been proposed as an application independent task to capture such semantic inferences and variability  dagan et al.  1 . a text t textually entails a hypothesis h if t implies the truth of h. textual entailment captures generically a broad range of inferences that are relevant for multiple applications. for example  a qa system has to identify texts that entail the expected answer. given the question  where was harry reasoner born    a text that includes the sentence  harry reasoner's birthplace is iowa  entails the expected answer form  harry reasoner was born in iowa.  in many cases  though  entailment inference is uncertain and has a probabilistic nature. for example  a text that includes the sentence  harry reasoner is returning to his iowa hometown to get married.  does not deterministically entail the above answer form. yet  it is clear that it does add substantial information about the correctness of the hypothesized assertion.  
                                                 
 
 
a probabilistic setting  
we propose a general generative probabilistic setting for textual entailment. we assume that a language source generates texts within the context of some state of affairs. thus  texts are generated along with hidden truth assignments to hypotheses. we define two types of events over the corresponding probability space: 
i  for a hypothesis h  we denote as trh the random variable whose value is the truth value assigned to h in the world of the generated text. correspondingly  trh=1 is the event of h being assigned a truth value of 1  true . 
ii  for a text t  we use t to denote also the event that the generated text is t. 
textual entailment relationship: we say that t probabilistically entails h  denoted as t h  if t increases the likelihood of h being true  that is if p trh = 1| t    p trh = 1 . entailment confidence: we quantify the marginal amount of information contributed by the text to assessing the truth of the hypothesis relative to its prior with the pointwise mutual information: i trh=1 t =log p trh = 1| t  / p trh = 1  . 
an unsupervised lexical model  
the proposed setting above provides the necessary grounding for probabilistic modeling of textual entailment. however  it is important to bear in mind that it is not trivial to estimate the constituent probabilities in the definition of textual entailment since the truth assignments of hypotheses for the corpus' texts are not observed.  
lexical entailment as text classification 
as modeling the full extent of the textual entailment problem is a long term research goal  we focus on a sub task we term lexical entailment - recognizing if the individual lexical concepts in a hypothesis are entailed from a given text. 
　when estimating the entailment probability we assume that the truth probability of a term in a hypothesis h is independent of the truth of the other terms in h  obtaining: 
	p trp trhh = 1|  = 1  = t  = u uhp trhp tru=1 u=1 |t  	 1  
at this point  it is perhaps best to think of the entailment problem as a text classification task in which the classes are an abstract binary notion of lexical truth  for the different words in the vocabulary . first  we construct the initial labeling based solely on the explicit presence or absence of each u in t. then we apply na ve bayes in an unsupervised fashion that derives analytically from the defined probabilistic setting. 
initial labeling 
as an initial approximation  we assume that for any document in the corpus the truth value corresponding to a term u is determined by the explicit presence or absence of u in that document.  
　in some respects the initial labeling is similar to systems that perform a boolean search  with no expansion  on the keywords of a textual hypothesis in order to find candidate  entailing  texts. of course  due to the semantic variability of language  similar meanings could be expressed in different wordings  which is addressed in the subsequent model. the initial labeling  however  may provide useful estimates for this model.  
na ve bayes refinement  
following the standard na ve bayes assumption  we can rewrite the probability p tru=1|t  as in  1 . in this way we are able to estimate p tru=1|t  based on the prior p v| tru=1  and the lexical probabilities p v| tru=1  and p v| tru=1  for every u  v in the vocabulary v. these probabilities are easily estimated from the corpus given the initial model's estimate of truth assignments  assuming a multinomial event model for documents and laplace smoothing   mccallum and nigam  1  . 
	 tru	1 	v t	 v | tru	1 	 1 
	 tru	1| t 	
      tr c   v | tr c  c {1} u v t u
  from above equations we have a refined probability estimate for p trh=1| t  and p trh=1  for any arbitrary text t and hypothesis h. the criterion for turning probability estimates into classification decisions is derived analytically from our proposed probabilistic setting of textual entailment. we classify positively for entailment if p trh=1| t    p trh=1  and assign a confidence score of log p trh=1|t  / p trh=1   for ranking purposes. in fact  the empirical evaluation showed this analytic threshold to be almost optimal.  
1 empirical evaluation 
experimental setup 
though empirical modeling of semantic inferences between texts is commonly done within application settings  there is no common dataset available to specifically evaluate a textual entailment system. in order to test our model we therefore needed an appropriate set of text-hypothesis pairs. we chose the information seeking setting  common in applications such as qa and ir  in which a hypothesis is given and it is necessary to identify texts that entail it. the evaluation criterion is application-independent based on human judgment of textual entailment. 
　experiments were done on the reuters corpus volume 1. an annotator chose 1 sentential hypotheses from the corpus sentences. we required that the hypotheses convey a reasonable information need in such a way that they might correspond to potential questions  semantic queries or ie relations. we created a set of candidate entailing texts for the given set of test hypotheses  by following common practice of morphological and wordnet-based expansion.  boolean search  with expanded words  was performed at the paragraph level over the full reuters corpus. paragraphs containing all original words of the hypothesis or their morphological derivations were excluded from the result set and selected a random set of 1 texts for each of the hypotheses.  
　the resulting dataset was given to two judges to be annotated for entailment. corresponding to the notion of textual entailment  judges were asked to annotate a text-hypothesis pair as true if  given the text  they could infer with some confidence that the hypothesis is true. they were instructed to annotate the example as false if either they believed the hypothesis to be false given the text or if the text is unrelated to the hypothesis. a subset of 1 pairs was crossannotated for agreement  resulting in a moderate kappa statistic of 1. overall  the annotators deemed 1% of the text-hypothesis pairs as positive examples of entailment. 
empirical results 
we trained our model on the reuters corpus  classified the text-hypothesis pairs of the dataset and compared the model's prediction with the human judgments. the resulting  macro  average accuracy was 1%. since our dataset did not include texts containing all content words of the hypotheses  the baseline model would have predicted none of the pairs to be correct  i.e. the text entails the hypothesis  yielding an average accuracy of only 1%.  
　we also compared our system's ranking ability. the entailment confidence score was used to rank the various texts of each hypothesis. the average confidence weighted score  cws  was measured for each hypothesis. the resulting cws macro average was 1 compared to average cws of 1 for random ordering. for further comparison  a state of the art idf semantic overlap measure  saggion et al.  1; monz and de rijke  1  achieved a score of 1. though our model performs just slightly better  the results are statistically significant at the 1 level. 
