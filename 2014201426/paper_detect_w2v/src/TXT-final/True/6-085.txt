 
hyperlink analysis is a successful approach to define algorithms which compute the relevance of a document on the basis of the citation graph. in this paper we propose a technique to learn the parameters of the page ranking model using a set of pages labeled as relevant or not relevant by a supervisor. in particular we describe a learning algorithm applied to a scheme similar to pagerank. the ranking algorithm is based on a probabilistic web surfer model and its parameters are optimized in order to increase the probability of the surfer to visit a page labeled as relevant and to reduce it for the pages labeled as not relevant. the experimental results show the effectiveness of the proposed technique in reorganizing the page ordering in the ranking list accordingly to the examples provided in the learning set. 
1 introduction 
pagerank  page et al  1  is the most popular algorithm to compute the page relevance in a collection of hyperlinked documents  like the web. this algorithm exploits the topology of the citation graph in order to determine the authority of each page using the intuitive idea that authoritative documents are linked by many authoritative documents. thus  the original algorithm computes a ranking which depends only on the graph connectivity without considering the page contents. however  focused versions of pagerank have been proposed in the literature  haveliwala  1; richardson and domingos  1; diligenti et al  1  in order to define more selective rankings for topic-specific search engines. these approaches allow a limited degree of adaptation related to the choice of the specific topic. a text classifier is trained using examples of pages on the topic of interest but no further level of learning is considered in the ranking models. in particular  the classifier is trained independently on the ranking algorithm. 
learning 　we propose a more general approach in which the learning algorithm is defined directly on the ranking model. in this case there is not a predefined topic  but a supervisor provides examples of pages to be promoted and pages which are considered as not relevant. thus  the learning algorithm is able to manage both positive examples  relevant pages  and negative examples  not relevant pages . the ranking function is based on the web surfer model  diligenti et al. 1 . the relevance of a page is defined by the probability of the surfer to visit the page when considering the stationary probability distribution of the markov chain which describes the surfer behavior. basically the learning algorithm computes the parameters which define the surfer model in order to reduce the probability of visiting a negative example and to increase the probability of being in a positive example. in order to reduce the number of free parameters  the surfer model is simplified by dividing the pages among a set of categories and assuming that the surfer behavior depends only on the category of the page where it is located. in particular  the set of categories does not need to be predefined but it is determined by a clustering algorithm based on the document contents. moreover  whereas the focused versions of the pagerank simply direct the web surfer to the most promising page  i.e. the page whose content is the most similar to the content of a target page   the proposed algorithm can exploit hierarchies of topics  allowing the surfer to follow complex paths  composed by many steps  leading to relevant pages. 
　the paper is organized as follows. in the next section the web surfer model is reviewed. then  in section 1 the learning algorithm is described. section 1 reports some experiments which show the effectiveness of the proposed learning algorithm. finally  in section 1 the conclusions are drawn. 
1 random walks on the web graph 
random walk theory has been widely used to compute the absolute relevance of a page in the web  page et al  1; lempel and moran  1 . the web is represented as a graph g  where each web page is a node and a link between two nodes represents a hyperlink between the associated pages. a common assumption is that the relevance ip of page p is represented by the probability of ending up in that page during a walk on this graph. 
　we consider the model of a web surfer who can perform one out of two atomic actions while visiting the web graph: jumping to a node of the graph  action j  or following a hyperlink from the current page  action i . in general  the action taken by the surfer will depend on the page contents and the links it contains. we can model the user's behavior by a set of probabilities which depend on the current page: 
1 

is the probability of following one hyperlink from 
page q  and 	is the probability of jumping from page 
q. these values must satisfy the normalization constraint  
　the two actions need to specify their targets. assuming that surfer behavior is time-invariant  then the targets are specified by the probability of jumping from page q to page p  and the probability of selecting a hy-
perlink from page q to page p. is not null only for the pages p linked directly by page 
being ch q  the set of the children of node q in the graph g. these sets of values must satisfy the probability normalization constraints 	and 
　the model considers a temporal sequence of actions performed by the surfer and it can be used to compute the probability that the surfer is located in page p at time  the probability distribution on the pages of the web is updated by taking into account the actions at time using the following equation 
figure 1: the context graph built by following the links backwards from an example page. 
   where  is the cluster which page q belongs to  is the number of pages in cluster  the parameter is the tendency of the surfer to follow a hyperlink from a is the set of the parents of node p. the relevance page in cluster to a page in cluster and the parameter is computed using the stationary distribution 
of the markov chain of equation  1 . this is a simplified 	is the tendency of the surfer to jump from a page version of the model proposed in  diligenti et al  1 . 	of cluster 	to a page of cluster 	the values 
1 	learning the page rank 
the random surfer model which is used to compute the page relevance scores depends on the values assigned to the proba-
bilities 	in most of the approaches proposed in the literature  these parameters are predefined or computed from some features describing the page p and/or the page q. for example  in the original pager-are initialized to 1  such that all links are equally likely to be followed. because of the model assumptions  the stronger is the tendency of the surfer to move to a cluster  the higher are the scores associated to the pages in that cluster. 
1 	the learning set 
we assume that a supervisor provides a set pages labeled as relevant and not relevant. these examples are used to define a specific view of the collection of documents contained in a ank scheme 	and 	search engine. in particular the supervisor provides: 
          a set of pages which should get a high rank; however  in the general case  it would be infeasible to estimate the parameters without any assumption to reduce their  a set of pages which should get a low rank. 
number. in particular  we assume that the pages are grouped into n clusters according to their content and that the surfer's behavior for any page p is dependent only on the cluster which p belongs to. in our experiments we used classical 
clustering techniques  e.g. k-means  on the bag-of-words representation of the pages  kobayashi and takeda  1 . by this hypothesis  the parameters of the model are computed as: in the first case we refer to  positive  or  good  pages  whereas in the latter case to  negative  or  bad  pages. 
　each example page is associated to its context graph  diligenti et al  1   representing how the page can be reached from the web. the graph is organized into layers as shown in figure 1. the example page is the only node in layer 1. then  each node in layer links only nodes in layer  thus  a page belongs to layer i if it is at least i links away 
 1  from the example page. the depth k of the context graph is defined as the number of layers in the graph including level 1. for a given example  the context graph can be built by backcrawling the web adding to layer  the pages which link the pages in layer i  up to a maximum level k. moreover  all the pages that are linked by at least a page of the graph are also considered. 
1 	learning 

　the learning set consists of the context graphs built from the examples in and is the set of all the pages contained in the learning set; is the union of the 
pages in the layer k of the context graphs of the positive  negative  examples. 
1 learning the transition parameters 
we define the set of transition probabilities  which represent the proba-
bilities of the surfer to select a hyperlink from a page in the cluster to a page in the cluster the rank of the pages in the set is increased  whereas the rank of the pages in is decreased if we increase the probability of the paths leading to pages in and we reduce the probability of paths leading to pages in . this effect can be obtained by maximizing the following cost function 
where is a discount factor which is used to reduce the impact on the cost function of the links too far from the target page. 
the probability  of moving one 
level in the context graph towards a good page  can be rewritten as 
reflects the surfer model assump-
tion that the links are selected depending only on the clusters similar relationships can be derived for the negative set. 
　the cost function  1  can be optimized by updating the parameters in using gradient ascent. the gradient with respect to the transition parameters can be 
computed using equation  1   yielding 
where we neglected that 	and 
actually depend on the transition parameters. 
since the model parameters are essentially a not normalized version of the transition parameters we can update them directly as  
where 1 is the learning rate. 
　once the model parameters are updated  a new score distribution can be computed  thus updating the estimate of the variables  using equation  1 . there-
fore  the function optimization and the score estimation can be iterated  using an em style algorithm  dempster et 1/.  1   till a termination criterion is satisfied. of the jump parameters. 

we start observing that  
learning 	1 

similar relationships can be derived for the negative set. 
　the cost function  1  is maximized by gradient ascent. the gradient can be computed using equation  1  and the equivalent equation for the negative set  yielding 
where we neglected the dependence of x q rameters. at each iteration  the parameters are updated using the direction of the gradient as  

being r; the learning rate  and then normalized to meet the probabilistic constraints 
1 	learning the surfer action bias 
the surfer action bias is represented by the value of   which is the probability of following a link from the cur-
rent page rather than jumping to another page  
 in order to favor the action leading to good pages  we maximize the following cost function which represents a weighted average of the probabilities of ending in the layer k of a positive context graph and not of a negative context graph  

 1  


		 1  
1 

where we assumed that all links out of a page are equally likely to be followed. the term  is the probability of jumping to the layer fc of a positive context graph from a page of cluster i  and can be computed as 
 1  
finally  can be computed at each iteration as in equation  1 . similar relationships hold for the negative set. 
　the gradient of the cost function  1  can be computed using equation  1  and the corresponding relationship for the negative set  yielding 
neglecting the dependence of 
i he parameters are updated by gradient ascent as 
where the learning rate must be chosen small enough in order to preserve the stochastic model of the surfer. 
　the adapted surfer model can re-surf the web graph yielding a new estimate of equation  1 . thus  the new gradient estimate can be used to update again the parameters and the procedure is iterated till the stop criterion is satisfied. 
1 experimental results 
in the following experiments  the learning algorithm was applied only to the transition parameters of the random surfer model. the experiments were performed on two datasets  each containing 1.1 documents  which were collected by focus crawling the web on the topics  wine  and  playstation   respectively. the documents were clustered using a hierarchical version of the k-means algorithm  fukunaga  1 . we considered different numbers of clusters generated by the k-means algorithm. in particular  in two different runs  we clustered the pages from the dataset on topic  playstation  into 1 and 1 sets  whereas the dataset on topic  wine  was clustered into 1 and 1 sets. 
　two topic-specific search engines were build creating the inverted lists containing the terms of all the documents. then  the rank of each page in the dataset was computed using the model described in section 1 by setting its parameters to reproduce the original pagerank ranking. we tested the quality of the ranking function by submitting a set of queries to this focused search engine. the documents matching the query were sorted by the scores assigned by the random surfer. by interacting with the search engine  wc found pages which were authorities on the specific topic but were  incorrectly  not inserted in the top positions of the result list. such pages were selected as examples of pages which should get a higher 
learning 


figure 1: plots of the values of the transition parameters for each cluster  resulting from the training of the surfer   a   wine  dataset.  b   playstation  dataset. 
rank  positive examples . on the other hand  we found pages having a high score which were not authorities on the topic. such pages were selected as negative examples. the learning algorithm  as described in section 1  was applied to the datasets  using the selected examples. in the experimental setup only a small set of examples  1  was typically labelled as positive or negative. 
1 	analysis of the effects of learning 
in a first group of experiments we aimed at demonstrating that the surfer model as learnt by the proposed training algorithm  could not be generated by a simpler  but less powerful  schema as the focused versions of pagerank  proposed in the literature. 
　in figure 1- a  1- b   we show the values of the transition parameters for each cluster  resulting from the training sessions for a surfer on the topic  wine  and  playstation   respectively. the learnt parameters represent a complex interaction of the resulting surfer with the page clusters. this interaction could not be expressed by a simple schema as  for example  the focused versions of pagerank proposed in the literature  richardson and domingos  1; diligenti et al.  1 . this demonstrates that the model  in order to learn the users' feedback  needed to exploit hierarchies of topics to model the complex path leading either to positive or negative examples. 
　in figure 1- a  and 1- b   we show for each cluster  the values of the probability of the surfer of being located in a page 
learning 


figure 1: plots of the probability of the surfer of being located in a page of a given cluster for the topic  wine . the probabilities are normalized with respect to the number of pages in the cluster   a  1 clusters   b  1 clusters. 
of that cluster. in particular we used the dataset on topic  wine  clustered into 1 and 1 groups  respectively. the probabilities arc normalized with respect to the number of pages in the cluster. the learning algorithm is able to increase the likelihood of the random surfer to visit pages belonging to a set of clusters  while decreasing its probability of visiting pages belonging to other clusters. 
　in figure 1  we report the rank of pages after the learning session with respect of their rank before learning. in particular  on the x axis the pages are sorted according to their page rank before the learning session. the closer a page to the origin  the more  relevant  the page is. on the y axis it is reported the corresponding page rank after the learning session. the plot shows the generalization capability of the learning algorithm  which is able to change the ranks of a significant percentage of pages  even if a small set of example was provided. 
1 	qualitative results 
after training the surfer and computing the scores for all pages in the dataset  we compared the rank before and after the training process. 
　figure 1 reports the pages which obtained the largest variation in their rank. the pages that obtained a negative rank variation were either topic-generic authoritative pages  e.g. www.netscapc.com  www.yahoo.com  or pages relevant for different topics than  wine   e.g.  www.forgottensoldier.org  . only the page  www.yahoo.com  was explicitly provided to the system as a negative example. on the other hand  the pages that obtained a positive rank variation were effectively authoritative pages on the topic  wine   e.g.  www-
.wine-searcher.com  or  webwinery.com  . among the most 
1 


figure 1: plot of the page ranks before  x axis  versus after learning  y axis . the closer a page is to the origin  the most  relevant  it is. 
ascending documents  only the page  www.winespectator.com  was explicitly provided to the system as a positive example  whereas all other pages were pushed up in the rank for the capability of the system to generalize from a small set of training data. 
1 conclusions 
in this paper we introduced a novel algorithm to learn the ranking function over a collection of web pages. the proposed framework allows us to tune a previously computed rank distribution  specifying which pages should get a higher or lower score. like in the original pagerank  we assume that the score of a page is proportional to the probability that a web surfer is visiting the page. the learning algorithm adapts the parameters of the surfer in order to increase the probability that the surfer is visiting a  good  page  while decreasing the probability that the surfer is visiting a  bad  page. the parameters are estimated from the context graphs  which compactly represent the topological context in which a web page is inserted. the learning algorithm is fast since only close ancestors of example pages are considered in the context graphs. thus  it can be applied to large scale repositories with little overhead  i.e. the ranking function must be computed more many times . further investigations are needed to compare the accuracy of the learnt ranking functions with respect to the focused versions of the pagerank and to exploit the complete learning algorithm which considers all the model parameters. 
