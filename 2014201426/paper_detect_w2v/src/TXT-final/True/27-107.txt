 
an overview is given  with new results  of mathematical models and algorithms for probabilistic logic  probabilistic entailment and various extensions. analytical and numerical solutions are considered  the former leading to automated generation of theorems in the theory of probabilities. ways to restore consistency and relationship with bayesian networks are also studied. 
1 	introduction 
numerous models and algorithms have been proposed for reasoning under uncertainty in knowledge-based systems. among these  models based on logic and the theory of probabilities are  after a period of relative disfavor  attracting much attention again. these models differ according to the independence assumptions made about the events or logical sentences under consideration and the amount of information requested from the expert or decision maker. at one extreme of the spectrum  well illustrated by the probabilistic logic and probabilistic entailment models of  nilsson  1   no independance assumptions are made and only the available information is used. moreover  this information may be vague  i.e.  expressed by probability intervals instead of precise values. in probabilistic logic  the probabilities of being true of m logical sentences are given. it is asked whether these probabilities are consistent or not. in probabilistic entailment an additional logical sentence is considered and it is asked to find best possible lower and upper bounds on its probability of being true. in both cases the joint probability distribution on the set of possible outcomes is only partially specified. at the other extreme of the spectrum  the bayesian network models  e.g.   pearl  1    lauritzen et ai  1   usually make strong independence assumptions and request sufficient information for the joint probability distribution to be entirely specified. when those requirements are satisfied the probability of any event may be computed  often in moderate time. moreover  evidence can be efficiently propagated through the network. attempts to combine both methodologies have been made by  van der gaag  1   van der gaag  1  and by  andersen 
et ai  1 . 
1 	reasoning under uncertainty 
   the purpose of this paper is to present an overview  with new results  of mathematical models and algorithms for probabilistic logic  probabilistic entailment and their extensions. motivation stems from the facts that both problems have a long history and are the object of research dispersed among several literatures. this explains recent overly pessimistic statements as to the possibility of solving large instances. after stating the problems mathematically  analytical solution is studied. it is shown that one can use fourier elimination or enumeration of vertices and extreme rays of polytopes. the latter approach leads to automated generation of theorems in the theory of probabilities. numerical solution of large instances is then discussed. the column generation approach of linear programming  combined with specialized nonlinear 1 programming techniques to solve auxiliary subproblems  computation of the most negative or positive reduced costs  leads to algorithms efficient in practice. extensions are then examined  i.e.  use of probability intervals  conditional probabilities  linear relations between probabilities and qualitative probabilities. moreover  we show that  i  restoration of consistency through minimal changes in the probability intervals can be handled by the same type of models and  1  elimination of inconsistency through minimal deletion of logical sentences can be solved by combining column generation with branch-and-bound. the bayesian logic model proposed by  andersen et a/.  1  is finally investigated: we show that while this model is one of nonlinear nonconvex programming  many cases to which it applies can in fact be expressed as linear programs. 



	hansen  etal 	1 


   while results such as the above are easily obtained by direct reasoning  automation becomes useful when more sentences are considered as the numbers of conditions and of terms in the bounds increase rapidly. 
1 	numerical solution of psat 
 psat  is np-hard  as it is in np and contains the nphard problem  sat  as a particular case   georgakopoulos et ai  1  . moreover  the problems  l - 1  and  l - 1  have a number of columns exponential in the size of the input when  as is usually the case  the size  or total number of variable occurrences  of the sentences si is bounded by a constant.  note that this restriction on size is natural  as otherwise reading the input would require time exponential in the number of variables . so writing  l - 1  or  l - 1  explicitly already requires exponential time. this has led  van der gaag  1   van der gaag  1  to surmise that solution of  psat  requires exponential time in general and not only in worst case.  in fact  many polynomial cases have been identified  see  georgakopoulos et ai  1    kavvadias et ai  
1    jaumard et ai  1  .  nilsson  1   nilsson  1  stresses less formally  but as strongly  the difficulty of solving instances of  psat  with many variables and suggests looking for heuristics.  frisch et at.  1  propose under the name of anytime deduction a heuristic approach to  psat  based on sequential application of rules giving smaller and smaller intervals. this has the advantage of allowing reasoning to be followed step by step but may not yield best possible bounds. however  the powerful column generation technique of linear programming  see  e.g.  chvatal  1   chapter 1  can be brought to bear. this was proposed by  zemel   1  for an application of  psat  to reliability  then for the general case by  georgakopoulos et ai  1   whose work is extended in  jaumard et a/.  1   and   hooker   1   see also  andersen et ai  1  . when solving a linear program by column generation a compact tableau is kept; at each iteration the entering column is found by solving a combinatorial subproblem and the tableau is updated following the rules of the revised simplex method. finding the column with minimum  maximum  reduced cost at a current iteration is equivalent to minimization  maximization  of 
		 1  
where the ui are the dual variables associated with constraints  1  and  1 . associating the values true with 1 and false with 1   1  may be rewritten 
 1  
which is a nonlinear expression in the variables xj with the operators v  a and -. these operators may be eliminated where x and y are logical variables. minimization  maximization  of the resulting nonlinear function in 1 variables can be done approximately by variable-depth search   kavvadias et ai  1   or tabu search   jaumard et ai  1   and exactly by an algebraic method   jaumard et ai  1    crama et ai  1   or by linearization   hooker   1    andersen et ai  1  . as an exact solution is only required when no more column with a reduced cost of adequate sign can be found heuristically  variable-depth and tabu search are useful even if one wants proved best possible bounds. heuristics will be used as long as possible and followed by a usually more time-consuming exact method. the column generation technique has led to solve large instances of  psat   with up to 1 variables and 1 sentences   jaumard et ai  1   in reasonable computing time. the number of columns generated is a very small proportion of the overall number in the instance  e.g.  about 1 columns for problems with 1 variables  and hence 1 1 columns  and 1 sentences . 
1 	extensions of psat 
in addition to uncertainty  expressed by probabilities  expert knowledge often suffers from vagueness. indeed giving a single value for the truth probability of a sentence is quite arbitrary in many situations. vagueness may be expressed in  psat  by using probability intervals  for the truth of sentences s  instead of single values. then the expert is not forced to provide more information than he has. generalizing  psat  in this way was already proposed by  hailperin  1 . constraints 
 1  are replaced by 
		 1  
the column generation technique for  psat  described above extends readily to this case  columns corresponding to slack or surplus variables being treated separately. the increase in computing time when replacing single probability values by intervals is moderate   jaumard et ai  1  . 
¡¡expert knowledge may also be precise in some situations only  which is expressed by using conditional probabilities i. such conditional probabilities can be integrated into  psat  in several ways. as 
prob   one can use  jaumard et ai  
1  the two constraints: 
where is true and 1 
otherwise  	is true for xk and 1 otherwise. a more compact expression  obtained by elimination of is: 

 1  
1 	reasoning under uncertainty 


	hansen  etal 	1 

1 	reasoning under uncertainty 

the optimal value is 1. 
¡¡clearly the number of sets of non-immediate predecessors of a node may be exponential. however  not all corresponding constraints need be written.  lauritzen et a/.  1  explain how to represent independence relations by an undirected graph g' in which all pairs of immediate predecessors are joined and edges are added until the graph is triangulated. then the joint probability distribution can be expressed as a product of marginal probability distributions on the maximal cliques of g'  adequately scaled.  van der gaag  1   van der gaag  1l  proposes to use this property in a decomposition method for  psat   discussed in a companion paper   douanya et al  1  . it is shown there that the usual  psat  model gives the same bounds as the decomposition-based version. consequently  psat  takes implicitly into account in the computation of the bounds the conditional independence constraints  1  involving variables which do not all belong to the same maximal clique. 
example  continued . a graph g' associated with the example after deletion of v1 and v1 is composed of triangles on v1 v1 v1 and on v1 v1 v1. this shows that when computing bounds on prob  x1ix1 x1 one neeed not take explicitly into account the constraints prob   i.e.  the four last ones listed above. 
¡¡ andersen c/ a/.  1  also explore cases where the number of independence constraints is limited. the main interest of bayesian logic is not  however  to propose an alternate method for the computation made in bayesian networks  but to consider more general assumptions. example  continued . assume as done by  andersen et a/.  1  that the atomic propositions x1 and x1 are not 
independent. then  1   prob x    1. replacing the line giving the marginal probability of a:1 by 

minimizing and maximizing yields bounds of 1 and 1. note that when using benders decomposition the computation of the lower bound required 1 iterations  i.e.  solution of 1  psat  and 1 signomial geometric programming problems. 
¡¡as discussed in  andersen et a/.  1   many other extensions of bayesian networks can be considered within the  psat  framework: one can replace single probability values by intervals  add constraints of different types than the conditional implications  allow for networks with cycles  etc. not all extensions will remain linear. for instance  if in the example  the marginal probabilities for x1 and x1 are replaced by intervals and the independence assumption is kept a quadratic con-
straint 

arises. the resulting quadratic programs can be solved in many ways using global optimization techniques. finding which are most efficient is an open problem. 
¡¡to conclude   psat  appears to be a flexible and computationnally tractable model for reasoning under uncertainty. it has already been extended in many ways  while remaining linear. further exploration of the problems which may be so expressed and of solution methods for the nonlinear case are attractive topics for future research. 
