 
　knowledge revision in incremental learning systems will usually be restricted by some external criteria to achieve a conservative behavior of the system. unfortunately  conservatism has some well known drawbacks. therefore  it can become necessary to drop these restrictions and to change over to a non-cumulative learning mode. in this paper the incremental learning system metaxa.1 is described 
which is able to perform a special kind of noncumulative knowledge revision enabling it to learn without requiring unrestricted resources or the absence of noisy data. the generalization approach is sketched and knowledge revision in metaxa.1 is 
described. 
i introduction and motivation 
　the appropriate generalization approach to be used in a learning system is determined by the learning task. the presence of noisy data  for example  requires the use of induction methods which are insensitive to noise  quinlan 1 . the learning task also places demands on the knowledge revision abilities of the learning system. 
　suppose a learning task description allows us to assume the availability of unrestricted resources for knowledge maintenance and knowledge revision  the existence of an 'oracle'  cf.  shapiro 1   or the possibility to make critical experiments  and the perfect reliability of incoming data. then  knowledge revision is  no  problem. unfortunately  the following scenario is possible as well: the learning task is such that the knowledge base of the learning system becomes contaminated with noisy data  'oracles' are not available  resources are restricted  and incremental learning is required. in addition  the learning task may be defined in a domain with counterexamples for all theories claiming a minimum of elegance. 
　in the latter case  difficulties will arise if the  no problem  solutions for generalization and knowledge revision of the first case are applied to acquire a domain theory: it will hardly be possible to find any theory at all  neither a 
* the work of the author is supported by the german ministry for research and technology  bmft  under contract itw1. 
1 	knowledge acquisition 
simple one which can be improved later  nor a complex one  because counterexamples will cause the refutation of these theories. this problem is well known and several researchers have proposed and/or used a heuristic called 'conservatism' to overcome this problem in their learning programs  cf. 
 salzberg 	1   	 emde/habel/rollinger 	1   
 rose/langley 1  . 
　conservatism includes both: first  to exclude uncommon counterexamples in the induction process and second  to choose the smallest changes to rectify a theory. it is advisibe to complete this strategy with another one known from philosophy and psychology: the system should look for confirmation rather than disconfirmation as it would follow from a logical point of view. confirmatory strategies together with conservatism constitute the 'confirmation bias'  cf.  tweny/doherty/myatt 1   of a learning system. 
　the use of a confirmation bias helps to develop a fully operational performance element in the early stages of induction. on the other hand confirmation bias also has many drawbacks  including convergence to local maxima. therefore  we assume that a learning system should have the possibility to choose between at least two learning modes: the  quasi- cumulative and the noncumulative learning mode. a learning system is said to be in the non-cumulative learning mode if the extent of modifications made by knowledge revision procedures is not restricted by conservatism. at the least  the learning system should leave the  quasi- cumulative learning mode if it has reached a local maxima or a domain theory which consists of a description of exceptions rather than a description of regularities. 
	in 	this paper 	the 	incremental learning system 
metaxa.1 is described which performs a specific kind of non-cumulative learning. in contrast to other approaches  non-cumulative learning in metaxa.1 relies neither on the simplicity of the learning task  'oracels' are available  backtracking is possible  ...   cf.  shapiro 1    rose/langley 1    nor it is simply a 'learning by scratch'  michalski 1  where the knowledge of the system is deleted and redeveloped all over again. this approach is influenced by the work of thomas kuhn and paul k. feyerabend and tries to take advantage of their analysis of scientific discovery processes  cf.  emde 1    tweny/doherty/myatt 
1  . 

ii the learning task 
　the learning task of metaxa.1 can informally be described as follows: data about properties of objects of a world and about relations among these objects are continuously presented to the system. the data may be 'noisy*. the system is supposed to be able to answer questions about the facts of the world after each input. a question should be answered with 'yes* if a corresponding input was accepted or if a corresponding fact can be deduced 
with inference rules induced from regularities found in the factual knowledge. furthermore  the system is supposed to use less resources than complete search in inductive and deductive processes would require. 

fig. 1: general architecture of metaxa.1 
　this learning task is illustrated with the general architecture of the system shown in figure 
1. 	a user-interface is provided by two functions 
 tell  and  ask . if an input  like 
    fl  heavier than block1 blockl   is consistent to the systems's knowledge it is stored in the factual knowledge base maintained by the inference engine with reasoning maintenance capabilities. after each input the induction process is triggered  which may cause the addition of one or more new inference rules  e.g. like rl  x wx y and wy are variables : 
 rl  weight x wx    weight y wy    
                 wx   wy -  heavier than x y   to the rule knowledge base of the inference engine. inference rules which have been added to the rule base will be used in further inductive or deductive processes. if an input is inconsistent to the knowledge of the system  it will be rejected and stored as noisy data. 
i l l the generalization approach 
- the set of already induced  meta-facts  describing the inferential knowledge of the system according to the rule-schema  e.g.: num comparative heavier than weight al1  num comparative longer than length all **  
- and meta-rules describing the semantic relations between higher concepts  e.g.: 
num comparative p q s  -  transitive p s  symmetrical p s  -  not num comparative p q s   
　the generalization process of metaxa.1 can roughly be described as follows: the process is triggered by each input to the inference engine. first  hypotheses about possible inference rules are generated. next  the hypotheses are ordered and finally tested using the corresponding characteristic situation  cs  schemata. if a  more than 'n'  usually 1  instances for the positive cs schema of a hypothesis can been found  b  no  or only a few  instances for negative cs schema  and c  only positive evidence for the corresponding meta-fact can be deduced with meta-rules ***  the hypothesis is confirmed. then  the rule is generated using the rule-schema and entered into the rule base. 
	iv 	non-cumulative knowledge revision 
	usually  	a learning system will 	learn in the 
 quasi- cumulative learning mode. some knowledge revision strategies which may be used in this mode have been described in  emde/habel/rollinger 1 . the non-cumulative learning mode should be entered either if a  crisis   as known from kuhn  takes place or if the quasi-cumulative learning mode has lead to the discovery of rules or concepts which 
might 	be used 	to 	restructure 	the 	system's knowledge. 
　the metaxa.1 system changes over to this learning mode if the experimental threshold of the maximum number of noisy data entries has been exceeded  a crisis  or if the system has induced a new rule with particular higher concepts. in both cases the system will then look for regularities in the factual knowledge which might be used to form a new theory. if a regularity  inconsistent to the old theory and factual knowledge  has been found metaxa.1 will try to develop a new theory and revise its knowledge according to this theory. in the following an example of the non-cumulative knowledge revision in metaxa.1 is given by a description of an actual run  see  emde 1  for more details . 

　the generalization approach of metaxa.1 is based on higher concepts like transitivity and conversity  cf.  emde/habel/rollinger 1  . the higher concept 'num erical -comparative*  e.g.  is defined to acquire rules like rule rl above. the definition of a higher concept like 'num comparative' includes - the rule-schema: 
numcomparati ve p q : 
q x nx    q y ny    nx   ny -  p x y  
 with q and p as predicate variables   
- a 	schema 	for 	 positive  	characteristic situations: 
q x nx  & q y ny  & nx   ny & p x y   
** the last argument of a meta-fact in metaxa.1 is used to specify the support set of the corresponding rule. a support set description not equal to 'all* will define a restriction to the applicability of the rule with regard to the arguments of the premises. 
*** when metaxa.1 is looking for counterexamples of a rule the search depth is more restricted than in the search for positive instances. thus  metaxa.1 is using a confirmatory strategy. 
	emde 	1 
　metaxa.1 induced a set of inference rules from a factual description of a world of floating and nonfloating objects which can be used to answer questions about whether or not a particular object is floatable- the system's first  theory  sight be summarized as follows: small objects are light; big objects are heavy; the weight of objects determines whether an object is heavy; light objects are able to float. 
　with the above theory the input about a nonfloating needle 'needle1* has been rejected because it was described as small object. on the other hand the noisy input about a floating 'cablel' has been accepted because it was described as small. 
　the system then was supplied with facts about the materials of several objects. this lead to the generalization of a rule  r1  which uses the specific weight of materials to infer the volume of objects. the rule is also represented declaratively as met a-fact  mf1  using the name of the corresponding higher concept. 
 r1  weight x wx    volume x vx    z is wx/vx   material x mxy    material y mxy    weight y wy    
	vy is wy/z 	-  volume y vy  
 mf1  const ratio material weight volume all  
　metaxa.1 interprets this generalization as an interesting new rule which might be used to revise the system's knowledge because the following heuristic  cp. figure 1  will fire: 
 hi  const ratio 1 p q s  -  eval treat  try shift thresh ratio p q     all    **** 
　this heuristic might be interpreted as:  if constant ratios have been found then try to develop a new theory with a 'thresh-ratio' meta-fact as its core hypothesis  i.e.  look for an extreme value of all ratios  as an implicit new concept  like e.g.  'specific weight'  to revise the old theory . after some search which is caused by unbound variables in hi mbtaxa.1 finds a regularity described by the meta-fact mf1 and can be read as:  if the ratio of weight to volume of an object is smaller than this ratio for ice-objects then this object will float. this rule is inconsistent to the system's first theory and would not be introduced in the quasi-cumulative learning mode. 
 mf1  thresh-ratio weight volume  is ice object floating all  
　mf1 and the mete-facts derivable from it by using meta-rules are then added to this meta-fact forming the core of the new theory. bach meta-fact of the old theory is classified according to whether or not it is consistent with the core of the new theory  once again by applying meta-rules . in the next step  a part of the factual knowledge is reclassified: facts probably classified as  not noisy  by deleted rules  e.g.  the input about the floating 'cablel'  and facts that might be 
**** conclusions with 'eval' as main operator are evaluated by the inference engine rather stored as fact. the program 'treat* adds a task to the agenda. 
1 	knowledge acquisition 
classified as 'noisy' by now rules are classified once more. furthermore  the 'noisy data' entries are classified once more to rehabilitate data which have erronously been classified as 'noisy'. then  mbtaxa.1 returns to the non-cumulative learning mode to work out the new theory. 
v discussion and acknowledgements 
　in this paper the non-cumulative knowledge revision on mbtaxa.1 has been described. in contrast to the 'learning by scratch' approach which is another  simple  kind of non-cumulative knowledge revision  mbtaxa.1 takes advantage of parts of the old knowledge in two different ways: first  the old knowledge is applied to develop the core of the next theory and second  the old knowledge consistent to the new theory is incorporated into the new theory. knowledge revision in mbtaxa.1 takes place on the meta-level without requiring complete bookkeeping as it is necessary  e.g. in the  quasi- cumulative discovery program stahlp  rose/langley 1 . in contrast to unimbm  lebowitz 1  and other programs metaxa.1 applies rules generalized at one stage in subsequent learning stages. many questions remain unanswered  such as how different theories can be compared and which heuristics can be used to estimate the usefulness of a theory-shift proposed by a heuristic in advance. 
　i would like to thank all members of the project group kit  in particular k. v. luck  k. morik  b. nebel  ch. peltason  s. thieme and st. wrobel  and the referees for helpful discussions and comments. 
