 
state abstraction is of central importance in remforcement learning and markov decision processes. this paper studies the case of variable resolution state abstraction for continuous-state  deterministic dynamic control problems in which near-optimal policies are required. we describe variable resolution policy and value function representations based on kuhn triangulations embedded in a kdtree. we then consider top-down approaches to choosing which cells to split in order to generate improved policies. we begin with local approaches based on value function properties and policy properties that use only features of individual cells in making splitting choices. later  by introducing two new non-local measures  influence and variance  we derive a splitting criterion that allows one cell to efficiently take into account its impact on other cells when deciding whether to split. we evaluate the performance of a variety of splitting criteria on many benchmark problems  published on the web   paying careful attention to their number-ofcells versus closeness-to-optimality tradeoff curves. 
1 	introduction 
this paper is about non-uniform discretization of state spaces when finding controllers for continuous optimal control problems. uniform discretizations suffer from impractical computational requirements when the size of the discretization step is small. in this paper we try to keep the convergence properties of the discretization methods while introducing a variable resolution approximation. here  we only consider the  general towards specific  approach : an initial coarse grid is successively refined at some areas of the state space by using a splitting process  until some desired measure of accuracy is reached. 
　we consider discounted deterministic control problems  which include the well-known reinforcement learning  rl  benchmarks of  car on the hill   moore  1   cart-pole  barto et al.  1  and acrobot  sutton  
1a . be the state of the system whose evolution is described by the controlled differential equation : 
		 1  
the objective of the control problem is to find  for any  1  
the 
boundary reinforcement  obtained at the boundary of the state-space    the discount factor  and  the exit time from x. the value function  vf   max-
imal value of the gain  is : 

it is known  see  fleming and soner  1   that v satisfies a first-order non-linear differential equation  called the hamilton-jacobi-bellman  hjb  equation : 
		 1  
with dv being the gradient of v. 
　section 1 introduces the discretization process ; section 1 describes several local splitting criteria  based on the value function and on the policy ; and section 1 proposes a heuristic for a global splitting criterion. 
1 	the discretization process 
the state-space is discretized into a variable resolution grid using a structure of a kd-tree. the root of the tree covers the whole state space  supposed to be a  hyper  rectangle. each node  except for the leaf nodes  splits in some direction the rectangle it covers at its middle into two nodes of half area. for each leaf  we use a kuhn triangulation to linearly interpolate inside the rectangle  see the triangulation of figure 1 . see  munos and moore  1  for more details. this defines a class of functions known as barycentric interpolators  munos and moore  1  which are piecewise linear  continuous inside each rectangle  but may be discontinuous at the boundary between two rectangles. 

1 	uncertainty and probabilistic reasoning 

　the discretization process is baaed on the finiteelement methods of  kushner and dupuis  1   extended to rl in  munos  1   which approximate  for any given discretization of the state space  the continuous deterministic control process by a markov decision process  mdp . note that the stochastic aspect of the 
mdp does not come from the continuous problem itself 
 which is deterministic here   but from the discretization process used. 
figure 1: an exam-
ple of discretization of the state-space. values are stored at the corners and the value for a 
non-corner point is linearly interpolated from the simplex of which it is a member. this is a 
fast process above 1 dimensions. 
1 	building the discretized m d p 
for a given discretization  we build the corresponding 
mdp in the following way. the state-space is the set of corners of the tree. for every corner  and control we approximate a part of the corresponding trajec-
tory by integrating the state dynamics  1  from initial state for a constant control  during some time until it enters inside a new simplex at some point  see figure 1 . at the same time  we also compute the integral of the current reinforcement : 
　　then we compute the vertices of the simplex containing and the corresponding barycentric coordinates 	 which  by definition  satisfy :  
the interpolated value at is thus just a linear combination of the values at the vertices of the simplex it belongs to  with positive coefficients that sum to one. doing this interpolation is mathematically equivalent to probabilistically jumping to a vertex. thus  we define the probabilities of transition of the mdp from state  and control u to states as these barycentric coordinates : and the dynamic programming  dp  equation corresponding to this mdp is : 
  1  
1 	t h e   c a r on the hill' example 
this problem  which will be made available on the web  is of 1 dimensions : position and velocity of the car. here  the current reinforcement is zero everywhere. the terminal reinforcement if the car exits from the left side of the state-space  and varies linearly between +1 and -1 depending on the velocity of the car when it exits from the right side of the state-space. the best reinforcement 1 occurs when the car reaches the right boundary with a null velocity  see figures 1 and 1 . the control u has only 1 possible values : maximal positive or negative thrust. because of the discount factor  we are encouraged to get to the goal as quickly as possible. 

 figure 1: the value function of the car-on-hill. frontier 1 shows the discontinuity of the vf. frontiers 1 and 1  the dash lines  stand where there is a change in the optimal control. 
　figure 1 shows the value function of this problem. the discontinuity along frontier 1 happens because a point beginning just above the frontier can eventually get a positive reward whereas any point below is doomed to exit on the left. note that there is no change in the optimal control around this frontier. there is a discontinuity in the gradient of the vf along the upper part of frontier 1 and along frontier 1 because of a change in the optimal control. 
 figure 1: the optimal policy is represented by several gray levels  light gray for the positive control  dark gray for the negative one . several optimal tra-
jectories are drawn for different initial starting points. 
1 	t h e variable resolution approach 
the basic idea is to start with an initial coarse discretization  build the corresponding mdp  solve it in order to 
	munos and moore 	1 

have a  coarse  approximation of the value function ; then refine locally the discretization by splitting rectangles according to some  splitting criterion   build the new mdp  and so on  see the splitting process in figure 1   until we estimate that the approximation of the value function or the optimal control is precise enough. the central purpose of this paper is the study of several possible splitting criteria. 
 figure 1: several discretizations resulting of successive splitting operations. 
　figure 1: the discretization of the statespace for the car-onhill problem using the value non-linearity criterion with a splitting rate of 1% after 1 iterations. 

1 	local splitting criteria 
1 	first criterion : average corner-value difference 
for every hyper-rectangle in the tree we can ask the question  do the values on the left-side of the rectangle tend to have a significantly different value on average than those on the right-side  . the notion of left and right depend on which axis we are considering splitting on  and in fact we iterate over all axes  finding the one with the most significant difference  which we will call the  corner-value difference . 
　having computed this for all cells in the current tree  we must choose which cells to actually split. we select some fraction  e.g. 1%  of the cells with the highest criterion values and break them in half along their most significant splitting directions. figure 1 represents the discretization obtained using this criterion. 
　figure 1: the discretization of the statespace for the car-onhill problem obtained after 1 splitting iterations  starting with a 1 
by 1 initial grid and using the corner-value difference criterion with a splitting rate of 1% of the rectangles at each iteration. 
1 	second criterion : value non-linearity 
for every rectangle  we compute the variance of the absolute increase of the values at the corners of the edges for all directions  this criterion is similar to the previous criterion except that it measures the extent to which the value throughout the cell is non-linear instead of non-constant. figure 1 shows the corresponding discretization. 
　the value non-linearity criterion splits more parsimoniously than corner-value difference because it intends to refine whenever the approximated function is non-linear instead of non-constant  see the difference of splitting in the area above the frontier 1  where the value function is almost linear . 
　we observe that in both cases  the refinement process does not split around the bottom part of frontier 1  because the vf is almost constant in this area although there is a change in the optimal control. 
　we observe that in both of these splitting processes based on the approximation of the vf  there is a huge amount of memory spent for the approximation of the discontinuity despite the optimal control being constant in this area. this is undesirable and so in the next section we will pay attention to the policy when splitting. 
1 	the policy disagreement criterion 
figure 1 shows the optimal policy and several optimal trajectories for different starting points. we would like to define a process that could refine around the areas of change in the optimal control  that is around frontiers 1 and 1  but not around frontier 1. 
　when we solve the mdp and compute the value function of the dp equation  we can compare the policy given by the arg maxu of equation  1  with the optimal control law derived from the local gradient of v  given by the arg maxu in equation 1 . we define the states of policy disagreement being those where these two measures of the optimal control diverge. 
　figure 1 shows the discretization obtained by splitting the rectangles containing states of policy disagreement. 
 figure 1: the discretization of the state-space for the car-on-hill using the policy disagreement criterion  here we used an initial grid of 1 and a splitting rate of 1% to obtain this discretization . 

1 	uncertainty and probabilistic reasoning 

　this criterion is interesting since it splits at the places where there is a change in the optimal control  thus refining the resolution at the most important parts of the state-space for the approximation of the optimal control. however  as we can expect  if we only use this criterion  the value function will not be well approximated  thus this process may converge to a sub-optimal performance  see section 1 . indeed  we can observe that on figure 1  the bottom part of frontier 1 is situated higher than its optimal position  illustrated on figure 1. this results in an underestimation of the value function at this area because of the lack of precision around the discontinuity  frontier 1 . 
1 	c o m b i n a t i o n of several criteria 
we can combine policy disagreement with the cornervalue difference or value non-linearity criterion in order 
to take the advantages of both methods : a good approximation of the value function on the whole state-space and an increase of the resolution around the areas of change in the optimal control. we can combine the previous criteria in several ways  for example by a weighted sum of the respective criteria  or by a logical operation  split if an and/or combination of these criteria is satisfied . figure 1 shows the discretization obtained by alternatively  between iterations  using the value nonlinearity criterion and the policy disagreement criterion. we notice that the lower part of frontier 1 is well refined. 
　figure 1: the discretization of the statespace for the  car on the hill  problem using the combination of the value non-linearity and the policy disagreement criterion. 
1 	comparison of the performances 
to compare the discretizations  we ran a set  here 1  of trajectories starting from initial states regularly situated in the state-space  using the policies resulting from the discretizations. the performance of a discretization is defined as the sum of the gain  defined by equation  1   of these trajectories. figure 1 shows the respective performances of several splitting criteria as a function of the number of states. 
we notice the following points : 
  both the corner-value difference and value nonlinearity splitting processes perform better than the uniform grids. for example  in order to obtain an almost optimal performance of 1  the variable resolution grids resulting of these criteria need around four times fewer states than the uniform one. 
 figure 1: the performance for 
the uniform versus variable resolution grids for several splitting criterion 
  the value non-linearity splitting performs slightly better than the corner-value difference one. 
  the policy disagreement splitting is very good for a small number of states but does not improve after  and leads to a sub-optimal performance. 
  the policy disagreement combined with the value non-linearity gives the best performance. 
　can we do better   so far  we have only considered local splitting criteria  in the sense that we decide whether or not to split a rectangle according to information  value function and policy  relative to the rectangle itself. however  the effect of the splitting is not local : it has an influence on the whole state-space. 
1 	a global splitting criterion 
in the following sections we introduce two new measures of an markov chain : influence and variance that have been designed to be efficient to provide useful indexes of which parts of state-space are most important to devote data gathering or computational resources to. we derive a global splitting heuristic. 
1 	influence of a m a r k o v chain 
let us consider a markov chain whose set of states is and probabilities of transition from state 
we assume that the discount 
factor is a function of the state  and is written 	with for some holding time 	when the system gets 
in state 	it receives a reinforcement  
　　we wish to define the influence of a state on another state  as a measure of how much the state 
 contributes  to the value function of state  
　for that purpose  let us define the discounted cumulative -chained probabilities  which represent the sum of the discounted transition probabilities of all sequences of k states from  : 
		 1  
	munos and moore 	1 

definition 1  influence  let we define the influence of a state on the state as the quantity : 
let he a subset of we define the influence of a state on the subset as  
we notice that if the holding times 	are 	then 
the influence is well defined  and is bounded by  with  the intuitive idea that  
represents the  contribution  of state on the vf of state  is formalized by the result that is the partial derivative of  by  
　moreover  we prove that the influence satisfies the following property : 
  1  
　computation of the influence. equation  1  is not a bellman's equation since the probabilities  do not sum to 1  so we cannot deduce that the successive iterations : 
  1  
converge to the influence by using the classical contraction property in max-norm  see  puterman  1  . however  we have the following property : 

and we have the contraction property for the 1-norm which insures convergence of the iterated to the unique solution  the fixed point  of  1 . 
　illustration on the  car on the h i l l   . for a given discretization of the state space  we approximate the optimal control problem by building the corresponding mdp through the process described in section 1. once the mdp is solved  we consider the markov chain resulting from the choice of the control for the optimal 
policy 	let us denote  
and  
　　figure 1 shows  in gray levels  the influence of the states on    = {1 points}. 
　　now define the subset of states of policy disagreement  in the sense of section 1 . figure 1 a  shows for a regular grid  of 1 x 1 . then  the influence of the states on is plotted in figure 1 b . 
　figure 1; influence on 1 points  the crosses . the influence on a state  follows  the direction of the optimal trajectory starting front that state through some kind of  diffusion process  . 

 a  states of policy d i s a g r e e m e n t   b   influence on these states 
 figure 1:  a  the set 	of policy disagreement and  b  the influence  
　the darkest zones are the places that  contribute  the most to the value function of states in  thus the areas that are expected to affect the vf at the places of change in the optimal control. 
　in the following section  we introduce the variance of the markov chain in order to get an estimation of how accurate the approximation of the vf is  for a given discretization. 
1 	variance of a m a r k o v chain 
by using the notation of the previous section  we have : for any satisfies the bellman's equation : 
 1  
which states that is the discounted average of the next values weighted by the probabilities of transition we are interested in computing the variance of these values in order to have an estimation of the range of the values averaged by 
 1  
also average some succes-
sive values so we would like that the variance also takes into account this averaging  as well as all the following ones. in what follows  we define the value function as an averager of the reinforcements and give the definition of the variance of a markov chain. 

1 	uncertainty and probabilistic reasoning 

　the value function as an averager of the reinforcements. let us denote a sequence of 
states whose first one is let be the set of all possible sequences let us denote 	the product of the probabilities of transition of the successive states in a 
sequence  and for  the cu-
mulative time of the ith first states of the sequence  with 
by definition  
we have the property that :  
1. we can prove that the value function satisfies the following equation : 
	 1  
thus  the value function is expressed as an average of the discounted reinforcements. now  we can define the variance of these values. 
definition 1  variance  we define the variance with e ♀  defined by  1 . thus the variance the sum of an immediate contribution into account the variation in the values of the immediate successors  plus the discounted average of the variance  of these successors. 
　this is a bellman equation and it can be solved by dp methods  thanks to a contraction property in maxnorm . 
　notice that the variance defined here has no relationship to the variance of the gain obtained by following any specific policy for the real problem  which is always 1 since the continuous problem is deterministic. here  the variance indicates the extent of uncertainty on the value function because of the discretization process  and thus gives an estimation of the quality of approximation of the value function for a given discretization. 
　　by using some geometrical considerations  see  munos and moore  1    we can prove that  is close to 1 in two specific cases : either if the gradient at the iterated point is low  i.e. the values of the successive states are almost constant  or if is close to one corner  then the barycentric coordinate is close to 1 and the are close to 1 . in both of these cases  is low and implies that the iteration of does not lead to a degradation of the quality of approximation of the value function  the variance does not increase . 
figure 1a shows the standard-deviation for the 
car-on-the-hill problem for a uniform grid  of 1 by 1 . we notice the following points : 
. the standard deviation is very high around the discontinuity of the value function. 
  there is a noticeable positive standard deviation around the 1 frontiers of discontinuity of the gradient of the value function  which correspond to a change in the optimal control. indeed  in these areas  the vf is the average of the discounted reinforcement for different exit times  depending on whether the car can reach the goal directly or has to do one more loop . 
  apart from these areas  the standard deviation is very low  which means that the discounted reinforcements averaged by the value function are almost constant. a refinement of the resolution in these areas has no chance of producing an important change in the value function. 
 figure 1: the standard deviation function for the  car on the hill. 
　thus it appears that the areas where a splitting might affect the most the approximation of the value function are the cells whose corners have the highest standard deviations. 
1 	a global splitting heuristic 
now we combine the notions of influence and variance described in the previous sections  in order to define a non-local splitting criterion. we have seen that : 
  the states  of highest standard deviation are the states of highest uncertainty on the quality of approximation of the vf  thus the states that could improve the most their approximation accuracy when split  see figure 1 a  for an illustration on the car-on-the-hill . 
  the states of highest influence  see figure 1 b   on the set of states of policy disagreement  figure 1 a   are the states whose value function affects the area where there is a change in the optimal control. 
thus  in order to improve the precision of approximation at the most relevant areas of the state-space  i.e. where there is a change in the optimal control   an heuristic should be to split the states of highest stdev inf criterion 
	munos and moore 	1 

 see figure 1  where : 
　figure 1 shows the discretization obtained by splitting the states of highest stdevjnf. 
figure 1:  a  description of the cart-pole 1d problem  
 b  the projection of the discretization  onto the plane obtained by the stdev lnf criterion and some tra-
 a  standard deviation 
 figure 1:  a  the standard-deviation jectories for several initial points. the hill  and  b  the stdevjnf criterion  

figure 1: the dis-
cretization 
resulting of the splitting of the states of highest standard deviation that have an influence on the states of policy 
disagreement  stdevjnf criterion.  
　we observe that the stdevjnf criterion does not split the areas where the vf is discontinuous unless some refinement is necessary to get a better approximation of the optimal control. this turns out to be critical in higher dimensions  where the cost to get an accurate approximation of the vf is too high. 
1 illustration on other control problems 
1 	the cart-pole problem 
the dynamics of this 1-dimensional physical system  illustrated in figure are described in  barto et al.  1 . the goal is defined by the area :  
  and no limits on ' this is a much narrower goal than in previous cart-pole rl work. notice that  minimum time maneuver to a small goal region  from an arbitrary start state is much harder than merely balancing the pole without falling. the current reinforcement is zero everywhere and the terminal reinforcement is  if the system exits from the state-space  and if it reaches the goal. 
　figure 1 shows the performance obtained for several splitting criteria previously defined. note that the local criteria do not perform better than the uniform grids because the vf is discontinuous at several parts of the state-space  because of boundary problems similar to the frontier 1 of the  car on the hill  problem  and the value-based criteria spend too many resources on approximating these useless areas. but note too that the stdevjnf criterion performs very well because it avoids this problem. 
 figure 1: comparison of the perform ances on the cart-pole problem for several splitting criteria. 
1 	t h e a c r o b o t 
the acrobot  which has a state-space  is a two-link arm with a single actuator at the elbow  see figure 1 . here  the goal of the controller is to balance the acrobot at its unstable  inverted vertical position  in the minimum time. 
　this actuator exerts a torque between the links. in  sutton  1b  the goal was to lift the hand to a given height  but here we perform the harder task of balancing the acrobot at its unstable  inverted vertical position  in the minimum time. the goal is defined by a very narrow range of on both angles around the vertical position 
                          figure 1 b    for which the system receives a reinforcement of  anywhere else  the reinforcement is zero. the two first dimensions of the state space have a structure of a torus  because of the  modulo on the angles   which is implemented in our structure by having the vertices of the 1 first dimensions being angle 1 and  pointing to the same entry for the value function. figure 1 shows the performance. 
interpretation of the results : as we noticed for 

1 	uncertainty and probabilistic reasoning 


figure 1:  a  description of the acrobot.  b  projection of the discretization  onto the plane  obtained by the stdevjnf criterion  and one trajectory. 
 figure 1: comparison of the performances on the acrobot problem for several splitting criteria. 
the two previous 1d problems  the local splitting criteria fail to improve the performance of the uniform grids because they spend too many resources on local considerations  either approximating the value function or the optimal policy . for example  on the cart-pole problem  the value non-linearity criterion will concentrate on approximating the vf mostly at parts of the state space where there is already no chance to rebalance the pole. and the areas around the vertical position  low 1   which are the most important areas  will not be refined in time  however  if we continue the simulations after about 1 states  the local criteria start to perform better than the uniform grids  because these areas get eventually refined . 
　the stdevjnf criterion  which takes into account global consideration for the splitting  performs very well for all the problems described above. 
1 	conclusion 
in this paper we proposed a variable resolution discretization approach to solve continuous time and space control problems. we described several local splitting criteria  based on the vf or the policy approximation. local value-based splitting is an efficient  model-based  relative of the q-learning-based tree splitting criteria used  for example  by  chapman and kaelbling  1; simons et al.  1; mccallum  1 . but it is only when combined with new measures based on policy and on global considerations  influence and variance  that we are able to get truly effective  near-optimal performance on our control problems. the tree-based state-space partitions in  moore  1; moore and atkeson  1  were produced by different criteria  of empirical performance   and produced far more parsimonious trees  but no attempt was made to minimize cost: merely to find a valid path. our planned future work includes pruning  in order to include  specific towards general  grouping of areas that have been over-refined  and studying behavior on continuous stochastic systems such as production scheduling. 
acknowledgments. 
　this research was sponsored by dassaultaviation and cmu. 
