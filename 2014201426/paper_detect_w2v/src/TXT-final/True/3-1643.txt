
active learning reduces the amount of manually annotated sentences necessary when training state-ofthe-art statistical parsers. one popular method  uncertainty sampling  selects sentences for which the parser exhibits low certainty. however  this method does not quantify confidence about the current statistical model itself. in particular  we should be less confident about selection decisions based on low frequency events. we present a novel twostage method which first targets sentences which cannot be reliably selected using uncertainty sampling  and then applies standard uncertainty sampling to the remaining sentences. an evaluation shows that this method performs better than pure uncertainty sampling  and better than an ensemble method based on bagged ensemble members only.
1 introduction
state-of-the-art parsers  collins  1; charniak  1  require large amounts of manually annotated training material  such as the penn treebank  marcus et al.  1   to achieve high performance levels. however  creating such labelled data sets is costly and time-consuming. active learning promises to reduce this cost by requesting only highly informative examples for human annotation. methods have been proposed that estimate example informativity by the degree of uncertainty of a single learner as to the correct label of an example  lewis and gale  1  or by the disagreement of a committee of learners  seung et al.  1 . this paper is concerned with reducing the manual annotation effort necessary to train a state-of-the-art lexicalised parser  collins  1 .
　uncertainty-based sampling has been successfully applied to the same problem problem  hwa  1 . here  sentences are selected for manual annotation when the entropy over the probability distribution of competing analyses is high. entropy quantifies the degree of uncertainty as to the correct analysis of a sentence.
　a problem with active learning methods such as uncertainty sampling is that they have no method for dealing with the consequences of low counts. for example  the parse tree probability of the most likely reading in a peaked distribution may depend on a probability which has been unreliably estimated from an as yet rarely observed event. in this case  the model would indicate certainty about a particular analysis where indeed it is not confident. in general  we would like to place less confidence in selection decisions based on entropy over probability distributions involving low frequency events. however  sentences whose predicted parse was selected on the basis of infrequent events may well be informative. since entropy will not in itself always allow us to reliably select such examples for labelling  we need to consider other mechanisms.
　we propose a novel two-stage method which first selects unparsable sentences according to a bagged parser  and applies uncertainty sampling to the remaining sentences using a fully trained parser. evaluation shows that this method performs better than single parser uncertainty sampling  and better than an ensemble method with bagged ensemble members.
　to explain our results  we show empirically that entropy and f-measure are negatively correlated. thus  selection according to entropy tends to acquire annotations of sentences with low f-measure under the current model. an oraclebased experiment demonstrates that preferably selecting low f-measure sentences is indeed beneficial and explains why uncertainty sampling is successful in general. furthermore  we find that exactly those sentences which our proposed methods targets show no such correlation between entropy and fmeasure. in other words  entropy will not reliably identify informative examples from this subset  even though these sentences have below average f-measure and should be particularly useful. these findings help to explain why the proposed method is a successful strategy.
1 active learning methods
popular methods for active learning estimate example informativity with the uncertainty of a single classifier or the disagreement of an ensemble of classifiers.
　uncertainty-based sampling  or tree entropy  chooses examples with high entropy of the probability distribution for a single parser  hwa  1 :
	fmte s τ  =  xpm t|s logpm t|s 	 1 
t（τ
where τ is the set of parse trees assigned to sentence s by a stochastic parser with parameter model m. less spiked distributions have a higher entropy and indicate uncertainty of the parse model as to the correct analysis. thus  it will be useful to know their true parse tree.
　ensemble-based methods for active learning select examples for which an ensemble of classifiers shows a high degree of disagreement. kullback-leibler divergence to the mean quantifies ensemble disagreement  pereira et al.  1; mccallum and nigam  1 . it is the average kullbackleibler divergence between each distribution and the mean of all distributions:
		 1 
where m denotes the set of k ensemble models  pavg is the mean distribution over ensemble members in m  pavg = pm pm t|s /k  and d ，||，  is kl-divergence  an information-theoretic measure of the difference between two distributions. it will be useful to acquire the manual annotation of sentences with a high kl-divergence to the mean. this metric has been applied for active learning in the context of text classification  mccallum and nigam  1 .
1 a novel two-stage selection method
acquiring the correct analysis of a sentence of which the predicted analysis was selected on the basis of infrequent events may well be informative. since entropy itself will not allow us to reliably select such examples for labelling  we need to consider other mechanisms. a simple  but effective method is to eliminate some infrequent events from the parsing model. simply bagging the current training set  and retraining the parser on this set allows to identify such examples for labelling.
　bagging is a general machine learning technique that reduces variance of the underlying training methods  breiman  1 . it aggregates estimates from classifiers trained on bootstrap replicates  bags  of the original training data. creating a bootstrap replicate entails sampling with replacement n examples from a training set of n examples. a bootstrap replicate will not only perturb all event counts to some degree  but will inevitably eliminate some of the low frequency event types.
　the proposed method operates in two stages. we first select sentences which are unparsable according to a single bagged version of the parser  but  possibly  parsable under the current fully trained model. from the remaining sentences  we select those with the highest entropy as determined by the fully trained model. we can express this formally as follows:
	fm mtwo 1 s τ  = max fmte s τm  failure s m1  	 1 
where fmte is tree entropy according to a fully trained model m  defined in  1 . the function failure s m1  returns infinity when sentence s is parsable given bagged parser model m1  and 1 otherwise.
1 experimental setup
for our experiment  we employ a state-of-the-art lexicalised parser  collins  1 .1 we employ default settings without expending any effort to optimise parameters towards the considerably smaller training sets involved in active learning.
　in common with almost all active learning research  we compare the efficacy of different selection methods by performing simulation experiments. we label sentences of sections 1 - 1 from the penn wsj treebank  marcus et al.  1   ignoring sentences longer than 1 words.
　we report the average over a 1-fold cross-validation to ensure statistical significance of the results. in a single fold  we randomly sample  without replacement  an initial labelled training set of a fixed size - 1 or 1 sentences  depending on the experiment - and a test set of 1 sentences. the remaining sentences constitute the global pool of unlabelled sentences  ca. 1 sentences . for a realistic experiment  we tag the test set with the tnt part-of-speech tagger as input for the parser  brants  1 . we train tnt on 1 sentences in the global resource. in a 1-fold cross-validation  the parser has 1% labelled precision and 1% labelled recall  when trained on 1k sentences and applied to test sets of 1 sentences.1
　we randomly sample  without replacement  a subset of 1 sentences from the global pool in each iteration. from this subset  1 sentences are selected for manual annotation according to the current sample selection method. then  annotated sentences are added to the training set.
　for consistent comparison across methods  we evaluate test set performance of a single parser trained on the entirety of the labeled training data at each step  regardless of the selection method being a single or an ensemble method.
length balanced sampling for situations such as active learning for parsing  the sentences in question need a variable number of labelling decisions. this may confound sample selection metrics and it is therefore necessary to normalise. for example  tree entropy may be directly normalised by sentence length  hwa  1   or by the binary logarithm of the number of parser readings  hwa  1 .
　we use the following method to control for sentence length in sample selection: given a batch size b  we randomly sample b sentences from the pool and record the number el of selected examples for sentence length l. then  for all lengths l = 1 ...1  we select from all sentences in the pool of length l the el examples with the highest score according to our sample selection metric. of course  the union of these sets will have b examples again. since we randomly sampled the batch from the pool  we may assume that the batch distribution reflects the pool distribution  in particular wrt. the distribution of sentence lengths.

figure 1: a random sampling learning curve. the maximal training set has 1k sentences  1k constituents . the number of constituents is given on a log scale.
　this method effectively reproduces the sentence length profile of the original corpus by construction and therefore guards against the selection of sentence length biased subsets. furthermore  it is equally applicable for all metrics and allows a direct comparison between metrics. note that this method is applicable in general for the sample selection of sequential data where one may expect to find correlation between sample length and score.
relevant evaluation measures active learning for parsing is typically evaluated in terms of achieving a given f-measure for some amount of labelling expenditure. the cost of acquiring manually annotated training material is given in terms of the number of constituents. f-measure itself is a composite term  being composed of precision and recall  black et al.  1 . fig. 1 shows a learning curve for a random sampling experiment. we see that precision and recall do not increase at the same rate. for this reason  it may well be advantageous to aggressively increase recall with minimal impact on precision  formulate stronger: still want to increase precision . one way of achieving this is to pursue sentences which cannot be parsed.
1 results
the experiments in this section address the following questions. is it generally useful to select unparsable sentences for manual annotation  what is the gain of using the novel twostage method over a state-of-the-art uncertainty-based sample selection method  given that the two-stage method has a bagged component  how does it compare against a stateof-the-art ensemble-based method which employs bagged ensemble members 

figure 1: comparison of f-measure learning curves. number of constituents is given on a log scale.
methodcostreductionrandom1n/aentropy/unparsed1.1%unparsed1.1%entropy1-1%table 1: annotation cost to reach 1% f-measure  and reduction over random sampling.
including/excluding unparsed sentences in this experiment  we compare methods which do or do not include unparsed sentences in the batch of selected examples. acquiring the correct parse tree of an unparsable sentence increases the size of the model structure of the grammar and  presumably  helps to increase coverage in the test set.
　a very simple method  unparsed  preferably includes unparsed pool sentences in the batch. should the number of unparsed sentences fall short of the batch size  we randomly sample parsable sentences from the pool to fill the batch. by contrast  entropy only selects parsable sentences with high entropy. the method entropy/unparsed preferably selects unparsed pool sentences  and fills the batch with high entropy examples. we may view this method as being composed of a binary parsability component  and a gradual uncertainty component. the baseline is random  a parser trained on randomly sampled training sets of different sizes.
　we start with an initial training set of 1 randomly sampled sentences  containing 1 labeled constituents and continue for 1 rounds until 1 sentences have been sampled  ca. 1k constituents . here  as in all subsequent experiments we employ length balanced sampling  cf. sec. 1.
　methods unparsed and entropy/unparsed perform consistently better than random  fig. 1 . note that their performance is nearly identical until more than 1k constituents have been labelled. method entropy performs consistently

1k 1k 1k 1k number of constituents
figure 1: the new two-stage method versus state-of-the-art uncertainty sampling.
methodcovprecrecfmrandom1111entropy/unparsed1111unparsed1111entropy1111table 1: parseval values for different metrics after 1 constituents have been annotated.
worse than random.
　methods unparsed and entropy/unparsed reduce the amount of labeled constituents necessary to achieve 1% fmeasure by around 1% as compared to random  while entropy actually increases the cost by 1%  tab. 1 .
　we also compare performance across methods for the same amount of annotation effort. tab. 1 shows precision  recall  and f-measure after labelling 1k constituents. methods unparsed and entropy/unparsed have considerably higher coverage than random  entropy has lower coverage. while all methods show comparable values for precision  they differ decidedly in their recall values. the two methods which aggressively pursue unparsed sentences  unparsed and entropy/unparsed  have more than 1% points higher recall than entropy  and accordingly higher f-measures than random and entropy.
　these results confirm the importance of including unparsed sentences. doing so helps achieving better coverage and a higher recall value which directly translates into higher fmeasure. accordingly  all of the following experiments will include unparsed sentences in the batch. the negative result for the purely entropy-based method shows clearly that a naive application of uncertainty sampling may have adverse consequences. it is extremely important to consider which phenomena a selection method is targeting.

figure 1: the new two-stage method versus a state-of-the-art ensemble-based method.
methodcostreductionrandom1n/atwo stage1.1%entropy/unparsed1.1%table 1: annotation cost to reach 1% f-measure  and reduction over random sampling.
atwo-stageselectionmethod the novel method addresses problems with sentences which cannot be reliably selected with popular active learning methods. therefore  we expect a gain in performance. method two stage preferably includes sentences which are unparsable according to a parser trained on a bagged version of the current training set. then  the batch is filled with  parsable  sentences which have high entropy according to a second  fully trained parser.
　given that composite methods which preferably select unparsed sentences perform nearly uniformly well  we will now use a considerably larger initial training set in order to be able to observe differences between these methods. we start with 1 sentences  1k constituents   and continue for 1 rounds until a total of 1 sentences has been sampled  ca. 1k constituents .
　method two stage performs consistently better than both random and entropy/unparsed  fig. 1 . it reduces the amount of labelled data necessary to reach 1% f-measure by 1% as compared to random  tab. 1 . the central result of this paper is that  to reach this level  two stage reduces the number of constituents by 1 constituents against the state-of-the-art method entropy/unparsed: a reduction by a further 1%. also  it has consistently higher precision and recall than entropy/unparsed after the labelling of 1k constituents  tab. 1 .

1k 1k 1k 1k number of constituents
figure 1: comparison of f-measure learning curves. number of constituents is given on a log scale.
methodcovprecrecfmrandom1111two stage1111entropy/unparsed1111table 1: parseval values for different metrics after 1 constituents have been annotated.
an ensemble method we now compare performance of the new method against a state-of-the-art ensemble method  namely the kl-divergence to the mean for an ensemble of n bagged parsers  cf. subsec. 1. the method kl-div/unparsed preferably selects unparsed pool sentences  and fills the batch with sentences having a high mean kl-divergence.  we consider a sentence as unparsed when at least one of the ensemble members fails to deliver an analysis.  as in the previous experiment  we start with 1 sentences and continue for 1 rounds. we set the ensemble size to n = 1.
　fig. 1 shows ensemble method kl-div/unparsed to perform better than two stage until 1k constituents. after this point  two stage performs clearly better than kldiv/unparsed. this is actually a surprising result  given that both methods perform similar jobs: they select unparsable sentences according to one or more bagged parsers  and then apply an information-theoretic measure  either entropy or kl-divergence. we conjecture that  after having filtered the difficult examples in the first stage  the second stage should make use of the information to be had from a fully trained parser. at any rate  our proposed method is conceptually simpler and also quicker to compute than an ensemble-based method.
1 understanding the new method
acquiring the annotation of objectively difficult sentences should improve the parser. we employ an oracle-based ex-
sentencesf-measurepearsonparsable11% 1unreliable1.1%1table 1: average f-measure and correlation coefficients between entropy and f-measure
periment to test this claim. method oracle selects sentences whose preferred parse tree  according to the current grammar  has low f-measure as determined against a gold-standard tree. fig. 1 shows that method oracle performs consistently better than our best result  the new two stage method. this suggests that a selection method which successfully targets difficult sentences  low f-measure  will perform well.
　in another experiment  we train the parser on a randomly sampled training set of 1 sentences  and apply it to a test set of 1 sentences. we are interested in the degree of correlation between the variables f-measure  preferred tree against gold-standard tree  and tree entropy. a correlation analysis over all 1 parsable sentences shows that the two variables are indeed  negatively  correlated  cf. tab. 1. pearson's coefficient is  1. given the size of the considered data set  correlation is highly significant  p = 1 . selection according to entropy will thus tend to pick low f-measure sentences. given the observation from the oracle experiment that it is beneficial to target low f-measure sentences  this finding explains why entropy is a useful selection method.
　if we now apply a bagged version of the parser to the same test set  more sentences become unparsable since we eliminate some infrequent parse events. focusing on the 1 unreliable sentences which are parsable under a fully trained model  but not under a bagged model  we find a pearson coefficient between entropy and f-measure close to 1  tab. 1 . in other words  entropy and f-measure are uncorrelated  and entropy cannot reliably select difficult examples within this class of sentences. what is more  the average f-measure within these unreliable sentences is more than 1 percentage points below average  indicating that acquiring their true parse trees will be particularly useful.
　note that the first stage of our new method targets exactly these kind of unreliable sentences. the above experiments demonstrate why the new method is indeed successful.
1 related work
preferably selecting high entropy examples has been shown to be an effective method for parsing  hwa  1 . selecting unparsed sentences has been previously suggested because of their high uncertainty  e.g. in  thompson et al.  1 . however  to the best of our knowledge  this effect has not been quantified before. bagging ensemble members  or alternatively random perturbation of event counts  has been explored in the context of active learning by  argamon-engelson and dagan  1; mccallum and nigam  1 . in particular  argamon-engelson and dagan have indicated that these methods target low frequency events. bagging  and boosting  a parser ensemble has been employed to increase parser performance  henderson and brill  1 . however  the application of a bagged parser ensemble to active learning is new. density estimation has been suggested as a method to guard against selecting of outliers  e.g  mccallum and nigam  1; tang et al.  1 . this approach is orthogonal to our suggested new method  and combining the two may well result in even better performance.
1 conclusion
we demonstrated a number of points in this paper. first  we investigated the effect of targeting unparsed sentences. this is a simple  but very effective way to increase labelled recall and thereby f-measure. this method has been used implicitly before  but to our knowledge the effect of this strategy has not been quantified previously. secondly  we presented a novel  two-stage method which particularly targets sentences which cannot be reliably selected using popular active learning methods. we showed that the proposed method works better than uncertainty sampling alone. also it compares favourably against a state-of-the-art ensemble method based on bagging. finally  an oracle-based experiment indicated that targeting  objectively  difficult sentences is a good strategy. furthermore  we demonstrated that entropy and fmeasure are significantly correlated in general. however  they are uncorrelated for exactly the class of sentences of which our new method takes care. this explains why the new two-stage method performs well. in future work  we would like to investigate if the proposed two-stage method can be applied to applications other than parsing.
acknowledgments
we would like to thank james henderson  mirella lapata  and andrew smith for valuable discussions and comments on this work.
