 
most research in learning for planning has concentrated on efficiency gains. another important goal is improving the quality of final plans. learning to improve plan quality has been examined by a few researchers  however  little research has been done learning to improve both efficiency and quality. this paper explores this problem by using the scope learning system to acquire control knowledge that improves on both of these metrics. since scope uses a very flexible training approach  we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation metrics. experimental results show that scope can significantly improve both the quality of final plans and overall planning efficiency. 
1 	introduction 
a considerable amount of planning and learning research has been devoted to improving planning efficiency  also known as  speedup learning   minton  1; leckie and zuckerman  1; estlin and mooney  1; 
kambhampati et a/.  1 . these systems construct domain-specific control rules that enable a planner to find solutions more quickly. another aim of planning and learning research  which has received much less attention  is to improve the quality of plans produced by a planner  perez  1; iwamoto  1 . in this type of learning  control rules guide the planner towards better solutions. generating high-quality plans is an essential feature for many real-world planning systems  as is generating these plans efficiently  chien et al.  1 . improving plan quality and planner efficiency can often be accomplished by using similar learning methods. however  little research has been done in acquiring control knowledge to improve both these metrics. 
   *this research was supported by the nasa graduate student researchers program  grant number ngt-1. 
　to investigate this issue  we employ the scope1 learning system  which uses a combination of machine learning techniques to acquire control rules for a partial-order planner. scope has previously been shown to significantly improve efficiency of a version of the well-known ucpop planner  estlin and mooney  1 . scope employs a flexible learning algorithm that can be trained to focus on different evaluation metrics. the primary focus of previous experiments was to avoid backtracking  and thus  improve planning efficiency. however  by using a particular training method  scope can be easily modified to learn rules that guide the planner towards only  high-quality  solutions. 
　the remainder of this paper is organized as follows. section 1 further introduces the issue of plan quality and why it is important. section 1 describes how scope is used to construct control rules  and in section 1 we discuss how scope was used to learn rules for improving plan quality. section 1 presents experimental results that show scope can improve both the quality of plans and planner efficiency. finally  section 1 discusses related work  section 1 presents ideas for future research and section 1 presents our conclusions. 
1 	plan quality 
in many real-world planning systems the quality of a plan may be just as important  if not more  than the time it takes to generate the plan. for instance  it may be vital in a manufacturing domain for a planner to produce plans with low resource consumption  or with the least number of possible steps. there are a variety of notions about what makes a good plan. some of the more common quality metrics are listed below: 
  the length of the plan  or the total number of steps  
  the execution time of the plan 
  the resource consumption required 
  the robustness of the plan 
1
　　search control optimization of planning through experience 
	estlin & mooney 	1 


figure 1: two refinement candidates for achieving the goal at-obj pkg1 airport1 . 
depending on the domain  different quality metrics will have varying importance. in this paper  we focus on improving the first quality metric  minimizing the number of plan steps. 
1 	the scope learning system 
scope was designed to learn search-control rules for planning decisions that might lead to failure  i.e. might be backtracked upon . figure 1 illustrates an example from the logistics transportation domain  veloso  1  where control knowledge could be useful. here  there are two possible refinement candidates for adding a new action to achieve the goal at-obj  pkgl city1-airport . only the first will actually lead to a solution  since in this domain  only planes can be used to transport packages between cities. for each set of refinement candidates  scope learns control rules in the form of selection rules that define when each refinement should be applied. for example  shown next is a selection rule for the first candidate  from figure 1  which contains several control conditions. 

this rule states that unload-airplane  x  y  z  should be selected to add at-obj  x  z  only when it is initially true that object  x starts at a location which is in a different city than  z. learned control information is incorporated into the planner so that attempts to select an inappropriate refinement will immediately fail. 
　scope is implemented in prolog  which provides an excellent framework for learning control rules. search algorithms can be implemented in prolog in such a way that allows control information to be easily incorporated in the form of clause-selection rules  cohen  1 . for its base planner  scope uses a version of the ucpop partial-order planning algorithm which has been reimplemented in prolog.1 planning decision points are represented in the planner as clause-selection problems  i.e. 
1
the main difference between our planner and ucpop is 
planning and scheduling 

figure 1: scope's high-level architecture 
each refinement candidate is formulated as a separate clause . 
　by analyzing a set of training examples  scope learns refinement-selection rules which are incorporated into the original planner in the form of clause-selection heuristics. as shown in figure 1  scope's algorithm has three main phases which are are briefly presented in the next few sections. a more detailed description can be found in  estlin  1 . 
1 	e x a m p l e analysis 
in the example analysis phase  training examples are solved using the original planner and two main outputs are produced: a set of selection-decision examples and a set of generalized proof trees. a  selection decision  is a planning subgoal that was solved by applying a particular plan refinement  such as adding a new action. selection-decisions record successful and unsuccessful applications of plan refinements. to collect these decisions  a trace of the planning decision process used to solve each training example is stored in a proof tree. then  scope uses the proof trees to extract examples of correct and incorrect refinement-selection decisions. 
　the second output of this phase is a set of generalized proof trees. standard explanation-based generalization  ebg   mitchell et a/.  1; dejong and mooney  1  techniques are used to generalize each training example proof tree. the goal of this generalization is to remove proof elements that are dependent on the specific example facts while maintaining the overall proof structure. 
that ucpop normally employs a best-first search strategy while our prolog planner operates using a depth-first backtracking search. as shown in section 1  our prolog planner actually performs better than the standard lisp implementation of ucpop on the sample problem sets used to test the learning algorithm. 


figure 1: learned control rules for the logistics domain 
generalized proofs provide a background context that explains the success of all correct planning decisions. information from these trees is used in the next phase to construct control rules. 
1 	control rule induction 
the goal of the induction phase is to produce an operational definition of when it is useful to apply a planning refinement. scope employs a version of the f o i l algorithm  quinlan  1  to learn control rules through induction. f o i l attempts to learn a control definition that is composed of a set of horn clauses. this definition covers all of the positive examples of when to apply a refinement  and none of the negatives. the selection-decision examples collected in the example analysis phase provide the sets of positive and negative examples for each refinement. 
　individual clause construction is done by using a general-to-specific hill-climbing search. f o i l adds antecedents to the developing clause one at a time. at each step f o i l evaluates all literals that might be added and selects the one which maximizes an information-based gain heuristic. one drawback to f o i l is that the hillclimbing search for a good antecedent can easily explode  especially when there are numerous background predicates with large numbers of arguments.1 scope circumvents this search problem by utilizing the generalized proofs of training examples. by examining the proof trees  scope identifies a small set of potential literals that could be added as antecedents to the current clause definition. scope also considers several other types of control rule antecedents during induction. these include negated proof tree literals  determinate literals  muggleton  1   variable codesignation constraints  and relational cliches  silverstein and pazzani  1 . 
1 	program specialization phase 
once refinement selection rules have been learned  they are passed to the program specialization phase which adds this control information into the original plan-
1
　　when selecting each new clause antecedent  foil tries all possible variable combinations for all predicates before making its choice. this search grows exponentially as the number of predicate arguments increases. 
ner. the basic approach is to guard each planning refinement with the selection information. this forces a refinement application to fail quickly on planning subgoals to which the refinement should not be applied. figure 1 shows two learned rules for the logistics transportation domain. the first rule selects the new action unload-airplane  x p loc  to achieve the goal at-obj  x.loc  when x is found to be initially located in a different city than the goal location and when the goal location is an airport. the second rule uses the initial state to achieve the goal at-track t loc  if there does not exist another action in the plan drive-truck t loc loc1  which moves the truck to a new location. 
1 	focusing scope on plan quality 
by learning rules which avoid search paths that lead to backtracking  scope has been shown to significantly improving the efficiency of a planner  estlin and mooney  1 . however  by modifying the method used to collect training data  it can easily improve other planning metrics as well. 
　in order to improve plan quality  we trained scope on only high-quality solutions. to improve plan lengths  scope was given the shortest solution plans for all training problems. this causes scope to collect positive and negative examples of when to apply a plan refinement based on finding the optimal plan. thus  it learns rules that not only avoid dead-end paths  but also that avoid paths that lead to sub-optimal solutions. 
　in order to train scope on high-quality solutions  the search method of depth-first iterative deepening  dfid   korf  1  was employed to solve the training problems. this method ensured that the shortest possible solutions were always returned. to improve upon other quality metrics  different training methods may have to be employed that return optimal  or near-optimal  solutions based on other evaluation functions. 
1 e v a l u a t i o n 
1 	e x p e r i m e n t a l setup 
the logistics transportation domain  veloso  1  was used to evaluate scope's ability to improve both quality and efficiency. in this domain  packages must be delivered to different locations in several cities. trucks are used to transport packages within a city  and planes are used to transport packages between different cities. training and test problems were produced by generating random initial and final states. problems contained one and two packages  two trucks and two planes  which were distributed among two cities. 
　as explained above  depth-first iterative deepening was used to solve the training problems. scope was trained on separate example sets of increasing size. five 
	estlin & mooney 	1 

figure 1: efficiency performance 
trials were run for each training set size  after which results were averaged. 
　for each trial  a test set of 1 independently generated problems was used to evaluate performance. no time limit was used during testing  but a depth bound was given that ensured all test problems could be solved. for comparison purposes we used several different search methods to solve the test examples. these include depth-first search  depth-first search + learned control information  depth-first iterative deepening search  and best-first search. the best-first search tests were done using the standard lisp implementation of ucpop  barrett et al.  1 . to utilize all of these search methods  only test problems with solutions under a certain length were used. this ensured all problems could be solved by all methods in a reasonable amount of time. 
1 	experimental results 
figure 1 shows improvement in planning efficiency. the times shown represent the number of seconds required to solve the problems in the test sets after scope was trained on a given number of examples. the best performance occurred when the planner utilized the learned control information. in these tests  scope was able to produce a new planner that was an average of 1 times faster than the original depth-first planner. depth-first iterative deepening performs better than depth-first but not nearly as well as scope. best-first search  on the other hand  performed worse than depth-first.1 
　figure 1 represents how scope improved final plan quality. the lengths shown in the graph represent the average solution lengths returned for the test problems. the depth-first iterative deepening line shows the aver-
     1  note that the best-first tests were done using the lisp implementation of ucpop. thus this result could be partially due to implementation differences between prolog and lisp. 
planning and scheduling 
figure 1: quality performance. here  depth-first iterative deepening and best-first have almost identical performances. 
age length of all optimal solutions. in this experiment  scope was able to produce a new planner that returned optimal solutions  and returned significantly shorter solutions than those returned by depth-first alone. bestfirst search also generated near-optimal solutions. 
　overall  these results indicate that is it possible to learn control rules that improve both planning efficiency and plan quality. by using depth-first iterative deepening to solve the training problems  scope can be trained on optimal solutions. scope can then learn control rules that not only help the planner find solutions quickly  but that also lead to high quality solutions. 
1 	scalability 
one other set of experiments was performed to test how 
scope performed on harder problems. using depth-first iterative deepening  scope was trained on 1 training problems from the same distribution explained above. four different test sets of increasing complexity were then used to evaluate scope's performance. during testing  a time limit of 1 seconds was used. no limits on solutions length were imposed on test problems in these experiments  however  a depth bound was used that ensured all test problems could be solved. five trials were run for these tests  after which results were averaged. 
　the results are shown in table 1. the first two columns of the table show the percentage of test problems that could be solved within the time limit  using depth-first search  before and after learning. the next set of columns report the planning time required. the last two sets of columns contain results on plan quality. in the first set  we show the average solution lengths of all solved test problems before and after learning. in the last set  we show the average solution lengths  and optimal solution lengths  for problems that could be solved 


table 1: efficiency and quality results on increasingly complex test problems in the logistics domain. 

by the original planner  without rules . this last set of columns show any improvements made in solution length by the learned rules. 
　for all four test sets  scope was able to improve planning efficiency  and when possible  increase the percentage of problems solved. for instance  on the first set of problems  containing 1 goal   scope was able to create a new planner that was an average of 1 times faster than the original. unfortunately  it was difficult to gather data on whether plan quality was improved in all test sets. for many examples  the original planner could not find a solution under the time limit  and thus we were often not able to compare solution lengths.  additionally  neither dfid or best-first search could find solutions for any problems in the two larger test sets.  for the first test set  scope was able to significantly improve final solution quality and always generated optimal solutions. for the second test set  only 1 problems could originally be solved under the time limit and these solutions were already at optimal length. 
1 	related work 
most systems that learn control knowledge for planning have been directed at improving planning efficiency. in this regard  the most closely  related system to scope is ucpop+ebl  kambhampati et al.  1 . in contrast to scope  which uses a combination of ebl and induction  ucpop+ebl uses a purely explanationbased approach to construct control rules in response to planning failures. this system has been shown to improve planning efficiency  however  scope has outperformed ucpop+ebl in previous experiments using the blocksworld domain  estlin and mooney  1 . most other research on learning control rules to improve planning efficiency has been conducted on linear  state-based planners.  minton  1; etzioni  1; leckie and zuckerman  1; bhatnagar and mostow  
1 . 
　very little research has been done in learning rules that improve plan quality. one learning mechanism for improving the quality of plans was introduced by  perez  1  and is built on top of the prodigy nonlinear planner  carbonell and et al.  1 . it uses ebl and an input quality evaluation function to explain why a higher quality solution is better than a lower quality one and converts this information into control knowledge. this method has been successfully used to improve solution quality in the process planning domain  gil  1 . in contrast  scope uses a combination of learning techniques to learn control rules that cover good planning decisions and rule out bad ones. also  scope has been focused on improving both quality and efficiency. 
　another learning method for improving quality  which was also runs on the prodigy nonlinear planner  was developed by  iwamoto  1 . this technique uses ebl to acquire control rules for near-optimal solutions in lsi design. this method is similar to the one employed by perez  however it does not make use of the quality evaluation function to build the explanation. 
　most other work in plan quality has concentrated on adding features to the plan algorithm itself that prefer least-cost plans  hayes  1; williamson and hanks  1  or on examining goal interactions and how they relate to solution quality  wilensky  1; foulser et al.  1 . 
1 	future directions 
there are several issues we hope to address in future research. first  we would like to experiment with different types of quality metrics. we are currently working on implementing a version of the logistics domain where actions have different execution costs. in this way  we can measure plan quality in terms of plan execution cost as well as plan length. we would also like to experiment with the truckworld domain utilized by  williamson and hanks  1  where resource consumption is important  and the process planning domain  gil  1   where a number of quality metrics would be applicable. 
　finally  we would like to devise a method for incremental learning and training for scope. scope could use learned control information for smaller problems to help generate training examples for more complex problems. this type of incremental approach could help scope scale up more effectively to solve harder problems. 
	estlin & mooney 	1 

1 	conclusion 
this paper describes experimental results from learning control knowledge to improve both planning efficiency and plan quality. most planning and learning research has concentrated on improving efficiency. another important learning goal is to improve the quality of the final solution. however  little research has been done in learning rules that improve both these metrics. in this paper  we have presented results utilizing the scope learning system that show both of these metrics can be improved simultaneously by focusing the learning system on avoiding both dead-end paths and paths that lead to sub-optimal solutions. 
