
the paper presents a support vector method for estimating probabilities in a real world problem: the prediction of probability of survival in critically ill patients. the standard procedure with support vectors machines uses platt's method to fit a sigmoid that transforms continuous outputs into probabilities. the method proposed here exploits the difference between maximizing the auc and minimizing the error rate in binary classification tasks. the conclusion is that it is preferable to optimize the auc first  using a multivariate svm  to then fit a sigmoid. we provide experimental evidence in favor of our proposal. for this purpose  we used data collected in general icus at 1 hospitals in spain; 1 of these include coronary patients  while the other 1 do not treat coronary diseases. the total number of patients considered in our study was 1.
1 introduction
the available models for predicting outcomes in intensive care units  icu  are usually scoring systems that estimate the probability of hospital mortality of critically ill adults. this is the case of apache  acute physiology and chronic health evaluation   knaus et al.  1   saps  simplified acute physiology score   le gall et al.  1   and mpm  mortality probability models   lemeshow et al.  1 . the score functions of these predictors were induced from data on thousands of patients using logistic regression. the data required by these systems come from monitoring devices  clinical analysis  and demographic and diagnostic features of patients. so  apache iii includes age  1 acute physiologic variables that use the worst value from the first 1 hours in the icu  temperature  heart rate  blood pressure  respiratory rate  oxygenation  acid-base status  serum sodium  serum blood urea nitrogen  serum creatinine  serum albumin  serum bilirrubin  serum glucose  white cell count  hematocrit  itemized glasgow coma scale score  and urine output   preexisting functional limitations  major comorbidities  and treatment location immediately prior to icu admission.
　these prognostic models are mainly used to measure the efficiency of icu treatments. the risk stratification of patients allows comparison of the observed outcomes versus accepted standards provided by score functions. icu assessment is very important since it is estimated that end-of-life care consumes 1% to 1% of all healthcare costs. moreover  in 1 the average daily cost per patient in icus was about $1 in the usa  provonost and angus  1 . on the other hand  the literature also shows that prognoses have constituted an important dimension of critical care  as patients and their families seek predictions about the duration and outcome of illness  lemeshow et al.  1 .
　in this paper we propose a new method for learning probabilities that will be tested on the probabilities of survival in icu patients. the method makes intensive use of the socalled support vector machines  svm   a powerful family of algorithms for learning classification and regression tasks. when used for binary classification  svm learn hypotheses that return continuous numbers: positive values for cases of one class  and negative for the other class.
　on the other hand  to measure the performance of predictions in medicine  and in general when classes are very unbalanced  the misclassification rate  or accuracy  is usually inadequate. frequently  the area under a receiver operating characteristic  roc  curve  auc for short  is used. this amount can be interpreted as the degree of coherence between a continuous output  such as the probability  or the continuous output of an svm  and a binary classification. it is important to emphasize that that coherence is established in terms of orderings. for this purpose  continuous outputs or scores are used to rank available cases  while classes in the icu problem are codified by '+1' when the patient has survived  and ' 1' otherwise.
　in this context  hanley and mcneil  showed that the auc is the probability of a correct ranking; in other words  it is the probability that a randomly chosen subject of class '+1' is  correctly  ranked with greater output than a randomly chosen subject of class ' 1'. therefore  auc coincides with the value of the wilcoxon-mann-whitney statistic.
　additionally  there are other measures of the goodness of probability estimations; for instance  the brier score is the average of quadratic deviations of true and predicted probabilities. the relation between auc and brier scores was studied in  ikeda et al.  1 . however  the relationship found is guaranteed only under very restrictive conditions that are difficult to check in real world cases. moreover  the relationship is not always direct even in the case study reported in the experimental section of this paper.
　to learn a probability distribution using svm  it is crucial to transform their scores or continuous outputs into probabilities. but this is what a method presented by platt  does. the core idea is to fit a sigmoid  using a maximum likelihood procedure. the novelty of the proposal reported in this paper is that we postulate that to compute platt's sigmoid it is better to look for an optimum auc first than to minimize the error rate with a classification svm. for this reason  in section 1 we shall discuss how to optimize the auc with a support vector method  herbrich et al.  1; joachims  1 .
　the rationale behind our proposal is that the quality of the sigmoid fit depends on the quality of the ranking of the scores. if most of the cases with a higher score than a given one of class y have a class greater than y  then the task of the sigmoid can be easily accomplished  and the performance of the final probability is nearly optimal.
　at the end of the paper we provide experimental evidence in favor of our proposal  comparing it with other alternative approaches. for this purpose  we used data collected in general icus at 1 hospitals in spain  1 of which include coronary patients  while the other 1 do not treat coronary diseases. the total number of patients considered in our study was 1  1% of whom did not survive.
1 predicting probabilities
in this section we shall start off by reviewing a standard method for learning probabilities based on support vector machines to then present our proposal. but first of all we must realize that the performance of classification learners is not satisfactory in the icu problem; otherwise  nobody would turn to probabilities. this is a general situation in medicine  as well as in other fields; accurate crisp predictions are difficult to make  but some useful knowledge can be drawn from data.
　the section will end with the description of a straightforward approach for learning probabilities using regression. this method will be used as a baseline for measuring the merits of the other options.
1 the goodness of probability predictions
let s = { x1 y1  ...  xn yn } be a training set for a learning task in which a function  or hypothesis  is sought that is able to return outputs yi from entries xi of an input space x. an important issue when we are learning is to fix the way in which we are going to measure the quality of the result. in fact  given s formally  the aim of learning is to find a hypothesis h  from a given hypothesis space  that minimizes the average loss extended over the set of independently identically distributed  i.i.d.  test sets s  usually represented by
.
　in the icu problem  training and test examples have no probability attached  they are labeled with +1 or  1.
therefore  we shall assume that the true class probability  x   is 1 when the class of x  y  is +1 and
　in general  when predictions are discrete probability distributions  there is basically one standard loss function: the average quadratic deviation. if there are two possible outputs  the probability loss is given by
	 pi 1	 1 
where the hypothesis h returns the estimation of the probability	stands for the observed
|xi .
　the measurement in equation  1  is frequently used in medicine and meteorology  and is known as the brier  index or score. if the number of possible outputs is greater than two  the estimated probabilities can be seen as a vector  and the mean square of the euclidean  mse  distance from predicted and observed probabilities is then used; see  for instance  melville et al.  1 . it can be seen that  in the icu problem  mse is 1 times the brier score.
1 optimizing accuracy plus a sigmoidal transformation
the straightforward approach to the icu problem is a binary classification svm followed by a sigmoid estimated using platt's method . thus  given the training set s  we can use a transformation φ defined from input entries in x into a feature space h  where classes should be mostly separable by means of a linear function. as is well known  h must have an inner product  and
		 1 
is called the kernel function of the transformation. we shall use the rbf kernel that is defined by
		 1 
　the work of the svm consists in solving the following convex optimization problem:
n
		 	 1 
	s.t.	 
then  the classification is accomplished by the hypothesis
		 1 
　it can be seen that the kernel and the vector α =  αi : i = 1 ... n  of lagrange multipliers define the implementation of function  1  computed from input space entries x as follows:

　according to  1   the aim of this function is to maximize the margin  between classes  and to minimize the training loss. in fact  the sum of the so-called slack variables 
  is an upper bound of misclassifications of  1  on the training set. it is acknowledged that the function  1  so achieved has good classification accuracy on unseen cases.
　in order to compute the probabilistic outputs  we get rid of the sign function  and we only consider the continuous outputs

platt's method then fits a sigmoid to estimate probabilities:
		 1 
　figure 1 depicts the fit of this sigmoid to the dataset of all patients  1  at all the available hospitals. notice that the fac values follow a bell-shape distribution with most individuals having positive values  which means that they have a survival prediction.
1 optimizing the auc first
when classification predictions are made comparing the values returned from patients' descriptions x by a rating function with a threshold  as in classification svm  see equation  1    then the performance of these predictions can be assessed using the auc. according to its probabilistic interpretation  the complementary of this amount  1-auc  can be used as a loss function. thus  if g is a hypothesis  its loss evaluated on a test set s is
		 1 
　let us stress that the explicit objective of svm presented in the preceding section is not to minimize equation  1 .  cortes and mohri  1  provide a detailed statistical analysis of the difference between maximizing the auc and minimizing the error rate in binary classification tasks.
　herbrich et al.  presented a direct implementation that solves a general ranking problem that is applicable to maximizing the auc. the core idea is that if a hypothesis f : φ x  ★ r is linear and has to fulfill that f φ xi     f φ xj    since yi   yj  then
	f φ xi     f φ xj     f φ xi    φ xj     1.	 1 
　notice that this statement converts ordering constraints into classification constraints  with one class   but now the input space is x 〜 x and each pair  xi xj  is represented by the difference φ xi    φ xj . according to this approach  the aim is to find a hypothesis such that w solves the following convex optimization problem:
		 1 
	s.t.	 
　for each x of the input space  the hypothesis so found returns

where αi j are again the lagrange multipliers computed by the optimizer.
　unfortunately  this approach leads to dealing with one constraint for each element of the dataset
	s． = { xi xj;+1  : yi = +1   yj =  1}	 1 
whose size is the number of positive  class +1  examples times the number of negatives  #pos〜#neg  i.e. o n1  when the size of s is only n. this means that some applications become intractable  although the approach  or a simplified version of it  has been successfully used on other occasions  joachims  1; bahamonde et al.  1 .
　to alleviate the difficulties caused by the size of data sets  it is not straightforward to reformulate herbrich's approach as an optimization problem with a small number of constraints. the main problem is that the loss function  1-auc   see equation  1   cannot be expressed as a sum of disagreements or errors produced by each input xi.
　following a different procedure  joachims  recently proposed a multivariate approach to solve this problem with a convex optimization problem that converges using only a few constraints.
the optimization problem is:
		 1 
	s.t.	
　despite the enormous potential number of constraints  the algorithm proposed in  joachims  1  converges in polynomial time. moreover  it only requires a small set of constraints. however  the most interesting result is that the solution w of problem  1  is also the same as that of the optimization problem  1 . additionally  the slack variables in both cases are related by
		 1 
　finally  the multivariate svm returns a function fauc of the form
	.	 1 
　then platt's method can fit a sigmoid to transform the output of fauc into a probability.


	1	1	1	1
figure 1: the fit of the sigmoid to the dataset of all patients  1 . the horizontal axis represents the outputs of an svm. each '*' mark is the average posterior probability for all examples falling into a bin of width 1. the sigmoidal function is the estimation computed by platt's method   the output values are labeled on the left vertical side   while the bell-shaped function is the histogram for pr f x   for all the examples  frequencies are labeled on the right .1 regression is a baseline approach
considering that probabilities are real numbers  regression algorithms must be a first attempt to learn them. for this purpose  all training examples of class  1 are labeled as 1.
　in order to maintain the uniformity of approach with preceding subsections  we considered the regression based on support vectors  therefore we used the so-called support vector regression  svr . although there are least squares svr  we used the standard version; i.e. a learner of a function
		 1 
where k is once again the rbf  1  kernel  and αi are the lagrange multipliers of the solution to the convex optimization problem:
n
		 	 1 
	s.t.	 
　however  given that nothing forces fre  1  outputs to be in  1   we set the hypothesis output to 1 whenever fre returns values above 1  and 1 for fre values below 1. in symbols  finally we have the hypothesis
	hre x  = max{1 min{1 fre x }}	 1 
1 experimental results
using a collection of data sets of survival probabilities in critically ill patients  we carried out an experimental comparison of four different learning approaches. svm followed by platt's fit of a sigmoid: the accuracy optimizer described in subsection 1  which will be represented by svm accu ; the multivariate version  aimed at optimizing the auc  subsection 1   for short svm auc ; and finally the regression approach  svr  subsection 1 . the fourth predictor used was the commercial system apache iii; we used the customization described in  rivera-fernandez＞ et al.  1  that was developed to improve its performance in spain.
　first of all  we have to point out that this is an unfair comparison  since apache iii was trained with a cohort of 1 patients from 1 different hospitals in the usa  knaus et al.  1 ; the spanish version used records of 1 patients from 1 icus; while the available data sets in our experiments only included 1 patients. nevertheless  this comparison is useful to test whether or not the scores achieved by svm methods are good enough to be considered for future learning tasks.
　to estimate the performance of the algorithms described in the preceding section  we used data collected from icus at 1 different spanish hospitals  1 of which include coronary patients. it is acknowledged among the medical community that coronary diseases generally have a lower mortality risk than other critical illnesses. so from a learning perspective  it makes sense to differentiate between icus with and without coronary patients.
	svm auc 	svm accu. 	svr	apacheiii
	# patients	hospitals	bs	auc  % 	bs	auc  % 	bs	auc  % 	bs	auc  % 
	1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
1	1	1	1	1	1	1	1	1
1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
	1	1	1	1	1	1	1	1	1
	averages	1	1	1	1	1	1	1	1
	1	{1 1}	1	1	1	1	1	1	1	1
	1	{1 1 1}	1	1	1	1	1	1	1	1
	1	all	1	1	1	1	1	1	1	1
table 1: brier scores  bs  and auc estimated by a 1-fold cross-validation for the three learners described in the text  and for the commercial system apache iii. all differences from svm auc  are significant according to a one tail t-test with threshold p   1  considering the results on the 1 hospitals. for ease of reading  auc scores are represented as percentages.the data were organized in 1 different training sets  one for each single hospital  two collecting the data from not coronary/coronary icus respectively  and the last one containing all the data. each patient in these data sets was described by the same set of variables used by apache iii. however  given that some of these variables have discrete values  we had to transform them to be handled by svm-based systems. thus  we codified each discrete variable using as many new binary variables  with values 1 and 1  as the number of possible values of the original variable  setting only the variable corresponding to the discrete value actually taken by the original variable to '1'.
　performance estimations were made using a 1-fold stratified cross-validation on each of the data sets  for all the algorithms except for apache iii; since it was already trained with a different data set  we used the available data to test its predictions. additionally  the data was standardized according to the mean and deviation observed on each training fold.
　it is important to recall that the auc achieved by the spanish version of apache iii in our experiments  1%
 in percentage  is similar to the amount reported by riverafernandez＞ et al. : 1%. this fact supports the representativeness of the sample of critically ill patients considered in the experiments described here.
　as usual  when dealing with svm  the parameter setting stage is very important. to set the regularization parameter c  see optimization problems in section 1  and the rbf kernel parameter σ  see eq.  1   in the three support vector based algorithms  we performed a grid search on a validation set formed by the patients at 1 hospitals: one hospital without coronary patients  1   and 1 with coronary patients  1 and 1 ; see table 1. the ranges searched were the following: for c we tested values from 1 to 1 varying the exponent in steps of 1; for σ we tested values from 1 to 1 varying the exponent in steps of 1. we found that the most promising values were c = 1 and σ = 1 for svm accu  and svr; and c = 1 and σ = 1 for multivariate svm auc . it is worth noting that for svm and svr the parameter search was aimed at minimizing the brier score  while for multivariate svm it was aimed at maximizing the auc.
　table 1 shows the results obtained  brier score and auc  in the experimental setting described above. focusing on the results obtained by the three support vector algorithms  we can observe that  in general  the best performance  lowest bs and highest auc  is achieved by multivariate svm auc . the differences are statistically significant according to a one tail t-test with threshold p   1. this should not be surprising for the auc measure  since this algorithm was specially devised to optimize such a measure. but it also outperforms svm and svr in terms of the brier score  whose parameters were set to optimize this score.
　let us stress that  although the optimization problem posed to svr is precisely the minimization of the distance between true and predicted probabilities  a large amount of data is required to tie the scores of svm auc  in the brier score. the underlying reason explaining this behavior may be that the hypothesis space used by svr is not adequate so as to induce probability distributions from a reduced set of training data  even with an rbf kernel.
　as regards the data sets used in the experiments  support vector machines yielded the worst performance on the first three data sets  i.e. the smallest. svr performance was particularly poor on these data sets. considering that the rows of table 1 are in ascending order of size of the data set  the trend indicates that performance could be improved if more training cases were available. in fact  when the data set included all available patients' records  the results obtained were similar to those yielded by apache iii  recall that it was trained with data sets that were several times bigger . on the other hand  we also observe that survival predictions seem to be slightly harder for icus without coronary patients  hospitals 1  1  1 and 1  than for icus including coronary patients.
1 conclusions
we have presented a learning method for estimating probabilities in a real world problem: the prediction of survival in critically ill patients. however  the approach is general enough to be applied to other learning tasks. the method is an alternative to the standard procedure when the learning machine is based on support vectors and uses platt's method  platt  1  to fit a sigmoid. instead of using an svm devised to optimize classification accuracy  we propose to use a learner that optimizes the area under the roc curve  auc . this can be done using a multivariate svm described in  joachims  1 .
　we experimentally compared the results obtained by this method with other approaches  and with a commercial scoring system trained with thousands of cases  apache iii  knaus et al.  1; rivera-fernandez＞ et al.  1 . in the reported experiments  we used real data from 1 icus at hospitals in spain that contain records from 1 patients. the medical description of each patient includes monitoring variables  clinical analysis  and demographic and diagnostic features.
　the method proposed here outperforms the standard svm approach  especially when the available data is scarce  which is the usual situation. on the other hand  increasing the number of training examples reduces differences in performance; even between probability predictions of apache iii and those made by the baseline method  a simple regression with the output trimmed to the interval  1 .
references
 bahamonde et al.  1  antonio bahamonde  gustavo f. bayon  jorge d＞ ＞ ez  jose ram＞ on quevedo  oscar lu-＞ aces  juan jose del coz  jaime alonso  and f＞	elix goy-＞ ache. feature subset selection for learning p