 
many abductive understanding systems explain novel situations by a chaining process that is neutral to explainer needs beyond generating some plausible explanation for the event being explained. this paper examines the relationship of standard models of abductive understanding to the case-based explanation model. in case-based explanation  construction and selection of abductive hypotheses are focused by specific explanations of prior episodes and by goal-based criteria reflecting current information needs. the case-based method is inspired by observations of human explanation of anomalous events during everyday understanding  and this paper focuses on the method's contributions to the problems of building good explanations in everyday domains. we identify five central issues  compare how those issues are addressed in traditional and case-based explanation models  and discuss motivations for using the case-based approach to facilitate generation of plausible and useful explanations in domains that are complex and imperfectly understood. 
1 introduction 
the success of abductive understanding systems depends on their methods for performing the two fundamental steps of abductive explanation: generating candidate explanations and choosing between competing alternatives. many models of abductive understanding construct explanations by chaining from scratch in a process that is neutral to any system goals beyond simply generating some plausible explanation for the event being explained. however  applying that method in the rich domain of everyday events is impeded both by well-known problems of explanation construction cost and by another crucial problem: that the goodness of explanations depends not only on their validity but also on whether they provide the information 
   *i would like to thank ashwin ram for helpful discussions of these issues and the ijcai reviewers for their useful comments. 
automated reasoning 
that the explainer needs  leake  1a; leake  1; ram and leake  1 . 
　the need for explanations to reflect explainer goals both complicates explanation selection and potentially increases the number of candidate hypotheses that must be generated. it is not sufficient for an explainer with overarching goals to generate any valid explanation; the explainer must generate a valid explanation that provides the information it needs. although in restricted domains it may be possible to limit system rules in such a way as to assure that all explanations will be relevant to system goals  this is not possible in general. in a system with a rich knowledge base that builds everyday explanations  focusing explanation is a key problem. 
　this paper examines the significance of the casebased explanation process  e.g.   kass and leake  1; leake  1; schank  1; schank and leake  1   and its theory of explanation evaluation  e.g.   leake  1a; leake  1   as methods for focusing construction and selection of abductive hypotheses. in the case-based approach  which has been applied to the task of abductive understanding of anomalous events in news stories  explanation construction by chaining is replaced by analogical reasoning focused by three types of context: system beliefs and expectations  prior episodes that the system has explained  and the goals that currently motivate explanation. case-based explanation builds new explanations by retrieving stored explanations for previous episodes and adapting them to fit current circumstances and needs. the case-based process models some aspects of human explanation  read and cesa  1   and  as we describe in the following sections  facilitates generation of plausible and useful explanations despite the problems of incomplete information and imperfect domain theories that mark everyday explanation. 
　because detailed descriptions of the mechanisms involved have been published previously  this paper will not discuss specifics of how those mechanisms make casebased explanation construction practical. instead  its goal is to delineate the relationship of this abductive reasoning process to other models of abductive understanding. 
　we begin with a brief overview of the case-based explanation model. we then identify five central issues in abductive understanding of everyday events  sketch how those issues are addressed in traditional and casebased explanation models  and discuss motivations for using the case-based approach to facilitate explanation in complex and imperfectly understood domains. 
1 overview of case-based explanation construction 
the case-based model of abductive understanding was originally investigated in swale  a story understanding system that uses case-based reasoning to generate abductive explanations of anomalous events  kass and leake  1; schank  1; schank and leake  1 . specific aspects of the model were further refined in swale's descendents abe  kass  1  and accepter  leake  1 . these systems use case-based reasoning to focus explanation of real-world events. rather than assuming perfect knowledge  their explanation effort applies limited background knowledge to the incomplete information provided in simple news stories. the goal of their processing is to generate explanations that are plausible and that provide adequate information for particular types of overarching goals. examples processed by accepter include the explosion of the space shuttle challenger  the accidental shoot down of an iranian airliner by the american warship vincennes  and the namesake example of swale: the story of a star racehorse named swale who was at the peak of his career when he was found dead in his stall. 
　in the swale example  a vast range of possible causes could be hypothesized to explain the death-given the lack of information in initial news reports  almost any cause of death was a potential candidate. however  people confronted with the swale story appeared to have little difficulty generating plausible hypotheses. as they generated these hypotheses  they often attributed the hypotheses to being reminded of prior episodes. for example  one person was reminded of the death of the runner jim fixx  who died when the exertion of recreational jogging overtaxed a hereditary heart defect. the explanation for fixx's death does not apply directly to swale-swale was unlikely to do recreational jogging- but minor adaptation of the explanation  substituting horse racing for jogging  produces the plausible explanation that the stress of running in a race overtaxed a hereditary heart defect. this example and similar informally-collected accounts helped to suggest the casebased model of explanation used by the swale system  in which new explanations are built by retrieving and adapting explanations of previous episodes. later psychological experiments support the psychological validity of the reminding-based explanation process and the tendency of people to favor explanations that are based on prior explanations of similar episodes  read and cesa  1 . 
　in somewhat more detail  the basic steps of swale's case-based explanation algorithm are as follows: 
  problem characterization: describe the infor-mation that a good explanation must provide  using the same vocabulary used to index explanations stored in memory. 
  explanation retrieval: use the problem charac-terization as an index for retrieving relevant explanations of prior episodes from memory.1 
  explanation evaluation: evaluate the retrieved explanations' plausibility and usefulness. generate problem characterizations for any problems. 
  explanation adaptation: if problems were found  use the evaluator's problem characterization to select adaptation strategies  kass  1  for modi-
fying the explanation to repair the problems. apply the strategies and return to the explanation evaluation phase to evaluate the new explanation. 
in order to establish this algorithm as a viable alternative to standard abductive methods  the effectiveness of each phase of the process must be substantiated. that substantiation is beyond the scope of this paper but has been provided elsewhere: problem characterization and retrieval issues are addressed in  leake  1b; leake  1   explanation evaluation issues are addressed in  leake  1a; leake  1   and adaptation issues are addressed in  kass  1 . 
1 perspective on the case-based model 
to delineate the ramifications of the case-based model and its relationship to other methods for abductive understanding  our central focus is to examine and put into perspective the tenets of the case-based model concerning five key issues:  1  how to build explanations   1  the nature of explanations   1  the plausibility evaluation process   1  the role of anomalies in focusing an understander's explanation  and  1  the influence of overarching goals on explanation. 
1 	building explanations 
explanation 	construction methods: 	in abduc-
tive understanding systems  standard theorem-proving chaining techniques are generally the mechanism for generating candidate explanations  e.g.   hobbs et al.  1; kautz and allen  1  . a well-known problem for these methods is the cost of explanation construction  due to the combinatorial explosion of alternatives to consider. methods have been proposed to control chaining cost  e.g.   charniak  1; hobbs et al.  1    but despite the benefits of these methods  the difficulty of efficiently generating explanations remains acute in rich domains. case-based explanation addresses this problem by adapting prior explanations to new situations  to avoid the cost of chaining from scratch. 
　the case-based approach to re-using explanations contrasts with another method that learns to facilitate explanation  explanation-based schema acquisition  e.g.   mooney  1  . when confronted with novel situations  explanation-based schema acquisition builds 
1
　　 swale  accepter and abe all start with libraries of stored explanations. in general  a case-based explanation system's initial explanation library could be provided by external sources  e.g.  by being told explanations or reading about explained episodes  or built up by traditional chaining methods  and the library is augmented by storing new explanations that the case-based explanation process generates. 
	leake 	1 
new explanations by chaining and then immediately uses explanation-based generalization of the explanatory chain to form a general schema for future use. instead of generalizing new explanations when they are first generated  case-based explanation construction stores variablized versions of the specific explanations. when those explanations are applied to new situations it adapts them to fit the new situation  using adaptation strategies that include not only generalization but also deletion  addition and replacement of components of the explanation  kass  1 . consequently  case-based explanation can apply prior explanations to a wider range of circumstances that explanation-based generalization. although a stored explanation can potentially be applied to an enormous range of new situations  processing effort is controlled because adaptation is only done to the extent needed to explain a specific new situation. 
   as an example of the contrast between the case-based approach and explanation-based schema acquisition  we return to the swale example. when the swale system explains swale's death  one of the explanations it retrieves concerns the death of another young superstar  janis joplin. joplin was driven to recreational drug use by the stress of being a star and the availability of recreational drugs  and she died from an accidental drug overdose. an explanation-based schema acquisition program that had previously explained joplin's death would have generalized the explanation for joplin's death at the time it was built  to form a general schema such as  stress and access to drugs can lead to death from accidental overdose.  however  such a generalization does not apply to swale. consequently  the explanation-based schema acquisition system could not apply it and would chain together a new explanation without guidance from the joplin explanation. 
   in swale  the applicability of a new explanation is not bound by a precomputed generalization. instead  after retrieving the specific explanation of joplin's death it attempts to decide how to adapt it in light of the particulars of the episode-swale's death-to which it will be applied. during adaptation  swale abandons the parts of the explanation that do not apply and retains the kernel of the explanation that is potentially applicable: the hypothesis that a drug overdose caused the death. that hypothesis is unsupported  so adaptation takes drug overdose as a starting point  considerably narrowing the field of options to consider  and seeks additional support. because its knowledge includes that racehorses are sometimes given performance-enhancing drugs  it generates the explanation that swale might have died from an accidental overdose of performanceenhancing drugs. thus the case-based approach uses experience to suggest alternatives even in situations that are not straightforwardly subsumed by generalizations of prior explanations  allowing more flexible reuse of the results of prior explanation construction. this process depends on having effective strategies for guiding adaptation  and a library of such strategies is proposed in  kass  1 . 
   we note that during the adaptation process  casebased explanation uses incremental evaluation of hy-
automated reasoning 
potheses to decide how to proceed further. this approach differs from models of abduction in which all explanation construction is assumed to precede any evaluation  and is in the same spirit as recent research on choosing which explanations to pursue according to validity estimates  e.g.   dekleer and williams  1; ng and mooney  1 . however  the case-based model uses on-going evaluation not only to choose which explanation to pursue  but  because the evaluation suggests the type of information to seek to repair a faulty explanation  to provide very specific guidance of how to proceed when augmenting or modifying candidate explanations. 
ramifications for q u a l i t y o f explanations i n i m perfectly u n d e r s t o o d d o m a i n s : explanation-based generalization is guaranteed to produce correct generalizations even from a single episode  provided it starts with a perfect domain theory. unfortunately  domain theories of the unrestricted everyday world are unavoidably imperfect. the case-based method generalizes and adapts explanations only to the extent required by new examples  and verifies the reasonableness of its results by evaluating those explanations in the current situation. this reduces the danger of faulty generalization. 
   in addition  the case-based approach helps to choose between competing explanations licensed by an inconsistent domain theory. by favoring explanations supported by specific similar experiences  case-based explanation takes advantage of regularities in the world-similar events are explained in similar ways-even if those regularities are not fully captured by the explainer's domain theory. previous research on overcoming imperfect theory problems focuses on methods for repairing an imperfect domain theory when problems arise  e.g.   dietterich and flann  1; rajamoney  1  ; our focus is how to generate reasonable explanations despite possible imperfections in the domain theory. 
1 	t h e n a t u r e of e x p l a n a t i o n s 
the need of everyday explainers to use imperfect information is reflected in how case-based explanation views the nature of explanations. in most ai views of explanation  explanations are treated as deductive proofs. abductive reasoning systems build their proofs by nondeductive methods  and additional assumptions may be required for those proofs to apply. however  their view is fundamentally the same in that if the abductive assumptions were shown to be true the resulting explanation would be considered a deductive proof. 
   the case-based approach  however  explicitly treats explanations as plausible reasoning chains that may be imperfect. in the case-based model  explanations are represented as explanation patterns  xps   schank  1 . xps trace a chain of reasoning that accounts for why antecedents of the xp provide support for belief in the consequent of the xp  but the reasoning chain is not considered to prove that the consequent must necessarily hold.  in this respect xps are in the same spirit as pearl's plausible inference networks  pearl  1 .  consequently  decisions about the plausibility of an xp in a given situation depend both on its abductive assumptions  as in standard models  and on how well the internal derivation of the belief-support chain of the xp applies in the current situation  leake  1 . 
1 	p l a u s i b i l i t y e v a l u a t i o n 
in case-based explanation  the first criterion for selecting likely explanations is experience in similar situations: explanations of new situations are considered most plausible if they have applied in similar prior situations. however  additional plausibility evaluation is needed to verify adapted parts of the retrieved explanation and to determine the plausibility of the explanation's assumptions and reasoning chain in the new circumstances. 
   in abductive reasoning systems  the dominant method for judging plausibility of explanations is to favor explanations that are in some sense structurally  minimal   e.g.  charniak  1; kautz and allen  1  . these methods stress factors such as the number of abductive hypotheses rather than their content. instead  the casebased model relies primarily on the content of the assumptions and explanatory chain. its approach to judging plausibility of assumptions is in the spirit of systems using probabilistic criteria  e.g.   charniak and goldman  1    but the situations it attempts to explain are situations for which probability information is unlikely to be available. consequently  the case-based approach evaluates individual parts of an explanation by similarity: it compares assumptions to standard patterns and considers the assumptions likely if they match those patterns. similarity-based methods are not guaranteed to parallel correct probabilities and can sometimes lead to errors  but they are heuristics that people appear to use to estimate likelihoods when probabilities are unavailable  kahneman et a/.  1 . a full description of our model's plausibility evaluation can be found in  leake  1 . 
1 t h e role of anomalies in focusing e x p l a n a t i o n 
standard abductive understanding systems take a neutral view of the events they explain: given an event to explain  they either accept any chain accounting for the event or always seek explanations focusing on a fixed aspect of the event  e.g.  always trying to explain in terms of the goals that an action satisfies  mooney  1  . in such systems  decisions of which explanation to select are based entirely on plausibility of the candidates 
 e.g.   charniak  1; charniak and goldman  1; hobbs et a/.  1; kautz and allen  1; ng and mooney  1  . 
   however  plausibility alone is not sufficient to distinguish between candidate explanations-many different valid explanations can be generated for any event. one possible response is to attempt to pursue explanations from all perspectives  ajjanagadde  1   but this aggravates the already expensive explanation task. instead  case-based explanation takes the view that considerations of information needs should determine the aspects of an event to explain. 
   for example  consider a few of the many possible explanations for the statement  john used a blowtorch to break into an automatic teller machine  atm . : 
  john needed money to pay back a loan shark for gambling debts.1 
  john believes that robberies of atms are more likely to succeed than bank robberies. 
  mark  a gang member who was originally expected to do the break-in  was sick  forcing john to replace him at the last minute. 
  the bank's security camera had been removed for repair. 
these examples show that plausibility considerations alone are inadequate to select a good explanation: all four explanations might simultaneously be valid  but their goodness depends on explainer motivation for explaining. for example  if the explanation effort was motivated by being surprised that john performed the robbery instead of mark  the explanation that mark was sick would be relevant but the explanation that the security camera was broken would not. if the explainer was surprised that the break-in succeeded despite bank precautions  the explanation that the security camera was missing would be relevant and mark's illness would be irrelevant. explanation construction processes that fail to reflect the reasons for explaining are doomed to impartially generate irrelevant candidates as well as relevant ones  exacerbating the problem of explanation construction cost and resulting in explanations that are irrelevant to the explainer's information needs. 
   in the case-based model  explanation retrieval is focused according to the explainer's information needs to resolve the anomalies that prompted explanation. acc e p t e r directs explanation search by retrieving explanations indexed as relevant to the anomalies prompting explanation  leake  1b; leake  1   and includes a processing phase that explicitly evaluates the relevance of candidate explanations to the anomaly being explained  leake  1 . 
   although standard abductive understanding systems do not attempt to select the aspects of an event on which to focus  at first glance it appears that their basic backwards chaining mechanism would be sufficient to allow explanation to be focused on resolving anomalies. for example  the query to be explained by a backwards chaining system could be the particular aspect of the event that was anomalous  such as john's decision or the failure of john to be seen during the break-in. however  this provides only part of the needed focus for actually resolving the anomaly. 
   for example  suppose the anomalous aspect of the break-in is that john was not seen  and suppose that the explainer tries to explain it by chaining from the query 
1
　　 we state the antecedents alone as a shorthand for the reasoning chain that makes up the complete explanation. in this example  the entire explanation might include the fact that loan sharks place their victims under duress to pay their debts; the inference that being placed under duress prompts a high-priority goal to obtain money; that robbery is a plan for obtaining money; that a step in robbery is forcibly gaining access to the money to be robbed  etc. 
	leake 	1 
  john was not seen during the break-in.  many different explanations can still be generated for this query  and their relevance would depend on the explainer's previous expectations. for example  the explanation  the lights were broken  is relevant if the understander thought that john would be seen because of a reasoning chain including that the a t m was well-lighted; it is not relevant if the understander already knew that the a t m was dark but also knew that the guard had night-vision goggles that would enable him to see john regardless of the darkness. thus relevance of an explanation depends not only on the aspect of the event being explained  which is easily captured by selection of the goal from which to do backwards chaining  but also on whether the derivation of that explanation provides information showing flaws in the understander's prior reasoning such as erroneous or overlooked beliefs. 
   in case-based explanation construction  it is possible to focus on candidate explanations that not only explain the right aspect of the event but also relate that aspect to likely flaws in prior system beliefs: stored explanations can be indexed by the anomalies they resolve  rather than only the surprising aspects of events   and only candidate explanations relevant to those anomalies need be considered. in  leake  1b; leake  1  we describe an indexing vocabulary for organizing a memory of explanations to facilitate this focused explanation retrieval. 
1 	t h e role of o v e r a r c h i n g goals 
the previous section shows that explanation must reflect system information needs in order to generate explanations that are useful for resolving anomalies. in addition to being driven by anomalies  however  everyday explanation is shaped by overarching goals. for example  a bank security officer may wish to explain the break-in in order to make future robberies less likely to succeed. achieving that goal requires an explanation of the breakdown in security. alternatively  the robber's parents may wish to understand the break-in in order to better understand the robber's character  which requires an explanation of his motives.  john needed money to pay back a loan shark  provides information useful for understanding why he committed a robbery-that he was under duress-while  the bank's security camera had been removed for repairs  does not. however  the second explanation is a better explanation for protecting the a t m in the future. 
   psychological evidence shows that people favor different explanations for an event if they have different goals. for example  subjects attempting to absolve themselves of blame will favor different explanations from those without that goal  snyder et a/.  1 . in general  in any multi-task system the only way to assure useful explanations is to explicitly evaluate their goodness according to current system goals  leake  1a; leake  1 . 
   although goal-based explanation selection is important regardless of the means used for explanation construction  the importance of goals beyond facilitating routine understanding has received little attention in 
automated reasoning 
research on other models of abductive understanding. however  the influence of overarching goals has begun to be investigated in research on abductive diagnosis 
 e.g.  for controlling large-scale diagnosis  freitag and friedrich  1  and for integrating diagnosis and response  rymon et al.  1  . in addition  much research in explanation-based learning  ebl  has addressed the question of what constitutes a useful explanation.1 however  although ebl has been applied to a wide range of tasks  such as object recognition  problem solving  and search control   within these tasks explanations have been used for a single purpose: forming rules for concept recognition.  see  keller  1  for a discussion of how diverse ebl systems can be placed within the concept recognition framework.  consequently  all these systems reflect the concept recognition task by making two basic assumptions about the form of explanations. first  they require explanations to be complete proofs showing sufficient conditions for concept membership. second  because the types of rules used within the derivations are irrelevant to concept recognition-only the antecedents are important to that task-they treat all explanations with the same antecedents as equivalent. 
   neither of those assumptions applies to the everyday explanation of events. first  complete explanations may not be necessary; partial explanations are sufficient for some tasks. for example  an explanation for preventing an undesirable event needs only to identify a single necessary condition for the event that is preventable in the future.  e.g.  a driver who knows that his car sometimes fails to start when it has been parked in the cold can prevent the problem on cold days by putting it in the garage  even if he does not know the other factors relevant to whether it will start.  in fact  everyday explanations are necessarily partial explanations; it is impossible to pro-
vide a complete account of the factors that are sufficient for an event to occur. in everyday explanation  it is vital for the explainer to be able to make a principled decision about which partial explanations to accept and to benefit from the information in those explanations even if more complete explanations are unavailable. 
   second  the goodness of everyday explanations often depends on their internal structure and the types of rules that they use. for example  even if an explanation shows that a disease can be predicted with absolute certainty  based on a set of environmental factors  that explanation will be worthless for developing a vaccine for the disease unless the explanation shows how those environmental factors cause the disease. 
   in  leake  1a; leake  1  we discuss ten disparate sets of requirements for good explanations that arise from different uses for explanations. a subset of these requirements has been implemented in accepter's explanation evaluation process. by basing evaluation on these criteria that dynamically reflect explainer goals  the system can direct explanation construction towards satisfying current needs and can decide when to accept partial explanations. 
1
　　 although that research focuses on deductive explanation  its usefulness criteria are equally applicable within an abductive framework. 

1 conclusions 
despite the difficulties of abductive explanation in complex and imperfectly understood domains  human explainers have comparatively little difficulty controlling the search for candidate explanations. case-based explanation is an explanation method modeled on human explanation that focuses the construction and selection of abductive hypotheses. it differs from standard models of explanation construction in building explanations by adapting specific explanations for prior experiences; by treating explanations as plausible reasoning  rather than as proofs; and by dynamically focusing explanation construction and evaluation to reflect goal-based explainer information needs. by combining reasoning based on specific experiences with focusing based on a theory of information needs and how they can be satisfied  case-based explanation provides guidance for explanation construction and facilitates generation of good explanations in domains that are complex and imperfectly understood. 
