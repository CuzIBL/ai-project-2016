 
difficult concepts arise in many complex  formative  or poorly understood real-world domains. high interaction among the data attributes causes problems for many learning algorithms  including greedy decision-tree builders  extensions of basic methods  and even backpropagation and mars. a new algorithm  lfc uses directed lookahead search to address feature interaction  improving hypothesis accuracy at reasonable cost. lfc also addresses a second problem  the general verbosity or global replication problem. the algorithm caches search information as new features for decision tree construction. the combination of these two design factors leads to improved prediction accuracy  concept compactness  and noise tolerance. empirical results with synthetic boolean concepts  bankruptcy prediction and bond rating show typical accuracy improvement of 1%-1% with lfc over several alternative algorithms in cases of moderate feature interaction. lfc also explicates latent relationships in the training data to provide useful intermediate concepts from the perspective of domain experts. 
1 	introduction 
we describe a learning method for complex concepts and practical results in the difficult financial domain of risk classification. financial data are collected in the form of observations and an overall assessment  often binary-to lend or not to lend . this constitutes an attribute-value representation  where each training example is described by an n-tuple of attribute values and a class value. a concept is an intensional description of the class; a learning algorithm constructs a hypothesis of the concept. learning that assumes class-membership varies little as hypothesis descriptions are modified slightly has been called similariiy-bastd  sbl . 
    this work was supported in part by nsf grant iri-1  a kpmg peat mar wick foundation award  a university of illinois research board award  and a beckman institute cognitive science and artificial intelligence fellowship. 
1 	machine learning 
shaw and gentry  have shown advantages of 
sbl over traditional financial classification methods such as discriminant analysis. some statistical approaches allow nonlinearity  but sbl is more readily comprehensible and facilitates human-computer interaction. while many concepts pose no problems for typical decision tree induction algorithms  complex domains often produce inaccurate trees. as shown later  c1 give low predictive accuracies for finacial concepts. other methods such as improved sbl and backpropagation also fare poorly. 
　financial risk classification is difficult because of interaction among the attributes. in a critique of current learning systems  rendell and ragavan  1  this volume  examine relationships among attribute interaction  real-world problems  and system requirements. 
　this paper explores a new algorithm called lfc  lookahead feature construction  which applies directed lookahead search to build decision trees and form new features. lfc uses several techniques to decide dynamically which paths to attempt  how far to pursue the search  and which new features provide the most concise hypothesis components. 
　section 1 details the algorithm. section 1 reports experiments comparing lfc with other methods  using controlled synthetic data  a bankruptcy concept  and a bond rating concept. on complex concepts  lfc is reasonably fast and much more accurate. in section 1 we also consider the value of the new feature constructions to a financial expert. consistent constructions support and extend the expert's own notions to promote a reliable model of the domain. 
1 	the lfc algorithm 
lfc  fig. 1  constructs new features using the ids information gain function o. its novelty lies in using a geometric representation for beam search with branch and bound  secs. 1 & 1 . the algorithm handles discrete-valued data representations  such as integers  booleans  and nominals. for continuous-valued representations  reals   lfc first intervalizes the data attributes  based on minimizing the class entropy in each interval  or on user-specified intervals . noisy training samples are handled through feature pruning and tree pruning. 
all three operators are given  fc chooses either conjunction or disjunction  whichever is better  to avoid repeating semantically equivalent features  since by demorgan's law  the set {-    is functionally complete. 
the constructed feature is pruned back by evaluating the number of misclassifications by the feature on an independent test sample  breiman et al.  1 . the last  usually least significant  attribute in the feature  which is a conjunction of attribute literals  is iteratively dropped  until either the error is minimized  or until a single attribute is left. 
after constructing a new feature for a decision-tree node  lfc splits the data and iterates on each partition block  until either a specified mix of positive and negative examples or a minimum sample size for splitting is reached  stopping criterion . the tree is then pruned back based on test sample error minimization. 
	1 	constraining construction geometrically 
a key to guiding feature construction search is to relate the value of a feature to the individual  values of its component attributes. normally this relationship is not explicit  and it is possible for two low  attributes to produce a high  feature when conjoined. in this section  we introduce a mapping technique for making this relationship more explicit. 
the information gain of an attribute  or constructed feature  depends only on the number of positive and negative examples after splitting the data using the attribute  rather than on the exact examples themselves. 
 increases when the mix of positive and negative examples in each partition block decreases  and is maximum for partition blocks comprising purely positive and negative examples. the best features thus have high values. 
mapping each data attribute or new feature based on the counts of positive and negative examples covered by the attribute gives a geometric representation of their quality. a similar study using values was discussed by mehlsam   although his algorithm for constructing features moves objects from one partition block to another in a greedy manner  like the scheme of breiman et al. . in contrast  lfc maps the original attributes to a geometric representation  then applies beam search and branch-and-bound directly using this representation. this technique applies lookahead search  fo-figure 1: pseudocode for fc  top  and lfc  bottom  algorithms. 
　lfc first selects a subset of a attributes from the n original attributes for the beam. these a features are used as operands. the beam features are initially selected from the original attributes  and subsequently from both the original attributes and the constructed features. after a maximum search depth of / levels  the  best   highest  feature is selected for the decision node. 
　the feature construction operators are negation and conjunction  although disjunction may also be used. if cused by natural constraints arising in the representation to prune off large chunks of the search space. the following section details these new techniques to guide feature construction. 
1 	mapping features by utility 
let pmax and nmax be the total number of positive and negative examples in the training data. in figure 1  s is the plane bounded by  1  and  pmax nmax  with axes p and n. the line b given by   1   {pmax  nmax   bisects s into a lower half-plane l and an upper half-plane u. initially  every original attribute-value x is mapped to a point  p  n  on 1  where p and n are the number of positive and negative examples covered by this attributevalue. subsequently  the beam features are also mapped 
	ragavan et al. 	1 


construction. this neighborhood contains far fewer features than the space of all possible features  making it is easier to find good features that lie in the range 1 to 1. 
　nevertheless  the number of features could still be large; this is tackled by brand-and-bound search. fc chooses a feature from the beam as one of its two 
operands; for its second operand  fc selects an original attribute value closest to  p m a x  1 . suppose these two component attribute values x1 at  p1  n1  and x1 at  p  n   produce the feature f1 = xi  x1 the number of positive  or negative  examples covered by f1 can at most be p1  n1  or p1  n1   whichever is lower. therefore p f1  lies inside the rectangle of intersection r1 given by   q 1    min p1  p1   min n1 n1  . the lowest value inside r  occurs either at  min pi p1  1  or at 
 1  rnm n1  n1  . if this value is higher than the value at  p1  n   or  p1 n1   then the best  lowest  value of * for f1 is worse  higher  than the values at x1 or x1- this indicates that f  is worse than the better of z  or x1  so f1 is eliminated.  a dual technique may be used for 
	1	moreover  all features formed from 
conjunctions of f1 may be dropped because of this property. at every step of the beam search  if no new feature is better than the existing ones  feature construction is stopped  even before the search depth of/ is reached  automatically terminating search early. choosing the best attributes for the beam corresponds to choosing the 
steepest gradients on the surface. starting from a point p x  on 1 corresponding to an original attribute x  fc produces new features mapping to points that move closer toward p  eventually stopping at p. 
empirical analysis 
we compare several algorithms with lfc. in addition to hypothesis predictive accuracy  the performance criteria also include speed  which we omit in this short version of the paper {lfc speeds are competitive . our use of feature construction suggests other criteria  such as consistency and comprehensibility  factors that promote useful expert-system interaction. the algorithms we tested are standard sbl methods  ids  c1  and pls   
1 	machine learning 

various sbl improvements  greedys  fringe  and lfc   and also backpropagation  bp . we first consider synthetic data to allow control of attribute interaction. next  we present results in two difficult financial domains. results for these domains appear in in three respective subsections.  ragavan & rendell  give other results . 
1 	comparisons using synthetic concepts 
as a preliminary assessment for lfc and two other methods  we use controlled attribute interaction in boolean concepts. an m/n parity concept is an m-bit boolean parity function described with training examples using n bits  m   n . for fixed n  attribute interaction increases as m increases; the concept becomes more difficult to learn because the number of training examples is the same for all the concepts. 
　figure 1 shows results with three algorithms when learning m/1 parity concepts. for different m  figure 1 shows predictive accuracy. greedy1 uses depth-first search for greedy feature construction to construct decision lists  pagallo  1 . each point in figure 1 was obtained by averaging over 1 runs  using independent training and testing sets. each algorithm was trained using 1 examples  and tested using 1 unseen examples. in each run  the tree was inconsistent with at most 1 out of 1 training examples. 
　figure 1 shows that lfc produces more accurate trees compared with ids and greedys. as m increases  attribute interaction increases  so the average predictive accuracy decreases for all three algorithms because of the sparse training sample. although the rate at which all algorithms degrade may be controlled by adjusting the size of the training sample  the sample size was held constant to study this degradation with higher attribute interaction. this gives the fairly high degradation rate observed in figure 1  but lfc still maintains higher accuracy  and shows slower degradation than the other algorithms. 
　we also investigated the behavior of lfc and other methods using complex financial problems: bankruptcy 

prediction and bond rating. the next two sections detail these results. the data attributes were developed by experts using cash-flow theories  and represent situations of real companies. the last two authors of this paper have found that lfc constructed features that are identifiable and significant from the domain perspective. 
1 	comparisons using bankruptcy data 
numerous empirical models have been developed that use annual financial information discriminate firms that declare bankruptcy from these that remain solvent. lenders and investors want to improve their ability to explain  interpret  and predict bankruptcy. our study used 1 bankruptcy data; half of the companies went bankrupt in a given period while the other half were financially healthy during the same time period. a cash flow model was used to define the fifteen attributes  which include net operating flow  nof   financial costs  nfc   and change in inventory  invf   gentry et al.  1 . 
1.1 	prediction accuracy 
　table 1 shows results with the bankruptcy data  which was randomly split into mutually exclusive training and testing sets containing 1% and 1% of the examples respectively. table 1 compares the predictive accuracy of lfc with that of five other algorithms. each accuracy figure was obtained by averaging over several runs  1 in most cases  using mutually exclusive training and testing sets. the means are shown with 1% confidence intervals. 
   pls and fringe gave 1% and 1% accuracy  comparable to the prior class frequency estimate of 1%. greedys produced an accuracy of 1%. possibly because of tree pruning  c1 gave a much better accuracy of 1%. however  the variability in the accuracy of its trees is high  ＼ 1% . consequently  the improvement is not significant at p   1   -test. we also tested backpropagation with many parameter settings and net structures. the best choice was 1 input nodes  1 intermediate nodes  and 1 output. the best accuracy we could obtain was 1%. 
in contrast  lfc gave an average accuracy of about 
1%  improving over each of the other algorithms  p   1  *-test . tree variability was very low  ＼ 1% . we measured the relationship of accuracy to the amount of lookahead used by lfc. the lookahead depth was varied from 1 to 1. the accuracies were  respectively  
	ragavan et al. 	1 

1%  1%  and 1%  1% confidence intervals were ＼ 1 for / = 1  and ＼ 1 for / = 1 . with no lookahead  lfc's performance approaches that of pls. as lookahead depth increases  lfc makes more informed selections for a decision node  improving accuracy and consistency. 
1.1 	discovering meaningful features 
   in our experiments with the bankruptcy data  lfc reduced verbosity and compressed the decision tree from 1 nodes for pls to 1 nodes on the average. lfc also improved the abstraction of the decision trees by discovering new financial concepts  which capture specific risk characteristics of a company  shown in table 1 . one example from table 1 is inventory financing infi  which relates the change in inventory  invf  to the fixed coverage expenditures  fce   and the flows from operations  nof . infi abstracts the three low level attributes nof  fce  and invf. this feature is an intermediate concept  fu and buchanan  1  charac-
terizing the company's strategy to finance its inventory. infi is true for companies that need to finance an increase in inventory but avoid financing with long-term debts; table 1 summarizes the features that occurred most frequently in six typical trees for the bankruptcy concept. each feature is expressed in terms of both the original attributes it relates and also its corresponding real-world concept. 
1 	comparisons w i t h bond rating data 
bond ratings are used in the capital market as yardsticks for estimating potential yields. the bond rating data set had 1 samples  and three classes. the concept gave an average accuracy of 1% with c1 for all three classes. no other comparison algorithm did better  except backpropagation  which achieved 1% as the best of many varied trials. lfc  extended for multiple classes  did considerably better  with an average accuracy of 1%. 
　for the first class  high risk bonds   the average predictive accuracy of the lfc trees was 1%. the average size was 1 decision nodes. in all  1 boolean conditions using the original attributes were generated. because each new feature could contain up to three original attributes  the feature search space contained 1 x 1 x 1 = 1 possible features per node. this figure is the total number of choices  i.e.  the number in 1-ply exhaustive and naive lookahead. 
　in contrast  lfc evaluated only 1 features  which is about 1 features/node  an enormous reduction from 1. hence lfc searches less than 1% of the feature space. of these 1 features  lfc selected 1 for the final nodes of the decision tree. 
　in terms of accuracy  exhaustive lookahead is the only competitor of lfc. although lfc is slower than some methods  its times are reasonable  especially in the light of the many hours required for exhaustive lookahead  we compared lfc in full search mode and also buntine's ind . other techniques  including backpropagation  were little or no faster  and produced significantly and considerably inferior hypotheses. lfc is also superior with respect to human comprehension and expertcomputer interaction. 
1 	machine learning 
1 	discussion 
a typical divide-and-conquer splitter  e.g.  cart  breiman et al.  1  iteratively partitions the data using the single best attribute to create a decision node. although this greedy procedure is efficient  it may get trapped by local optima and degrade the quality of the induced decision tree  ragavan and ren dell  1 . this occurs when the concept has a high degree of attribute interaction  rendell and ragavan  1  this volume . attribute interaction increases the number of original attributes  conjuncts  required to specify meaningful subclasses at the leaves. attribute interaction also causes node replication  pagallo  1   which degrades accuracy  consistency  and comprehensibility. 
some algorithms have extended greedy splitters; e.g.  
fringe  pagallo  1  uses the decision tree produced by a greedy splitter to construct new features for improving tree quality. repeated fringe patterns near the leaves of the tree are coalesced to give new features  which are combined with the original attributes to build a new decision tree. the process iterates. but such strategies are insufficient when attributes interact much. this is because the original decision tree is greedy and because repeated decision patterns are distributed throughout the tree  rather than being confined to fringes. replications become complex and global. 
　one way to extend sbl splitters is to look ahead to observe the effects of current attribute selection further down in hypothesis construction. the best sequence may be found when all possible expansions of a decision tree are evaluated. this avoids problems of attribute interaction. norton  found fairly good improvement with an exhaustive lookahead algorithm idx  though it does not construct new features to cache the search knowledge. while splitters can construct better hypotheses with full lookahead search  naive lookahead search is expensive  and unnecessary in many cases. 
　a practical cure for attribute interaction is dynamic lookahead. lfc constructs features while building the decision tree. lfc can replace a greedy algorithm for the decision tree construction phase of fringe-like algorithms  but it largely prevents tree patterns from repeating in the first place. a sequence of decisions down any path of a decision tree is treated as a conjunction of attributes for constructing a new feature. the features are not evaluated individually  but instead as a group  effectively projecting multi-dimensionally  ragavan and rendell  1 . forming a new feature from the attributes economically caches this search information. 
   lfc guides lookahead search and feature construction by taking advantage of patterns in the training data and natural search constraints that arise in its geometric search representation. the algorithm deepens search only when necessary  stopping when it reaches the p axis  fig. 1 . the window filter is used for initial selection of candidate original attributes for feature construction  based on their quality. subsequently  the beam is chosen based upon where the features and attributes map in the geometric plane. variable beam width is a type of iterative broadening  ginsberg and harvey  1 . the distinction in lfc is the way it focuses on essential parts 

table 1: intermediate concepts created by lfc. 


of the search space and prunes irrelevant parts with its branch-and-bound. fc uses only the coordinates of the component attributes to determine whether to eliminate new features constructed from them  so the data are not split on a feature that may be discarded  or conjunctions thereof. ragavan and rendell  present other comparative results using lfc. 
　new features help to build a high-level knowledge structure for more accurate and comprehensible models. an important issue is closer interaction between the learning algorithm and the domain expert. in the complex financial domains of section 1  the higher-level hypotheses produced by lfc facilitate interpretation of results and help to refine important attribute relationships. 
1 	conclusions 
severe attribute interaction in real-world domains makes learning hard for similarity-based and other induction algorithms. we analyzed a global search technique for feature construction that improves learning for controlled synthetic and difficult natural financial domains. accuracy improves by as much as twenty percentage points. interpretation of the new features and decision trees by domain experts corroborates improvement in their abstraction. repeatability of the constructed features improves expert-machine interaction. 
　in lfc the complexity of feature construction is reduced by a combination of analytic and heuristic techniques. the algorithm is distinctive because it tackles both the attribute interaction problem  using directed lookahead  and the global replication problem  through feature construction . rendell and ragavan  1  this volume  detail these phenomena. 
lfc improves learning in several respects: 
  predictive accuracy of decision trees 
  consistency of the trees  measured by the variance in their predictive accuracy 
  reduction in the complexity of lookahead search for feature construction  through a geometric procedure   significance of the tree nodes  features  from a domain perspective. 
