 
at the heart of many optimization procedures are powerful pruning and propagation rules. this paper presents a case study in the construction of such rules. we develop a new algorithm  complete decreasing best fit  that finds the optimal packing of objects into bins. the algorithm use a branching rule based on the well known decreasing best fit approximation algorithm. in addition  it includes a powerful pruning rule derived from a bound on the solution to the remaining subproblem. the bound is constructed by using modular arithmetic to decompose the numerical constraints. we show that the pruning rule adds essentially a constant factor overhead to runtime  whilst reducing search significantly. on the hardest problems  runtime can be reduced by an order of magnitude. finally we demonstrate how propagation rules can be built by adding lookahead to pruning rules. this general approach - optimization procedures built from branching rules based on good approximation algorithms  and pruning and propagation rules derived from bounds on the remaining subproblem - may be 
effective on other np-complete problems. 
1 introduction 
when presented with a new combinatorial problem  how do we construct an effective optimization procedure  korf has demonstrated how to convert approximation algorithms for number partitioning into branching rules within optimization procedures  korf  1 . he concluded that this  presents an example of an approach that may be effective on other combinatorial problems. namely  we took a good polynomial-time approximation algorithm  and made it complete  so that the first solution found is the approximation  and then better solutions are found as long as the algorithm continues to run  eventually finding the optimal solution   korf  1 . we test this claim for a closely related problem  bin packing using the decreasing best fit approximation algorithm. 
     supported by epsrc award gr/k/1. we thank the members of the apes group  especially paul shaw. 
1 	search 
¡¡a shortfall of korf's proposal is that optimization procedures also benefit from pruning rules to terminate unproductive lines of search  and propagation rules to determine when branching decisions are forced. we identify a very general pruning rule derived from a bound on the solution to the remaining subproblem. the bound is constructed by using modular arithmetic to decompose the numerical constraints. since pruning rules need to be cheap to test  we describe an implementation that adds essentially a constant factor overhead to the runtime whilst significantly reducing the number of nodes explored. this pruning rule is more effective on harder and more constrained problems. we then show how propagation rules can be derived from pruning rules. finally  we derive similar pruning rules for other domains including number partitioning and knapsack problems. 
¡¡our pruning bound is based on reasoning about the numerical constraints of a problem using modular arithmetic. for example  can we pack objects with weights 1 1 1 into two equally sized bins leaving no empty space  the parity bit alone tells us this is impossible. as the sum of the weights is 1 and the two bins are the same size  each bin has 1 units of capacity. it is clearly impossible to pack objects with even weights into bins with an odd capacity and leave no empty space. in this paper  we generalize such reasoning to other bit positions and to non-binary bases. 
¡¡this research suggests a general strategy for building optimization procedures that may be useful in other np-complete problems. that is  we construct branching rules from good polynomial-time approximation algorithms  and pruning and propagation rules from bounds on the solution to the remaining subproblem. for combinatorial problems involving numbers  modular arithmetic may be able to decompose the constraints and suggest bounds useful for pruning and propagation. 
1 bin packing 
to explore korf's claim  we chose bin packing. there exist several good polynomial-time approximation algorithms for bin packing but few effective optimization procedures. bin packing is of practical and theoretical importance. many businesses like post offices have real bin packing problems to solve that are of economical value  slatford and yeadon  1 . problems such as scheduling  processor allocation  fabric cutting and minimization of vlsi circuit size and delay  can be modeled by bin packing problems. bin packing is np-complete in the strong sense  garey and johnson  1 . 
¡¡we consider a general bin packing problem in which the bins can have different capacities. this allows us to reason about the subproblems encountered during search when bins may be partially full. in addition  as we show in sec. 1  many other problems like number partitioning can be seen as instances of such general bin packing problems. a bin packing problem consists of a set of objects s and a set of bins b. each object s € s has a weight w$  and each bin b ¡ê b has a capacity cb. the difference between the sum of the bin capacities and the sum of the weights is the spare capacity  sc. the aim is to partition the objects between the bins so that the sum of weights in each bin is less than or equal to its capacity. all weights and capacities are integers. if all the bins have the same capacity  this is the np-complete problem sri from  garey and johnson  1 . 
¡¡decreasing best fit   d b f   is one of the best polynomial-time approximation algorithms for bin packing. best fit puts the next object into the fullest bin that will accommodate it without exceeding the capacity. d b f simply sorts the objects into decreasing order of their weight before calling best fit. objects with the largest weights are thereby packed first. dbf is guaranteed asymptotically to use no more than 1 times the optimal number of bins  compared to best fit which can use 1 times the optimal  garey and johnson  1 . 
1 a branching rule 
to develop an optimization procedure for bin packing  we follow korf's methodology  and convert the decreasing best fit approximation algorithm into a branching rule within the complete decreasing best fit   c d b f   optimization procedure. c d b f computes a lower bound on the number of bins required. if all the bins have the same capacity  c then we need at least  bins where is the sum of the weights. search begins with the num-
ber of available bins equal to this lower bound. if the search tree is exhausted before a packing is found  the number of available bins is incremented. this usually relaxes the constraints sufficiently to make packing easy. 
¡¡the objects are sorted into decreasing order of their weight and packed into bins by the branching rule. the first choice of the branching rule is that made by the decreasing best fit approximation algorithm. that is  it packs the next object into the fullest bin that accommodates it. to make the optimization procedure complete  we must decide what the branching rules does on backtracking. a natural generalization is to order bins by how full they are. the branching rule thus tries to pack the next object into the fullest bin that accommodates it  and on backtracking  tries bins in increasing order of their unused capacity. a branch terminates successfully if all the objects are packed into bins. a branch terminates unsuccessfully and forces backtracking when the next object cannot be packed into any bin. that is  when the weight of the next object exceeds the largest unused capacity. to avoid duplicating search  the branching rule does not pack an object into more than one bin with the same unused capacity. we therefore never open more than one new bin for any object. as a consequence  the first object is always packed into the first bin  the second object into the first or the second bin  etc. in the worst case  when packing n objects into 1 bins  c d b f explores all bn/b! different packings. 
1 a pruning rule 
a bin packing problem imposes constraints on how the weights in the bins add up. we decompose these constraints into constraints on how the bit positions add up. to do this  we take the modulus of the weights and capacities to a set of bases. often the bases will be all the powers of 1 since  as we show in sec. 1  we can then use simple logical operations on the binary representation. however  other bases can be used at little extra cost. 
¡¡the constraint on each bin is that the sum of the weights is no more than the capacity of the bin. to reason about how the spare capacity is distributed across the bins  we construct a set  containing sc  dummy  objects  each with a weight of 1. we now have to find an exact packing of so that the sum of the weights in each bin is strictly equal to the capacity of the bin. that is  for each 
to focus on different bit positions  we take the modulus of both sides of this equation to the base m  

and sum over the bins  

unfortunately  we cannot use this equation directly as we do not know which objects will be put in each bin. we can  however  give an upper bound for the left hand side of  1  assuming the worst possible partition of weights for the mod sum. to derive the bound  we use the fact that for any integers a and 1  

applying this repeatedly to the left hand side of     gives 

but the nested summation sums over all the weights in all the bins. hence  

	gent & walsh 	1 

this is our general pruning bound. the first two terms of the lhs of  1  represents the maximum contributions mod m that we can expect from the spare capacity and s. the third term represents the capacities mod m that we need these contributions to reach. if at any time  the lhs is less than zero  the packing of the bins is impossible. we can then prune search. 
¡¡as an example  consider again the bin packing problem from the introduction. this problem has no spare capacity. mod 1  we have objects with zero weight  i.e. the weights provide no parity bits   and two bins each with a capacity of 1  i.e. each bin needs a parity bit . the lhs of  1  is 1. since this is less than zero  we cannot pack the objects into the two bins. 
¡¡as a second example  can objects with weights 1  1  1  1 and 1 be packed into two bins of capacity 1  consider this problem mod 1. the ob-
jects contribute weights mod 1 of 1  1  1  1 and 1 respectively. in addition  we have 1 units of spare capacity. however  each bin needs 1 mod 1 = 1. the objects cannot therefore be packed into the bins. 
the pruning bound reflects this; the lhs of  1  is 
1 +  1 + 1 + 1 + 1 + 1  -  1 + 1 . as this is less than zero  we cannot pack the objects into the bins. 
1 implementation 
to implement checking the bound efficiently  we make three critical observations. first  if the moduli used are all powers of a given base  e.g. m  m1  m1 . . .     many computations from one power of the base can be reused in the next higher power. for example  with powers of 1   a mod 1     1 mod 1  if either the 1's digit of a is larger than that of 1  or if the 1's digits of the numbers are the same and  a mod 1     1 mod 1 . similar reuse of calculated values can be made for other operations such as longhand addition and subtraction. 
¡¡second  the lhs of  1  changes only slightly when an object is assigned to a bin. in fact  detailed analysis shows that if we put an object s into a bin 1  the value of the lhs of  1  does not change at all if  cb mod m   and is otherwise simply reduced by m. it is more efficient therefore to initialise the values of the lhs of  1  at the start of search  and merely compute the change to these values at each node. to restore the values on backtracking  we use one bit for each modulus m indicating if the lhs of  1  was decremented. 
¡¡these two observations combine together. we initially find the base m expansion of the weights and capacities. 
then at each node  we perform the subtraction cb - wb 
1 	search 
longhand in base m. in doing this  the values of the lhs of  1  in each power of m are decremented when there is a borrow in that digit position  since a borrow is necessary when  ws mod m     cbmod m . the complexity of the operations performed at each node is thus the same as that of the subtraction cb- w$  which has to be performed in any case. the only other operation is that of decrementing the lhs of  1  and incrementing it on backtracking. the lhs of  1  is always a multiple of m so we store the value divided by m and decrement it and increment it by 1 instead of m. since this value is a number bounded above by n  these decrements and increments take less than o logn  time. in practice n is small and it may take just a single machine operation. we can thus implement checking the bound in all powers of a given base with a constant factor overhead compared to c d b f without modular pruning. our experiments support this observation. 
¡¡our third observation does not affect the theoretical complexity but does make a practical implementation more efficient. a natural set of bases to use is all powers of 1. this is the set of bases used in the experiments reported here. this choice allows us to take advantage of the binary representation of numbers in computers. for example  finding the value of an integer modulus a power of 1 is very cheap. in addition  we do not need to perform the subtraction longhand since the bits of  a xor 1 xor a - b  indicate whether a borrow was necessary into a given bit position when computing a - b 
1 experiments 
as in previous experimental studies  mcgeoch  1   we pack objects with pseudo-random integer weights into bins that are twice their maximum size. we generate n objects each with a weight drawn uniformly and randomly from  1 /  and pack into bins of capacity 1/. exploratory tests with this model show large variation in problem difficulty. for example  the worst case for a sample of 1 bin packing problems with 1 objects of size 1 took 1 1 nodes whereas 1% of problems needed just 1 branch. the data suggests that problem difficulty is very dependent on the spare capacity. 
¡¡we therefore modify the model to generate problems with a predetermined spare capacity. we use two additional parameters: a lower bound on the number of bins  d and a spare capacity  sc. the new model constructs bin packing problems which  if they pack into d bins of capacity 1/  leave a spare capacity of sc. we generate n - 1 objects with weights randomly and uniformly distributed on  1 / . let w be  this is the weight the nth and final object would need to have to give the required spare capacity. if  then we assign the nth object this weight. if not  we throw away the n - 1 objects and start again. we experimented with a variety of spare capacities  but found that performance was broadly similar with weights in the range  1  /  and spare capacity sc as with weights in the range and spare capacity 1. in the rest of the paper we restrict 

attention to problems with no spare capacity. 
to determine the overhead modular pruning adds to 
cdbf in practice  fig. 1 gives a scatter plot of nodes searched to find the optimal packing against c p u time with and without modular pruning. we use 1 problems at each value of n from 1 to 1 and random 1-bit weights which leave no spare capacity when packed into n/1 bins. in this and subsequent experiments  we prune with bases that are powers of 1. c d b f was coded into common lisp and run on a network of identical dec 
alpha 1lx's with 1mhz processors. this graph supports our claim that pruning adds a constant factor overhead to runtime. regression for cpu seconds per node suggests that the overhead is about a factor of 1 for n = 1 but declines to 1 at n = 1. the nearly linear nature of this graph also supports the practice of using nodes searched as a proxy for cpu time. 

figure 1: nodes searched against c p u time 
¡¡the pruning rule can reduce search significantly. fig. 1 shows the mean nodes searched to find the optimal packing against log1 / . we again generate bin packing problems with 1 to 1 objects which have no spare capacity when packed into n/1 bins. we use 1 problems at each value of n and /. as in other np-complete problem classes  cheeseman et al 1   there is a phase transition as the constrainedness varies. at small log1 /   almost all problems pack into n/1 bins easily. at large l o g 1   l   l m o s t all problems require an extra bin. the hardest bin packing problems tend to occur in the phase transition inbetween. we see similar behavior to fig. 1 in the median and other percentiles of performance. 
¡¡as was expected  the effectiveness of the pruning rule increases as log1 /  increases and we have more bits with which to prune. for example at n = 1 and log1 /  = 1 mean cpu time was reduced from 1 to 1 seconds with mod pruning  despite the overhead. the effectiveness of the pruning rule also increases as the number of ob-
jects being packed increases. fig. 1 supports our claim that modular pruning reduces search significantly. on the harder problems  runtimes can reduce by an order of magnitude even in the phase transition region. for example  at n = 1 and log1 /  = 1  one problem took 

figure 1: mean number of nodes searched against the size of the weights 
1 cpu seconds without modular pruning but only 1 seconds with it. the pruning rule cannot increase the number of nodes searched  so where cpu time is greater it is by no more than the implementation overhead. 
¡¡in conclusion  the pruning rule adds essentially a constant factor overhead to but reduces search significantly. on the hardest problems  runtime can be reduced by an order of magnitude. for problems larger than considered here  the search tree may become too large to explore exhaustively. on such problems  we might consider a search strategy like ilds  korf  1 . this systematically explores decisions made against the branching heuristic. unlike depth-first search  ilds can undo branching mistakes high in the search tree at little cost  as well as taking advantage of pruning and propagation rules. 
1 multiple mod pruning 
if we multiply the weights and capacities by some constant then we get an equivalent bin packing problem. however  the modulus of the numbers may now offer new pruning opportunities. to test this hypothesis  fig. 1 plots median and 1th percentile in nodes searched with pruning bounds derived by multiplying the weights and capacities by 1  1  1  and 1. we use 1 problems n = 1 for varying /. fig. 1 shows that multiple modular pruning offers further reductions in search over the regular bound. by adding pruning possibilities we can only reduce the number of nodes searched. these reductions in search can result in shorter runtimes on harder problems than single modular pruning or no modular pruning. 
1 a propagation rule 
propagation rules determine when branching decisions are forced. for example  the davis putnam satisfiability procedure  davis and putnam  i1  has the unit propagation rule. this assigns truth values to variables in unit clauses as these values are forced. this rule contributes significantly to the effectiveness of the davis putnam procedure. propagation rules can often be constructed by adding look-ahead to a suitable pruning rule. 
	gent & walsh 	1 


figure 1: modular pruning with the multiples  {1 1} 
for instance  the davis putnam procedure also has an empty clause pruning rule. this prunes search when an empty clause is generated. the unit propagation rule is merely a one-step look-ahead on top of the empty clause pruning rule. suppose we have a unit clause  /. looking ahead  if we instantiate / to false  then the empty clause pruning rule would fire. we therefore assign / to true. but this is precisely the unit clause propagation rule: if we have a unit clause  / then we assign / to true. 
¡¡we can construct a propagation rule based on the pruning rule for bin packing in an identical fashion. in each base  the propagation rule packs the object with largest weight in that modulus into one of the bins  and tests the pruning bound. if the bound rejects every bin  we prune search. if the bound rejects all but one bin  we pack the object into the relevant bin. this adds an inter-node cost of  if we use all powers of 1 as bases  as we do here. fig. 1 shows the median and 1th percentile in nodes searched with and without this propagation rule. we again use 1 problems at n = 1. despite the larger overheads  the reductions in search can result in shorter runtimes on the harder problems compared to cdbf either with or without modular pruning. 

figure 1: modular pruning and propagation 
1 other applications 
we now describe some other application domains. 
1 	subset s u m 
given a set is there a subset whose weights have a target sum subset sum is problem sp1 in  garey and johnson  1 . it is equivalent to bin packing into two bins with capacities t and where is the sum of the weights. the pruning bound becomes  

for example  is there a subset of 1  1  1 and 1 with sum 
1  consider the numbers mod 1. we have 1  1  1 and 1 with which to meet the target sums of 1 mod 1 and 1 mod 1  that is  1 and 1 . it is therefore impossible to find a subset with the required sum. this is confirmed by the bound  with the lhs being less than zero. 
1 number partitioning 
given a set s is there a partition of s into sets whose weights have an equal sum  for this is sp1 in  garey and johnson  1 . number partitioning is equivalent to bin packing into  equal bins with capacities where is the sum of the weights  assumed to be a multiple of to construct a pruning bound that applies in the middle of search  we assume that we have constructed a partial partition ofs  that the partial partitions have weights with sums  and that let be the numbers still to be partitioned. the remaining problem is equivalent to bin packing into bins with capacities this gives the pruning boumd  
note that the first term is less than and the second is less than for a fixed number of objects  we are therefore more likely to break the bound for large this agrees with our intuitions. the more bins we have  the more likely we will be able to deduce using modular arithmetic that we cannot fill one of the bins. a similar bound can be derived for inexact number partitioning problems in which the sum of the weights is not necessarily a multiple of and the sums of the partitions is within some to construct a bound here  we add to s an extra set of  dummy  objects with unit weight. 
1 	k n a p s a c k p r o b l e m s 
in the 1 knapsack problem  we have a set of objects 1  each object 1  s has a weight  and a value we also have a target value v and a knapsack which can contain a weight  is it possible to put objects in the knapsack to reach the target value without exceeding the weight  this is problem mp1 in  garey and johnson  1 . for simplicity  consider the special case where the weights of the objects equal their values. let  be the 

sum of the weights. then we wish to find a subset of 
1 	search 

s with weight at least v but no more than w. this is equivalent to packing into two bins  with capacities w and and with a spare capacity of 
1 related work 
korf converted the greedy and the karmarkar-karp approximation algorithms for number partitioning into branching rules for optimization procedures  korf  1 . the pruning rules korf used are the analogues of the simple pruning rule in c d b f without modular pruning. 
¡¡mcgeoch has performed extensive experiments on the first fit  best fit  decreasing first fit and decreasing best fit approximation algorithms  mcgeoch  1   using objects with integer weights and bins of capacity 1 - 1. she observed a 'critical region' in which decreasing first fit gave packings with a large amount of empty space. as these packings tended to occur when there was a statistical excess of objects with large weights  this region may become less important as n increases and the distribution of weights tends to become more uniform. 
¡¡bin packing has a polynomial-time asymptotic approximation scheme  papadimitriou  1 . this uses modular arithmetic to reduce the grain size of the weights! to approximate within  each weight w is replaced by  w/q  where q is the quantum size of weights   and c is the bin capacity. knapsack problems have been proposed as the basis of a public-key cryptosystem  merkle and hellman  1 . the receiver deciphers the encoded message by multiplying the weights of the knapsack problem by a secret key and taking the modulus using a second secret key. this gives a knapsack instance that can be rapidly solved. brickell  personal communication cited in  lagarias and odlyzko  1   suggested that difficult and 'dense' cryptographic knapsack problems with many objects and small weights might be solved by converting them to 'low-density'problems through one or more modular multiplications.  bright et a/.  1  describes a parallel method for solving knap-
sack problems. modular arithmetic may still be of use in such methods for eliminating parts of the search space. 
1 conclusions 
the main contributions of this paper are new pruning and propagation rules for bin packing and some closely related problems. the pruning rule uses a bound constructed by decomposing the numerical constraints with modular arithmetic. we incorporated this rule into a new optimization procedure  complete decreasing best fit. we showed that the pruning rule adds essentially a constant factor overhead to runtime whilst reducing search significantly. on the hardest problems  runtime can be reduced by an order of magnitude. finally  we demonstrated how propagation rules can be built by adding lookahead to pruning rules. there are many directions for future research. for example  how do we identify good sets of bases and multiples for pruning  
¡¡what contributions might this research make beyond bin packing  first  the pruning bound can be applied to many other partition problems like subset sum  number partitioning  and knapsack problems. second  modular arithmetic may be useful in constructing pruning bounds and propagation rules in other combinatorial problems involving numerical constraints. third  new propagation rules may be developed by adding lookahead to existing pruning rules. fourth  this paper presents an example of a general methodology for building optimization procedures in new domains. that is  we construct a branching rule based on a good approximation algorithm  and pruning and propagation rules derived from a bound on the solution to the remaining subproblem. this approach may be effective on other np-complete problems. 
