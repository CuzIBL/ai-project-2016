 
most recent research of scalable inductive learning on very large dataset  decision tree construction in particular  focuses on eliminating memory constraints and reducing the number of sequential data scans. however  state-of-the-art decision tree construction algorithms still require multiple scans over the data set and use sophisticated control mechanisms and data structures. we first discuss a general inductive learning framework that scans the dataset exactly once. then  we propose an extension based on hoeffding's inequality that scans the dataset less than once. our frameworks are applicable to a wide range of inductive learners. 
1 	introduction 
most recent research on scalable inductive learning over very large dataset focuses on eliminating memory-constraints and reducing the number of sequential data scans  or the total number of times that the training file is accessed from secondary storage   particularly for decision tree construction. state-of-the-art decision tree algorithms  sprint  shafer et al  1   rainforest igehrke et al 1   and later boat  gchrke et a/.  1  among others  still scan the data multiple times  and employ rather sophisticated mechanisms in implementation. most recent work ihulten and domingos  1 applies hoeffding inequality to decision tree learning on streaming data in which a node is reconstructed iff it is statistically necessary. besides decision tree  there hasn't been much research on reducing the number of data scans for other inductive learners. the focus of this paper is to propose a general approach for a wide range of inductive learning algorithms to scan the dataset less than once from the secondary storage. our approach is applicable not only to decision trees but also to other learners  e.g.  rule and naive bayes learners. 
　ensemble of classifiers has been studied as a general approach for scalable learning. previously proposed metalearning  chan  1  reduces the number of data scans to 1. however  empirical studies have shown that the accuracy of the multiple model is sometimes lower than respective single model. bagging  breiman  1  and boosting ifreund and schapire  1  are not scalable since both methods scan the dataset multiple times. our proposed method scans the 
shaw-hwa lo statistics department  columbia university new york  ny 1 
slo stat.columbia.edu 
dataset less than once and has been empirically shown to have higher accuracy than a single classifier. 
　based on averaging ensemble  we propose a statisticallybased multiple model inductive learning algorithm that scans the dataset less than once. previous research  fan et al  1bl on averaging ensemble has shown that it is more efficient and accurate than both bagging and meta-learning. in this paper  we apply hoeffding inequality to estimate the probability that the partial and complete models are equal in accuracy. when the probability is higher than a threshold  the algorithm stops model construction and returns the current model  resulting in less than one sequential scan of the dataset. our objective is completely different from  hulten and domingos  1  on determining whether to change the shape of a decision tree. unlike previous research  hulten and domingos  1; gehrke et a/.  1   our algorithm is not limited to decision tree  but is applicable to a wide range of inductive learners. when it is applied to decision tree learning  the accuracy is higher than a single decision tree. another advantage is that the ensemble reduces the asymptotic complexity of the algorithm besides scanning less data. one important distinction is that we are interested in reducing sequential scan from secondary storage. once the data can be held entirely in memory  the base learning algorithm is allowed to scan the data in memory multiple times. 
1 	one scan 
we first describe a strawman algorithm that scans the data set exactly once  then propose the extension that scans the data set less than once. the strawman algorithm is based on probabilistic modeling. 
1 	probabilistic modeling 
suppose is the probability that x is an instance of class in addition  we have a benefit matrix that records the benefit received by predicting an example of class 
 to be an instance of class 	. for traditional accuracy-
based problems  	and 
for cost-sensitive application such as credit card fraud detection  assume that the overhead to investigate a fraud is $1 and 	is the transaction amount  then b fraud  fraud  - and 	fraud  fraud  	=  using benefit 
matrix and probability  the expected benefit received by pre-

learning 	1 

dieting x to be an instance of class 
expected benefit:  1  
based on optimal decision policy  the best decision is the label with the highest expected benefit: 
	max = argma* 	 1  
　assuming that is the true label of x  the accuracy of the decision tree on a test data set is 
	accuracy: a = 	 1  
for traditional accuracy-based problems  a is always normalized by dividing  for cost-sensitive problems  a is usually represented in some measure of benefits such as dollar amount. for cost-sensitive problems  we sometimes use  total benefits  to mean accuracy. 
1 the straw man algorithm 
the strawman algorithm is based on averaging ensemble ifan et al.  1b . assume that a data set s is partitioned into k disjoint subsets with equal size. a base level model is trained from each given an example x  each classifier outputs individual expected benefit based on probability 
 1  
the averaged expected benefit from all base classifiers is therefore 
　　　　　　  1  we then predict the class label with the highest expected retrain s  sy  a   p  
data : training set s  validation set partition number a'  confidence p result : multiple model with size  
begin 
i partition s into a' disjoint subsets of equal size 

train c  from si; test c  on sv; 
then 
end 
algorithm 1: less than one data scan 

turn as in eq. 
	optimal decision: 	 1  
　the obvious advantage is that the strawman algorithm scans the dataset exactly once as compared to two scans by meta-learning and multiple scans by bagging and boosting. in previous research  fan et al  1b   the accuracy by the strawman algorithm is also significantly higher than both meta-learning and bagging.  fan et al.  1b  explains the statistical reason why the averaging ensemble is also more likely to have higher accuracy than a single classifier trained from the same dataset. 
1 	less than one scan 
the less-than-one-scan algorithm returns the current ensemble with number of classifiers when the accuracy of current ensemble is the same as the complete ensemble with high confidence. for a random variable y in the range of with observed mean of y after n observations  without any assumption about the distribution of y  hoeffding's inequality states that with probabilit    the error of  to the true mean is at most 
		 1  
for finite population of size tv  the adjusted error is 
 1  
the range r of expected benefit for class label can be found from the index to the data  or predefined. when k base models are constructed  the hoeffding error e  can be computed by using eq. for data example x  assume than is the highest expected benefit and  is the secondhighest  and are the hoeffding errors. if 
　　　　　　with confidence p  the prediction on x by the complete multiple model and the current multiple model is the same. otherwise  more base models will be trained. the algorithm is summarized in algorithm 1. 
scan of validation set s v 
if an example x satisfies the confidence p when k classifiers are computed  there is no utility to check its satisfaction when more classifiers are computed. this is because that an ensemble with more classifiers is likely to be a more accurate model. in practice  we can only read and keep one example x from the validation set in memory at one time. we only read a new 

1 	learning 

instance from the validation set if the current set of classifiers satisfy the confidence test. in addition  we keep only the predictions on one example at any given time. this guarantees that the algorithm scans the validation dataset once with nearly no memory requirement. 
training efficiency 
the extra overhead of the hoeffding-based less than one scan algorithm is the cost for the base classifiers to predict on the validation set and calculate the statistics. all these can be done in main memory. as discussed above  we can predict on one example from the validation set at any given time. assume that we have k classifiers at the end and n is the size of the validation set  the total number of predictions is approximately  on average. the calculation of both mean and standard deviation can be done incrementally. we only need to keep and for just one example at anytime and 
 1  
 1  
the average number of arithmetic operation is approximately 

　the problem that the proposed algorithm solves is one in which the training set is very large and the i/o cost of data scan is the major overhead. when i/o cost is the bottleneck  the extra cost of prediction and statistical analysis is minimum. 
1 	experiment 
in empirical evaluation  we first compare the accuracy of the complete multiple model  one scan as well as less than one scan  and the accuracy of the single model trained from the same data set. we then evaluate the amount of data scan and accuracy of the less than one scan algorithm as compared to the one scan models. additionally  we generate a dataset with biased distribution and study the results of the less than one scan algorithm. 
1 	datasets 
the first one is the well-known donation data set that first appeared in kddcup'1 competition. suppose that the cost of requesting a charitable donation from an individual x is $1  and the best estimate of the amount that x will donate is y x . its benefit matrix is: 

as a cost-sensitive problem  the total benefit is the total amount of received charity minus the cost of mailing. the data has already been divided into a training set and a test set. the training set consists of 1 records for which it is known whether or not the person made a donation and how much the donation was. the test set contains 1 records for which similar donation information was not published until after the kdd'1 competition. we used the standard training/test set splits to compare with previous results. the feature subsets were based on the kdd'1 winning submission. 
to estimate the donation amount  we employed the multiple linear regression method. 
　the second data set is a credit card fraud detection problem. assuming that there is an overhead $1 to dispute and investigate a fraud and  is the transaction amount  the following is the benefit matrix: 

as a cost-sensitive problem  the total benefit is the sum of recovered frauds minus investigation costs. the data set was sampled from a one year period and contained a total of 1m transaction records. we use data of the last month as test data  1 examples  and data of previous months as training data  1 examples . 
the third data set is the adult data set from uc1 repository. 
for cost-sensitive studies  we artificially associate a benefit of $1 to class label f and a benefit of $1 to class label n  as summarized below: 

we use the natural split of training and test sets  so the results can be easily replicated. the training set contains 1 entries and the test set contains 1 records. 
1 	experimental setup 
we have selected three learning algorithms  decision tree learner c1  rule builder ripper  and naive bayes learner. we have chosen a wide range of partitions  k {1 1 1}. the validation dataset sv is the complete training set. all reported accuracy results were run on the test dataset. 
1 	experimental results 
in tables 1 and 1  we compare the results of the single classifier  which is trained from the complete dataset as a whole   one scan algorithm  and the less than one scan algorithm. we use the original  natural order  of the dataset. later on in section 1  we use a biased distribution. each data set under study is treated both as a traditional and cost-sensitive problem. the less than one scan algorithm is run with confidence 
p = 1%. 
accuracy comparison 
learning 	1 the baseline traditional accuracy and total benefits of the single model are shown in the two columns under  single  in tables 1 and 1. these results are the baseline that the one scan and less than one scan algorithms should achieve. for the one scan and less than one scan algorithm  each reported result is the average of different multiple models with k' ranging from 1 to 1. in tables 1 and 1  the results are shown in two columns under accuracy and benefit. as we compare the respective results in tables 1 and 1  the multiple model either significantly beat the accuracy of the single model or have very similar results. the most significant increase in both accuracy and total benefits is for the credit card data set. the total benefits have been increased by approximately $1 ~ $1; the accuracy has been increased by approximately 1% ~ 1%. for the kddcup'1 donation data set  the total c1 

table 1: comparison of single model  one scan ensemble  and less than one scan ensemble for accuracy-based problems 
benefit has been increased by $1 for c1 and $1 for nb. 
　we next study the trends of accuracy when the number of partitions k increases. in figure 1  we plot the accuracy and total benefits for the credit card data sets  and the total benefits for the donation data set with increasing number of partitions k. c1 was the base learner for this study. as we can sec clearly that for the credit card data set  the multiple model consistently and significantly improve both the accuracy and total benefits over the single model by at least 1 % in accuracy and $1 in total benefits for all choices of k. for the donation data set  the multiple model boosts the total benefits by at least $1. nonetheless  when k increases  both the accuracy and total benefits show a slow decreasing trend. it would be expected that when k is extremely large  the results will eventually fall below the baseline. 
　another important observation is that the accuracy and total benefit of the less than one scan algorithm are very close to the one scan algorithm. their results are nearly identical. 
data scan 
in both tables 1 and 1  we show the amount of data scanned for the less than one scan algorithm. it ranges from 1%  1  to about 1%  1 . the adult dataset has the most amount of data scanned since the training set is the smallest and it requires more data partitions to compute an accurate model. c1 scans more data than both ripper and nb. this is because we generate the completely unpruned tree for c1  and there are wide variations among different models. 
　in table 1  we compare the differences in accuracy and amount of training data when the validation set is either read completely by every classifier  under  batch   or sequentially only by newly computed base classifiers  under  seq    as discussed in section 1 . our empirical studies have found that  batch  mode usually scans approximately 1% to 1% more training data  and the models computed by both methods are nearly identical in accuracy. the extra training data from the c1 

ripper 

table 1: comparison of single model  one scan ensemble  and less than one scan ensemble for cost-sensitive problems 
 batch  method is due to the fact that some examples satisfied by previously learned classifiers have high probability  but may not necessarily be satisfied by more base classifiers. however  our empirical studies have shown that the difference in how the validation set is handled doesn't significantly influence the final model accuracy. 
1 	biased distribution 
when a data is biased in its distribution  the less than one scan algorithm need to scan more data than uniform distribution to produce an accurate model. with the same amount of datascan  it may not have the same accuracy as uniform distribution. we have created a  trap  using the credit card dataset. we sorted the training data with increasing transaction amount. the detailed results are shown in table 1 a  and  b . the accuracy  and total benefits  in table 1 a  are nearly identical to the results of  natural distribution  as reported in tables 1 and 1. however  the amount of datascan by the less than one scan algorithm is over 1 as compared to approximately 1 for natural distribution. as shown in table 1 b   when the datascan is less than 1  the confidence is not satisfied and less one scan will continue to compute more model   the total benefits are much lower. when distribution is biased  the variations in base classifiers' prediction are wider. it requires more data to compute an accurate model and the less than one scan algorithm is performing in the correct way. 
1 	training efficiency 
we recorded both the training time of the batch mode single model  and the training time of both the one scan algorithm and less than one scan algorithm plus the time to classify the validation set multiple times and statistical estimation. we then computed serial improvement  which is the ratio that the one scan and less than one scan algorithm are faster than training the single model. in figure 1  we plot results for the credit card dataset using c1. our 

1 	learning 

figure 1: plots of accuracy and total benefits for credit card data sets  and plot of total benefits for donation data set with respect to k when using c1 as the base learner 


c1 
1 	accuracy | 1 	data scan 1 batch seq | 1 batch seq donation 1% 1% 1 1l 1% 1% 1 1 adult 1% 1% 1 1 ripper 

nb 

table 1: comparison of accuracy and amount of datascan using the batch  or all in memory  and sequential method for accuracy-based problems 
training data can fit into the main memory of the machine. any single classifier algorithm that reduces the number of data scan  shafer et al 1; gehrke et al. 1; 1; hulten and domingos  1  will not have training time less than this result. as shown in figure 1  both one scan and less than one scan algorithm are significantly faster than the single classifier  and the less than one scan algorithm is faster than the one scan algorithm. 
1 	related work 
to scale up decision tree learning  sprint  shafer et ai  
1j generates multiple sorted attribute files. decision tree is constructed by scanning and splitting attribute lists  which eliminates the need for large main memory. since each attribute list is sorted  for a data file with / attributes and n examples  the total cost to produce the sorted lists is 
/ 1 n - log n  . external sort is used to avoid the need for main memory. each attribute list has three columns-a unique record number  the attribute value and the class label; the total size of attribute lists is approximately three times the size of the original data set. when a split takes places at a node  figure 1: serial improvement for credit card dataset using one scan and less than one scan 

numbar at parthona 

 a . performance for different classifier for biased distribution 

 b . performance of c1 with different amount of data scanned under the biased distribution 
table 1: performance of less than one scan under biased distribution 
sprint reads the attribute lists at this node and breaks them into r sets of attribute lists for r child nodes  for a total of / file read and r   / file writes. later  rainforest  gehrke et ai  1  improves the performance of sprint by pro-
jecting attribute list on each unique attribute value into an avc-set. the complete avc-sets of all attributes are called avc-group. when the avc-group can be held in main memory  the selection of predictor attribute can be done efficiently. to construct the initial avc-group  it incurs the same cost as 
sprint to construct initial attribute lists plus one more scan over the sorted lists to project into avc-sets. whenever a split happens  rainforest has to access the data file again and reconstruct the avc-group for child nodes. the exact number of read and write is based on variations of rainforest that is chosen. in the best scenario where the avc-group of every node in the tree fit in memory  the rf-read version still has to scan the whole data once at each level of the tree and write it into r files. when this condition is not met  rainforest solves the problem by multiple read and write. more recently  boat  gehrke et ai  1  constructs a  coarse  

learning 	1 

tree from a data sample that can fit into main memory. the splitting criterion in each node of the tree is tested against multiple decision trees trained from bootstrap samples of the sampled data. it refines the tree later by scanning the complete data set  resulting in a total of two complete data read. 
　meta-learning  chan  1  builds a tree of classifiers and combine class label outputs from base classifiers. it is based on heuristics and the total number of datascan is two. the improvements by our methods are in many folds. we combine probabilities instead of class labels. the combining technique is straightforward and can estimate the final accuracy prior to full construction. the total amount of sequential scan is less than once. there is a strong statistical reason to support why the multiple model method works  fan et ai  1bl. besides meta-learning  both bagging  breiman  1 and boosting  freund and schapire  1j have been shown to have higher accuracy than a single model. however  bagging and boosting scan the dataset many times  and are not scalable for large dataset. previous research  fan et ai  1b  has shown the our combining method is more accurate than bagging. 
　one earlier work to use hoeffding s inequality is online aggregation  hellerstein et ai  1  to estimate the accuracy of aggregate queries. when the confidence about the accuracy of the partial query is sufficient  the user jan terminate the query to avoid a complete scan. one recent work using hoeffding inequality is on building decision tree from streaming data  hulten and domingos  1 . they keep the number of examples that satisfy or fail the predicate of a node in the tree. when it is confident that the current node is not accurate for the new data  it will be reconstructed. one limitation of  hulten and domingos  1  is that it is only applicable to decision tree learning. our ensemble method using hoeffding's inequality is applicable to a wide range of inductive learners. empirically  we have applied it to decision tree  rule learner as well as naive bayes learner. beside its generality  the proposed one scan and less than one scan algorithms have been shown to have potentially higher accuracy than the single model  existing approaches are single model method  hulten and domingos  1; gehrke et ai  1; shafer et ai  1; gehrke et ai  1  . another advantage is the asymptotic complexity using the less than one scan algorithm is approximately  while computing a sin-
gle model is still 1 n . our empirical studies have shown that both the one scan and less than one scan algorithms are significantly faster than learning a single model. in previous work  fan et ai  1a   we have used central limit theorem  clt  to implement a concept of progressive modeling where we estimate the range of accuracy of the final model. the learning stops when the estimated accuracy of the final falls within a tolerable range with confidence p. clt requires the data distribution to be uniform. hoeffding inequality has a different implication  in which we estimate the probability that the partial and final models are exactly the same. 
1 	conclusion 
in this paper  we propose two scalable inductive learning algorithms. the strawman multiple model algorithm scans the data set exactly once. we then propose a less than one scan 
1 
extension based on hoeffding's inequality. it returns a partial multiple model when its accuracy is the same as the complete multiple model with confidence  since hoeffding inequality makes no assumption about the data distribution  the advantage of this method is that the data items can be retrieved sequentially. we have also discussed how to sequentially read the validation set exactly once using minimal memory. we have evaluated these methods on several data sets as both traditional accuracy-based and cost-sensitive problems using decision tree  rule and naive bayes learners. we have found that the accuracy of all our methods are the same or far higher than the single model. the amount of data scan by the less than one scan algorithms range from 1 to 1 for the original natural distribution of data. for a significantly biased dataset  the amount of datascan by the less than one scan algorithm is over 1. it needs extra data to resolve the bias in data distribution in order to compute an accurate model. in addition  our empirical studies have shown that both methods are significantly faster than computing a single model even when the training data can be held in main memory  and the less than one scan algorithm is faster than the one scan algorithm. 
