 
people naturally express themselves through facial gestures. we have implemented an interface that tracks a person's facial features robustly in real time  1hz  and does not require artificial artifacts such as special illumination or facial makeup. even if features become occluded the system is capable of recovering tracking in a couple of frames after the features reappear in the image. based on this fault tolerant face tracker we have implemented realtime gesture recognition capable of distinguish 1 different gestures ranging from  yes    no  and  may be  to winks  blinks and  asleep . 
1 	introduction 
to advance the field of robotics greater efforts are needed to improve the communication interface between humans and robots. an important area of this work is the recognition of gestures of the head  body and expressions of the face. such body language is regarded as one of the most natural forms of human expression. if we are to create intelligent robots in the future  human body language must be understood. robots that can communicate in this manner will be particularly useful for disabled people and to users who are inexperienced in working with traditional computer interfaces. a gesture interface could open the door to many applications ranging from control of machines to  helping hands  for the elderly and disabled. the crucial aspects of such systems are their robustness and their real-time capability. at present many computer vision systems are still lacking in computational speed and can't cope with tracking errors  temporary occlusion of features. we have overcome these problems by using dedicated computer vision hardware that is capable of real-time feature tracking and by developing specialised algorithms that make face tracking robust. 
　the principle of our vision system is image correlation  or template matching . template matching is not directly applicable to face tracking especially under the constraints of non-homogeneous illumination and contrast enhancing makeup can't be used. the appearance  i.e shape and shade  of facial features changes when an observed person's head moves. a large difference occurs between the reference template and the features being tracked. this effect can be further compounded when people tilt their heads to the side and cause a rotation of the feature. so tracking of the facial feature fails and there is no way to recover from this error. 
　implementations of gesture interfaces based on image correlation have been previously reported  azarbayejani et al.  1; hager and toyama  1; 1 . the work 
of  azarbayejani et a/.  1  reported on the manipulation of a virtual reality clone. this system automatically acquired the tracking templates at runtime using heuristic filters. with this approach it is not possible to determine the initial state of features e.g. eyes are open and the position of the face can only be measured relative to the initial image. the  hager and toyama  1; 1  implementation uses deformable template matching performed in software to track a face. this system does not exploit the geometry of the facial features to assist in tracking or error recovery so only small rotations of the head are allowed. related to the template based approach is the method proposed by  maurer and von der malsburg  1 . instead of using bitmap templates for tracking this system calculates the local response to a garbor filter of different frequencies and orientations. the video frame rate required for this method is reported to be  1hz. although  even using reduced images of 1 pixels the calculation time for one frame is 1sec. like the hager et al system it does not exploit the geometric relationship between features. both systems are not able to recover from tracking errors caused by temporary occlusion of features. 
　an alternative approach is to use specialised filters instead of templates to localise facial features. the filters are implemented as small programs that classify pixels  e.g. belonging to the iris or to the eyeball or to the eyelid 
	heizmann & zelinksy 	1 

of an eye. the classifier usually uses heuristics or probabilistic techniques. a system controlling a clone in virtual reality using this approach is reported by  saulnier et a/.  1 . to improve the reliability of pixel classi-
fication contrast enhancing makeup has to be used and the system only runs at 1hz. since the system does not exploit facial geometries the face tracking can fail when features are temporarily occluded. a system running theoretically at 1hz is reported by  gee and cipolla  1 . this impressive performance is achieved because the system uses simple filters that are not able to validate the tracking results. thus  the tracker can neither detect tracking errors nor recover from them. like the other systems this tracker is not able to pick up a face that recently entered the image. another face tracker reported by  jacquin and eleftheriadis  1  uses a oval shape model to detect the location of the face in the image. using this general model a high robustness and person independence can be achieved  though  the required computation is high and makes the approach unsuitable to real-time applications. 
　reliable and rapid tracking of the face gives rise to ability to recognise facial gestures. this should not be confused with work being done to understand facial expressions  darrel and pentland  1; black and yacoob  1; saulnier et a/.  1 . the work described in  darrel and pentland  1  is concerned with the classification of the expression into states such as happiness  anger  fear  sadness or disgust. the system uses nearest neighbour matching to a set of templates to classify the expression state. templates can be acquired automatically or synthesised by the system. the detection of facial expression runs at about 1hz. the aim of the system reported in  black and yacoob  1  is similar  but the classification is based on the affine transformation parameters of individual templates for different facial features. however  the system requires high resolution images  1 pixels  so real-time performance could not be achieved. 
　due to the real-time constraints of the problem of gesture recognition based on head motion there have been few projects undertaken in this area.  darrel and pentland  1  describe a system that is able to recognise hand and facial gestures based on the template matching system developed by  azarbayejani et al.  1 . the tracking output consists of different motion parameters of the face. sequences of motion vector parameters can be recorded where the start and the end must be manually determined. such a sequence forms a  space-time  gesture. during recognition the last motion vectors are compared with the stored gestures using a dynamic time warping algorithm similar to that used in speech recognition applications. after receiving each new motion vector the history of motion vectors have to be matched 
videos 
with all gestures stored in the library. this has a heavy impact on the calculation time. using only two gestures of length 1 state vectors the system can only run at 1hz. the system we describe in this paper has proved to run at 1hz and can distinguish 1 different gestures. this is possible due to a specialised gesture model which requires little computation yet can still detect gestures robustly. 
1 	real-time vision system 
the mep tracking vision system manufactured by fu-
jitsu is designed for real time tracking of multiple objects in the frames of a ntsc video stream. it consists of two vme-bus cards  a video module with frame grabber and overlay memory for displaying graphics and textual information  and a tracking module which can track over 
1 templates at video frame rate  1hz for ntsc . a mc1mhz processor card running vxworks executes the application program and controls the vision system. 
   objects are tracked using template correlation. there are two sizes for templates available  1 or 1 pixels  but both sizes can be magnified by a factor between 1 and 1 in the x- and y-direction independently. the frames of the video stream are digitised by the video module and stored in dedicated video ram which can also be accessed by the tracking module. next  the templates are matched within specified search windows of the image. the comparison itself of the template and an area of the digitised frame is done by a cross correlation using the mean absolute error method. the distortion  which is the sum of the absolute greyscale differences of corresponding pixels in the template and the video frame  indicates how similar the two images are. low distortions indicate a good match and high distortions appear when the images are very different. the formula for the distortion d is shown in formula 1 where size is the template size  1 or 1   gt{x y  is the grey value of the template at the specified coordinates  gf{x y  is the grey value of the pixel in the last frame  mx and my are the magnifications of the template in x- and y-direction and ox and oy are the offsets in the frame. 

　to track the template of an object it is necessary to calculate the distortion not only at one point in the image but at a number of points within the search area. to track the motion of an object the tracking module finds the position in the search window where the template matches with the lowest distortion. by moving the search window along according to the tracking results objects can be tracked easily this method works perfectly for objects that do not change their appearance or shade and that get occluded by other objects. 

figure 1: the tracked features and their references 
1 improving the system robustness 
our basic idea to overcome the problem of temporarily distorted or occluded features is to have individual search windows help each other to track their features. since the geometric relationship between the features in a face is known a lost search window can be relocated by assistance from those that are still tracking. in a first approach a two dimensional model of the face is used where the features are placed on a virtual grid. during face tracking the search windows can monitor their relative position to the other windows and readjust their coordinates if necessary. figure 1 shows the face of a person where the boxes mark the 1 features that are used for tracking. the lines indicate the connections between the search windows. 
　the robustness of the tracker heavily depends on the method used to fuse the tracking results of the vision system with the geometric data derived from the 1d face model. in difficult situations  when the head is turned and all the templates match with high distortions  it is not possible to determine whether or not a feature has been lost based on the distortion reported by the vision system. thus  a probabilistic approach must be chosen to merge the uncertain tracking data with the information derived from the 1d facial model. we believe that kalman filters are an appropriate method to cope with this problem. 
1 	multiple interdependent kalman 
filters 
the kalman filter is a recursive linear estimator which merges the measurements of sensors observing the environment with a system state prediction that is derived from a system model  bozic  1; durrant-whyte  1 . kalman filters are used in many applications such as navigation of planes  missiles and mobile robots where uncertain measurements from sensors that observe landmarks are used to localise a vehicle. by merging sensor information with the internal model information  the kalman filter guarantees an optimal estimate of the system's state vector. 
　the use of kalman filters in feature tracking has been previously reported by mclaughlan et al.  mclauchlan et a/.  1  to estimate 1d structure from motion. however  in our approach features are used to assist each other in tracking. hager et al.  hager and toyama  1  proposed a similar idea using geometric constraints and feature states for tracking. however  a binary switching method between winning and loosing features is used to decide the features that guide the other tracking features. our approach uses all the features for tracking and is weighted to the features that are tracking best. in difficult situations  when the head is turned around and all templates match with high distortions our merging method shows much more robust behaviour than binary switching. so larger rotations of the head can be tracked than with previously reported methods. 
　an individual extended kalman filter is applied to each feature. the tracking results provided by the vision system are used as sensor input where the distortion is converted to a variance of this measurement. the results of the different templates are fed into a selectionunit which selects the template with the lowest distortion. only the coordinates and the associated variance of the selected template are forwarded to the kalman filter. instead of a system model describing the behaviour of the feature itself an estimation of the features position based on the position of the reference features is fed into the kalman filter. equation 1 shows how a mergeunit derives a position estimation from the positions of the reference features with the according variance the displacement vectors between the ref-
erence feature and the feature itself and the movement vector m. 

the variance ppi associated with this position estima-
heizmann & zelinksy 
tion is calculated from the variance of the last position and the variances of the reference positions according to formula 1. 

　next  according to the general kalman filter equations the position of the feature is calculated. these computations must be performed for each feature in each frame cycle. in the next cycle the position calculation is used by other features to estimate their own position. 
　the system described thus far will track facial features however strictly limited turning and tilting of the head is only possible. the rigid connections in the 1d facial model can force well matching templates to leave their associated features to satisfy the geometric requirements of the 1d model. we overcome this problem by introducing a projection to deal with the deformations of the 1d model. the rotational angle  and the compressions factors cx and cy in x- and y-direction of the model are calculated each frame. if the calculated values are used as raw input the system becomes unstable. however  a simple low pass filter sorts these problems out. 
1 	further enhancing robustness 
at this stage the system is capable of robustly tracking the face in the rotational range of about 1 degrees. further rotations causes features to disappear and others to be highly distorted. should all the features disappear or deform in a way that the system can't track them anymore  the tracking fails completely and is not able to recover even if the features reappear in the image. a similar situation arises when the system is started up and the person may not be in the image yet or the face is located differently from the coordinates in the initialisation file. the search windows can't lock onto the features and thus guide others to their correct locations. these situations are common in real world applications and manual initialisation of the feature coordinates in the first frame of a recorded video sequence can't deal with this problem. a mechanism is required to perform online recovery in real-time. another desirable characteristic in this context is that the system does not need to switch between an initialisation mode trying to localise any feature and a tracking mode because this implies that the system can determine whether all features are lost or not. 
　the single mode solution proposed in this paper increases the system resources used to relocate a feature as the variance of its position increases. additional search windows  called area search windows  that use the same template as the feature tracking window are added in the surrounding area of the estimated feature position. the size of the area and the number of search windows 
videos 
used are functions of the feature's variance. since system resources are limited  for each template we must determine the number of area search windows to allocate. our system distributes the available search windows according to the requirements and the uniqueness of the features. characteristic features like the eyes and eyebrows go first  while the features with poor contrast around the mouth go last. the last problem to be solved is when to believe a feature was actually lost and found by one of its area search windows or when should the feature be relocated to the position of a well matching area search window. at the moment we are using a malus factor1 applied to the distortion of the area search window and compare the resulting distortion to the distortion of the feature tracking window. if the area distortion after application of the malus factor is smaller than the centre distortion  the feature is relocated by selecting the area search windows coordinates as one input of the kalman filter. 
   another problem to be addressed is the similarity of the two eyes and eyebrows. during the recovery phase the window of an eye can easily get stuck on the wrong eye  and the accompanying eyebrow will also match very well on the wrong eyebrow. the system remains in this state without any chance of to recover unless the eye  that is tracked by the wrong window  disappears for some reason. for this reason a third eye is introduced into the system. the search window for the 1rd eye is set to the coordinates calculated by adding the vector connecting the left eye and the right eye to the coordinates of the right eye  this is where the right eye would be if the right eye window tracks the left eye. the variance is set to the variance of the left eye and the appropriate number of area search windows is added. this variance is usually large if the left eye is being searched in the area of the ear and low if both eyes are being tracked the right way. so only when one eye appears to be lost then the area search windows are used to recover it. 
　the decision whether or not an error has occurred is done in the same way as for area search windows. the best distortion of the left eye is compared with the best distortion of the 1rd eye search window multiplied with a malus factor1. if the 1rd eye search window detects an eye  then the coordinates of the right eye are set to those of the 1rd eye and those of the left eye are set to those of the right eye. 
　the search position of the 1rd eye window is alternated each frame  so both sides  left and right  of the facial feature network are checked at a frequency of 1hz. 
1
the malus factor for area search windows is 1. 

	b  	c  
figure 1: various deformations of the feature grid 1  for the 1rd eye the malus factor is 1. 1 	gesture recognition 
robust real-time face tracking gives rise to the possibility of recognising gestures based on motions of the head. gesture recognition and face tracking are implemented as independent processes. both processes run at the ntsc video frame rate  1hz . 
　the design of the algorithms for gesture recognition is determined by hard real-time constraints since dozens of gestures must be compared with the data stream from the face tracking module. also  gesture recognition must be flexible in respect to the performance of the gesture including timing and amplitudes of the parameter. the system should assign confidence values to the gestures that it recognises. this allows higher level processes to use the gestures as input and interpret them optimally. to avoid the computationally expensive time warping method we developed a recursive method taking only the current motion vector of the face into account. a set of finite state machines implicitly store the previous tracking information. 
　gesture recognition is implemented as a two layer system based on the decomposition of gestures into atomic actions. the lower layer recognises basic motion primitives and state primitives called atomic actions. the output of an atomic action consists of its activation which is a measure for the similarity of the recent motion vectors and the atomic action definition. the system incorporates 1 predefined atomic actions for which the activation is calculated in each frame cycle. 
　the upper layer is concerned with the recognition of patterns in the activation of the set of atomic actions. a gesture is defined by a sequence of atomic actions and time constraints for the occurrence of each of them. each time the first atomic action of a gesture definition is activated an instance of a finite state machine is generated dynamically. this instance then observes the activation of the next atomic action in the gesture definition and does the transition to the next state if the activation occurs within a given time frame. if no activation occurs in the time frame the finite state machine instance is deleted from the system. when the state machine reaches it's final state the atomic action sequence is completely recognised and the gesture is send to output together with a confidence measure of how well the observed pattern matched the gesture definition. figure 1 shows a selection of gestures recognised by the system. for a full description of gesture recognition refer to /citehei1. 

	look down 	look left 	turn to left 
figure 1: example gestures 
1 	experimental results 
the complete system was extensively tested in our laboratory. sequences of several minutes of persons performing gestures were video taped and used for analysis. the face tracker proved to be reliable under stable illumination and tolerated rotations of the face not exceeding 1＜. figure 1 shows the output of the face tracker and the recognised gestures. the recognition rate for the 
heizmann & zelinksy 
gestures differs significantly. complex but easy to track gestures like yes and no have the best recognition rate  1% . difficult to track gestures like the turn-gestures are sometimes not identified due to tracking errors but still have recognition rate over 1%. short gestures such as winking and blinking are recognised whenever they are performed  though due to the shortness of the gesture the number false detections can be as high as 1%. 

figure 1: activation pattern of the atomic actions eyes.open  half.open  closed during a blink 
1 	conclusion and future work 
our system is able to track the features of the face at 1hz without special illumination or contrast enhancing makeup. it is able to automatically initialise tracking without any restrictions to the position of the face in the first frame. it is also able to lock on temporary occluded features as soon as they are appearing in the image and even if all features where occluded the system can recover within in a few seconds. 
　the gesture recognition module runs in parallel and is able to recognise 1 different gestures running at video frame rate. by decomposing gestures into atomic actions it also provides ways of measuring the conformity of a performed gesture to the gesture definition. 
　in our future work we plan to develop a 1d adaptive model of the face and use dynamic template acquisition during tracking. our ultimate aim is to use the facial gesture recognition system in a robotic system for the disabled. 
acknowledgements 
this work has been funded by the australian research council and by the generous support of wind river systems  supplier of vxworks. 
