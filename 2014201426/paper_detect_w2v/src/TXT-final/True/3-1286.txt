
viterbi alignment and decoding are two fundamental search problems in statistical machine translation. both the problems are known to be nphard and therefore  it is unlikely that there exists an optimal polynomial time algorithm for either of these search problems. in this paper we characterize exponentially large subspaces in the solution space of viterbi alignment and decoding. each of these subspaces admits polynomial time optimal search algorithms. we propose a local search heuristic using a neighbourhood relation on these subspaces. experimental results show that our algorithms produce better solutions taking substantially less time than the previously known algorithms for these problems.
1 introduction
statistical machine translation  smt  is a data driven approach to machine translation  brown et al.  1    alonaizan et al.  1    berger et al.  1 . two of the fundamental problems in smt are  brown et al.  1 : a  = argmaxpr f a|e   viterbialignment 
a
 	 decoding 
e a
viterbi alignment has a lot of applications in natural language processing  wang and waibel  1    marcu and wong  1 . while there exist simple polynomial time algorithms for computing the viterbi alignment for ibm models 1  only heuristics are known for models 1. recently   udupa and maji  1a  showed that the computation of viterbi alignment is np-hard for ibm models 1. decoding algorithm is an essential component of all smt systems  wang and waibel  1    tillman and ney  1    och et al.  1    germann et al.  1    udupa et al.  1 . the problem is known to be np-hard for ibm models 1 when the language model is a bigram model  knight  1 .
unless p = np  it is unlikely that there exist polynomial time algorithms for either viterbi alignment or decoding. therefore  there is a lot of interest in finding fast heuristics to find acceptable solutions for the problems.
previous approaches to the viterbi alignment problem have focussed on defining a graph over all possible alignments where the connectivity is based on local modifications. the best known algorithm for decoding  both in terms of speed and translation quality  employs a greedy search algorithm. however  these approaches can look at only polynomial number of alignments in polynomial time.
in this paper  we characterize exponentially large subspaces of the solution space of viterbi alignment and decoding. we propose polynomial time optimal dynamic programming algorithms to solve viterbi alignment and decoding when the search space is restricted to a particular subspace. there are exponentially many such subspaces. so  we perform a local search on these subspaces under a suitable neighbourhood relation. though  there is no polynomial bound on the number of iterations of the local search  experiments show that our algorithms procduce better solutions in significantly less time than the current algorithms.
we introduce the notion of an alignment generator in section 1. there are m! distinct alignment generators  where m is the length of the source language sentence . each alignment generator  gi  where 1 뫞 i 뫞 m!  is associated with an exponentially large subspace of alignments  ai . the solution space of viterbi alignment and decoding can be written as the union of these subspaces. we next consider the graph induced on the set of ais by a neighbourhood relation and perform local search on this graph. the explicit mathematical definition of ai and the neighbourhood relation are presented in section 1. we present polynomial time optimal algorithms for viterbi alignment and decoding over a particular ai  section 1 and section 1 respectively . experiments show that our search algorithms produce better solutions taking subtantially less time than current algorithms  section 1 .
1 preliminaries
ibm models 1 assume that there is a hidden alignment between the source and target sentences.
definition 1  alignment  an alignment is a function a m  l  : {1 ... m} 뫸 {1 ... l}.
{1 ... m} is the set of source positions and {1 ... l} is the set of target positions. the target position 1 is known as the null position. we denote a. the fertility of the target position i in alignment a m  l  is 뷋i = . if 뷋i = 1  then the target position i is an infertile position  otherwise it is a fertile position. let {k ... k + 붺   1}   {1 ... l} be the longest range of consecutive infertile positions  where 붺 뫟 1. we say that alignment a m  l  has an infertility width of 붺. representation of alignments: since all source positions that are not connected to any of the target positions 1 ... l are assumed to be connected to the null position  1   we can represent an alignment  without loss of generality  as a set of tuples { j aj |aj 1= 1}. let m1 = m   뷋1. thus  a sequence representation of an alignment has m1 tuples. by sorting the tuples in ascending order using the second field
as the key  a m  l  can now be represented as a sequence
 where the s are non-
decreasing with increasing k. note that there are  l + 1 m possible alignments for a given m l   1. let a be the alignment {1 ... v} 뫸 {1 ... u} where all the tuples  such that have been removed.
definition 1  generator  a generator is a bijection g m  : {1 ... m} 뫸 {1 ... m}.
representation of generators: a generator g m  can be represented as a sequence of tuples  j1 ... jm m . note that there are m! different possible generators for a given m   1. the identity function g m  j  = j is denoted by
모 m  g1	.
definition 1  g m   뫸 a m  l   an alignment a m  l   with tuple sequence   is said to be generated by a generator g m   with tuple sequence  k1 ... km m   if j1 ...jm1 is a subsequence of k1 ...km.
note that the above definition does not take into account the infertility width of the alignment. hence  we refine the above definition.
definition 1  g m   붺뫸 a m  l   an alignment a m  l  is said to be 붺-generated by g m   if g m   뫸 a m  l  and the infertility width of a m  l  is at most 붺.
 
definition 1  

thus  for every generator g m  and integer 붺 뫟 1 there is an associated family of alignments for a given l 뫍 n.
further .
swap	operation:	let g m 	be	a	generator	and
 j1 ... jk k ... jk1 k1 ... jm m  be its tuple sequence. the result of a swap operation on the kth and k1th tuples in the sequence is another tuple sequence  j1 ... jk1 k ... jk k1 ... jm m . the new tuple sequence defines a generator g1 m . the relationship between any two generators is given by:
lemma 1 if g m  and g1 m  are two generators then g1 m  can be obtained from g m  by a series of swap operations.
모모proof: follows from the properties of permutations. we can apply a sequence of swap operators to modify any given generator g m  to g1 m . the source position rv  1 뫞 v 뫞 m  in g m  corresponds to the source position v
 m 
in g1 . for brevity  we denote a sequence vi vi+1 ...vj by vij. the source sentence is represented by f1m and the target sentence is represented by el1.
1 framework
we now describe the common framework for solving viterbi
alignment and decoding.	the solution space for viterbi
alignment can be written as 뫋g m  agl  l m   and the solution
모모모모모모모모모모모모모모모모모모모모모모m  space of decoding can be approximated by 뫋g m for a large enough 붺. the key idea is to find the optimal solution in an exponentially large subspace a corresponding to a gen-
           m  erator  andfor viterbi alignment and
decoding respectively . given an optimal search algorithm for a  we use lemma 1 to augment the search space. our framework is as follows
1. choose any generator g m  as the initial generator.
1. find the best solution for the problem in a.
1. pick a better generator g1 m  via a swap operation on g m .
1. repeat steps 1 and 1 until the solution cannot be improved further.
1 algorithms
viterbi alignment
the algorithm for finding a good approximate solution to viterbi alignment employs a subroutine viterbiforgenerator to find the best solution in
the exponentially large subspace defined by ag붺  l m . the implementation of this subroutine is discussed in section 1.
algorithm 1 viterbi alignment
1: .
1: while  true  do
1:	a  m  l  = viterbiforgenerator g.
1: find an alignment a1 m  l  with higher score by using swap operations on a  m  l .
1:	if a1 m  l  = nil then
1:	break.
1:	end if
1:	g m  뫹 a generator of a1 m  l .
1: end while
1: output a  m  l 

decoding
our algorithm for decoding employs a subroutine decodeforgenerator to find the best solution in the exponentially large subspace defined by . the implementation of this subroutine is discussed in section 1.

1: .
1: while  true  do
1:	decodeforgenerator g.
1: find an alignment a1 m  l  with higher score by using swap operations on a  m  l  and e 1l.
1:	if a1 m  l  = nil then
1:	break.
1:	end if
1:	g m  뫹 a generator of a1 m  l .
1: end while
1: output e 1l.

1 theory of generators
in this section  we prove some important results on generators.
structure transformation operations
we define a set of structure transformation operations on generators. the application of each of these operations on a generator modifies the structure of a generator and produces an alignment. without loss of generality 1  we assume
모모모모모모모모모모모모모모모모모모모모모모모모모모 m  that the generator is the identity generator g1 and define the structural transformation operations on it. the tuple se-
 m 
quence for g1 is  1 ... m m . given an alignment a with 뷋i   1  we explain the modifications introduced by each of these operators to the jth tuple.
1. shrink: extend a to a
모모모모모모 j i  such that aj	= 1.
1. merge: extend a to a
 j i 
	such that aj	= i.
1.  붺 k -grow  1 뫞 k 뫞 붺 : extend a
 j i+k+1 
	to asuch that aj	= i+k+
1.
by removing the infertile positions at the end  we obtain the following result:
lemma 1 a and has at least one fertile position iff there exists s such that 
 and 뷋l s   1.
theorem 1 if a m l s  뫍 ag붺  l m   s such that 뷋l s   1 then
                              m  it can be obtained from g1 by a series of shrink  merge and  붺 k -grow operations.
 m 
모모proof: wlog we assume that the generator is g1 . we provide a construction that has m phases. in the vth step  we
모모모 v  construct afrom g1	in the following manner:
  we construct a from g1 v 1  and employ
a shrink operation on the tuple .
  we construct a from g1 v 1  and employ a merge operation on the tuple  v v  if 뷋u   1.
  if 뷋u = 1: let a define 뷋t = 1 for u   k 뫞 t 뫞 u   1 and 뷋u k 1   1  where 1 뫞 k 뫞 붺.
 v 1 
	construct a	 from g1	and apply
 붺 k -grow to the  v v  tuple.  note: if  u k 1  = 1  then consider the alignment where {1 ... v   1} are all aligned to the null position 
as these are the only possibilities in the vth phase  at the end of the mth phase we get a m l s .	
the dynamic programs for viterbi alignment and decoding are based on ideas in lemma 1 and theorem 1.
lemma 1  is given by the coefficient of xl in
		 1 
  where.
	proof: refer  udupa and maji  1b 	
theorem 1 
	proof: substitute x = 1 in equation 1	
decodeforgenerator finds the optimal solution over all possible alignments in ag붺   m  in polynomial time.
theorem 1 for a fixed.
모모proof: refer  udupa and maji  1b 	viterbiforgenerator finds the optimal solution over all possible alignments in in polynomial time.
lemma 1 let be a set of genera-
tors. define span . let p .  be a polynomial. if l 뫟 1 and n = o p m    then there exists an alignment a m  l  such that a m  l  1뫍 span s .
	proof: refer  udupa and maji  1b 	
this lemma shows that by considering any polynomially large set of generators we can not obtain the optimal solutions to either viterbi alignment or decoding.
1 viterbi alignment
we now develop the algorithm for the subroutine viterbiforgenerator for ibm model 1. recall that this subroutine solves the following problem:
	a  m  l  =	argmax	pr.
a m  l 뫍ag붺  l m 
our algorithm has a time complexity polynomial in m and l.
without loss of generality  we assume that g.
note that a  where 1 뫞 s 뫞 붺 and 뷋l s   1  can be obtained from g1 using the structure transformation operations described in section 1. therefore  we build the viterbi alignment from left to right. we scan the tuple sequence from left to right in m phases and in any phase we determine the best structure transformation operation for that phase.
we consider all possible partial alignments between f1v and eu1  such that 뷋u =     1 and 뷋1 =  1. let
b  u v  1    be the best partial alignment between f1v and eu1 and a u v  1    be its score1. here   1 is the number of french words aligned to e1 in the partial alignment.
the key idea here is to compute a u v  1    and b  u v  1    recursively using dynamic programming. in the vth phase  corresponding to the tuple  v v    we consider each of the structure transformation operation.
1. shrink: if  1   1  the shrink operation extends b  u v   1  1   1 뷋  and the score of the resulting partial alignment is
s1 = ca u v   1  1   1   
where c = t fv|null  .
1. merge: if     1  the merge operation extends b  u v   1  1     1  and the score of the resulting partial alignment is
s1 = ca u v   1  1     1 
where.
1.  붺 k -grow: let  1 = argmaxb  u   k   1 v   1  1 k .
k
 붺 k -grow extends b  u   k   1 v   1  1  1  if   = 1. if u   k   1   1  the score of the resulting partial alignment is
s1 k = ca u   k   1 v   1  1  1 
where
u 1
c = n 1|eu t fv|eu d rv|u l m  y n 1|ep .
p=u k
if u   k   1 = 1  then
.
we choose that operation which gives the best score. as a result  b  u v  1    now represents the best partial alignment so far and a u v  1    represents its score. we have 
	a u v  1    =	argmax
a v  u 뫍ag붺  u 뷋u=   뷋1= 1
 we determine a u v  1    and b  u v  1     for all 1 뫞 u 뫞 l  1 뫞 v 뫞 m  1     뫞  max and . to complete the scores of the alignments  we multiply each n 1|ep  for l   붺 뫞 u 뫞 l.
let
.
the viterbi alignment is  therefore  b  u m  	  1    .
1 time complexity
the algorithm computes b  u v  1    and a u v  1    and therefore  there are entries that need to be computed. note that each of these entries is computed incrementally by a structure transformation operation.
  shrink operation requires o 1  time.
  merge operation requires o 1  time.
   붺 k -grow  1 뫞 k 뫞 붺  operation requires o  max  time for finding the best alignment and another o 1  time to update the score1.
each iteration takes o 붺 max  time. computation of the tables a and b takes time. the final step of finding the viterbi alignment from the table entries takes o m붺 max  time1. thus  the algorithm takes time. in practice  붺 and  max are constants. the time complexity of our algorithm is.
1 space complexity
there are entries in each of the two tables.
assuming 붺 and  max to be constants  the space complexity of viterbiforgenerator is.
ibm models 1 and 1 extending this procedure for ibm models 1 and 1 is easy and we leave it to the reader to figure out the modifications.
1 decoding
the algorithm for the subroutine decodeforgenerator is	similar	in	spirit	to	the	algorithm	for viterbiforgenerator.	we provide the details for ibm model 1 as it is the most popular choice for decoding. we assume that the language model is a trigram model. ve is the target language vocabulary and ie   ve is the set of infertile words.	our algorithm employs dynamic programming and computes the following:
	argmax	a
let h be the set of all partial hypotheses which are translations of f1v and have e1 as their last two words  뷈 is the center of cept associated with e1 and     1 is the fertility of e1 . observe that the scores of all partial hypothesis in h are incremented by the same amount thereafter if the partial hypotheses are extended with the same sequence of operations. therefore  for every h it suffices to work only on the best partial hypothesis in it.
the initial partial hypothesis is h1  . . 1 1  with a score of p1m. initially  the hypothesis set h has only one hypothe-
                                                              m  sis  h1  . . 1 1 . we scan the tuple sequence of g1 left to right in m phases and build the translation left to right. in the vth phase  we extend each partial hypothesis in h by employing the structure transformation operations. let  be the partial hypothesis which is be-
ing extended.
	  shrink:	we	create	a	new	partial	hypothesis
 whose score is the score of  times
t fv|null  pp1  m   m1  1  1  1   1m+ 1   1 .
	we	put	the	new	hypothesis	in	to
h.
	  merge:	we	create	a	new	partial	hypothesis
	whose	score	is	the	score of times
	n   + 1|e1 	|e1  dnew .
       1  t fv	dold n  |e
here  dold is the distortion probability score of the last tableau before expansion  dnew is the distortion probability of the expanded hypothesis after rv is inserted into the tableau and 뷈1 is the new center of the cept associated with e1. we put the new hypothesis into
h.
   붺 k -grow: we choose k words from ie and one word  from ve. we create a new partial hypothesis 
 otherwise. the score of this
hypothesis is the score of times
k
n 1|ek+1 t fv|ek+1 d rv   뷈|a e1  b  fv   y n 1|ep
p=1
	we	put	the	new	hypothesis	in	to
h.
at the end of the phase  we retain only the best hypothesis for each h. after m phases  we append at most 붺 infertile words to all hypotheses in h. the hypothesis with the best score in h is the output of the algorithm.
1 time complexity
 at the beginning of vth phase  there are at most  distinct partial hypotheses. it takes
o 1  + o  max  + o |ie|붺 |ve| max  time to extend a hypothesis. therefore  the algorithm takes totally
 time. now  since |ve|  |ie|   max
and 붺 are assumed to be constant  the time complexity of the algorithm is.
1 space complexity
in each phase  we need space. assuming
|ve| and  max to be constant  the space complexity is
.
extending this procedure for ibm models 1 and 1 is easy.
1 experiments and results
1 decoding
we trained the models using the giza++ tool  och  1  for the translation direction french to english. for the current set of experiments  we set 붺 = 1 and  max = 1. to compare our results with those of a state of the art decoder  we used the greedy decoder  marcu and och  1 . our test data consisted of a corpus of french sentences with sentence length in the range 1  1 ... 1. each sentence class had 1 french sentences. in table 1  we compare the mean logscores  negative logarithm of the probability  of each of the length classes. lower logscore indicates better probability score. we observe that the scores for our algorithm are  1%  better than the scores for greedy algorithm. in the same table  we report the average time required for decoding for each sentence class. our algorithm is much faster than the greedy algorithm for most length classes. this demonstrates the power of our algorithm to search an exponentially large subspace in polynomial time.
we compare the nist and bleu scores of our solutions with the greedy solutions. our nist scores are 1% better while our bleu scores are 1% better than those of the greedy solutions.
1 viterbi alignment
we compare our results with those of local search algorithm of giza++  och  1 . we set 붺 = 1 and  max = 1. the initial generator for our algorithm is obtained from the solution of the local search algorithm.
in table 1  we report the mean logscores of the alignments
  for each length class  found by our algorithm and the local  search algorithm. our scores are about 1% better than those of the local search algorithm.
time  sec. scoreclassourgreedyourgreedy1111111111111111111111111111111111111111111111111111111table 1: mean time and mean logscores
bleunistclassourgreedyourgreedy1111111111111111111111111111111111111111111111111111111table 1: bleu and nist scores
1 conclusions
it would be interesting to see if the theory of alignment generators can be applied successfully for other problems in nlp.
