intermediate decision trees 
	l a w r e n c e b 	h o l d e r 
department of computer science and engineering 
university of texas at arlington 
box 1  arlington  tx 1 

a b s t r a c t 
intermediate decision trees are the subtrees of the full  unpruned  decision tree generated in a breadth-first order an extensive empirical investigation evaluates the classification error of intermediate decision trees and compares their performance to full and pruned trees em pirical results were generated using c1 with 1 databases from the uci machine learning database repository results show that when attempting to minimize the error of the pruned tree produced by c1  the best intermediate tree performs significantly better in 1 of the 1 databases these and other results question the effectiveness of decision tree pruning strategies and suggest further consideration of the full tree and its intermediates also  the results reveal specific properties satisfied by databases in which the intermediate full tree performs best such relationships improve guidelines for selecting appropriate inductive strategies based on domain properties 
1 	introduction 
numerous decision  tree pruning methods have been developed to reduce decision tree error due to overfitting on the part of the decision tree induction method  mingers  1  of course  as schaffer  1  1  emphasizes no one pruning method can improve the performance of decision tree induction on all domains however  certain pruning methods can improve performance on certain domains although this paper presents results indicating the possibility of yet another decision tree pruning method  the results also indicate in which domains such a pruning method might be useful 
　the perspective on which these results are based is the intermediate decision tree section 1 defines the intermediate decision tree  but  in general  an intermediate decision tree is one of the sequence of subtrees generated using a breadth-first traversal of a full decision tree  l e   a decision tree whose leaf nodes contain examples of one class  previous results have shown that the intermediate decision tree compares favorably to other pruning methods  holder  1a   and intermediate learned 
1 	learning 
concepts in general address overfitting issues in both inductive and speedup learning  holder  1  1b  holder and chaudhry  1  the main result of this paper is that the best intermediate tree of a full decision tree is often better  less error  than the pruned tree the suggested pruning strategy would be to find the number of splits leading lo this minimum-error intermediate de cision tree a pre-pruning strategy of performing just that number of splits would produce a pruned decision tree without the need to first generate the full tree 
　at one extreme to this approach  holte  suggests that the smallest intermediate decision tree consisting of one split performs well in comparison lo a larger decision tree however  elomaa  qualifies holte's results by showing the small difference between c1 trees and one-split trees is significant in favor of the c 1 trees elomaa further points out that in some domains simple classifiers will never outperform a multilevel decision tree exhaustive tests run by murphy and pazzam  indicate that the smallest consistent decision trees have more error than slightly larger consistent trees weiss and indurkhya  conclude that with at least 1 examples their cross-validation cost-complexity pruning method outperforms no pruning thus  although pruned decision trees clearly compare favorable to full decision trees  there is no indication that a particular level of tree complexity will prevail 
　the results presented in this paper deal exclusively with decision trees produced by the g1 program  quinlan 1   therefore  section 1 briefly summarizes the properties of this program section 1 discusses the intermediate decision tree in detail section 1 presents the experimental results based on a large sampling of the university of california  irvine machine learning database repository  murphy and aha  1  the appendices describe these databases and show a sample of the ex penmental results section 1 considers specific hypotheses regarding the performance of intermediate decision trees as compared to c1 pruned trees  derives specific conditions underwhich intermediate decision trees perform best  and describes the implied pruning strategy section 1 concludes with directions for future work 

1 	decision tree induction 
the method of decision tree induction used throughout this paper follows the method employed by c1  quinlan  1  c1 uses the standard recursive splitting technique to produce a decision tree whose leaf nodes contain training examples of one class if a split branch yields a node with no training examples  then this node is replaced by a leaf node whose classification is the majority class of the parent node if a node with examples from more than one class cannot be split further  then this node is replaced by a leaf node whose classification is the majority class of the examples at this node 
　c1 uses the gain ratio criterion for selecting the test attribute at each non-leaf node a continuous attribute a is split based on the best test a   t  where t is one of the values appearing in the examples examples with unknown values for an attribute are distributed across all values and weighted according to the frequency of the known values in the examples classification of examples with unknown values is treated similarly  where the class is based on the maximum sum of the weights 
of the classes in the leaf nodes reached by the example although c1 is capable of splitting based on subsets of attribute values  this feature was not utilized in the experiments 
　c1 employs two tjpes of pruning during tree construction c1 requires that an  split result in at least two branches having a minimum number weight mw of examples the default value of mw is 1  but can be changed by a program option larger values of mw help prevent overfitting of noisy data the main post pruning strategy used in c1 is a form of pessimistic pruning in which a subtree s error is estimated based on the binomial probability distribution of e errors occurring within n trials with a confidence factor c f the default value of cf is 1  but can be changed by a program option smaller values of cf tighten the error estimate and increase the amount of pruning in our experiments the values of mw and cf are tuned automatically using a hill climbing approach described in section 1 
1 	i n t e r m e d i a t e d e c i s i o n trees 
an intermediate decision tree  idt  of a decision tree is any subtree in the sequence of subtrees generated from a breadth-first traversal of the internal nodes  splits  of the decision tree for example  figure la-d depicts the ordered sequence of idts for the decision tree in figure le 
　figure 1 reveals the motivation for the breadth-first traversal the figure depicts typical error curves that plot the error of different sequences of idts for a deci sion tree whose final error corresponds to the rightmost point on the curve x the best-first traversal orders the splits based on information gain both the depth-first and best-first traversals indicate high error until a majority of the splits have been performed the breadth-first traversal  however  quickly achieves a low error  which 
   'these particular curves come from the dnf1 domain in  pagallo and haussler  1  
gradually ascends to the final error level this behavior of the trror curves is tvpical for many domains the specific behavior in which the minimum of the breadth-first traversal is less than the final error is the motivation for our interest in intermediate decision trees 
1 	experiments 
in order to evaluate the performance of intermediate trees  an extensive empirical investigation was performed using the decision tree induction program c1 release 1  quinlan 1  on 1 databases from the uci machine learning databases repository  murphy and aha  
1  appendix a describes the databases 
　four different trees were considered for comparison the full tree ft  no post pruning   the pruned tree pt  the best  lowest error  intermediate tree of the full tree ift  and the best intermediate tree of the pruned tree 
	holder 	1 

ipt experiments measured the classification error and number of splits for each tree averaged over 1 trials each using two-thirds training and one-third testing examples randomly sampled without replacement four different experiments were run  each attempting to minimize the error for a different tree 
　a hill-climbing parameter tuner accomplished the minimization by tuning the mw and cf parameters in order to minimize the average test-set error for one of the four tree types starting from the default values 
                     the tuner measures the average error  over 1 trials  for and 
if none of the four new parameter settings improve the error  then the search terminates otherwise  the search continues from the best new parameter setting the resulting optimized trees are subsequently referred to as 
 although the opli 
mized trees consider the test set to tune mw and cf these trees are used only to indicate what is possible the test set is not considered during the generation of the other trees or during the pruning phase 
　in addition to measuring the average error and number of splits for each of the four tree types  the experiment also measured the difference between the error and number of splits for the different pairwise combinations of trees a one-sided  upper-tailed  matched-pairs test measured the significance of the differences subsequent mention of  significantly less' or  significantly greater implies a significance level of 1 or less  i e   the prob ability that the difference is significant is at least 1  
　the results are too numerous to completely tabulate in this paper  but appendix b tabulates a sample of the experimental results when optimizing the pruned tre 
1 	discussion 
many conclusions can be drawn from the experimental results in terms of the performance of intermediate decision trees  the types of databases for which they are useful  and the trees' role in a pruning strategy 
1 	hypotheses 
following is a series of hypotheses drawn from the experimental results the evidence indicates that intermediate decision trees outperform pruned trees in a majority of the databases 
hypothesis 1ft has less error than pt 
evidence 1 the results shown in appendix d indicate that ift has significantly less error than ptop  in 1 databases ptpi has significantly less error than ift in 1 databases results not shown indicate that iftop  has significantly less error than pt in 1 databases  and pt has significantly less error than iftop  in 1 databases 
hypothesis 1 ift has less error than ipt 
evidence 1 	ift has significantly less error than the 
iptop  in 1 databases iptp  has significantly less error than ift in 1 databases iftppt has significantly leas error than ipt in 1 databases ipt has significantly less error than ift in 1 databases 
hypothesis 1 	ipt has less error than ft 
1 	learning 
evidence 1 ipt has significantly less error than ft1pt in 1 databases has significantly less error than ipt in 1 databases has significantly less error than ft in 1 databases ft has significantly less error than iptop  in 1 databases 
hypothesis 1 	pt has less error than ft 
evidence 1 pt has significantly less error than ftopi 1 databases has significantly less error than pt in 1 databases has significantly less error than ft in 1 databases ft has significantly less error than  in 1 databases 
hypothesis 1 ift has less error than ft evidence 1 ift has significantly less error than f rop  in 1 databases iftopi has significantly less error than ft in 1 databases ift never has greater error than ft 
hypothesis 1 ipt has less error than pt evidence 1 ipt has significantly less error than ptopr in 1 databases ipt1prt has significantly less error than pt in 1 databases ipt never has greater error than pt 
　based on the evidence  ift performs better than pt in a majority of the databases  indicating that intermediate trees deserve attention when attempting to minimize the error of a decision tree furthermore  ipt performs better than pt in a majority of the databases  indicating that the pessimistic post-pruning used in c1 still has room for improvement ift tends to outperform ipt indicting that a intermediate tree-based pruning strategy should consider intermediate trees of the full  unpruned  tree instead of intermediates of the pruned tree also  the superiority of ift over ft indicates that the breadth first ordering is capturing a minimum in the error curve less than the error for both full and pruned trees 
　similar results measuring the number of splits indicate that the ift has significantly fewer splits than ft  but 
significantly more splits than ipt results comparing the number of splits between ift and pt are inconclusive thus although ift has less error than pt and ipt this gain comes with an increased number of splits 
1 	database properties 
due to the large number of databases included in this study  we can attempt to discern patterns in the properties of databases in which the intermediate decision tree performs best specifically  we used c1 to produce rules governing the database properties from ta ble 1 after labeling each database as a positive or negative example of the hypothesis specifically  databases in which ift had significantly less error than ptop  were labeled as positive examples  the remaining databases were labeled negative using these examples  the c1 rule generator  quinlan  1  produced the following rules  the numbers to the right of each rule indicate the correct/incorrect classifications  



　these rules correctly classify 1 databases  where the errors consisted of one false positive and five false negatives the last rule  describing 1 databases suggests that ift has less error than pt for reasonablesize databases in which more than 1% of the examples do not belong to the majority class verification of the accuracy of these rules requires application to other databases and is left for future work 
1 	i d t - b a s e d p r u n i n g 
the benefits of the intermediate decision tree  idt  can be utilized in a cross-validation pruning method similar to that employed by weiss and indurkhya  weiss and indurkhya use cross-validation to empirically determine the best cost-complexity tradeoff for pruning the decision tree the same method can be used to empirically determine the number of splits corresponding to the best idt 
　specifically  given a training and testing set  divide the training set into k disjoint subsets for each subset i  generate a decision tree- using the other k 1 subsets and prune back the tree in a reverse breadth-first order until the error on subset i is minimized record the number of splits made in this intermediate tree repeat this procedure for each of the k subsets  and determine the average number of splits for the best idt then generate a new decision tree using the entire training set  but only perform the number of splits  in breadth-hrst order  as determined by the cross validation procedure 
　unfortunately  initial evaluation of this idt-based pruning strategy shows that the c1 pruning outperforms idt pruning in a majority of the databases however  a closer look at the results reveals that idt pruning consistently underestimates the number of splits needed to reach the minimum error of the intermediate decision tree further evaluation of modifications to this idt-based method are necessary to develop a betterperforming pruner  but this is left to future work 
1 	conclusions 
an extensive empirical study has revealed that th = be t intermediate decision tree outperforms the pruned tree produced by c1 in a majonly of the databases avail able from the uci repository the larger number  1  of databases considered in this study allowed the detection of specific patterns in the database properties for which the intermediate decision tree will perform better than the pruned tree therefore  a pruning strategy based on finding the best intermediate decision tree  l e   number of splits leading to this intermediate decision tree in a breadth-fir1t order  may outperform current pruning strategies in databases satisfying the detected patterns further experimentation is required to verify this hypothesis 
　the relationships found between database properties and induction strategies are rare in the literature  and more work here will be of great benefit to the understanding of decision tree induction specifically  these relationships need to be tested on artificial databases designed to verify and refine the patterns the use of the c1 pruning parameter optimizer provides further infor mation from which to discern patterns between database properties and proper parameter settings 
　no one inductive strategy will perform well on all databases however  continued derivation and verification of relationships between database properties and induction strategies will lead to improved guidelines for 
selecting the proper strategy 
a 	databases 
　all databases used in the experiments were taken from the uci repository of machine learning databases  murphy and aha  1  table 1 lists the 1 databases and the corresponding label used for the database throughout the paper although many more than 1 databases are available in the repository  the remainder were not included because they either were domain theories  were data generators  had continuous class values  or  in the one case of david slate s letter recognition database  look loo long to process 
　some of the dat ibases underwent minor modifications for use with c1 the ann database consists of the combined examples from the data and test files  and attributes with missing values for all examples were lg nored for imp the symboling attribute was used as the class 1 if symboling  if symboling  for 
bc r  attributes age tumor size  }nv wodes and dtg 
walig were changed to continuous 
　version 1 or the bridges  br1-br1  databases was used in each database the ldentifier attribute was ignored values of y for the river attribute were changed to 1  and fractional values of the location attribute were rounded to the nearest integer the five databases  br1br1  use attributes 1 and class attributes l-or-d  ma ternal  span re/-/ and type  respectively examples with 
missing class values  1 or mil  were deleted 
　for htc class values of 1 and 1 were changed to 1 to vleld a two-class problem the iior database consists of the examples from both the training and testing files 
attribute 1 was used as the class attribute attributes 1  1  1 and 1 were ignored  values of 1 for the age attribute were changed to 1 and values of 1 for the capillary refill time attribute were changed to 1 
　the flr dmaba&l used the flare1 data file the three class attributes describing which type of flare appeared  if any  were reduced to one binary class attribute as to whether any flare occurred in databases tii1-th1 and th1  attribute 1 was ignored  because all values were mssing 
	for 	the 	sponge 	 spg  	database  	attribute 	1 
 l papilas  was used as the class attribute for its even distribution and minimal dependence on other attributes attribute 1  sponge.name  was ignored as was attribute 1  l numero de papilas  for its depen dence on the class attribute attributes 1 were typed continuous and case 1's illegal value of s1.tipo for attribute 1 was set to 1-tipo 
　the wisconsin breast cancer database  bcw  was originally obtained from the university of wisconsin 
	holder 	1 

table 1 three-letter labels of 
	label 	database directory/file  
	ann 	an ne-ali ng / vine aj 
	aud 	audiology/audiology at and archied 
	imp 	autoa/imports-bs 
	bal 	balance-scale 
	ba1 	bajloonj/adult+st retch 
	ba1 	balloons/adult stretch 
	ba1 	balloons/yellow small 
	ba1 	balloona/yellow small+adult ureidi 
bcr breast cancer bcw breast cancer wisconsin 
	br1 	bridges/material 
	br1 	bndgea/re.1 
	br1 	bndges/ipan 
	br1 	bridge*/1 or d 
	brs 	bridges/type 
	chs 	chess/king rook vs-king pawn/kr vs-kp 
crx credit screening/erx flo flags/flag1 
	gls 	glass 
	har 	h ayes-roth 
	htc 	heart diveaae/cleveland 
	hep 	hepatitu 
	hor 	horse-colic 
	seg 	image/segmentation 
	ion 	ionosphere 
	iri 	ins 
	lab 	labor negotiations/labor neg 
	len 	lenses 
	ldb 	liver disordeni/biipo 
	lng 	lung-cancer 
	lym 	lymphography 
	mm 	monlts-problems/monwkl 
	mk1 	monks problems /monk- 1 
hospitals in madison thanks to dr william h wolberg  mangasatian and wolberg  1  the breast cancer  bcr   lymphography  lym  and primary tumor  pri  databases were originally obtained from the university medical centre  institute of oncology  ljubljana yugoslavia thanks to m zwitter and m soklic the 
cleveland heart disease database  htc  was originally obtained from the cleveland clinic foundation thanks to dr robert detrano 
　table 1 lists the properties of the individual databases the number in parentheses next to the database label is the number of different class values appearing in the data size is the total number of available examples .error is the classification error on all examples by guessing the most frequent class appearing in the data the next ten columns describe the non-class attributes of the database  empty entries indicate zeros in the first eight of these columns cont is the number of continuous attributes the next seven columns show the number of discrete attributes with the indicated number of different  non-missing  values appearing in the data miss is the number of attributes containing missing values in the 
data  and tot is the total number of attributes 
b 	s a m p l e of e x p e r i m e n t a l r e s u l t s 
experimental results for the 1 databases were generated for the optimization of the full tree  pruned tree  intermediate full tree and intermediate pruned tree the number of splits used for each tree was also recorded  but only the error was optimized the last three columns of table 1 show a sample of the results when optimizing the pruned tree produced by c1 according to the hill-
1 	learning 
d at abases used in the experiments 
label database directory/flle mk1 
mus 
pid 
pop 
pri 
slc 
flr 
sb1 sbl ore 
lrs 
spg 
aus 
hrt 
sat sht veh 
thd 
th1 
th1 th1 
th1 th1 
th1 
th1 ths th1 
ttt trn 
son 
vot 
win monks-probkms/monks-1 mushroom/ agaric us-lepiota pima-mdians-diabetes 
postoperative-patient data/post operative primary tumor 
shuttle-landing control 
solar flare/flare soybean/soybean small soybean/soybean large space-shuttle/o-nng eroaion 
peel rometer/ ira 
sponge 
statlog/australian 
tailog/heart tatlog/sat image/sat tatlog/ihuttle tatlog/vehicle 
thyroid diaease/alibp 
thyroid disease/allhyper thyroid disease /all hypo thyroid diseate/allrep thyroid diieaae/anri thyroid disease/dis thyroid disease /hypo thyroid thyroid disease/new thyroid thyroid disease/sick euthyroid thyroid disease/sick tic tac toe trains 
undocumented/connectionisl ber ch/sonar 
voting records/house-voles-1 vine zoo 	too 
climbing optimization strategy described in section 1  the first two columns show the average error and standard deviauon for 1 trials using two-thirds training and one-third testing sampled without replacement the fi nal column shows the difference between the error of 1ft and pi opt the significance level of a one-sided  upper tailed matched pairs test is shown in parenthe-
ses  where a value of 1 or less indicates a significant positive difference and a value of 1 or more indicates a significant negative difference 
