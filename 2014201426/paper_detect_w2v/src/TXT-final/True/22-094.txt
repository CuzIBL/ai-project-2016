 
this paper describes an explanation-based approach lo learning plans despite a computationally intractable domain theory. in this approach  the system learns an initial plan using limited inference. in order to detect plans in which the limited inference causes a faulty plan the system monitors goal achievement in plan execution. when a plan unexpectedly fails to achieve a goal  or unexpectedly achieves the goal  a refinement process is triggered in which the system constructs an explanation for the expectation violation. this explanation is then used to refine the plan. by using expectation failures to guide search the learner avoids a computationally intractable exhaustive search involved in constructing a complete proof of the plan. this approach has the theoretical property of convergence upon a sound plan. 
1 introduction 
most explanation-based learning ebl   dejong1  mitchell1  systems require that their domain theories be complete  sound  and computationally tractable1. but in real-world domains  large amounts of knowledge are necessary to adequately describe world behavior. with the necessary complex domain theory  complete reasoning becomes a computationally intractable task. 
　consider getting to work in the morning. your plan might be influenced by weather conditions  road construction  traffic conditions  your spouse's plans that day  small errands you might have to run  and the condition of your car. the number of potentially relevant factors is enormous. 
yet how do people plan in complex situations  
one way in which this is accomplished is by using simplifying assumptions. by using simplifications  people can develop approximate plans despite complexity - and subsequently use and refine these ap-
this research was supported by an ibm graduate fellowship  a university of illinois cognitive science/artificial intelligence fellowship  the office of naval research under grant n-1 -1-k-1  and the national science foundation under grant nsf-iri-1. proximate plans as the flaws in the plans become apparent. 
　　the paper describes an approach to explanation-based learning using a complete and sound but intractable domain theory. in this approach the system uses limited inference in determining interactions between actions in the plan. because this limited inference introduces the possibility of learning flawed concepts  the system monitors the execution of learned plans. when the system observes a discrepancy between the predicted and observed goal achievement  it begins a refinement process in which the violated expectation is explained. if the discrepancy is an unexpected plan failure  the explanation describes how a support in the plan is blocked. this explanation represents a set of inferences not previously explored by the system because of inference limitations. if the discrepancy is an unexpected plan success  the explanation describes how an anticipated failure was prevented by a method previously not considered due to inference limitations. in both cases the explanation is then used to modify the plan to avoid the incorrect prediction in the future. 
　by using expectation violations to direct the search for plan interactions the system avoids the computationally intractable blind search for interactions. additionally  a concrete example of the failure aids the explanation process and subsequent failure analysis. however  these computational gains come at the cost of the failures used to guide the search. 
1 overview 
our approach to incremental explanation-based learning has three requirements. first  that a complete and sound domain theory exists. this means that in theory the system can generate sound explanations given unbounded time and computational resources. second  that the system is given a set of default methods to use to determine approximate truth values1. third  that the utility of a correctly learned concept is adequate to offset the cost of the failures required to learn it in our approach. this approach has been tested by implementation of an incremental ebl system. this model of problem-solving and learning consists of four steps: initial 

learning  expectation violation  explanation of expectation violation  and knowledge modification. 
1. initial learning: the system can learn plans from internal problem-solving or from observation. in each case  the initial plan is an explanation for goal achievement using limited inference. 
1. expectation violation: there are two types of expectation violations: unexpected successes and 
unexpected failures. unexpected failures can result from problem-solving or observation and occur when a plan explanation for goal achievement  either observed or constructed by the planner  is applicable but the goal is not achieved. an unexpected success occurs when the system observes a plan from its plan library and predicts failure  due to an applicable failure explanation attached to the plan in the plan library  but observes the plan to succeed. 
1. explanation of expectation violation: the system constructs an explanation of the violated expectation. 
1. knowledge modification: the system analyzes the explanation of the violated explanation to determine how to modify the preconditions and/or ordering constraints of the plan. 
the system uses a representation based on situation calculus which significantly extends strips operators. facts can be asserted in four ways. first  a fact may be a direct effect of an operator. second  unlike strips operators  a fact may be asserted as an inferred effect of an operator  allowing representation of operators whose effects depend upon the state of the world in which they are executed  e.g. if i pull the trigger of a gun  the effects depend upon whether or not the gun is loaded  the direction in which the gun is pointed  etc. . third  a fact may be inferred to be true by intra-situational rules  e.g. if a is above b and b is above c then a is above c . finally  of key importance to this research  a fact may be assumed to be true by a persistence assumption. 
　in a complete inference model  as each new situation is caused by the execution of an operator  the system must exhaustively prove each fact in the new situation. in a complex domain  persistence of facts may depend on many other factors  making computation of updates expensive. this is the qualification problem  shoham1  and is one aspect of the frame problem. in general  learning using complete reasoning requires determining the truth value of facts in a non-linear plan involving conditional effects which is np-hard  chapman1|. 
　in order to deal with this difficulty the system performs limited updating based on inferred effects. concretely  the system examines inferred effects only when direct effects cannot account for observations. for example  if an operator is executed  and direct effects do not predict that one of its preconditions was true  the system attempts to prove the precondition involving inferred effects. complete updating of direct effects is performed. the system also uses a defeasible persistence rule to allow potentially unsound updating of the model. a formal definition of the persistence simplification rule is:1 
 v p si sj sk in p si    - 1   contradicts q p  
	  i n   1   s j   a 	precedes si sj  a precedes sj sk   
　　　　　　- in p sk  where in p s  means that the fact p is believed by the system in situation s  a fact p is said to contradict a fact q if p is ~q or if p and q specify conflicting attribute values for the same object  and precedesa b  means that the situation a temporally precedes the situation b. intuitively  this rule states that a fact p can be assumed to persist to a later situation provided it is not explicitly contradicted by a belief in an intervening situation. an important point to note is that because the system performs complete updating of direct effects  any incorrect persistence simplification must be due to an unconsidered inferred effect. 
　a plan is a constrained set of operators to achieve a desired world state and consists of a goal state  plan preconditions  operators  ordering constraints  and a causal explanation. the goal state is a partial situation that the system believes will be true after the execution of the plan. the plan preconditions are a partial world specification required for the proper execution of the plan. operators is the set of the operators which cause the goal state to be true. ordering constraints is a set of constraints upon the order in which the operators may be executed to achieve the goal. the causal explanation is a description of how the plan preconditions and operators achieve the goal state. 
　in the following section  the initial learning process is described and an example shown. then the refinement process consisting of the detection  explanation and modification steps outlined above is described and several examples are discussed. finally  comparisons to other work and future work are covered. 
1 initial learning 
there are three parts to learning an initial plan representation: 1  goal explanation  1  explanation generalization  and 1  computation of ordering constraints. as the system processes operators in a training example  the system maintains a causal model. the causal model is the systems view of the world updated using limited inference as described above. after processing the operator sequence  the system constructs a causal explanation for goal achievement in a backward-chaining best-first manner with preference towards simplest explanations. this explanation is then generalized using the eggs technique  dejong1  to produce the plan preconditions. 
 1 these inferences arc currently implemented by specialized lisp code. 	development of a single general inference engine for both defeasible and standard inferences and a declarative representation for defeasible inferences are areas for future work. 
	chien 	1 
　next  the system computes constraints required by supports in a plan. for a fact to be true at a given state  it must be true in a previous state  achievement  and remain true until the state it is needed  protection . the achievement constraint requires that a fact be achieved before it is needed. consequently when there is a chain of inferences  effects  or persistence assumptions from an operator a that establishes a precondition for operator b  a must be ordered before b. for example  suppose an object must be at the drilling station in order to be drilled. then if an object is moved to the drilling station and then drilled  the move operator would support the drill operator through the location precondition of the drill operator. the system determines support relations by examining the explanation for goal achievement and posts these ordering constraints upon the plan. 
　additionally  if a fact is achieved prior to immediately before it is needed  it must not be falsified by another action before it is needed. this is a protection constraint and is satisfied by ordering the clob-
bering action before the protected fact is achieved or after the protected fact is needed. note that 1  if the fact is true from the initial state the clobberer cannot be moved before the protection interval and 1  if the fact is part of the goal specification the clobbering action cannot be moved after the protection interval. some clobberings will be dependent upon how the plan is instantiated and can be prevented by posting constraints on variable bindings. for example  - on  x  y  will clobber  on  z  y  only if  z and  y refer to the same object. this results in an ordering constraint dependent on the plan instantiation. returning to the drilling station example  the location of the object to be drilled must be protected from the time it is achieved by the move until the time it is used by the drill operator. if at another time in the plan the object was moved to another location  this other move cannot be executed during the protection interval because it would clobber the fact that the object is at the drilling station. 
　the system computes protection intervals by tracing each precondition or goal support to the action which asserted it  or the initial state . these facts are then compared against effects of actions in the plan to determine potential clobberers. each interval and action for which there exists a legal instantiation of variables which would cause the action to clobber the interval causes a protection constraint. 
　in order to clarify the initial learning process  an example from the system's workshop domain will now be discussed. in this example  the system learns a plan to construct a small assembly called a widget. in this plan  the system is shown an initial state with a metal rod  a gear  and a sheet. first the rod is heated and rolled. next  the gear and sheet have holes drilled in them. then the rod is inserted into the gear for a tight friction fit and through the sheet with a loose fit that allows the rod to spin. the system is told that this assembly is a widget. the initial 
machine learning 

the system explains how the operators achieve the definition of a widget which involves a gear and rod spinning with respect to a sheet. 
　the simplified explanation for the plan uses 1 rules  1 persistence assumptions  and 1 initial state facts. the simplifications covered facts persisting through 1 situations. our exhaustive explanation used 1 rules to prove persistence  resulting in a total of 1 rules. hence  the reduction in complexity  1 / 1 = 1  over an order of magnitude in proof size  by the persistence simplifications is significant. these savings are even more significant when considering that the explanation search space is exponential in the proof size. 
　yet the temporal limited inference used in learning this plan is not without cost. in this example  the system makes the assumption that the shape of the gear does not change from the start state through the end of the plan. in the exhaustive formulation this persistence depends upon the fact that the gear is made of metal. however  the simplified plan does not account for place any restriction upon the composition of the gear used in the plan. 
　the learned plan abstracts many of the specifics of the example. for instance  in the example  the rod and gear were at the same initial location. because it is not required by the explanation  the plan does not have this constraint. additionally  the ordering of the actions in the plan is only partially constrained by support and protection constraints. for example  moving the rod to the oven  ml  must precede the heat operator  h1  because the move operator supports the heat operator through the location precondition. however  the plan does not include any interaction between the preparation of the gear  rod  and sheet before the insert. 
1 refinement 
refinement consists of three steps: expectation violation  explanation  and modification. the detection is done by monitoring goal achievement. the system monitors the top-level goal that is given to the problem-solver.1 when there is a discrepancy between 
expected and observed goal achievement  the system begins the refinement process. 
　given this monitoring strategy  there are two possible cases: unexpected goal failure and unexpected goal achievement. in the case of unexpected goal failure the system encounters a plan execution that it expected to succeed but the plan fails. because the domain theory used by the system is sound  a failure must be due to a violated persistence assumption which results in a protection interval violation. this failure explanation represents sufficient conditions for the failure and provides information on how the failure affects the plan applicability. there are three ways in which the failure can be prevented: 
1. ordering: the failure explanation describes an inferred effect of an operator clobbering a protected fact. if this operator is ordered before or after the interval it cannot clobber the protection and the failure will not occur. 
1. block failure constraints: the failure explanation has preconditions and ordering constraints. if any of these constraints arc not satisfied  the failure explanation is not applicable and the failure will not occur. for example  suppose a failure explanation has an operator a which establishes a condition  fact  causing operator c to clobber a protection in the plan through an inferred effect. this failure explanation would require a ordered before c. if c were ordered after a the failure would be prevented. 
1. block persistence in failure explanation: if a persistence assumption in the failure explanation is blocked by an effect of an operator in the plan the failure will not occur. this blocking will impose additional constraints upon the plan in several ways. first  all of the constraints of the blocking explanation must be satisfied  ordering and preconditions . second  the effect must occur within the protection interval in the failure explanation  the converse of protection . 
　because of the potentially large number of persistence assumptions in the failure explanation and the expense of planning the system performs limited search to find block persistence in failure explanation methods of failure prevention in this situation. the applicability conditions of the refined plan arc formed as follows. concretely  let p be the original conditions  o be the constraints on an ordering fix  c be the constraints to block a failure constraint  and s be a disjunct of the ways to block failure persistences found by limited search. the new constraints on the plan are  p a  o v c v s  . 
　in the case of unexpected goal achievement  the system observes a plan that it expects to fail but the plan succeeds. because the limited inference always results in over-generalization  the incorrect prediction 
 while it is possible to monitor subgoals  there is a tradeoff between monitoring effort and debugging effort. more detailed monitoring facilitates concept debugging because discrepancies may be delected closer to the underlying causes. 
must be due to an over-general failure explanation. since the system's failure analysis regarding ordering and block failure constraints is complete  the flaw must be due to an occurrence of block persistence in failure explanation as described above. directed by this expectation failure  the system explains how a persistence in the failure explanation is blocked. the effective preconditions of the plan are now  p   o v c v s v n   where n represents the constraints needed to produce the blocking persistence method occurring in the example. 
　in order to clarify the refinement process  two examples of refinement for the widget plan are now described. in the first example shows an unexpected failure resulting in a precondition refinement to the plan. in the second example an unexpected success is observed and the system learns a method for preventing the failure by adding an operator to the plan. 
　in the first example  the system attempts to use the widget plan and the plan fails. the system backtraces the support network in the plan and verifies supports for the goal which should be true in the goal stale. the system determines that all of these supports are met except that the gear is no longer gear-shaped. it determines that in the real world the gear is deformed in the final situation. 
　next  the system explains why the gear is deformed in the final situation. the system derives the explanation shown in figure 1 . the deformed shape of the 

gear is explained as follows. the gear gl is plastic; it became hot in situation s1 and melted  causing it to be deformed. the plastic composition property of the gear persisted from the start state. the gear became hot because it was at the same location as the rod  caused by the last moves of the gear and rod  and the rod was hot from being heated  from the heat operator . this failure explanation is generalized using the egos algorithm. the generalized explanation is then used to determine ways to prevent the failure using the three methods described above. 
	chien 	1 
　first  the system checks for an ordering fix. however  because the shape of the gear is an initial state fact used in the goal state  it is not possible to order the plan actions around the protected interval to prevent the clobbering. thus there is no ordering fix to the failure. 
　second  the system tries the block failure constraints method. the failure explanation has the precondition that the gear be plastic  hence if the gear is not plastic  the failure does not apply. the system also determines that the failure ordering constraints arc that the heat operator must be ordered before the move operators. however  this constraint is required by the plan  and hence cannot be denied. 
　finally  the system tries the block persistence method but is unable to find a way to prevent the failure due to the limited search constraint. the first refinement example concludes with the plan being modified by adding the constraint that the gear not be plastic. 
　the second refinement example demonstrates the process of refinement due to unexpected successes. the system observes an initial state in which the gear is plastic and an operator sequence in which the metal rod is cooled after being rolled. the system predicts that the plan will fail because the failure conditions for the deformed gear failure from the previous example applies to the current example. however  the plan is observed to succeed  resulting in an unexpected success. hence  a persistence assumption in the failure must have been blocked by the new plan. the system explains that the cooling step effect of the rod being cool blocks the persistence assumption regarding the hot temperature of the rod in the failure explanation  thus preventing the deformed gear failure as shown in figure 1. 
　because the cooling step did not appear in the general explanation for goal achievement  it was previously thought by the system to be an unimportant action. while in theory the system had the base-level knowledge necessary to understand this prevention upon initially learning the plan  this would have required the system to postulate the possibility of the melted gear failure which is a computationally intractable process. however  after the system has observed the failure and learned the corresponding refinement  the system predicts that the plan will fail and observed data is that the plan has succeeded. this expectation failure focusses the system's attention on portions of the failure that can be blocked; allowing tractable understanding of the preventive measure  chien1 . 
　the next step is to find the appropriate constraints on the plan required to prevent the expected failure. this is done by propagating the constraints between the plan and failure explanation  yielding the correspondences between the objects and values in the failure and the objects and values in the plan   computing the constraint that the effect of the cool action must block the persistence assumption in the failure explanation  and adding the supporting causal expla-
machine learning 
nation for the cooling step to the plan. this process results in the constraint that the object that is cooled must be the rod. next the system analyzes the ordering constraints. it finds that the cooling step must occur during the protection interval in the failure in order to block the failure. this requires that the cool step be ordered after the rod has been heated but not after both moves1. the new plan constraints are formed by adding these constraints in the disjunct for blocking preventions. 
1 discussion 
other related work includes techniques by mooney 
 mooney1  which addressed order generalization for strips operators from a standard fbl framework and did not involve refinement. additionally there have been numerous failure-driven refinement systems including  doyle1  gupla1  hammond1  which also deal with intractability but address neither refinement from unexpected successes nor order generalization. prodigy  minton1   failsafe  mostow1   and soar learn control rules from search failures  failures to find a solution  rather that failure to achieve the goal  as described in this paper  and hence do not make domain-level defeasible inferences. abstraction work in soar  unruh1  also does not make defeasible domain-level inferences. other work by chien  chien1  also involves learning from unexpected successes but these successes due to multiple  disjunctive  methods to achieve goals not unsound simplifications. the general approach of refinement directed by violated expectations is similar to that described in  hayes-roth1  schank1 . another approach to learning in intractable domains involves constructing explanations in a transformed concept space such as  tadepalli1  which learns goal structures of chess plans. a related research area addresses intractability in- using learned plans  kellcr1  while our work focusses on intractability in learning plans. 
　the algorithm presented in this paper has the following theoretical properties: 
1. complete model convergence  soundness : as the plans are modified to prevent observed failures by methods which do not add operators the plan analysis converges on that dictated by the exhaustive analysis. because we assume there are a finite number of inferred effects for each operator and operators in the plan  there are a finite number of potential clobberings. with each failure  the planner uncovers a previously undetected clobbering. as a result  the planner will eventually consider all clobberings. 
1. valid explanation generation  completeness : suppose we have a set of examples for which there is a general plan for achieving a goal in our representation  e.g. no loops  conditionals  etc. . the learning algorithm described will eventually find an explanation which covers the examples in the complete analysis. basically  the proof proceeds as follows. suppose we 
 1 note that the cool step must also follow the roll step to prevent clobbering the temperature protection interval which runs from the heat step to the roll step. 
generate an explanation structure. because of the complete model convergence property  any non-covering explanation cannot be refined to cover all the examples. hence we can discard any explanation which fails in this manner. because there are a finite number of operator effects and operators  there are a finite number of explanations for any given number of inferences. because explanations are generated in order of increasing number of inferences we will eventually generate any explanation of finite length. since there exists a covering explanation and we discard non-covering explanations  we will eventually generate a covering explanation. 
1 conclusion 
this paper has described an approach to dealing with the complexity of learning plans involving inferred effects. in this approach  the system uses limited inference and defeasible inference rules in learning an initial plan. because this limited inference may result in a flawed plan  the system monitors goal achievement. incorrect prediction of goal achievement triggers a refinement process in which an explanation of the expectation violation is used to modify the plan. this refinement process is shown to produce a sound covering explanation. by using expectation violations to direct the search for inferred interactions  computationally intractable blind search through the space of possible interactions is avoided. additionally  because the system has a concrete example of the expectation violation to explain  the explanation portion of the refinement process is simplified. thus  by using the examples to guide the learning process  our approach can learn correct concepts despite the computational difficulties of a complex domain theory. 
acknowledgements 
this work benefited from discussions with ray 
mooney. comments and direction from my advisor  gerald dejong  and the rest of the csl learning group are also gratefully acknowledged. 
