panel discussion 
under what conditions can a machine attribute meanings to symbols 
aaron sloman 
 panel chairman  
university of sussex 
     
it is easy to produce arguments purporting to show that computers can never be conscious  have purposes  understand anything. one standard response is to claim that since computers can manipulate symbols they have the potential to support any kind of mental state or process  if appropriately programed. the task is then to investigate what sort of programming w i l l transform a symbol user into something with mental states. much work in al is concerned with specific subgoals of this investigation  taking it for granted that machines can manipulate symbols with meanings. 
     but can this be taken for granted  certainly computers construct and transform patterns. we can interpret them as meaningful  but it does not f o l low that the computer does  any more than a f i l i n g cabinet interprets documents. if interpreting symbols already presuppose being conscious  having beliefs and goals  etc.  then the ability to assign meanings to symbols cannot explain these other a b i l i t i e s . 
     one popular reply is to postulate sensors and motors. no mind or computational process can refer to particular events or objects in the physical world without being causally embedded in the world. perhaps even reference to properties and relations 
 universals  requires such causal embedding. so if we find suitable causal relationships between internal processes and external events  then this would seem adequate to justify the claim that the internal processes use symbols with a meaning. 
     but is this necessary for every form of mentali t y   every meaningful use of symbols  can't we imagine someone totally paralysed  blind  deaf  anaesthetised  yet conscious  thinking  even experiencing hallucinatory physical sensations - or at least solving problems about number theory  or do we only think we can imagine this  people often assemble meaningful words into deceptively incoherent phrases  like  the view of the universe from outside time . 
     1 claim that we should accept the a n t i behaviourist intuition that the meaningful use of symbols can be purely internal. this is not just a semantic quibble about the meaning of  meaning . rather  the point is that different sorts of internal computational architecture have profound implications for different sorts of mentality  independently of external relations. it is s i g n i f i cant that computers have been designed in such a 
     way as to support talk of machine-languages  instructions  addresses  conditionals  logical operations  arithmetic operations  and the l i k e . the use of such terms in computer science does not depend on external relations. it is justified by the structure of internal processing. for instance  a computer can use numbers to count internal operations  or to count internal symbols  just as we use them externally. 
     the issues are surprisingly tricky. given an uninterpreted computer you may be able to work out which symbols the machine interprets as booleans. but could you t e l l which one was taken as  true  and which as  false   can the distinction can only be made only in terms of a computational architecture rich enough to support a division between beliefs and goals  we don't notice the need for this normally because machines come with manuals which beg the question by telling us which operation is  and   which  or   etc. 
     vve should explore the ability of different sorts of computational architectures to support such notions as reference  instruction  goal  description  truth  plan  desire etc. in particular  we need to understand the conditions under which a machine could have beliefs and goals relating only to internal states and processes  such as the belief that two l i s t s have the same contents  or a goal like counting the number of even integers in an internal l i s t .  the differences between beliefs and goals lie only in their roles in processing.  the relevant architecture need not be directly implemented in a physical machine. it could be a virtual machine implemented in a lower level machine with a simpler architecture. all this contradicts a philosophy which stresses the social  communicative  role of meaning. 
	bo ordinary digital 	computers  	suitably 	pro-
grammed  seem capable of supporting quite complex notions of meaning and mentality independently of external connections. paradoxically  it is far less obvious how brains can support the selfreferential a b i l i t i e s which enable ordinary computers to be described in mentalistic language. do brains intrinsically require external connections for their use of symbols with meanings  
     i have deliberately invited a biased panel to discuss these issues. when one side of a debate is so easy to argue for  we may hope to profit most by listening to the other side. 
