 
problem solvers fall along a wide spectrum ranging from highly deliberative to highly reactive. highly deliberative systems are able to design optimally efficient solutions to problems  but they require complete world models and consume inordinate computational resources. reactive systems move in real time but cannot guarantee efficient solutions. they are also subject to looping behavior. one way to generate incrementally more efficient solutions is to be incrementally more deliberative  e.g.  to increase the amount of mental search between actions. this paper presents an alternative method for generating more efficient solutions: increasing the number of reactive agents simultaneously attacking a given problem. this method provides a second  orthogonal degree of freedom. we find that in many domains  increasing agents is dramatically superior to increasing single-agent deliberativeness. this is because solution quality improves rapidly as more reactive agents are added  but search time only increases linearly. this contrasts with adding more deliberativeness  which incurs exponentially increasing time costs. ample empirical evidence is presented to support our conclusions. 
1 introduction 
this paper considers two aspects of computational problem solving: 
 1  search t i m e - h o w long it takes to come up with a solution. 
 1  solution quality-how good that solution is  in terms of resources needed to execute it. 
there is an intuitive trade-off between  1  and  1 . the longer we think about a problem  the better chance we have of finding a good solution. while search algorithms like a*  hart et al.  1  strive to limit  1  while optimizing  1   time limitations often force us to settle for suboptimal  or  satisficing   simon  1   solutions. 
   different situations will place different emphases on  1  and  1 . consider the problem of sending an interplanetary probe to neptune. in this case  it may be worth spending days or weeks to plot an optimal trajectory  since such calculations could save months of travel time. on the other hand  consider the case of hernan cortes  the spanish conqueror of mexico. while still a teenager in spain  finding himself on the wrong end of a jealous husband's musket  cortes immediately devised a plan to travel to the new world. the efficiency of his plan was not critical. what was important was that he get started right away. 
   this paper studies search time versus solution quality in the context of the real-time-a*  rta*  algorithm devised by  korf  1 . the next section reviews how rta* interleaves planning and execution  and how this leads to a flexible time/quality trade-off. subsequent sections introduce new algorithms and empirical results. 
1 real-time heuristic search 
motivated by research on two-player games   korf  1  investigated single-agent search under the constraints of having to take action within a given time limit and/or having limited information about the environment. sample single-agent search tasks include robot navigation  the blocks world  and the 1-puzzle  figure 1 . korf's algorithm  called real-time-a*  rta*   alternates between two phases: plan and execute. during each planning phase  rta* makes a decision about which action to take  based on the current situation. it then executes the action in the world  and starts planning again. this continues until it reaches its goal. rta* can vary the amount of planning versus executing it does by changing how deeply it looks into the future during the planning phases. here is the algorithm: 
1. set variable n to the start state. 
1. generate all of the successor states of n. if any of the successors is the goal state  then move to the goal and quit. 
1. estimate the heuristic value of each successor s by performing a fixed-depth tree search rooted at s. 
1. let s1 be the successor with the best backed-up value. let v1 be the value of the second-best successor. take whatever action corresponds in the world to moving to state s1. store state n in a hash table as a key with value v1. if the state n is ever generated again in step 1  use the value stored in the table instead of performing the search of step 1. 
1. set n to s i   and go to step 1. 
   although rta* may enter the same state several times  the values of previously visited states  stored in the hash table  prevent rta* from entering a fixed loop. the depth of the tree search in step 1 determines how much time rta* spends planning instead of executing actions. 
   rta* is useful in both complete- and incompleteinformation domains. when information about the world is incomplete  it is impossible to plan out an entire solution ahead of time. in such a case  interleaving planning and execution is necessary. the algorithm's utility in complete-information domains comes because large search spaces impose practical limitations to lookahead search. while the optimal solution to a 1-puzzle problem may contain 1 moves  current computers would take months or years to exhaustively search a tree to that depth. rta* solves such problems by making the move that seems locally best  recording that move in its hash table  and repeating until the goal is reached. no current techniques based on heuristic search can find optimal solutions to the 1-puzzle  yet rta* returns a solution in seconds. the catch is that the solution is not optimal. 
   korf demonstrated that by increasing the lookahead horizon  he could induce rta* to come up with shorter solutions to the 1-puzzle  using the standard manhattan distance heuristic function . figure 1 illustrates this phenomenon. the top curve is the one reported by korf: it is the actual number of steps  executed  by rta*. the lower curve represents the number of steps left after we have removed the cycles from the solution path. 1 of course  if we were using rta* in a real-time application  we would not be able to remove those cycles-the cost would have already been incurred. for the remainder of this paper   solution quality  refers to the length of a solution with cycles deleted. 
   of course  high quality plans come at a cost. as we increase the lookahead horizon  we produce better moves  but individual moves require more time to contemplate. figure 1 shows the well-known exponential nature of tree search.1 
1
　　we have found that a slight modification to the rta* algorithm allows it to delete cycles during the search. 
1
　　in this and subsequent figures  time means user time  in seconds  of a c implementation of rta* running on an ibmpc/rt. due to the large number of runs  most experiments in this paper were performed on the 1-puzzle rather than larger puzzle sizes  but see section 1 for results from the 1puzzle. 

lookahead horizon  h  
figure 1: solution quality as a function of lookahead horizon in the 1-puzzle. plotted points are average values of rta* running on 1 randomly generated problems.  dotted line = optimal solution quality . 
   the next step is to compare solution quality and search time  as shown in figure 1. each point along the curve marks a particular choice of lookahead horizon. this data confirms one of the surprising results of  korf  1 : if our goal is simply to find a solution-any solution-to the 1-puzzle  the fastest way to do it is to set the lookahead horizon to 1. that is: don't plan  just move. be reactive. 
   one way to interpret the data in figure 1 is as follows: if you have t seconds to spend looking for a solution  expect to find a solution with quality q - f t . likewise  if you desire a solution of quality q  expect to spend 
 q  seconds looking for it. thus  figure 1 gives us a whole range of deliberativeness and reactivity to choose from. 
1 multiple agent search 
the problem with relying on figure 1 is that rta*'s behavior is highly erratic. the data points in figure 1 are averages of 1 trials each. figure 1 shows a 1-triai histogram of solution quality for a reactive agent  lookahead horizon of 1 . why the unpredictability  since rta* makes decisions based on limited lookahead  various alternatives often look equally good. in that case rta* must make a random choice. of course  it may end up finding a terrible solution  and taking a long time to boot. 1 
   how can we fix this problem  taking a clue from figure 1  we might run 1 independent agents to completion  then consult the agent that found an average-
1
　　actually  there are two sources of variation: one is rta*'s random choice mechanism  and the other is the fact that some instances of the 1-puzzle are harder that others. the latter source of variation has a minimal effect  however: no instances require 1-move solutions  but rta* routinely returns such solutions. 
	knight 	1 


figure 1: search time per move as a function of lookahead horizon in the 1-puzzle 

figure 1: solution quality vs. search time in the 1puzzle. optimal solution quality is represented by dashed line. small numbers indicate different values of the lookahead horizon  h . points plotted are averages of 1 trials each. 

figure 1: variance in solution quality over 1puzzle problems  lookahead horizon h = 1 . 

figure 1: solution quality as a function of the number of reactive agents solving instances of the 1-puzzle. compare with figure 1. 
length solution. but then  we might as well take the best solution instead of the average one. what kind of solution quality can we expect to see if we take the best of n agents attacking a single problem  
   figure 1 shows how the number of agents affects solution quality. the figure depicts maximally reactive agents  lookahead horizon of 1 . note that solution quality improves with each additional agent  just as it improved when we increased the lookahead horizon of a single agent. 
1 increasing deliberativeness versus increasing the number of agents 
at this point  we have two distinct methods for improving solution quality. we already know the exponential time costs associated with increasing the lookahead horizon. the next step is to investigate the cost of increasing the number of agents. then we will be able to construct a new time versus quality curve. 
　the cost depends crucially on how the multiple agents are implemented. there are at least three possibilities: 
 1  end to end-run several agents  one after another  on a sequential machine. 
 1  parallel-run all agents simultaneously  each on its own processor. 
 1  dovetail-simulate parallelism on a sequential machine by repeatedly giving each agent a time slice. 
   in case  1   search time increases linearly with the number of agents  here  k1 is simply the average solution time of a single trial. in case  1   search time decreases with number of agents. this is because when one processor finds a solution  all processors can halt. the more processors we have  the more likely it is that one of them will find a very good solution very quickly. in the limit  we will find optimal solutions. at that point  adding more processors will cease to improve either solution quality or search time. 
   case  1  is a very practical method for sequential machines. like parallel search  dovetailing terminates when any one of the independent agents succeeds. in the case of large n  time increases linearly with n . if n is 


figure 1: search time as a function of number of agents in the 1-puzzle.  total steps taken is proportional search time here  since h is fixed at 1 . compare with figure 1. 
so large that near-optimal solutions are being generated  then doubling n will simply double the search time. but the slope constant is much smaller than case  1 . instead of the average solution time  k1 is the near-optimal solution time. at smaller values of n  there are two opposing forces at work. increasing n improves solution quality  so fewer steps are needed. but since there are multiple agents to dovetail among  search time will suffer. experimental results are summarized in figure 1. in this figure  lookahead horizon  h  is held constant at 1. with the horizon constant  search time is a  linear  function of the number of steps taken by all agents.  since steps can be measured more accurately than search time  the figure uses steps.  
   still holding the lookahead horizon constant at 1  we can compute a search time versus solution quality curve for dovetailed agents. each data point in figure 1 represents a different value of n. 
   now we can compare the two methods of improving solution quality: increasing h  figure 1  and increasing n  figure 1 . the following table includes average search time and solution quality for three possible combinations of h and n. 


figure 1: solution quality versus search time for reactive agents in the 1-puzzle. small numbers indicate varying numbers of agents. execution is dovetailed on a sequential machine. compare with figure 1  especially the x-axis. 
the first two lines in the table represent single-agent search  reactive and deliberative. the third line represents multiagent search. the dramatic result here is that eight reactive agents-dovetailed on a sequential machine-can match the solution quality of a single deliberative agent  and do so spending only a fraction of the time. this demonstrates the superiority of adding more reactive agents over increasing the deliberativeness of a single agent. 
　the benefit of multiagent search derives from the wide variation in solution quality for a single agent. the time cost is only linear in the number of agents. on the other hand  the benefit of deliberation derives from the knowledge gained by looking ahead. but the time cost is exponential in the lookahead horizon. the benefits are comparable  but the costs are not. 
   we can now state our results in terms of parallel speedup  i.e.  uniprocessor time divided by multiprocessor  multiagent  time. to make a fair comparison  it is necessary to fix the desired solution quality  as has been done in the above table. it shows 1 agents achieving the same result as 1  but doing it faster by a factor of 1  dovetailed on a sequential machine  and by a factor of 1 tn parallel. this is a superlinear speedup  and it holds for all fixed values for solution quality. superlinear speedups offer tremendous savings and have been reported most notably in  mehrotra and gehringer  1; janakiram et a/.  1; rao and kumar  1 . of course  our speedup is relative to rta*  not to the best sequential algorithm for generating solutions of fixed quality. superlinear speedups are strictly impossible in such cases  since the dovetailed algorithm can be run on a sequential machine. by dovetailing agents  we have created a new uniprocessor algorithm against which new parallel algorithms must be measured. 
the preceding discussion applies to the offline use of 
rta*. in real-time  incomplete-information domains  theoretical superlinear speedups over the best singleagent algorithm are possible. 
	knight 	1 

1 other domains 
the following table shows some results for the 1-puzzle  the 1 version of the 1-puzzle: 

here  the lookahead horizon  h  is held constant at 1  while the number of agents  n  varies. notice that moving from 1 to 1 agents yields a large improvement in solution quality at virtually no cost in search time. 
   we have also obtained a full set of empirical results for the  1-block  blocks world domain. the results are just as compelling as those for the n-puzzle. it is far more advisable to tackle a blocks-world problem with many reactive agents than a few deliberative ones. for example: 

   work on applying our ideas to planning systems is currently under way. planners like prodigy  minton et a/.  1  solve hard problems  but do not guarantee good quality solutions; other planners provide near-optimal solutions but do not scale up. we are exploring ways to bridge this gap by randomizing the arbitrary decisions made by a planner and employing multiple agents. 
1 agent communication and dispersal 
the results of the previous sections indicate that where substantial variation in solution time and quality exists  many reactive agents should be employed instead of a few deliberative ones. in this section  we consider two issues that naturally arise:  1  if the agents are allowed to communicate and coordinate  can their performance be improved  and  1  how can agents disperse themselves in the absence of random tie-breaking  
   we consider one rudimentary communication schemeagents communicate by sharing a single hash table  which records the states visited by all. thus  one agent can benefit from the experience of another  who may have already mapped out a portion of the search space. empirical experiments show that communicating reactive agents yielded solutions about 1% shorter than non-communicating agents. search time savings vary with the number of agents: for 1 agents  there is a 1% improvement; for 1 agents a 1% improvement; for 1 agents  a 1% improvement. this communication scheme is easy to implement  and there is clearly room for more intelligent schemes. 
　the second issue is dispersal. domains like the 1puzzle use a small set of effectively discrete heuristic estimators. this leads rta* to perform a large number of random tie-breaks  since alternative moves often look equally good. fortunately  these tie-breaks also serve to disperse multiple agents. but domains like path planning through obstacles  russell and wefald  1  involve an infinite number of real-valued estimators. in such domains  our reactive agents simply move about in a single clump  since what looks best to one agent also looks best to another. 
   we have investigated one algorithm for effectively dispersing agents. this algorithm treats heuristic estimates as probabilities. we obtained probabilities by solving 1 sample problems using rta*  and recording at each action cycle:  1  what the backed-up estimates were for various alternatives  and  1  what the best alternative really was. here   best  means  on an optimal path to the goal   optimal paths were computed at each point with ida*  korf  1  . we define stochastic rta* as an algorithm that uses such probabilities to occasionally make what rta* would consider a bad move. agents using stochastic rta* disperse themselves automatically. in our initial experiments  a single  reactive  stochastic agent returned solutions of equal quality compared to a normal rta* agent  but consumed 1%  more time. this slight decrease in single-agent performance washes out when multiple agents are employed. 
1 related work 
 korf  1  is the only other work to address multiple agents in the context of real-time heuristic search. it reports initial experimental results  but it does not compare multiple agents with increased deliberation  nor does it measure solution quality. also  it does not analyze the results in terms of superlinear speedup. 
   beam search is another closely related algorithm. roughly  multiple agent rta* is to beam search as single agent rta* is to beam search with a beam of width one. rta* is guaranteed to find a solution  by looping back if necessary  while beam search may prune solutions completely. rta* can also be used in reactive or deliberative mode  and in real-time or offline domains. furthermore  multiple agent rta* can be implemented straightforwardly on a general-purpose multiprocessor  whereas beam search involves large overhead costs due to synchronization  bisiani  1 . 
   previous work in parallel processing has pioneered the use of multiple processors for reliability and performance enhancement.  mehrotra and gehringer  1  report superlinear speedups when individual processors have varying runtimes due to randomization.  smith and maguire  1b  investigate using parallelism and randomization to tackle or-parallelism in prolog.  janakiram ei a/.  1  also tackle this blind search problem and remark that it would be interesting to pursue randomizing heuristic search. they also analyze the expected speedup for various running time probability distributions  but unfortunately not for the log-normal distribution of the type seen in our experiments .  smith and maguire  1a  and  goldberg and jefferson  1  

present similar work; none of these papers addresses heuristic search or solution quality. 
　ai has seen work in parallel heuristic search.  kumar and rao  1  and  rao and kumar  1  report super linear speedups in depth-first ida*.  rao and kumar  1j study speedup under varying assumptions about the the distribution of solution states.  saletore and kale  1  investigate how to achieve a reliable  consistent linear speedup. these algorithms concentrate on optimal or near optimal solutions  whereas we are concerned a flexible tradeoff between solution quality and search time. also  these algorithms do not interleave planning and execution  which is necessary in incomplete-information domains. other parallel work can be found in  powley and korf  1    huang and davis  1   and  li and wah  1 . 
1 conclusions and future work 
the rta* algorithm yields a tradeoff between search time and solution quality. increasing rta*'s lookahead horizon yields better solutions  but the search time increases exponentially. this paper has investigated another method for improving solution quality. it uses n agents  each of which is repeatedly given a time slice. search time only increases linearly with n  but solution quality improves very rapidly. when solution quality is held constant  employing n parallel agents yields superlinear speedups. 
　there are several directions in which to expand this work:  1  investigate new algorithms for agent dispersal and communication  building on work described in section 1;  1  investigate the behavior of heterogeneous collections of agents  e.g.  agents that use different heuristic evaluation functions  or agents with varying levels of deliberativeness; and  1  apply the method to a wide range of search and planning domains  e.g.  ones with different solution densities  action costs  and heuristic value distributions. 
1 acknowledgements 
this work was supported in part by the national science foundation under contract iri-1. thanks to yolanda gil  milind tambe  and gary knight for suggestions and assistance. 
