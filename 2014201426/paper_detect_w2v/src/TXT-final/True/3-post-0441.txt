
this paper introduces a new bootstrapping method closely related to co-training and scoped-learning. the method is tested on a web information extraction task of learning course names from web pages in which we use very few labelled items as seed data  1 web pages  and combine with an unlabelled set  1 web pages . the overall performance improved the precision/recall from 1%/1% for a baseline em-based method to 1%/1% for intimate learning.
1 intimate learning
the expensive nature of labelling data for machine learning methods and the lack of success in using purely unsupervised methods have motivated the study of learning methods that combine labelled and unlabelled data. successful methods in this area include bootstrapping methods like cotraining and scoped-learning. in this paper we introduce a novel method called intimate learning for bootstrapping that is related to these methods. the task is to learn the target function ht x  뫍 y where x is the set of all feature sets and y is the set of class labels. the input to our learning algorithm is a set of n labeled examples of the form  xi yi . xi 뫍 x has pi features xi1 xi1 ... xipi associated with the ith example. yi 뫍 y is the label of the ith example. in intimate learning  we find another class label yi1 뫍 y 1 which is relevant to yi and we assume that a new function h1 x  뫍 y 1 requires fewer labelled items to learn. we say that y 1 is relevant to y if accurately finding  can help in identifying yi 뫍 y in example i. we then create the new target function h x h1 x   뫍 y that performs the same classification as ht x  does. we call y 1 the intimate class  h1 x  the intimate function. intimate learning is related to the co-training algorithm  blum and mitchell  1   in which for training examples  xi yi   xi is decomposed into a pair  xi1 xi1  corresponding to two different  views   and the target function h x  = h1 x1  = h1 x1  predicting a single label class. while in our model  x has only one  view  but labeled into two classes  target and intimate classes   i.e.  ht x  1= h1 x 1. other related work is scoped-learning  blei et al.  1   which uses a classifier trained on global features from the entire training data and classifiers trained on

	1
모모intimate learning is not the same as feature selection: y is not directly observed. furthermore y 1 doesn't determine y ; x combined with y 1 can improve prediction of y .
scope-limited features which are more specific to local subsets of the data.
1 the information extraction task
we apply intimate learning to the problem of identifying course names from those pages identified as course web pages in webkb which consists of web pages collected from four universities. 1 for our experiment we used: 1 web pages as seeds  1 web pages as unlabeled training data and 1 web pages as test data. all web pages are tokenized using space and punctuation symbols. we used an em-based decision list learning algorithm  collins and singer  1  as our baseline method for combining labeled and unlabeled data  more details in 뫫1 . the course name feature model is initialized using the seed data and then trained using em on the training data. our observation on the training data indicated that the course number  which does not form part of our target label is likely to co-occur with a course name. course names have similar characteristics to other named entities like names of people or organizations. compared with course names  course numbers have far more regular forms  which usually consist of the department abbreviation and the number of the course  e.g.  cmpt 1. as a result identifying a course number is easier and we take this class to be our intimate class y 1. our target class y determines whether a string is a course name. in this paper  we have y and y 1 with one element each  +course-name and +course-number   rather than a 1-class classification task  +course-name   course-name . see 뫫1 for details on how test examples are handled. we explain the details of our algorithm using figure 1  for now  ignore the dashed arrow . the algorithm is a two-stage process. the left half of figure 1 illustrates the first stage  in which the intimate class is learned by a classification algorithm that is identical to the em-based baseline system  see 뫫1 . the second stage in the right half implements almost the same em-based algorithm  but with course numbers added from the first stage as an extra feature towards learning of the course name  the darker line with an arrow .
1 em learning for course number identification the features used for identifying a course number are:
  the html format of the course number  which is the pair of html tags before and after the course number

1
모모other applications of intimate learning include finding product names in web pages  which appear together with prices  quantities  locations which are generally much easier to identify.

figure 1: course name identification system chart
respectively.
  department name s and length |s|.
  course id: integer i and its digit length |i|.
  the separator symbol sequence between s and i.
features are collected for each string sj from the seed data. we estimate the conditional probability fnum = p y1 | xi j  of seeing the label y1 given the feature xi j. fnum defines a decision list of rules xi j 뫸 y1 ranked by confidence score p y1 | xi j . in training  we use em to  re estimate the course number probability model pnum y1 | s   where s is the input string. in the e-step  pnum is re-estimated based on fnum. we assume all features do not equally contribute to course number identification and assign different weight ci to xi  and each parameter of pnum is computed by:
		 1 
in the m-step  fnum is adjusted with respect to pnum:

since y1 is a single label +course-number  for test examples we pick those to be labelled as a course number by using 1means clustering to separate those examples that have high confidence scores  high p y1 | xi j  values  from those examples that have low confidence scores. this method avoids hand-picking or using a held-out set to pick a confidence threshold.
1 course name identification
the learning procedure of the course name identification is the same as that for the course number  except that the course number identified using the model defined in 뫫1 becomes an important feature in predicting whether an example is labelled as a course name. for each candidate course name  the features used are:
  intimate class: course number preceding candidate
  the pair of html tags before and after course name.
figure 1: experimental results of course name identification.
the precision and recall curves for partial and full matchings.
  each word in the course name and the number of words.
  each separator symbol between the course number and course name and total number of such symbols.
the course name feature model fnam is initialized from the seed data and trained by the em algorithm defined in 뫫1. applying the trained feature model to the test data generates all course name candidates with their probabilities. we again apply the 1-means clustering algorithm defined in 뫫1. note that the chosen candidates are individual words instead of the full string as a course name. we simply group contiguous words as a single course name. the baseline system is identical to the course name feature model fnam except that it does not use the identified course number as an input feature.
1 experiments and discussion
two metrics are applied to the performance evaluation of course name identification: partial matching  in which the course name is correctly recognized if any of its words is predicted  and full matching  in which the course name is correctly predicted only if all words of the course name are predicted and in the correct order. the precision/recall of the baseline system is 1%/1% for full matching. figure 1 illustrates the performance of our implementation on course name identification  for full and partial matchings. our experiments show that the intimate learning algorithm exhibits significant gains in performance over the baseline system obtaining 1%/1% for full matching and 1%/1% for partial matching. since the course number and course name are two related classes  in addition to intimate learning  a cotraining-based extension can also be applied to training one class by the other  and vice versa as shown by the dashed arrow in figure 1. for details and full set of references  please refer to http://natlang.cs.sfu.ca/researchproject.php s=1.
