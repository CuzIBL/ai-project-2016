
learning capabilities of computer systems still lag far behind biological systems. one of the reasons can be seen in the inefficient re-use of control knowledge acquired over the lifetime of the artificial learning system. to address this deficiency  this paper presents a learning architecture which transfers control knowledge in the form of behavioral skills and corresponding representation concepts from one task to subsequent learning tasks. the presented system uses this knowledge to construct a more compact state space representation for learning while assuring bounded optimality of the learned task policy by utilizing a representation hierarchy. experimental results show that the presented method can significantly outperform learning on a flat state space representation and the maxq method for hierarchical reinforcement learning.
1 introduction
learning capabilities in biological systems far exceed the ones of artificial agents  partially because of the efficiency with which they can transfer and re-use control knowledge acquired over the course of their lives.
모to address this  knowledge transfer across learning tasks has recently received increasing attention  ando and zhang  1; taylor and stone  1; marthi et al.  1; marx et al.  1 . the type of knowledge considered for transfer includes re-usable behavioral macros  important state features  information about expected reward conditions  and background knowledge. knowledge transfer is aimed at improving learning performance by either reducing the learning problem's complexity or by guiding the learning process.
recent work in hierarchical reinforcement learning
 hrl  has led to approaches for learning with temporally extended actions using the framework of semi-markov decision processes  smdps   sutton et al.  1   for learning subgoals and hierarchical action spaces  and for learning abstract representations  givan et al.  1 . however  most of these techniques only address one of the aspects of transfer and do frequently not directly address the construction of action and representation hierarchies in life-long learning.
모the work presented here focuses on the construction and transfer of control knowledge in the form of behavioral skill hierarchies and associated representational hierarchies in the context of a reinforcement learning agent. in particular  it facilitates the acquisition of increasingly complex behavioral skills and the construction of appropriate  increasingly abstract and compact state representations which accelerate learning performance while ensuring bounded optimality. moreover  it forms a state hierarchy that encodes the functional properties of the skill hierarchy  providing a compact basis for learning that ensures bounded optimality.
1 reinforcement learning
in the rl framework  a learning agent interacts with an environment over a series of time steps t = 1 1 ... . at each time t  the agent observes the state of the environment  st   and chooses an action  at   which causes the environment to transition to state st+1 and to emit a reward  rt+1. in a markovian system  the next state and reward depend only on the preceding state and action  but they may depend on these in a stochastic manner. the objective of the agent is to learn to maximize the expected value of reward received over time. it does this by learning a  possibly stochastic  mapping from states to actions called a policy. more precisely  the objective is to choose each action at so as to maximize the expected return     where 붺 뫍  1  is a discountrate parameter. other return formulations are also possible. a common solution strategy is to approximate the optimal action-value function  or q-function  which maps each state and action to the maximum expected return starting from the given state and action and thereafter always taking the best actions.
to permit the construction of a hierarchical learning system  we model our learning problem as a semi-markov decision problem  smdp  and use the options framework  sutton et al.  1  to define subgoals. an option is a temporally extended action which  when selected by the agent  executes until a termination condition is satisfied. while an option is executing  actions are chosen according to the option's own policy. an option is like a traditional macro except that instead of generating a fixed sequence of actions  it follows a closed-loop policy so that it can react to the environment. by augmenting the agent's set of primitive actions with a set of options  the agent's performance can be enhanced. more specifically  an option is a triple oi =  ii 뷇i 붹i   where ii is the option's input set  i.e.  the set of states in which the option can be initiated; 뷇i is the option's policy defined over all states in which the option can execute; and 붹i is the termination condition  i.e.  the option terminates with probability 붹i s  for each state s. each option that we use in this paper bases its policy on its own internal value function  which can be modified over time in response to the environment. the value of a state s under an smdp policy 뷇o is defined as  boutilier et al.  1; sutton et al.
where
		 1 
and

where rt denotes the reward at time denotes the event of an action under policy 뷇o being initiated at time t and in state s  sutton et al.  1 .
1 hierarchical knowledge transfer
in the approach presented here  skills are learned within the framework of semi-markov decision processes  smdp  where new task policies can take advantage of previously learned skills  leading from an initial set of basic actions to the formation of a skill hierarchy. at the same time  abstract representationconcepts are derived which capture each skill's goal objective as well as the conditions under which use of the skill would predict achievement of the objective. figure 1 shows the approach.
모the state representations are formed here within the framework of boundedparameter markovdecision processes  bpmdps   givan et al.  1  and include a decision-level model and a more complex evaluation-level model. learning of the new task is then performed on the decision-level model using q-learning while a second value function is maintained on the evaluation-level model. when an inconsistency is discovered between the two value functions  a refinement augments the decision-level model by including the concepts of the action that led to the inconsistency.
모once a policy for the new task is learned  subgoals are extracted from the system model and corresponding subgoal skills are learned off-line. then goal and probabilistic affordance concepts are learned for the new subgoal skills and both  the new skills and concepts are included into the skill and representation hierarchies in the agent's memory  making them available for subsequent learning tasks.

figure 1: system overview of the approach for hierarchical behavior and state concept transfer.
1	learning transferable skills
to learn skills for transfer  the approach presented here tries to identify subgoals. subgoals of interest here are states that have properties that could be useful for subsequent learning tasks. because the new tasks' requirements  and thus their reward functions  are generally unknown  the subgoal criterion used here does not focus on reward but rather on local properties of the state space in the context of the current task domain. in particular  the criterion used attempts to identify states which locally form a significantly stronger  attractor  for state space trajectories as measured by the relative increase in visitation likelihood.
모to find such states  the subgoal discovery method first generates n random sample trajectories from the learned policy and for each state  s  on these trajectories and determines the expected visitation likelihood   where  is the sum over all states in sample trajectories hi 뫍 h weighed by their accumulated likelihood to pass through s. the change of visitation likelihoods along a sample trajectory  hi  is then determined as   where st is the tth state along the path. the ratio of this change along the path is then computed as
붟h st 

max 1 붟h st+1  
for every state in which 붟h st    1. finally  a state st is considered a potential subgoal if its average change ratio is significantly greater than expected from the distribution of the ratios for all states 1. for all subgoals found  corresponding policies are learned off-line as smdp options  oi  and added to the skill hierarchy.
definition 1 a state s is a direct predecessor of state s  if under a learned policy the action in state s can lead to
.
definition 1 the count metric for state s under a learned policy  뷇  is the sum over all possible state space trajectories weighed by their accumulated likelihood to pass through state s.
let c뷇  s  be the count for state s  then:
		 1 
and
		 1 
		 1 
where n is such that. the condition  prevents the counting of self loops and  is the probability of reaching state s from state s by executing action. the slope of c뷇  st  along a path  뷈  under policy 뷇 is:
		 1 
where st is the tth state along the path. in order to identify subgoals  the gradient ratio 붟뷇 st /max 1 붟뷇 st+1   is computed for states where 붟뷇 st    1. a state st is considered a potential subgoal candidate if the gradient ratio is greater than a specified threshold 뷃   1. appropriate values for this user-defined threshold depend largely on the characteristics of the state space and result in a number of subgoal candidates that is inversely related to the value of 뷃. this approach is an extension of the criterion in  goel and huber  1  with max 1 붟뷇 st+1   addressing the effects of potentially obtaining negative gradients due to nondeterministic transitions.
in order to reduce the computational complexity of the above method in large state spaces  the gradient ratio is here computed using monte carlo sampling.
definition 1 let h = {h1 ... hn} be n sample trajectories induced by policy 뷇  then the sampled count metric 
  for each state s that is on the path of at least one path hi can be calculated as the average of the accumulated likelihoods of each path  hi  1 뫞 i 뫞 n  rescaled by the total number of possible paths in the environment. we can show that for trajectories hi and sample size n such
that
		 1 
the following statement is true with probability p:

theorem 1 let h = {h1 ... hn} be n sample trajectories induced by policy 뷇 with n selected according to equation 1.	if   then with probability 뫟 p.
theorem 1 implies that for a sufficiently large sample size the exhaustive and the sampling method predict the same subgoals with high probability.
1	learning functional descriptions of state
the power of complex actions to improve learning performance has two main sources;  i  their use reduces the number of decision points necessary to learn a policy  and  ii  they usually permit learning to occur on a more compact state representation. to harness the latter  it is necessary to automatically derive abstract state representations that capture the functional characteristics of the actions. to do so  the presented approach builds a hierarchical state representation within the basic framework of bpmdps extended to smdps  forming a hierarchical bounded parameter sdmp  bpsmdp . model construction occurs in a multistage  action-dependent fashion  allowing the model to adapt rapidly to action set changes.
모the bpsmdp state space is a partition of the original state space where the following inequalities hold for all blocks  bpsmdp states  bi and actions oj  asadi and huber  1 :

where r s o  is the expected reward for executingoption o in state is the discounted transition probability for option o initiated in state s to terminate in state s. these properties of the bpsmdp model ensure that the value of the policy learned on this model is within a fixed bound the optimal policy value on the initial model  where the bound is a function of givan et al.  1 .
모to make the construction of the bpsmdp more efficient  the state model is constructed in multiple steps. first functional concepts for each option  o  are learned as termination concepts ct o  indicating the option's goal condition  and probabilistic prediction concepts   affordances    cp o x  indicating the context under which the option will terminate successfully with probability. these conditions guarantee that any state space utilizing these concepts in its state factorization fulfills the conditions of equation 1 for any single action.
모to construct an appropriate bpmdp for a specific action set ot = {oi}  an initial model is constructed by concatenating all concepts associated with the options in ot. additional conditions are then derived to achieve the condition of equation 1 and  once reward information is available  the reward condition of equation 1. this construction facilitates efficient adaptation to changing action repertoires.
모to further utilize the power of abstract actions  a hierarchy of bpsmdp models is constructed here where the decisionlevel model utilizes the set of options considered necessary while the evaluation-level uses all actions not considered redundant. in the current system  a simple heuristic is used where the decision-level set consists only of the learned subgoal options while the evaluation-level set includes all actions.
모let p = {b1 ... bn} be a partition for state space s derived by the action-dependent partitioning method  using subgoals {s1 ... sk} and options to these subgoals {o1 ... ok}. if the goal state g belongs to the set of subgoals {s1 ... sk}  then g is achievable by options {o1 ... ok} and the task is learnable according to theorem
1.
theorem 1 for any policy 뷇 for which the goal g can be represented as a conjunction of terminal sets  subgoals  of the available actions in the original mdp m  there is a policy 뷇p in the reduced mdp  mp   that achieves g as long as for each state st in m for which there exists a path to g   there exists a path such that f g|st 뷇p st     붻.
if g /뫍 {s1 ... sk} then the task may not be solvable using only the options that terminate at subgoals. the proposed approach solves this problem by maintaining a separate value function for the original state space while learning a new task on the partition space derived from only the subgoal options. during learning  the agent has access to the original actions as well as all options  but makes decisions only based on the abstract partition space information. while the agent tries to solve the task on the abstract partition space  it computes the difference in q-values between the best actions in the current state in the abstract state space and in the original state space. if the difference is larger than a constant value  given by theorem 1   then there is a significant difference between different states underlying the particular block that was not captured by the subgoal options. theorem 1  kim and dean  1  shows that if blocks are stable with respect to all actions the difference between the q-values in the partition space and in the original state space is bounded by a constant value.
theorem 1 given an mdp m =  s a t r  and a partition p of the state space mp   the optimal value function of m given as v   and the optimal value function of mp given as vp  satisfy the bound on the distance

where and

when the difference between the q-values for states in block bi are greater than   then the primitive action that achieves the highest q-value on the original state in the mdp will be added to the action space of those states that are in block bi and block bi is refined until it is stable for the new action set. once no such significant difference exists  the goal will be achievable in the resulting state space according to theorem 1.
1	learning on a hierarchical state space
to learn new tasks  q-learning is used here at the decisionlevel of the bpsmdp hierarchy. because the compact decision-level state model encodes only the aspects of the environment relevant to a subset of the actions  it only ensures the learning of a policy within the pre-determined optimality bounds if the policy only utilizes the actions in the decisionlevel action set. since  however  the action set has to be selected without knowledge of the new task  it is generally not possible to guarantee that it contains all required actions.
모to address this  the approach maintains a second value function on top of the evaluation-level system model. while decisions are made strictly based on the decision-level states  the evaluation-level value function is used to discover value inconsistencies  indicatingthat a significant aspect of the state space is not represented in the evaluation-level state model. the determination of inconsistencies here relies on the fact that the optimal value function in a bpmdp  vp   is within a fixed bound of the optimal value function  v    on the underlying mdp  givan et al.  1 .
모inconsistencies are discovered when the evaluation-level value for a state significantly exceeds the value of the corresponding state at the decision level. in this case  the action producing the higher value is included for the corresponding block at the decision level and the block is refined with this action to fulfill equations 1 and 1 as illustrated in figure 1.

figure 1: decision-level model with 1 initial blocks  b1 b1 b1  where block b1 has been further refined.
1 experiments
to evaluate the approach  it has been implemented on the urban combat testbed  uct    a computer game
 http://gameairesearch.uta.edu . for the experiments presented here  the agent is given the abilities to move through the environmentshown in figure 1 and to retrieve and deposit objects.

figure 1: urban combat testbed  uct  domain.
모the state is here characterized by the agent's pose as well as by a set of local object precepts  resulting in an effective state space with 1 states.
모the agent is first presented with a reward function to learn to move to a specific location. once this task is learned  subgoals are extracted by generating random sample trajectories as shown in figure 1.
모as the number of samples increases  the system identifies an increasing number of subgoals until  after 1 samples 

figure 1: number of subgoals discovered using sampling.
all 1 subgoals that could be found using exhaustive calculation have been captured.
모once subgoals are extracted  subgoal options  oi  are learned and termination concepts  ct oi and probabilistic outcome predictors  cp oi x are generated. these subgoal options and the termination and prediction concepts are then transferred to the next learning tasks.
모the system then builds a hierarchical bpsmdp system model where the decision-level only utilizes the learned subgoal actions while the evaluation-level model is built for all available actions. on this model  a second task is learned where the agent is rewarded for retrieving a flag  from a different location than the previous goal  and return it to the home base. during learning  the system augments its decision-level state representation to allow learning of a policy that is within a bound of optimal as shown in figure 1.

figure 1: size of the decision-level state representation.
모figure 1 shows that the system starts with an initial state representation containing 1 states. during learning  as value function inconsistencies are found  new actions and state splits are introduced  eventually increasing the decision-level state space to 1 states. on this state space  a bounded optimal policy is learned as indicated in figure 1. this graph compares the learning performance of the system against a learner that only transfers the discovered subgoal options and a learner without any transfer mechanism. these graphs show a transfer ratio1 of 뫘 1 when only subgoal options are trans-

figure 1: learning performance with and without skill and representation/concept transfer.
ferred  illustrating the utility of the presented subgoal criterion. including the representation transfer and hierarchical bpsmdp learning approach results in significant further improvement with a transfer ratio of 뫘 1.
1 comparison with maxq
dietterich  dietterich  1  developed an approach to hierarchical rl called the maxq value function decomposition  which is also called the maxq method. like options and hams  this approach relies on the theory of smdps. unlike options and hams  however  the maxq approach does not rely directly on reducing the entire problem to a single smdp. instead  a hierarchy of smdps is created whose solution can be learned simultaneously. the maxq approach starts with a decomposition of a core mdp m into a set of subtasks {m1 ... mn}. the subtasks form a hierarchy with m1 being the root subtask  which means that solving m1 solves m. actions taken in solving m1 consist of either executing primitive actions or policies that solve other subtasks  which can in turn invoke primitive actions or policies of other subtasks  etc.
each subtask  mi  consists of three components. first  it has a subtask policy  pi  that can select other subtasks from the set of mi 's children. here  as with options  primitive actions are special cases of subtasks. we also assume the subtask policies are deterministic. second  each subtask has a termination predicate that partitions the state set  s  of the core mdp into si  the set of active states in which mi's policy can execute  and ti  the set of termination states which  when entered  causes the policy to terminate. third  each subtask mi has a pseudo-reward function that assigns reward values to the states in ti. the pseudo-reward function is only used during learning.
 figure 1 shows the comparison between the maxq decomposition and the learning of an smdp with the sampling-base subgoal discovery but without action-dependent partitioning. this experiment illustrates that maxq will outperform an smdp with options to the subgoals that are discovered by sampling-based subgoal discovery. the reason for this is that while subgoals are hand designed in the maxq decomposition  the sampling-based method is fully autonomous and does not rely on human decision. as a result  subgoal dis-

figure 1: comparison of policies derived with the maxq method and with a smdp with sampling-based subgoal discovery.
covery generates additional subgoal policies that are not required for the task at hand and might not find the optimal option set. figure 1 illustrates the comparison between learning time in maxq and the bpsmdp constructed by the actiondependent partitioning method. this experiment shows that action-dependent partitioning can significantly outperform the maxq decomposition since it constructs state and temporal abstractions resulting in a more abstract state space. in this form  it can transfer the information contained in previously learned policies for solving subsequent tasks.

figure 1: comparison of policies derived with the maxq method and action-dependent partitioning with autonomous subgoal discovery.
1 conclusion
most artificial learning agents suffer from the inefficient reuse of acquired control knowledge in artificial. to address this deficiency  the learning approach presented here provides a mechanism which extracts and transfers control knowledge in the form of potentially useful skill and corresponding representation concepts to improve the learning performance on subsequent tasks. the transferred knowledge is used to construct a compactstate space hierarchythat captures the important aspects of the environment in the context of the agent's capabilities and thus results in significant improvements in learning performance.
