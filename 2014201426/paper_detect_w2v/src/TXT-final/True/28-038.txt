 
this paper presents an average-case analysis of the fc-nearest neighbor classifier  k-nn . our analysis deals with m-of-n// concepts  and handles three types of noise: relevant attribute noise  irrelevant attribute noise  and class noise. we formally compute the expected classification accuracy of fc-nn after a certain fixed number of training instances. this accuracy is represented as a function of the domain characteristics. then  the predicted behavior of fc-nn for each type of noise is explored by using the accuracy function. we examine the classification accuracy of fc-nn at various noise levels  and show how noise affects the accuracy of fcnn. we also show the relationship between the optimal value of k and the number of training instances in noisy domains. our analysis is supported with monte carlo simulations. 
1 	introduction 
the fc-nearest neighbor classifier  fc-nn  is one of the most widely applied learning algorithms. although fcnn is a powerful algorithm and has been studied by many researchers  it is not clear how noise affects the classification accuracy of fc-nn. moreover  it is also unclear which value should be chosen for k to maximize accuracy in noisy domains. these are crucial problems in fc-nn applications  because there are few noise-free problems in practical domains. 
　variants of fc-nn have been proposed to tolerate noise  e.g.   aha and kibler  1    and to choose an appropriate value of k  e.g.   creecy et al  1  . these proposals exhibit the high performance of fc-nn by empirical evaluations. however  the noise effects on the accuracy of fc-nn and on the optimal value of k still remain unclear. it is therefore important to understand the noise effects on fc-nn and the optimal k by theoretical evaluations. 
there have been several theoretical analyses of fc-nn. 
the upper bound of fc-nn error rate  risk  is twice the optimal bayes risk under the assumption of an infinite number of training instances  cover and hart  1 . 
1 	case-based reasoning 
moreover  fc-nn risk converges to the optimal bayes risk as fc approaches infinity  cover  1 . for a finite set of training instances  the new bounds of 1-nn risk are given using bayes risk  drakopoulos  1 . aha et al.  analyze 1-nn with a similar model to pac  probably approximately correct  learning  and this analysis is generalized to fc-nn  albert and aha  1 . although these theoretical results are important and give some insights into the behavior of fc-nn  all of these studies assume noise-free instances. 
　an average-case analysis is a useful theoretical framework to understand the behavior of learning algorithms  pazzani and sarrett  1 . this framework is based on the formal computation of the expected accuracy of a learning algorithm for a certain fixed class of concepts. using the result of this computation  we can explore the predicted behavior of an algorithm. there have been some average-case analyses of fc-nn. langley and iba  analyzed 1-nn for conjunctive concepts  and we analyzed fc-nn for m-of-n concepts without irrelevant attribute  okamoto and satoh  1 . however  these studies assumed noise-free instances. recently  we presented an average-case analysis of 1-nn for mof-n concepts with irrelevant attribute in noisy domains  okamoto and yugami  1 . this paper generalizes our recent study for 1-nn to fc-nn. 
　in this paper  we present an average-case analysis of fcnearest neighbor classifier for noisy domains. our analysis handles m-of-n concepts with l irrelevant attributes  and deals with three types of noise: relevant attribute noise  irrelevant attribute noise  and class noise. first  we formally compute the expected classification accuracy  i.e.  predictive accuracy  of fc-nn after n training instances are given. this accuracy is represented as a function of the domain characteristics: fc  n  m  n  /  the probabilities of occurrence for relevant and irrelevant attributes  and noise rates. using the accuracy function  we explore the predicted behavior of fc-nn in noisy domains. we describe the predictive accuracy of fc-nn at various noise levels  and show the effects of noise on the accuracy of fc-nn. we also show the relationship between the optimal value of fc and the number of training instances in noisy domains. our theoretical analysis is supported with monte carlo simulations. 

1 	problem description 
our analysis deals with m-of-n// concepts defined over the threshold m  n relevant and i irrelevant boolean attributes  murphy and pazzani  1 . these concepts classify an instance as positive if and only if at least m out of n relevant attributes occur  i.e.  take the value 1  in its instance. 
　our analysis handles three types of noise. each type of noise is independently introduced by the following common definition. relevant  irrelevant  resp.  attribute noise flips an arbitrary relevant  irrelevant  resp.  attribute value in each instance with a certain probability 
 class noise replaces the class label for each 
instance with its negation with a certain probability 
　we investigate a k-nearest neighbor classifier using hamming distance  i.e.  the number of attributes on which two instances differ  as a distance measure. for the distribution over the instance space  our analysis assumes every relevant and irrelevant attribute independently occurs with a certain probability p and q. each training instance is independently drawn from the instance space. after the effects of each type of noise  all training instances are stored into memory to allow for duplication. when a test instance is given  a;-nn classifies the test instance into a majority class  positive or negative  among its k nearest training instances. if the number of positive instances equals that of negative instances among its k nearest neighbors  then k-nn randomly determines the class of the test instance  this situation can occur only when k is an even number . 
1 	predictive accuracy 
we formally compute the predictive accuracy of k-nn for m-of-n// target concepts after tv training instances are given. the predictive accuracy is represented as a function of the domain characteristics: k  n  m  n  /  p  
 however  to avoid complicated nota-
tion  we will not explicitly express these characteristics as parameters of the accuracy function with the exception of k. 
　we compute the predictive accuracy in the case where each type of noise affects only training instances. after this computation  we also give the accuracy function in the case where noise affects both test and training 
instances. 
　to compute the predictive accuracy  we use a set of instances in which x relevant attributes and y irrelevant attributes simultaneously occur  we denote this set with i x y  . let  be the probability that an arbitrary noise-free instance belongs to i x y . this probability is given by 
under our assumptions given in section 1  k-nn has the same expected probability of correct classification for an arbitrary test instance in i{x y . hence  we can represent the predictive accuracy of k-nn after n training 
	okamoto & yugami 	1 

1 	case-based reasoning 


figure 1: the predictive accuracy of fc-nn against the value of fc for a 1-of-1 concept. the lines and the error bars represent the theoretical results and the empirical results of monte carlo simulations. each circle denotes the accuracy for the optimal fc at the corresponding noise level. the number of training instances is fixed at 1. 

when fc-nn selects exactly  a: - a  out of b training instances with distance d from t x y   these  k - a  training instances comprise exactly w out of v instances with the positive class label and exactly  fc - a - w  out of 
 1 - v  instances with the negative class label. hence  is given by 
　we have computed the predictive accuracy of fc-nn in the case where each type of noise affects only the training instances. when noise affects test instances  the appearance probability for an arbitrary test instance with the positive  negative  resp.  class label in i x y  is pp x y   pn x y/   resp. . hence  when noise affects both test and training instances  the predictive accuracy of fc-nn after tv training instances can be represented as 

1 	p r e d i c t e d b e h a v i o r 
　using the accuracy function described in section 1  we explore the predicted behavior of fc-nn. although the accuracy function was obtained for both noise-free and noisy test instances  our exploration deals with only noise-free test instances for lack of space. moreover  we investigate the effects of each individual noise type on fc-nn. 
　for irrelevant attribute noise  we can formally prove the following claim from the accuracy function  the proof is omitted here due to space limitations . 
claim 1 
if the probability of occurrence for irrelevant attribute is 1  then the predictive accuracy of k-nn for m-of-n/l concepts is entirely independent of the noise rate for irrelevant attributes. 
from this claim  we can expect that irrelevant attribute noise does not greatly affect the classification accuracy of fc-nn  nor the optimal value of fc. therefore  the following discussions focus on the effects of relevant attribute noise and class noise. throughout our exploration  we set the probabilities of occurrence for both relevant and irrelevant attributes to 1. 
　in addition to the theoretical results from the accuracy function  we give the results of monte carlo simulations to confirm our analysis. for each case  1 training sets are randomly generated in accordance with each noise rate  then the data is collected as the classification accuracy measured over the entire space of noise-free instances. for each case  we report a 1% confidence interval for the mean accuracy of 1 data items. in the following figures  the error bar indicates this confidence interval. 
1 	accuracy against value of k 
first  we report the predicted behavior of fc-nn against the value of fc at several levels of noise  as shown in figure 1. in this figure  the number of training instances is fixed at 1  and the target is a 1-of-1 concept. the lines indicate the theoretical results from the accuracy function  and the error bars represent the empirical results of monte carlo simulations. the theoretical results agree well with the empirical ones for both relevant attribute noise and class noise. 
　figure 1 shows that the predictive accuracy of fc-nn markedly drops off for each noise level when fc is an even number. this negative influence of an even number for fc on the accuracy is caused by a random determination of class when a tie occurs. this negative influence suggests that a choice of even number for fc is undesirable when applying fc-nn. 
	okamoto & yugami 	1 


figure 1: the effects of noise on the predictive accuracy of k-nn. each curve for k-nn indicates the accuracy of k-nn with the optimal value of a;. the number of training instances is fixed at 1. 

　in figure 1  each circle represents the predictive accuracy for the optimal value of k at the corresponding noise level. for each odd number for k  the accuracy of k-nn for a 1% noise level has two peaks. one appears at k = 1  while the other appears at the optimal value of k. in contrast  the accuracy for a 1% noise level and a 1% level have one peak at the corresponding optimal k. this is because the peak at k = 1 disappears due to the effect of noise. 
1 	effects of noise on the accuracy 
we further investigate the effects of noise on the predictive accuracy of k-nn  as shown in figure 1. in this figure  the number of training instances is fixed at 1  and all curves come from the theoretical results. each curve for k-nn represents the predictive accuracy of knn with the optimal value of k at each noise level. 
　figure 1 a  shows the effects of relevant attribute noise on the predictive accuracies of 1-nn and the optimal knn. when the noise level is 1%  the accuracy of 1-nn is comparable to that for the optimal k-nn  for both 1-of1 and 1-of-1 concepts. however  the predictive accuracy of 1-nn almost linearly decreases with an increase in the noise level. for a 1% noise level  the accuracy of 1-nn equals that of a random prediction algorithm which predicts the same class as that for a randomly selected training instance. these observations suggest that 1-nn is strongly sensitive to relevant attribute noise. in contrast  the predictive accuracy of the optimal k-nn exhibits slower degradation. for the disjunctive concept  l-of-1 concept   the accuracy of the optimal k-nn is not greatly changed as the noise level increases. 
　figure 1 b  shows the effects of class noise on the predictive accuracies of 1-nn and the optimal k-nn. for the 1-of-1 concept  both 1-nn and the optimal k-nn exhibit similar behavior to the corresponding tests with relevant attribute noise. however  the effects of class noise on the accuracy differ entirely from ones of relevant attribute noise for the disjunctive concept. the predictive accuracy of 1-nn linearly decreases to 1. in contrast  the optimal k-nn's accuracy does not substan-
1 	case-based reasoning 
tially change until about a 1% noise level  whereafter it rapidly decreases to 1%. 
　these observations show that the predictive accuracy of 1-nn is strongly affected by both relevant attribute noise and class noise. also  they suggest that we can restrain the degradation in the predictive accuracy of knn caused by an increase in noise level by optimizing the value of k. 
1 	optimal value of k 
finally  we give the relationship between the optimal value of k and the number of training instances in noisy domains  as shown in figure 1. in this figure  the optimal value of k comes from the theoretical results  and the target is a 1-of-1 concept. in the following discussions  we use n to refer to the number of training instances. 
　for a 1% noise level  the optimal value of k remains k = 1 until n = 1. there is a rapid increase in the optimal k at n = 1  and then the optimal k almost linearly increases with an increase of n. this rapid increase is caused by the change of the peak given the highest accuracy from k = 1 to another  as mentioned in section 1  k-nn's predictive accuracy has two peaks . 
   for each level  1%  1%  and 1%  for both relevant attribute noise and class noise  the optimal value of k is changed from k = 1 to another at small n. this observation can be explained by the strong sensitivity of the accuracy of 1-nn to both relevant attribute noise and class noise  as mentioned in section 1 . that is  the peak at k = 1 disappears due to the effect of noise  even though n is a small number. after changing from k = 1 to another  the optimal value of k almost linearly increases with an increase of n. 
　these observations from figure 1 show that the optimal value of k almost linearly increases with an increase of n after the optimal k is changed from k = 1 to another  regardless of the noise level for both relevant attribute noise and class noise. that is  the optimal value of k strongly depends upon the number of training instances in noisy domains. 


figure 1: the optimal value of k against the number of training instances for a 1-of-1 concept. 

1 	conclusion 
in this paper  we presented an average-case analysis of the fc-nearest neighbor classifier  k-nn  for m-of-n/l target concepts in noisy domains. our analysis dealt with three types of noise: relevant attribute noise  irrelevant attribute noise  and class noise. 
　we formally defined the predictive accuracy of k-nn as a function of the domain characteristics. using the accuracy function  we explored the predicted behavior of k-nn for each type of noise. the predictive accuracy of k-nn was given at various levels of noise  then the noise effects on k-nn's accuracy were shown. we also show that the optimal value of k almost linearly increases with an increase in the number of training instances in noisy domains. our analysis was supported with monte carlo simulations. 
　in the future  we will extend the framework of averagecase analysis to relax many restrictions such as boolean attributes  a fixed class of target concepts  and a fixed distribution over the instance space. using the extended framework  we would like to analyze learning algorithms to give more useful insights into their practical applications. 
