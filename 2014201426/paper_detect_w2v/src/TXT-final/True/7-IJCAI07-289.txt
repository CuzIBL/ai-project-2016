
lexical paraphrasing aims at acquiring word-level paraphrases. it is critical for many natural language processing  nlp  applications  such as question answering  qa   information extraction  ie   and machine translation  mt . since the meaning and usage of a word can vary in distinct contexts  different paraphrases should be acquired according to the contexts. however  most of the existing researches focus on constructing paraphrase corpora  in which little contextual constraints for paraphrase application are imposed. this paper presents a method that automatically acquires context-specific lexical paraphrases. in this method  the obtained paraphrases of a word depend on the specific sentence the word occurs in. two stages are included  i.e. candidate paraphrase extraction and paraphrase validation  both of which are mainly based on web mining. evaluations are conducted on a news title corpus and the presented method is compared with a paraphrasing method that exploits a chinese thesaurus of synonyms -- tongyi cilin  extended   ciline for short . results show that the f-measure of our method  1  is significantly higher than that using ciline  1 . in addition  over 1% of the correct paraphrases derived by our method cannot be found in ciline  which suggests that our method is effective in acquiring out-of-thesaurus paraphrases. 
1 	introduction 
paraphrases are alternative ways that convey the same information. paraphrasing tasks can be classified as lexical  phrase-level  sentence-level  and discourse-level. lexical paraphrasing aims to acquire paraphrases of words. 
　lexical paraphrasing is important in many nlp applications since it is an effective solution to the problem of  word mismatch . for example  in qa  expanding a question by paraphrasing its content words can make it easier to find the answer  hermjakob et al.  1 . in ie  words and phrases in the ie patterns should be paraphrased in order to extract the target information expressed in various ways  shinyama and sekine  1 . in mt  paraphrases of unseen source words can be incorporated into the statistical mt process. specifically  paraphrases of unseen words can be translated rather than leave the words untranslated  callison-burch et al.  1 . 
　two broad approaches to lexical paraphrasing have dominated the literature. one approach acquires paraphrases from dictionaries  such as wordnet. some researchers extract wordnet synonyms as paraphrases  langkilde and knight  1   while some others use looser definitions  barzilay and elhadad  1 . in general  the correspondence between paraphrasing and types of lexical relations defined in wordnet is not clear  barzilay and mckeown  1 . in chinese  ciline has been exploited for paraphrasing in stead of wordnet  li et al.  1 . 
　the other approach collects lexical paraphrases from monolingual or bilingual corpora. lin  identified words that are similar in meaning by measuring the similarity of the contextual words. barzilay and mckeown  extracted paraphrases from a corpus of multiple english translations of the same source text. bannard and callison-burch  derived paraphrases using bilingual parallel corpora. wu and zhou  extracted lexical paraphrases with multiple resources  including a monolingual dictionary  a bilingual corpus  and a monolingual corpus. 
　these methods facilitate the acquisition of paraphrases. however  none of them specify the contexts in which the paraphrases can be adapted. this problem is crucial as almost all paraphrases can only be adapted in certain contexts. 
　recently  topic adaptation for paraphrasing has been researched. for example  kaji and kurohashi  selected lexical paraphrases according to different topics. however  the topics are limited and predefined. thus  their method cannot paraphrase a word according to any given context. 
　this paper addresses the problem of context-specific paraphrasing  csp   which aims at acquiring specific paraphrases according to a given context. in lexical csp  words are to be paraphrased. accordingly  a specific context means a sentence in which a word occurs. specifically  if a word occurs in different sentences  then different paraphrases should be extracted within each sentence. 
　the remainder of the paper is organized as follows: section 1 introduces the context-specific paraphrasing method in detail. section 1 describes the experiments and results. conclusion and future work are presented in section 1. 
1 method 
two main stages are included in the method: candidate paraphrase extraction and paraphrase validation. for a given sentence s  in the first stage  a set of similar sentences are retrieved from the web using a search engine  baidu1 in the experiments . from the similar sentences  candidate paraphrases for words in s are extracted by measuring syntactic similarities. the candidates are filtered using part-of-speech  pos  information. in the second stage  candidate paraphrases are validated using a combined similarity measurement  which integrates co-occurrence similarity  syntactic similarity  and semantic similarity. both the web and ciline are exploited in the validation stage. 
1 candidate paraphrase extraction 
1.1 	motivation 
candidate paraphrase extraction is based on a web mining method. a similar method has been exploited for paraphrasing answer patterns in qa  ravichandran and hovy  1 . using the web as a paraphrasing resource has three advantages compared with conventional resources  monolingual parallel corpora  monolingual unparallel corpora  and bilingual parallel corpora . first  the web is not domain limited. almost all kinds of topics and contexts can be covered. second  the scale of the web is extremely large  which makes it feasible to find any specific context on it. in addition  the web is dynamic  which means that new words and concepts can be retrieved from the web. 
　the method for candidate paraphrase extraction is based on two principles: 
　principle 1: authors on the web create information independently. thus their  vocabularies  vary greatly  cui et al.  1 . 
　this principle means that different people tend to use different words to express the same meaning. in other words  if a concept is widely discussed on the web  then various expressions  lexical paraphrases  will be found in the corresponding web documents. however  a principle for detecting these paraphrases and extracting them from the web is needed. therefore  the second principle is introduced. 
　principle 1: lexical paraphrases play similar syntactic functions in sentences. 
　the second principle indicates that paraphrases of a given word w can be derived by extracting words whose syntactic functions are similar with w. 
　in fact  the principles above have been used in recognizing paraphrases in previous work. shinyama et al.  acquired paraphrases using different reports on the same event of the same day  based on principle 1 . lin  clustered similar words by measuring syntactic similarities  based on principle 1 . the rationality of the principles has been verified in their work. 
1.1 	procedure of candidate paraphrase extraction two steps are included in candidate paraphrase extraction: 
　step1: query s on the web and retrieve similar sentences. obviously  only similar sentences of the given sentence s need to be considered when extracting candidate paraphrases since if two words are context-specific paraphrases they should occur in identical or at least similar sentences. in this step  s is searched on the web using baidu. from the retrieved snippets  sentences whose similarities with s exceed a predefined threshold tce  tce=1 in our case  are retained for further candidate extraction  these sentences are called candidate sentences hereafter . word overlapping rate  wor  is used here for computing the similarity between s and any candidate sentence sc: 
　　　　　　　| s s	           1  wor s  sc   max | s | | sc | 
where  s  sc  denotes the common words in both s and sc. 
 |.|  denotes the number of words. 
　step1: extract candidates according to syntactic similarity. as stated in principle 1  lexical paraphrases usually play similar syntactic functions in sentences. this is an important clue for candidate extraction. in this step  sentence s and all the candidate sentences obtained in the step1 are first parsed by a chinese dependency parser  in which 1 dependency relations  e.g. sbv  vob  att...  are defined. figure 1 depicts the dependency tree of an input sentence. 
 

figure 1: an example of dependency trees 
 
　in a dependency tree of a sentence  two words and their dependency relation are represented as a triple. for example      sbv     is a triple. the criterion shown in figure 1 is used for extracting candidate paraphrases: 
 
given: 
s: original sentence;  
sc: candidate sentence; 
dt s : dependency tree of s;  
dt sc : dependency tree of sc; 
 w1  rel  w1 : a triple in dt s ; 
　　 w1'  rel'  w1' : a triple in dt sc . criterion: 
if rel=rel' and w1=w1' and w1' then w1' is extracted as a candidate paraphrase of w1. 
if w1=w1' and rel=rel' and w1' then w1' is extracted as a candidate paraphrase of w1. figure 1: criterion for candidate paraphrase extraction 
　it is obvious that  a word and its paraphrases should have identical pos. therefore  if the word w and its candidate paraphrase w' have different poses  w' is filtered. 
1 paraphrase validation 
since the constraints in the candidate extraction stage are quite loose for the sake of recall  there exists much noise in the candidates. the experiments show that over 1% of the candidates are incorrect. therefore  a paraphrase validation stage is necessary. 
　paraphrase validation is one of the key issues in the research of paraphrasing. hence many methods have been presented. for example  barzilay and mckeown  recognized phrasal paraphrases using rules automatically extracted from the contexts. however  the method must be based on monolingual parallel corpora. bannard and callison-burch  validated paraphrases by computing the probability that two phrases can be translated to  or from  the same foreign language phrases. nevertheless  a large bilingual corpus is needed. lin  identified lexical paraphrases based on distributional hypothesis  which computes the similarity of two words' contexts to judge whether they are paraphrases. 
　the main disadvantage of the above methods is that none of them can determine whether two words are paraphrases within a certain sentence. in other words  they are not context-specific paraphrasing methods. in our work  a novel paraphrase validation method is proposed  in which both web information and a thesaurus are exploited. 
1.1 	paraphrase validation using web mining 
generally  when a query is searched using a search engine  the retrieved snippets are related to the query and can be viewed as  description  of the query. therefore  it can be assumed that if two queries can retrieve similar snippets  then they are similar. 
　this assumption is used in paraphrase validation. in detail  let w be a paraphrased word in sentence s  w' be a candidate paraphrase of w within s  and s' be a sentence constructed by replacing w with w' in s. if similar snippets are retrieved by searching s and s'  then we can say that replacing w with w' does not notably change the meaning of s. in other words  w and w' can be viewed as paraphrases in s. 
　suppose sni s  and sni s'  are the snippets corresponding to s and s' respectively. here  sentences that do not contain w  w'  are removed from sni s   sni s'   to filter noise. 
　two similarity measurements are defined to measure the snippet-based similarity between w and w'  i.e. the vector space model  vsm  similarity  simvsm  and the syntactic similarity  simsyn . 
　simvsm: in vsm  snippets sni s  and sni s'  are represented as vectors v s  and v s' . the weight of each word is calculated using a tf itf heuristic  equation 1 . 
max tf  t' ccd   
	tf itf  t  sni s    tf  t  sni s  	log	t'	   1  
tf  t ccd  
where tf t  sni s   denotes the term frequency of term t in snippets sni s . tf t  ccd  is t's term frequency counted on a china daily corpus  ccd . max tf t'  ccd   is the largest term frequency obtained on the corpus. note that the itf part in the equation is similar to the idf part in tf idf heuristic which is widely used in nlp and information retrieval  ir  applications. the underlying hypothesis is that the words occur frequently in the whole corpus should be  punished  when weighing the words. 
　the vsm similarity between w and w' is calculated as the cosine similarity between v s  and v s' : 
	v  s  v  s' 	   1  
simvsm  w  w'   cos v  s  v  s'  
	v  s 	v  s' 
where     denotes the inner product of two vectors.     denotes the length of a vector. 
　simsyn: in order to compute the syntactic similarity  sni s  and sni s'  are first parsed using the same dependency parser described above. the syntactic similarity is calculated with the same method as described in  lin  1b   which is rewritten in equation  1 . the similarity is calculated through the surrounding contextual words which have dependency relationships with the investigated words according to the parsing results. 
simsyn  w  w'    1  
where t w  denotes the set of words which have the dependency relation rel with w. i w rel t  is the point-wise mutual information  as defined in equation  1 : 
p w rel t 
	i w rel t 	log	     1  
p w | rel p t | rel p rel 
  simvsm and simsyn measure the snippet-based similarity of two words from different aspects. simvsm can also be called co-occurrence similarity since it measures whether w and w' co-occur with similar words in the snippets. all words in the snippets are assumed to be independent with each other and no syntactic relations are considered. in contrast  simsyn measures whether w and w' play similar syntactic functions in the snippets. only the words that have dependency relations with w  or w'  in the snippets are counted and the dependency relation types are taken into account. 
1.1 paraphrase validation using ciline beside simvsm and simsyn  the semantic similarity  simsem  is also investigated for paraphrase validation. 
　simsem: ciline is organized as a hierarchy of five levels  in which the first level is the highest and the fifth is the lowest  figure 1  a  . given two words  the lower their  common ancestor node is  the more similar their word senses are. each word in ciline has a sense code  determining its position in each level of the hierarchy  figure 1  b  . 
 

root level 1 level 1 level 1 level 1 level 1  b  sense code: da1# b 1# d a 1 level 1 level 1 level 1 level 1 level 1figure 1: hierarchy of ciline and an example of word sense code 
 
　for word w and its candidate paraphrase w'  wsi and ws j' denote the i-th sense of w and the j-th sense of w'  note that a word may have more than one word sense . simsem of w and w' is defined in equation  1 : 
simsem  w w'            1  
where l f wsi  ws j'   is the lowest ancestor node that two sense codes have in the hierarchy. ltotal is the number of total levels  ltotal =1 . for w and w'  the maximal similarity of their senses is defined as the semantic similarity. 
　obviously  simsem only measures the semantic distance between two words  in which no context information is considered. however  it is useful in paraphrase validation as a supplement to the snippet-based similarity. in our future work  a refined semantic similarity measurement  lin  1a  will be investigated. 
1.1 linear combination of similarities the three similarities defined above measure the similarity of two words from different sides. in order to integrate the similarities  we get them linearly combined: 
simcom  w  w' 
  1  
	* simvsm  w  w' 	* simsyn  w  w' 	sem  w  w' 
where     and are positive and1. the combined similarity simcom is used in paraphrase validation. detailedly  for word w and its candidate paraphrase w'  if simcom w w'  t  t is a predefined threshold   then the candidate w' will be validated as a true paraphrase. otherwise  w' will be filtered.  and t are estimated using a development set  section 1.1 . 
1 	evaluation 
1 data and metrics 
in order to evaluate the csp method  a sentence corpus is needed. in our experiments  a corpus of news titles is used as test data. the reasons are two folded. on the one hand  news titles are usually well-formed sentences. on the other hand  in many applications  such as qa  ie  and multi-document summarization  the words and sentences to be paraphrased are usually from news articles. the news titles are from the  important news  section of  sina news1 . all titles from march 1  1 to april 1  1 are downloaded. after removing some duplicated ones  1 titles are left for constructing the test data. likewise  another 1 titles from april 1  1 to april 1  1 are downloaded to form the development data. 
　the metrics in the experiments are macro-averaged precision  recall  and f-measure. let m1  ...  mt be t paraphrasing methods to be compared  in our experiments  the compared methods are mcsp  mciline  mcsp-can  mvsm+syn  mvsm+sem  and msyn+sem  which will be described in the next section   n the number of sentences in test data  nti the number of words in the i-th sentence that can be paraphrased by method mt  1 t   ntij the number of acquired paraphrases for the j-th paraphrased word in the i-th sentence using method mt  mtij the number of correct paraphrases  judged manually  in the ntij paraphrases. the precision of method mt is defined as: 
precision m t   nti         1  
　compared with precision  recall is more difficult to calculate since it is impossible to enumerate all paraphrases that a word has within a context. therefore  an approximate approach is used to calculate recall of each method. specifically  for the j-th paraphrased word in the i-th sentence  all its correct paraphrases acquired by the t methods are put together  with duplication removed . let ni be the number of words in the i-th sentence that can be paraphrased by at least one method  mij the total number of correct paraphrases for the j-th word. we assume that mij is the number of paraphrases that the word can really have within this specific sentence. then the recall of method mt is defined as: 
recall mt   ni           1  
　note that  the recall of a method will be over-estimated using the definition of equation  1   since some correct paraphrases may be absent. however  it is reasonable to get a set of methods compared in this way. 
the f-measure of method mt is defined as: 
	1	precision mt  	recall mt     1  
f	 measure mt   precision mt  	recall mt  
1 results and analysis 
1.1 	comparison between mcsp and mciline 
in this section  we compare the csp method  mcsp  with the method extracting ciline synonyms as paraphrases  mciline . 
in mciline  word sense disambiguation  wsd  is first conducted. a supervised method based on bayesian model is used in the wsd module  which can achieve a precision of 1% in our evaluation. then  all synonyms of a word under the chosen sense are extracted as its paraphrases. in mcsp  the development data is used to determine the parameters. the parameters for getting highest f-measure scores on the development data are selected. as a result  the coefficients   and in equation  1  are 1  1  and 1 respectively. the similarity threshold t is 1. the comparison results are shown in table 1: 
method precision recall f-measure mcsp1 1 1 mciline1 1 1  

table 1: comparison between mcsp and mciline
 
　it can be seen from table 1 that the precision of mciline is quite low  which shows that most synonyms defined in ciline are not paraphrases in specific contexts. on the other hand  the recall of mcsp is lower than mciline  this is mainly because ciline can provide some correct paraphrases that are not used in web documents. however  it is found that over 1% of the correct paraphrases derived in mcsp are not synonyms in ciline. this suggests that mcsp is effective in extracting  new  paraphrases. an example of the derived paraphrases of the two methods is illustrated in figure 1  words in bold are manually judged correct paraphrases : 
 
sentence 	 	 	 1 	 	 
 tourist boat sinks off bahrain  1 persons died  
results /sink -- /wreck; of /tourist boat -- /sunken ship  /ferry mcsp boat  /passenger ship  /pleasure boat; 
	/die -- 	/die; 
results /sink -- /deposit  /open caisson  of /subside  /submerge  /sag  /sag 
	mciline	/fall 	/subside;
	/person --	/personage 	/denizen 
	/personality 	/candidate 	/scholar;
	/die --	/be murdered 	/be in distress 
　　/meet with disaster  /suffer injury  /die  /be murdered  /be in danger 
	/suffer 	/die in a disaster; 
figure 1: example of the derived paraphrases of mcsp and mciline
 
1.1 	evaluation of validation methods 
in this section  we first evaluate the paraphrase validation method. therefore  we compare mcsp with the method that only extracts candidate paraphrases as described in section 
1 without validation  mcsp-can . the comparison results are shown in table 1. 
method precision recall f-measure mcsp1 1 1 mcsp-can1 1 1  

table 1: comparison between mcsp and mcsp-can 
　it can be found that mcsp outperforms mcsp-can greatly in precision  which indicates that the validation method is effective in filtering incorrect candidates. at the same time  recall decreases after validation  which suggests that some correct paraphrases are filtered by mistake. nevertheless  the increase in f-measure demonstrates the effectiveness of paraphrase validation. 
　as mentioned before  three kinds of similarities are combined during paraphrase validation in mcsp. the contribution of each one should be evaluated. therefore we compare 
mcsp with mvsm+syn  combining simvsm and simsyn   mvsm+sem  combining simvsm and simsem   and msyn+sem 
 combining simsyn and simsem . 
　for the three methods to be compared  the coefficients   and in equation  1  and the similarity threshold t are also estimated using the development data  table 1 . the comparison results are shown in table 1. 
 

mvsm+syn	1 	1 	- 	1 
mvsm+sem	1 	- 	1 	1 
msyn+sem	- 	1 	1 	1 
table 1: paramet
 ers for the methods to be compared method precision recall f-measure mcsp1 1 1 mvsm+syn1 1 1 mvsm+sem1 1 1 msyn+sem1 1 1 table 1: evaluation of each similarity measurement 
 
　we can find from table 1 that eliminating each similarity in the paraphrase validation can produce a notable degradation in precision  drop 1%  1%  and 1%  respectively  and f-measure  drop 1%  1%  and 1%  respectively   while the impact on recall is slight. 
 the comparison results suggest that each of the similarity measurements is useful in filtering incorrect candidates and the combination of all the three similarities can achieve the best performance. 
　we believe that three major factors should be taken into consideration in paraphrase validation  that is  whether two words co-occur with similar contextual words  whether they play similar syntactic functions in sentences  and whether their semantic distance is small. the combined similarity in equation  1  integrates all these factors. hence the f-measure is significantly enhanced. 
1.1 	error analysis 
false positives1 are analyzed after experiments. it is found that nearly 1% of the false positives are due to the reason that non-paraphrases occur in similar contexts. for example  in sentence       ding junhui failed to enter the final of the snooker match     
 final   is paraphrased into  	 main draw match    since these two words have occurred in very similar sentences from the reports about ding's two different matches. in order to solve this problem  we believe that  some new features should be used when representing contexts  such as named entities  ne .  
　another 1% mistakes are owing to ciline  because it assigns high semantic similarities to some non-paraphrase word pairs  such as   tsinghua university   and    renmin university    during paraphrase validation. 
　the other 1% false positives are due to mistakes of the preprocessing modules  including word segmentation  pos tagging  and syntactic parsing. 
1 conclusion 
this paper proposes a web mining method to automatically acquire context-specific lexical paraphrases. there are three main contributions. first  this work focuses on the problem of context-specific paraphrasing  which is very important but has seldom been addressed before. second  a novel two-stage method is presented  which uses the web as resource instead of monolingual or bilingual corpora used in conventional work. third  three similarity measurements are investigated and combined for paraphrase validation.  
　experiments are carried out on a news title corpus and the results show that our method can achieve a precision of 1  which is dramatically higher than the method extracting paraphrases from ciline  1 . in addition  over 1% of the paraphrases derived by mcsp cannot be extracted from ciline  which suggests that the web is an eligible resource for acquiring context-specific paraphrases and the presented method is effective. results also show that all the similarity measurements introduced in paraphrase validation make notable contribution to filtering incorrect candidates. 
　the main disadvantage of this method is that its time complexity is high  as snippets must be downloaded in the validation of each candidate paraphrase. this may make the method impractical in some applications  such as ir and qa. in the future work  we will construct a context-specific paraphrase corpus using the csp method  which contains not only paraphrase pairs  but also contexts in which the paraphrases can be adapted. context-specific paraphrases can be extracted directly from the corpus in practice. 
　besides  the csp method will be extended to the acquisition of phrase-level paraphrases. in addition  this method will be tested on normal sentences other than news titles. 
acknowledgments 
this research was supported by national natural science foundation of china  1  1  1 . we thank wanxiang che for his valuable advice and comments. 
reference 
 bannard and callison-burch  1  colin bannard and chris callison-burch. paraphrasing with bilingual parallel corpora. in proceedings of acl  1. 
 barzilay and elhadad  1  regina barzilay and michael elhadad. using lexical chains for text summarization. in proceedings of the acl workshop on intelligent scalable text summarization  1. 
 barzilay and mckeown  1  regina barzilay and kathleen r. mckeown. extracting paraphrases from a parallel corpus. in proceedings of the acl/eacl  1. 
 callison-burch et al.  1  chris callison-burch  philipp koehn  and miles osborne. improved statistical machine translation using paraphrases. in proceedings of naacl  1. 
 cui et al.  1  hang cui  ji-rong wen  jian-yun nie  and wei-ying ma. probabilistic query expansion using query logs. in proceedings of www  1. 
 hermjakob et al.  1  ulf hermjakob  abdessamad echihabi  and daniel marcu. natural language based reformulation resource and web exploitation for question answering. in proceedings of the trec-1 conference  1. 
 kaji and kurohashi  1  nobuhiro kaji and sadao kurohashi. lexical choice via topic adaptation for paraphrasing written language to spoken language. in proceedings of ijcnlp  1. 
 langkilde and knight  1  irene langkilde and kevin knight. generation that exploits corpus-based statistical knowledge. in proceedings of the coling/acl  1. 
 li et al.  1  weigang li  ting liu  yu zhang  sheng li  and wei he. automated generalization of phrasal paraphrases from the web. in proceedings of iwp  1. 
 lin  1a  dekang lin. an information-theoretic definition of similarity. in proceedings of icml  1. 
 lin  1b  dekang lin. optimizing synonym extraction using monolingual and bilingual resources. in proceedings of coling/acl  1. 
 ravichandran and hovy  1  deepak ravichandran and eduard hovy. learning surface text patterns for a question answering system. in proceedings of acl  1. 
 shinyama and sekine  1  yusuke shinyama and satoshi sekine. paraphrase acquisition for information extraction. in proceedings of iwp  1. 
 shinyama et al.  1  yusuke shinyama  satoshi sekine  kiyoshi sudo  and ralph grishman. automatic paraphrase acquisition from news articles. in proceedings of hlt  1. 
 wu and zhou  1  hua wu and ming zhou. optimizing synonym extraction using monolingual and bilingual resources. in proceedings of iwp  1. 
1 http://www.baidu.com 
1 http://news.sina.com.cn 
　　　1 false positives are word pairs recognized as paraphrases  but actually are not. 
---------------
　
------------------------------------------------------------
　
　---------------
　
　------------------------------------------------------------
　
ijcai-1
　
ijcai-1
　
ijcai-1
　
