
solving large partially observable markov decision processes  pomdps  is a complex task which is often intractable. a lot of effort has been made to develop approximate offline algorithms to solve ever larger pomdps. however  even stateof-the-art approaches fail to solve large pomdps in reasonable time. recent developments in online pomdp search suggest that combining offline computations with online computations is often more efficient and can also considerably reduce the error made by approximate policies computed offline. in the same vein  we proposea new anytime online search algorithm which seeks to minimize  as efficiently as possible  the error made by an approximate value function computed offline. in addition  we show how previous online computations can be reused in following time steps in order to prevent redundant computations. our preliminary results indicate that our approach is able to tackle large state space and observation space efficiently and under real-time constraints.
1 introduction
the pomdp framework provides a powerful model for sequential decision making under uncertainty. however  most real world applications have huge state space and observation space  such that exact solving approaches are completely intractable  finite-horizon pomdps are pspacecomplete  papadimitriou and tsitsiklis  1  and infinitehorizon pomdps are undecidable  madani et al.  1  .
¡¡most of the recent research in the area has focused on developing new offline approximate algorithms that can find approximate policies for larger pomdps  braziunas and boutilier  1; pineau et al.  1; poupart  1; smith and simmons  1; spaan and vlassis  1 . still  successful application of pomdps to real world problems has been limited due to the fact that even these approximate algorithms are intractable in the huge state space of real world applications. one of the main drawbacks of these offline approaches is that they need to compute a policy over the whole belief state space. in fact  a lot of these computations are generally not necessary since the agent will only visit a small subset of belief states when acting in the environment. this is the strategy online pomdp algorithms tries to exploit  satia and lave  1; washington  1; geffner and bonet  1; mcallester and singh  1; paquet et al.  1 . since we only need to plan for the current belief state when acting online  one needs only to compute the best action to do in this belief state  considering the subset of belief states that can be reached over some finite planning horizon.
¡¡one drawback of online planning is that it generally needs to meet hard real-time constraints when one is in face of large pomdps. nevertheless  recent developments in online pomdp search algorithms  paquet et al.  1; 1  suggest that combining approximate offline and online solving approaches may be the most efficient way to tackle large pomdps. effectively  we can generally compute a very approximate policy offline using standard offline value iteration algorithms and then use this approximate value function as a heuristic function in an online search algorithm. using this combination enables online search algorithm to plan on shorter horizons in order to respect online real-time constraints and retain a good precision. furthermore  doing an exact online search on a certain horizon also reduces the error made by approximate value functions  and consequently  does not require as much precision in the value function to be efficient.
¡¡in this paper  we propose a new anytime online search algorithm which aims to reduce  as efficiently as possible  the error made by approximate offline value iteration algorithms. our algorithm can be combined with any approximate offline value iteration algorithm to refine and improve the approximate policies computed by such algorithm. it can also be used alone  as a simple online search algorithm that can be applied in stationary or dynamic environments.
¡¡we first introduce the pomdp model and some offline and online approximate solving approaches. then we present our new algorithm and some experimental results which show its efficiency.
1 pomdp model
in this section we introduce the pomdp model and present different approximate offline and online approaches to solve pomdps.
1 model
a partially observable markov decision process  pomdp  is a model for sequential decision making under uncertainty. using such a model  an agent can plan an optimal sequence of action according to its belief by taking into account the uncertainty associated with its actions and observations.
	a	pomdp	is	generally	defined	by	a	tuple
 s a ¦¸ t r o ¦Ã  where s is the state space  a is the action set  ¦¸ is the observation set 
 1  is the transition function which specifies the probability of ending up in a certain state s  given that we were in state s and did action a   is the reward function which specifies the immediate reward obtained by doing action a in state s  is the observation function which specifies the probability of observing a certain observation o  given that we did action a and ended in stateis the discount factor.
¡¡in a pomdp  the agent does not know exactly in which state it currently is  since its observations on its current state are uncertain. instead the agent maintains a belief state b which is a probability distribution over all states that specifies the probability that the agent is in each state. after the agent performs an action a and perceives an observation o  the agent can update its current belief state b using the belief update function ¦Ó b a o  specified in equation 1.
		 1 
¡¡here  b is the new belief state and b is the last belief state of the agent. the summation part specifies the expected probability of transiting in state s  given that we performed action a and belief state b. afterward  this expected probability is weighted by the probability that the agent observed o in state s after doing action a. ¦Ç is a normalization constant such that the new probability distribution over all states sums to 1.
¡¡solving a pomdp consists in finding an optimal policy ¦Ð  which specifies the best action to do in every belief state b. this optimal policy depends on the planning horizon and on the discount factor used. in order to find this optimal policy  we need to compute the optimal value of a belief state over the planning horizon. for the infinite horizon  the optimal value function is the fixed point of equation 1.

¡¡in this equation  r b a  is the expected immediate reward of doing action a in belief state b and p o|b a  is the probability of observing o after doing action a in belief state b. this probability can be computed using equation 1.

¡¡this equation is very similar to the belief update function  except that it needs to sum over all the possible resulting states s in order to consider the global probability of observing o over all the state space.
¡¡similarly to the definition of the optimal value function  we can define the optimal policy ¦Ð  as in equation 1.

o¡Ê¦¸
 1 
¡¡however  one problem with this formulation is that there is an infinite number of belief states and as a consequence  it would be impossible to compute such a policy for all belief states in a finite amount of time. but  since it has been shown that the optimal value function of a pomdp is piecewise linear and convex  we can define the optimal value function and policy of a finite-horizon pomdp using a finite set of sdimensional hyper plan  called ¦Á-vector  over the belief state space. this is how exact offline value iteration algorithms are able to compute v   in a finite amount of time. however  exact value iteration algorithms can only be applied to small problems of 1 to 1 states due to their high complexity. for more detail  refer to littman and cassandra  littman  1; cassandra et al.  1 .
1 approximate offline algorithms
contrary to exact value iteration algorithms  approximate value iteration algorithms try to keep only a subset of ¦Ávectors after each iteration of the algorithm in order to limit the complexity of the algorithm. pineau  pineau et al.  1; pineau  1  has developed a point based value iteration algorithm  pbvi  which bounds the complexity of exact value iteration to the number of belief points in its set. instead of keeping all the ¦Á-vectors as in exact value iteration  pbvi only keeps a maximum of one ¦Á-vector per belief point  that maximizes its value. therefore  the precision of the algorithm depends on the number of belief points and the location of the chosen belief points. spaan  spaan and vlassis  1  has adopted a similar approach  perseus   but instead of updating all belief points at each iteration  perseus updates only the belief points which have not been improved by a previous ¦Á-vector update in the current iteration. since perseus generally updates only a small subset of belief points at each turn  it can converge more rapidly to an approximate policy  or use larger sets of belief points  which improves its precision. another recent approach which has shown interesting efficiency is hsvi  smith and simmons  1; 1   which maintains both an upper bound defined by a set of points and a lower bound defined by ¦Á-vectors. hsvi uses an heuristic that approximates the error of the belief points in order to select the belief point on which to do value iteration updates. when it selects a belief to update  it also updates its upper bound using linear programming methods.
1 approximate online algorithms
satia & lave  satia and lave  1 developedthe first online algorithmto solve pomdps. their heuristic search algorithm uses upper and lower bounds  computed offline  on the value function to conduct branch-and-bound pruning in the search tree. the pomdp is represented as an and-or graph in which belief states are or-nodes and actions are and-nodes. the root node  b1  of such an and-or graph represents the current belief state as it is presented in figure 1. the authors suggested solving the underlying mdp to get an upper bound and to use the value function of any reasonable policy for the lower bound. the heuristic they proposed to guide their search algorithm will be compared to our proposed heuristic in section 1.

figure 1: a search tree.
¡¡the bi-pomdp algorithm  washington  1  uses the classic ao* algorithm  nilsson  1  online to search the and-or graph. the author slightly modified ao* to use lower and upper bounds on the value function. for the lower bound  bi-pomdp pre-computes offline the min mdp   lowest possible value in every state   and uses this approximation at the fringe of the tree. for the upper bound  they use the qmdp algorithm  littman et al.  1  to solve the underlying mdp and use this bound as an heuristic to direct the search of ao* toward promising actions. they also use the difference between the lower and upper bound to guide the search toward fringe nodes that require more precision.
¡¡the rtbss algorithm  paquet et al.  1  is another similar algorithm which uses a branch and bound technique to search the and-or graph from the current belief state online. the search in the tree is done in a depth-first-search fashion up to a certain pre-determined fixed depth. when it reaches this depth  it uses a lower bound heuristic to evaluate the long term value of the fringe belief state. rtbss also uses an upper bound heuristic in order to prune some branches in the tree. pruning is only possible when the upper bound value of doing an action is lower than the lower bound of another action in the same belief state.
¡¡several other techniques have been proposed to conduct online search in pomdps. geffner  geffner and bonet  1  adapted the rtdp algorithm to pomdps. this approach requires the belief state space to be discretized and generally needs a lot of learning time before it performs well. another online search algorithm which uses observation sampling has also been proposed by mcallester  mcallester and singh  1 . instead of exploring all possible observations  this approach samples a pre-determined number of observations  at each and-node from a generative model of the environment. a more recent online approach  called sovi  g. shani and shimony  1   extended hsvi into an online value iteration algorithm. its authors also proposed a few improvements to speed up the upper bound updates and evaluations. the main drawback of this approach is that it is hardly applicable online in large environments with real time constraints since it needs to do a value iteration with ¦Á-vectors online  and this has a very high complexity.
1 aems
we now present our new online search algorithm  called anytime error minimization search  aems . this algorithm aims to determine the best action to do in the current belief state by doing a look-ahead search. this search is done by exploring the tree of reachable belief states from the current belief state  by considering the different sequence of actions and observations. in this tree  belief states are represented as or-nodes  we must choose an action child node  and actions are represented as and-nodes  we must consider all belief state child nodes  associated to the different possible observations  as presented before in figure 1.
¡¡this tree structure is used to determine the value of the current belief state b1 and the best action to do in this belief state. the values of the actions and belief states in the tree are evaluated by backtracking the fringe belief state values  according to equation 1. however  since the search cannot be conducted on an infinite horizon  we use an approximate value function at the fringe of the tree to approximate the infinite-horizon value of these fringe belief states. as the tree is expanded  the estimate we will get for the current belief state b1 is guaranteed to be more precise by the discount factor.
¡¡aems conducts the search by using a heuristic that provides an efficient way to minimize the error on the current belief state b1 and to handle large observation space. aems is also able to reuse the computations done at previous time steps in order to prevent the redundant computations. finally  aems is also an anytime algorithm which is able to exploit every bit of time available at each turn.
¡¡the key idea of aems consists in exploring the search tree by always expanding the fringe node that has the highest expected error contribution to the current belief state b1. expanding this belief state will reduce its error and will lead to better precision at the current belief state b1 for which we are planning.
1 expected error evaluation
there are three key factors that influence the error introduced by a fringe belief state on the current belief state. the first is the actual error committed by using a lower bound value function instead of the exact value function to evaluate the value of the fringe belief state. in order to evaluate this error  we can compute the difference between our upper and lower bound to get the maximal possible error introduced by our lower bound function on the fringe belief state. we will refer to this approximation as the function  defined by equation 1.
		 1 
¡¡here  u b  is the upper bound on v   b  and l b  is the lower bound on v   b . the real error  which is defined by  is always lower or equal to our approximation.
¡¡however  this error is multiplied by different factors  when it is backtracked into the search tree  that must be taken into account to get a good evaluation of its impact. by looking back at equation 1  we notice that the value of a child belief state is always multiplied by the discount factor ¦Ã and the probability p o|b a  of reaching this child belief state given the action taken in the parent belief state. since these factors have a value in interval  1   they reduce the contribution of this error on the parent belief state's value.
¡¡another implicit factor that must be considered is the max operator  since it indicates that we need to consider only the values of belief states that can be reached by doing a sequence of optimal actions. in other words  if we know the optimal action in a certain belief state  than we would only need to pursue the search in this action's subtree  because the other action values will not be considered in the value of b1. in our case  since we only have an approximate value function  we are generally not sure whether a certain action is optimal or not. nevertheless  we can take this uncertainty into account by considering the probability that an action becomes the optimal action  given its current bounds. in particular  if the upper bound value of a certain action a is lower than the lower bound value of another action a in a belief state  then we are sure that a is not the optimal action   i.e.  its probability of becoming the optimal action is 1 . however  most of the time we might encounter cases where the upper bound is higher than the highest lower bound. to handle such case  we will assume the other actions lower bound fixed and we will assume that the exact value of the parent belief state is evenly distributed between its current lower and upper bounds. we could also consider other types of distributions  in particular if we know that a certain bound is more precise than the other. using these assumptions  we can evaluate the probability that a certain action can still become the best action in the future using equation 1.
		 1 
¡¡here u a b  is the upper bound on the value of action a. l b  and u b  corresponds to the current lower and upper bound of belief state b  i.e. which can be obtained from the maximum lower and upper bound of the actions in belief state b. so basically  what we are computing is p u a b  ¡Ý v   b |v   b  ¡« uniform l b  u b     i.e. the probability that u a b  is greater than v   b   assuming that v   b  follows a uniform distribution between l b  and u b . this formula is valid when u a b    l b  and  as we mentioned earlier  if this is not the case  then p a|b  = 1 because we are sure that action a will not be the optimal action. this probability can also be interpreted as the probability that we cannot prune action a in belief state b if v   b  ¡« uniform l b  u b  . while p a|b   is not a probability distribution  it still gives a measure of how likely a certain action will not be pruned in the future and remain as the optimal action.
¡¡an alternative way to approximatethe max operatorwould be to consider the current action with the highest upper bound as the optimal action. in such a case  we can use the alternative definition for p a|b  presented in equation 1.
if a = argmax
	 	  = 1 
	1	otherwize
¡¡furthermore  if we want to know what the probability is  that a certain fringe belief state can be reached by a sequence of optimal actions  we can use the product rule to combine the probabilities of optimal action at each depth.
¡¡combining all these factors  we find that the probability of reaching a certain fringe belief state bd at depth d  denoted p bd   can be computed using equation 1.
		 1 
¡¡in this equation  oi  ai and bi denote the observation  action and belief state encountered at depth i that leads to belief state bd at depth d.
¡¡consequently  we can compute the expected error introduced by a certain fringe belief state bd at depth d on the current belief state b1 by using equation 1.
		 1 
¡¡therefore  we can use equation 1 as an heuristic to choose the fringe node that contributes the most to the error in b1. since we propose two different definitions for the term p a|b   we will refer to e bd  using equation 1 as the heuristic aems1 and e bd  using equation 1 as the heuristic aems1.
¡¡intuitively  e bd  seems a sound heuristic to guide the search since it has several desired properties. first  it will favor exploration of nodes with loose bounds. loose bounds generally indicate that at least one of them is flawed and therefore  exploring such node is generally important to get more precision and make better decisions afterwards. in addition  if we have very tight bounds on the value of a belief state then we do not need to search this belief state any longer since it would have a very low impact on the quality of our solution in belief state b1. moreover  e bd  favors the exploration of the most probable belief states we will encounter in the future. this is good for two reasons. firstly  if a belief state has a really low probability of occurring in the future  then we do not need a high precision on its value because better precision on this belief state would only have a small impact on the value of the actions in b1  and consequently on our action choice in b1. secondly  exploring the most probable belief states also increases the chance that we will be able to reuse the computations done for this belief state in the future  which will improve our precision in the future. finally e bd   favors the exploration of actions that look promising. this behavior is desired for multiple reasons. generally  we will only hesitate between a few actions for the best action choice. these actions will have the highest probability of being optimal  and by concentrating the search to these actions  we should be in a better position to decide which one is the best. if for some reason the promising actions were not optimal  then we should find it pretty quickly when we get better precisions on their upper bounds.
¡¡we can also compare how this heuristic differs from other heuristics that have been proposed to guide best-first-search in pompds. satia & lave actually proposed a similar heuristic  i.e. they suggested exploring at each iteration the k fringe nodes that maximize the term.
this term differs from our heuristic by the fact that they do not consider the term p a|b . we will actually see that this makes a big difference in terms of performance in practice. on the other hand  bi-pomdp always explore the fringe node  reached by a sequenceof actions that maximizes the upper bound  that maximizes  i.e. it is equivalent to choosing the fringe node that maximizes using equation 1 for p a|b . this heuristic does not take into account the probability that a certain belief state is going to be reached  or the discount factor that applies to the value of this belief state  such that it may explore belief nodes in the tree that do not have a lot of impact on the bounds in b1. again  our experiments will show that this affects the performance of the bi-pomdp approach.
1 anytime error minimization search
as mentioned before  the expected error e b  will be the term we will seek to minimize. this can be done efficiently in an anytime fashion by always exploring the fringe belief state that has the highest e b . since this term includes the probability p o|b a   we can possibly handle large observation space because generally in such an environment  only a few observations will have high probabilities and therefore  the search will be conducted only in the most probable part of the tree. furthermore  the probability p a|b  in e b  implicitly does pruning of the non optimal actions and limits the search to the parts of the search tree where the actions have small probabilities of being optimal. a detailed description of our algorithm is presented in algorithm 1.

algorithm 1 aems : anytime error minimization search

function aems t 
static : g: an and-or graph representing the current search tree.
t1 ¡û currenttime   while currenttime   t1 ¡Ü t do
b  ¡û argmaxb¡Êfringe g  e b 
expand b  
¡¡backtrack b   end while return bestact root g   

¡¡the aems algorithm takes the time allowed to search the tree in parameter and returns the best action to do in the current belief state. this current belief state is stored in the root node of the and-or graph g. the graph g is also kept in memory to resume the search at the fringe in the next time steps. after an action is executed in the environment  the graph g is updated such that our new belief state will be the root of g. this is simply done by setting the root to the node we reach by following the action-observation path from the old root node. the value e b  can be computed quickly since all information needed to compute its value can be stored in the belief state nodes when they are created or updated.
¡¡the expand function simply does a one-step look-ahead  from the fringe belief state given in parameter  by constructing the action and-nodes and the next belief state or-nodes resulting from all possible action and observation combinations. it also computes the lower and upper bounds for all the next belief states using the lower bound l and upper bound u functions. for the actions  the lower and upper bounds are simply computed using equation 1 in which v   is replaced by l and u respectively. notice that if we reach a belief state that is already somewhere else in the tree  it will be duplicated  since our current algorithm does not handle cyclic graph structure. we could possibly try to use a technique proposed for ao*  lao* algorithm  hansen and zilberstein  1   to handle cycle  but we have not investigated this further and how it affects the heuristic value.
¡¡once a node has been expanded  we need to backtrack its new upper and lower bounds in the tree  in order to update the probabilities that each action is optimal and reconsider our best action choices. this is done by the backtrack function which recursively recomputes the bounds  using equation 1 in which v   is replaced by l and u   the probabilities p a|b  and the best actions of each ancestor nodes that leads to the expanded node b . notice that if the bounds of a certain ancestor node do not change  then we do not need to pursue the backtracking process because all the subsequent ancestor node bounds will remain the same.
1 empirical results
we now present empirical results with our aems algorithm. we have tested our algorithm in the rocksample environment  smith and simmons  1  and a modified version of this environment  called fieldvisionrocksample  fvrs . fvrs is a new environment that we introduce to test our algorithm in an environment with a big observation space; its observation space size is exponential in the number of rocks  as presented below.
¡¡in each of these environments we first computed a very approximate policy with pbvi by limiting its computation time and number of belief points to a very small value. then we evaluated this policy empirically and we compared the improvement yielded by different online approaches using this policy as a lower bound on v  . for the upper bound  we used the qmdp algorithm and solved the underlying mdp  and provided the resulting value function to every online algorithm. the online time available for decision making was constrained to 1 second per action and all the different online heuristics presented  satia  bi-pomdp  aems1  aems1  were implemented in the same best-first-search algorithm  such that only the search heuristic could affect the performance and not the different implementations. we also compared our heuristic search to the performance of the depthfirst search algorithm rtbss  using the same pbvi and qmdp value functions for the lower and upper bounds.
1 rocksample
in rocksample  rs  environment  a robot has to explore the environment and sample good rocks. each rock can be either good or bad  no scientific value  and the robot receives rewards accordingly. the robot also receives rewards by leaving the environment  by going to the extreme right of the environment . at the beginning  the agent knows the position of each rock  but not their scientific value  good or bad . the robot has an noisy sensor to check if a rock is good or not before choosing to go to this rock and sample it. the accuracy of this sensor depends on the distance between the robot and the rock checked.
¡¡in table 1  we compare the performance of the different algorithms in this environment. to get our results  we ran 1 runs on each of the possible starting rock states and the initial belief state for each of these runs was the uniform distribution over all rock states. we present the average reward  the average online time per action  the average error reduction1 per time step  the average number of belief nodes in the search tree and the average percentage of belief nodes reused in the next time step. for pbvi  the time column shows the offline time used to compute its value function and the belief nodes column shows the number of ¦Á-vectors representing its value function. the number of states  actions and observations of each of the different rocksample environments are shown in parenthesis.
table 1: results in rocksample.
heuristic /errorbeliefnodesalgorithmrewardtime  s reduction  % nodesreused  % rs 1   1s 1a 1o pbvi11-1-satia1111.1bi-pomdp1111.1rtbss 1 1111aems1.1.1.11aems1.1.1.11rs 1   1s 1a 1o pbvi11-1-satia1111.1aems1.1.1.11rtbss 1 1111aems1.1.1.11bi-pomdp1111.1rs 1   1s 1a 1o pbvi11-1-satia1111.1rtbss 1 1111aems1.1.1.11bi-pomdp1111.1aems1.1.1.11rs 1   1s 1a 1o pbvi11-1-satia1111.1bi-pomdp1111.1rtbss 1 1111aems1.1.1.11aems1.1.1.11¡¡the results obtained shows that our aems1 heuristic obtains a significantly better performance than the other heuristics  obtaining the highest error reductionpercentagein all environments. it also obtains the best average reward in all environments except for rs 1  where bi-pomdp obtained surprisingly good rewards. on the other hand  aems1 obtains average performance in most environments. this may be due to the fact that equation 1 does not correctly approximate the probability that the action is optimal. still  aems1 provided interesting performance compared to satia  bi-pomdp and rtbss in rs 1  and rs 1 . we also observethat aems1 generally had a greater reuse percentage than all other approaches which also indicates that the heuristic is really guiding the search toward belief nodes that are more likely to be encountered in the future.
1 fieldvisionrocksample
fieldvisionrocksample  fvrs  differs from the original rocksample by the fact that the robot's noisy sensor perceives all rocks at each turn. so basically  instead of having only 1 observations  good or bad   there are 1r observations  where r is the number of rocks in the environment. as in rocksample  the probability that the sensor is accurate for a certain rock at distance d is computed according to ¦Ç/1 + 1 where ¦Ç = 1 d/d1  d1 being the half efficiency distance  but since the robot perceives all rocks after each action  we had to decrease the accuracy of its sensor in order for the problem to remain partially observable. thus  we have chosen to set the half efficiency distance d1 of the sensor to  where n is the width of the square grid. the probabilities of the joint observations are obtained by simply taking the joint probability of each rock observations using the product rule  we consider that each rock observation is independant . the action space size is reduced to 1   1 move action and a sample action   since the robot has no need to do check actions.
¡¡in table 1  we compare the same statistics that we presented for rocksample in the fvrs environments.
table 1: results in fieldvisionrocksample.
heuristic /errorbeliefnodesalgorithmrewardtime  s reduction  % nodesreused  % fvrs 1   1s 1a 1o pbvi11-1-bi-pomdp1111.1satia1111.1aems1.1.1.11aems1.1.1.11rtbss 1 1111fvrs 1   1s 1a 1o pbvi11-1-rtbss 1 1111bi-pomdp1111.1satia1111.1aems1.1.1.11aems1.1.1.11fvrs 1   1s 1a 1o pbvi11-1-rtbss 1 1111bi-pomdp1111.1satia1111.1aems1.1.1.11aems1.1.1.11¡¡as in rs  we observe that our aems1 heuristic performs substantially better than all other heuristics in the fvrs environments. while rtbss obtained the best result on
fvrs 1   we see that it does not scale as well when the branching factor increases significantly  in fvrs 1  and fvrs 1   since it explores all action/observation combinations. fvrs has a much greater branching factor than the standard rs and the fact that we succeeded in significantly improving the policy obtained with pbvi shows that our heuristic scales well even in environments with big observation space.
1 conclusion
the online search algorithm we proposed has shown very good performance in large pomdps and can tackle big observation space more efficiently than other previous online search techniques. it has shown the best overall performance both in terms of rewards and error reduction compared to the other existing online algorithm. while our search heuristic seems to work very well in practice  much more theoretical work needs to be done in order to bound the error yielded by our algorithm. we also want to investigate further improvement to the algorithm. as we mentioned  our current algorithm does not handle cycle which makes it do redundant computations when belief states are duplicated. we would also like to investigate further different variants of the term p a|b  and see whether we can come up with better approximations of the true probability that a certain action is optimal. finally  we have only tried to combine our approach with the pbvi algorithm  since it is fairly easy to implement. however combining our algorithm with hsvi would be promising since hsvi computes quickly lower and upper bounds that we could use online with our algorithm.
