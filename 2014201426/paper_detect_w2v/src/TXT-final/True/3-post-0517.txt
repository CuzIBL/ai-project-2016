
in many real-world multiagent applications such as distributed sensor nets  a network of agents is formed based on each agent's limited interactions with a small number of neighbors. while distributed pomdps capture the realworld uncertainty in multiagent domains  they fail to exploit such locality of interaction. distributed constraint optimization  dcop  captures the locality of interaction but fails to capture planning under uncertainty. this paper present a new model synthesized from distributed pomdps and dcops  called networked distributed pomdps  nd-pomdps . exploiting network structure enables us to present a distributed policy generation algorithm that performs local search.
1 introduction
in many real-world multiagent applications such as distributed sensor nets  a network of agents is formed based on each agent's limited interactions with a small number of neighbors. for instance  in distributed sensor nets  multiple sensor agents must coordinate with their neighboring agents in order to track individual targets moving through an area. in particular  we consider in this paper a problem motivated by the real-world challenge in  lesser et al.  1 . here  each sensor node can scan in one of four directions - north  south  east or west  see figure 1   and to track a target  two sensors with overlapping scanning areas must coordinate by scanning the same area simultaneously. we assume that there are two independent targets and that each target's movement is uncertain but unaffected by the actions of the sensor agents. additionally  each sensor receives observations only from the area it is scanning and this observation can have both false positives and false negatives. each agent pays a cost for scanning whether the target is present or not but no cost if turned off. existing distributed pomdp algorithms  montemerlo et al.  1; nair et al.  1; becker et al.  1; hansen et al.  1  although rich enough to capture the uncertainties in this

figure 1: sensor net scenario: if present  target1 is in loc1  loc1 or loc1  and target1 is in loc1 or loc1.
domain  are unlikely to work well for such a domain because they are not geared to take advantage of the locality of interaction. as a result they will have to consider all possible action choices of even non-interacting agents  in trying to solve the distributed pomdp. distributed constraint satisfaction and distributed constraint optimization  dcop  have been applied to sensor nets but these approaches cannot capture the uncertainty in the domain. hence  we introduce the networked distributed pomdp  nd-pomdp  model  a hybrid of pomdp and dcop  that can handle the uncertainties in the domain as well as take advantage of locality of interaction. exploiting network structure enables us to present a novel algorithm for nd-pomdps - a distributed policy generation algorithm that performs local search.
1 nd-pomdps
we define an nd-pomdp for a group ag of n agents as a tuple hs a p   o r bi  where s = ¡Á1¡Üi¡Ünsi ¡Á su is the set of world states. si refers to the set of local states of agent i and su is the set of unaffectable states. unaffectable state refers to that part of the world state that cannot be affected by the agents' actions  e.g. environmental factors like target locations that no agent can control. a = ¡Á1¡Üi¡Ünai is the set of joint actions  where ai is the set of action for agent i.
¡¡we assume a transition independent distributed pomdp model  where the transition function is defined as p s a s¡ä  = pu su s¡äu ¡¤q1¡Üi¡Ün pi si su ai s¡äi   where a=ha1 ... ani is the joint action performed in state s=hs1 ... sn sui and is the resulting state. agent i's transition function is defined as pi si su ai s¡äi  = pr s¡äi|si su ai  and the unaffectable transition function is defined as pu su s¡äu  = pr s¡äu|su . becker et al.  also relied on transition independence  and goldman and zilberstein  introduced the possibility of uncontrollable state features. in both works  the authors assumed that the state is collectively observable  an assumption that does not hold for our domains of interest.
¡¡  = ¡Á1¡Üi¡Ün i is the set of joint observations where  i is the set of observations for agents i. we make an assumption of observational independence  i.e.  we define the joint observation function as o s a ¦Ø  = q oi si s  ai ¦Øi   where s = hs  ... s  s i  a = ha1 ... ani  ¦Ø = h¦Ø1 ... ¦Øni pr ¦Øi|s1 su ai .
the reward function  r  is defined as r s a  =
pl rl sl1 ... slk su hal1 ... alki   where each l could refer to any sub-group of agents and k = |l|. in the sensor grid example  the reward function is expressed as the sum of rewards between sensor agents that have overlapping areas  k = 1  and the reward functions for an individual agent's cost for sensing  k = 1 . based on the reward function  we construct an interaction hypergraph where a hyper-link  l  exists between a subset of agents for all rl that comprise r. interaction hypergraph is defined as g =  ag e   where the agents  ag  are the vertices and e = {l|l   ag ¡Ä rl is a component of r} are the edges. neighborhood of i is defined as ni = {j ¡Ê ag|j 1= i ¡Ä  l ¡Ê e  i ¡Ê l ¡Ä j ¡Ê l }. sni = ¡Áj¡Ênisj refers to the states of i's neighborhood. similarly we define ani   ni  pni and oni.
¡¡b  the distribution over the initial state  is defined as b s  = bu su ¡¤q1¡Üi¡Ün bi si  where bu and bi refer to the distributions over initial unaffectable state and over i's initial state  respectively. we define bni = qj¡Êni bj sj . we assume that b is available to all agents  although it is possible to refine our model to make available to agent i only bu  bi and bni . the goal in nd-pomdp is to compute joint policy ¦Ð = h¦Ði ... ¦Ðni that maximizes the team's expected reward over a finite horizon t starting from b. ¦Ði refers to the individual policy of agent i and is a mapping from the set of observation histories of i to
ai. ¦Ðni and ¦Ðl refer to the joint policies of the agents in ni and hyper-link l respectively.
¡¡nd-pomdp can be thought of as an n-ary dcop where the variable at each node is an individual agent's policy. the reward component rl where |l| = 1 can be thought of as a local constraint while the reward component rl where l   1 corresponds to a non-local constraint in the constraint graph. in the next section  we push this analogy further by taking inspiration from the dba algorithm  yokoo and hirayama  1   an algorithm for distributed constraint satisfaction  to develop an algorithm for solving nd-pomdps.
¡¡we define local neighborhood utility of agent i as the expected reward accruing due to the hyper-links that contain agent i:

	si sni su	l¡Êe s.t. i¡Êl
 1 
while trying to find best policy for agent i given its neighbors' policies  we do not need to consider nonneighbors' policies. this is the property of locality of interaction that is used in the following section.
1 locally optimal policy generation
the locally optimal policy generation algorithm called lid-jesp  locally interacting distributed joint equilibrium search for policies  is based on the dba algorithm  yokoo and hirayama  1  and jesp  nair et al.  1 . in this algorithm  see algorithm 1   each agent tries to improve its policy with respect to its neighbors' policies in a distributed manner similar to dba. initially each agent i starts with a random policy and exchanges its policies with its neighbors. it then evaluates its contribution to the global value function  from its initial belief state b with respect to its current policy and its neighbors' policy  function evaluate   . agent i then tries to improve upon its current policy by calling function getvalue  which returns the value of agent i's best response to its neighbors' policies. agent i then computes the gain that it can make to its local neighborhood utility  and exchanges its gain its neighbors. if i's gain is greater than that one any of its neighbors  i changes its policy and sends its new policy to all its neighbors. this process of trying to improve the local policy is continued until termination  which is based on maintaining and exchanging a counter that counts the number of cycles where gaini = 1. this is omitted from this article in order to simplify the presentation.
¡¡the algorithm for computing the best response is a dynamic-programming approach similar to that used in
jesp. here  we define an episode of agent i at time t as
. given that the neighbors' poli-
cies are fixed  treating episode as the state  results in a
single agent pomdp  where the transition function and observation function can be defined as follows:
p¡ä eti ati eti+1 =pu stu stu+1  ¡¤ pi sti stu ai sti+1  ¡¤
pni stni stu ani stn+1i   ¡¤ oni snt+1i  sut+1 ani ¦Øni 
	o¡ä eti+1 ati ¦Øit+1 	=oi sti+1 stu+1 ai ¦Øi 
the function getvalue   returns the optimal policy for agent i given the above definitions of transition and observation functions and the policies of ni. in this paper  getvalue   was implemented via value iteration but any other single agent pomdp algorithm could have been used.
1 experimental results
for our experiments  we ran the lid-jesp algorithm on the sensor domain  see figure 1 . the first benchmark
algorithm 1 lid-jesp agent i 
1: ¦Ði ¡û randomly selected policy  prevv al ¡û 1
1: exchange ¦Ði with ni
1: while termination not detected do
1:	for all si sni su do1:prevv al	¡û+	bu su 	¡¤	bi si 	¡¤	bni sni 	¡¤
evaluate agent i si su  sni ¦Ði ¦Ðni hi hi 1 t 1:gaini ¡û getvalue agent i b ¦Ðni 1 t    prevv al1:exchange gaini with ni1:maxgain ¡û maxj¡Êni¡È{i}gainj1:winner ¡û argmaxj¡Êni¡È{i}gainj1:if maxgain   1 and i = winner then1:initialize ¦Ði1:findpolicy agent i b hi ¦Ðni 1 t 1:communicate ¦Ði with ni1:else if maxgain   1 then1: receive ¦Ðwinner from winner and update ¦Ðni 1: return ¦Ði jesp  is nair et al.'s jesp algorithm   which uses a centralized processor to find a locally optimal joint policy and does not consider the interaction graph. the second benchmark  lid-jesp-no-nw  is lid-jesp with a fully connected interaction graph. figure 1 a  shows the run time in seconds on a logscale on the y-axis for increasing finite horizon t on the x-axis  while figure 1 b  shows the value of policy found on the y-axis and increasing finite horizon t on the x-axis. all run times and values were obtained by averaging 1 runs  each with different randomly chosen starting policies . as seen in figure 1 b   the values obtained for lid-jesp  jesp and lid-jesp-no-nw are quite similar  although lidjesp and lid-jesp-no-nw often converged on a higher local optima than jesp. in comparing the run times  it should be noted that lid-jesp outperforms lid-jespno-nw and jesp  which could not be run for t   1 within 1 secs .
1 acknowledgments
this material is based upon work supported by the darpa/ipto coordinators program and the air force research laboratoryunder contract no. fa1- 1-c-1.the views and conclusions contained in this document are those of the authors  and should not be interpreted as representing the official policies  either expressed or implied  of the defense advanced research projects agency or the u.s. government.
