 
we present a novel  laser range finder based algorithm for simultaneous localization and mapping  slam  for mobile robots. slam addresses the problem of constructing an accurate map in real time despite imperfect information about the robot's trajectory through the environment. unlike other approaches that assume predetermined landmarks  and must deal with a resulting dataassociation problem  our algorithm is purely laser based. our algorithm uses a particle filter to represent both robot poses and possible map configurations. by using a new map representation  which we call distributed particle  dp  mapping  we are able to maintain and update hundreds of candidate maps and robot poses efficiently. the worst-case complexity of our algorithm per laser sweep is log-quadratic in the number of particles we maintain and linear in the area swept out by the laser. however  in practice our run time is usually much less than that. our technique contains essentially no assumptions about the environment yet it is accurate enough to close loops of 1m in length with crisp  perpendicular edges on corridors and minimal or no misalignment errors. 
1 	introduction 
the availability of relatively inexpensive laser range finders and the development of particle filter based algorithms have led to great strides in recent years on the problem of robot localization - determining a robot's position given a known map  fox et al  1 . initially  the maps used for these methods were constructed by hand. however  the accuracy of the laser suggests its use for map-making as well as localization. potential applications for accurate map-making would include search and rescue operations  as well as space  underwater and subterranean exploration. 
　even with an accurate laser range finder  map-making presents a difficult challenge: a precise position estimate is required to make consistent updates to the the map  but a good map is required for reliable localization. the challenge of simultaneous localization and mapping  slam  is that of producing accurate maps in real time  based on a single pass over the sensor data  without an off line correction phase. straightforward approaches that localize the robot based upon a partial map and then update the map based upon the maximum likelihood position of the robot tend to produce maps with errors that accumulate over time. when the robot closes a physical loop in the environment  serious misalignment errors can result. 
　the em algorithm provides a very principled approach to the problem  but it involves an expensive off-line alignment phase  burgard et ah  1 . there exist heuristic approaches to this problem that fall short of full em1  but they are not a complete solution and they require additional passes over the sensor data  thrun  1 . scan matching can produce good maps flu and milios  1; gutmann and konolige  1  from laser range finder data  but such approaches typically must explicitly look for and require additional effort to close loops. in varying degrees  these approaches can be viewed as partially separating the localization and mapping components of slam. 
　the fastslam algorithm  montemerlo et a/.  1j  which does not require explicit loop-closing heuristics  is a recent slam approach which has made great progress in the field. fastslam follows a proposal by murphy  murphy  1 using a rao-blackwellized particle filter to sample robot poses and track the position of a fixed number of predetermined landmarks using a kalman filter.  the landmark positions are conditionally independent given the robot pose.  this method mitigates some of the challenges in mapping at the expense of some challenges in landmark selection and identification. the latter can involve a fairly complicated data association problem  although recent progress has been made in addressing this  montemerlo and thrun  1. 
　we present a novel  laser range finder based algorithm called dp-slam that  like fastslam  exploits the conditional independences noted by murphy. however  our algorithm is purely laser based and makes no landmark assumptions. we avoid the data association problem by storing multiple detailed maps instead of sparse landmarks  thereby subsuming association with localization. our algorithm uses a particle filter to represent both robot poses and possible map configurations. using a new map representation  which wc call distributed particle  dp  mapping  we are able to maintain and update hundreds or thousands of candidate maps and robot poses in real time as the robot moves through the environment. the worst-case complexity of our algorithm per laser sweep is log-quadratic in the number of particles we 
　　　lthis approach is heuristic because it does not maintain a joint probability distribution over maps and poses. 
maintain and linear in the area swept out by the laser. however  in practice our run time is usually much less. our technique makes essentially no assumptions about the environment yet it is accurate enough to close loops of 1m in length with crisp  perpendicular edges on corridors and minimal or no misalignment errors. this accuracy is achieved throughout the mapping process  without a need for explicit algorithms to correct the loop as temporally distant observations begin to overlap. in fact  dp-slam could be further complimented by inclusion of existing algorithms for closing loops  though the addition may be unnecessary if a sufficient number of particles is used. 
1 	particle filters for localization and mapping 
a particle filter is a simulation-based method of tracking a system with partially observable state. we briefly review particle filters here  but refer the reader to excellent overviews of this topic idoucet el ai  1  and its application to robotics  thrun  1j for a more complete discussion. 
　a particle filter maintains a weighted  and normalized  set of sampled states   cahed particles. at each 
step  upon observing an observation o  or vector of observations   the particle filter: 
1. samples  m s with replacement. 
1. propagates each new state through a markovian transition  or simulation  model: . this entails sampling a new state from the conditional distribution over next states given the sampled previous state. 
1. weighs each new state according to a markovian observation model: 
1. normalizes the weights for the new set of states 
　particle filters arc easy to implement have been used to track multimodal distributions for many practical problems  doucet et al  1 . 
1 	particle filters for localization 
a particle filter is a natural approach to the localization problem  where the robot pose is the hidden state to be tracked. the state transition is the robot's movement and the observations are the robot's sensor readings  all of which are noisy. 
　the change of the state over time is handled by a motion model. usually  the motion indicated by the odometry is taken as the basis for the motion model  as it is a reliable measure of the amount that the wheels have turned. however  odometry is a notoriously inaccurate measure of actual robot motion  even in the best of environments. the slip and shift of the robot's wheels  and unevenness in the terrain can combine to give significant errors which will quickly accumulate. a motion model differs across robots and types of terrain  but generally consists of a linear shift  to account for systematic errors and gaussian noise. thus  for odometer changes of x  and a particle filter applies the error model and obtains  for particle 

the a and b terms are linear correction to account for consistent errors in motion. the function.  returns random noise from a normal distribution with mean 1 and standard deviation a  which is derived experimentally and may depend upon the magnitudes of and 
　after simulation  we need to weight the particles based on the robot's current observations of the environment. for pure localization  the robot stores a map in memory. the position described by each particle corresponds to a distinct point and orientation in the map. therefore  it is relatively simple to determine what values the sensors should return  given the pose within the map. the standard assumption is that sensor errors are normally distributed. thus  if the first obstruction in the map along a line traced by a laser cast is at distance d and the reported distance is the probability density of observing discrepancy is normally distributed with mean 1. 
for our experiments we assumed a standard deviation in laser measurements of 1cm. given the model and pose  each sensor reading is correctly treated as an independent observation 1 murphy  1. the total posterior for particle i is then 

where  is the difference between the expected and perceived distances for sensor  laser cast   and particle i. 
1 	particle filters for s l a m 
some approaches for slam using particle filters attempt to maintain a single map with multiple robot poses  thrun  1   an approach that we avoid because it leads to errors that accumulate over time. the basic problem is that the hidden state is actually both the robot pose and the map itself. an important consequence of this problem is that all observations arc no longer compared to a single map  which is presumed to be correct. instead  the observations are compared against an incomplete and possibly incorrect map  identified with the particle in question. the map itself is created by the accumulation of observations of the environment and estimates of robot positions. 
　in principle  this  solves  the slam problem. in practice  it replaces a conceptual problem with an algorithmic one. particles with errors in robot position estimates will make erroneous additions to their maps. an error in the map will then  in turn  cause another error in localization in the next step  and these inaccuracies can quickly compound. thus  the number of particles required for slam is typically more than that required for localization since the price of accumulated errors is much higher. note that in pure localization  small errors in position estimation can be absorbed as part of the noise in the motion model. 
ing this problem is to assume that the uncertainty in the map 
1 	robotics 　the algorithmic problem becomes one of efficiently maintaining a large enough set of particles to obtain robust performance  where each particle is not merely a robot pose  but a pose and map. since maps are not light weight data structures  maintaining the hundreds or thousands of such maps poses a serious challenge. one reasonable approach to tamcan be represented in a simple parametric form. this is essentially the approach taken by fastslam  for which the map is a kalman filter over a set of landmark positions. this is certainly the right thing to do if one is given a set of landmarks that can be quickly and unambiguously identified. we will show that this strong assumption is not required: by using raw laser data  combined with an occupancy grid and efficient data structures  we can handle a large number of candidate maps and poses efficiently  achieving robust performance. 
1 dp-slam 
in this section we motivate and present the main technical contribution of the dp-slam algorithm. dp-slam implements what is essentially a simple particle filter over maps and robot poses. however  it uses a technique called distributed particle mapping  dp-mapping   which enables it to maintain a large number of maps very efficiently. 
1 	naive s l a m 
when using a particle filter for slam  each particle corresponds to a specified trajectory through the environment and has a specific map associated with it. when a particle is resampled  the entire map itself is treated as part of the hidden state that is being tracked and is copied over to the new particle. if the map is an occupancy grid of size m and p particles are maintained by the particle filter  then ignoring the cost of localization  o mp  operations must be performed merely copying maps. for a number of particles sufficient to achieve precise localization in a reasonably sized environment  the naive approach would require gigabytes worth of data movement per update1. 
1 	distributed particle mapping 
by now the astute reader has most likely observed that the naive approach is doing too much work. to make this clearer  we will introduce the notion of a particle ancestry. when a particle is sampled at iteration i to produce a successor particle at iteration we call the generation i particle a parent and the generation particle a child. two children with the same parent are siblings. from here  the concept of a particle ancestry extends naturally. suppose the laser sweeps out an area of size  and consider two siblings  s1 and s1. each sibling will correspond to a different robot pose and will make at most a updates to the map it inherits from its parent. thus  s1 and s1 can differ in at most a map positions. 
　when the problem is presented in this manner  the natural reaction from most computer scientists is to propose recording the  diff  between maps  i.e  recording a list of changes that each particle makes to its parent's map. while this would solve the problem of making efficient map updates  it would create a bad computational problem for localization: tracing a line though the map to look for an obstacle would require working through the current particle's entire ancestry and consulting the stored list of differences for each particle in the ancestry. the complexity of this operation would be linear in the number of iterations of the particle filter. the challenge is  therefore  to provide data structures that permit efficient updates to the map and efficient localization queries with time complexity that is independent of the number of iterations of the particle filter. we call our solution to this problem distributed particle mapping or dp-mapping  and we explain it in terms of the two data structures that are maintained: the ancestry tree and the map itself. 
maintaining the particle ancestry tree 
the basic idea of the particle ancestry tree is fairly straightforward. the tree itself is rooted with an initial particle  of which all other particles are progeny. each particle maintains a pointer to its parent and is assigned a unique numerical id. finally each particle maintains a list of grid squares that it has updated. 
　the details of how we will use the ancestry tree for localization are described in the subsequent section. in this section we focus on the maintenance of the ancestry tree  specifically on making certain that the tree has bounded size regardless of the number of iterations of the particle filter. 
　we maintain a bounded size tree by pruning away unnecessary nodes. first  note that certain particles may not have children and can simply be removed from the tree. of course  the removal of such a particle may leave its parent without children as well  and we can recursively prune away dead branches of the tree. after pruning  it is obvious that the only particles which are stored in our ancestry tree are exactly those particles which are ancestors of the current generation of particles. 
　this is still somewhat more information than we need to remember. if a particle has only one child in our ancestry tree  we can essentially remove it  by collapsing that branch of the tree. this has the effect of merging the parent's and child's updates to the map  a process described in the subsequent section. by applying this process to the entire tree after pruning  we obtain a minimal ancestry tree  which has several desirable and easily provable properties: 
proposition 1 independent of the number of iterations of particle filtering  a minimal ancestry tree of p particles j. has exactly p leaves  
1. has branching factor of at least 1  and 
1. has depth no more than p. 
map representation 
the challenge for our map representation is to devise a data structure that permits efficient updates and efficient localization. the naive approach of a complete map for each particle is inefficient  while the somewhat less naive approach of simply maintaining history of each particle's updates is also inefficient because it introduces a dependency on the number of iterations of the particle filter. 
　our solution to the map representation problem is to associate particles with maps  instead of associating maps with particles. dp-mapping maintains just a single occupancy grid.  the particles are distributed over the map.  unlike a traditional occupancy grid  each grid square stores a balanced tree  such as a red-black tree. the tree is keyed on the ids of the particles that have made changes to the occupancy of the square. 
　the grid is initialized as a matrix of empty trees. when a particle makes an observation about a grid square it inserts its id and the observation into the associated tree. notice that this method of recording maps actually allows each particle to behave as if it has its own map. to check the value of a grid square  the particle checks each of its ancestors to find the most recent one that made an observation for that square. if no ancestor made an entry  then the particle can treat this position as being unknown. 
　we can now describe the effects of collapsing an ancestor with a single child in the ancestry tree more precisely: first  the set of squares updated by the child is merged into the parent's set. second  for each square visited by the child  we change the id key stored in the balanced tree to match that of the parent.  if both the child and parent have made an update to the same square  the parent's update is replaced with the child's.  the child is then removed from the tree and the parent's grandchildren become its direct children. note that this ensures that the number of items stored in the balanced tree at each grid square is 1{p . 
1 	computational complexity 
the one nice thing about the naive approach of keeping a complete map for each particle is the simplicity: if we ignore the cost of block copying maps  lookups and changes to the map can all be done in constant time. in these areas  distributed particle mapping may initially seem less efficient. however  we can show that dp maps are in fact asymptotically superior to the naive approach. 
　lookup on a dp-map requires a comparison between the ancestry of a particle with the balanced tree at that grid square. let d be the depth of the ancestry tree  and thus is the maximum length of a particle's ancestry. strictly speaking  as the ancestry tree is not guaranteed to be balanced  d can be o p . however  in practice  this is almost never the case  and we have found   as the nature of particle resampling lends to very balanced ancestry trees.  please see the discussion in the following section for more detail on this point.  therefore  we can complete our lookup after just d accesses to the balanced tree. since the balanced tree itself can hold at most p entries  and a single search takes 1 lgp  time. accessing a specific grid square in the map can therefore be done in o dlgp  time. 
　for localization  each particle will need to make 1 a  accesses to the map. as each particle needs to access the entire observed space for its own map  we need o ap  accesses  giving localization with dp-maps a complexity of o adplgp . 
　to complete the analysis we must handle two remaining details: the cost of inserting new information into the map  and the cost of maintaining the ancestry tree. since we use a balanced tree for each grid square  insertions and deletions on our map both take 1 lgp  per entry. each particle can make at most 1 a  new entries  which in turn will only need to be removed once. thus the procedure of adding new entries can be accomplished in o adlgp  per particle  or o adplgp  total and the cost of deleting childless particles will be amortized as o adplgp . 
　it remains to be shown that the housekeeping required to maintain the ancestry tree has reasonable cost. specifically  we need to show that the cost of collapsing childless ancestry tree nodes does not exceed o adplgp . this may not be obvious at first  since successive collapsing operations can make the set of updated squares for a node in the ancestry tree as large as the entire map. we now argue that the amortized cost of these changes will be o adplgp . first  consider the cost of merging the child's list of modified squares into the parent's list. if the child has modified n squares  we must perform 1 nlgp  operations  n balanced tree queries on the parent's key  to check the child's entries against the parent's for duplicates. 
　the final step that is required consists of updating the id for all of the child's map entries. this is accomplished by deleting the old id  and inserting a new copy of it  with the parent's id. the cost of this is again 1 nlgp . consider that each map entry stored in the particle ancestry tree has a potential of d steps that it can be collapsed  since d is the total number of nodes between its initial position and the root  and no new nodes will ever be added in between. at each iteration  p particles each create a new map entries with potential d. thus the total potential at each iteration is o adplgp . the computational complexity of dp-slam can be summarized as follows: 
proposition 1 for a particle filter that maintains p particles  laser that sweeps out a grid squares  and an ancestry tree of depth d  dp-slam requires: 
  o adplgp  	operations for localization arising from: 
- p particles checking a grid squares 
- a lookup cost of o dlgp  per grid square 
  o aplgp  operations to insert new data into the tree  arising from: 
- p particles inserting information at a grid squares 
- insertion cost of o lgp  per new piece of informa-tion 
  ancestry tree maintenance with amortized cost o adplgp  arising from 
- a cost of o lgp  to remove an observation or move it up one level in the ancestry tree 
- a maximum potential of adp introduced at each iteration. 
1 	complexity comparison 
our analysis shows that the amortized complexity of 
1 	robotics dp-slam is o adplgp   which can be as large as 1 ap1lgp . for the naive approach  each map is represented explicitly  so lookups can be done in constant time per grid square. the localization step thus takes only o ap . without the need for updates to an ancestry tree  map updates can likewise be done in o ap  time. however  the main bulk of computation lies in the need to copy over an entire map for each particle  which will require 1 mp  time  where m is the size of the map. since typically  this obviously is the dominant term in the computation. 
dp-slam will be advantageous when 
even in the worst case where d approaches p  there will still be a benefit. for a high resolution occupancy grid  m will be quite large since it grows quadratically with the linear measure of the environment and would grow cubically if we we consider height. moreover  a  will be a tiny fraction of m. the size of p will  of course  depend upon the environmental features and the sampling rate of the data. it is important to note that since the time complexity of our algorithm does not depend directly on the size of the map  there will necessarily exist environments for which dp-slam is vastly superior to the naive approach since the naive approach will require block copying an amount of data will exceed physical memory. 
　in our initial experiments  we have observed that p was surprisingly small  suggesting that in practice the advantage of dp-slam is much greater than the worst-case analysis suggests. for some problems the point at which it is advantageous to use dp-slam may be closer to . this phenomenon is discussed in more detail in the subsequent section. 
1 	empirical results 
we implemented and deployed the dp-slam algorithm on a real robot using data collected from the second floor of our computer science building. in our initial implementation our map representation is a binary occupancy grid. when a particle detects an obstruction  the corresponding point in the occupancy grid is marked as fully occupied and it remains occupied for all successors of the particle.  a probabilistic approach that updates the grid is a natural avenue for future work.  
　on a fast pc  1 ghz pentium 1   the run time of dpslam is close to that of the data collection time  so the algorithm could have been run in real time for our test domains. in practice  however  we collected our data in a log file using the relatively slow computer on our robot  and processed the data later using a faster machine. to speed things up  we also implemented a novel particle culling technique to avoid fully evaluating the posterior for bad particles that are unlikely to be resampled. this works by dividing the sensor readings for each particle into  disjoint  evenly distributed subsets. the posterior for the particles is computed in passes  where pass i evaluates the contribution from subset at the end of each pass  particles with significantly lower  partially evaluated  posterior in comparison to the others are assigned 1 probability and removed from further consideration. we also set a hard threshold on the number of particles we allowed the algorithm to keep after culling. typically  this was set to the top 1% of the total number of particles considered. in practice  this cutoff was almost never used since culling effectively removed at least 1% of the particles that were proposed. a value if gave us a speedup of approximately 
　for the results we present  it is important to emphasize that our algorithm knows absolutely nothing about the environment or the existence of loops. no assumptions about the environment are made  and no attempt is made to smooth over errors in the map when loops are closed. the precision in our maps results directly from the robustness of maintaining multiple maps. 
1 robot and robot model 
the robot we used for testing dp-slam is an irobot atrv 
jr. equipped with a sick laser range finder attached to the front of the robot at a height of 1cm. readings are made across 1＜  spaced one degree apart  with an effective distance of up to 1m. the error in distance readings is typically less than 1mm. 
　odometric readings from the atrv jr.'s shaft encoders are unreliable  particularly when turning. our motion model assumes errors are distributed in a gaussian manner  with a standard deviation of 1% in lateral motion and 1% in turns. turns are quasi-holonomic  performed by skid steering. we obtained our motion model using an automated calibration procedure that worked by positioning the robot against a smooth wall and comparing odometry data with the movement inferred from the laser readings. 
1 test domain s  and map s  
we tested the algorithm on a loop of hallway approximately 1m by 1m. a new set of observations was recorded after each 1cm motion  approximately . the maps were constructed with a resolution of 1cm per grid square  providing a very high resolution cross section of the world at a height of 1cm from the floor. 
　figure 1 shows the highest probability map generated after the completion of one such loop using 1 particles1. in this test  the robot began in the middle of the bottom hallway and continued counterclockwise through the rest of the map  returning to the starting point. 
　this example demonstrates the accuracy of dp-slam when completing a long loop  one of the more difficult tasks for slam algorithms. after traveling 1m  the robot is once again able to observe the same section of hallway in which it started. at that point  any accumulated error will readily become apparent  as it will lead to obvious misalignments in the corridor. as the figure shows  the loop was closed perfectly  with no discernible misalignment. 
　to underscore the advantages of maintaining multiple maps  we have included the results obtained when using a single map and the same number of particles. figure 1 shows the result of processing the same sensor log file by generating 1 particles at each time step  keeping the single particle with the highest posterior  and updating the map based upon the robot's pose in this particle. there is a considerable misalignment error where the loop is closed at the top of the map. larger  annotated versions of the maps shown here  as well as maps generated from different sensor logs  will be available at http://www.es.duke.edu/~parr/dpslam/. 




figure 1: depth of coalescence as a function of time. 
1 	coalescence 
in our sample domain  the second floor of the duke university department of computer science   we have found that dp-slam produces very shallow ancestry trees due to a phenomenon we refer to as coalescence. over a number iterations of the particle filter  nearly all of the particles will have a common ancestor particle in the not-too-distant past. this point of coalescence  or common ancestry  varies over time  and is sensitive to the environment. observations made on our empirical results indicate that while this number peaked during events of higher uncertainty  coalescence was often as recent as 1 generations in the past  and never exceeded 1. this means that beyond about 1 steps in the past  every particle has an identical map. the fact that we can close loops so precisely while maintaining relatively shallow coalescence points suggests that we are able to maintain distributions over map regions long enough to resolve ambiguities  but not longer than is needed. while the phenomenon deserves more careful study  our initial impression is that by the time a region has passed outside of the robot's field of view  it has been scanned enough times by the laser that there is only a single hypothesis that is consistent with the accumulated observations. 
　additional experiments were run to show the relationship between coalescence and uncertainty  as well as the ability of dp-slam to automatically preserve more information when required. we considered two different scenarios: the original dp-slam algorithm with the same sensor data used to generate figure 1  and a handicapped version of dp-slam run on the same sensor log  but with three out of every four laser casts ignored at each iteration. the handicapped version required 1 particles to produce good maps. 
　to study the effects of uncertainty on coalescence  we ran both algorithms with the same number of particles and then compared the coalescence points. the results in figure 1 show how the two versions of dp-slam performed on the same sensor log with the same number of particles. the decrease in observational information for the handicapped version of dp- slam makes it more difficult for the robot to be certain about its position. this is reflected in the higher peaks in the graph  and the less recent point of coalescence overall. 
　in both cases  strong correlation can be seen between the higher peaks in this graph and the turns that the robot took in the map.  note the iteration number annotations at the turns in figure 1.  this is because odometry is particularly unreliable during skid turns and the robot has greater difficulty localizing. the points at which our turns occur in our office environment also tend to produce more ambiguous sensor data since turns are typically in open areas where the laser scans will be more widely spaced. other peaks can be mapped to areas of clutter in the map  such as the recycling bins. 
　figure 1 was created from the same two experiments  and tracks the amount of memory used over time  as measured by the total number of leaves in the balanced trees stored in our occupancy grid. since no leaves are stored for empty space and our maps are essentially two one-dimensional surfaces  the amount of memory required for leaves should grow roughly linearly over time. the peaks corresponding to the turns in the map are even more pronounced for the handicapped version of dp-slam in this graph  indicating that under conditions of greater uncertainty  the ancestry tree grows in width as well as depth. 
　of course  shallow coalescence may not occur and may not even be desirable in all environments  but it has significant implications for the efficiency of the algorithm when it 
does occur. it implies making our o bound closer to in practice. moreover  we get maximum benefit from the compactness of our map representation. as the particles coalesce  irrelevant entries are removed. thus all areas in the map that were observed before the point of coalescence contain only one entry in their list. similarly  those areas which have not been observed by any particle yet have no entries in their list. if we can assume that the depth of coalescence is bounded by some reasonable constant c  then all map squares but those that have been altered prior the point of coalescence can be thought of as requiring a constant amount of storage. this means that the total space required to store the map tends to be close to in practice  not the theoretical worst case of o mp . 
　a sceptic may wonder if coalescence is indeed a desirable thing since it may imply a loss of information. if the algorithm maintains only a limited window of uncertainty  how can it be expected to close large loops  which would seem to require maintaining uncertainty over map entries for arbitrarily long durations  the simple answer is that coalescence is desirable when it is warranted by the data and undesirable when the accumulation of sensor data is insufficient to resolve ambiguities. in fact  coalescence is a direct product of the amount of uncertainty still present in the observations. our algorithm is both able to maintain ambiguity for arbitrary time periods and benefit from coalescence when it is warranted  so there is nothing negative about the fact that we have observed this phenomenon in our experiments. 
　of course  since we are representing a complicated distribution with a finite number of samples  there is always a danger that the low probability  tails  of the distribution will be lost if too few particles are used. this would result in premature coalescence due to failure to resample a potentially 

robotics 	1 


figure 1: total map entries used over time. 

to one part of the model to compensation for unmodeled noise in other parts. nevertheless  in future work we would like to develop a more comprehensive error model and a principled scheme for updating the map that handles partially or transiently occupied grid squares. naturally  we would like to consider other map representation methods too. 
   the most important future direction for this work is to incorporate less reliable sensors as such video or sonar to construct a more comprehensive  three-dimensional view of the environment. 
acknowledgments 
we are very grateful to sebastian thrun for critical feedback and encouragement for this line of research.we also thank tammy bailey  dieter fox  carlos guestrin  dirk haehnel  
mark paskin  and carlo tomasi for helpful feedback and comments. 

useful and correct map. there will certainly be some environments  typically those with very sparse sensor data  where it will be difficult to avoid premature coalescence without an excessive number of particles. in situations with sparse data  a parametric representation of uncertainty over landmarks may be more tractable and more appropriate. 
1 	conclusion and future work 
we have presented a novel particle-filter based algorithm for simultaneous localization and mapping. using very efficient data structures  this algorithm is able to maintain thousands of maps in real time  providing robust and highly accurate mapping. the algorithm does not assume the presence of predetermined landmarks or assume away the data association problem created by the use of landmarks. our algorithm has been deployed on a real robot  where it has produced highly detailed maps of an office environment. 
   we believe that this is the first time this level of accuracy has been achieved for the type of data we consider using an algorithm that does not explicitly attempt to close loops and that has no domain specific knowledge. nevertheless  our algorithm does have some limitations. most reasonably priced laser range finders scan at a fixed height  giving us an incomplete view of the environment. our current map representation is very simplistic - a grid where each cell is presumed to be completely clear or completely opaque to the laser. this obviously creates discretization errors at the edges of objects and confusion for very small objects. our small cell size reduces this problem  but does not eliminate it entirely. for example  power cords hanging off of the edge of desks tend to introduce localization errors and increase the number of particles needed to get robust performance. 
   there is a logical inconsistency in the way we treat the laser data. when we add new information to the map  we treat the laser like a deterministic device  but when we localize  we treat it like a noisy device with a standard deviation in measurement that can span several grid squares. exaggerated error models like this are common in many real-world applications of statistical models  where additional noise is added 
