  
bart selman at&t bell laboratories 
murray hill  nj 1 u.s.a. selman research.att.com 
http://www.research.att.com/orgs/ssr/people/selman/ 
	abstract 	1 hard problems and phase transitions 
　
computationally hard instances of combinatorial problems arise at a certain critical ratio of constraints to variables. at the critical ratio  problem distributions undergo dramatic changes. i will discuss how an analogous phenomenon occurs in phase transitions studied in physics  and how experiments with critically constrained problems have led to surprising new insights into average-case complexity and stochastic search methods in ai. 
1 	introduction 
many ai formalisms  such as used in planning  reasoning  and learning  have been shown to be inherently intractable. these intractability results are generally based on a worstcase analysis  and hence  there has been much debate about their practical relevance. average-case complexity analysis  on the other hand  would appear to be more directly applicable. however  such analysis requires a precise model of the distribution of input instances. since we do not have a good understanding of real-world problem distributions  averagecase results generally assume relatively simple distributions of randomly generated problem instances. 
　average-case results based on such distributions show that almost all randomly generated instances of combinatorial problems are surprisingly easy to solve. this has led some ai researchers to dismiss many of the negative worst-case complexity results. however  as discussed below  recent work has shown that the positive average-case results are largely due to the particular choice of input distribution. by being more careful in generating instances  one can in fact quite easily obtain extremely hard search and reasoning problems. the key property of such hard random instances is that they have to be critically constrained. that is  they occur at a certain critical ratio of variables to constraints. at this ratio  the problem distributions undergo dramatic changes. i will discuss how an analogous phenomenon occurs in phase transitions studied in physics  and how tools from statistical mechanics can be used to analyze the transition phenomenon. 
　in the second part of my talk  i will show how the recent new insights into computationally hard problems have led to the development of powerful new stochastic search methods. these methods now offer a viable alternative to the more traditional systematic methods. 
1 	invited speakers 
in   cheeseman et al. study the computational cost of solving randomly generated graph coloring problems. starting with a given number of nodes  a random graph is generated by adding edges between randomly chosen pairs of nodes . in graph coloring  the goal is to assign a color to each node in such a way that no two nodes connected by an edge have the same color. cheeseman et al consider the computational cost of coloring random graphs with a fixed number of colors  using a backtrack-style algorithm. 
　the average cost of coloring random graphs was found to be directly dependent on the ratio of the number of edges to the number of nodes. when graphs contain relatively few edges  they tend to have many valid colorings  and a backtrack style algorithm can find a coloring quite easily - early on in its search. on the other extreme  when a graph contains many edges  a valid coloring often does not exist. because of the many potential conflicts between the nodes  the global inconsistency of the coloring problem can be detected relatively easily. finally  at a certain critical ratio of edges to nodes  it becomes quite difficult for a coloring procedure to determine whether the graphs are colorable or not. intuitively  the instances are  critically constrained . the average computational cost of running the coloring procedure on the critically constrained graphs scales exponentially with the size of the graphs. so  determining whether such graphs are colorable becomes infeasible even for moderate size graphs. 
　in our own work on boolean satisfiability  sat  testing  we also observed a transition phenomenon . our initial interest in the satisfiability problem arose from early reports that many satisfiability problems are easily solvable. for example  goldberg  describes a class of random sat problems that are surprisingly easy for the davis-putnam satisfiability procedure . goldberg's work led to an extensive theoretical exploration of his particular random instance model. he considered cnf formulas. each formula consists of a conjunction of disjunctions  clauses  of literals. each clause is generated by selecting literals with some fixed probability. this leads to clauses of varying length. a rigorous analysis  reported in a series of papers  1; 1; 1; 1   has shown that in this model  the average-case complexity is polynomial for almost all choices of parameter settings. in other words  it is difficult to generate computationally hard problem instances. note that this does not mean that hard instances do not exist; it is simply means that such instances 
　
are very rare. 
　in   we show how by using a different model for generating random formulas  called the fixed-clause-length model  one can easily generate hard problem instances. consider generating a random 1cnf formula. each clause is generated by randomly selecting three variables from among n variables; each of these variables is negated with probability 1. we generate a total of m clauses. we found that the key in generating computationally hard instances is the ratio between m and n. fig. 1 shows the median cost of solving randomly generated instances at different ratios of variables to clauses. we see that the cost peaks at around 1 clauses per variable.1 our experimental data shows that at this point the cost of determining satisfiability grows exponentially with the size of the formulas. 
　fig. 1 gives the fraction of formulas that are unsatisfiable as a function of the ratio of clauses to variables. at low ratios  few clauses compared to the number of variables  almost all instances are satisfiable  i.e.  the unsatisfiable fraction is almost zero . at relatively high ratios of clauses to variables  in a sense too many constraints  almost all randomly generated instances are unsatisfiable. a sudden change occurs around the critical ratio of 1. at this ratio  there is aphase transition  
'for large n  this ratio converges to around 1.  1; 1 . 
from the mostly satisfiable phase to the mostly unsatisfiable phase. from fig. 1  we see that the phase transition region coincides with the area with the hardest problem instances. in this region  the instances are again critically constrained. in the next section  we will take a closer look at what happens inside the phase transition region. 
　the randomly generated critically constrained problem instances have been used extensively in the study and development of algorithms for graph coloring and satisfiability testing. a key question is whether the results obtained for such instances are at all indicative of the behavior of the algorithms on more structured  real-world instances. the results of the recent dimacs challenge on satisfiability testing  suggest that the behavior of algorithms on hard random problems can indeed be representative of the behavior on more structured problems. the dimacs benchmark problem set contained several hard random instances and numerous more structured problems. the satisfiability algorithms fell in two categories: complete systematic procedures  and incomplete stochastic methods. these methods complement each other  in that there are problem classes where the stochastic methods are best  whereas on other problem classes the systematic methods are superior. however  within each category  algorithms that were fastest on the hard random instances usually also performed best on the more structured problems. apparently  the hard random instances do exercise the various time critical parts of the algorithms. therefore  the performance of algorithms on such hard random instances is a reasonably good indicator of the overall performance on a more diverse set of problem instances. 
　aside from being useful as benchmark problems  there is also some indication that critically constrained problems may occur naturally in real-world applications. nemhauser  studied a large airline scheduling problem  involving approximately 1 planes. after a substantial computational effort  his group found a provably optimal solution. given that the original schedule was only approximately optimal  it was expected that an optimal solution would lead to a savings of one or more planes. however  quite surprisingly  the optimal schedule did not save a single plane. the explanation appears to be that the problem had become critically constrained: because of economic factors  the airline had assigned additional routes to planes that were idle during parts of the day in the original schedule. so  external factors can give rise to critically constrained real-world planning and scheduling problems.1 
1 	a closer look at the phase transition 
fig. 1 shows the phase transition for 1s at for several different values of n  the number of variables . note how the threshold function sharpens up for larger values of n. in   we show that the threshold has characteristics typical of phase transitions in the statistical mechanics of disordered materials. 
　physicists have studied phase transition phenomena in great detail because of the many interesting changes in a system's 
　　1 the ratio of constraints to variables will probably differ from the critical ratios found in hard random instances  because of the inherent internal structure of real-world problems. in this abstract  i cannot do 
justice to the large amount of recent work in this area. the reader is encouraged to consult any of the following additional references  1; 1; 1; 1; 1; 1; 1; 1; 1; 1; 1 . 
	selman 	1 
　
macroscopic behavior that occur at phase boundaries. one useful tool for the analysis of phase transition phenomena is called finite-size scaling. this approach is based on rescaling the horizontal axis by a factor that is a function of tv. the function is such that the horizontal axis is stretched out for larger n. so  in effect  rescaling  slows down  the phase transition for higher values of nt and thus gives us a better look inside the transition. fig. 1a shows the result of rescaling the curves from fig. 1. the original curves are rescaled into a single universal curve.1 the fit becomes better for higher values of k  the clause length . see  for example  fig. 1b for 
rescaled 1sat data. 
　from the universal curve  applying the scaling function backwards  the actual transition curve for each value of n can be derived. this approach also localizes the 1%-satisfiablepoint for any value of n  which allows us to generate the hardest possible sat instances. 
　finite-size scaling can also be used to study properties other than satisfiability in the critical region. see f 1  for a rescaling of computational cost curves  and  for a rescaling of the prime implicate function. 
1 	stochastic search 
in section 1  we discussed how we can generate satisfiability problems that are hard for the davis-putnam procedure. a natural question to consider is whether there are other methods that are better at solving such instances. recent experimental work has shown that from among the systematic search procedures  the basic davis-putnam procedure is in fact the most effective  1; 1; 1; 1; 1 . in   we show however that a stochastic method can outperform the davis-putnam procedure. our method  called gsat  is based on a randomized local search strategy  1; 
1 . 
　the original impetus for trying a local search method on satisfiability problems was the successful application of such methods for finding solutions to large n-queens problems  first using a connectionist system   and then using greedy local search . we originally assumed that simply indicated that n-queens was an easy problem  and felt that such techniques would fail in practice for sat. in particular  it would seem that local search methods would easily get stuck in local minima  with a few clauses remaining unsatisfied. our gsat experiments have shown  however  that certain local search strategies often do reach global minima. 
　the basic gsat procedure  fig. 1  starts with a randomly generated truth assignment. it then changes  'flips'  the assignment of the variable that leads to the greatest decrease in the total number of unsatisfied clauses. such flips are repeated until either a satisfying assignment is found or a pre-set maximum number of flips  max-flips  is reached. this process is repeated as needed  up to a maximum of max-tries times. 
　in   we show that gsat substantially outperforms backtracking search procedures  such as the davis-putnam procedure  on various classes of formulas  including randomly gen-
   1 we do not yet have a closed form expression for the universal curve  but the function e-1   where a is the rescaled m/n ratio  
is a good approximation . 
1 	invited speakers 
　

the parameter p controls the amount of noise. in table 1  from 
  we compare the random walk strategy on hard random 1cnf formulas with basic gsat  simulated annealing   and a random noise strategy. we see that the biased walk strategy significantly outperforms the other methods. it enables us to solve hard random instances with over 1 variables. in contrast  the current best systematic search methods can only handle instances with up to around 1 variables. in  1; 1   we show how stochastic search can also be effective on highly structured instances  such as encodings of circuit design problems  steiner tree problems  and problems in finite algebra. some of our problem encodings contain over 1 variables and around 1 clauses. this is a good indication of the significant progress that has been made in recent years: up to around 1  experimental work on satisfiability testing was limited to instances with less than a hundred variables and at most one or two hundred clauses. for further experiments with  and comparisons to gsat and gsat-style procedures  see also  1; 1; 1; 1; 1; 1; 1; 1; 1 . 
1 applications of stochastic methods in reasoning and search 
since gsat does not systematically explore the space of all truth assignments  if no satisfying assignment is found  it does not mean that no such assignment exists. in other words  gsat  like other stochastic methods  is inherently incomplete . as a consequence  such methods are suitable for model finding but not for theorem proving  which requires showing inconsistency.1 the success of stochastic search methods suggests  therefore  that it may be useful to encode ai tasks in terms of model-finding  rather than the more usual formulations based on theorem proving. in   we show how planning can be encoded as a model-finding task. certain forms of abduction and default reasoning can also be formulated in terms of model-finding  1; 1 . when no direct model-based encoding exists  one can consider approximate encodings by adapting methods developed for theory compilation . finally  model-finding procedures can also be used to find representative  or characteristic  models of a knowledge base. such models can be used to answer certain classes of queries in a highly efficient manner  1; 1 . 
1 	conclusions 
we have discussed the significant progress that has been made recently in our understanding of the nature of computationally hard problems. the hardest problem instances occur at certain critical ratios of constraints to variables. at such ratios  we observe dramatic changes in the problem distributions. this phenomenon can be analyzed with tools from statistical physics. we also discussed new stochastic methods for solving hard problem instances. these methods can substantially outperform systematic methods. finally  we described how such stochastic methods suggest new ways of dealing with computational challenges in reasoning and search in ai. 
