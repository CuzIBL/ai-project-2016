 
we equate nonlinear dimensionality reduction 
 nldr  to graph embedding with side information about the vertices  and derive a solution to either problem in the form of a kernel-based mixture of affine maps from the ambient space to the target space. unlike most spectral nldr methods  the central eigenproblem can be made relatively small  and the result is a continuous mapping defined over the entire space  not just the datapoints. a demonstration is made to visualizing the distribution of word usages  as a proxy to word meanings  in a sample of the machine learning literature. 
1 	background: graph embcddings 
consider a connected graph with weighted undirected edges specified by edge matrix w. let  be the positive edge weight between connected vertices i and j zero otherwise. let d = diag wl  be a diagonal matrix where  the cumulative edge weights into vertex /. the following points are well known or easily derived in spectral graph theory  fiedler  1; chung  1 : 
1. the generalized eigenvalue decomposition  evd  
	w v = d v a 	 1  
 and eigenvalues 
1. premultiplying equation  1  by d 1 makes the generalized eigenproblem into a stochastic eigenproblem 
 1  
where is a stochastic transition matrix having nonnegative rows that sum to one. the largest eigenvalue of equation  1  is therefore stochastic and its paired eigenvector is uniform 
1. expanding and collecting terms in wij reveals the geometric meaning of the eigenvalues: 
 1  
 the d eigenvectors paired to eigenvalues through  therefore give an embedding of the vertices in with minimal distortion vis-a-vis the weights  in the sense that a larger stipulates a shorter embedding distance. formally  the embedding 
 1  
 1  
 = i sets the scale of the embedding and causes vertices of high cumulative weight to be embedded nearer to the origin. 
1. y can be rigidly rotated in without changing its distortion. the distortion measure is also invariant to rigid translations  but the eigenproblem is not  thus there is an unwanted degree of freedom  dof  in the solution. due to stochasticity  this dof is isolated in the uniform eigenvector     which is suppressed from the embedding 
without error  because . a d d i n g t o y rigidly translates the embedding by 
1. premultiplying by   and rearranging equates equation 1 to the evd of the graph laplacian d - w: 
 1  
1. premultiplying by connects equation 1 to the  symmetric  kvd of the normalized laplacian: 
 1  
with 
in summary: equation 1 gives an optimal embedding of a graph in rc/ via eigenvectors  eigenvalue  -iis stochastic and the corresponding eigenvector    is uniform; this is an important property of the evd solution because it isolates the problem's unwanted translational degree of freedom in a single eigenvector  leaving the remaining eigenvectors unpolluted. 
learning 	1 　many embedding algorithms can be derived from this analysis  including the fiedler vector  fiedler  1   locally linear embeddings  lle   roweis and saul  1   and laplacian eigenmaps  belkin and niyogi  1 . for example  direct solution of equation 1 gives the laplacian eigenmap; as a historical note  the symmetrized formulation was proposed by fiedler in the 1s and has been used for heuristic graph layout since the 1s  mohar  1 . 
1 	transformational embeddings 
now consider a more general problem: we are given some 
information about the vertices in a matrix z =  z          zn  c rjxn  whose columns are generated by applying a vectorvalued function z -  -  z ♀ rd to each vertex of the graph. we seek a linear operator which transforms z to the optimal graph embedding: g z  -  y. we will call this the  transformational embedding   to distinguish it from the  direct embedding  discussed above. 
　a natural candidate for the algebraic statement of the transformational embedding problem is the generalized evd 
	 zwz t  v =  zdzt va  	 1  
because setting y = v t z makes this equivalent to the original direct embedding problem. again  there is an equivalent symmetric eigenproblem: make cholesky1 decomposition rtr  - zdz t into upper-triangular r g rdxd and let 
b=  r- ' zwz 1 it 1   eruxd. 
then  1  bv' = v'a 
with  1  		 in 
this gives an embedding   and a computational advantage: is a short m a t r i x   the original n x n eigenproblem can be reduced to a very small d xd problem  and the matrix multiplications also scale as 1 d1n  rather than 1 n1   due to the sparsity of w and d. 
1 correcting problematic eigenstructure 
it is generally the case that -there is no linear combination of the rows of z giving y  so the desired linear mapping g z  -* y does not exist. equations 1 give the optimal least-squares approximation 
this approximation can have a serious flaw: i then the first eigenvector vi is not uniform; it cannot be discarded as the unwanted translational dof. worse  all the other eigenvectors will be variously contaminated by the unwanted dof  resulting in an embedding polluted with artifacts. for this reason  we call direct solution of equation 1 a raw approximation. 
　our options for remedy are limited to those that modify the row-space of z to reintroduce the uniform eigenvector. for reasons that will become obvious below  we will restrict ourselves to operations that can be applied to any column of z without knowing any other column. 
the simplest such operation is to append a uniform row to 
z  so that . this makes the relation between z 
1  any gram-like factorization will work. for example  given evd 
. the cholesky is especially attrac-
tive for its numerical stability  sparsity  and easy invertibility. 
and y affine and guarantees that is uniform  but it can also force the eigenvectors to model additional variance that is not part of the problem. 
　working backward from the desiderata that the leading column of v z should be uniform  let such 
that zk is a modified representation of the vertices with values of z   reweighted on a per-vertex basis: 
clearly  has a uniform first column  since each row is divided by its first element. 
it follows immediately that the related eigenproblem 
		 1  
is stochastic  and  is an embedding 
with the unwanted translational degree of freedom totally removed. note that the raw and stochastic approximations are orthogonal  under metric  is a diagonal matrix; the other methods are not. 
it should be noted that-when scaled to have equal norm 
- none of these approximations has uniformly superior distortion scores; but in monte carlo trials with random graphs  wc find a clear ordering from lowest to highest distortion: reweighted  affine  stochastic  raw  see figure 1 . 

figure 1: comparison of methods for modifying the rowspace of z. the graph shows distortion from the optimal embedding  averaged over 1 trials with 1-node matrices having random edge weights and random z e  r1. 
the raw approximation is suboptimal because information about the d-dimensional embedding is spread over d + 1 eigenvectors  no subset of which is optimal. the stochastic approximation is also suboptimal-it optimizes a different measure implied by equation 1. in practice  when computing embeddings of graphs whose embedding structure is known a priori  wc find that the reweighted and stochastic approximations give results that are clearly very similar  and superior to the other approximations. 
1 	learning 　the need for any such correction stems from the fact that-the literatures of spectral graph theory and nldr notwithstanding-equation 1 is not a completely correct statement of the embedding problem. we will show in a forthcoming paper that  as a statement of the embedding problem  equation 1 is both algebraically underconstraincd and numerically ill-conditioned. in particular  point #1 is not strictly true: the stochastic eigenvalue is not always 
paired to a uniform eigenvector. this leads to pathologies that can ruin the embedding  whether obtained from the basic or derived formulations. nldr algorithms that can be derived from equation 1  e.g.   roweis and saul  1; belkin and niyogi  1; teh and roweis  1   do not remediate the problem. 
　a forthcoming paper makes a full analysis of these issues  identifies the correct problem statements for both equations 1 & 1  and gives closed-form optimal solutions to both problems. the approximation methods discussed in this section are still useful in that they are faster and give reasonably high-quality embeddings. for the nldr method and datasets considered below  the result of the reweighted approximation is almost numerically indistinguishable from the optimal embedding  and requires substantially less calculation. the reweighting method can also be justified as a pade approximation of the optimal solution. 
1 	nonlinear dimensionality reduction 
let .     be a set of points sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. a reduced-dimension embedding y = is a set of low-dimensional 
points with the same local neighborhood structure. we desire instead a mapping which will generalize the correspondence to the whole continuum  with reasonable interpolation and extrapolation to be expected in the neighborhood of the data. spectral methods for nldr typically require solution of many and/or very large eigenvalue or generalized eigenvalue problems  kruskal and wish  1; kambhatla and leen  1; tcnenbaum et al.  1; roweis and saul  1; belkin and niyogi  1   and with the exception of  teh and roweis  1; brand  1  offer embeddings of points rather than mappings between spaces. 
　here we show how a leverage the transformational embedding of section 1 into a continuous nldr algorithm  specifically a kernel-based mixture of affine maps from the ambient space to the target space. to do so  we must show how the edge weight matrix w and vertex matrix z are specified. let iff x; and xj satisfy some locality criterion  otherwise as stated above  an 
embedding y of x should satisfy 

where larger wy penalize large distances between y - and y . 
　how should wij be computed  / is a measure of similarity: the graph-theoretic literature usually takes /     = 1. while nldr methods typically take  to be a gaussian kernel  on analogy to heat diffusion models  belkin and niyogi  1 . the uninformative setting wij = 1 is only usable when there is a very large number of points  and edges   so that connectivity information alone suffices to determine metric properties of the embedding. the gaussian setting has a complementary weakness: it can be very sensitive to small variations in distance to neighbors  that may be introduced by the curvature of the data manifold or measurement noise in the ambient space . / should be monotonically decreasing  relatively insensitive to noise  d/ should be small   and it should lead to exact reconstructions of data sampled from manifolds that are already 
flat. straightforward calculus shows that equation 1 has the 
desired minimum when   or more generally  the multiplicative inverse of whatever distance measure is appropriate in the ambient space1.  by contrast  the lle weightings are not correlated with distances.  to make the problem scale invariant  we scale w such that its largest nonzero off-diagonal value is 1  consequently everywhere / is computed . 
　let us now situate some gaussian kernels p* x  =  on the manifold. in this paper  we will take a random subset of data points as kernel centers  and set all ; these kernels are radial basis functions. let vector 
 1  
be the kth local homogeneous coordinate of xi scaled by the posterior of the kth kernel. k; is an optional local dimensionality-reducing linear projection. let representation vector 
		 1  
be the vertical concatenation of all such local coordinate vectors. collect all such column vectors into a basis matrix 

　to summarize thus far  our goal now is to find a linear transform of the basis  kernel-weighted coor-
dinates  that is maximally consistent with our local distance constraints  specifically 
		 1  
this is isomorphic to the graph embeddings of section 1; the methods developed there apply directly to w and z. the continuous mapping from ambient to embedding space immediatly follows from the continuity and smoothness of z{- : 

where the evd determines the transformation g = 
 of the continuous kernel representation defined over the entire ambient space: 
		 1  
1
   proof: consider three p o i n t s o n a 1d manifold. what similarity measure causes the distortion to have a global minimum at without loss of generality  we fix the global location and scale of the embedding by fixing the endpoints: 
solving for the unique zero of the distortion's first derivative  we obtain the optimum at y1 = w1 w1 -i- w1 . since this is a harmonic relation  the unique continuous satisficing measure is 
. this sets 	and 	; some 
simple algebra confirms that indeed .at the optimum. the induction to multiple points in multiple dimensions is direct. 

learning 	1 

　as a matter of numerical prudence  we recommend using the reweighted approximation: 
		 1  
at first blush  it would seem that reweighting should not be necessary: by construction  - and the denominator-should be uniform at the datapoints. however  as mentioned above  even when the algebra predicts this structure  numerical eigensolvers may not find it. 
　　to obtain an approximate inverse mapping  we map the means and covariances of each kernel 	into the target 
1 illustrative example 
　we will use a variant of the  swiss roll   a standard test manifold in the nldr community  to illustrate the arguments and methods developed in this paper. we sampled a twisted version of the manifold regularly on a 1 x 1 grid and added a small amount of gaussian isotropic noise. figure 1 shows the ideal r1 parameterization and two views of the ambient r1 embedding. points are shown joined into a line to aid 
figure 1: the swiss roll. visual interpretation of the embeddings. all experiments in this 
section use a w matrix that was generated using the 1 nearest neighbors to each point and the inverse distance function. 
　the laplacian eigenmap embedding  figure 1  shows the embedding specified by the w matrix. 	note 
that it exhibits some folding at the corners and top and bottom edges  due partly to problems with the uniform eigenvector and exacerbated by the fact that spectral embeddings tend to compress near the boundaries. the laplacian 
eigenmap requires solution figure 1: laplacian eigenmap of a large 1 x 1 eigen- embedding  problem  and offers no mapping off the points. kernel eigenmaps will be approximations to this embedding. 
　we now show some kernel eigenmaps computed using the transformational embedding of section 1. all embedding methods are given the same inputs. 
　figure 1 shows a raw kernel eigenmap embedding computed using a basis  z matrix  created from 1 gaussian unit-a kernels placed on random points. this required solving a much more manageable 1 x 1 eigenproblem. 1 trials were performed with different sets of randomly placed kernels. in all 
figure 1: kernel eigenmap trials  the reweighted and embedding  raw result. stochastic maps gave the 
the raw and affine maps exhibited substantial folding at the edges and corners of the embedding. 
	figure 	1 	shows 	a 
reweighted kernel eigenmap computed from the same w and z as figures 1 & 1. the result is smoother and actually exhibits less folding than the original laplacian eigenmap. 
　the problem can be regularized by putting positive mass on the diagonal of w  e.g.  w + w +1   thereby making the recovered kernel eigenmap more isometric  bottom figure 1 . this regularization is appropriate when it is believed that all neighborhoods are roughly the same size. 
the recently proposed 
locality preserving projec-
tion  lpp   he and niyogi  1   is essentially the raw approximation  direct solution of equation 1  with figure 1: kernel eigenmap embedding  reweighted and regularized results.  and z = x  thereby giving a linear pro-
jection from the ambient space to the target space that best preserves local relationships. 
     lpp is admirably simple  but it can be shown that the affine approximation from section 1 will always have less distortion. lpp can also suffer from loss of the unifigure 1: lpp embedding and form eigenvector. figure 1 our affine upgrade. show embeddings of the swiss roll produced by lpp and by an affine modification of it that is equivalent to our method with a trivial single uniform-

1 	learning 

density kernel. upgrading lpp to an affinc projection captures more of the data's structure. even so  there is no affine  view  of this manifold that avoids folding. 
1 visualizing word usages 
in statistical analyses of natural language  similar usage patterns for two words are taken to indicate that they have similar meanings or strongly related meanings. latent semantic analysis  lsa  is a linear dimensionality reduction of a termdocument co-occurence matrix. the principal components of this matrix give an embedding in which similarly used words are similarly located. literally  co-location is a proxy for collocation  the propensity of words to be used together  and synonymy. we may expect that the kernel eigenmap offers a more powerful nonlinear analysis: 
　the nips 1 corpus1 features a matrix counting occurrences of 1+ words in 1+ documents. we modeled the first 1 words and the last 1 documents in the matrix. this roughly corresponds to one year's papers  a reasonable  snapshot  of the ever-changing terminology of the field. we stemmed the words and combined counts for the same roots  then determined distance between two word roots as the cosines of the angles between their log-domaintransformed occurrence vectors  x   -* log1 l + xij  . the w matrix was generated by adding an edge from each word to its 1 closest neighbors in cosine-space. the representation z was made using 1 random words as kernel centers. figure 1 discusses the resulting 1d embedding  in which technical terms arc clearly grouped by field and many of the more common english words arc tightly clustered by common semantics. the first two lsa dimensions  also shown in figure 1  of the same data arc reveal significantly less semantic structure. 
1 	discussion 
the kernel eigenmap generates continuous nonlinear mapping functions for dimensionality reduction and manifold reconstruction. suitable choices of kernels can reproduce the behavior of several other nldr methods. one could put a kernel at every local group of points  perform local dimensionality reduction  e.g.  a pc a  at each kernel  and thereby obtain from equations 1 and 1 an nldr algorithm much like charting  brand. 1  or automatic alignment  teh and roweis  1 . or  as in the demonstrations above  the kernel eigenmap can simultaneously determine the local dimensionality reductions and their global merger. 
　the kernel eigenmap typically substitutes a small dense cvd for the the large sparse lvd of graph embedding problems. in the sparse case  a specialized power method can compute the desired eigenvectors in significantly less than the 1 n1  time required for a full evd. in the kernel setting  similar efficiencies apply because both w and z are typically sparse  allowing fast construction of the reduced evd problem zwz 1 ; this too is amenable to fast power methods. of course  the most important efficiency of the kernel method is its ability to embed new points quickly via the function 
1 courtesy s. roweis  available from the u. toronto website. 
g x -there is no need to compute a new global embedding or revise the evd. 
　the reweighting scheme  although theoretically mooted by our subsequent discovery of a better problem formulation and closed-form solution  is still practically viable as a fast approximation for large problems  and as a post-conditioning step for unavoidable numerical error of any nldr algorithm based on eigenvalue decompositions. 
　in this paper we have used random kernels. there are numerous avenues to discovering stronger methods by investigating placement and tuning of the kernels  stability of the embedding and its topological structure  and sample complexity. in short  all the issues that proved fertile ground for research in classification and regression can be studied anew in the context of estimating the geometry and topology of manifolds. 
