 
　　　designed for an operational prospect  the charade system automatically learns consistent rule systems from a description language  a set of axioms reflecting the language semantics and a set of examples. the technique advocated below is based on a  generate and test  mechanism where the description space is explored from the more general to the more specific descriptions. rules and properties to be obtained are translated into exploration procedure constraints thanks to formalization of the learning set with two boolean lattices.the underlying theoretical framework allows to both justify the heuristics conventionnaly used similarity based-learning and to introduce global properties to be satisfied by a rule system during its construction. 
1. introduction 
     in a pragmatic prospect  that of knowledge base construction and maintenance  the global properties of the rule system play a central part. they will assist the specialists at the time of transfer of expertise to prevent errors and possibly correct them. yet there is a major gap between individual rule juxtaposition and a complete and efficient usable rule system. charade proposes to bridge that gap. it has been designed to detect logical or statistical regularities existing in a set of examples and generate production rule systems which  reflecting such regularities  can operate on commercial inference engines. the method presented is related to the  generate and test  techniques developed by buchanan  1  and michalski  1   the description space being explored from the more general to the more specific descriptions. however  instead of considering only one ordering relation  as it is usually the case in learning  we shall distinguish two ordering relations  the first being linked to the inclusions of subsets of the learning set  the other to the logical implications on the description space. in fact  these are two aspects of the generalisation notion  and even if they are complementary and indissociably linked together  they cannot be reduced to one another. each can be formalised with a boolean lattice. taking simultaneously into account these two aspects allows to translate the properties of rules and rule systems to be constructed into constraints for the exploration procedure  which considerably limits the number of vertices to be explored. such a formalisation of learning with two boolean lattices establishes a theoretical base for the learning heuristics used in classical systems and still allows the introduction of new properties. amongst such properties  it is possible to translate the rule system semantic characteristics  as structuration and nature of rules - logical or approximate - as well as the relative relevance of descriptors. last  completeness  consistency and minimality of the rule systems obtained can be proved. after a precise demonstration of all that differenciates a rule system from a conglomerate of individual rules  we shall study the detail of techniques used to generate rule systems and see how the rule system properties can be translated into this formalism. 
1. rule systems 
     modularity of production rule systems must not be deceptive. it is not enough for the rules  individually to make sense  they must also  as a whole  meet operational criteria as lack of redundancy and cycle  consistency  completeness  etc. to insure that such criteria are verified knowledge acquisition assistance tools are built and it is to be able to rid from such verifications that we hope to create systems to learn rules from examples. however  in most cases rule learning systems are limited to the acquisition of concept description from examples and counter-examples. to be convinced of it let us study classical similarity based learning system procedure: being a set of examples  e cl ={el e1 ... en  and a set of counter-examples  ce e1  = {ce 1  ce1 ...  cep  of a concept cl a generalization is looked for: gl of el e1 ... en discriminating cel ce1 .. . cep . when gl has been found  the rule gl -  cl is generated. 
     in spite of the difficulty to introduce a disjunction in the gl generalization  the same operation must be done for all the concepts to be learnt. moreover  once defined concepts cl and c1  to define a concept c1 so that cl&c1 -  c1   descriptors cl and c1 must be introduced in the example descriptions  which means that rule chainings can be learnt only if they are predetermined and fixed. thus  rules are isolated from one another during acquisition. 
　　reversely  charade does not refer to the notion of example or counter-example for separated concepts; it considers globally the set of examples  each of them being described by a logical formula  and explores the space of the elementary descriptors present in the examples conjunctions. be d1 &d1&-. &d n one of those  charade looks for descriptors f1  f1     fp  correlated to the set of examples covered by d 1 &d 1 &. -&dn  then it generates rule d 1 &d 1 &...&d n - f q   ...  fm  eliminating amongst f     f1  ...  fp descriptors fi  already derived from d1&d1&. -&dn through the rule system. 
1. detection of regularities 
　　it will be considered that from now on  an example is described by a descriptor conjunction. with an example e  we shall call d e  description of e: d e  = d1&d1&.. .& dn 
     each descriptor di will be originally assumed to be either an atomic proposition or the negation of an atomic proposition. the latter restriction may be removed  cf.  1   and it may be then possible to process any form of 
  attribute  selectorxvalue   representation  integrating the semantical properties of attributes and selectors in the learning process. 
　　with the restrictive assumptions made above  it is now easy to carry out the elementary operation in the field of learning  that is generalization: e1 and e1 are two examples 
	ganascia 	1 
with d e1  = a1&a1&...& an and d e1  = b1&b1&...& bm. let us call gen e1  e1  the least generalization of e1 and e1. if semantic relations between atomic descriptors present in these descriptions are not known  gen e1  e1  consists of the conjunction of common descriptors of e1 and e1.more formally gen e1 e1  = &d such that de 
example: let us assume that: 
d e|  = blue eyes & tall &fair hair  d e1  = blue eyes & short & red hair 
it is then easy to calculate gen e 1   e1  = blue eyes 
     if now the learning set is represented by a function tr associating to each descriptor d the set of examples containing d in their description  to say that blue eyes is a generalisation of 
e 1 and e1 is easily translated  it means thats {e 1  e1}  blue eyes . the inclusion relation introduced here translates the subsumption of a set of examples under a concept  this can be expressed by a function sub : sub e  = {d such that 
ectr d }. the two functions tr and sub allow to go from the description space to the learning set and from the learning set to the description space. now  combining the two functions one can associate to a description d the set of descriptors 
tr sub d   =  such 	a	s	a	d 
thus  generate directly a production rule. the method that we advocate is based on this principle. as it does not refer to a preferential set of examples  the notion of example and counter-example become void of sense  the category of an example is only one descriptor among others. moreover  this method generates all the logical correlations and thus allows to detect rule chainings. however  among those correlations  some are useless  others are accidental  the rest of this paper deals with the description of an exploration procedure limited to descriptions d likely to generate an interesting rule. 
     as the following statements might be very abstract  we shall illustrate them with an example drawn from  1  and modified to meet the requirements of this presentation. this example only intends to give an intuitive content to our presentation; it consists of the description of 1 individuals described by 1 attributes each: size  hair colour  eye colour  complexion and class: 
el=  size=sh tt & haii*fair & eyes=bto c1= sizc= l & hair=fair & eycs=brown & cx mplexion matt & class -  e1=  size=tau & hair*red & eyes m 
 c 1= si1c=short & hair=brown & cyes=bluc & complcxion=matt & class=-  c1=  size = tall & hair*brown & eycs=bhie & complcxion=pale & class -  e1= sizes1au & haii*fair & eyes=blue & c  e1= size=tall & haii brown & eye  e1= size=short & haii fair & cycs=brown & complcxion=maa & e1=  size=tall & hair=fair &   
     the learning set can easily be represented by function tr: tr  size=tall    e1 c1 c1 e1 c1 c1   tr  size=short  = el c1 c1   tr  hair=brown = e1 e1 e1   tr  hair=fair mel e1 e1 e1 e1   etc. 
     thanks to this representation the set of training instances covered by a conjunction of descriptors  for instance 
 size=short & hain=fair   is automatically given by the intersection of the set of training instances covered by each term of the conjunction. also  for disjunction  the union of sets are sufficient. so we obtain tr  size=short & hair=fair = el e1  and tr  size=short v hair=fair    cl c1 e1 e1 e1 c1  
     once function tr has been defined  it is very simple to obtain function sub: sub e  = {d such that 
in our example it becomes: sub  el e1 e1  ={ size short }  
sub  e1e1  ={ hair=brown   class - } etc... 
　　in accordance with what we had announced above  we should generate  among others the following rules: 
1 	knowledge acquisition 
sizeshort -  size=short  hair=brown - hair=brown & classe=  and hair-brown & eyes=blue -  hair=brown & eyes blue & class=-. 
     however such rules comprise many redundancies which must be eliminated. to do so the boolean lattice structure of the set of subsets of the descriptors set is used and function imp is created to describe the set of non trivial implications derived from a descriptor: 
imp d  -sub tr d   -
we have then imp  hair=brown & eyes=biue   = sup tr  hairssbrown   * { hair=brown    class=- } and  sup tr  cycs bluc     { eyes=blue }. 
　　we can also obtain: imp  hair=brown  ={ class=- } and imp  eyes=blue  -1. 
　　the mathematical properties of boolean lattices allow to simplify this formula into : 
imp d  - sub tr d   -
     therefore we could construct rules on the pattern s imp s . however  redundancies would remain. to be convinced of it let us note that 
imp  hair=fair & eyes-blue  =   complexion=pale   class=+ }  and imp  size=short & complexion=pale  = { hair=fair   eyes blue   class=+ } so we ought to have simultaneously the two following rules:  hair=fair & eyes=blue  -   complexion=pale & class*+  
 size=short & complexion pale  -   hain=fair & eyes=blue & class=+  
     now this second rule is obviously redundant  as when it is trigered the first one must be trigered too. such redundancies come from the implication transitivity. we free ourselves from it with a transformation x which demonstrably maintains all the properties of the rule system  cf.  1  . in our example transformation x would tranform the second rule into: 
 size=short & complexion=fair  -  hair=fair & eyes*blue  
     after the transformation it is possible to generate  for each description  rules of the type s -  x imp s  . the rule system so established reflects all the logical relations between descriptors. in that way it is complete. moreover  one can prove that it is minimal  cf.  1  . each rule plays a part and  if eliminated  it diminishes the deductive power of the rule system. for instance  a few rules obtained with the last example are shown below : 
if hair = red then class = +  eyes = blue  size = tall. 
if hair = fair  eyes = blue then class = +  complexion = pale. 
if size = short  eyes * brown then complexion = matt  hair = fair. 
if size = short  complexion = pale then hair = fair  eyes = blue. 
if eyes = blue  size = tall then complexion = pale. 
if hair = brown then class = -. 
if eyes = brown then class = -. 
if complexion - matt then class = -. 
     the function x o imp  that we shall name ru  is able to constitue a set of rules  this remains to be built and so for all the description space. this is the subject of the next sub-section. 
1. rule construction 
     the first way to build ru would be to explore completely all the description space. yet a procedure based on this principle would be cumbersome  as the calculation time would be proportional to the number of possible descriptions. but as descriptions for which function ru is not nil are the only ones of interest as being the only ones to lead to rules  it is sufficient to determine a-priori nullity criteria for that function to curb the exploration procedure. 
     first of all  one can note that it is enough to limit oneself to the conjunctions of descriptors  in fact:  if and 

only if 	and 	as 	is equivalent 
to and now as all the rules are generated  if the rule d1vd1- d was to be present  the rules and 
would be present too and this would make the rule useless. the study of all descriptions is then the study of the descriptors conjunctions. as the set of descriptors conjunctions is isomorphic to the set of parts of the set of descriptors  it can be represented by a boolean lattice which properties are used to prove theorems predicting the uselessness of a description d  that is the nullity of ru d . 
     more precisely  the description space is explored from the more general to the more specific. thus  if there are three descriptors  d1  d1 and d1  the description space will be explored in the following order : {1  d1  d1  d1  d1&d1  d 1 &d 1   d 1 &d 1   d1&d1&d1}. to avoid examining all the conjunctions of descriptors  the useless descriptions are characterized : we shall say that a description d is useless if ru d  is nil and if all descriptions d' more specific than d verify also if a description verifies such a property neither this description  nor the more specific descriptions derived from it will be explored; it will then be possible to considerably reduce the space to study. now  one can formally demonstrate that the properties of rules and rule systems allow to characterize the useless descriptions. to make the presentation more simple  let us define a predicate us to characterize the uselessness of a description: us d   d is useless. as regards the proof of these properties refer to  1 . 
1. properties of rules 
let us give first  the translation of some rule properties: theorem 1: for all descriptions d1 and d1. 
 and  d &d1   us d &d1  
     intuitively  this means that when the descriptor d is logically derived from description d1 that is when then it is not necessary to study the description d1 = d &d&.. as 

theorem 1: for all descriptions d1   d1 and d1  
tr d1&d1  tr d1&d1  us d1&d1&d1  
     in other words  this theorem stipulates that if the set of examples covered by the description d&d1 is included in the set of examples covered by d&d1 then it is not necessary to study the descriptions d&d 1 &d 1 &.. as they will not tell anything new. 
1. rules systems 
     these theorems are fundamental. they insure the technique feasability  but they are not alone. other theorems are related to the properties of the rule system. here  as an example  are a few of the characteristic properties of a rule system as they may be introduced in charade to define the exploration procedure parameters: 
     - goal of the rule system: disease diagnosis  determination of remedy etc. 
     - structure of the rule system: rules that go from the symptom to the disease  from the disease to the type of problem  the remedy and the potential danger. 
   - minimum number of examples to be covered by a rule premise. thanks to this coefficient it is possible to control the ill-effects due to noise: a rule that would be verified by a single example could not be generated. thus  a parameter v is introduced so that any description d covering a number of examples smaller than v  that is such as be a-priori nil for ru  and so no rule could be generated. 
     - maximum number of descriptors present in a rule premise. 
     - descriptor relevance: this characteristic allows  heuristically  to eliminate a-priori rules which do not make sense. in fact  there are descriptors which  per se  have no meaning but which  in conjunction with symptom descriptors  limit their field. thus  considering an expert system in agronomy  the optimum temperature or the level of humidity cannot lead by themselves to a conclusion. this would be absurd. nevertheless  in conjunction with other descriptors  such as the colour of spots on the leaves  they can be favorably introduced in the rules. 
     - example coverage: this is the generalisation of a heuristic used by michalski  1 . it consists in stopping the exploration as soon as a number n of rules cover the examples and conclude as to the final condition. 
     - class partition: in the same spirit  it is possible to determine the minimal proportion of examples in the learning set covered by a class d and a rule  this allows to introduce disjunctions in the rules and at the same time to avoid having too specific rules covering one example only. 
     - etc. 
     all these properties give rise to a set of parameters which characterize the rule system taken as a whole and give a formal base to the various deletions made in the process of exploring the description space. actually  one should note that all the heuristics are parameterized by the user and that the adjusment of such parameters depends as much on the properties of the system of rules to be generated as on those of the learning set. thus  if a classification system must be created and that the number of examples is large  a high coefficient v will be introduced whereas if there is only one proptotypical example in each class  we shall necessarily have v=1. 
1. conclusion 
     charade has been implemented on macintosh plus. programmed in le lisp  it was tested in several fields  from tomato pathology to first call at bridge  classification of archaeological objects and galaxy recognition. running time is reasonable  approximately 1 minutes in the interpreted version for 1 examples and 1 boolean descriptors  and the space explored is sufficiently limited to leave room for expectations. moreover  as regards classification  the system returns the examples that it has not been able to classify  which allows a feed back on the description language. also  the rule construction technique allows to take into account noise in data to generate approximate rules using certainty factors  cf.  1  . 
