 
　　　word-position-independent and word-position dependent n-gram probabilities were estimated from a large english language corpus. a textrecognition problem was simulated  and using the estimated n-gram probabilities  four experiments were conducted by the following methods of classification: without contextual information  raviv's recursive bayes algorithm  the modified viterbi algorithm  and a proposed heuristic approximation to raviv's algorithm. based on the estimates of the probabilities of misclassifixation observed in the four experiments  the above methods are compared. the heuristic approximation of raviv's algorithm performed just as well as raviv's and required far less computation. 
	1. 	introduction 
　　　some of the contextual text-recognition a l gorithms invoke the assumption that english language is a markov source of order r   l   l   . having 
made these assumptions these algorithms use either sequential or nonsequential decision theory. 
this paper describes an ex-
perimental investigation of text-recognition by four methods: without contextual information  raviv's recursive bayes algorithm   1     the modified viterbi algorithm 1    and a proposed heuristic approximation to raviv's algorithm. the objective of the experiments was to compare the above methods. 
　　　it was assumed that english language is a markov source of order r=l . to conduct the experiments  it was necessary that unigram  single-letter  and first-order transition probabilities be estimated from the english language. the probabilities are collectively referred to as the n-gram probabilities. 
1. estimation of n-gram probabilities from the english language 
　　　a multi-authored english language corpus written on ten different subject matters  comprising over half-a-million words was compiled. probability estimates were obtained from this corpus for the following: 
 i  word-position-independent unigram probabilities p c1  . 
  i i   	word-position-independent transition probabilities 	pccjc.  . 
  i i i   	word-position-dependent unigram probab i l i t i e s 	p  c.  	for position 	m m 	i 
 l m 1  . 
 iv  word-position-dependent transition probabilities p  c |c.  for 1 m 1 . 
	m 	i 	j 
natural 	lan tia e 1 
p  c ic   is the probability of occurrence of character c. in position  m+1  of a word given that character c occurs in position m. 
　　　since the corpus was large  it was assumed that pcc     p c1 |cj   v v '   m   c i l c j   e 1 t l   
mated from the corpus  reflect closely the true n-gram probabilities of the english language for the 1-character set  tf and the letters a to z. each experiment described in section 1 was repeated twice: once  by using the word-positionindependent  wpi  n-gram probabilities and a second time  by using word-position-dependent  wpd  n-gram probabilities. 
1  	simulation of the text-recognition problem 
　　　images on 1 x 1 arrays of mixed-font machine printed letters were taken from ryan's* data set. the images were size-normalized and a 1-dimensional feature-vector x=  x..  x   . . .  x   was extracted from each image . 
　　　the total number of patterns available was 1. 	these were divided into two sets: 	a training set of 1 patterns and a testing set of 1 patterns. 	the classifier was trained  and 	p x|c   -the probability distribution of 	x 
conditioned on 	c -was estimated  
k 
　　　a passage  called the old passage  was compiled from ten segments  such that each segment was a r b i t r a r i l y chosen from one of the ten subject matters in the corpus  described in section 1 . another passage  called the new passage  was similarly compiled but from sources outside the corpus   the old passage consisted of 1 words and the new passage of 1 words. these two passages were used as text to be recognized in the experiments described in section 1. 
1. the text-recognition problem and the experiments conducted 
　　　patterns of the passages were presented sequentially to the classifier. it was assumed that a l l blanks were perfectly recognizable. 
let 
x * x q   x＼1 x 1 x n   x n + 1 
be the pattern-sequence input to the classifier 
t this research was supported by the national research council of canada under grant number nrc-a1. 
* pattern recognition data base no. 1.1 a  ieee computer society 
 1: 	scmnphal 


