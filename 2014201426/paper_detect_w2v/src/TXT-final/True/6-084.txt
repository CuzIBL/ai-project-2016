 
single-class classification  scc  seeks to distinguish one class of data from the universal set of multiple classes. we present a new scc algorithm that efficiently computes an accurate boundary of the target class from positive and unlabeled data  without labeled negative data . 
1 introduction 
single-class classification  scc  seeks to distinguish one class of data from the universal set of multiple classes   e.g.  distinguishing apples from fruits  identifying  waterfall  pictures from image databases  or classifying personal homepages from the web   throughout the paper  we call the target class positive and the complement set of samples negative.  
   since it is not natural to collect the  non-interesting  objects  i.e.  negative data  to train the concept of the  interesting  objects  i.e.  positive data   scc problems are prevalent in real world where positive and unlabeled data are widely available but negative data are hard or expensive to acquire  yu et al. 1; letouzey et al.   1; decomite et a/.  1 . for example  in text or web page classification  e.g.  personal homepage classification   collecting negative training data  e.g.  a sample of  non-homepages   is delicate and arduous because manually collected negative data could be easily biased because of a person's unintentional prejudice  which could be detrimental to classification accuracy. in an example of diagnosis of a disease  positive data are easy to access  e.g.  all patients who have the disease  and unlabeled data are abundant  e.g.  all patients   but negative data are expensive if detection tests for the disease are expensive since all patients in the database cannot be assumed to be negative samples if they have never been tested. further applications can be also found in pattern recognition  image retrieval  classification for data mining  rare class classification  etc. in this paper  we focus on this scc problem from positive and unlabeled data  without labeled negative data . 
1 	previous approaches for scc 
traditional  semi- supervised learning schemes are not suitable for scc without labeled negative data because:  1  the portions of positive and negative spaces are seriously unbalanced without being known  i.e.  pr p    pr p    and 
 1  the absence of negative samples in the labeled data set makes unfair the initial parameters of the model and thus it leads to unfair guesses for the unlabeled data. 
¡¡active learning methods also try to minimize the labeling labors to construct an accurate classification function by a different approach that involves an interactive process between the learning system and users  tong and koller  1 . 
   valiant in 1  valiant  1  pioneered learning theory from positive examples based on rule learning. in 1  f. denis defined the probably approximately correct  pac  learning model for positive and unlabeled examples  and showed that k-dnf  disjunctive normal form  is learnable from positive and unlabeled examples  denis  1 . after that  some experimental attempts to learn using positive and unlabeled data have been tried using k:-dnf or c1  letouzey et al.  1; decomite et al  1 . rule learning methods are simple and efficient for learning nominal features but tricky to use for the problems of continuous features  high dimensions  or sparse instance spaces. 
¡¡positive example-based learning  pebl  framework was proposed for web page classification  yu et ai  1 . their method is limited to the web domain with binary features  and its training efficiency is poor because of using svm iteratively whose training time is already at least quadratic to the size of training data set. this problem becomes critical when the size of unlabeled data set is large. 
¡¡a probabilistic method for the scc problem has been recently proposed for the text domain  liu et ai  1 . as they specified in the paper  their method - a revision of the em algorithm - performs badly on  hard  problems due to the fundamental limitations of the generative model assumption  the attribute independence assumption which results in linear separation  and the requirement of good estimation of prior probabilities. 
¡¡osvm  one-class svm  also distinguishes one class of data from the rest of the feature space given only a positive data set  tax and duin  1; manevitz and yousef  1 . based on a strong mathematical foundation  osvm draws a nonlinear boundary of the positive data set in the feature space using two parameters - v  to control the noise in the training data  and  to control the  smoothness  of the boundary . they have the same advantages as svm  such as efficient handling of high dimensional spaces and systematic nonlinear classification using advanced kernel functions. 

learning 	1 


figure 1: boundaries of svm and osvm on a synthetic data set. big dots: positive data  small dots: negative data 

however  osvm requires a much larger amount of positive training data to induce an accurate class boundary because its support vectors  svs  of the boundary only comes from the positive data set and thus the small number of positive svs can hardly cover the major directions of the boundary especially in high dimensional spaces. due to the svs coming only from positive data  osvm tends to ovcrfit and undcrfit easily. tax proposed a sophisticated method which uses artifically generated unlabeled data to optimize the osvm's parameters that  balance  between ovcrfitting and undcrfitting  tax and duin  1 . however  their optimization method is infeasibly inefficient in high dimensional spaces  and even with the best parameter setting  its performance still lags far behind the original svm with negative data due to the shortage of svs which makes  incomplete  the boundary description. figure 1 a  and  b  show the boundaries of svm and osvm on a synthetic data set in a two-dimensional space. 
 we used l1bsvm version 1' for svm implementation.  in this low-dimensional space with  enough  data  the obstensibly  smooth  boundary of osvm is not the result of the good generalization but instead is from the poor expressibility caused by the  incomplete  svs  which will become much worse in high-dimensional spaces where more svs around the boundary are needed to cover major directions in the highdimensional spaces. when we increase the number of svs in osvm  it ovcrfits rather than being more accurate as shown in figure l c and d . 
1 contributions and paper layout 
we first discuss the  optimal  scc boundary  which motivates our new scc framework mapping-convergence  mc   where the algorithms under the mc framework generate the boundary close to the optimum  section 1 . in section 1  we present an efficient scc algorithm support vector mapping convergence  svmc  under the mc framework. we prove that although svmc iterates under the mc framework for the  near-optimal  result  its training time is independent of the number of iterations  which is asymptotically equal to that of a svm. we empirically verify our analysis of svmc by extensive experiments on various domains of real data sets such as text classification  e.g.  web page classification   pattern recognition  e.g.  letter recognition   and bioinformatics  e.g.  diagnosis of breast cancer   which shows the outstanding performance of svmc in a wide spectrum of scc prob-
1  http://www.csie.ntu.edu.tw/~cjlin/libsvm lems  with nominal or continuous attributes  linear or nonlinear separation  and low or high dimensions   section 1 . 
1 	notation 
we use the following notation throughout this paper. 
  x is a data instance such that 
  v is a subspace for positive class within u  from which positive data set p is sampled. 
  u  unlabeled data set  is a uniform sample of the universal set. 
  u is the feature space for the universal set such that u c rm where m is the number of dimensions. 
¡¡for an example of web page classification  the universal set is the entire web  u is a uniform sample of the web  p is a collection of web pages of interest  and x € rm is an instance of a web page. 
1 mapping convergence  mc  framework 
1 	motivation 
in machine learning theory  the  optimal  class boundary function  or hypothesis  h x  given a limited number of training data set { x / }  i is the label of x  is considered the one that gives the best generalization performance which denotes the performance on  unseen  examples rather than on the training data. the performance on the training data is not regarded as a good evaluation measure for a hypothesis because the hypothesis ends up overfitting when it tries to fit the training data too hard.  when a problem is easy  to classify  and the boundary function is complicated more than it needs to be  the boundary is likely overfitted. when a problem is hard and the classifier is not powerful enough  the boundary becomes undcrfit.  svm is an excellent example of supervised learning that tries to maximize the generalization by maximizing the margin and also supports nonlinear separation using advanced kernels  by which svm tries to avoid overfitting and underfitting  burges  1 . 
¡¡the  optimal  scc classifier without labeled negative data also needs to maximize the generalization somehow with highly expressive power to avoid ovcrfitting and undcrfitting. to illustrate an example of the  near-optimal  scc boundary without labeled negative data  consider the synthetic data set  figure 1  simulating a real situation where within u   1  the universal set is composed of multiple groups of data   1  
learning 


figure 1: synthetic data set simulating a real situation. p: big dots  u: all dots  big and small dots  
the positive class v is one of them  supposing v is the data group in the center   and  1  the positive data set p is a sample from v  assuming that the big dots are the sample p . 
osvm draws v  a tight boundary around p  as shown in 
figure 1 a   which overfits the true class area v due to the absence of the knowledge of the distribution of u. however  the  near-optimal  scc classifiers must locate the boundary between v and u outside v  figure 1 b   and thus maximize the generalization. the mc framework using u systematically draws the boundary of figure 1 b . 
1 	negative strength 
let h x  be the boundary function of the positive class in u  which outputs the distance from the boundary to the instance x in u such that 
if x is a positive instance  if x is a negative instance  if x is located farther than x' from the boundary in u. 
definition 1  strength of negative instances . for two negative instances x and x' such that h x    1 and h x'    1  if h x      h x'    then x is stronger than x'. 
example 1. consider a resume page classification function h x  from the web  u . suppose there are two negative data objects x and x'  non-resumepages  in u such that h x    1 and h x'    1: x is  how to write a resume  page  and x' is  how to write an article   page. in u  x' is considered more distant from the boundary of the resume class because x has more relevant features to the resume class  e.g.  the word  resume   in text  though it is not a true resume page. 
1 	m c f r a m e w o r k 
the mc framework is composed of two stages: the mapping stage and the convergence stage. in the mapping stage  the algorithm uses a weak classifier   which draws an initial approximation of  strong negatives  - the negative data located far from the boundary of the positive class in u  steps 1 and 1 in figure 1 . based on the initial approximation  the convergence stage runs in iteration using a second base classifier $1  which maximizes the margin to make a progressively better approximation of negative data  steps 1 through 1 in figure 1 . thus the class boundary eventually converges to 

figure 1: example of the spaces of the mc framework in u 
the boundary around the positive data set in the feature space which also maximizes the margin. 
input: 	- positive data set p  unlabeled data set u 
output: 	- a boundary function h  
: an algorithm identifying  strong negatives  from u 
: a supervised learning algorithm that maximizes the margin 
algorithm: 
1. use  to construct a classifier ho from p and u which classifies only  strong negatives  as negative and the others as positive 
1. classify u by h1 examples from u classified as negative by ho 
:= examples from u classified as positive by ho 
1. set tv := 	and i := 1 
1. do loop 
1. n 	:= 	nun  
1. use 	to construct hi 1 from p and tv 
1. classify   
:= examples from pi classified as negative by hi+1 examples from px classified as positive by hi+1 
1. i := i + 1 
1. repeat until 
1. return hi 
figure 1: mc framework 
¡¡assume that v is a subspace tightly subsuming p within u where the class of the boundary function for v is from the algorithm  e.g.  svm . in figure 1  let no be the negative space and be the positive space within u divided by h1  a boundary drawn by    and let ni be the negative space and be the positive space within  divided by hi  a boundary drawn by  then  we can induce the following formulae from the mc framework of figure 1.  figure 1 illustrates an example of the spaces of the framework in u.  
 1  
 1  
where i is the number of iterations in the mc framework. 
theorem 1  boundary convergence . suppose u is uniformly distributed in u. if algorithm does not generate 

learning 	1 

false negatives  and algorithm maximizes margin  then  i  the class boundary of the mc framework converges into the boundary that maximally separates p and u outside  and  1  i  the number of iterations  is logarithmic in the margin 
between 	and 
proof   because a classifier constructed by the algorithm does not generate false negative. a classifier h  constructed by the algorithm trained from the separated space and divides the rest of the space which is equal to into two classes with a boundary that maximizes the margin between and the first part becomes and the other becomes repeatedly  a classifier constructed by the same algorithm trained from the separated space and divides the rest of the space into and with equal margins. thus  always has the margin of half of  for therefore  i will be logarithmic in the margin between and 
¡¡the iteration stops when where there exists no sample of u outside therefore  the final boundary will be located between p and u outside while maximizing the margin between them. 	
¡¡theorem 1 proves that under certain conditions  the final boundary will be located between p and u outside  however  in the example of figure 1 b   our framework generates the  better  boundary located between p and u outside v because in theorem 1  we made a somewhat strong assumption  i.e.  u is uniformly distributed  to guarantee the boundary convergence. in a more realistic situation where there is some distance s between classes-figure 1 shows some gaps between classes-if the margin between and becomes smaller than at some iteration  the convergence stops because becomes empty. the margin b e t w e e n a n d reduces by half at each iteration as the boundary approaches to and thus the boundary is not likely to stop converging when it is far from unless u is severely sparse. thus  we have the following claim: 
claim 1. the boundary of mc is located between p and u outside v if u and p are not severely sparse and there exists visible gaps between v and u. 
validity of the component algorithms 
j. 	must not generate false negatives. 
most classification methods have a threshold to control the trade-off between precision and recall. we can adjust the threshold of  so that it makes near 1% recall by sacrificing precision.  some violations of this can be handled by the soft constraint of  e.g.  svm .  determining the threshold can be intuitive or automatic when not concerning the precision quality much. the precision quality of  does not affect the accuracy of the final boundary as far as it approximates a certain amount of negative data because the boundary will converge eventually. figure 1 visualizes the boundary after each iteration of svmc. the mapping stage only identifies very strong negatives by covering a wide area around the positive data  figure 1 a  .  we used osvm for the algorithm of the mapping stage. we intuitively set the parameters of osvm such that it covers all the positive data without much concern for false positives.  although the precision quality of mapping is poor  the boundary at each iteration converges  figures 1 b  and  c    and the final boundary is very close to the true boundary drawn by svm with p and n  figure 1 a  and 1 d  . our experiments in section 1 also show that the final boundary becomes very accurate although the initial boundary of the mapping stage is very rough by the  loose  setting of the threshold of . 
1. 	1 must maximize margin. 
svm and boosting are currently the most popular supervised learning algorithms that maximize the margin. with a strong mathematical foundation  svm automatically finds the optimal boundary without a validation process and without many parameters to tune. the small numbers of theoretically motivated parameters also work well for an intuitive setting. for these reasons  we use svm for for our research. in practice  the soft constraint of svm is necessary to cope with noise or outliers. the soft constraint of svm can affect / and the accuracy of the final boundary. however  p is not likely to have a lot of noise in practice because it is usually carefully collected by users. in our experiments  a low setting  i.e.  v = 1  of  the parameter to control the rate of noise in the training data  performs well for all cases for this reason.  we used i/-svm for the semantically meaningful parameter  chang and lin  1 .  
learning 

1 support vector mapping convergence  svmc  
1 motivation 
the classification time of the final boundary of smc   simple  mc with  = svm  is equal to that of svm because the final boundary is a boundary function of the training time of smc can be very long if is very large because the training time of svm highly depends on the size of data set  and smc runs iteratively. 
assuming the number of 	iterations 	andtsvm = 
1 |f/|1  where  is the training time of a classifier  tsvm is known to be at least quadratic to and linear to the number of dimensions . refer to  chang and lin  1  for more discussion on the complexity of svm. however  decreasing the sampling density of u to reduce the training time hurts the accuracy of the final boundary because the density of u will directly affect the quality of the svs of the final boundary. 
1 	svmc 
svmc prevents the training time from increasing dramatically as the sample size grows. we prove that although svmc iterates under the mc framework for the  nearoptimal  result  its training time is independent of the number of iterations  and thus its training time is asymptotically equal to that of a svm. 
¡¡the approach of svmc is to use minimally required data set at each iteration such that the data set does not degrade the accuracy of the boundary while it saves the training time of each svm maximally. to illustrate how svmc achieves this  consider the point of starting the third iteration  when in smc.  see step 1 in figure 1.  after we merge into n  we may not need all the data from n in order to construct h1 because the data far from h1 may not contribute to the svs. the set of negative svs of h1 is the representative data set for and so we only keep the negative svs of h1 and the newly induced data set to support the negative side of/1. 
claim 1  minimally required negative data . minimally 
required negative data at th iterationthat makes h{+1 is as accurate as the boundary constructed from  negative support vectors of h{. 
rationale. the negative svs of will be f r o m a n d the negative svs of hi because is the closest data set to /ij+i and because the directions not supported by in the feature space will be supported by the negative svs of h  which are the representing data set for  however  if any of the negative svs of hi is excluded in constructing might suffer because the negative svs of hi need to support the direction that tvj does not support in the feature space. thus   negative support vectors of hi are the minimally required negative data set at  i + l th iteration. 
¡õ 
¡¡for the minimally required data set for the positive side  we cannot definitely exclude any data object from p at each iteration because positive svs are determined depending on negative svs  and it is hard to determine the positive data that cannot be svs independent of negative svs or svm parameters. 
¡¡surprisingly  adding the following statement between step 1 and 1 of the original mc framework of figure 1 completes the svmc algorithm. 
reset n with negative svs of 
theorem 1  training time of svmc . suppose 
proof. for simplicity of the proof  we approximate each value as follows. 

¡¡theorem 1 states that the training complexity of svmc is asymptotically equal to that of svm. our experiments in section 1 also show that svmc trains much faster than smc while it remains the same accuracy. figure 1 visualizes the boundary after each iteration of svmc on the same data set of figure 1. 
1 empirical results 
in this section  we show the empirical verification of our analysis on svmc by extensive experiments on various domains of real data sets - web page classification  letter recognition  and diagnosis of breast cancer - which show the outstanding performance of svmc in a wide spectrum of scc problems  with nominal or continuous attributes  linear or nonlinear separation  and low or high dimensions . 
1 	datasets and methodology 
due to space limitations  we reports only the main results. 
our evaluation is based on the f  measure 
r   p is precision and r is recall  as was used in  liu et al  1  - one of the most recent works on scc from positive and unlabeled data1. we also report the accuracy. 
¡¡we used the letter recognition and breast cancer data sets from the uc1 machine learning repository1 for direct comparisons with osvm.  osvm is often used for letter or digit 
¡¡¡¡1 refer to  liu et al.  1  for the justification of using the fl measure for scc. 1  http://www.ics.uci.cdu/~mlcarn/m lrepository.html 

learning 	1 

class  p   u  i'vi f1  accuracy  %  t-time  sec.  tsvm smc svmc osvm svm.nn smc svmc letter a 1 1 1 1  1 1  1 1  1 1  1 1 1 1 1 'b' 1 1 1 1 1 1  1 1  1 1  1 1  1 1 1 'c' 1 1 1 1  1 1  1 1  1 1  1 1  1 1 1 'd' 1 1 1 1 1 1  1 1  1 1 1 1  1 1 1 'e' 1 1 1 1  1 1  1 1  1 1 1 1 1 1 1 b-canccr 1 1 1 1  1 1  1 1  1 1 1 1  1 1 1 course 1 1 1 1  1 1  1 1  1 1  1 1  1 1 1 faculty 1 1 1 1  1 1  1 1  1 1 1 1 1 1 1 student 1 1 1 1  1 1  1 1 1 1  1 1 1 1 1 table 1: performance results.  pu ' # of positives in u  t-time: training time 
recognition  tax and duin  1 .  we also used webkb1 for web page classification as used in  liu et ai  1  for the indirect comparison with it. we set up the experiment environment in the same way as  liu et ai  1   except our setup is more realistic: in  liu et ai  1   u is composed of b% of positives  e.g.  student  and samples from another class  e.g.  course . our u is b% of positives  e.g.  student  and the remainder is from all other classes.  refer to  liu et 
ai  1  for the rest of the data set description.  
1 	results 
table 1 shows the performance results. tsvm  traditional svm  shows the ideal performance using svm from p and manually classified n from u. svmjmn  svm with noisy negatives  is svm from p  with u as a substitute for n.  u can be thought of as a good approximation of n.  note that for tsvm  svmjmn  and mc  smc and svmc   we used theoretically motivated fixed parameters without performing explicit optimization or validation process. for osvm  we thoroughly searched for the best parameters based on the testing data set since optimizing parameters as specified in  tax 
and duin  1  is infeasibly inefficient especially in high dimensional spaces. 
   mc  smc and svmc  without labeled negative data show performance close to that of tsvm. svmc trains much faster than smc for most data sets. the performance of smc and svmc is comparable.  they differ a little because of the soft constraints of svm and noise in the data.  osvm performs fairly well on letter recognition and breast cancer  of low dimensionality with large amounts of data  but poor on webkb  of high dimensionality . svm.nn suffers from very low f  scores because negative prediction dominates due to many false positives in the training data. 
1 conclusion 
we present the mc framework and its instance algorithm 
svmc  a new scc method from positive and unlabeled data  without labeled negative data . svmc without labeled negative data computes an accurate classification boundary around the positive data using the distribution of unlabeled data in a systematic way. 
1
   http://www-1.cs.cmu.edu/afs/cs.cmu.edu/project/theo1/www/wwkb/ 