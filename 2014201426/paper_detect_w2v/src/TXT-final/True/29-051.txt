 
overfitting is a well-known problem in the fields of symbolic and connectionist machine learning. it describes the deterioration of generalisation performance of a trained model. in this paper  we investigate the ability of a. novel artificial neural network  bp-som  to avoid overfitting. bp-som is a hybrid neural network which combines a multi-layered feed-forward network  mfn  with kohonen's self-organising maps  soms . during training  supervised back-propagation learning and unsupervised som learning cooperate in finding adequate hidden-layer representations. we show that bp-som outperforms standard backpropagation  and also back-propagation with a weight decay when dealing with the problem of overfitting. in addition  we show that bp-som succeeds in preserving generalisation performance under hidden-unit pruning  where both other methods fail. 
1 	on avoiding overfitting 
in machine-learning research  the performance of a trained model is often expressed in its generalisation performance  i.e.  its capability to process correctly new instances not present in the training set. when the generalisation performance of the trained model is much worse than its performance on the training material  i.e.  its ability to reproduce the training material   we speak of overfitting. overfitting is sometimes due to the sparseness of the training material: e.g.  the training material does not sufficiently cover the characteristics of the classification task. a second cause for overfitting might be a high degree of non-linearity in the training material. in both cases  the learning algorithm might not be able to learn more from the training material than the classification of the training instances itself  see  e.g.  norris  1 . 
　the issue of avoiding overfitting is well-known in the field of symbolic and connectionist machine learning 
 e.g.  wolpert  1; schaffer  1; jordan and bishop  1 . in symbolic machine learning  a commonly used 
1 	neural networks 
heuristic to avoid overfitting is minimising the size of the induced models  cf. quinlan's  1  c1 and c1rules   in the sense of the minimum'descriptionlength  mdl  principle  rissanen  1 . for instance  smaller  or less complex  models should restrict the number of parameters to the minimum required for learning the task at hand. 
　in connectionist machine learning  neural networks   avoiding overfitting is closely related to finding an optimal network complexity. in this view  two types of methods of avoiding overfitting  or regularisation  can be distinguished:  i  starting with an undersized network and gradually increasing the network's complexity  fahlman and lebiere  1   and  ii  starting with an oversized network and gradually decreasing its complexity  e.g.  mozer and smolensky  1; le cun  denker  and solla  1; weigend  rumelhart  and huberman  
1; hassibi  stork  and wolff  1; prechelt  1; weigend  1 . 
　in this paper we analyse the overfitting-avoidance behaviour of a novel artificial neural-network architecture  bp-som  weijters  1   which belongs to the second type of connectionist machine-learning methods. in bp-som  the network complexity is reduced by guiding the hidden-layer representations of a multi-layer feedforward network  mfn  rumelhart et a/.  1  to simplified vector representations. to achieve its aim  bpsom combines the traditional mfn architecture with selforganising maps  soms   kohonen  1 : each hidden layer of the mfn is associated with one som  see figure 1 . during training of the weights in the mfn  the corresponding som is trained on the hidden-unit activation patterns. the standard mfn error-signal is augmented with information from the so ms. the effect of the augmented error signals is that  during learning  the hidden-unit activation patterns of clusters of instances associated with the same class tend to become highly similar. intuitively speaking  the self-organisation of the som guides the mfn into arriving at adequate hiddenunit representations. 
　we demonstrate that bp-som avoids overfitting by reducing the complexity of the hidden-layer representations. in section 1  we provide a description of the bp-som architecture and learning algorithm. section 

figure 1: an example bp-som network. 
1 presents experiments with bp-som trained on three benchmark classification tasks  focusing on the ability to avoid overfitting. in addition  we study the robustness of bp-som to hidden-unit pruning. our conclusions are given in section 1. 
1 	bp-som 
learning in bp-som is a cooperation between supervised learning in bp and unsupervised learning in som. the unsupervised dimension reduction and clustering on the som guide bp learning during the development of adequate hidden-layer representations on the mfn. the influence of the som causes clusters of hidden-layer representations associated with the same class  to become increasingly similar to each other. in this section we describe how the cooperation between bp and som is implemented. 
1 	the bp-som architecture 
the bp-som architecture is composed of two types of well-known building blocks  viz. an mfn combined with one or more soms. the number of s1ms equals the number of hidden layers in the mfn; each som is associated with one hidden layer. figure 1 illustrates a bp-som network with five input units  one hidden layer with four units  two output units  and one som. the size of the som is arbitrarily chosen to be 1 x 1. each of the nine elements  denoted by  contains an acti-
vation vector  denoted by vei  of length 1  equal to the number of hidden units   a class label  two class counters  equal to the number of output classes   and a reliabilityvalue field. 
1 	the bp-som learning algorithm 
we assume that bp-som  with / hidden layers and l associated soms  /   1   is trained on a classification task with two or more distinct output classes. training of the mfn and soms proceeds in parallel as follows: after feed-forward activation of an input instance  vhidden  the activations of the units of each hidden layer of the mfn 
	weijters  et al. 	1 

1. initialise the mfn and the corresponding soms  i.e.  assign random numbers within predefined bounds to all connection weights and som-element vectors . initialise the class labels  the class counters  and the reliability field of all som elements to unlabelled  zero  and zero  respectively. 
1. train the mfn during a fixed number of cycles  to . for each cycle do 
a. for each training instance with its associated output class in the training set: 
  compute the mfn output via feed-forward activation through the mfn. collect for all hidden* 
layers their  and train the corresponding soms on these vectors using kohonen's  1  som learning algorithm. 
t calculate  and update the mfn weights according to equation  1 . 
b. after each nth training cycle  1   n   m   re-compute the class labels and reliability of all som elements by performing a class-labelling procedure as described in the text. 
figure 1: a conceptual description of the bp-som learning algorithm. 
the threshold parameter is used to prevent unreliable 
som elements to influence  during the last 
step of processing a training instance  the bp-som algorithm handles the bpsom-errorj of each hidden unit in the same way as bp uses the bp errorj in the traditional on-line back-propagation weight-updating rule. a conceptual description of the bp-som learning algorithm is given in figure 1. 
　upon completion of learning  the soms become redundant  as they do not interact with the propagation of activation through the mfn network. 
1 experiments with bp-som 
in this section  we present three experiments with the bp-som architecture and the bp-som learning algorithm. in these experiments  we compare bp-som with  i  standard bp  rumelhart et al. 1   and  ii  bp augmented with weight decay  hinton  1   henceforth denoted by bpwd. weight decay is included in the comparison  as it is a simple  commonly used method for avoiding overfitting in bp-trained mfns. bp-som  bp  and bpwd are trained on three benchmark classification tasks  viz. the date-calculation task  the parity-1 task  and the task of detecting splices in dna sequences. the datecalculation task and the parity-1 task are hard to learn for many learning algorithms  often leading to overfitting  norris  1; schaffer  1; thornton  1   because the output classification in both tasks depends on the values of ail input features. for example  knowing the date and the month and not the year makes it impossible to estimate the day of the week with a probability greater than chance. the genc-splicc detection task does 
1 	neural networks 
not have this property. 
   we have used a fixed set of parameters for all three experiments described. the parameter values for bp  including the number of hidden units for each task  were determined by performing pilot experiments with bp  and taking the values which led to optimal validation performance. hence  the bp learning rate was set to 1 and the momentum to 1. bp  bpwd  and bp-som were trained for a fixed number of cycles m = 1. the following parameter values  specific for bp-som  gave optimal generalisation performance in a number of pilot experiments on the date-calculation task. in all som*s a decreasing interaction strength from 1 to 1  and a decreasing neighbourhood-updating context from a square with maximally 1 units to only 1 unit  the winner  was used. the influence of the som error vector  a  was set to 1  and the reliability threshold t to 1. class labelling in bp-som was performed at each fifth cycle  n = 1 . the term was enabled after the first five cycles. early stopping  a common method to prevent overfitting  was used in all experiments with bp  bpwd  and bp-som: the performance of a trained network was calculated in percentages of incorrectly-processed test instances at the cycle where the classification error on validation material was minimal  prechelt  1 . the minimal classification error was always reached well within the limit m. 
1 	i m p r o v i n g generalisation p e r f o r m a n c e 
first experiment: date calculation 
date calculation is an example of a task which easily leads to overfitting in mfns trained by bp; hence the trained mfn has a low generalisation performance  norris  1 . the task is: given a certain date  e.g.  march 
1  1   determine the day of the week it fell on  e.g.  monday . norris  1  describes a connectionist model of an idiot-savant date calculator  using an mfn and bp. he concluded that bp was not able to learn this task  unless it was decomposed into three easier subtasks. 
　in our experiments with bp-som  we concentrated on the same interval of dates as norris  i.e.  training and test dates ranged from january 1  1 to december 1  1. we generated two training sets  each consisting of 1 random selected instances  i.e.  one-fifth of all dates. we also generated two corresponding test sets and two validation sets  with 1 instances each  of new dates within the same 1-year period. in all our experiments  the training set  test set  and validation set had empty intersections. the date november 1  1 is represented by setting unit 1 in the day field to 1  unit 1 in the month field to 1  and unit 1 of the year field to 1. all other 1 input units are set to zero. the output is represented by 1 units  one for each day of the week. the mfn contained one hidden layer with 1 hidden units for bp  found to result in optimal validation performance   and 1 hidden units for bpwd and bp-som. the corresponding som of the bp-som network was of size 1 x 1. two different data sets were generated. on each set  five runs with different random weight initialisations were 


table 1: average generalisation performances  plus standard deviation  after  averaged over ten experiments  in terms of incorrectly-processed training and test instances  of bp  bpwd  and bp-som  trained on three tasks. 

performed with bp  bpwd  and bp-som. the average classification errors on the test material for these thirty experiments are reported in the top row of table 1. 
　from table 1 it follows that the average classification error of bp is high: on test instances bp obtains a classification error of 1%  while the classification error of bp on training instances is 1%. compared to the classification error of bp  the classification errors on both training and test material of bpwd and bp-som are much lower. 
　however  bpwd's generalisation performance on the test material is seriously worse than its performance on the training material: this is an indication of over-fitting. we note in passing that the results of bpwd contrast with norris'  1  claim that bp is unable to learn the date-calculation task when it is not decomposed into subtasks. the inclusion of weight decay in bp is sufficient for a good approximation of the performance results of norris'  1  decomposed network. 
　the results in table 1 also show that  the performance of bp-som on the training material is comparable  albeit very roughly  with that of bpwd; however  the performance of bp-som on test material is significantly better than that of bpwd  t 1 =1  p 1 . considering that the performances of bp-som on training and test material are in the same range  we conclude that for the date-calculation task  bp-som has been able to avoid overfitting. 
second experiment: parity-1 
bp  bpwd  and bp-som were applied to the parity-1 task  viz. the task to determine whether a bit string of 1's and l's of length 1 contains an even number of l's. the training set contained 1 different instances selected at random out of the set of 1 possible bit strings. the test set and the validation set contained 1 new instances each. the hidden layer of the mfn in all three algorithms contained 1 hidden units  and the som in bp-som contained 1 elements. the algorithms were run with 1 different random weight initialisations. the second row of table 1 displays the classification errors on training instances and test instances. the results again indicate that bp-som performs significantly better than bp and bpwd on test material  t 1 =1  p 1 and t 1 =1  p 1  respectively   and that bp-som is able to avoid overfitting better than bp. the results show that bpwd is also able to avoid overfitting  but this is at the cost of low performance on both training and test material. 
　to visualise the differences between the representations developed at the hidden layers of the mfns trained with bp  bpwd  and bp-som  we also trained soms with the hidden layer activities of the trained bp and bpwd networks. the left part of figure 1 visualises the class labelling of the som attached to the bp-trained mfn after training; the middle part visuahses the som of the bpwd-trained mfn  and the right part displays the som of the bp-som network after training on the same material the som of the bp-som network is much more organised and clustered than that of the soms corresponding with the bp-trained and bpwd-trained mfns. the rehabihty values of the elements of all three soms are represented by the width of the black and white squares. it can be seen that the overall rehabihty and the degree of clusteredness of the som of the bp-som network is considerably higher than that of the som of the bp-trained and bpwd-trained mfns. 
third experiment: gene-splice detection 
a third comparative experiment was performed by using the gene-1  gene-1  and gene-1 benchmark data sets extracted from the probeu1 benchmark collection  prechelt  1 . the three data sets are different partitionings of a large data set representing the task of detecting sphce boundaries between dna exons and introns on the basis of a window of 1 dna sequence elements  nucleotides . each data set features 1 training instances  1 validation instances  and 1 test instances. the mfn used in the bp  bpwd  and bp-som experiments contained 1 input units  1 hidden units  and 1 output units  the optimal numbers of units as reported in prechelt  1 . the size of the corresponding som was set to 1 x 1. the three bottom rows of table 1 display the generalisation performances of bp  bpwd  and bp-som on the three gene tasks  again indicating a clear advantage of bp-som over bp. for all three data sets  the generalisation performance of bp-som was significantly better than that of bp  gene1: t 1 =1 p 1; gene-1: t 1 =1  p 1; gene-1: t 1 =1  p 1 . bp-som also performs significantly better than bpwd on test material  gene1: t 1 =1  p 1; gene-1: t 1 =1  p 1; gene-1: t 1 =1  p 1 . nevertheless  the differences between classification errors on training and test-
	weijters  et al. 	1 


figure 1: graphic representation of a 1 x 1 som associated with a bp-trained mfn  left  and a bpwd-trained mfn 
 middle   and a 1 x 1 som associated with a bp-som network  right   all trained on the parity-1 task. white squares represent class 'even'; black squares represent class 'odd'. the width of a square represents the reliability of the element; a square of maximal size represents a reliability of 1%. 

ing material for all three algorithms indicate that neither bp  nor bpwd  nor even bp-som succeed in fully avoiding overfitting. 
1 	pruning of hidden units 
by including  in the error signal during bp-
som learning  the hidden-unit activation patterns associated with the same class tend to become more similar. when looking at the hidden-unit activations in bp-som networks  we observed an additional effect  viz. that hidden-unit activations culminated in switching between two or three values  or resulted in having a stable activity with a very low variance. this clearly contrasts with hidden units in mfns trained with bp  of which all activations usually display high variance. to illustrate this phenomenon  figure 1 shows the standard deviations on training material of the 1 hidden-unit activations of an mfn trained with bp  top row   bpwd  middle row   and 
bp-som  bottom row   all three algorithms trained on the parity-1 task  1 instances . the standard deviations of ten of the twenty units in the bp-som network are equal to 1 or lower. 
　whenever a unit has a stable activity with a low standard deviation for all training instances  it is redundant in the input-output mapping. in that case  the unit can be pruned from the network. its effect on the following layer  i.e.  its mean activation multiplied by its weights to units in the following layer  can be included in the weights from the bias unit. this pruning can be performed both during and after training. 
　we trained bp  bpwd  and bp-som on the datecalculation task and the parity-1 task  attempting to prune hidden units according to a threshold criterion during training.  the gene-splice task was not included because only two hidden units were used in the mfn.  we introduced a stability threshold parameter s  denoting the standard deviation of the unit's activation below which it is pruned. after a number of pilot experiments with different values for s  we performed experiments with 1 = 1. all three algorithms were trained on each 
1 	neural networks 

figure 1: standard deviations of the activations of the 1 hidden units of an mfn trained with bp  top   with bpwd  middle   and with bp-som  bottom   all three trained on the parity-1 task  1 instances . 

of the two tasks with ten different random initialisations. 
　we found that bp-som was able to prune 1 out of 1 hidden units for the case of the date-calculation task  and 1 out of 1 hidden units for the case of the parity-1 task  in both tasks  it is an averaged result over 1 experiments   without loss of generalisation performance. with the same setting of s  trained on the same tasks  no hidden units could be pruned with bp  nor with bpwd. only with s = 1 hidden units could be pruned during bp and bpwd learning; however  this led to seriously worse generalisation performance of these networks. 
1 	conclusions 
we have shown that bp-som has been able to avoid overfltting for the date-calculation task and also for the parity-1 task. moreover  it outperforms bp and bpwd when dealing with the gene-splice detection task  although on this task it does not succeed fully to avoid overfltting. furthermore  bp-som is shown to increase the amount of hidden units that can be pruned without loss of generalisation performance. these improvements are due to the cooperation in bp-som between supervised and unsupervised learning; i.e.  its ability to guide clusters of hidden-unit representations associated with the same class to be more similar to each other. the highly varying hidden-unit representations of mfns trained with bp and bpwd are simplified by bp-som learning to representations with limited activation values. this reduction of complexity occurs during learning  which enables the pruning of hidden units  the reduction of the size of the network  during learning. 
