 
learning in first-order logic  fol  languages suffers from a specific difficulty: both induction and classification are potentially exponential in the size of hypotheses. this difficulty is usually dealt with by limiting the size of hypotheses  via either syntactic restrictions or search strategies. 
this paper is concerned with polynomial induction and use of fol hypotheses with no size restrictions. this is done via stochastic matching: instead of exhaustively exploring the set of matchings between any example and any short candidate hypothesis  one stochastically explores the set of matchings between any example and any candidate hypothesis. the user sets the number of matching samples to consider and thereby controls the cost of induction and classification. 
one advantage of this heuristic is to allow for resource-bounded learning  without any a priori knowledge about the problem domain. experiments on a real-world problem pertaining to organic chemistry fully demonstrate the potentialities of the approach regarding both predictive accuracy and computational cost. 
1 	introduction 
this paper is concerned with learning from examples expressed in  a restriction of  first-order logic  fol . this language is required to describe examples that include several objects of the same kind  e.g. a car involves several wheels  a molecule involves several atoms   either when these objects cannot be ranked in a canonical way  e.g. atoms in a molecule  or when it makes sense to compare objects of different ranks  e.g. front left and front right wheels . 
learning in fol  usually referred to as inductive logic 
programming  ilp   muggleton k de raedt  1   is receiving growing attention as it appears the only way for machine learning to tackle complex domains such as organic chemistry  king  srinivasan  & sternberg  1  
learning 
celine rouveirol 
lri  universite d'orsay 
1 orsay  france 
celine.rouveirol lri.fr 
or natural language  mooney  1 . and a number of fol learners have been developed: foil  quinlan  1   ml-smart  bergadano k giordana  1   focl  pazzani k kibler  1   kbg  bisson  1   regal  giordana k neri  1   progol  muggleton  1   ribl  emde k wettscherek  1  and remo  zucker k ganascia  1  to name a few. 
　the specific difficulty of ilp is related to the matching step: if a clause involves three literals part and the description of a given device involves 1 parts  there are 1 possible ways  termed matchings  to instantiate the literals in the clause by the literals describing the device. discriminant induction requires all such matchings to be considered  to ensure the clause discriminates the device ; and the same goes for deduction in the worst case  to check whether the clause covers the device . in other words  induction and deduction are exponential in the size of fol hypotheses. this limitation is dealt with in the literature by inducing short hypotheses  by means of search strategies  quinlan  1; bergadano k giordana  1; pazzani & kibler  1; muggleton  1  and/or syntactic restrictions  giordana k neri  1; kietz k lubbe  1; muggleton  1 . 
　this paper is interested in polynomial induction and use of fol hypotheses  with no size restrictions. this is made possible by stochastic matching: one only considers some samples of matchings between examples and hypotheses -- instead of exhaustively considering all matchings between examples and  short  hypotheses. the considered matchings are constructed by a stochastic sampling mechanism  and the number of samples allowed is supplied by the user. by so-doing  s/he controls both the induction cost  linear in the number of samples  and the quality of the hypotheses  the more samples are considered  the more likely the hypotheses are consistent . 
　this heuristic is embedded in the disjunctive version space framework  divs   sebag  1   which constructs all consistent hypotheses covering at least one training example. this framework extends that of version space  mitchell  1  and likewise implies a single bias  that of the hypothesis language. in particular  as it does not restrict the size of hypotheses  divs shows in-

tractable for hypothesis languages such as logic programs or constraint logic programs1  sebag & rouveirol  1 . 
　the algorithm combining divs and stochastic matching is termed still for stochastic inductive learning. stochastic matching allows still to learn hypotheses of any size  at most as long as the examples  in polynomial time  and to use them in polynomial time too. stochastic matching corresponds to a new kind of learning bias  mitchell  1 . the stochastic bias contrasts with language and- search biases  inasmuch it does not involve any expert knowledge about the problem or about how to find relevant solutions. rather  it reflects the available computational resources; and the more resources  the better. 
　this paper is organized as follows. section 1 briefly presents divs; the reader is assumed to be familiar with the version space  vs  framework  mitchell  1 . section 1 details the stochastic sampling mechanism and outlines the induction and classification algorithms of still. experimental validation on the mutagenesis problem  king  srinivasan  & sternberg  1  is presented in section 1; the influence of the stochastic matching mechanism is discussed  and still is compared to prominent learners. we conclude with some research perspectives. 
1 	disjunctive version space 
this section recalls how disjunctive version space addresses the limitations of version space  failures  exponential complexity . details on the algorithms are found in  sebag  1; sebag & rouveirol  1 . 
1 	o v e r c o m i n g vs failures 
basically  vs fails when the maximally specific complete hypotheses  set 1  are not more specific than the maximally general consistent hypotheses  set g . but noisy examples lead to over-generalize 1 and over-specialize g; and examples from a disjunctive concept result in an over-generalization of 1 if the hypothesis language is conjunctive. this explains why vs fails to handle realworld problems  which include noisy examples and tackle disjunctive concepts most of the time. 
　vs failures are soundly prevented in three cases: a  when there is no negative example; b  when there is no positive example; and c  when there is exactly one positive example e  provided that e does not belong also to the negative examples  which can easily be checked . the third case is preferred as it is more robust with regard to noise  sebag  1 . 
　the general case  that is  learning from several positive and negative examples  is amenable to the favorable case of a unique positive example by hybridizing version space and the aq algorithm  michalski  1 : for each 
 positive or negative  seed example e. one constructs the star h e  as the set of all consistent hypotheses covering 
   1 constraint logic programming notably contains the extensions of logic programming concerned with number handling. but  this point will not be discussed herein. 
e. if f1..fm denote the examples not belonging to the same class as e  termed counter-examples1 to e  h e  exactly is the version space learned from e as unique positive example  and fx as negative examples. 
　the disjunction of the stars h e   for e ranging over the training set  constitutes the disjunctive version space of all consistent partially complete hypotheses. 
1 	overcoming vs intractability 
building h e  is intractable even within a propositional language: h e  is characterized by its lower bound  1 = e  and its upper bound g  which is the disjunction of an exponential number of conjunctive hypotheses  haussler  1 . 
　divs overcomes this limitation by characterizing h e  as a conjunction of disjunctions. this is done as follows. let d e f}  denote the set of hypotheses covering e and discriminating counter-example fi. h e  includes all hypotheses discriminating all fi  hence it is equal to the conjunction of d e  ft   for fi ranging over the counter-examples to e. 

in an attribute value language  d e fi  gets characterized as the disjunction of all maximally general selectors  michalski  1  discriminating e and fi. table 1 illustrates how d e  f  can be built from e and f with linear complexity in the number of attributes; selectors are here restricted to  att = v   where v denotes a value in the domain of a nominal attribute att  or an interval in the domain of a linear attribute att  selector  size =  is written  size   1  for the sake of simplicity . 
table 1: a pair of examples e and f and d e f  
color size shape weight class f  grey white 1 
1  wings saucer 1 ton   plane 
ufo 
　as in most bottom-up approaches  missing values are handled without problems and no preliminary discretization of linear attribute domains is required. 
　finally  the complexity of building h e  is in o{n x p   where n denotes the number of examples and p the number of attributes. the whole disjunctive version space is characterized with complexity 1 n1 x p . 
1 	flexible classification with divs 
classification in divs much resembles a k-nearestneighbor  k-nn  classification process: an instance e' is said to be neighbor of a training example e iff there exists a hypothesis in h e  that covers e'  one says for short that e' belongs to h{e  ; and e' is classified according to the majority vote of its neighbors. 
   1the counter-examples to a positive example are the negative examples; and vice versa. 
	sebag & rouveirol 	1 

table 1: fol and tabular representations of e and f 
e: 	active{a  	: - atom a  a1 carbon  1   atom a a1  hydrogen  1   cc{a a1 a1  
f : 	inactive b  	: - atom b  1  hydrogen  1   atom b  b1  oxygen  1  


　these neighborhoods  and hence the classification process  can be tuned to cope with noise. formally  e' belongs to h e  iff it belongs to d e  fi  for all fi counterexamples to e. this condition is relaxed by allowing e exceptions  counter-examples fi such that e'does not belong to d e  fi  . 
　independently  h{e  can also be made more specific to cope with sparse data. formally  d e fi  is a dis-
junction of selectors and e' belongs to d e fi  iff it satisfies at least one of these selectors. this is modified by handling from now on d e fi  as an m-of-p concept: e' thereafter belongs to d e fi  iff it satisfies at least m selectors in d f fi . 
　this way  the set of consistent and partially complete hypotheses is constructed once and for all. still  classification can employ hypotheses of any degree of consistency and generality - at no extra cost: the complexity of classification is o n1 x p . 
1 	divs in first-order logic 
let us see how the construction of the set d e f  of hypotheses covering e and discriminating f extends to fol. let e and f be now described as definite clauses1  the head of which are built on opposite target predicates  table 1 . let us express seed e as e =  where c is the clause obtained from e by turning every term tt in e into a distinct variable xj  and  is the substitution on c defined by 
　let the hypothesis language be that of constrained clauses gp  where g generalizes c and p is a conjunction of constraints generalizing   a formal presentation is found in  sebag &: rouveirol  1  . such clauses generalize seed e by construction; they discriminate f iff they do not generalize the clause ~ f   built from f by replacing the predicate in its head  e.g. inactive  by the opposite target predicate  active . 
　clause c allows for a tabular representation of e and  f  table 1 . by construction  e is represented by substitution  let the clause built from c by dropping all predicates absent from f  here cc   be still denoted c by abuse of notation. then by construction ~f is subsumed by c  and f is described1 by the set of substitutions  on c  such that . in our example   in-
   1the reader interested in learning from constrained clauses is referred to  sebag & rouveirol  1 . 
   1given the hypothesis language  predicates in f that are absent from e can be omitted with no loss of information. 
learning 
cludes four substitutions  which correspond to the four ways of mapping the two literals atom in c onto the two literals atom  in f. 
　this attribute-value reformulation of fol examples much resembles the linus and remo approaches  lavrac & dzeroski  1; zucker &: ganascia  1 . the difference is twofold. first  the tabular reformulation in linus and remo operates on the whole dataset; the format of the table is derived from a single clause  specialized if no satisfactory hypothesis is found during the current induction step. second  the reformulation is one-to-one in linus  thanks to syntactic restrictions  and it is one-to-many in remo  but the exponential factor is limited by the size of the clause . in contrast  the reformulation in divs is rather bottomup than top-down: the format of the table is derived from the current seed example  and one considers at once all information conveyed by the seed. hence  the reformulation is one-to-one for the seed  e is completely described by given c  whereas it is one-to-many for the counter-examples. as the exponential factor is not limited here  divs turns out to be intractable on real relational problems  see below . 
   given this reformulation of e and f  building d e  f  is amenable to attribute-value discrimination: substitutions on c can be handled as attribute-value examples and conjunctive constraints on c can be handled as conjunctions of selectors in the same attribute-value language. finally  let pf denote the set of predicates in e absent from f; then gp in the hypothesis language belongs to d f  f  iff either g includes a predicate in 

1 	stochastic induction and classification 
this section describes how to construct fol hypotheses at a polynomial cost  with no size restrictions. further  these hypotheses can be used at polynomial cost too. 
1 	stochastic induction 
as seen in section 1  divs can construct hypotheses including as many literals as the seed. the size of the matching set  can thus be exponential in the size of 


	sebag & rouveirol 	1 

   1with same electric charge if possible; otherwise  with same atomtype; otherwise  with same type. the complexity of the advanced sm gets quadratic in the number of literals  instead of linear for the. basic sm. 
l e a r n i n g 
　1however  still is definitely not a black box: the classification process constructively exhibits hypotheses relevant to the classification of the current instance. one can thus justify the classification of any instance from an intelligible sub-theory  extracted from the whole theory. 
junctive hypotheses  version spaces are indeed of exponential size  haussler  1 . 
   another aspect of still is that it combines logical aspects and example neighborhoods  in the line of kbg  bisson  1   rise  domingos  1  and ribl 
 emde & wettscherek  1 . the specificity of still is that it involves neighborhoods which are constructed by induction  whereas the above learners rely on a built-
in similarity or distance. 
   but the main originality of still  due to the stochastic matching heuristic  is to allow a number of expensive hypotheses to be approximately characterized and used. this contrasts with the main trend in ilp  oriented toward the exact characterization of a few affordable hypotheses. further  stochastic matching allows a fine control of the computational cost  with no expert knowledge on the problem domain. 
   note that the use of stochasticity in still radically differs from what is done in ga-based learners such as regal  giordana & neri  1 : in still the stochastic mechanism samples the matching space and itoperates as a pre-processor of induction; in contrast  the stochastic mechanism in regal samples the hypothesis space and so to say replaces1 induction. note also that still does not directly pertain to the bayesian inductive logic programming framework  muggleton  1   nor to probabilistic induction  in the sense that it does neither assume nor take as input any a priori probability distribution on the hypothesis space. 
   a main perspective of research is to give still a pac model in the sense of valiant  1 : stochastic induction and deduction approximate standard induction and deduction  and one would like to know how the number of samples n and k relate to the probability for this approximation to be correct. 
r e f e r e n c e s 
bergadano  f.  and giordana  a. 1. guiding induction with domain theories. in kodratoff  y.  and michalski  r.  eds.  machine learning : an artificial intelligence approach  volume 1. morgan kaufmann. 1. 
bisson  g. 1. learning in fol with a similarity measure. 
in proceedings 	of 1th 	aaai 
domingos  p. 1. rule induction and instance-based learning: a unified approach. in proceedings of ijcai-1  1. morgan kaufmann. 
emde  w.  and wettscherek  d. 1. relational instance based learning. in saitta  l.  ed.  proceedings of the 1th international conference on machine learning  1. 
giordana  a.  and neri  f. 1. search intemsive concept induction. evolutionary computation 1 :1. 
haussler  d. 1. quantifying inductive bias : al learning algorithms and valiant's learning framework. artificial intelligence 1-1. 
1
　　but it is true to say that the selection and recombination of hypotheses are based on inductive considerations. 
karalic  a. 1. first order regression. ph.d. dissertation  institut josef stefan  ljubljana  slovenia. 
kietz  j.-u.  and liibbe  m. 1. an efficient subsumption algorithm for ilp. in cohen  w.  and hirsh  h.  eds.  proceedings of icml-1  international conference on machine learning  1. morgan kaufmann. 
king  r.; srinivasan  a.; and sternberg  m. 1. relating chemical activity to structure: an examination of ilp successes. new gen. comput. 1. 
lavrac  n.  and dzeroski  s. 1. inductive logic programming: techniques and applications. ellis horwood. 
michalski  r. 1. a theory and methodology of inductive learning. in michalski  r.; carbonell  j.; and mitchell  t.  eds.  machine learning : an artificial intelligence approach  volume 1. morgan kaufmann. 1. 
mitchell  t. 1. generalization as search. artificial intelligence 1-1. 
mitchell  t. 1. the need for bias in learning generalizations. in readings in machine learning. morgan kaufmann. 1. 
mooney  r. 1. ilp for natural language processing. in muggleton  s.  ed.  proceedings of ilp'1. springer-verlag. forthcoming. 
muggleton  s.  and de raedt  l. 1. inductive logic programming: theory and methods. journal of logic programming 1 1. 
muggleton.. s. 1. bayesian inductive logic programming. in warmuth  m.  ed.  proceedings of colt-1  acm conference on computational learning  1. acm press. 
muggleton  s. 1. inverse entailment and progol. new gen. comput. 1-1. 
pazzani  m.  and kibler  d. 1. the role of prior knowledge in inductive learning. machine learning 1-1. 
quinlan  j. 1. learning logical definition from relations. machine learning 1-1. 
sebag  m   and rouveirol  c. 1. constraint inductive logic programming. in de raedt  l.  ed.  advances in ilp  1. 1s press. 
sebag  m. 1. delaying the choice of bias: a disjunctive version space approach. in saitta  l.  ed.  proceedings of the 1th international conference on machine learning  1. morgan kaufmann. 
srinivasan  a.  and muggleton  s. 1. comparing the use of background knowledge by two ilp systems. in de raedt  l   ed.  proceedings of ilp-1. katholieke universiteit leuven. 
valiant  l. 1. a theory of the learnable. communication of the acm 1 1. 
webb  g. 1. further experimental evidence against the utility of occam's razor. journal of artificial intelligence research 1 1. 
zucker  j.-d.  and ganascia  j.-g. 1. representation changes for efficient learning in structural domains. in saitta  l.  ed.  proceedings of the 1th international conference on machine learning  1. 
	sebag & r o u v e i r o l 	1 
1 

1 

1 







1 

1 

1 

1 





