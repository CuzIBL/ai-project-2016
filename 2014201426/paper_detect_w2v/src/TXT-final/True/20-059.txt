 
　　reading cursive script involves elements of visual perception at one level of processing and those of language perception and understanding at a higher level. the problem is approached as one in which a word image is transformed through a representational hierarchy of levels based on descriptions that use points  contours  features  letters  and words. global control is hierarchical until an intermediate level after which it is heterarchical. two modes of learning are defined: supervised training with user feedback and unsupervised adaptation to the writer. 
l introduction 
　　reading cursive script is the problem of transforming text from the iconic form of cursive  continuous or running  writing into symbolic representation. there exist several early approaches to the problem  sayre 1 . the goal of this work was to develop a modern framework for reading a well-formed  prerecorded binary valued word image which is assumed to represent a word in a given lexicon. the problem is approached as of signal to symbol transformation between a series of representation levels: points  contours  shapes  letters  and words. the approach here lies in between two different methods of using lexical context in wotd recognition: one is segmentation followed by classification of segments using a lexicon to constrain alternatives  srihari 1  and the other is to classify words based on global word shape without any segmentation  hull 1 . 
il system oper a tion 
　　the system processes the input data hierarchically until a certain level  and the processing is heterarchical  interaction between levels  after that  erman and lesser 1   figure 1 . 
　　the raw image  after initial preprocessing of smoothing and slant removal is brought to the i mage level slant removal and smoothing operations eliminate any significant overall slant of the strokes  and remove minor contour discontinuities and roughness  bringing input to the i- e e . an input word  might  is depicted in it's /-level form in figure 1. the three main vertical zones of script - the middle one  where the  bodies  of all letters reside  and upper and lower ones  corresponding to ascenders and descenders are found by a reference-line-finding operation. 
a. presegmentation 
　　the next preprocessing step is to do loose segmentation  i.e.  identify each potential presegmentation point  psp . the task is to find as small a psp set over all words as possible  such that a subset sp is the set of true segmentation points between letters. a candidate 

psp may or may not be the actual psp  e.g.  the partial inscription corresponding to the first letter might could represent an m  ui  or in sequence  and we would like to have each of these potential letter borders marked as a psp. this operation results in the first letter  m  spanning three presegments  1   h spanning two presegments  1  and all the others one each  figure 1 . the fourth presegment  1  corresponds to no letter  and will be interpreted as such and indicated with a quote  1 symbol. 

figure 1. example of word might with reference lines and psf  
b. contour 	tracing 
　　preliminary processing yields reference lines  psps  and a smoothed and slant-corrected image at the 1level. the next task is to transform this to the ciontour -level which describes the image in terms of contours and their topology. the c-level representation of might is: 

there are six contours present: the main outside contour  followed by four inside contours   holes   within it  three in g and one in t  and another outside contour  the dot over 1. 
c. event detection 
　　the c-level is then transformed into the e vent ' level which is a description in terms of features  or events  and their locations  bouma 1 . this description of might records that there exists an i -shape  peak  between psp 1  a lower curve  1   a dot  1   upper curve  1   etc. the actual representation is a binary matrix of events and locations. the columns represent sixteen events  and rows represent their location in terms of presegments - either single ones or sequences of two. the l etter level consists of a series of alternative letter strings  ot prefixes  that account for parts of the original /-level. the top level output is an ascii word formed from a lexicon of legal possibilities  and such that it corresponds to the entire /-level representation. 
d. letter hypothesization 
　　until the e -level  successively more refined unique descriptions of the whole word image are generated. at the l-level there are multiple representations competing with each other  which are not all generated at once. competing letter hypotheses can account for exactly the same portion of the image  in which case there is only a classification difference between them; or  they can partially overlap thereby displaying segmentation differences. after a series of strict hierarchi-
cal transformations  letter hypothesization is the first global module that interacts with higher ones in a heterarchical manner  and as such  when called upon might not produce a set of full word representations on the l-level; rather  it generates partial representations of the size and at the positions that are required by higher-level modules. 
　　letter hypothesization consists of two parts: letter selection and rating computation. letter selection is an operation in which certain letter hypotheses are generated based on partial ♀-level data. in rating computation all of the e-level evidence is taken into account while generating an overall likelihood score for each hypothesis. for each selected letter / we compute the a posteriori probability p le  where e is the vector of all  abstract  events which have occurred. 
　　knowledge about letter formation is specified in three places: rules of presegmentation  which are procedurally fixed; tables of r'-values  one per context  total of nine   each one 1 x 1  number of events  in size  each element of which is a real number measuring the dependency between a letter and event in that context; and rules for event compatibility  which are built into the event detection process. the letter hypothesization scheme has both statistical and syntactic elements. within each ps  letters  or parts  are matched statistically. the syntactic process is reflected by the varying number of ps that different letters can span  since separate statistics are compiled for each of the different contexts. 
　　once events are detected  a search procedure examines the e-level matrix and proceeds in parallel through a lexicon represented as a trie  srihari 1 . in doing so it calls the letter hypothesization module. given a psp  the latter generates an l-level description  one letter long  of the portion of the word from that point to at most three points to its right. thus  starting from the left end of the word  or point 1  it generates the sequence of letter hypotheses. thus might is 
	srihari and bozinovic 	1 

represented by the list: 

each entry contains the letter  its rating on a  -1  1  scale  and the number of presegments it covers. expansion of the best current hypothesis is done by making its rightmost presegmentation point the starting point for new letter hypothesizing. 
　　since the current best hypothesis is m  which covers three presegments  the next letter hypothesization starts from point three with the /.-level description 

all these letter hypotheses are attached to the current top solution  m  from the right  and sent for lexicon lookup and rating recomputation  giving a list of prefix hypotheses. the representation here has the form  prefix  rating  ps length  letter ps length s  . actual hypotheses in iteration 1 for our example are 

therefore  the top hypothesis has been replaced by its high-rated descendants  and the whole list reordered according to descending score values. in this case  one of them  m  is the correct interpretation  but has fallen from first place  and will have to wait until it regains it to get expanded further. this happens as the hypotheses above it get expanded  and as the evidence supporting their extensions diminishes  they drop in ranking or disappear. by iteration 1  the correct candidate resumes top position  and retains it until iteration 1  as m'ig as seen below: 

letter h does not get a high rating  and migh drops again in iteration 1. finally  by iteration 1  all the other alternatives have diminished in rating themselves  the correct interpretation is topmost once again  and in the next step it yields a hypothesis that is a legal lexicon word  i.e. not only a prefix   and also accounts for the whole word  1 presegments   thus allowing the algorithm to teturn it as the answet. the final rating for might is of the form: 

　　two questions that need further explanation are:  l  how do certain combinations get preferred over others in a quest for a unique solution  this is done by way of direct comparison between these combinations - and preferably in a way that does not depend on their length or position. given an e-level description  the goal of the recognition procedure is to search the trie and come up with a full word as an answer. the 
search strategy employed here is a modified stack decoding algorithm  jelinek 1 .  1  when and where to ask for how many more letter hypotheses to be generated  this is solved by specifying an appropriate control mechanism. word hypothesization takes place left-to-right. the computation of ratings for prefix hypotheses is based on two requirements:  i  lengthuniformity of values  allowing for comparison on equal basis of hypotheses of different length  and  ii  length-independence of the computation  which is to be performed as a function of only the parent hypothesis' rating. 
hi. training and adaptation 
　　several parameters concerning the contour and feature levels are learned in an initial training phase. these parameters can be gradually altered with change of writers and styles in a process of adaptation. 
　　in training  the user provides feedback to the system in order to facilitate the learning of r -values. thus it is a case of supervised learning. for a given letter / and event e the estimation of these values is based on the formula: 
i 
this training scheme  while requiring the training set to contain all letters that are to be recognized  and in sufficient quantities to make statistical judgements about them  the selection of words themselves can be arbitrary  and isolated letter prototypes are not necessary. segmentation of a word into letters through presegmentation and feedback allows information from each letter to be gathered independently from each other. 
　　in the case of adaptation the identity of the letters has to be decided without outside feedback. such information is not present at the time of letter hypothesization  but it is available at the end of the recognition procedure in the form of the claimed word identity  the only remaining problem being the reconstruction of sets of relevant events for each occurred letter. such problems of learning from solution paths are not uncommon  sleeman 1 . its p string is always 

1 	perception 

retained  and is thus available also for the final word answer. this way  the increments for all the n-

counters can be precisely identified  and their values updated. an identical procedure is used in the training part  except that the word identity comes from a different source. in cases when a word is deemed unrecognizable  no counter updating takes place. when 
a new writer is installed  the counter values are reset at the original  unadapted  level  and subsequent adaptation for that writer takes place from there. v. summary 
　　the main results of this enterprise of developing a cursive script word recognition system are:  i  a scheme of knowledge interaction  with the appropriate control and data structures  and clealy identified representation levels was defined;  ii  a method for hypothesizing and rating certain objects from their 
 possibly unrelated  constituent features was developed 
- it is uniform over all features  objects  their locations                                        and sizes and is computationally simple;  iii  several iv. performancetwo lexicons were used in the experiments: most 

experiments were based on a small lexicon of 1 words and the rest on a lexicon of 1 words  kucera 1 . in the first experiment the training set consisted of. 1 words  where the writing naturally conformed with the constraints of being horizontal and not slanted. the test set consisted of another 1 words. with the small lexicon 1% were correctly recognized 
 first choice   1% incorrectly recognized  and 1% rejected. when the search procedure was extended to consider the first two returned words for the correct answer  the results were 1% correct  1% error and 1% rejection. the second experiment involved learning. intent learning  counter updating  took place during the first experiment  and upon its completion the acquired knowledge was invoked. after that  the same set of 1 words was run over again. in this case it is justified to run the retraining set as a test set  since it is combined with the original training set and in that sense not the only source of knowledge  and the feedback comes from inside. thus no improvement is guaranteed in advance  e.g.  wrong learning  in case of misrecognized words  could solidify the misrecognition of this input in the second pass  unless heavily counterbalanced by other evidence . the results: 1% correct  1% incorrect  and 1% rejected. in the third experiment  a second writer and the small lexicon were used. no slant restrictions were placed on the writing since the slant removal module was used. a total of 1 word samples was taken  eight of which were used for retraining  and the remaining 1 as the testing set. the retraining was performed in the same way as the original training  except that the counters obtained from this pass were used to update  in an unweighted manner  those from experiment 1  thus just slightly modifying the original revalues. the results: 1% correct answer in top two  1% incorrect  and 1% rejected. the results were similar when a third writer was used with 1 word samples and 1 used for retraining. 
image level operations were introduced/refined; and  iv  learning/adaptation capabilities for cursive script were distinguished. 
