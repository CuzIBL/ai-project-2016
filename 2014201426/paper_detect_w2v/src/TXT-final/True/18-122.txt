user modelling panel 
d. sleeman  stanford: moderator   doug appelt sri   kurt konollge sri   elaine rich mcc   ns sridharan bbn labs  & bill swartout isi . 
　
introduction 
　in various sub-areas of al we talk about  tailoring  the system's response to the user. nl systems and tutoring systems being two prime examples. additionally  some discussion of this issue arises in building explanation facilities tor expert systems. 
  how do they achieve this tailoring  how similar are the techniques used  short-term 
information   how do such user models differ from the plans inferred in planning systems  long-term information   how explicit are the user models even in systems which are able to adapt to the user  
even if the user population is very large  this is where data bases are important. 
user models capture many kinds of information about users. two important dimensions that characterize this information are shown in the following chart: 
	individual 	canonical 
	user 	user 
　
  how deep/knowledgeable do user models need to be  
  how is this sophistication dependent on the type of interaction  superficial conversation versus diagnostic/tutorial   the goal of the dialogue  the nature of the domain etc  
　in this panel we will review many of the areas in which some form of user model is used  look at commonalities of approaches  and seek to characterize when a particular approach is appropriate. 
user modelling: some approaches 
elaine rich 
　user modelling straddles the boundary between artificial intelligence and data base technology. it has all of the problems that each of these areas possesses; we hope it will also be able to draw on both areas for solutions. this double dependency arises from the interaction between the two main subproblems that user modelling must address: 
  how can models of users  their knowledge  goals  etc.  be inferred from their behavior and used in reasoning to improve the performance of a target system  this is where a.i. comes in. 
  how can models of a large number of users be maintained efficiently so that each is available when necessary but system performance does not degrade square 1 does not make much sense  but each of the other squares poses specific problems that user-modelling systems must address. 
　when user modelling is looked at from the a.i. point of view  the following issues emerge: 
  how can specific user plans be inferred from behavior   this relates to square 1.  doing this requires a system that is itself capable of forming plans but it also  since it is a diagnosis task and not a design task the way most planning problems are  requires a sophisticated matching procedure so that the plan that the user has selected can be isolated from other possible plans  
  how can user knowledge and planning strategies be inferred from behavior   this relates to square 1.  people's knowledge and their problem-solving strategies change quite slowly over time and should be remembered from one session to the next because they may substantially influence both the way the user will behave and the way the system should behave for maximum effectiveness. 
  how can general knowledge and planning procedures be represented and used effectively   this relates to square 1.  this is the standard a.i. question. 
when user modelling is looked at from the database point of view  the following issues emerge: 
　
  how can information about a large number of users be stored most efficiently   this relates to square 1.  is it more efficient to store each model separately or to store models as differences from some canonical model  is it better to organize the model around a particular user  clustering together everything known about that person  or is it better to arrange the model around a particular topic or piece of knowledge  clustering together what is known about all users with respect to that issue  
  how can a lot of detailed knowledge about a particular session be collapsed into a concise description of the knowledge that may be useful for later sessions   this relates to square 1.  
student models in intelligent tutoring systems 
d. sleeman 
　the field of intelligent tutoring systems identified the need for having a model a database which summarized the student's actions some time ago. the earliest adaptive cai programs often represented the student's level of sophistication by a scalar value. scholar  a program which discussed the geography of south 
america  was the first to use a more sophisticated representation 
- namely a semantic network. essentially  the knowledge of the domain was represented as such a network and each node had a numerical value associated with it indicating the likelihood that a particular student knew the knowledge associated with the node. this type of model is referred to as an overlay model. a differential model which simply reports the differences between an expert's and the student's knowledge was introduced in the west system. 
　all these systems assumed that the student's knowledge was merely a subset of the expert's. recent studies in cognitive science have shown this is frequently not a valid assumption  and so models which allow both the correct and incorrect knowledge to intermingle have been introduced. perturbation models have boon used by brown & burton in their debuggy system to model student's errors with arithmetic tasks  and by sleeman in lms/pixie to capture student's knowledge of algebra. 
　an important aspect of these latter models is that they are process models - and so can be executed by an appropriate interpreter - thus enabling them to be used predictively. both debuggy and pixie address the issue of inferring models by observing the student's performance on a series of tasks. technical issues addressed by these systems include how to make the search computationally tractable  and how to overcome noise  i.e.  spurious responses . additionally  pixie is addressing the issue of how to remove the closed-world assumption - making the systems truely responsive to the student's input. currently  modelling systems merely search - an albeit very large - model space generated by combining more primitive components  in pixie's case of correct and incorrect rules . 
d. sleeman et al. 1 
explanation & the role of the user model: 
how much will it help  
bill swartout 
　there seems to be a growing consensus among researchers in explanation and text generation that a solid  detailed user model  if we only knew how to build it  would significantly improve the kinds of explanations and texts we can produce mechanically. currently proposed system designs often call for a detailed user model that expresses what facts the system believes the user knows  how he likes to have information presented  and so forth. in such designs  presentation strategies use the model to select just the right thing to present to a user. is such a detailed model feasible  do people seem to have detailed knowledge of their listeners  this approach may place too much emphasis on the user model. it often seems that people do not have detailed knowledge of their listeners but instead rely on general  stereotypical knowledge and an ability to alter their explanation tactics when the listener appears not to understand. i would like to suggest that an explanation system that allows for feedback from the user about the understandability of explanations and that relies on a general user model expressing knowledge of stereotypes might be more feasible that one that depends on a detailed user model. 
the role of user modelling in language generation 
& communication planning doug appelt 
　the analyses of searle and grice clearly demonstrate that communication is a process of intended recognition of intention  whereby the speaker formulates utterances with the intention that the hearer use that utterance to understand the speaker's intentions that the hearer hold some different prepositional attitudes as a result of understanding the utterance. this intention recognition property is essential to communication - - if it is absent  then whatever activity is going on is something other than communication. 
　if a user perceives natural language being used as input and output to a system  it is very natural for him to assume that it is being used as a medium of communication  much the same as people use it among themselves. therefore  there is a very strong tendency for the user to impute intention recognition capabilities to the system and to assume that it is taking his own intentions into account. of course  most users of currently available natural language interfaces soon learn that this is not the case. the objective of research in communication planning is not so much being able to construct ever more complex sentences involving increasingly difficult semantic concepts  but rather to understand the processes of intention communication and recognition well enough to enable a system to participate in a natural dialogue with its user. 
　therefore a system that plans communication must have a very detailed model of the user. there are a large number of alternative means of representing the beliefs and intentions of agents  and the requirements of communication planning do not dictate what form such a representation must take  but rather dictates a set of requirements about what kinds of reasoning must be done. the following is at least a partial list of the representation and reasoning capabilities necessary for communication: 
  the ability to represent believe a  p   believe a  ~p   ~believe a  p . 
  the ability to represent all of the above with respect to mutual belief. 
1 d. sleeman et al. 
  the ability to represent all of the above with respect to intention. 
  the ability to deduce for any p whether or not a believes p  and similarly for mutual belief and intention. 
  given an individual  reason about what is believed or mutually believed about it. 
  the ability to reason about the effect of actions on belief  mutual belief  and intentions. must be able to reason for any act and proposition p about whether or not  act believe a  p  holds  and believe a   act p . 
　language production is not a faculty that can  in general  be isolated from the general reasoning processes of a system. natural communication requires knowing about the plans and goals of a speaker with respect to the entire task  and the ability to plan goals having to do with the communication process itself as well as the domain. therefore  it is impossible to take some existing system  add a user model  tack on a natural language font end and back end  and expect it to engage in natural communication. the need for communication must be in the mind of the designer from the beginning  with domain and communication reasoning incorporated as a consistent whole. 
user modelling  common-sense reasoning & the 
belief-desire-intension paradigm kurt konolige 
　user modelling is important wherever an al system must interact with human agents. i say here  human agents   but this is not necessarily meant to exclude other types of agents; as computer system become more complex  the same principles used for efficient communication with people will hopefully apply to artificial agents. indeed  in methodologies below i note that analyzing the communication requirements of artificial agents may lead to insights about communication in general. 
　i think it would be an understatement to say that current al systems which incorporate user models have a long way to go. this is not because too little attention has been paid to the problem  but simply because the problem encompasses   a significant part of current al research. there may be very restricted situations in which a crude parameter model  for example  a verbosity switch  is all that is necessary; but for the more open-ended dialogues that normally take place in questionanswering  explanation  and tutoring  to name a few application areas   a more accurate model of the user's cognitive state is required. i would like to give a personal view of some of the major lines of research that are being pursued or should be pursued to achieve a realistic user model. 
methodologies 
　at present  most models of cognition in al are variations of a 
　bdi  belief-desire-intention  paradigm. an agent has beliefs about the world  and desires some states of the world more than others. rational agents form intentions or plans to affect the state of the world to fulfill their desires  given the current state of their beliefs. this picture is a kind of commonsense psychology  and seems to be implicit in the way we use words like 'belief ' 'desire/ 'plans ' etc. hardly any work has been done on a general theory relating these cognitive components. still  the bdi paradigm is a useful general framework for constructing user models for particular applications. in many cases  it is possible to simplify the model considerably: fcr example  in question-answering on a database it is assumed that the user has a goal of extracting information  and the problem of forming intentions from conflicting desires does not arise. 
　while the bdi paradigm can provide an overall hatrack for organizing cognitive models  it does not tell us what particular hats we should put on it. agents' beliefs  for example  can be quite complicated  incorporating complex commonsense reasoning about space  time  physical systems  and so on  as well as particular beliefs about the domain at hand. how do we go about developing such theories  this might be called the knowledge problem for user modelling. there are two sources for such theories. one is the cognitive science path  in which attention to protocols of subjects can yield interesting insight into cognitive processes acting in complex environments. the other is in al planning systems: artificial agents whose cognitive structure is designed to solve a particular task. the former might be described as theory-poor but data-rich: the subjects actually do act intelligently in the domain  but the actual cognitive structures they employ are not accessible. the latter are theory-rich but data-poor: the design of the agent is useful as a theory of reasoning in the domain  but the agent may not actually act as intelligently as desired. so it would seem that both approaches are desirable  - for example  analyzing the way people use language yields data on desirable properties of a language-using system  while studying the requirements for efficient communication between agents can lead to a simplified model of communication that helps organize linguistic phenomena. 
the knowledge problem 
　at the very minimum  a useful user model for open-ended dialogues should include the following: 
   domain-dependent knowledge. this is the type of reasoning most often capture by expert systems  which are good at a very specialized type of problem in a narrowly defined setting. 
   theories of the commonsense world. this type of knowledge is tacitly assumed in all human communication. it includes areas such as: 
   intentionality and beliefs of other agents. 
   common-sense theories of time  space  and physical processes. 
   knowledge of the interaction process. this includes principles of efficient communication  such as  new information comes first   or  use the most specific applicable term.  
　there is much significant work being done in al on theories of this sort  e.g.  work on qualitative reasoning  modelling space and time  naive physics  logics of knowledge and belief  communication act theory  and so on. 
the inference problem 
　the most important inference problem for user modelling is the following: 
　given the observed behavior of the user  find the appropriate state of the model that accounts for the behavior. 
　note that this is a very different problem from performing inferences using one of the commonsense theories just mentioned. in general  the latter is a deduction problem: find the consequences of a given theory. the inference problem for user modelling is inductive: from a pattern of behavior  induce the correct structure that produces the behavior. much of the work in script- or frame-based systems addresses this problem. however i think it is a much more difficult inference problem  and deserving of much more intensive research. 
　
user modelling & plan recognition 
n.s. sridharan 
　a number of interesting and important questions have been raised for this panel. i wish to survey a small set of different tasks and show the diversity of responses possible depending on the characteristics of the task. i conclude by discussing a set of task dimensions which forms a framework for understanding  in a broad manner  the connections between tasks and user models. 
i. discussion of different domains 
　i.a automatic prompting and simple help on workstations 
　an implicit assumption is made that the user may wish to know some information relevant to the command to be issued; and that it wont hurt to display such information automatically. a sketchy finite state machine model can be used compute allowable actions  allowable operands and a canned help text can be put up on the screen. no detailed user modelling is used. 
　i.b tutoring introductory programming:  elliott soloway  yale  
　there is an intimate connection between plan recognition and user modelling. in fact  not viewing user-modelling as plan recognition has hampered progress in this field. the student must be seen as trying to follow a plan; a program that is being constructed is a realization of a plan. what the student is trying to accomplish  his goal is important; goal recognition is an important problem. an approach using bug catalogs or plan catalogs is inherently limited. that kind of approach will not go beyond small and simple programs. plan recognition must be viewed as a constructive task; plan revision approach is very important. it will be not enough to think of selecting a user plan from a finite set of pre formed plans.  for approaches to student modelling in tutoring domains where strong assumptions can be made about the student s goal at any stage  see the earlier section student 
models in intelligent tutoring systems.  
　i.c sensor signal interpretation:  c.f. schmidt  rutgers  
　the problem arises in connection with an interactive system to assist in interpreting multiple unreliable sensor signals. an implicit user model is used to effect  offline  tailoring of the system. only the pragmatic consequences of accepting a model of user needs to be represented. the model used need only be accurate enough to predict the right actions; that is  the model is viewed only in terms of it implications for the system. 
　i.d pilots assistant:  dick pew  bbn  
　in automating the cockpit display for a military aircraft  it is clear that the display function must be customized to the user. however  in this domain  plans of pilots can only be defined is vague terms; e.g. as phases of a mission  and a conditional set of responses plus model of goals to maintain their priorities. the pilot is operating in an extremely dynamic situation  and operates generally by adopting opportunistic behavior and reactive behavior. in this domain it appears that the user model is better structured in terms of attributes such as focus of attention  span attention  attention switching speed  memory limitations  speed of observation and assimilation. 
　i.e natural language dialog systems  candy sidner and jim schmolze  bbn  
　question-answering systems often limit themselves to dealing with individual questions separately; whereas  dialog systems attempt to include the context of the dialog so far. an important aspect of such contexts is a model of the user's intentions  capabilities  beliefs  knowledge  and preferences. formation of such a model is viewed as an incremental process. understanding communication requires accessing/hypothesizing the intentions. this process is one of forming a hypothesis  and thus is inherently error-prone. the process used should be robust enough to recognize errorful hypotheses and to take steps to rectify them. 
d. sleeman et al. 1 
　i.f office automation task: non-linguistic  non communicative domain  vic lesser  u of mass.  
　the user is engaged in a task such as filling out a purchase order for equipment. the system is watching over his shoulders  so to speak  and attempting to guide the user. the user may be in error; the system is watching to predict and correct the steps taken by the user. plan recognition relies mostly on domain-based heuristics; and is less dependent on modelling the user  his beliefs or plans. this is because the goal is to get the task accomplished rather than to train or educate the user. 
ii. framework for discussion of user modelling 
　very simple user models suffice for a number of tasks. in spelling correction a simple model of user errors  rather than plans  can be immediately helpful. in detecting errors in novices' program  a model of errors made frequently by novices  not their plans  may be very useful. 
　the attempt here is to set up a framework for exploring the analogies portrayed below. 
  user 	customization 	 -  	prediction  correction .modification 
  user models  -  plans incorporating beliefs  intentions and goals 
  user modelling process  -  plan recognition 
the purpose of the framework to be developed is to answer questions like: how complex should the user model be  what characteristics of the task are relevant in deciding how to acquire and use such user models  
dimensions to consider and evaluate in exploring this analogy 
1. richness of response space: assuming that the model is to guide a suitable action from a repertoire of actions  the user model must be  just  rich enough to 
guide choice of response  but should be minimal. if the potential responses are not diverse the model can and should be quite simple. 
1. static vs dynamic customization: it is useful to consider whether the programmer is customizing the program to the user  or whether the system is adapting itself. static customization may lend itself to implicit user models. 
1. who bears responsibility  system or user : how complicated can the system become  in an interactive situation  the user will be formulating a model of the system  while the system is modelling the user. if the system is simple enough  the user may be willing to take responsibility for his own actions  since he can more readily form a model of the system. if the system is taking the responsibility for overall behavior  then the system should model the user accurately.  black box vs glass box issue  
　
1 d. sleeman et al. 
1. risk or penalty for being wrong: plan recognition and user modelling is inherently error-prone  being an inductive task. one must judge the consequence of this. if the penalty for using a wrong hypothesis is high  one must either not attempt to model the user or be in a setting where interactive verification of such hypotheses is feasible. 
　iii. what is user modelling  what is in a user model  there are different considerations that affect how one views the process of user modelling: 
  static vs dynamic models  built-in vs acquired  
  focus on immediate actions vs focus on eventual goals 
  deterministic vs probabilistic models 
  predictive vs descriptive models 
　similarly one can imagine a variety of ways in which the user model is set up: 
  parameter models 
  state machine model  compute allowable actions; has a sense of history  
  recursive models  user's model of the system; system self-model  
  plan-based models  has a sense of goal . beliefs  intentions and knowledge attributable to the user. often these can be integrated in the form of a plan plus a context in which these plan are likely to be executed. p