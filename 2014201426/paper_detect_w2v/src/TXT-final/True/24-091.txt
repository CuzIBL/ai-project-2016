 
in this paper  we argue that a mobile robot's environment can be determined by computing local maps surrounding feature points  called fixation points. these fixation points are obtained by searching the scene for points which present some interesting cue for robot navigation. this 1-d computation is based on a monocular active vision system composed of a camera  mounted on a rotating table accurately controlled by a computer  which gazes the fixation point as the robot moves. the system then computes the local map and updates it with each new observation in order to increase its accuracy and robustness. real experimentation in a complex indoor scene illustrates that the 1-d scene coordinates can be obtained with a good accuracy by integrating several observations. 
1 introduction. 
robot navigation in a simple environment can be achieved through a two dimensional analysis in which a single camera is used to find free space and avoiding obstacles  kuhnert 1 . nevertheless  as soon the robot environment becomes too complex  such a system fails and the computation of 1-d structure seems necessary. usually  the determination of this 1-d structure in mobile robotics  is based on stereo or similar techniques which reconstruct the robot's complete environment  tsuji 1    ayache 1    brooks 1 . while such a 1-d representation can be extremely useful for some recognition or inspection tasks  the huge amount of features used for 1-d reconstruction requires complex and time consuming computations. 
the idea the authors want to stress in this paper  is that the robot navigation in a complex environment can be efficiently performed by computing not the whole structure of the environment  but only the structure surrounding a few selected feature points. navigation can then be considered as a goal-oriented task in which each goal represents the area surrounding the feature point detected in the scene. this paper  concerns the determination of the surrounding 1-d structure based on a new concept of robotics called active vision. aloimonos  aloimonos 1  has presented theoretical works for estimating structure from motion  shading and contour using a camera which gazes a feature point  called  fixation point   at its focal center. ballard  ballard 1  ballard 1   has developed the animated vision system which controls the vergence of a pair of cameras in order to keep the projection of the fixation point at the center of both the image planes. the 1-d structure of surrounding points is then computed. recently  sandini fsandini 1  also proposed a technique for determining 1-d information on the basis of constraints imposed by the active motion. 
 the concept of such an approach can be extended for estimating the local 1-d structure in the vicinity of fixation points for mobile robot navigation. our active visual system is composed of a camera mounted on a rotating table accurately controlled by a computer which gazes a fixation point while the robot moves. during the robot motion  assumed to be composed of either a pure translation or pure rotation centered on a fixation point  the projection of the fixation point is kept on the camera optical center. the 1-d structure of features surrounded this point is then computed. this local map is updated with each new frame by combining new data to improve accuracy and robustness. as soon this analysis is achieved and no more information is expected  a new fixation point is processed. figure 1 represents a robot navigation scenario in which the robot looks respectively at point a and b before rotating around point c and carrying on by looking at next fixation points of the scene. 

	figure 	1 
the selection of fixation points requires a lot of attention. 
such a point must either provide a cue for robot navigation 
stelmaszyk  ishiguro  and tsuji 

or represent a potential obstacle. with respect to these 1 constraints  we consider that a fixation point has to be selected in the scene with respect to geometric and photometric properties. geometrical properties consist in the detection of areas with respect to the robot's distance. each such an area is classified for determining the processing order. the closest area will be processed first and  after a while  the second nearest one and so on. the photometric properties  based on gradient  texture and color analysis  consist in pointing out the fixation point  inside the area  which allows an efficient  robust and easy detection under different angle of view or different illumination. 
for achieving the fixation point selection with respect to the above considerations  a static camera is added to our robot  see figure 1 . such a camera  equipped with a wide angle lens  allows the perception of all scene points in the image  the active camera's field of view is constrained to a small scene area by the short range lens . the 1-d structure of all these scene points  included the scale factor which corresponds to the distance to the fixation point  can be roughly estimated by a motion stereo technique. 

	figure 	1 
in this paper  we will demonstrate that the local map computation can be obtained with good accuracy. we will suppose the robot does not stop at each image acquisition and then consider the continuous robot motion. basic equations for reconstructing the 1-d position of scene points are developed in the next section. section 1 briefly presents the algorithm which integrates the different observation of the same fixation point from different robot position. real experimentation validate the technique in section 1. 
1 l o c a l c o m p u t a t i o n of 1-d structure. 
our method is based on the assumption that the robot moves either along linear or circular paths centered on feature points. an active camera is mounted on a rotating table  and the 1-d structure of scene points are computed up to a scale factor. the table controller uses visual feedback for keeping the fixation point at the image center while the robot moves. only the 1-d structure of vertical edge lines is computed due to the lack of a three axis rotation controller in our experimentations. such a limitation requires us to estimate scene points location in a 1-d top view representation. although such information is sufficient in indoor scene environments composed of many vertical lines  an extension of the equations developed in this paper to the general case is straightforward. 
let us assume a coordinate system x  y  z attached with respect to the camera  with the z axis pointing along the optical axis. let t =  u  v  w t be the translational component of the camera motion is and r =  a  b  c t be the rotational component . let p =  x  y  z t be the instantaneous coordinates of some point in the environment. the perpendicular component u of the instantaneous velocity is expressed by  bruss 1 : 
  1  
in which f is the camera focal length and  x  y  is p's projection in the image plane. 

	figure 	1 
let us assume that the robot moves linearly. by controlling the panning of the camera  the fixation point can be made to project always to the optical image center. location of scene points relative to the fixation point are estimated while the robot moves. the 1-d structure computation is based on an intermediate cylindrical representation  1 and l  which simplifies the presentation. as indicated on figure 
1  l is the distance between the fixation point and the object point and the angle formed by the center of projection  the fixation point and the object point as explained before  we consider a top view and do not deal 

with the height of scene points. since the robot moves on a flat floor the camera motion parameters are expressed by the following vectors:  
in which uo represents the linear velocity of the robot  co the camera angular velocity and  the angle between the camera axis and a perpendicular to the robot path. the general equation 1 can then be written as : 
	 1  
since the fixation point is fixed at the image center while the robot moves  both optical flow and position are equal to zero. from equation  1   we can compute the distance zf. 

the distance zs  for some other point in the field of view of the camera  is derived from equation  1 : 

and the computation of the ratio zs/zf  noted  allows the elimination of the robot velocity uo : 

the scene points in cylindrical coordinates are obtained by basic geometric relation in which d  as explain later  represents the scale factor: 
	 1  
	 1  
as mentioned before  the 1-d structure of scene points is estimated relative to the fixation point. if some external sensors can be used for accurately determining the distance between the fixation point and the camera  sandini 1   we prefer to consider that all scene point coordinates are expressed up to a scale factor represented by d in equation  1 . although such a scale factor limits the interest of our technique  it can be estimated with sufficient accuracy using motion stereo. 
in the case of rotational motion  it has been demonstrated 
 ishiguro 1  that we obtain the same equation for while the angle  is set to zero when computing l. but in both 
the cases  the equation  1  cannot be computed when  = 1. this situation occurs when the scene point is located along a line parallel to the image plane and passing through the fixation point. however  the determination of the distance l can be directly computed by using basic trigonometric 

	figure 	1 
while moving  the 1-d structure of all visible scene points are expressed in the fixation point centered coordinate system. by looking at figure 1 which represents the robot in two consecutive positions  the cartesian coordinates can be deduced by basic trigonometric relations : 

1 integration of the 1-d observations. 
even if the accuracy is rather good in the fixation point vicinity  the error is large enough to justify the use of techniques for decreasing it. in this section  we will briefly present how the different observations are combined and merged. 
the integration process is illustrated on figure 1. tokens 
 vertical edges in our application  tracked in consecutive images are reconstructed in the fixation point coordinates system and considered to us as a 1-d observation. the first 1-d observation is assigned into to the model and then updated at each new observation. each 1-d observation is composed of the 1-d coordinates of each scene point  as far 
stelmaszyk  ishiguro  and tsuji 

as its uncertainty  represented by a covariance matrix and its label number. 

if a false match occurs during the tracking  the 1-d reconstruction process may produce a 1-d observation with non-realistic value. this problem is overcame  in our case  by computing the mahanobolis distance between the 1-d model and the 1-d observation. a distance less than 1 means that the probability that the token corresponds to the same physical entity is greater than 1 % in the twodimensional case  ramparany 1 . the 1-d observation and the 1-d model are then merged based on an extended kalman filter which provides a new estimation of both the position and the uncertainty. each 1-d observation being expressed in the same frame coordinates  centered on the fixation point   the merging process can be performed without any geometrical transformation between two different observations. 
each 1-d element of the model also contains two factors. 
the first one is a confidence factor which represents the story of the sequence. this factor is incremented each time a match between the observation and the model is performed. a high value then indicates the element is present in the scene with a great confidence. the second factor is intended to deal with temporal perturbations  like vibrations  noise  shadow ...  which affect the computation of the 1-d element if the matching is done  this factor is incremented. otherwise  it is decrement and removed from the model if its value falls beneath a threshold and the confidence factor is not high enough to make sure that this element really exists in the scene. the final local map will be only composed with high confidence factor elements. 
1 results. 
the following experimental assessments have been performed in the case of linear robot motion. a camera is mounted on a rotating platform which swivels with a step of 1 degree. during the robot displacement  the platform is controlled so that the camera tracks a fixation point the first and last images of a sequence are shown in figure 1  the robot being successively located at the positions labeled 1 and 1. between each observation  the robot moves 1 cms. 
the equations developed in section 1  give the 1-d structure in the continuous case which assumes the robot doesn't stop between two acquisition. such an assumption requires special hardware for processing data in real time which was not available for these experiments. nevertheless  in the following  the continuous equations have been used for validating the approach although the robot displacement is expressed in term of distance instead of velocity. 
all image edge segments are tracked in the consecutive images of the sequence. each token contains a label number which allows the determination of the corresponding edge lines in two images separated by a large distance  crowley 1  stelmaszyk 1 . in our application  the distance correspond to 1 image frames  around 1 cms . for each pair of images  1  1  ...n&n+1   we perform the 1d reconstruction given in section 1. each 1-d reconstruction corresponds to the same physical scene viewed by a different point but represented in the same fixation point centered coordinates. 
 the figure 1 indicates the front view  left side  and the top view  right side  of the reconstructed scene when the robot has integrated twelve 1-d observations  images 1  1 ... 1 . the front view indicates the positions of segments which have been tracked at least 1 times in the different images  confidence factor = 1 . we can check that the fixation point is always located on the image center. in the lop view  the fixation point is represented by the intersection of the vertical and horizontal axes and the robot trajectory is given at the bottom. the lines joining the fixation point and the robot trajectory show the robot position and the camera angle during different acquisitions. scene points are shown as crosses followed by their label number. looking at such a label  one can find the corresponding segment in the front view  place it into the raw image 1 and check roughly the matching validity. nevertheless  the front view must be seen by positioning ourselves at the robot position. for instance  the point labeled 1  located on the upper right part of the top view  is effectively located on the left of the fixation point if we look at it from the robot's position. 
linear motion d = 1 cms  u = 1 cms/frame  f = 1 pixels  t = 1 frames point 
 lab  	x 	i 	z 	i 	l 
meas meas meas 
 cms   cms   cms  conf 
facto r x 
real 
 cms  z 
real 
 cms  error  cms  1 1 -1 1 1 1 -1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 -1 1 1 1 -1 1 1 -1 -1 1 1 -1 -1 1 1 1 1 1 1 1 1 1 	1 1 i 1 	1 1 1 1 1 table 1 


figure 1 
first and last raw images of the sequence. the fixation point is surrounded by the circle. 

figure 1 
left side: all vertical edges matched in the sequence. right side: top scene view. 
stelmaszyk  ishiguro  and tsuji 

table 1 gives the error and confidence values for coherent measurements found in figure 1. here  the scale factor has been assigned to the real distance in order to facilitate the comparison. this table shows that the error does not exceed 1 cms when the scene point is located at less than 1 meter from the fixation point if we ignore the point labelled 1 which was merged only 1 times. 
1 	conclusion. 
this paper presents a technique for computing 1-d structure  up to a scale factor  in the vicinity of a fixation point. this fixation point corresponds to a feature point detected in the scene which is gazed by an active camera located on a mobile robot. as the robot moves  the camera rotates so as to keep the projection of this fixation point at the camera center. instead of using the robot motion parameters in the computation of the 1-d structure  the authors have introduced the camera angular velocity. this information is measured accurately by a precise shaft encoder. 
by taking into account the error attached to each measurement  we combine different observations so as to increase both accuracy and robustness. the integration process is facilitated by the use of active vision which expresses the 1-d coordinates in an object centered coordinates system. such a representation is invariant with respect to the robot motion and merging two different robot position doesn't require any geometrical transformation. 
experimentations in a real and complex environment demonstrate that the error of measurements is less than 1 cms for points which have been merged at least five times. 
this experimentation validates the technique and the authors are working now on the integration of several local maps into a global one. such an integration requires the automatic selection of the fixation points as far as their positions. a motion stereo technique  based on the static camera mounted on the robot  should allow the computation of both the scale factor and the fixation point's position in the global map. although inaccurate  such an information will be sufficient for allowing the description of a global map represented in a path centered coordinates system  asada 1   zheng 1 . the main feature of such a representation it attached the origin directly to the robot path. each fixation point centered coordinates system will be then inserted in the path centered coordinates system. we also hope to develop a real time implementation of the 1-d structure computation equations in order to verify the validity of the continuous image acquisition's approach. 
