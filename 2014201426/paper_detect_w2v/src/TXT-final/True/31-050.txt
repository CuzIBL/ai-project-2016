 
the utility problem occurs when the cost associated with searching for relevant knowledge outweighs the benefit of applying this knowledge. one common machine learning strategy for coping with this problem ensures that stored knowledge is genuinely useful  deleting any structures that do not contribute to performance in a positive sense  and essentially limiting the size of the knowledge-base. we will examine this deletion strategy in the context of casebased reasoning  cbr  systems. in cbr the impact of the utility problem is very much dependant on the size and growth of the case-base; larger case-bases mean more expensive retrieval stages  an expensive overhead in cbr systems. traditional deletion strategies will keep performance in check  and thereby control the classical utility problem  but they may cause problems for cbr system competence. this effect is demonstrated experimentally and in reply two new deletion strategies are proposed that can take both competence and performance into consideration during deletion. 
1 introduction 
the traditional wisdom in knowledge-based systems has been that more knowledge is a good thing. however  in recent years this view has been questioned with demonstrations that certain  harmful  knowledge may actually degrade system performance  usually performance is taken as problem solving efficiency or time . this problem has been dubbed the utility problem  minton  1; tambe et al  1 . in case-based reasoning systems  cbr  a special case of the utility problem arises called the swamping problem ffrancis & ram  1b . the swamping problem relates to the expense of searching large case-bases for appropriate cases with which to solve the current problem. 
　one could argue that efficient parallel retrieval algorithms are the solution to the swamping problem; in other words by keeping retrieval time constant  increasing the case-base size no longer threatens system efficiency. while this is true to a degree  it is not currently realistic  and until massively parallel machines become the norm other strategies must be used; in fact  while massively parallel retrieval algorithms do help in delaying performance degradation  they do not eliminate it altogether. 
mark t. keane 
department of computer science  trinity college dublin  dublin  ireland. 
email: mark.keane cs.tcd.ie 
　markovitch and scott  have characterised different strategies for dealing with the utility problem in terms of information filters applied at different stages in the problem solving process  e.g.  acquisition  feature extraction  matching  learning . two common strategies which are especially relevant to the swamping problem are selective utilization and selective retention. selective utilization filters  markovitch & scott  1  prevent the problem solver from  seeing  certain knowledge items for some period of time  and are often implemented in cbr systems as indexing schemes  kolodner  1  or by time bounding the case-retrieval mechanism  brown  1; veloso  1 . in these approaches only a subset of the case-base is made available to the retrieval mechanism with the attendant disadvantage that certain cases may be missed at retrieval time. unfortunately  these omissions can result in unnecessarily complex adaptation stages or even problemsolving failures. as an alternative  selective retention filters permanently remove knowledge from the knowledge-base  holland  1; markovitch & scott  1; minton  1  and are represented in cbr systems by techniques that only learn certain cases or that delete cases that appear to be redundant; the selection of a case to delete is controlled by a deletion policy. an advantage with selective retention is that the whole case-base is available at retrieval time but it is critical that only high quality cases are stored to keep the case-base small and retrieval time at acceptable levels. 
　in this paper  we investigate a number of traditional deletion policies and their application to cbr. in section 1  we argue that such policies may not be directly applicable to 
cbr systems because they damage the competence potentials of such systems over time. section 1 introduces two new policies designed specifically with cbr in mind. these new policies recognise the possibility of competence degradation through case deletion and safe-guard against it by using an explicit model of case competence to guide deletion. finally  in section 1  before a concluding look at the applicability and appropriateness of our approach  we demonstrate its effectiveness with the aid of experimental results. 
1 coping with the utility problem 
machine learning research offers a variety of different deletion policies  most of which should work well to preserve performance in cbr systems  and hence limit the 
	smyth and keane 	1 

classical utility problem . however  they can result in a gradual but irreversible reduction in system competence. in short  to properly control the cbr utility problem  a 
deletion policy is required that preserves both performance and competence. 
1 traditional deletion policies 
a simple deletion policy is random deletion. according to this policy a random item is removed from the knowledgebase once the knowledge-base size exceeds some predefined limit. surprisingly  random deletion can work very well and can often be as effective as more principled  and expensive  methods  markovitch and scott  1 . 
　one such more principled approach is minton's utility metric  minton  1  --
utility= applicationfreq*averagesavings -matchcost 
- which chooses a knowledge item for deletion based on an estimate of its performance benefits. this utility deletion policy removes knowledge items with negative utility. although it is not necessary to directly limit the size of the knowledge-base  the frequency factor tends to indirectly control size because as the knowledge-base grows the application frequency of a particular item tends to drop and so its utility estimate degrades. by using this policy substantial performance improvements were observed in the prodigy/ebl system  minton  1 . similar policies have been employed elsewhere with equally successful results 
 markovitch and scott  1; markovitch and scott  1; keller  1 . 
1 problems with classical deletion policies 
cbr systems are prone to utility problems  especially swamping problems  in much the same way as other problem solvers. the hope is that deletion policies such as the above will transfer well to case-based systems. however  there is one important difference between pure cbr systems and the speed-up learning systems  epitomised by prodigy/ebl and soar  where random and utility based deletion have been successful. pure cbr systems do not have a first-principles problem solver; that is  without cases  cbr systems cannot solve problems at all. in contrast  the case-like knowledge of systems such as prodigy/ebl and soar serves only as speed-up knowledge. therefore  it is perfectly fine for speed-up learners to use performance related policies  like minton's utility metric  because no matter what knowledge is deleted  the system's competence is not altered; problems can always be solved from first-principles. however  these deletion policies can have disastrous results for case-based reasoners. the deletion of critical cases can significantly reduce the competence of a cbr system  rendering certain classes of target problems permanently unsolvable. 
　in cbr  all cases are not equal. some cases contribute mainly to the competence of the system and others may predominantly contribute to its performance. pivotal cases empower the system with its basic competence. auxiliary cases  on the other hand  only contribute to performance. the deletion of a pivotal case results in an irreversible reduction in the competence a cbr system. traditional deletion policies are not sensitive to these different categories of cases. consequently  such techniques can delete pivotal cases if they do not contribute to performance. the result is an irreversible drop in competence. 
1 competence-preserving deletion 
in this section  we describe how the competence of a cbr system can be modelled and how deletion policies can exploit this model to guard against competence depletion while controlling the size of the case-base in a manner that guards against the swamping problem. 
1 case competence categories 
we have found it useful to consider four basic classes of cases. first  there are pivotal and auxiliary cases that represent the extremes of our competence model. in addition  there are intermediate categories of spanning and support cases that correspond to cases whose deletion may or may not reduce competence depending on what other cases remain in the case-base. by categorising cases according to whether they are pivots  spanning  supporting or auxiliary  it is possible to obtain a picture of the case-related competence of a given system. 
coverage and reachability 
the key concepts in categorising cases are coverage and reachability. the coverage of a case is the set of target problems that it can be used to solve. the reachability of a target problem is the set of cases that can be used to provide a solution for the target. obviously  computing these sets for every case and target is impossible; the space of target problems is  in general  simply too vast. 
　a more tractable solution is to assume that the case-base itself is a sample of the underlying distribution of target problems. now  we can estimate the coverage of a case by the set of cases that can be solved by its retrieval and adaptation  and the reachability of a case by the set of cases that can bring about its solution  see definition 1 and 1 respectively . 

　figure 1 illustrates the different case categories in terms of their coverage  and reachability  sets. each case is numbered and its conesponding coverage set is labelled with the same number; for example  in figure 1 b   coverage 1 ={1  and reachable 1 ={ 1 . we will refer to these configurations in the coming sections. 
pivotal cases 
a case is a pivotal case if its deletion directly reduces the competence of a system  irrespective of the other cases in the case-base . using our above estimates of coverage and reachability a case is pivotal if it is reachable by no other case but itself  definition 1 . 
	definition 1: 	pivotal case 

　pivotal cases are generally outliers  being too isolated to be solved by any other case. consequently  target problems falling within the region of pivot can only be solved by that pivot. for example  in figure 1 a  two pivotal cases are shown  cases 1 and 1. it is clear that should either be deleted then at least two target problems cannot be solved  namely the problems corresponding to case 1 and 1. 
auxiliary cases 
auxiliary cases do not effect competence at all. their deletion only reduces the efficiency of the system. a case is an auxiliary case if the coverage it provides is subsumed by the coverage of one of its reachable cases  definition 1 . 
	definition 1: 	auxiliary case 

　auxiliary cases tend to lie within clusters of cases. if one is deleted then a nearby case can be used to solve any target that the deleted auxiliary could solve  so competence is not reduced. figure 1 a  has two auxiliary cases  1 and 1. the coverage of each is a proper subset of the coverage offered by case 1 and therefore any target problem that case be solved by cases 1 or 1 can also be solved by case 1. 
spanning cases 
spanning cases do not directly affect competence. they are so named because their coverage spaces link  or span  regions of the problem space that are independently covered by other cases  see definition 1 . if cases from these linked regions are deleted then the spanning case may be necessary. 
　in figure 1 b  case 1 is a spanning case with its coverage space joining case 1 and case 1 but offering no more coverage that 1 and 1 provide together. however it is important to realise that if case 1 is deleted then the spanning case is now necessary. 
	definition 1: 	spanning case 

support cases 
support cases are a special class of spanning cases and again do not affect competence directly. they exist in groups  each support providing similar coverage as the others in a group. while the deletion of any one case  or indeed any proper subset  of a support group does not reduce competence  the removal of the group as a whole is analogous to deleting a pivot  and does reduce competence. more formally  the definition of support cases is given below in definition 1. 
	definition 1: 	support case 

　furthermore  a set of cases c form a support group if they all provide the same coverage  definition 1 . 
	definition 1: 	support group 

　figure 1 c  shows a support group of three support cases. the removal of any two of these cases does not influence competence  the coverage offered by the third being equivalent to that offered by the deleted two. 
1 modelling case competence 
while computing the competence categories may be expensive they are only computed once as a start-up cost. strictly speaking  during future problem solving as cases are learned and deleted from the case-base  the case categories must be updated by recomputing the coverage and reachability sets of the appropriate cases and adjusting the categories accordingly. this is obviously too expensive to perform every time learning and deletion occurs. instead  we propose the following heuristics for efficiently updating the case categories. algorithm 1 outlines how the competence categories change as new cases are learned. 
learning update  target base : 
if pivotal base  then 
　remove the base from the set of pivotal cases add the base and target as a new support group and mark this group as pivotal in origin elself support base  then 
add the target to the base's support group 
elself spanning base  then 
make new support group from the base & target and mark this group as spanning in origin 
elself auxiliary base  then 
add the target to the set of auxiliary cases 
endlf 
algorithm 1. learning update 
	smyth and keane 	1 

　algorithm 1 describes what happens on deleting a case from the case-base. 

algorithm 1. deletion update 
　note that all of these procedures are estimates  they make the assumption that the space of target problems is accurately approximated by the case-base. if this is true then  as our experiments will show  an accurate picture of competence can be maintained and our new deletion policies work well. if it is not true then the effectiveness of these policies will deteriorate. 
1 the footprint deletion policy 
ideally a deletion policy should work to remove irrelevant cases thereby guiding the case-base towards an optimal configuration of cases  optimal in the sense that it maximises competence while minimising size . we call this chosen is the one which can be solved by the greatest number of existing cases  thus limiting the impact of the deletion on the real system competence. another  approach is to choose the case with the least coverage. 

algorithm 1. the footprint deletion algorithm 
1 combining competence & performance 
footprint deletion is not designed to eliminate the need for performance-based methods such as utility deletion. since it is only designed to consider competence we still need to deal with the performance aspect. indeed  one interesting use of footprint deletion is in collaboration with existing performance-based policies. for example  in partnership  footprint and utility based policies can constrain deletion to preserve both competence and performance. 
　combining footprint deletion and utility deletion is very simple; we will refer to the resulting hybrid strategy as 
optimal case-base the competence footprint  the competence is used within the sclectpivot  selectspanning  footprint-utility deletion  fud . first  the footprint method is used to select candidates for deletion. if there is only one such candidate then it is deleted. if  however  there a number of candidates  then rather than selecting the one with the least coverage or largest reachability set  the candidate with the lowest utility is chosen. in other words the utility metric 

footprint provides the same competence as the entire case-
base but with fewer cases. 
　the case categories described above provide a means of ordering cases for deletion in terms of their competence contributions. the auxiliary cases are the least important as they make no direct contribution to competence  next are the support cases  then the spanning cases  and finally the pivotal cases. this is the basis of the first of our new policies; as outlined in the algorithm below  algorithm 1  auxiliary cases are selected for deletion before support cases  which in turn are chosen before spanning and pivotal cases. 
　this is the basic footprint deletion  fd  strategy. in addition  further sub-strategies must be formulated. for 
example: which auxiliary case is chosen from the set of auxiliary cases ; given a number of equal sized support groups  which group is chosen and which case from this group is selected; finally  if a pivot must be deleted  then which one  one approach is to choose the candidate with the largest reachable set. this would mean that the case selectsupport  and selectauxiliary procedures referred to above; in this way utility estimates can be used to select between a number of alternative auxiliary cases or between a number of support cases belonging to the same support group or between a number of pivotal cases. 
　in fact  a disadvantage of using footprint deletion on its own is that low utility cases may be preserved while high utility cases with the same competence contributions may be deleted. by combining footprint deletion and utility deletion this problem is immediately solved because low utility cases will always be deleted instead of high utility cases with the same competence characteristics. 
1 experimental analysis 
our approach to controlling the swamping problem is to restrict the size of the case-base to ensure retrieval costs are kept within acceptable limits. that this strategy controls 

swamping should be clear. our objective in the following experiments is to provide empirical support for two claims. 
　first  that traditional deletion methods are subject to competence degradation whereas as the footprint policies are not. this is investigated in experiment 1 by imposing a swamping limit on the case-base and monitoring the competence of the system as different cases are deleted according to different policies. 
　second  that the footprint policies work towards a casebase that maximises competence while minimising size whereas the random and utility methods do not. this is investigated in experiment 1 by varying the swamping limit imposed on the case-base and again monitoring how systems competence develops. 
　the experiments were run on a simple cbr system for residential property valuation using a simple nearestneighbour retrieval algorithm; this domain has been introduced in smyth & cunningham . 
1 experiment 1 
an initial case-base of 1 cases and a set of 1 target problems were formulated. first  to test the basic competence of the case-base the target problems were presented to the system with no learning taking place. the result was a benchmark competence of 1% for the given case-base and problem set; that is  the test case-base was capable of solving all of the target problems. in the next studies  learning was allowed but the size of the case-base was restricted to 1 cases  the swamping limit. 
　four studies were carried out by using the same target problems but by varying the deletion policy. during each run  learning proceeded unhindered until the case-base size reached the swamping limit. subsequent learning could only take place after a case had been removed from the case-base. 

　figure 1 shows the results obtained. the first thing to notice is that the two competence preserving policies  footprint deletion  fd  and footprint-utility deletion  fud  managed to maintain overall competence at its benchmark level of 1%. in contrast  as predicted  the random deletion  rd  and utility deletion  ud  policies resulted in significant drops in competence as important cases were deleted over the course of the experiment; almost immediately on reaching the swamping limit  after 1 trials  competence begins to drop as crucial cases are removed  and their absence renders certain target problms unsolvable. in the end overall competence had fallen to 1% and 1% of its former level for the utility and random deletion policies respectively. 
1 experiment 1 
in cbr systems it is generally desirable to optimise the competence provided by the case-base. that is  a deletion policy should maximise the competence of the case-base while minimising its size. so it is not enough that the deletion policy simply guard against the deletion of cases that contribute to competence. it is also necessary to ensure that if a competence contributing case must be deleted  that this case is carefully selected to minimise the inevitable competence loss; alternatively  if a case can be added to the case-base then the one chosen should provide the greatest competence contribution. the four different deletion policies are tested for this property in the following experiment. 
　the previous experiment were re-run a number of times with a different swamping limit imposed each time and the results are shown in figure 1. the swamping limit is varied from 1 cases to 1 cases. after each re-run the overall competence is noted and graphed. 

　clearly the two footprint policies are optimising competence whereas the traditional policies are not; the curves of the former rise to the maximum benchmark level much more rapidly that the curves of the latter. this means that as more and more cases are allowed into the case-base  the two footprint policies seek out those which make the largest competence contributions. in contrast  because the traditional random and utility based methods have no understanding of competence  a competence contributing case is only stored in the case-base if it is beneficial from the performance viewpoint. 
　in fact  the qualitative nature of the footprint deletion and footprint-utility deletion results can be explained in terms of the distribution of pivotal  spanning  support  and auxiliary cases in the case-base. table 1 shows this distribution in the 
	smyth and keane 	1 

　for the residential property cases an optimal case-base can be constructed from all the pivotal cases plus one case from each support group; that is  to achieve benchmark competence the case-base must contain at least 1 cases  all of the pivots plus 1 of the supports  one from each group . so one would expect that if the ideal deletion policy  ideal from the point of view of preserving competence  constrains the case-base towards this optimal competence configuration  then competence should reach the benchmark maximum once the swamping limit facilitates a case-base of at least 1 cases. this is what happens in the above experiment. in figure 1  both footprint deletion and footprint-utility deletion reach about 1% of the benchmark competence once the swamping limit is just over 1 cases  whereas the random and utility policies only reach about 1% competence at this same swamping limit. 
1 conclusion 
this paper demonstrated that while traditional deletion policies are effective in controlling the swamping problem from a performance perspective  they may lead to competence degradation in many cbr systems. the solution proposed uses a model of case competence to guide the learning and deletion of cases. we outlined a suite of algorithms for constructing and efficiently maintaining this competence model at runtime  and two new deletion policies  footprint deletion and footprint-utility deletion  were presented which can preserve competence by referring to this model at deletion time. the preliminary experimental results are promising in demonstrating that the competence estimates did prove useful in preserving the actual system competence. in particular  the hybrid footprint-utility deletion method should enjoy the same performance advantages as the traditional utility methods while at the same time guarding against competence degradation. 
　the main assumption on which the new policies are based is that the case distribution is a good representation of the target distribution. if the case distribution is not a reliable estimate of the target distribution then the competence model will not be reliable. consequently  the effectiveness of the competence deletion policy will be limited. it is our contention that  while not every domain may be characterised in this way  many can and by exploiting this property we can safe-guard against the utility problem while at the same time avoiding the possibility of competence degradation. 
　our competence modelling approach may also be used during the initial case acquisition stage of system development. it is often undesirable to store every available case in the initial case-base; for one thing there is the utility problem and secondly irrelevant cases may introduce noise into the retrieval stage and lead to the selection of suboptimal cases or difficulties in tuning the similarity metric. by modelling the competence of the available casts an optimal initial case-base can be formed  providing a more manageable source of problem solving expertise. 
