 
　　this note describes two methods of assigning p r i o r i t y scores to p a r t i a l l y developed hypotheses about a speech utterance for determining which hypotheses to extend further. these methods guarantee the discovery of the best matching interpretation of the utterance  when used in an appropriate control framework. although presented in the speech context  the algori thms are applicable to a general class of optimization and heuristic search problems. the density method is especially interesting since it is not an instance of the general a* al gorithm of hart  nilsson  and raphael  and appears to be superior to it in the domains in which it is applicable. proofs of the guaranteed discovery of the best interpretation and some empir ical comparisons of the methods are g iven. 
	1. 	introduction 

　　this paper is concerned with control strategies governing the formation and refinement of p a r t i a l hypotheses about the identity of an utterance in a continuous speech understanding system. we assume a system that contains the following components: 
o  a lexical ketrieva 1 component that can find the k b est matching words starting or ending at any given po i n t in the utterance for any number k  and can be reca l i e d to continue enumerating word matches in decreasing order of goodness at a given position. we assume that this component is inter faced to appropriate signal processing  acoustic-phonetic and phonological analysis components as in  woods et a l .   1   and that it assi igns a  quality  score to each word match reflecting the good ness of the match. 
b  a linguistic component that  given any sequence of words  can determine whether that sequence can be parsed as a possible i n i t i a l   f i n a l   or internal subsequence of a syntactically correct and semantically and pragmatically appropriate utterance  and can propose compatible classes of words 
the hwim speech understanding system developed at bbn  woods et a l .   1; wolf and woods  1  has such c a p a b i l i t i e s . a control strategy for such a system must answer questions such as: 
a  at which points in the utterance to c a l l the lexical retrieval component  and when  
b  what number of words to ask for  
c  when to give subsequences of the results to the linguistic component  
and 
d  when to recall the lexical retrieval component to continue enumerating words at a given point. 
the goal of the control strategy is to d iscoyer the best scor inc; sequence of words that covers the entlre utterance and ts acceptable to  the lingutsttc component. 
we w i l l consider here a particular class of control strategies which we refer to as  island-dr iven . 
	1* 	island-driven strateg ies 
　　in an island-dr iven control strategy  p a r t i a l hypotheses about the possible i d e n t i t y of the utterance are formed around i n i t i a l  seed words somewhere in the utterance and are grown into larger and larger  island hypotheses by the addition of words t o one or the other end of the island. occa sionally  two islands may   c o l l i d e   by pro posing and discovering the same word in th e gap between them and 
may be combined i nto a single larger island. each such island hypothesis is evaluated by the lexical retrieval component to determi ne i t s degree of match with the acoustic ev idence and checked for syntactic  semant i c   and pragmatic consistency by the linguistic component. we w i l l refer to a p a r t i a l hypothesis that has been so evalu ated and checked for consistency as a  th eory . the strategies that we w i l l c onsider operate by successively proces sing  events  on an event queue  where events correspond to suspended or dorma nt processes that may result in the creati on of theories. 
　　the general algorithm operates as follows: 
at each end of such a sequence. 	continuation 	events  	which 	can 	be 
	natural 	lanptia pe-1: woods 
1 　　 1  an i n i t i a l scan of the utterance is performed by the lexical retrieval component to discover the n best matching words anywhere in the utterance according to some c r i t e r i o n of  best  and for some value n. an i n i t i a l seed event is created for each such word and placed on the event queue. in addition  one or more processed to continue the enumeration of successively lower scoring words  are created and placed on the queue. each seed event is assigned a p r i o r i t y score  derived from the guality score-  that the lexical retrieval component gave it in one of several ways to be described s h o r t l y     and each continuation event is assigned a p r i o r i t y score that can be guaranteed to bound the p r i o r i t y score of any word that can be generated by that event   e . g .   derived from the score of the last word enumerated prior to the continuation . the events are ordered on the event queue by their p r i o r i t y scores and are processed in order of p r i o r i t y . 
　　 1  the highest p r i o r i t y event is selected for processing  which consists of  i  creating the corresponding theory  a one-word theory in the case of a seed event     i i   calling the linguistic component to check the consistency of the theory and to make predictions for words and/or word classes that can occur adjacent to i t     i i i   calling the lexical retrieval component to enumerate the k best matching words satisfying the predictions at each end of the theory  and  iv  generating a  word  event for each such word found. a word event is an event that w i l l add one word to a theory to create a larger theory. continuation events are also created that w i l l continue the enumeration of successively lower scoring words adjacent to the theory. if i s l a n d - c o l l i s i o n is permitted as an operation  then each word event generated is checked against an island table to see if the same word  at the same position in the input  has been proposed and found in the other direction by some theory  and if so  an   i s l a n d - c o l l i s i o n   event is created that w i l l combine the new word and the two theories on either side of i t . both word and i s l a n d - c o l l i s i o n events are assigned p r i o r i t y scores derived from the quality score of the new word and the scores of the theories to which it is being added and are inserted into the event queue according to their p r i o r i t i e s . 
　　 1  continue selecting the top p r i o r i t y event from the event queue  step 1  u n t i l a theory is discovered that spans the entire utterance and is syntactically  semantically  and pragmatically acceptable as a complete sentence. 
　　the main topic in this paper is the assignment of p r i o r i t y scores to the events in the above algorithm in order to guarantee that the f i r s t complete theory found w i l l be the best scoring one that can be found. using the quality scores assigned by the lexical retrieval component d i r e c t l y as p r i o r i t y scores does not o r d i n a r i l y provide such a guarantee. 
1
	   	i♀*♀ ′j1＜ lt♀♀ii ♀c＜ li!1 method 
	1.a  	assumptions 
　　the s h o r t f a l l method assumes that the quality scores assigned to word matches by the lexical matching component are additive  so that theories are appropriately assigned scores that are the sums of the scores of the word matches contained in them. it also assumes that word matches have associated beginning and ending positions that correspond to boundary positions in the input utterance. in the hwim system  the quality scores are logarithms of estimates of the relative p r o b a b i l i t i e s of the correctness of a theory given the acoustic evidence. 
	1.b  	the basic shortfall 
scojlidji. procecture 
　　let t   i   be the time in milliseconds of the i-th boundary in the utterance; nsegs  the number of segments in the utterance; and seg i  be the region of the input utterance from t   i - l   to t   i   for i from 1 to nsegs. 
　　for a word match from position i to j with score q  we w i l l allocate in some systematic way the total word score q to the segments seg i-h  . . . seg j  covered by the word match. for this discussion  let us allocate it proportional to the durations of the segments. 
　　for a given utterance  we w i l l determine for each segment seg i  the maximum score max i  that can be allocated to that segment by any word match that covers the segment. the score for any word match from i to j w i l l hence be bounded by the sum max i+l + . . . +max j   and the maximum score for any complete theory w i l l be bounded by t the sum from 1 to nsegs of max i . 
　　every p a r t i a l theory w i l l consist of a contiguous sequence of word matches spanning a region from some boundary i to some bounda r y j . each such theory w i l l carry with it two scores m and q  where m is the sum of the max i  for the segments covered by the sequence and q is the sum of the word scores of the theory. we w i l l assign each theory a p r i o r i t y score p * t - m + q  which can be thought of as the maximum to tal score t for any theory minus the shortfal1 from this ideal to which one i s committed by choosing this particular sequence of words   i . e .   p * t- m-q  . alternatively  it can be thought of as the estimated best possible future scor e consisting of the score q 
natural lanp;uap:e-1: woods 1 which has already been achieved for the region cove red plus the best potential score t-m for the region not yet covered   i . e .   p = qmt-m    . because t-m is an upper bound on the possible score that can be achieved on the region not covered  the p r i o r i t y scores p have the characteristic that they are non-increasing as theories grow. 
　　in the s h o r t f a l l scoring strategy  the p r i o r i t y scores of the individual seed events are simply the s h o r t f a l l scores of the words. a p r i o r i t y score for a continuation event that w i l l be an upper bound on the p r i o r i t y score of any words that might result from the continuation can be computed as follows: since the lexical retrieval component enumerates words in decreasing order of score  the quality score of any word that results from the continuation w i l l be no greater than that of the last word enumerated so far. moreover  we can derive from the lexicon 1 lower bound on the length of a 
　　word and from this we can deduce the shortest region of the utterance that such a word could cover  and hence the smallest possible m score that such a word could have. from these two numbers  we can bound the p r i o r i t y score  t-m+q  of any future word and use that as the p r i o r i t y score of the continuation event.  this bound is somewhat conservative  and in actual practice  it should be possible to derive a much tighter bound  but this argument is sufficient to guarantee that such a bound can be computed.  
continuation events for finding lower scoring seeds or lower scoring words to add to the ends of islands  w i l l already have fallen low enough in i t s p a r t i a l score  q score  that no possible match sequence in the remaining region of the utterance can bring i t s total score above that of the spanning theory. also  the presence of the continuation events in the queue makes the search process complete in the sense that any word in the vocabulary would be enumerated if the process were continued long enough. thus there is no possible word sequence across the utterance that would not be considered by this search algorithm if it were run s u f f i c i e n t l y far. hence  any complete theory of the utterance w i l l have a s h o r t f a l l  m-q  at least as great as that of the f i r s t complete theory discovered. since a l l spanning theories have the same maxscore m = t  it follows that the f i r s t spanning theory also has the maximum possible quality score  q  of any spanning theory. 
	1.e  	notes 
　　note that the process can be continued to obtain the second best complete theory  and so on. note also that the admissibility holds for this method whether the process is l e f t - t o - r i g h t   i . e .   seeds only at the l e f t end of the utterance  or middle-out  seeds anywhere in the utterance   and that it does not require the island c o l l i s i o n feature. 

　　as new theories arise from processing events that link an existing theory with a new word match  the m and q scores of an event and the new theory that it w i l l create are simply the respective sums of the m and q scores of the old theory and the word being added to i t . thus  after assigning an m score to a word match by summing the max numbers for the segments that it covers  the m score of any new theory that includes it can be computed by a single addition. 
	1.d  	admissibi 1 i ty of  the method 
ci aim: 
　　the f i r s t complete spanning theory found by the s h o r t f a l l scoring method w i l l be one of the best scoring complete theories  there could be more than one  that can be found by any strategy   i . e .   the algorithm is  admissible  in the conventional terminology . 
proof ; 
　　at the time the f i r s t complete spanning theory has been processed  every other event on the event queue  including the s h o r t f a l l method works with almost any type of grammar. it makes no assumptions that the grammar is f i n i t e - s t a t e   as do most markovian strategies. in the middle-out modes  it does require the l i n g u i s t i c consultant to have a parser  such as the bidirectional atn parser in the current hwlm system  that can take an arbitrary island fragment in the middle of an utterance and judge whether it is a possible subsequence of an acceptable sentence. in practice  it also helps immensely if the parser can also use the grammar to predict the acceptable words and classes adjacent to the island  and if the lexical retrieval component can use such predictions to constrain i t s search  as in hwim   but this is not essential to the formal admissibility of the algorithm. 
	1   f  	avoiding 	duplicate 	theories 
natural 	lanrnnpp-1: woods 
1 　　note that in the middle-out  island-driven strategies there are many d i f f e r e n t ways of eventually arriving at the same theory. for example  if we have an island w with a possible word x on the l e f t and a possible word y on the r i g h t   
then we can f i r s t form the theory  xw  and then  xwy  or we can form the theory  wy  and then d e r i v e  xwy  from t h a t . which of these two routes is taken w i l l depend on the scores of the words  but it is q u i t e p o s s i b l e   i n f a c t   l i k e l y   t h a t i n the course of working toward a complete theory a s t r a t e g y w i l l a r r i v e at the same subtheory several d i f f e r e n t times by a l t e r n a t e r o u t e s . 
　　if we do not include checks for the d u p l i c a t i o n of t h e o r i e s   then we would o f t e n get two copies of the same t h e o r y . these would forever d u p l i c a t e the same p r e d i c t i o n s and theory f o r m a t i o n s   g i v i n g r i s e to a r a p i d e x p o n e n t i a l e x p l o s i o n of the search process. if we i n c l u d e a t e s t each time a theory is formed to determine whether t h a t theory has been formed p r e v i o u s l y   tnen we can avoid t h i s e x p o n e n t i a l process. in f a c t   if each time we are about to put an event on the 
event queue we check the event to see if the set of word matches t h a t it uses is the same as that of some other e v e n t   then we can t e r m i n a t e t h i s d u p l i c a t i o n before making the e n t r y on the queue and consuming the queue space  and c e r t a i n l y before c a l l i n g the l i n g u i s t i c component to 
check 	it 	out 	and 	make 	f u r t h e r pred i c t i o n s   . 
　　the check for d u p l i c a t i o n among a l l the events t h a t have been created can amountto a c o n s i d e r a b l e amount of t e s t i n g if done in a b r u t e force exhaustive t e s t   although it can be c o n s i d e r a b l y reduced by i n d e x i n g events by t h e i r beginning and end p o i n t s or other t r i c k s . however  if one can r e l y on the events being generated in the order determined b y the basic s h o r t f a l l s t r a t e g y   then the f o l l o w i n g simple check based only on the word matches at each end of an event can be used to determine whether an event is redundant   i . e .   w i l l produce the same theory as some event already g e n e r a t e d   : 
　　if the new word is at the l e f t end and has the same or greater s h o r t f a l l as the word at the r i g h t end  then t h i s event i s redundant. 
     if the new word is at. the r i g h t end and has s t r i c t l y g r e a t e r s h o r t f a l l than the word at the l e f t end  then t h i s event is redundant. 
the argument for the v a l i d i t y of t h i s t e s t is as f o l l o w s : 
　　in the search space we are c o n s i d e r i n g   it 	is 	p o s s i b l e   	w i t h o u t 	a 	check 	for 
d u p l i c a t i o n   to d e r i v e a given theory w i t h words w     w 1 # . . . r w u in 1 k ~ i d i f f e r e n t ways one corresponding to each of the p o s s i b l e b i n a r y d e r i v a t i o n t r e e s s t a r t i n g w i t h some one of the w- as a seed  and then s u c c e s s i v e l y adding words e i t h e r to the r i g h t or the l e f t end.  proof e i t h e r w-  or w  was chosen l a s t   hence there are two ways to d e r i v e a s t r i n g of l e n g t h k for every p o s s i b l e d e r i v a t i o n of a s t r i n g of l e n g t h k - 1 . there is one p o s s i b l e way -- i . e .   as a seed -- to d e r i v e a s t r i n g of l e n g t h 1.  of a l l these d e r i v a t i o n t r e e s   the f i r s t one t h a t w i l l be found is the one t h a t uses the w. w i t h the s m a l l e s t s h o r t f a l l as a seed  and at subsequent steps adds the b e t t e r   i n terms of s h o r t f a l l   of the two words at e i t h e r end  assume for the moment t h a t no two of the words have e x a c t l y the same s c o r e   . hence  any d e r i v a t i o n t h a t attempts to add a word to one end of an i s l a n d when t h a t word has a smaller s h o r t f a l l than the word at the other end of the i s l a n d w i l l be d u p l i c a t i n g a theory t h a t has a l r e a d y been d e r i v e d  or at l e a s t a l r e a d y has an event for it on the event queue . in the case of two competing seeds w i t h the same s h o r t f a l l or words at each end of an i s l a n d t h a t have the same s h o r t f a l l   we have a r b i t r a r i l y picked the l e f t m o s t as the p r e f e r r e d one  which we w i l l p e r m i t the a l g o r i t h m t o f o l l o w f u l l y   and we block the d e r i v a t i o n of d u p l i c a t e s from the other one. thus  if we have a word being added to the l e f t end of a theory t h a t has the same s h o r t f a l l as the word at the r i g h t end  then t h i s event is redundant  since the p r e f e r r e d order w i l l generate an e q u i v a l e n t event t h a t adds the l e f t end word f i r s t . 
　　thus  a very simple check between the score of the word being added to a theory and the score of the word at the other end o f the theory w i l l s u f f i c e t o e l i m i n a t e 
　　the f o r m a t i o n of redundant e v e n t s . 
	1.g  	fuzzy wor d matches 
　　the above d i s c u s s i o n does not e x p l i c i t l y mention the problem o f f i n d i n g the same word in e s s e n t i a l l y the same place but w i t h s l i g h t l y d i f f e r e n t end p o i n t s and d i f f e r e n t scores. we have observed t h i s kind of output from the l e x i c a l r e t r i e v a l component of hwim and indeed f i n d it d e s i r a b l e to know the degree of v a r i a t i o n p o s s i b l e in the end p o i n t s of a word match and the a p p r o p r i a t e d e g r a d a t i o n in score for each. however  i t i s w a s t e f u l t o give s e v e r a l d i f f e r e n t events to the l i n g u i s t i c component  a l l of which are adding word matches to a given theory t h a t d i f f e r o n l y i n t h e i r e n d p o i n t s and s c o r e s . for t h i s reason  we have i n t r o d u c e d a s t r u c t u r e t h a t groups together m u l t i p l e e q u i v a l e n t word matches i n t o a s i n g l e e n t i t y c a l l e d a .fuzzy word match  or   f u z z y   for s h o r t     which ts given the score of i t s best member. a theory c o n t a i n i n g fuzzy word matches 

na tural 1 lan♀ua*e  1: woods 1 

actually represents a class of grammatically equivalent theories and carries the score of the best one. 
　　when an event is created to add a word match to a theory containing a fuzzy word match at that end  the score of the event must be computed using a   r e c t i f i e d   score that takes into account the best member of the fuzzy that is compatible with the new word   i . e .   has boundaries that hook up to the new word and s a t i s f i e s appropriate phonological word boundary constraints . in general  when several fuzzies are adjacent  the best compatible sequence of members must be chosen  and when the new word match is i t s e l f a fuzzy  the best combination of one of i t s members with a corresponding r e c t i f i e d score for the theory must be taken. the event is thus given the score of the best of the grammatically equivalent  non-fuzzy events for which it stands. 
it word matches returned by the lexical 
retrieval component are grouped into fuzzy matches whenever possible  and word events are given appropriately r e c t i f i e d scores  then the above admissibility result s t i l l holds   i . e .   the f i r s t complete theory processed w i l l be the best . the only difference  aside from the elimination of separate processing for grammatically equivalent theories  w i l l be that certain word events   i . e .   those using a less-than-best path through the existing theory  w i l l be formed earlier than they otherwise would have. however  these events w i l l s t i l l be placed on the queue with the correct score so that they w i l l reach the top and be processed in exactly the same order as they would in the strategy without fuzzies. 
	1.h  	discussion 
　　the s h o r t f a l l scoring method is similar in some respects to the well-known branch and bound  technique  except for the characteristic in the middle-out version that the same partial interpretation may be reached by many different paths  and the fact that the space of possible solutions is determined by a grammar. it can also be modeled as an example of the a* algorithm of hart  nilsson  and raphael 
  for finding the shortest path through a graph  where  in this case  the nodes in the graph are partial interpretations of the utterance  and the connections in the graph correspond to the seed and word events. consequently  it shares with that algorithm a certain kind of optimality that hart  nilsson  and raphael prove. it is simpler than the general a* algorithm  however  in that we are looking for the best scoring node  and we are not interested in scores of paths leading to that node  in fact a l l such paths have the same score in our case  . the simple argument given previously suffices to show the admissibility of the s h o r t f a l l method  whereas the general a* algorithm is more complicated. 
　　measuring the s h o r t f a l l from any p r o f i l e that is a per word upper bound of quality score would be sufficient to assure the theoretical admissibility of the method. however  the tightness of the upper bound affects the number of events tried and p a r t i a l theories created in the search for a successful interpretation   i . e .   the  breadth  of the search . by assigning the upper bound as a segment-by-segment p r o f i l e determined by allocated shares of actual word match scores  a f a i r l y tight upper bound is achieved. a further effect of scoring the s h o r t f a l l from such a maxscore p r o f i l e is that the score differences in different parts of the utterance are effectively leveled out  so that events in a region of the utterance where there are not very good scoring words can hold their own against alternative interpretations in regions where there are high scoring words. this promotes the refocusing of attention from a region where there may happen to be high scoring accidental word matches to events whose word match quality may not be as great  but are the best matches in their regions. thus  an apparently satisfactory and i n t u i t i v e l y reasonable strategy for focusing attention emerges from the same strategy that guarantees to get the best scoring theory f i r s t . 
　　when using the shortfa 1 method for understanding an utte ranee  the overwhelming tendency is for an event adding a new word to an isla nd to pick up additional s h o r t f a l l and f a l l some distance down in the queue  the result is that other events are proces sed before any additional work is done on that island .  occasionally  the new wo rd is the best word in i t s region and buys no additional s h o r t f a l l   but this is a r a r i t y .   the distance that this new even t f a l l s down the queue is determined by the amount of additional s h o r t f a l l that it has just picked up and the shortfalls of the events that are competing with it on the queue. this distance d i r e c t l y affec ts the degree of   d e p t h - f i r s t   vs. breadth-first  processing done by the algor ithm. if the new word scores w e l l   the ev ent f a l l s only s l i g h t l y   and few  if a ny  alternate events are processed before i t . in this case the algorithm is re l a t i v e l y depth f i r s t . if the new word scor es badly  the event f a l l s further down t he queue  many more alternative events have p r i o r i t y over it and the algorithm is more breadth f i r s t . 

natural 	langua pe-1: woorfs 
1 

　　the above characterization is only an i n t u i t i v e approximation  since  the actual number of events processed before the new event is considered depends on the number of new events that w i l l be generated by the intervening events that w i l l s t i l l score higher than this one. in some cases  the number of such events can be extensive. the general e f f e c t   however  is that the s h o r t f a l l scoring method provides a dynamically varying combination of d e p t h - f i r s t and b r e a d t h - f i r s t search which is determined by the r e l a t i v e q u a l i t i e s of the events that are in competition. 
1. density scor ing with island collisions 
　　density scoring consists of using the score of an event divided by the duration of the region covered by the event as the p r i o r i t y score. one way to view t h i s strategy is to consider again the task of estimating the expected score to be achieved in the region not covered by an event and consider estimating this score as a d i r e c t extrapolation of the same score per millisecond that has already been achieved by the event -- i . e .   add to the current score an estimated potential score consisting of the score density of the current event times the duration of the region not covered by the event. since the resulting t o t a l estimated score is just the score density of the event times the t o t a l duration of the utterance  and the t o t a l duration of the utterance is a constant  we can compare only the score densities of the events themselves and achieve the same decisions. 
　　when we think of the score density as an extrapolation of the score already achieved by an event into the region not covered by the event  we are c l e a r l y no longer obtaining an upper bound on the possible future score of an event  and the previous proof of a d m i s s i b i l i t y used for the s h o r t f a l l method no longer applies  in p a r t i c u l a r   whereas the s h o r t f a l l is a monotonically increasing function as an island grows  the score densities can get smaller when a bad word is picked up and then get larger again as the theory grows and picks up better words  averaging the score of the bad word over a larger d u r a t i o n . thus  it is not true that the score density of descendants of an event must be no greater than that of the event i t s e l f . 
　　however  when used w i t h the i s l a n d c o l l i s i o n f e a t u r e t h a t a l l o w s one t o combine t o g e t h e r in one s t e p the word l i s t s o f two d i f f e r e n t e v e n t s t h a t are n o t i c i n g the same word from o p p o s i t e 
　　d i r e c t i o n s   the d e n s i t y method a l s o g u a r a n t e e s t h a t the f i r s t complete t h e o r y found is the b e s t o n e . to prove t h i s c l a i m   we must use a d i f f e r e n t argument than f o r the b a s i c s h o r t f a l l s t r a t e g y . the argument depends on the a b i l i t y to d e r i v e the same t h e o r y in d i f f e r e n t ways from d i f f e r e n t seeds - - i . e .   the m i d d l e - o u t c o n t r o l s t r a t e g y i s e s s e n t i a l f o r the a d m i s s i b i l i t y o f the d e n s i t y s c o r i n g method. 
lemma: 
　　using the m i d d l e - o u t d e n s i t y s c o r i n g method and using i s l a n d c o l l i s i o n e v e n t s   any t h e o r y c o v e r i n g any r e g i o n of the u t t e r a n c e can be d e r i v e d by a sequence of e v e n t s a i l of which have a score d e n s i t y n o l e s s than t h a t o f the t h e o r y i t s e l f . 
p r o o f : 
　　by i n d u c t i o n on the number of words in the t h e o r y .  1  the h y p o t h e s i s is t r i v i a l l y t r u e f o r one-word t h e o r i e s b y means of a seed e v e n t .  1  suppose we have a t h e o r y of k + 1 words w i t h d e n s i t y d. let w be the word in the t h e o r y w i t h the l o w e s t score d e n s i t y  or one cvf them if t h e r e are s e v e r a l s u c h     and l e t x and y be the word sequences on e i t h e r s i d e of w  one of which may be e m p t y   . then the d e n s i t i e s of b o t h x and y must be at l e a s t as g r e a t as d and the d e n s i t y of w must be no g r e a t e r than d. if e i t h e r x or y is empty   i . e .   w is at one end    then by the i n d u c t i v e h y p o t h e s i s   the o t h e r has a d e r i v a t i o n using e v e n t s o f d e n s i t y n o l e s s than i t s own and hence no l e s s than d and the word event to add w w i l l have d e n s i t y d. if w is not at the e n d   then by the i n d u c t i v e h y p o t h e s i s   b o t h x and y have d e r i v a t i o n s u s i n g e v e n t s o f d e n s i t y n o l e s s than t h e i r own  and hence no l e s s than d . t h e r e f o r e   b e f o r e events o f d e n s i t y l e s s than d can reach the t o p of the s t a c k   b o t h of the t h e o r i e s x and y would have been p r o c e s s e d   and b o t h would have n o t i c e d the word w from o p p o s i t e s i d e s ; hence  a n i s l a n d c o l l i s i o n event would have been c o n s t r u c t e d f o r the combined t h e o r y and would have the combined d e n s i t y d . 
c o r o l l a r y : 
n a t u r a l 	lanpuak -1: 	woor s 　　when a spanning theory of some density has been found by the middle-out density scoring method with island c o l l i s i o n s   any spanning theory of higher density could have been completely derived using events of higher density  and thus would have been found before the theory in question. hence  the f i r s t complete spanning theory found by the density scoring strategy using island c o l l i s i o n s w i l l be one of the best possible i n t e r p r e t a t i o n s . 
1.
	 	s h o r t f a l l density 
　　the above proof of the a d m i s s i b i l i t y of density scoring makes no assumptions about the scoring metric whose density is being taken other than that it be a d d i t i v e . hence  the density method can be applied to either the o r i g i n a l q u a l i t y score assigned by the lexical retrieval component  or to the local s h o r t f a l l described previously  giving rise to strategies which we refer to as q u a l i t y density and s h o r t f a l l density  respectively. i n i t i a l experimental comparison of the algorithms  see sec. 1  suggests that the s h o r t f a l l density method is superior to either the q u a l i t y density 
or the s h o r t f a l l method alone. 
1   	other 	heuristics 
　　in addition to the basic choice of p r i o r i t y scoring metric used tor ranking the event queue  there are additional heuristics that can be used to improve the performance of the island-driven strategies without loss of a d m i s s i b i l i t y guarantees. two of these are the use of  ghost  words  and the selection of a preferred d i r e c t i o n for events from a given theory. 
1 . a  	ghost words 
　　the ghost words option is a feature that can be added to any island-driven strategy without affecting the a d m i s s i b i l i t y of the strategy to which it is added. every time a theory is given to the l i n g u i s t i c consultant for evaluation  proposals are made on both sides of the resulting island  unless the island is already against one end of the utterance . although events can only add one word at a time to the island  and this must be at one end or the other  eventually a word w i l l have to be added to the other end  and that word cannot score better than the best word -that was found at that end the f i r s t time. the ghost words feature consists of remembering with each event the l i s t of words found by the lexical retrieval component at the other end and scoring the event using the best of the ghost words as well as the words in the event proper. the result is that bad p a r t i a l interpretations tend to get bad twice as f a s t   since they have essentially a one-word look-ahead at the other end that comes free from the l i n g u i s t i c consultant each time an event is processed. on the other hand  an event that has a good word match at the other end gets c r e d i t for it early  so that it gets processed sooner. the ghost words feature  thus  is an accelerator that causes extraneous events to f a l l faster down the event queue and allows the desired events to rise to the top faster. experimental use of t h i s feature has shown it to be very e f f e c t i v e in reducing the number of events that must be processed to find the best spanning event. 
	1.b  	choo lqsl a♀ preferred 	direction 
　　when a theory is evaluated by the l i n g u i s t i c consultant  predictions are made at both ends of the i s l a n d . when one of the events resulting from these predictions is later processed  adding a new word to one end of the i s l a n d   the predictions at the other end of the new island w i l l be a subset of the predictions previously made at that end of the old i s l a n d . in general  words found by t h i s new island at that end w i l l also have been found by the old i s l a n d   and if the score of the new island is s l i g h t l y worse than that of the old island  the normal s i t u a t i o n     then the strategy w i l l tend to revert to the old island to t r y events picking up a word at the other end. this leads to a rather f r u s t r a t i n g derivation of a given theory by f i r s t enumerating a large number of d i f f e r e n t subsequences of i t s f i n a l word sequence. 
　　since any eventual spanning theory must eventually pick some word at each end of the i s l a n d   one could a r b i t r a r i l y pick either d i r e c t i o n and decide to work only in that d i r e c t i o n u n t i l the end of the utterance is encountered  and only then begin to consider events in the other d i r e c t i o n . this would essentially eliminate the duplication described above  but could cause the algorithm to work into a region of the utterance where the correct word did not score very well without the benefit of additional syntactic support that could have been obtained by extending the island further in the other d i r e c t i o n for a while. 
　　without s u f f i c i e n t syntactic constraint at the chosen end  there may be too many acceptable words that score f a i r l y well for the correct poorly scoring word to occur within a reasonable distance from the top of the queue. by working on the other end  one may tighten that constraint and enable the desired word to appear  although t h i s can never cause a better scoring word to appear than those that appeared for the shorter i s l a n d   . 
natural 	language: 	woods 　　a flag in the hwim system causes the algorithm to pick a preferred or  chosen  d i r e c t i o n for a given theory as the d i r e c t i o n of the best scoring event that extends that theory  and to mark the events going in the other d i r e c t i o n from that theory so that they can only be used for making tighter predictions for words at the chosen end. this is accomplished by blocking any events for one of the ghost words at the inactive end of an event it that event is going counter to the chosen d i r e c t i o n . this blocking  alone  eliminates a s i g n i f i c a n t number of redundant generations of d i f f e r e n t ways to get to the same theory. an even greater improvement is obtained by rescoring the events that are going counter to the chosen d i r e c t i o n by using the worst ghost at the other end rather than the best ghost. since only word matches that score worse than any of the ghosts at that end arc being sought by these events  this is a much better estimate of the potential score of any spanning theories that might result from these events. 
　　the effect of rescoring the events in the non-chosen d i r e c t i o n using the worst ghost is that  in most cases  these events f a l l so low in the event queue as to be t o t a l l y out of consideration. only in those cases where there was l i t t l e syntactic constraint in the chosen 
d i r e c t i o n and the worst matching word at that point was s t i l l quite good  do these events stay in contention  and in those cases  the use of the worst ghost score provides the appropriate ranking of these events in the event queue. 
	1. 	empir ical compar i son of 
-t!1♀ dt f e r . e n t strateg ies 
　　in the hw1m speech understanding system  approximations to the above algorithms have been implemented. the major approximation is that continuation events are not. implemented  but instead the i n i t i a l values of n and k are chosen large enough that one believes that the correct i n t e r p r e t a t i o n of the utterance is found before any of the continuation events would have reached the top of the queue. if such is the case  then a l l of the decisions made by the approximation arc the same as those of the admissible theoretical algorithm  and hence the f i r s t complete theory found w i l l s t i l l be guaranteed to be the best. 
　　details of the hwlm system and i t s general performance are found in  woods et a l .   1 . comparative performance results on a set of 1 utterances for the s h o r t f a l l  s    s h o r t f a l l density  sd    and q u a l i t y density  qd  scoring strategies are shown in table 1 below. the option of using the q u a l i t y score  1  alone as a p r i o r i t y score is given for comparison. 
　　these experiments were run using the ghosts  i s l a n d - c o l l i s i o n   and preferred d i r e c t i o n heuristics with a resource l i m i t of 1 theories to process before the system would give up with no response. the ten sentences used for the test were chosen at random from a test set of 1 recorded sentences. 
　　although a test set of only ten utterances is admittedly too small  1 believe that the trends indicated in thefigure are generally correct. 
s p e c i f i c a l l y   while the quality density alone leads to a spanning interpretation in r e l a t i v e l y few theories  it does so without any expectation of getting the best i n t e r p r e t a t i o n . in this case  only two out of three of i t s answers arc correct. a l l of the other methods spend additional e f f o r t in making sure that the best i n t e r p r e t a t i o n is found  and consequently found fewer spanning interpretations within the resource l i m i t a t i o n . we did not t r y running the q u a l i t y scoring strategy beyond the f i r s t i n t e r p r e t a t i o n to see if a better i n t e r p r e t a t i o n could be found since  among other things  it is not clear when to terminate such a process. running in this mode  one could easily enumerate more theories than the other methods and s t i l l not have any guarantee that the best i n t e r p r e t a t i o n had been discovered. 
　　none of the admissible algorithms found incorrect i n t e r p r e t a t i o n s   so the r e l i a b i l i t y of their interpretations when they get them is 1%  providing the acoustic phonetic analysis of the . input utterance does not cause some incorrect i n t e r p r e t a t i o n to score higher than the correct one  a s i t u a t i o n that occurs sometimes in the hvim system  but is not a factor in this experiment . 
unfortunately  the s h o r t f a l l strategy alone is so conservative in doing this that it f a i l e d to find any interpretations within the resource l i m i t . both of the density methods are c l e a r l y superior to the straight s h o r t f a l l method. 
　　the s h o r t f a l l density strategy ranked superior to the q u a l i t y density strategy in terms of the number of events that needed to be processed to find the f i r s t spanning i n t e r p r e t a t i o n and consequently found more correct interpretations within the resource l i m i t a t i o n s . 
	the effects 	of 	the 	island 	c o l l i s i o n 
 c    	ghosts 	 g    and preferred d i r e c t i o n 
 d  heuristics are shown in table 1  where sd+1 means s h o r t f a l l density without c o l l i s i o n s   ghosts  or chosen d i r e c t i o n   sd+c means s h o r t f a l l density with island c o l l i s i o n s   e t c .   . the inclusion of a heuristic does not always guarantee that the system w i l l understand an utterance in fewer theories  but the pooled results shown  note especially the series sd+1  sd+g  sd+gd  sd+gdc  suggest that the successively added heuristics produce improvements in both accuracy and number 

natural lan uare-1: woorls 

o l t h e o r i e s r e q u i r e d .  note t h a t our a d m i s s i b i l i t y r e s u l t s h o l d o n l y f o r the sd+c and sd+gdc cases above.  1  c o n c l u s i o n 
　　he have p r e s e n t e d two b a s i c p r i o r i t y s c o r i n g methods  s h o r t f a l l and d e n s i t y s c o r i n g   t h a t p r o v i d e a d m i s s i b l e search s t r a t e g i e s f o r f i n d i n g the best matching i n t e r p r e t a t i o n of a c o n t i n u o u s speech u t t e r a n c e . moreover  the two methods can be used in c o n j u n c t i o n   and the combined method appears to be more e f f i c i e n t than e i t h e r of the methods by t h e m s e l v e s . although the methods are presented here in the c o n t e x t of speech u n d e r s t a n d i n g   analogous methods should be a p p l i c a b l e to other p e r c e p t u a l t a s k s such as v i s i o n w i t h a p p r o p r i a t e g e n e r a l i z a t i o n s o f segment  w o r d   and p h r a s e . the d e n s i t y s c o r i n g s t r a t e g y i s e s p e c i a l l y i n t e r e s t i n g since it is not an i n s t a n c e of the a* a l g o r i t h m and  at l e a s t f o r the speech u n d e r s t a n d i n g p r o b l e m   appears to be s u p e r i o r to the c o r r e s p o n d i n g a * a l g o r i t h m  the s h o r t f a l l method  in the number of hypotheses t h a t need to be e x p l o r e d to o b t a i n the optimum s o l u t i o n . i t a p p a r e n t l y g a i n s t h i s s u p e r i o r i t y b y i t s a b i l i t y t o work o n d i f f e r e n t p a r t s o f the s o l u t i o n i n d e p e n d e n t l y and combine them by the mechanism of i s l a n d c o l l i s i o n . the d e n s i t y method i s o f course not a p p l i c a b l e to as wide a c l a s s of problems as the g e n e r a l a* a l g o r i t h m   but should be a p p l i c a b l e to any search problem where scores are accumulated from p a r t i a l hypotheses a s s o c i a t e d w i t h some analog of a r e g i o n . in p a r t i c u l a r it can be 
g e n e r a l i z e d t o t w o - d i m e n s i o n a l r e g i o n s f o r such problems as v i s i o n  woods  1 1   . 
hart  	p.  	n. n i l s s o n   	and 	b. raphael 
 1  
 a 	formal 	basis 	for 	the 	heuristic 
determination of minimum cost 	paths   	iebe trans. sys 	sci 	cybernetics  	j u l y   
v o l . ssc-1  no. 1  pp. 1. 
wolf  j . j . and w.a. woods 	 1  
 the hwim speech understanding system   conference record  ieee i n t e r n a t i o n a l conference on acoustics s♀eech and signal pfocessing  h a r t f o r d   conn.  may. 
woods  w.  m. bates  g. brown  b. bruce  c. cook  j. klovstad  j. makhoul  
b. nash-webber  	r. schwartz  	j. wolf  v. zue 	 1  
 speech understanding systems - final technical progress report   1 october 1 to 1 october 1  bbn report no. 1 vols. i-v  bolt bcranek and newman i n c .   cambridge  ma. 
woods  w.a. 	 1  
 theory formation and control in a speech understanding system with extrapolations towards v i s i o n     proc. oj wor kshop on 
computer vis ion systems  u n i v e r s i t y of massachusetts  amherst  june. 


table 1 . the e f f e c t s o f i s l a n d c o l l i s i o n s   g h o s t s   and d i r e c t i o n p r e f e r e n c e . 
n a t u r a l lanfnage- 1 : wonh* 
1c 












1 

1 

1 







