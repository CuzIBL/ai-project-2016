shape recognition and illusory conjunctions 
geoffrey e. hinton and kevin j. lang 
computer science department 
carnegie-mellon university 
pittsburgh pa 1 
a bst ract 	and-egg problem can be solved by using a cooperative computation in 

one way to achieve viewpoint-invariant shape recognition is to impose a canonical  object-based frame of reference on a shape and to a parallel network. the computation performs a parallel  iterative search which settles on both the reference frame and the shape description at the same time. 

describe the positions  sizes and orientations of the shape's features relative to the imposed frame. this compulation can be implemented in a parallel network of neuron-like processors  but the network has a tendency to make errors of a peculiar kind: when presented with several shapes it sometimes perceives one shape in tlie position of another. the parameters can be carefully tuned to avoid these  illusory conjunctions  in normal circumstances  but they reappear if the visual input is replaced by a random mask before the network has settled down. treisman and schmidt  1  have shown that people make similar errors. 
1. introduction 
people can recognize objects from a wide range of viewpoints  even if they have never seen them from these precise viewpoints before. there is psychological evidence  rock  1; marr and nishihara  
1  that people do this by imposing an appropriate frame of reference on the object to be recognized. they then redescribe the retinotopic features of the object relative to the imposed frame and thus they get an object-based description that docs not depend on viewpoint. for example  given a square tilted at 1бу  a person can impose an object-based frame tilted at 1бу. relative to this frame  the object has horizontal edges at top and bottom and vertical sides  so it is a square. alternatively  a vertical frame can be imposed and all the edges will then be seen as diagonal and the object will be recognized as a diamond. the fact that we can see the same shape in either way is good evidence that we impose object-based frames in order to separate out the effects of viewpoint from the intrinsic shape of the object. 
in general  it is difficult to choose the appropriate frame to impose on a collection of retinotopic features even if we have already solved the problem of segmenting out a set of features that all belong to a single object we would like to use the frame that leads to a familiar object description  but the frame is required in order to access object descriptions so it is hard to see how object descriptions can give topdown guidance in picking the frame. when we see an upside-down capital r  for example  wc recognize it by imposing an upside-down frame of reference  but how do wc know to use this frame before we have recognized it as an r  hinton  1a  suggests that this chickenthis paper describes a computer simulation which supports that suggestion and it shows that the network makes a peculiar kind of error when it is presented with several shapes which arc then removed before the network has had time to settle on a percept of any one of them. the error consists of rccogni/ing one shape in the position of another. the network was not designed to produce such errors. rather  they appear to be an inevitable consequence of this method of shape recognition and it actually requires considerable fine-tuning to stop them from occurring in normal circumstances  i.e. when the input is not removed prematurely . 
1 illusory conjunctions 
treisman and schmidt presented subjects with cards containing a row of three colored letters surrounded by two black digits. the cards were presented for about 1ms and were then replaced by a mask of random features. the task was to first report the two black digits by saying a two digit number and then to report the positions and colore of as many letters as possible. subjects often made errors which consisted of the shape of one letter in the position of another. they were sometimes very confident in these errors  claiming to actually see the illusory conjunction rather than simply guessing. also there was no distance effect. if a letter changed position it was as likely to move two places away as one. these results are counter-intuitive. why should people get a clear percept of a combination of shape and location that isn't in the stimulus  and why is there no distance effect  
1 an overview of the network 
the network used in the simulation contains four different kinds of units. at the highest level there are single units that stand for specific letters. these are connected to units that stand for object-based features - strokes or junctions whose position and orientation are described relative to the appropriate object-based frame. each objectbased feature is connected to all the retinotopic features that could depict it. each of these connections is gated by a  mapping unit  that represents a particular choice of object-based frame  see figure 1 . 
when a single mapping unit is active it opens up connections that pair each retinotopic feature with exactly one object-based feature. so if 

this research was supported by a grant from the system development foundation. 

бб1letters could also take on the cola of other letter ; in the display wc shall not mention color again  but it fits the model wc present provided there is a central location 
for representing the coloi of the object currently being perceived 

the frame of reference is known in advance the rctinotopic features can he made to activate the appropriate object-based features by simply turning on the right mapping unit and inhibiting all the others. 
hach pairing of an activated object-based feature and an activated rctinotopic feature sends activation to the corresponding mapping unit  so if the shape is known in advance it is easy to find the reference frame by simply activating the object-based features of the shape. if the shape contains f features there will be f correct pairings of an activated object-based feature with an activated rctinotopic feature. these f pairings will all send activation to the same mapping unit and it will therefore be able to win a competition among the mapping units even though some of its rivals receive input from other spurious pairings of activated object-based and rctinotopic features. 
when neither the reference frame nor the shape are known in advance  the network will still settle to a consistent state when presented with a single familiar letter. initially many different mapping units will be active and the activity in the object-based units will represent the superposition of many different ways of mapping the retinotopic features. however  combinations of object-based features that correspond to familiar letters will receive top-down support from the letter units and so they will be enhanced. once this happens  the mappings that led to these features will be enhanced because the input to a mapping unit depends on the product of activity levels in the object-based and rctinotopic units. this mutual enhancement is a non-linear cooperative process that eventually leads to one set of object-based features and one mapping unit becoming dominant. 
the image 
	g. hinton and k. lang 	1 
a different  but equivalent  view of the network is that the retinotopic units gate the connections between the mapping units and the objectbased units. kach reunotopic feature is consistent with various possible conjunctions of a mapping with an object-based feature and so it allows all such pairs to support each other. the network settles into a state which allows as many of these pairs as possible to be active  subject to the constraint that only one mapping unit must be active in the final state. this view is helpful in understanding illusory conjunctions. it is the retinotopic features which determine the consistency between object-based features and mappings. if they are removed before the network has finished settling  there is nothing to prevent the object-based units and the mapping units from settling to inconsistent states. 
1. the simulation 
to test the claim that a parallel model of shape recognition can explain the psychological evidence on illusory conjunctions  a simulation of such a model was performed on a symbolics 1 lisp machine. a network of continuous valued  neuron-like units was presented with the rctinotopic features of several letters  and the activation levels of the units were repeatedly updated starting from balanced initial values and using a deterministic  synchronous relaxation algorithm until a letter had been located and identified. when allowed to run to completion  the network never made any errors. however  when a random mask replaced the image before the network was finished  illusory conjunctions occurred. 


retinotopic units 	object-based units 
figu re 1: this shows four types of unit and how they are connected. 
only the units and connections relevant for one particular input are shown. 

1 	g. hinton and k. lang 
1 structure of the network 
the units of the net are organized into four sets that are called planes to emphasize their spatial interpretation. the retinotopic plane contains units that represent retinotopic features that could be extracted by low-level visual processing. the mapping plane contains units that represent pairings of rotations and translations which are used to map the retinotopic image into a standard orientation on the object plane so that the image can be identified. the mappings used in this simulation do not perform scaling  so the letters presented on the retinotopic plane must all be of the same size. units on the retinotopic  mapping  and object planes are connected by the special three-way links that arc describe above. each letter unit is connected to the features on the object plane which define the shape of the letter. 
1.1 the retinotopic plane 
the features on the retinotopic plane are based on an imaginary 1 by 
1 array. the two types of features that are represented by units on the plane arc strokes and junctions. a stroke is defined to be a horizontal or vertical line segment whose length is 1. 1. 1  or 1 pixels. there arc 1 units of this type on the retinotopic plane. junctions occur at the endpoints of segments. a corner occurs when two strokes meet at a 1 degree angle at their endpoints. a t-joint occurs when the endpoint of one stroke meets the interior of another stroke at a 1 degree angle. any endpoint that is not a corner or a t-joint generates a free-end feature. every junction unit also has an orientation of up  down  right  or left  so that the t-joint on the left side of an e is different from the t-joint on the top of an i. there are a total of 1 junction units on the retinotopic plane. it is redundant to have both stroke units and junction units  since the information in either set can be derived from the other  this redundancy is crucial when the network needs to map several letters onto the object plane at the same time without confusing them. the junctions contain information about which stroke goes with which other stroke and similarly the strokes contain information about which junctions go together. every unit in the network has a real-valued activation level between zero and one  but the units on the retinotopic plane arc always clamped to zero or one in this simulation. 
figure 1 shows the display used to illustrate the state of the network. 
retinotopic units arc drawn on a 1 by 1 array of squares that suggests the image from which the features could have been extracted. stroke units are drawn as stippled rectangles with the same length  orientation  and position as the strokes which they represent. corner units arc drawn as white triangles in the corners of the squares. t-
joint units arc shown by white triangles at the sides of the squares. free-end units are drawn as small white squares on the ends of the strokes. the orientation of each junction unit is represented by the orientation of its symbol in the obvious way. 
1.1 the object plane 
the object plane is basically just a smaller version of the retinotopic plane. it uses 1 stroke units and 1 junction units to encode all possible features in its 1 by 1 array of object-based locations. in figure 1  the object plane is drawn the same size as the retinotopic plane  even though it contains fewer units. this provides the space required to clearly represent the activation level of each unit by the size of its symbol. the area of a junction symbol is exactly proportional to activation of its unit  but for stroke units  the correspondence is only approximate. 
1.1 the mapping plane 


figure 1: the network the mapping plane contains units that represent rules for matching features on the retinotopic plane with features on the object plane. the 1 units on the mapping plane are the cross product of nine x-translations  nine y-translations  and four rotations. these mappings specify all the ways that the object plane can be associated with a 1 by 1 subset of the retinotopic plane. in figure 1  the mappings arc drawn as small black triangles in a 1 by 1 array of squares which corresponds to the central portion of the retinotopic plane. the 
square in which a triangle is drawn shows the point which will be mapped to the center of the object plane. the rotation which a mapping performs is indicated by the orientation of its triangle. try looking at figure 1 to compare the positions of the vertical t and the sideways f with the two active mappings that associate them with their canonical images on the object plane. the activation levels of mapping units are indicated by the area of the triangles which represent them. 
1.1 the letter plane 
the six units on this plane represent the letters e  f  l  i  t  and h. bach unit's activation level is shown by the area of the black square in its box. 
1.1 links 
the first type of link in the network is a simple two-way link between a letter unit and a feature unit on the object plane. the existence of such a link means that the feature is part of the definition of the letter. there are 1 of these links in the current model. the second type of link is a three-way link between a rctinotopic feature  a mapping  and an object feature. there are 1 of these links  which means that some sort of optimization technique is needed to reduce the space and time requirements of the simulation. although it is conceptually attractive to view the links as channels between the rctinotopic plane and the object plane which are gated by mappings  one may think of them as channels between mappings and object-based features which are are gated by rctinotopic features. in fact  the clamping on the rctinotopic plane means that there channels arc cither completely open or completely closed  so it is possible in a simulation to scan the rctinotopic plane for features that are on  arrd then wire up only the corresponding links between the mapping plane and the object plane. this reduces the network to about 1 two-way links that can be implemented in the same way as the links between letters and object features. 
1 relaxation 
the detcrminisuc relaxation algorithm employed in this simulation proceeds in synchronous cycles which have three phases. at the beginning of each cycle  the units exchange activation via their links. next  a competition phase cuts the units' activation down by factors which depend on their size. finally  all activations arc scaled up by constant factors which are computed separately for each plane so that the total activation in the plane is normalized to a predetermined quantity. in more intuitive terms  each unit is a competitor of the other units on its plane but is an ally of the units to which it is connected. 
in the following sections  units arc designated by lower-case greek letters near the beginning of the alphabet. their activation levels are represented by the corresponding latin letters. these activations are functions of time  which is measured in iterations and indicated by 
subscripts on the activations. 
1.1 propagation of activation 
let a  denote the activation level of unit a at time i. then the activation of at time i+1 is given by 
		 1  
	g. hinton and k. lang 	1 
where  is the set of all units which arc linked to is the weight of the link between and and is a constant that is associated with  plane. the purpose of is to adjust the rate at which activation levels change. setting close to 1 causes the units to evolve slowly and feel little influence from their links. it turns out that t also affects the competitive behavior of a plane. a winner-takeall selection cannot occur unless  is set high enough to preclude an equilibrium state where the fresh activation that each unit gets from its links balances out the effect of competition on that unit 
with these facts in mind  it is easy to make rough estimates of appropriate t values for the model. the letter and mapping planes should evolve slowly so they will have time to communicate their hypotheses  and should settle down to a state where only one unit has been selected on each plane. the high values of  which were originally chosen worked moderately well  but some experimentation was required to locate the values of 1 for the letter plane and 1 for the mapping plane that always worked correctly. the object plane  on the other hand  should have fast reaction time but weak competition so that the other planes can exchange information quickly and keep many options open. in fact  a number of features should remain alive even in the final state. the first guess of = 1 has worked for every version of the model ever tested  so the network is clearly not very sensitive to the exact value of this parameter. 
1.1 competition within planes 
without competition  the network rapidly settles into an uninteresting equilibrium state where a large number of units are on and the activation boosts that arc being transmitted over the links balance each other out. in order to force progress towards a solution  it is necessary to exert pressure on the units which will cause weakly supported ones to die out. the method of competition used in this simulation consisted of taking the activation level of each unit and raising it to some power greater than one. since the activations arc between zero and one  this forces them to shrink by an amount that is a function of their size  with small activations getting beaten down more than large ones. furthermore  the discrimination against weak units increases as the exponent increases  which provides the simulator with the ability to control the intensity of competition. the precision of the competition control mechanism and the smoothness of activation decay under the influence of the competition algorithm were the keys to achieving interesting behavior from the network while using monotonic competition schedules. the exact schedules which proved to be acceptable arc discussed in section 1. 
1.1 normalization 
the final step in an iteration is to normalize the activation levels in each plane so that they add up to a target sum. this total corresponds to the number of units that ought to be on in a solution state. for the mapping and letter planes  the target sum is clearly 1. for the object plane  it is harder to say what the total should be  since the number of units that ought to be on at the end depends on the letter that is being recognized. to solve this problem  a new target sum is computed before every iteration by having the letter units vote for how much activation they would like to see on the object plane. 
once a target sum has been chosen for a plane  the normalization is performed by adding up the activations of all the units on the plane  and then multiplying each activation by the ratio of the target sum to the actual sum. 
1 	g. hinton and k. lang 
1.1 initial state 
each relaxation of the network begins in a balanced initial state where the units on a given plane have equal activations that add up to the target sum defined in section 1.1. 
1.1 orientation bias 
the network described so far is unrealistic because it gives no preference to letters in their upright orientation. this can be corrected by modifying equation  1  to read 
		 1  
where v is a factor that equals 1 for a unit o on the letter or object plane. for a on the mapping plane  v equals 1 if the unit being updated is rotated from the vertical and the mapping with the same translation but no rotation has nonzero activation. otherwise  v equals 1 for the mapping plane also. the purpose of v is to deflate rotated mappings that are directly competing with nonrotated mappings. however  this rule is not strong enough to kill off mappings by itself  because the deflating effect of v quickly comes into equilibrium with the positive boost coming over the links. the only effect is to de-emphasi/c rotated mappings in the presence of upright mappings so that the latter can win through ordinary competition. 
1 assignment of weights 
each link possesses a weight which can be thought of as an amplification factor for signals sent over that link. weights arc needed to balance the network so that the simple relaxation technique is able to find an appropriate final state for every input. the main problem is that the unit with the most links often wins the competition even when other units are receiving more support per link. for example  the letter unit for e will always beat the unit for l just because e has more features and therefore more links. the solution to this problem is to compute a link's weight using a function that takes into account the connectivity of both of the link's endpoints. 
1.1 the weighting fun ctlon 
the weight of the link between units  is given by 
		 1  
where  is a weighting factor that compensates for the number of connections a has with units on b's plane. specifically  
		 1  
where is the set of units which arc linked to and is the plane on which a is located. for a concrete example  let a be the letter unit which represents e and be a feature unit which is linked to a. since e has links to 1 stroke units and 1 junction units  the denominator of equation  1  is 1. the numerator is the average number of links to the object plane from units on the letter plane  namely is the weighting factor for e. the unit for l only has links with 1 stroke units and 1 junction units  so its weighting factor is  the weighting factors for units on the letter plane cancel out inbalances caused by the different number of features found in the various letters. 
the mirror image factors  for o on the object plane anc on the letter plane take care of a more subtle problem. letter units can support each other by sending activation indirectly through units representing shared features. unfortunately  the six letters fall into the cliques based on shared feature counts. 
without these weighting factors  the survival of a letter unit depends on the size of its clique. finally  when and are on the object and mapping planes  since every object unit is linked to every mapping unit and vice versa. 
1.1 tuning the weights 
equation  1  provides a good approximation to the weights required to balance the network. however  some hand tuning needs to be performed to ensure proper handling of superimposed images on the object plane. this extra tuning can be implemented by amending the definition of  to read 
		 1  
where is an adjustment function that is associated with the ordered pair 
the mam challenge in balancing the letters is keeping subset relationships untangled. for example  the features of f arc basically a subset of the features of e. so it is difficult to find a set of weights for which e is not identified as f and f is not identified as e. the problem becomes much worse when more than one letter is being mapped onto the object plane at the same time  because f + l looks like  and so on. since the features being mapped onto the object plane contain redundant information and equation  1  balances things out about right  a one-dimensional adjustment based on discriminating between large letters and small letters is sufficient to solve these difficulties. a function which performs this discrimination is for unit a on the letter plane and unit on the object plane. since k is added to the weighting factor for each link to a letter unit  it provides a boost which is a function of the letter's size. a k value of 1 prevents confusion with up to three letters on the retina. 
the next adjustment function is necessary because the clique effect described in section 1.1 is much less pronounced than the other imbalances between letters. the weighting factor which primarily addresses this effect should therefore be de-emphasized by setting 
 for units a on the object plane and  on the letter 
plane. 
1 competition schedules 
the behavior of the network while it is settling down into a solution state is primarily determined by the rules for varying the amount of competition through time. by changing these rules  a variety of behaviors can be induced  most of which arc pathological when there arc multiple letters in the input the network tends to get wedged into either an uninteresting equilibrium state where too many units arc active  or an inconsistent final state where the winners on the various 

planes do not correspond with each other or the input. the next two sections describe a strategy for managing the settling process and the effect of replacing the input with a backward mask before the network has finished settling. 
1.1 a successful search strategy 
the ideal method of running the simulation would be to gradually increase the competition on all of the planes  so that a variety of options would remain open as long as possible  with a consensus eventually emerging as to what was seen where. the final selection of a letter unit and a mapping unit would occur simultaneously. figure 1 shows why this approach doesn't quite work. the simulation has progressed to the point where the correct pair of mappings and the correct pair of letters arc winning. however  due to slight inbalances in the amount of support the various units are giving to each other  the letter unit for t is winning its competition  while on the mapping plane the unit transmitting f is ahead. if the simulation were to proceed with the same gradual competition on both planes  the units which arc currently ahead would win  resulting in a spontaneous illusory conjunction. although the weights could be adjusted to balance the activations in this example  the new weights would make the problem worse for some other pair of letters. 
a belter solution to the problem is to allow one plane to decide first and then transmit its choice to the other plane so that it can choose the corresponding option. during a relaxation based on this strategy  the competition increases gradually at first  as in the ideal simulation described above. a small number of likely mappings emerge as likely candidates  and the images which they specify are superimposed on the object plane. the letter units also evolve slowly  relying on the redundant features which encode the letters to sort out the combined image they see on the object plane. after the network has run for a while in this consensus-building phase  the competition is turned up on the letter plane to force a choice. figure 1 shows the state of the network shortly after this letter selection process has begun. the unit for l is leading because all of its features are strongly on  whereas 
	g.hinton and k.lang 	1 
some of the features for e are less activated.  without the weight tuning described in бь1.1  e would be winning at this point because it is linked to units containing more total activation.  after the unit for l has won  the image on the object plane will look much more like an 
l than an e  and the correct mapping will win after about 1 more iterations. 
although the scenario just described sounds quite reasonable  it is very difficult to adjust the competition schedules so that the network never makes mistakes. a workable solution depends on getting the rate of mapping competition exactly right  and is bounded by the following problems. in figure 1 it is apparent that the mapping for e is far ahead of the mapping for l  even though the letter unit for l is going to win. if the mapping competition docs not occur slowly enough  the mapping unit for l will be so far behind at this point that it will have no hope of catching up. if  however  the mapping competition occurs too slowly  the spurious mappings1 visible in figure 1 will sufficiently distort the relative activations of the various features on the object plane to cause either outright misidentification of the letters  or the kind of letter blending mentioned in section 1.1. 
a competition schedule that works is shown in figure 1  which is a plot of the competition control parameters for the three active planes as functions of time. these parameters are the exponents used in the competition scheme described in бь1.1. the behavior of the network is sensitive to the exact shape of the parameter curves  and simple piecewise-linear functions were not sufficient to eliminate all errors. it is important to note that the primary type of error which occurs is illusory conjunction  and that all of this tuning effort is necessary to eliminate them  not to introduce them. the final result of the adjusuncnt process was a network that would correctly select and identify a letter from any set of one  two  or three upright letters encoded on the retinotopic plane. 
бббб1 these mappings are actually partial symmetries of the letters  which explains why the figure resembles an x-ray diffraction pattern 


figure 1: a normal simulation 

1 	g. hinton and k. lang 

figure 1: competition as a function of time 
1.1 backward masking 
in the absence of interference  the network always works correctly. 
on the other hand  if the input is replaced by noise at some point in the relaxation process  a variety of errors can occur  depending on when the disruption occurs. if a backward mask replaces the input very early  the network will end up in a random suite completely unrelated to its original input if the mask is introduced in the middle of the simulation when the selection process is already underway  the relaxation will either finish correctly or produce an illusory conjunction  depending on how the noise interacts with the unstable activations in the network. for an example of a simulation that produced an illusory conjunction  look at figure 1  which belongs to the same sequence as figure 1. the letter unit for l has gone on to win  and there is a fairly clear image of an l on the object plane  but the mapping units no longer have any meaningful information on the rctinotopic plane with which they could correlate that image. 
consequently  the leftmost mapping unit proceeds to win in this case  which would produce the perception of an l on the left to any sort of binding mechanism which looks at the letter and mapping planes to determine what was perceived. it is interesting to note that in this case it is the absence of meaningful information on the rctinotopic plane  and not the noise per se that causes the error. human subjects need a random mask to produce this type of error because information persists in their lower vision centers unless it is actively overwritten. 
1. discussion 
the model we have described is incomplete in many ways. it ignores the problem of integrating perception across many fixations  though the scheme can be extended to allow this  hinton  1b . it also uses a two-dimensional domain instead of a three-dimensional one  though the extension to 1-d would be feasible if the scheme could be made more efficient at present  the network requires too much hardware. even if we restrict ourselves to rigid transformations  our scheme requires n1 gated connections to map n rctinotopic features through n possible mappings into n object-based features. 
one way to save hardware is to take advantage of the structure of the set of possible mappings. the scheme we presented would work for any set of one-to-one mappings. but the allowable spaual transformations are much more restricted than this. for example  small changes in the mapping cause small changes in the parameters of the object-based feature to which a given rctinotopic feature maps. this means that it is possible to use  coarse coding   hinton  1b  to cconomi/c on units. coarse coding uses activity in a unit to represent a whole collection of similar alternatives and it represents each specific alternative by the joint activity of many such units. it 



figure 1: a simulation disrupted by a random mask 

relies on local linearity: if b lies between a and c  then the representation of b must have the average of the effects of the representations of a and c. on a local scale  this holds for spatial features and their transformations. 
an even stronger property of the set of rigid transformations is that each of them can be expressed as a matrix which operates on a vector containing the retinotopic parameters of one feature to produce a vector containing the object-based parameters of the transformed feature. arbitrary one-to-one mappings cannot be expressed in this way. it should be possible to make great economies by representing features and mappings as collections of parameters rather than as single active units  though we know of no connectionist scheme which fully exploits this possibility. 
another way of economizing on gated connections is to introduce several sequential stages into the network  ballard  1; ballard and sabbah  1 . each stage handles one aspect of the overall transformation. a typical decomposition is to let one stage handle translation and the next stage handle rotation and scaling. if there arc six degrees of freedom in the total transformation and each degree of freedom has d discriminate values  this two stage method reduces the number of gated connections per rctinotopic feature from d1 to 1 x d . however  it also slows down the relaxation process and makes it much harder to ensure that it converges to a sensible solution. 
a possible criticism of the simulation is that we have not proved that the network converges  and we have not given any systematic procedure for tuning the connection strengths. hopfield  1  1  and hummel and zucker  1  have shown that in networks with symmetrical connections  like ours   there is an  energy  function which governs the behavior of the network. if each unit computes the derivative of the energy function and updates its state accordingly  the network is guaranteed to find an energy minimum. our relaxation procedure has similarities to the model in hopfield  1 . the use of a variable power law to suppress weak activations plays the same role as the variable gain in hopfield's model. a further elaboration is to use a stochastic decision rule  hinton & sejnowski  1; geman and geman  1 . this allows networks to escape from local energy minima and it also leads to a simple local algorithm for tuning the weights  ackley  hinton and sejnowski  1 . we have deliberately avoided using these more sophisticated relaxation techniques in this simulation because they introduce extra complexity which simply obscures the relationship between parallel cooperative models of shape perception and illusory conjunctions. 
the use of coarse coding or multiple sequential stages or stochastic relaxation would not remove the tendency of these networks to produce illusory conjunctions  and so it would not affect our central result: illusory conjunctions are a natural consequence of using relaxation to search for consistent states of a parallel network in which connections between rctinotopic and object-based features are gated by mapping units that explicitly represent the viewpoint 
	g. hinton and k. lang 	1 
