
we provide an experimental study of the role of syntactic parsing in semantic role labeling. our conclusions demonstrate that syntactic parse information is clearly most relevant in the very first stage - the pruning stage. in addition  the quality of the pruning stage cannot be determined solely based on its recall and precision. instead it depends on the characteristics of the output candidates that make downstream problems easier or harder. motivated by this observation  we suggest an effective and simple approach of combining different semantic role labeling systems through joint inference  which significantly improves the performance.
1 introduction
semantic parsing of sentences is believed to be an important task toward natural language understanding  and has immediate applications in tasks such information extraction and question answering. we study semantic role labeling  srl  in which for each verb in a sentence  the goal is to identify all constituents that fill a semantic role  and to determine their roles  such as agent  patient or instrument  and their adjuncts  such as locative  temporal or manner.
　the propbank project  kingsbury and palmer  1   which provides a large human-annotated corpus of semantic verb-argument relations  has enabled researchers to apply machine learning techniques to improve srl systems  gildea and palmer  1; chen and rambow  1; gildea and hockenmaier  1; pradhan et al.  1; surdeanu et al.  1; pradhan et al.  1; xue and palmer  1 . however  most systems rely heavily on the full syntactic parse trees. therefore  the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which state of the art  collins  1; charniak  1  is still far from perfect.
　alternatively shallow syntactic parsers  i.e.  chunkers and clausers   although not providing as much information as a full syntactic parser  have been shown to be more robust in their specific task  li and roth  1 . this raises the very natural and interesting question of quantifying the necessity of the full parse information to semantic parsing and whether it is possible to use only shallow syntactic information to build an outstanding srl system.
　although propbank is built by adding semantic annotation to the constituents on syntactic parse trees in penn treebank  it is not clear how important syntactic parsing is for building an srl system. to the best of our knowledge  this problem was first addressed by gildea and palmer . in their attempt of using limited syntactic information  the parser was very shallow - clauses were not available and only chunks were used. moreover  the pruning stage in  gildea and palmer  1  was too strict since only chunks are considered as argument candidates  meaning that over 1% of the arguments were not treated as candidates. as a result  the overall recall in their approach was very low. as we will demonstrate later  high recall of the pruning stage is in fact essential to a quality srl system.
　using only the shallow parse information in an srl system has largely been ignored until the recent conll-1 shared task competition  carreras and marquez  1`  . in this competition  participants were restricted to only shallow parse information for their srl systems. as a result  it became clear that the performance of the best shallow parse based system  hacioglu et al.  1  is only 1% in f1 below the best system that uses full parse information  pradhan et al.  1 . in addition  there has not been a true quantitative comparison with shallow parsing. first  the conll-1 shared task used only a subset of the data for training. furthermore  its evaluation treats the continued and referential tags differently  which makes the performance metric stricter and the results worse. second  an srl system is usually complicated and consists of several stages. it is still unknown how much and where precisely the syntactic information helps the most.
　the goal of this study is twofold. first  we make a fair comparison between srl systems which use full parse trees and those exclusively using shallow syntactic information. this brings forward a better analysis on the necessity of full parsing in the srl task. second  to relieve the dependency of the srl system on the quality of automatic parsers  we improve semantic role labeling significantly by combining several srl systems based on different state-of-art full parsers.
　to make our conclusions applicable to general srl systems  we adhere to a widely used two step system architecture. in the first step  the system is trained to identify argument candidates for a given verb predicate. in the second step  the system classifies the argument candidate into their types. in addition  it is also common to use a simple procedure to prune obvious non-candidates before the first step  and to use post-processing inference to fix inconsistent predictions after the second step. we also employ these two additional steps.
　in our comparison between the systems using shallow and full syntactic information  we found the most interesting result is that while each step of the system using shallow information exhibits very good performance  the overall performance is significantly inferior to the system that uses full information. this necessity of full parse information is especially noticeable at the pruning stage. in addition  we produce a state-of-the-art srl system by combining of different srl systems based on two  potentially noisy  automatic full parsers  collins  1; charniak  1 .
　the rest of the paper is organized as follows. section 1 gives a brief description of the semantic role labeling task and the propbank corpus. section 1 introduces the general architecture of an srl system  including the features used in different stages. the detailed experimental comparison between using full parsing and shallow parsing is provided in section 1  where we try to explain why and where the full parse information contributes to srl. inspired by the result  we suggests an approach that combines different srl systems based on joint inference in section 1. finally  section 1
　concludes this paper.
1 semantic role labeling  srl  task
the goal of the semantic-role labeling task is to discover the verb-argument structure for a given input sentence. for example  given a sentence   i left my pearls to my daughter-in-law in my will   the goal is to identify different arguments of the verb left which yields the output:
 a1 i   v left    a1 my pearls   a1 to my daughter-in-law   am-loc in my will .
here a1 represents the leaver  a1 represents the thing left  a1 represents the benefactor  am-loc is an adjunct indicating the location of the action  and v determines the verb. in addition  each argument can be mapped to a constituent in its corresponding syntactic full parse tree.
　following the definition of the propbank and conll1 shared task  there are six different types of arguments labeled as a1-a1 and aa. these labels have different semantics for each verb as specified in the propbank frame files. in addition  there are also 1 types of adjuncts labeled as amadj where adj specifies the adjunct type. in some cases  an argument may span over different parts of a sentence  the label c-arg is used to specify the continuity of the arguments  as shown in the example below.
 a1 the pearls     a1 i   v said     c-a1 were left to my daughter-in-law .
moreover in some cases  an argument might be a relative pronoun that in fact refers to the actual agent outside the clause. in this case  the actual agent is labeled as the appropriate argument type  arg  while the relative pronoun is instead labeled as r-arg. for example 
 a1 the pearls   r-a1 which   a1 i   v left     a1 to my daughter-in-law  are fake.
　the distribution of these argument labels is fairly unbalanced. in the official release of propbank i  core arguments  a1-a1 and aa  occupy 1%  where the largest parts are a1  1%  and a1  1% . the rest portion is mostly the adjunct arguments  1% . the continued  c-arg  and referential  r-arg  arguments are relatively fewer  occupying 1% and 1% respectively. for more definitions of propbank and the semantic role labeling task  readers can refer to  kingsbury and palmer  1  and  carreras and marquez ` 1 .
1 srl system architecture
our srl system consists of four stages: pruning  argument identification  argument classification  and inference. in particular  the goal of pruning and argument identification is to identify argument candidates for a given verb predicate. the system only classifies the argument candidate into their types in the stage of argument classification. linguistic and structural constraints are incorporated in the inference stage to resolve inconsistent global predictions. this section describes how we build these four stages  including the features used in training the classifiers.
1 pruning
when the full parse tree of a sentence is available  only the constituents in the parse tree are considered as argument candidates. in addition  our system exploits the heuristic rules introduced by xue and palmer  to filter out simple constituents that are very unlikely to be arguments. the heuristic is a recursive process starting from the verb of which arguments to be identified. it first returns the siblings of the verb as candidates; then it moves to the parent of the verb  and collects the siblings again. the process goes on until it reaches the root. in addition  if a constituent is a pp  propositional phrase   its children are also collected.
1 argument identification
the argument identification stage utilizes binary classification to identify whether a candidate is an argument or not. when full parsing is available  we train and apply the binary classifiers on the constituents supplied by the pruning stage. when only shallow parsing is available  the system does not have the pruning stage  and also does not have constituents to begin with. therefore  conceptually the system has to consider all possible subsequences  i.e.  consecutive words  in a sentence as potential argument candidates. we avoid this by using a learning scheme by first training two classifiers  one to predict the beginnings of possible arguments  and the other the ends. the predictions are combined to form argument candidates that do not violate the following constraints.
1. arguments cannot cover the predicate.
1. arguments cannot overlap with the clauses  they can be embedded in one another .
1. if a predicate is outside a clause  its arguments cannot be embedded in that clause.
　the features used in the full parsing and shallow parsing settings are described as follows.
features when full parsing is available
most of the features used in our system are standard features which include
  predicate and pos tag of predicate features indicate the lemma of the predicate verb and its pos tag.
  voice feature indicates passive/active voice of the predicate.
  phrase type feature provides the phrase type of the constituent.
  head word and pos tag of the head word feature provides the head word and its pos tag of the constituent. we use rules introduced by collins  to extract this feature.
  position feature describes if the constituent is before or after the predicate relative to the position in the sentence.
  path records the traversal path in the parse tree from the predicate to the constituent.
  subcategorization feature describes the phrase structure around the predicate's parent. it records the immediate structure in the parse tree that expands to its parent. we also use the following additional features.
  verb class feature is the class of the active predicate described in propbank frames.
  lengths of the target constituent  in the numbers of words and chunks separately.
  chunk tells if the target argument is  embeds  overlaps  or is embedded in a chunk with its type.
  chunk pattern encodes the sequence of chunks from the current words to the predicate.
  chunk pattern length feature counts the number of chunks in the argument.
  clause relative position feature is the position of the target word relative to the predicate in the pseudo-parse tree constructed only from clause constituent. there are four configurations-target constituent and predicate share the same parent  target constituent parent is an ancestor of predicate  predicate parent is an ancestor of target word  or otherwise.
  clause coverage describes how much of the local clause  from the predicate  is covered by the target argument.
  neg feature is active if the target verb chunk has not or n't.
  mod feature is active when there is a modal verb in the verb chunk. the rules of the neg and mod features are used in a baseline srl system developed by erik tjong kim sang  carreras and marquez  1`  .
features when only shallow parsing is available
features used are similar to those used by the system with full parsing except those that need full parse trees to generate. for these types of features  we either try to mimic the features with some heuristics rules or discard them. the details of these features are as follows.
  phrase type uses a simple heuristics to identify only vp  pp  and np.
  head word and pos tag of the head word are the rightmost word for np  and leftmost word for vp and
pp.
  shallow-path records the traversal path in the pseudoparse tree constructed only from the clause structure and chunks.
  shallow-subcategorization feature describes the chunk and clause structure around the predicate's parent in the pseudo-parse tree.
  syntactic frame features are discarded.
1 argument classification
this stage assigns the final argument labels to the argument candidates supplied from the previous stage. a multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage. in addition  to reduce the excessive candidates mistakenly output by the previous stage  the classifier can also classify the argument as null  meaning  not an argument   to discard the argument.
　the features used here are the same as those used in the argument identification stage. however  when full parsing are available  an additional feature introduced by xue and palmer  is used.
  syntactic frame describes the sequential pattern of the noun phrases and the predicate in the sentence.
1 inference
the purpose of this stage is to incorporate some prior linguistic and structural knowledge  such as  arguments do not overlap  or  each verb takes at most one argument of each type.  this knowledge is used to resolve any inconsistencies of argument classification in order to generate final legitimate predictions. we use the inference process introduced by punyakanok et al. . the process is formulated as an integer linear programming  ilp  problem that takes as inputs the confidences over each type of the arguments supplied by the argument classifier. the output is the optimal solution that maximizes the linear sum of the confidence scores  e.g.  the conditional probabilities estimated by the argument classifier   subject to the constraints that encode the domain knowledge.
1 the necessity of syntactic parsing
we study the necessity of syntactic parsing experimentally by observing the effects of using full parsing and shallow parsing at each stage of an srl system. in section 1  we first describe how we prepare the data  as well as the basic system including features and the learning algorithm. the comparison of full parsing and shallow parsing on the three stages  excluding the inference stage  is presented in the reversed order  sections 1  1  1 .
1 experimental setting
we use propbank sections 1 through 1 as training data  and section 1 as testing. in order to apply the standard conll1 evaluation script  our system conforms to both the input and output format defined in the shared task.
　the conll-1 evaluation metric is slightly more restricted since an argument prediction is only considered correct when all its continued arguments  c-arg  are correct and referential arguments  r-arg  are included - these requirements are often absent in previous srl systems  given that they only occupy a very small percentage of the data. to provide a fair comparison  we also report the performance when discarding continued and referential arguments. following the notation used in  xue and palmer  1   this evaluation metric is referred as  argm+   which considers all the core arguments and adjunct arguments. we note here that all the performance reported excludes v label which usually improves the overall performance if included.
　the goal of the experiments in this section is to understand the effective contribution of full parsing versus shallow parsing using only the part-of-speech tags  chunks  and clauses. in addition  we also compare performance when using the correct  gold standard  versus using automatic parse data. the automatic full parse trees are derived using charniak's parser  charniak  1   version 1 . in automatic shallow parsing  the information is generated by a stateof-the-art pos tagger  even-zohar and roth  1   chunker  punyakanok and roth  1   and clauser  carreras and marquez  1`	 .
　the learning algorithm used is a variation of the winnow update rule incorporated in snow  roth  1; roth and yih  1   a multi-class classifier that is tailored for large scale learning tasks. snow learns a sparse network of linear functions  in which the targets  argument border predictions or argument type predictions  in this case  are represented as linear functions over a common feature space. it improves the basic winnow multiplicative update rule in several ways. for example  a regularization term is added  which has the effect of trying to separate the data with a large margin separator  grove and roth  1; hang et al.  1  and voted  averaged  weight vector is used  freund and schapire  1 .
　experimental evidences have shown that snow activations correlate with the confidence of the prediction and can provide an estimate of probability to be used for both argument identification and inference. we use the softmax function  bishop  1  to convert raw activation to conditional probabilities. specifically  if there are n classes and the raw activation of class i is acti  the posterior estimation for class i is
                   acti score .
1 argument classification
to evaluate the performance gap between full parsing and shallow parsing in argument classification  we assume the argument boundaries are known  and only train classifiers to classify the labels of these arguments. in this stage  the only difference between full parsing and shallow parsing is the construction of three full parsing features: path  subcategorization and syntactic frame. as described in section 1  path and subcategorization can be approximated by shallow-path and shallow-subcategorization using chunks and clauses. however  it is unclear how to mimic the syntactic frame feature since it relies on the internal structure of a full parse tree. therefore  it does not have a corresponding feature in the shallow parsing case.
　table 1 reports the experimental results of argument classification when argument boundaries are known. although full parsing features seem to help when using the gold standard data  the difference in f1 is only 1% and 1% for the conll-1 and argm+ evaluation respectively. when the automatic  full and shallow  parsers are used  the gap is smaller.
full parsingshallow parsinggold11auto11gold  argm+ 11auto  argm+ 11table 1: the accuracy of argument classification when argument boundaries are known
lesson when the argument boundaries are known  the performance of the full paring systems is about the same as the shallow parsing system.
1 argument identification
argument identification is an important stage that effectively reduces the number of argument candidates after pruning. given an argument candidate  an argument identifier is a binary classifier that decides whether or not the candidate should be considered as an argument. to evaluate the influence of full parsing in this stage  the candidate list used here is the pruning results on the gold standard parse trees.
　similar to the argument classification stage  the only difference between full-parse and shallow-parse is the use of path and subcategorization features. again  we replace them with shallow-path and shallow-subcategorization when the binary classifier is trained using the shallow parsing information.
　table 1 reports the performance of the argument identifier on the test set using the direct predictions of the trained binary classifier. the recall and precision of the full parsing system are around 1 to 1 percents higher than the shallow parsing system on the gold standard data. as a result  the f1 score is 1% higher. the performance on automatic parse data is unsurprisingly lower  but the difference between full parsing and shallow parsing is relatively the same. in terms of filtering efficiency  around 1% of the examples are predicted as positive. in other words  both argument identifiers filter out around 1% of the argument candidates after pruning.
full parsingshallow parsingprecrecf1precrecf1gold111111auto111111table 1: the performance of argument identification after pruning  based on the gold standard full parse trees 
full parsingshallow parsingprecrecf1precrecf1gold111111auto111111table 1: the performance of argument identification after pruning  based on the gold standard full parse trees  and with threshold=1
　since the recall in argument identification sets the upper bound of the recall in argument classification  in practice  the threshold that predicts examples as positive is usually lowered to allow more positive predictions. that is  a candidate is predicted as positive when its probability estimation is larger than the threshold. table 1 shows the performance of the argument identifiers when the threshold is 1.
　since argument identification is just an intermediate step of a complete system  a more realistic evaluation method is to see how each final system performs. table 1 and table 1 report the final results in recall  precision  and f1 in conll and argm+ metrics. the f1 difference is about 1% when using the gold standard data. however  when automatic parsers are used  shallow-parse is in fact slightly better. this may be due to the fact that shallow parsers are more accurate in chunk or clause predictions compared to a regular full parser  li and roth  1 .
full parsingshallow parsingprecrecf1precrecf1gold111111auto111111table 1: the conll-1 evaluation of the overall system performance when pruning  using the gold standard full parse trees  is available
full parsingshallow parsingprecrecf1precrecf1gold111111auto111111table 1: argm+ performance of the overall system when pruning  using the gold standard full parse trees  is available
lesson full parsing helps in argument identification. however  when the automatic shallow parser is more accurate than the full parser  using the full parsing information may not have advantages over shallow parsing.
1 pruning
as shown in the previous two subsections  the performance difference of full parsing and shallow parsing is not large when the pruning information is given. we conclude that the main contribution of the full parse is in the pruning stage. since the shallow parsing system does not have enough information for the pruning heuristics  we train two word based classifiers to replace the pruning stage. one classifier is trained to predict whether a given word is the start  s  of an argument; the other classifier is to predict the end  e  of an argument. if the product of probabilities of a pair of s and e predictions is larger than a predefined threshold  then this pair is considered as an argument candidate. the pruning comparison of using the classifiers and heuristics is shown in table 1.
full parsingclassifier th=1precrecf1precrecf1gold111111auto111111table 1: the performance of pruning
　amazingly  the classifier pruning strategy seems better than the heuristics. with about the same recall  the classifiers achieve higher precision. however  to really compare systems using full parsing and shallow parsing  we still need to see the overall performance. we build two semantic role systems based on full parsing and shallow parsing. the full parsing system follows the pruning  argument identification  argument classification  and inference stages  as described earlier. for the shallow parsing system  pruning is replaced by the word-based pruning classifiers  and the rest stages are designed only to use shallow parsing information as described in previous sections. table 1 and table 1 show the overall performance in the two evaluation methods.
full parsingshallow parsingprecrecf1precrecf1gold111111auto111111table 1: the conll-1 evaluation of the overall system performance
full parsingshallow parsingprecrecf1precrecf1gold111111auto111111table 1: argm+ performance of the overall system
　as indicated in the tables  the gap in f1 between the full parsing and shallow parsing systems enlarges to more than 1% on the gold standard data. at first glance  this result seems to contradict our conclusion in section 1. after all  if the pruning stage of the shallow parsing srl system performs equally well or even better  the overall performance gap in f1 should be small.
　after we carefully examine the output of the word-based classifier pruning  we realize that it in fact filters out  easy  candidates  and leaves examples that are difficult to the later stages. to be specific  these argument candidates often overlap and differ only with one or two words. on the other hand  the pruning heuristics based on full parsing never outputs overlapping candidates. the following argument identification stage can be thought of as good in discriminating different types of candidates.
lesson the most crucial contribution of full parsing is in pruning. the internal tree structure helps significantly in discriminating argument candidates  which makes the work easy to the following stages.
1 combine different srl systems
the empirical study in section 1 indicates the performance of an srl system primarily depends on the very first stage - pruning  which is derived directly from the full parse trees. this also means that in practice the quality of the syntactic parser is decisive to the quality of the srl system. to improve semantic role labeling  one possible way is to combine different srl systems through a joint inference stage  given that the systems are derived using different full parse trees.
　to test this idea  we first build two srl systems that use collins' parser1 and charniak's parser respectively. in fact  these two parsers have noticeably different output. applying punning heuristics on the output of collins' parser produces a list of candidates with 1% recall. although this number is significantly lower that 1% recall produced by charniak's parser  the union of the two candidate lists still significantly improves recall to 1%. we construct the two systems by implementing the first three stages  namely pruning  argument identification  and argument classification. when a testing sentence is given  a joint inference stage is used to resolve the inconsistency of the output of argument classification in these two systems.
　we first briefly describe the inference procedure introduced by punyakanok et al. . formally speaking  the argument classifier attempts to assign labels to a set of arguments  s1:m  indexed from 1 to m. each argument si can take any label from a set of argument labels  p  and the indexed set of arguments can take a set of labels  c1:m （ pm. if we assume that the argument classifier returns an estimated conditional probability distribution  prob si = ci   then  given a sentence  the inference procedure seeks an global assignment that maximizes the following objective function 
                     m c 1:m = argmax xprob si = ci  	 1 
c1:m（pm i=1
subject to linguistic and structural constraints. in other words  this objective function reflects the expected number of correct argument predictions  subject to the constraints.
　when there are two or more argument classifiers from different srl systems  a joint inference procedure can take the output estimated probabilities for these candidates as input  although some candidates may refer to the same phrases in the sentence. for example  figure 1 shows the two candidate sets for a fragment of a sentence   ...  traders say  unable to cool the selling panic in both stocks and futures.  in this example  system a has two argument candidates  a1 =  traders  and a1 =  the selling panic in both stocks and futures ; system b has three argument candidates  b1 =  traders   b1 =  the selling panic   and b1 =  in both stocks and futures .
...  traders say  unable to cool the selling panic in both stocks and futures.

figure 1: the output of two srl systems: system a has two candidates  a1 =  traders  and a1 =  the selling panic in both stocks and futures ; system b has three argument candidates  b1 =  traders   b1 =  the selling panic   and b1 =  in both stocks and futures . in addition  we create two phantom candidates a1 and a1 for system a that correspond to b1 and b1 respectively  and b1 for system b that corresponds to a1.
　if we throw all these variables together into the inference procedure  then the final prediction will be more likely dominated by the system that has more candidates  which is system b in this example. the reason is because our objective function is the sum of the probabilities of all the candidate assignments.
　this bias can be corrected by the following observation. although system a only has two candidates  a1 and a1  it can be treated as it also has two additional phantom candidates  a1 and a1  where a1 and b1 refer to the same phrase  and so do a1 and b1. similarly  system b has a phantom candidate b1 that corresponds to a1. because system a does not really generate a1 and a1  we can assume that these two phantom candidates are predicted as null  i.e.  not an argument . we assign the same prior distribution to each phantom candidate. in particular  the probability of the null class is set to be 1 based on empirical tests  and the probabilities of the rest classes are set based on their occurrence frequencies in the training data.
　tables 1 and 1 report the performance of individual systems  as well as the joint system. the joint system based on this straightforward strategy significantly improves the performance compared to the two original srl systems in both recall and precision  and thus achieves a much higher f1.
1 conclusions
in this paper  we make a fair comparison between the srl systems using full parse trees and using only shallow syntactic information. what we found confirms the necessity of full parsing for the srl problem. in particular  this information is the most crucial in the pruning stage of the system  and relatively less important to the following stages. inspired by this observation  we proposed an effective and simple approach that combines different srl systems through a joint inference stage. the combined system significantly improves the performance compared to the original systems.
precrecf1collins' parse111charniak's parse111combined result111table 1: the performance in conll-1's evaluation of individual and combined srl systems
precrecf1collins' parse111charniak's parse111combined result111table 1: the performance in argm+'s evaluation of individual and combined srl systems
acknowledgments
we thank dav zimak  kevin small  and the anonymous referees for their useful comments. we are also grateful to dash optimization for the free academic use of xpress-mp. this research is supported by the advanced research and development activity  arda 's advanced question answering for intelligence  aquaint  program  a doi grant under the reflex program  and an onr muri award.
