experiments with a search algorithm for the data base 
of a human belief structure 
kenneth mark colby-
lawrence tesler 
horace enea 

summary 
　　　a large data base was collected from a human informant. the data consisted of beliefs regarding parent-child relations. a variety of factors in searching the data base were manipulated in an attempt to discover which were the more important in contributing to estimates of credibility. problems of data collection  data representation and a searching algorithm are discussed in detail. 
introduction 
　　　it is clear that people hold beliefs. what is not so clear is how these beliefs are processed to judge the credibility of an input proposition. as an aid in understanding credibility processes  we constructed a computer model which was intended to simulate the belief processes of a particular human informant. we shall begin with a clarification of terminology used in describing the model. 
terminology 
　　　a model consists of a set of interacting components. the major components of the model to be described are state-descriptions  data base   process-descriptions  procedures  and an interpreter whose logic governs the applications of processdescriptions to state-descriptions in accordance with the aim or task of judging credibility. thus the model is two levelled with the interpreter at the top supervising interactions between procedures and data base. 
　　　the data base is made up of a conceptual graph whose basic structures consists of conceptualizations in turn composed of elementary conceptions. conceptualizations in the model are held propositions which symbolically represent states of affairs or situations. conceptualizations can be represented in both natural and computer languages by an ordered set of name-tokens. thus a given conceptualization might be described in english as ' b i l l likes mary' and in a programming language as the l i s t   agent b i l l    action like   object mary  . the particular conceptualizations we focussed on were those which semantically involved certain relations between humans. since people conceptualize their experience with persons in terms of human action  the elementary conceptions of the data base involve agents  actions  objects and  optionally  settings  modalities and rationales. 
　　　the term f belief' in human belief structures refers to  a  an affective attitude of acceptance  rejection or neutral judgment towards   b  a held conceptualization. each conceptualization held or prehended by a belief structure is either accepted to some degree as true  rejected to some degree as false or held in suspension as a neutral candidate for belief. 'true' here means that the situation conceptualized is accepted as being certainly  probably  or possibly being the case while 'false' stands for the opposite of these three modal quant i f i e r s . it is important to note that the attitude of rejection or incredibility is towards a conceptualization prehended within a structure. a disbelieved conceptualization is not expelled from the structure but is prehended with an attitude of rejection. a conception is thus judged to be credible  incredible or somewhere in between. 
　　　we postulate credibility to be a function of foundation and consistency. the foundation of a given belief is a measure of those beliefs which imply it as opposed to its negation. consistency refers to a degree of consonance and dissonance found in those beliefs a given belief implies. the term 'imply' does not refer to logical implication but to psychological implication which involves rules of expectancy. we also assume a weight which determines the relative importance foundation and consistency have for one another in a particular domain of interest. 
　　　conceptualizations with their associated credb i l i t i e s make up one component of the data base. we term these conceptualizations 'facts' since they stand for that which is or is not the case in the structure. a second component of the data base consists of rules. by the term 'rule' we mean a connectivity relation holding between two or more conceptualizations. the components of this relation contains variables as well as name-tokens. hence  a rule might read ' i f x likes y then x helps y'  where both x and y are variables to which the nametokens of persons can be bound. as mentioned  this if-then relation represents a type of psychological implication. our interpretation of psychological implication is that given conceptualization a to be the case  conceptualization b is expected to be the case by the structure. for example  humans commonly expect that if a person likes another person  the f i r s t person w i l l help the second person. such general expectancy rules allow a variety of inference processes to be carried out. a further discussion of psychological implication can be found in abelson1. 
-1-　　　the remaining components of the data base consist of definitions and classifications. for 
example  the name-token 'love1 is defined as similar-to and stronger-then 'like'. 'dislike1 is definable as negation-of 'like'. these definitions are used in finding similarities and contrasts between conceptualizations. classifications consist of set memberships and set inclusions. 
     in summary  the data base represents information in the form of various kinds of state descriptions. when the model runs  this information is subjected to procedures governed by a top-level interpreter. the procedures are called into operation by the interpreter in accordance with the task involved. the main task we were interested in involves estimating the credibility of a given proposition describing some actual or hypothetical situations. given such a proposition  how might a person judge its credibility using the information-processing capacities he has available  
     we approached this problem by selecting an informant  collecting certain beliefs  and representing them in a data base. we planned first to conduct certain information-processing experiments on the data base and second to attempt a validation of the simulation. this report will be concerned with the first phase of experimentation. 
data collection 
     to find an informant for this research  we advertised in a college newspaper for persons who might be interested. out of 1 applicants  we interviewed 1 and then selected a 1 year old 
married woman on grounds that she was intelligent  articulate  interested in the research and serious of purpose.* 
     several times a week she would write down in natural language beliefs which occurred to her about events in her life. 	each week we would try to reduce these natural language statements to a simpler form suitable for the model's data base and 
processing. at intervals we would show the data base to the informant for her corroboration or correction of our paraphrasings. initially we planned to obtain her beliefs regarding all the important people in her life space. preliminary experience showed that while collection of such data is possible  the labor required to organize and represent this amount of data in a computer 
model makes the task extremely difficult with 
currently available methods. 
     we found two main disadvantages to this method of data-collection. first  it is cumbersome and time-consuming  requiring hundreds of man-hours to obtain a data base of 1 facts  rules  definitions and classifications. a better method should be developed whereby an informant could type information directly into a data-base by means of a manmachine dialogue. this input might be in an a r t i f i cial and simplified language which an informant 
*for obvious reasons we cannot give her name  but we would like to use this footnote to express our gratitude for her help. 
could learn. however an artificial language is distracting and constrains expressiveness. it would be better to allow the rich freedom of natural language but there are great problems involved in the machine handling of this sort of input. we have had some experience along these lines1 and we are con-
tinuing an attempt to make a conceptual parsing of natural language sentences in order to translate them into conceptualizations suitable for the data base of a belief structure   
     a second drawback to our initial data collection method involved the problem of extensiveness versus intensiveness. an extensive data base is one in 
which there are a great variety of conceptualizations but not a great number around any one theme. while containing a large amount of information  this type of data is too sparse to permit the model to come to very many conclusions regarding credibility. few beliefs of relevance can be found for a given proposition unless it is of a very general and hence trivial nature. 
     we then attempted to concentrate on a particular theme in order to make the data base dense around selected conceptions. since our informant was the mother of a three year old child and interested in the problem of child raising  we concentrated on her beliefs in this area. for each of her beliefs in this domain  we obtained a weighting of a degree of credibility. we used crude categories of strong  
medium and weak for these weightings. to obtain data rules we would ask the informant for reasons for each belief. for example  if a belief were 'a child ought not hit another child' and the reason given by the informant was 'because if a child hits another child then the second child gets hurt'  a general if-then expectation rule can be constructed about the relation between hitting and hurting. 
     one difficulty to be anticipated in simulating a human belief structure involves keeping the model updated along with the informant. if a person's beliefs are continually changing  one cannot keep a model in close enough correspondence to test out comparisons between the person's and the model's performances in estimating credibility. however  we found that with our informant  these particular beliefs about parent-child behavior changed very l i t t l e over a period of several months. in the case of only two beliefs did she change a credibility weight from strong to medium. hence the structure appeared quite stable over this period of time. it should also be remarked that there occurred no major environmental event in this domain of interest which might be expected to have great impact on a belief structure. 
     we attempted to model the credibility processes of a single individual. this approach is in the research tradition of an intensive design in contrast to an extensive design. an extensive design might make one observation on 1 persons while an intensive design would make 1 observations on one 
person. both designs attempt to account for variation in the phenomena observed. in an extensive design  the unit of variability is an individual and variation between individuals is studied whereas in 

an intensive design we are studying the variability 	 f agent barb   action like   object children  
within an individual. 	in modelling a single case we 	 credibility 1   with the symbol 	f 	indicating are trying to understand the mechanisms involved 	this is a fact. 	more complicated is the representain intra-individual processes. 	an intensive design 	tion of a rule because of the problem of binding attempts to show what can and does happen. 	the 	variables unambiguously. 	for example  the natural frequency of this sort of happening in a population 	language statement 'parents spank children' has a and which population is another matter. 	after learn- number of possible semantic interpretations. 	it was ing how to model one person we can model another and necessary to check carefully with the informant in so build up a series of cases. 	the inductive problem order to be clear about which interpretation she 
of generalizing then becomes one of sampling and of s t a t i s t i c a l measures to discover how general the informant's beliefs might be in a population. this 
was not our problem at this state of the inquiry. our problem was how to construct a good model of the informant's belief processes. the criteria for 
'good' can be varied. and are we getting at what the informant 'really' believes  what 'really' means here is obscure but it is common knowledge that people have limited accessibility to their beliefs at a given moment. even worse  they have intended. 	in this case she did not mean that the set 
'parents' spank the set 'children' nor did she mean each member of the set 'parents' spank each member of the set 'children'. by the expression 'parents spank 
children' she meant that a given parent spanks his children. more formally  if x is a parent and y is a child and x is a parent of y  then x spanks y. 
　　　the relation 'is a parent of must f i r s t be defined in terms of certain constrained variables. for instance  the variable p . i s defined as a the capacity to deceive themselves to rationalize  parent who is a parent of ~ is defined as and to distort their own beliefs. over time we a child who is a child of when assignments are hoped to increase accessibility  realizing there are 
always l i m i t s . in worrying about what is 'really' believed we found it useful to keep in mind that we made to such variables only those name tokens which 
qualify can be substituted. 	thus the rule ' i f a were constructing a model of a model. a belief struc- parent likes his child  then his child is happy' ture is a representation and in giving information would be represented as the l i s t : about himself  an informant t e l l s us what he believes he believes. he simulates himself and it is his accessible model of himself that becomes the data base of a computer model. humans' a b i l i t y to 
simulate themselves and to make models of other 
models is of course a most interesting property for a symbolic system to have. 
data representation 
　　　in building a data base for the model we thought of the collected facts  rules  definitions and classifications as a graph. physically in the 
model they were l i s t s in the programming language mlisp1. mlisp is a high level l i s t processing language which translate algol-like meta-statements  m-expressions  into the symbolic statements  sexpressions  of lisp 1. the program runs on the pdp-1 time-sharing system of the stanford a r t i f i c i a l intelligence project. 
　　　each conceptualization was represented as a l i s t of elements consisting of english-like name tokens or atoms  of l i s t s of atoms  and of l i s t s of l i s t s which contained semantic and numerical infor-
mationt as mention in the section 'terminology'  the conceptualizations reflect a human action 
model of situations in the interpersonal world. from this perspective  agents carry out actions towards objects which in turn can be agents or other 
situations. 	in the data base each agent  action  
object  etc.  is identified by an atom. 	on the property l i s t of each atom is a l i s t of pointers to a l l conceptions in the data base in which that particular atom occurs. 	a hash coding scheme is used for rapid look-ups and retrieval of relevant conceptions. with the symbol 	r 	indicating that it is an impli-
cations! rule. when facts are search to match the components of a rule  the fact 'john likes mary' 
would f i t this rule only if john is held to be a parent of mary so that 'john' can be substituted for  and 'mary' for these constrained 
variables are global in the program. they permit the binding of variables to be unambiguous and allow rules to be arbitrarily complex since the qualifications required for the variable may involve multiple conditions. 
　　　representation of definitions is in the form of a simple l i s t . to indicate conceptual relations between 'love' and ' l i k e '   the l i s t appears as 
 love similar like s  
where the symbol 	s 	indicates 'love' is stronger in 
intensity than ' l i k e ' . the following are some relations represented in definitions  s meaning 
stronger  	w weaker  and e 	equal. 
similar  love similar like s  different  men different women  negative  notlike negative like e  opposite  love opposite hate e  kindof  spank kindof aggression  classifications take the simple form                                                                  f agent matches   action is a   object things   the representation of a fact such as 'barb likes 
children 1 appears on the l i s t : 	i n i t i a l experience with a data base of 1 facts  
rules  definitions and classifications not only 
-1-

taught us about the density requirements of data but also brought to light an implementation problem. when output from a running model is not satisfactory  it may be due to errors in the data as well as to inadequacies of the procedures.＊ a small data error   such as the term 'notlike'. in a conceptualization instead of ' l i k e '     originating from human mistakes in inputting data into the data base  can give rise to an incorrect c r e d i b i l i t y estimate. when a data base is large  it becomes extremely d i f f i c u l t to trace entirely by hand what happened in a given run of the model. we t r i e d frequently to check the data for errors and the informant repeatedly studied a l i s t i n g of the data-base searching for mistakes. in spite of this labor of scrutiny  disconcerting data errors would s t i l l crop up. to make sure the procedures were operating as postulated  we f i r s t selected a very restricted subset of the data base and then gradually added to it as the program became debugged. 
procedures 
　　　the modelling program scales variables such as c r e d i b i l i t y   foundation  and consistency into the range 1 to 1. the interpretation of these numbers is as follows: 
1 strongly positive 1o-1 weakly to moderately positive 1 undecided 1 moderately to weakly negative 1 strongly negative. credibility is a function of two components: foundation and consistency. 
　　　the foundation of a proposition is a measure of the model's evidence for and against the proposition. if the positive evidence outweighs the negative  then the foundation is high; if the negative evidence is stronger  the foundation is low. 
　　　credibility is a function mainly of foundation. when foundation is moderate  consistency has more influence on c r e d i b i l i t y than when foundation is extreme. thus  if the evidence concerning a proposition is not dominantly pro or con  then the model 
gives extra weight to i t s consistency in determining i t s c r e d i b i l i t y . a formula for c r e d i b i l i t y which incorporates this factor is given below. 
c r e d i b i l i t y = foundation +  consistency-1  x 
 1 -|foundation-1|  x weight 
the  weight  is a number between 1 and 1 which indicates the relative importance of consistency in this computation. if consistency is irrelevant  the weight is zero. if it is dominant  the weight is 1. 
examples 
credibility as a function of foundation and consistency 
1 
foundation 
	1 	1 
	1 	1 
	1 	1 consistency 1 
1 
1 1 
1 
1 
1 1 
1 
1 1 
1 
1 
1 	the consistency of a proposition 	p is com- puted by finding a few highly relevant beliefs and measuring the consonance of p with these beliefs. relevance is defined o b j e c t i v e l y . i f p is a proposition of the predicate form f  p  and if there 
is a rule in the model that says: 
	if 	f x  	then g x  
and if g p  is the predicate form of a belief q i   then q i is highly relevant to p. if the model already disbelieves qi. - or believes -1 qi. - then p is dissonant with i t . if the model neither believes or disbelieves qi. then the consistency of p is not affected. 
　　　the computation of consistancy consists of determining the percentage of qi.'s with which 	p is consonant. 	more weight is given to consonance with more credible qi.'s than to consonance with less credible 	qi.'s. 	a set of formulas that incorporate this weighting is given below. 
	sc = z credibility  qi.   where p 	qi. 	and 
　　　c r e d i b i l i t y  q i     1 cc= count of q i 's contributing to sc cn = count of qi 's where p o -1 qi. and c r e d i b i l i t y  qi    1 
sc + 1 
consistency = cc + en + .1 
　　　the foundation of a proposition p is computed by finding relevant beliefs and seeing whether they imply that p is or is not the case. in the search for relevant beliefs  graph paths through beliefs consonant with p are searched harder than paths through beliefs dissonant with p if p seems to be highly consistent. the reverse strategy is used if p seems to be inconsistent. this is done so that model can attempt to l i m i t i t s search for evidence in such a way as to maintain the consistency of i t s entire belief structure. 
　　　formulas for foundation in terms of evidence for and against p are: 



     the search for relevant beliefs is controlled by a  work  factor. a consistency search will do  say  1 units of work while a foundation search 
will do  say  1 units of work. this work allotment is apportioned among the possible graph paths that lead from the proposition in question to relevant areas of the graph. 
     the algorithm for searching is as follows. the directly relevant beliefs in the graph are found. a directly relevant belief is one which can be derived from p in one step by any one of these methods: 
1  replace the verb by a similar  or opposite  verb. 
1  replace the subject or object by an analogous  or complementary  noun. 
1  replace the predicate adjective by a similar  or opposite  adjective. 
1  generate a belief which implies p  or p implies  according to any one rule. 
     these beliefs are the heads of paths to be searched. 	a certain amount of work is used up just in finding them; say  1 units for each relevant rule used  1 for each step of an analogy that is drawn  and 1 for each similar verb that is found  plus 1 units overhead even if nothing relevant is found. 
     if there is any work that remains unused after finding the heads of these paths  it is divided up among the paths for further searching. 
     in the consistency search  all paths receive equal treatment. 	however  in the foundation search  the division among paths is affected by consistency. to compute the consistency of p with these paths  a recursive short-depth search is performed along each path; these searches are alotted  say  l/1 of the paths with highest proportions are searched first and receive a proportionately greater work allotment  
     if not all the work along a path is exhausted  the remainder is divided among the remaining paths. if  after searching any path  enough relevant beliefs have been found to compute a credibility exceeding 1 or below 1o  then the search of the rest of the paths is cancelled. 
experiments 
     the program performs two major experiments. the first experiment assumes that the belief structure is unchanging. one proposition at a time is presented to the structure and its credibility is judged. in the second experiment  the belief structure does change. after each proposition's credibility has been evaluated  it becomes incorporated into the structure as a belief. 
     the first experiment is run by presenting each belief in the structure to all the other beliefs and judging its credibility. the result can be compared 
with the prestipulated credibility of the belief. then  a list of new propositions is presented to the structure for evaluation. in both experiments  many factors of the evaluation are varied. 
     one factor to vary is the means of finding relevant beliefs. 	there are four variations: 
1  use only rules - no definitions. 
1  use  l  plus rules to find supersets. 
1  use  l  and  1  plus  similar  and  opposite  rules. 
1  use  l    1   and  1  plus rules to find instances. 
     another factor to vary is the use of consistency. there are two variations: 
1  use foundation and not consistency. 
1  use also consistency. 
other factors varied are: 
1  weight of consistency relative to foundation in computing credibility. 
1  amount of work expended in search. 
every meaningful combination of other factors. 
eg = consistency of p with whole system cp = consistency of p with this path 

-1-the remaining work. 	from the resulting consistencies  proportions are computed according to the following formulas: belief. 
all values of these factors are combined with 1  the initial credibilities assigned each experimental results 
     for the data base used in the experiments so far  a few interesting results were obtained. 
     the search for relevant beliefs was effective when both rules and supersets were utilized in the search. 	without supersets  many relevant beliefs 
were missed. 	the addition of similar and opposite rules expanded the search enough to discover nearly a l l beliefs considered relevant by the experimenters. in only a handful of cases did the application of instance rules improve the relevance search. 
     the use or disuse of consistency made no noticeable difference in the credibility computation. it is planned to see whether consistency will make a difference with different data or with work allot-
ments that have not yet been tried. 
     the amount of work alloted made a difference in the success of finding relevant beliefs. it is intended to measure this difference quantitatively  but techniques for this have not yet been developed. 
     scaling the credibilities of all the beliefs in the system by a factor x seemed to affect the credibility computed for an input proposition by that same multiple  x. this showed that the complex search combined with the quotient formulas for credibility s t i l l preserved linearity. 
search algorithm. 	instead we explored a variety of 
procedures in an effort to learn more about their respective merits in processing the same data base. we intend to discuss the validation problem in a future report. 
