 
this paper is a study on lraam-based  labeling recursive auto-associative memory  classification of symbolic recursive structures encoding terms. the results reported here have been obtained by combining an lraam network with an analog perceptron. the approach used was to interleave the development of representations  unsupervised learning of the lraam  with the learning of the classification task. in this way  the representations are optimized with respect to the classification task. the intended applications of the approach described in this paper are hybrid  symbolic/connectionist  systems  where the connectionist part has to solve logic-oriented inductive learning tasks similar to the termclassification problems used in our experiments. these problems range from the detection of a specific subterm to the satisfaction of a specific unification pattern  and they can get a very satisfactory solution by our approach. 
1 	introduction 
in this paper  we show that basic classification tasks on symbolic recursive structures  logical terms  may be solved with a connectionist approach. we regard this approach to logic-oriented inductive learning as a supplement to classical symbolic ai  which is mainly deductive. it could be used in hybrid  symbolic/connectionist  systems  especially for lifting the expressiveness of these systems from prepositional logic to predicate logic. 
　the generalizations that are necessary for the termclassification problems presented in this paper range from the detection of specific subterms to finding a most specific term  subsuming all terms from the positive class. while these tasks  if known in advance  are trivially solved by ad hoc symbolic procedures  it is very difficult to induce their nature when only a set of positive and negative examples is given. the problem of representing recursive structures of arbitrary size 
　*this work was partially supported by the dfg  ec  da ad and crui  vigoni project . 
has been so far the major obstacle to using connectionist approaches for such logic-oriented inductive learning tasks. neural networks like the lraam-model  labeling recursive auto-associative memory   sperduti  1a; 1b   however  propose a solution to this problem. the lraam is an unsupervised method for devising fixed-width distributed representations of labeled variable-sized recursive data structures  such as graphs  lists and logical terms. the appropriateness of these distributed representations for subsequent classification tasks  in the context of natural language processing  has been shown e.g. in  cadoret  1   where the distributed representations of syntactical trees devised by an lraam are automatically classified according to the typology of dialogue acts. our approach  however  is a little bit different. we interleave the unsupervised development of representations  learning of the lraam  with the supervised learning of the classification task. in this way the representations are optimized with respect to the classification task. the classification becomes much easier. in fact  in all the problems we investigated  we did not need an additional complex multilayer network for classification. a single sigmoidal unit connected to the hidden layer of the lraam is sufficient. 
　in section 1 we introduce the lraam-model. in section 1 we describe the different options for using the lraam for classification tasks. the approach chosen by us is introduced within this framework. in section 1 the term-classification tasks used for our experiments are described. in section 1 we present the results and an analysis of the representations found for the terms. in section 1 we conclude proposing directions for future research. 
1 	labeling r a a m 
the labeling raam  lraam   sperduti and starita  
1; sperduti  1a; 1b; 1  is an extension of the raam model  pollack  1  which allows one to encode labeled structures. the general structure of the network for an lraam is shown in figure 1. the network is trained by backpropagation1 to learn the identity 
　*we verified faster learning using the descending-epsilon heuristic technique  yu and simmons  1 : during the learning phase we maintain a list of the patterns having a decoding error higher than a specified threshold. the backsperduti  starita  and goller 1 
function. the idea is to obtain a compressed representation  hidden layer activation  of a node of a labeled directed graph by allocating a part of the input  output  of the network to represent the label  nl units  and the rest to represent one or more pointers. this representation is then used as pointer to the node. to allow the recursive use of these compressed representations  the part of the input  output  layer which represents a pointer must be of the same dimension as the hidden layer  nh units . thus  a general lraam is implemented by aiv/- nh - n  feed-forward network  where ni - nl + nnh  and n is the number of pointer fields. 
　labeled directed graphs can be easily encoded using an lraam. each node of the graph only needs to be represented as a record  with one field for the label and one field for each pointer to a connected node. the pointers only need to be logical pointers  since their actual values will be the patterns of hidden activation of the network. a graph is represented by a list of these records  and this list constitutes the initial training set for the lraam. during training the representations of the pointers are consistently updated according to the hidden activations. consequently  the training set is dynamic. in the original formulation of the lraam  at the beginning of training the representations for the nonvoid pointers and void pointers are set at random. after each epoch  depending on the hidden activation obtained in the previous epoch for each pattern  the representations for the non-void pointers in the training set are updated. the void representations are  on the other hand  copied from the output. 
　the convention used for the void pointers  however  has the drawback that no fixed representation for the void pointer is used. this means that when we want to encode a structure not in the training set we do not know which representation to use for the void pointer. one simple solution to this problem was proposed by cadoret  cadoret  1 : the void pointer is represented by a vector with null components and its output error not considered when backpropagating the error across the network. in this paper  we adopted the same strategy. 
　once the training is complete  the patterns of activation representing pointers can be decoded to retrieve information. in order to decide whether a pointer is void 
propagation is performed only on the patterns of the list: when all patterns are below the threshold  we lower it and resume the backpropagation. the procedure stops when we obtain the perfect decoding. 
1 	connectionist models 
or not  one bit of the label is allocated for each pointer field to represent the void condition. notice that multiple labeled directed graphs can be encoded in the same 
lraam. 
encoding of terms 
for our purpose  logical terms can conveniently be represented as labeled directed acyclic graphs  ldags   where function symbols are mapped to internal nodes  while constants and variables are mapped to terminal nodes. the label of each node is used to store the symbol associated to the assigned entity. some examples of terms represented by ldags are given in figure 1. the advantage of this representation consists in the possibility to uniquely represent identical subterms within a term  as shown on the right side of figure 1  where the same variable x appears both as first argument of the function f    and as argument of the function g  . this feature allows us to represent terms very compactly. when considering a set of terms  we use the same representational strategy used for a single term: each term  or subterm  is represented only once   and repetitions of the same term are handled by resorting to pointers. an example is given in figure 1. notice that  an lraam trained to encode a set of terms will generate a reduced representation for each intermediate term representation  pointer to a subtree  regardless of the fact that it constitutes a term we are interested in or not. 
　in this paper  we will consider only the representation and classification of ground terms  i.e.  terms which do 
   notice that the space and time complexity of the learning algorithm depends on the number of subterms in the training set. 


not involve variables  however  the classification tasks we propose involve the concept of logical variable. 
1 	classification of terms 
the main subject of this report is to demonstrate the feasibility of classification of ground terms encoded by an lraam. the aim is to devise a system able to generalize on a wide range of classification tasks  from the detection of a particular symbol within the term to be classified to the recognition that the term satisfies a given unification scheme. for this  we propose to classify the reduced representations devised by an lraam through a feedforward neural network. specifically  we propose to use one of the network architectures shown in figure 1. in both of them  the first  part is constituted by an lraam  notice the double side arrow connections  whose task is to encode the terms as discussed in the previous section. the classification task is then performed in the second part of the network through a simple sigmoidal neuron  network a  or a multi-layer feed-forward network with one or more hidden layers  network b . a very similar approach was used by stolcke and wu in  stolcke and wu  1   where they try to learn how to unify very simple terms encoded in a raam. 
　several options for the training of the proposed architectures are available. first of all note that  given a set of positive and negative terms  we have no information regarding the classification of subterrns  which can also appear within both positive and negative terms. this means that the classifier will be used only on reduced representations of terms and its output will be disregarded on reduced descriptors of subterrns. 
　assuming that the training of the system may end when the classification task is performed correctly  the different options we have for the training can be characterized by the proportion of the two different learning rates  for the classification error and the decoding error  and by the different degrees x y of presence  or absence  of the following two basic features: 
  the training of the classifier is started not until x percent of the training set is correctly encoded and successively decoded by the lraam; 
  the error coming from the classifier is backpropa-gated across y levels of the structures encoded by the lraam1. 
　notice that  even if the training in the classifier is started only when all the structures in the training set are properly encoded and decoded  still the classifier's error can change the reduced representations which  however  are maintained consistent by learning in the 
lraam. 
　the reason for allowing different degrees of interaction between the classification and the representation tasks may be due to the necessity of having different degrees of adaptation of the reduced representations to the requirements of the classification task. if no interaction at all is allowed  i.e.  the lraam is trained first and then its weights frozen  y= 1   the reduced representations will be such that similar representations will correspond to similar structures  while if full interaction is allowed  i.e.  the lraam and the classifier are trained simultaneously  the reduced representations will be such that structures in the same class will get very similar representations1. 
　in this paper  we explore the classification capabilities of the network architecture a  i.e.  a single sigmoidal unit connected to the hidden layer of the lraam. the training algorithm we used is as follows: 
  the training of the classifier is started simultane-ously  x= 1  with the training of the lraam; 
  the error coming from the classifier is backpropa-gated only to the pointers of the terms to be classified  y= 1 . 
we will see that  even with this very simple architecture and learning algorithm  we are able to obtain very good results. 
1 	description of the classification problems 
in order to test the ability of the proposed architecture to deal with several classification tasks involving terms  we have generated a set of  simple  classification problems. we have summarized the characteristics of each problem in table 1. the first column of the table reports the name of the problem  the second one the set of symbols  with associated arity  compounding the terms  the third column shows the rule s  used to generate the positive examples of the problem1  the fourth column reports the number of terms in the training and test set respectively  the fifth column the number of subterrns in the training and test set  and the last column the maximum depth1 of terms in the training and test set. 
1  the backpropagation of the error across several levels of the structures can be implemented by unfolding the encoder of the lraam  the set of weights from the input to the hidden layer  according to the topology of the structures. 
1  moreover  in this case  there is no guarantee that the 
lraam will he able to encode and decode consistently all the structures in the training set  since the training is stopped when the classification task is performed correctly. 
1 remember that the terms are all ground. 
1  we define the depth of a term as the maximum number of edges between the root and leaf nodes in the term's ldagrepresentation. 
	sperduti  starita  and goller 	1 
　for each problem about the same number of positive and negative examples is given. both positive and negative examples have been generated randomly. training and test sets are disjoint and have been generated by the same algorithm. 
　it must be noted that the set of proposed problems range from the detection of a particular atom  label  in a term to the satisfaction of a specific unification pattern. specifically  in the unification patterns for the problems instl  instl .long and inst 1 the variable x occurs twice making these problems much more difficult than inst 1  because any classifier for these problems would have to compare arbitrary subterms corresponding to x. however concerning the data sets presented here  this is true only for instl and instl -long  and it will be shown later  that both of them can be solved quite well by our approach. for problem inst1  randomly generating negative examples leads to terms not very similar to t i x }g x b  b . the reason for this is that there are so many possible symbols and that t i x   g x  1   b  has already a very specific structure. therefore there is no negative example being an instance of t i x   g y  1   1  with x y neither in the training nor in the test set  what may make inst1 unexpectedly easy. 
1 	results 
in table 1  we have reported the best result we obtained for each problem  described in table 1  over 1 different network settings  both in number of hidden units for the lraam and learning parameters . the simulations were stopped after 1 epochs  apart for problem instl -long for which we used a bound of 1 epochs  or when the classification problem over the training set was completely solved. we made no extended effort for optimizing the size of the network and the learning parameters  thus it should be possible to improve on the reported results. the first column of the table shows the name of the problem  the second one the number of units used to represent the labels  the third the number of hidden units  the fourth the learning parameters  n is the learning parameter for the lraam  e the learn-
1 	c1nnecti1nist models 

ing parameter for the classifier  u the momentum for the lraam   the fifth the percentage of terms in the training set which the lraam was able to properly encode and decode  the sixth the percentage of terms in the training set correctly classified  the seventh the percentage of terms in the test set correctly classified  and the eighth the number of epochs the network emploied to reach the reported performances. 
　from the results  it can be noted that some problems get a very satisfactory solution even if the lraam performs poorly. moreover  this behavior does not seem to be related with the complexity of the classification problem  since both problems involving the simple detection of an atom  label  in the terms  lbloccljlong  and problems involving the satisfaction of a specific unification rule  inst1 long  inst1  can be solved without the need of a fully developed lraam. thus  it is clear that the classification of the terms is exclusively based on the encoding power of the lraam's encoder which is shaped both by the lraam error and the classification error. however  even if the lraam's decoder is not directly involved in the classification task  it helps the classification process since it forces the network to 


generate different representations for terms in different classes1. 
　in order to give a feeling of how learning proceeds  the performance of the networks during training is shown for the problems termoccl  inst1 and inst1 long in figures 1  where the encoding-decoding performance curve of the lraam on the training set is reported  together with the classification curves on the training and test set. 
reduced representations for classification in this section  we briefly discuss the representational differences between a basic lraam  without classifier  and the architecture proposed in this paper. the basic lraam organizes the representational space in such a way that similar structures get similar reduced representations  see  sperduti  1a  for more details . this happens because  even if the lraam is trained in supervised mode both over the output of the network and over the relationships among components  i.e.  the 
　'actually  the decoder error forces the lraam network to develop a different representation for each term  however  when the error coining from the classifier is very strong  it can happen that terms in the same class get almost identical representations. 
information about the pointers   the network is autoassociative and thus it decides by itself the representation for the pointers. consequently  the learning mode for the lraam is  mainly  unsupervised. when a classifier is introduced  as in our system   the training of the lraam is no longer mainly unsupervised  since the error of the classifier constrains the learning. the resulting learning regime is somewhat between an unsupervised and a supervised mode. 
　in order to understand the differences between representations devised by a basic lraam and the ones devised in the present paper  we trained a basic lraam  of the same size of the lraam used by our network  over the training set of instl  then we computed the first  second  and third principal component of the reduced representations obtained both for the training and test set1. these principal components are plotted in figure 1. it can be noted that the obtained representations mainly cluster themselves in specific points of the space. terms of the same depth constitute a single cluster  and terms of different depth are in different clusters. the same plot for the reduced representations devised by our network  as from table 1  row 1  is presented in figure 1. the overall differences with respect to the basic lraam plot consists in a concentration of more than half  1%  of the positive examples of the training and test sets in a well defined cluster  while the remaining representations are spread within two main subspaces. the well defined cluster can be understood as the set of representations for which there was no huge interference between the decoder of the lraam and the classifier  this allowed the formation of the cluster   while the remaining representations do not preserve the cluster structure since they have to satisfy competitive constraints coming from the classifier and the decoder. specifically  the classifier tends to cluster the representations into two well defined clusters  one for each class   while the lraam decoder tends to develop well distinct reduced representations since they must be decoded to different terms. 
the above considerations on the final representations 
   1 we considered only the reduced representations for terms. no reduced representation for subterms was included in the analysis. 
	sperduti  starita  and 1ller 	1 


1 	c1nnecti1nist models 

a srn and our network is in the unsupervised learning performed by the lraam. however  when forcing the learning parameters for the lraam to be null  we obtain the same learning algorithm as in srn. consequently  we can claim that srn is a special case of our network. moreover  our network can be considered more general with respect to a srn because it allows the classification of labeled directed graphs. 
　we have shown that basic classification tasks on complex terms can be solved by our network. with independent test sets we have verified that the networks really find the right generalizations. based on these very promising results we want to continue our research in the following two directions. 
　on the one side we want to investigate different network architectures and learning options  see section 1  in order to achieve faster learning with smaller representations  fewer units in the hidden layer of the lraam . based on the results in this paper  we conclude that it pays to optimize the representations for the classification task. therefore  the most promising option to investigate seems to be the recursive backpropagation of the classification error over the structures. 
　on the other side we plan to experiment with more complex examples. the single basic tasks which have been solved separately in this paper  occurrence of a specific label  occurrence of a specific subterm and the satisfaction of a specific unification pattern  have to be combined  disjunctively and conjunctively  to more complex tasks. furthermore we want to introduce two additional basic tasks: the occurrence of unification patterns within a term and the recognition of simple regular languages on the paths from the top function symbol of a 
　term to its leaves. according to our knowledge  no single symbolic learning system is able to solve all these basic learning tasks or even combinations of them. our experiments should also be supplemented by examples coming from real applications. the application we are currently working on is a hybrid  symbolic/connectionist  reasoning system. the symbolic component of this system is the theorem prover setheo. connectionist approaches are used within this system to learn search control heuristics from examples  suttner and ertel  1; goller  1 . 
