 
agent tracking involves monitoring the observable actions of other agents as well as inferring their unobserved actions  plans  goals and behaviors. in a dynamic  real-time environment  an intelligent agent faces the challenge of tracking other agents' flexible mix of goaldriven and reactive behaviors  and doing so in real-time  despite ambiguities. this paper presents resc  real-time situated commitments   an approach that enables an intelligent agent to meet this challenge. resc's situatedness derives from its constant uninterrupted attention to the current world situation - it always tracks other agents' on-going actions in the context of this situation. despite ambiguities  resc quickly commits to a single interpretation of the on-going actions  without an extensive examination of the alternatives   and uses that in service of interpretation of future actions. however  should its commitments lead to inconsistencies in tracking  it uses singlestate backtracking to undo some of the commitments and repair the inconsistencies. together  resc's situatedness  immediate commitment  and single-state backtracking conspire in providing resc its real-time character. resc is implemented in the context of intelligent pilot agents participating in a real-world synthetic air-combat environment. experimental results illustrating resc's effectiveness are presented.1 
1 	introduction 
in a multi-agent environment  an automated agent often needs to interact intelligently with other agents to achieve its goals. agent tracking - monitoring other 
   1  we thank rick lewis and yasuo kuniyoshi for helpful feedback. this research was supported under subcontract to the university of southern california information sciences institute from the university of michigan  as part of contract 
n1-k-1 from the advanced systems technology 
office  asto  of the advanced research projects agency  arpa  and the naval research laboratory  nrl . agents' observable actions and inferring their unobserved actions  plans  goals and behaviors - is a key capability required to support such interaction. 
모this paper focuses on agent tracking in real-time  dynamic environments. our approach is to first build agents that are  reasonably  successful in agent tracking in such environments  and then attempt to understand the underlying principles. thus  the approach is one of first building an  interesting  system for a complex environment  and then understanding why it does or does not work  see  hanks et a/.  1  for a related discussion . in step with this approach  we are investigating agent tracking in the context of our on-going effort to build intelligent pilot agents for a real-world synthetic air-combat environment tambe et a/.  1 . this environment is based on a commercially developed simulator called modsaf calder et ai  1   which has already been used in an operational military exercise involving expert human pilots. for an illustrative example of agent tracking in this environment  consider the scenario in figure 1. it involves two combating pilot agents - l in the light-shaded aircraft and d in the dark-shaded one. 

figure 1: a simulated air-combat scenario. an arc on an aircraft's nose shows its turn direction. 
모initially  l and d's aircraft are 1 miles apart  so they can only see each other's actions on radar. for effective performance  they have to continually track these actions. indeed  d is able to survive a missile attack by l in this scenario due to such tracking  despite the missile being invisible to d's radar. in particular  in figure 1a  d observes l turning its aircraft to a collision-course heading  i.e.  at this heading  l will collide with d at 
	tambe and rosenbloom 	1 
the point shown by x . since this heading is often used to reach one's missile firing range  d infers the possibility that l is trying to reach this range to fire a missile. in figure 1-b  d turns its aircraft 1뫢 right. l reacts by turning 1뫢 left  to maintain collision course. in figure 1-c  l reaches its missile range  points its aircraft at d's aircraft and fires a radar-guided missile. while d cannot see the missile on its radar  it observes l's turn  and infers it to be part of l's missile firing behavior. subsequently  d observes l executing a 1뫢 turn away from its aircraft  figure 1-d . d infers this to be an fpole turn  typically executed after firing a missile to provide radar guidance to the missile  while slowing the closure between the two aircraft. while d still cannot observe the missile  it is now sufficiently convinced to attempt to evade the missile by turning 1뫢 relative to the direction of l's aircraft  figure 1-e . this beam turn causes d's aircraft to become invisible to l's  doppler  radar. deprived of radar guidance  l's missile is rendered harmless. 
모meanwhile  l tracks d's beam turn in figure 1-e  and prepares counter-measures in anticipation of the likely loss of both its missile and radar contact. 
모thus  the pilot agents need to continually track their opponents' actions  such as turns  and infer unobserved actions  high-level goals and behaviors  such as the fpole  beam or missile firing behaviors. this agent tracking capability is related to plan-recognition kautz and allen  1; azarewicz et a/.  1 . the key difference is that plan-recognition efforts typically focus on tracking a narrower  plan-based  class of agent behaviors  as seen in static  single-agent domains. in particular  they assume that agents rigidly follow plans step-by-step. in contrast  agent tracking involves the novel challenge of tracking a broader mix of goal-driven and reactive behaviors. this capability is important for dynamic environments such as air-combat simulation where agents do not rigidly follow plans - as just seen  pilot agents continually react to each other's maneuvers. 
모agent tracking and plan recognition are both part of a larger family of comprehension capabilities that enable an agent to parse a continuous stream of input from its environment  whether it be in the form of natural language or speech or music or simulated radar input  as is the case here  e.g.  see  rich and knight  1  chapter 1  . resolving ambiguities in the input stream is a key problem when parsing all of these different types of input. one example of the ambiguity faced in agent tracking can be seen in l's turn in figure 1-c. from d's perspective  l could be turning to fire a missile. alternatively  l could be beginning a 1뫢 turn to run away from combat. or l could simply be following its flight plan  particularly if it has a much shorter radar range  and thus is likely unaware of d. despite such ambiguities  d has to track l's actions with sufficient accuracy so as to respond appropriately. the novel challenge in this domain - at least with respect to previous work in plan recognition - is that the ambiguity resolution has to occur in real-time. as the world rapidly moves on  an agent cannot lag behind in tracking. thus  if d is late or inaccurate in its tracking of l's missile firing maneuvers in figure 1-c  it may not evade the missile in time. 
모this paper describes an approach called resc  realtime situated commitments  for agent tracking that addresses the above challenges. resc's situatedness rests on its constant attention to the current world situation  and its tracking of other agents' actions in the context of this situation. despite its situatedness  resc does make some commitments about the other agent's unobservable actions  behaviors and goals  and attempts to use those in tracking the agent's future actions. in ambiguous situations  these commitments could be inappropriate and could lead to failures in tracking - in such cases  resc modifies them on-line  without re-examining past world states. together  resc's situatedness  immediate commitments  despite the ambiguities   and its on-line modification of commitments provide resc its real-time character. 
in the following  we first describe the process that 
resc employs for tracking other agent's flexible and reactive behaviors  section 1 . this process enables resc to be situated in its present as it tracks an agent's actions. subsequently  resc's ambiguity resolution and real-time properties are described in section 1. these descriptions are provided in concrete terms  using an implementation of the pilot agents in a system called tacair-soar tambe et a/.  1j  built using the soar architecture newell  1; rosenbloom et ai  1 . we assume some familiarity with soar's problem-solving model  which involves applying operators to states to reach a desired state. 
1 tracking flexible goal-driven and reactive behaviors 
in an environment such as air-combat simulation  agents possess similar behavioral flexibility and reactivity. thus  the  architectural  mechanisms that an agent employs in generating its own behaviors may be used for tracking others' flexible and reactive behaviors. consider  for instance  d's tracking of l's behaviors in figure 1-c. d generates its own behavior using the operator hierarchy shown in figure 1-a.  the solid lines indicate the actual hierarchy  and the dashed lines indicate unselected options.  here  at the top-level  d is executing its mission - to protect its home-base for a given time period - via the execute-mission operator. since the termination condition of this operator - completion of d's mission - is not yet achieved  a subgoal is generated.1 d rejects options such asfjollow-flight-plan and run-away in this subgoal in favor of the intercept operator  so as to combat l. in service of intercept  d selects employmissile in the next subgoal. however  since d has not reached its missile firing range and position  it selects get-firing-position in the next subgoal. skipping to the final subgoal  maintam-heading enables d to maintain 
   1  a soar operator has termination conditions - if the operator's application  or new sensor input  changes the state so as to satisfy the termination conditions  then that operator and all of its subgoals are terminated. if the termination conditions remain unsatisfied  a subgoal is created  within which new operators are applied. 

figure 1: operator hierarchies: solid lines indicate actual selections; dashed indicate unselected options. 
모the operators used for generating d's own actions  such as in figure 1-a  will be denoted with the subscript d  e.g.  interceptd. operatord will denote an arbitrary operator of d. stated will denote the global state shared by all of these operators. it maintains all of the dynamic sensor input regarding d's own aircraft  such as its heading and altitude. it also maintains dynamic radar input regarding l's aircraft  such as heading  range  collision course and other geometric relationships. additionally  it maintains non-sensor information  e.g.  d's missile capabilities. together  stated and the operatord hierarchy constitute the introspectable aspect of d  and in this sense may be considered as d's model of its present self  referred to as modeld. 
모modeld supports d's flexible/reactive behaviors via its embedding within soar; and in particular  via two of soar's architectural features:  i  a decision procedure that supports flexibility by integrating all available knowledge about absolute or relative worth of candidate operators right before deciding to commit to a single operator;  ii  termination conditions for operators that support reactivity by terminating operators in response to the given situation rosenbloom et a/.  1 . the point here is not that these specific architectural features are the only way to yield such behavior  but rather that there are such features  and that they can be reused in tracking other agents' behaviors. to illustrate this re-use  we assume for now that d and l possess an identical set of maneuvers.  note that this sameness of maneuvers is not necessary; all that is required is for d to have an accurate model of its opponent's maneuvers.  
모thus  d uses a hierarchy such as the one in figure 1b to track l's behaviors. here  the hierarchy  the solid lines in figure 1-b  represents d's model of l's current operators in the situation of figure 1-c. these operators are denoted with the subscript dl. this operatordl hierarchy  and the statedl that goes with it  constitute d'smodelof l or modeldl. within modeldl  executemission-qij denotes the operator that d uses to track l's mission execution. since l's mission is not yet complete  d applies the interceptdl operator in the subgoal to track l's intercept. the unselected alternatives here  e.g.  run-away-dl indicate the ambiguity in tracking l's actions  however  assume for now that this is accurately resolved . in the next subgoal  employ-missiledl is applied. since l has reached its missile firing position  in the next two subgoals  final-missile-maneuverdl tracks l's final missile maneuver  and point-at-targetdl tracks l's turning to point at d. in the final subgoal  d applies the stari-&-maintatn-turndl operator to statedl   which does not  can not  actually cause l turn. instead  this operator predicts l's action and matches the prediction against l's actual action. thus  if l starts turning to point at d's aircraft  then there is a match with modeldl's predictions - d believes l is turning to point at its target  d  to fire a missile. when l's aircraft turns sufficiently to point straight at d's aircraft  figure 1-c   the termination condition of the point-at-targetdl operator is satisfied  and it is terminated. a new operator  push-fire-buttondl  is then applied in the subgoal of final-misstle-maneuvedl. this operator predicts a missile firing  although the missile cannot actually be observed. statedl maintains a representation of the missile  and marks it with a low likelihood. following that  the fpole-rightdl operator predicts l's right turn for an fpole. when this prediction is matched with l's turn in figure 1-d  the missile's likelihood is changed to high. d now attempts to evade the missile  with beam-rightd.  d currently chooses arbitrarily between the execution of operatord and operatordl   as it generates its own actions  while also tracking l s actions.  
모the above agent tracking process is related to previous work on model tracing in intelligent tutoring systems its  for tracking student actions anderson et a/.  1; ward  1 . however  that work has primarily focused on static environments. a recently developed its  react hill and johnson  1   extends model tracing to a more dynamic environment. react relies upon a plan-driven tracking strategy  and deals with the more dynamic aspects of the domain as special cases. it specifically abstracts away from tracking students' mental states. in contrast  pilots appear to track their op-
	tambe and rosenbloom 	1 



describes single-state backtracking. 
1 	reducing ambiguities 
there are two classes of strategies used in resc to resolve ambiguities: active and passive. the active strategies rely upon an agent's active participation in its environment to gather information to resolve ambiguities. in particular  an automated pilot  such as d  can act in its environment and force its opponent l to react and provide disambiguating information. consider again the example in figure 1-a. as discussed earlier  d faces ambiguity in siatedl about whether l's radar has detected d. this gets resolved with l's turn to collision course. 
unfortunately  if l just happens to be on collision course  it may not turn any further  and the ambiguity would be more difficult to resolve. in such cases  d can randomly turn 1뫢  as shown in figure 1-b  causing l to react if it wishes to maintain collision course. this provides d sufficient disambiguating information - l's radar has detected d. unfortunately  d's actions in service of active ambiguity resolution may interfere with its other goals  such as firing a missile at l. in general  such interference is difficult to resolve. therefore  currently  active ambiguity resolution is based on a fixed set of known maneuvers  supplied by human experts . 
모in contrast  passive ambiguity resolution strategies rely on existing information to resolve ambiguities. one key piece of information is that in this hostile environment  an opponent is likely to engage in the most harmful maneuver. this information is used in the form of a worst case strategy for disambiguation. thus  given a choice  d always selects the worst-case operatordl  from its own perspective  while tracking l's actions. for instance  if there is ambiguity between run-awaydl or interceptdl   d will select intercept dl  which is more harmful. similarly  d resolves ambiguity in the static information in statedl via the worst-case strategy  e.g.  it assumes that l's aircraft is carrying the most powerful missiles and radar that it can carry. unfortunately  this worst-case strategy can lead to overly pessimistic behavior. in the absolute worst-case  the only option for d is to run away. therefore  d applies it selectively  typically in cases where it has to disambiguate rapidly  and yet no other means are available. thus  as seen above  d does not automatically assume detection by l's radar  even though that would be the worst-case assumption. 
모a second passive ambiguity resolution strategy is test incorporation bennett and dietterich  1 . the key idea is to generate fewer incorrect alternatives in ambiguous situations. in particular  modeldl generates alternative operatorsdl that are tested by matching against l's actual actions. observations regarding these actions can be used to avoid generating alternatives that are guaranteed to lead to match failures. for instance  in figure 1-d  fpole-rightdl and fpole-left-fii  are two alternatives available to d in tracking l's actions. if d already sees l turning to its right  then fpole-leftdl can be eliminated  since it would be guaranteed to lead to a match failure. test incorporation relies on such spatial relationships. 
a third passive ambiguity resolution strategy is goal incorporation  e.g.  see  van beek and cohen  1  . the key idea here is to resolve ambiguities only to the extent necessitated by an agent's goals. for example  given the reality of the simulation environment  l's aircraft often unintentionally deviates from its intended heading. given such deviations  l sometimes makes corrections to its headings. however  d does not really need to track and disambiguate these small deviations and corrective actions. it therefore uses fuzz-box filters that disregard specified deviations in l's actions. for instance  for point-at-targetdl  which tracks l's pointing maneuver  figure 1-c   the fuzz-box filter disregards 1뫢 of deviation in l's heading. such filtering also helps to avoid tracking of detailed aspects of statedl  and avoids ambiguities there. 
1 	single-state backtracking in resc 
based on the above disambiguation strategies  resc commits to a single statedl and a single operatordl hierarchy  which track l's actions as described in section 1. however  should this cause a match failure  single-state backtracking is used to undo some commitments. as its name suggests  this backtracking takes place within the context of a single statedl . starting from the bottom of the operatordl hierarchy  operators are terminated one by one in an attempt to get alternatives to take their place. some alternatives do get installed in the hierarchy  and possibly change statedl   but lead to match failures. these are also replaced  until some alternative leads to an operator dl hierarchy that culminates in match success.1 
모why is this process real-time  the main reason is that backtracking occurs without a re-examination of past sensor input or mental recreation of older statesdl. in particular  while backtrack search would normally involve revisiting old statesdl and reconsidering the different operatorsdl possible in each of those states - creating an opening for combinatorics - resc completely avoids such computation. furthermore  although resc does backtrack over the operator hierarchy  there are three factors that ameliorate the combinatorics there. first  given resc's situatedness  backtracking remains tied to the present statedl. thus  while a match failure is recognized and the oacktrack process begun  l and d's aircraft continue to move and turn  changing their speeds  headings  altitudes  and relative geometric relationships  e.g.  range  collision course  etc . statedl is continuously updated with this latest information. the backtracking process takes place in the context of this continuously changing state. thus  only those alternative operatorsdl that are relevant to the current statedl get applied. similarly  in some cases  changes in statedl cause portions of the operatordl hierarchy to terminate automatically during the backtrack process. in other words  resc is continuously dragged forward as the world changes. second  resc does not oblige d to address the match failure before d can execute 
   1  in a few cases  there are pending changes related to ambiguities in statedl  e.g.  has l detected d  these are applied first  hoping they cause changes to operatordl and lead to success. 
	tambe and r1senbl1m 	1 

any of its own operatorsd. thus d is free to act to the extent it can. finally  indeed  if the world were to magically become static  resc's strategy will result in a complex search  although still within the context of a 
single statedl . however  it is unclear if this is necessarily problematic - a static world should possibly merit a more thorough search. 
모let us consider some examples of single-state backtracking. as a simple example  suppose d has commited to the modeldl in figure 1-b. initially  point-attarget dl has match success in that  as predicted  l indeed starts turning towards d see figure 1-a for an illustration . however  l really has decided to run away; so it continues turning 1뫢 without stopping when pointing at d  figure 1-b . this leads to a match failure 
in the operatordl hierarchy. single-state backtrackingnow ensues  terminating operators beginning from the bottom of the hierarchy. finally  interceptdl  is terminated and replaced by run-away dl. this predicts l to be turning towards its home-base  which successfully matches l's actions  figure 1-c . thus  d successfully applies run-away dl  predicting and matching l's actions  without mentally recreating the state dl in which l may have initiated its run-away maneuver. 
figure 1: l continues to turn to run away. 
모a slightly more complex example involves situations where l is engaging in a beam maneuver. here  d initially matches fpole-rightdl  and even infers l's missile firing  as part of statedl . however  as l keeps turning  there is soon a matcn failure  causing d to backtrack until beam-right dl successfully matches. there are two key points here. first  again d is successful in applying beam-rightdl  without mentally recreating the statedl 
in which l may have initiated its beam maneuver. second  d's earlier inference of l's missile firing is not removed  even though it is based on a sequence of operators that eventually led to a match failure. this is because it is difficult for d to decide if l was initially maneuvering to fire a missile and then switched to beam  or if it was always engaged in beam. not knowing any better  d does not eliminate the earlier inference from statedl. fortunately  when aircraft turn 1뫢 to beam  they cannot provide radar guidance to their missiles. therefore  with l's beam  d infers that the missile that it earlier inferred on statedl has lost guidance and become harmless. the end result is identical to a case where d had successfully tracked l's beam maneuver  without the failed intermediate inference of an fpole-rightdl maneuver. 
모we have so far found resc's single-state backtracking to be successful in the air-combat simulation domain  see section 1 . given the potential application of this approach for other areas of real-time comprehension  it 

tracking is a non-trivial task for d. furthermore  higher percentages of operator executions may be dedicated to agent tracking with increased numbers of opponents. 
모the fifth column shows the percentage of agent tracking operators involved in match failures  counting operators at the bottom of the hierarchy that encountered the failure  but not their parents . the main point here is that the overall percentage of these operator is low; at most 1% of the agent tracking operators are involved in match failures. 
모in all of these cases  d is successful in tracking opponents in real-time so as to react appropriately. even in cases where d encounters match failures  it is able to backtrack to track the on-going activities in real-time and respond appropriately. however  as the number of opponents increases  d does face resource contention problems. with four opponents  it is unable to track the actions of all of the agents in time  and gets shot down  hence fewer operators . this resource contention issue is under active investigation tambe  1 . 
모our second set of experiments involved soar-vsmodsaf simulated air-combat scenarios. modsafbased calder et a/.  1  pilot agents are controlled by finite state machines combined with arbitrary pieces of code  and do not exhibit high behavioral flexibility. while d was in general successful in agent tracking in these experiments -뫴 it did recognize the maneuvers in real-time and respond to them - one interesting issue did come up. in particular  in one of the scenarios here  there was a substantial mismatch in d's worst assumptions regarding its opponent's missile capabilities and the actual capabilities - leading to tracking failures. dealing with model mismatch is also an issue for future work. 
모the second aspect to understanding the effectiveness of tacair-soar resc is some quantitative estimate of the impact of agent tracking on improving d's overall performance. in general  this is a difficult issue to address  see 
for instance the debate in  hanks et a/.  1  . nonetheless  we can at least list some of the types of benefits that d accrues from this capability. first  agent tracking is crucial for d's survival. indeed  it is based on agent tracking that d can recognize an opponent's missile firing behavior and evade it. second  agent tracking improves d's overall understanding of a situation  so it can act/react more intelligently. for instance  if an opponent is understood to be running away  d can chase it down  which would be inappropriate if the opponent is not really running away. similarly  if d is about to fire a missile  and it recognizes that the opponent is also about to do the same  then it can be more tolerant of small errors in its own missile firing position so that it can fire first. finally  agent tracking helps d in providing a better explanation of its behaviors to human experts.  such an explanation capability is currently being developed johnson  1  . if human experts see d as performing its task with an inaccurate understanding of opponents' actions  they will not have sufficient confidence to actually use it in training. 
	tambe and r1senbl1m 	1 

1 	lessons learned 
this paper presented an approach called resc  for agent tracking in real-time dynamic environments. our investigation was based on a real-world synthetic environment that has already been used in a large-scale operational military exercise tambe et ai  1 . lessons learned from this investigation - as embodied in resc - are as follows: 
  to track other agents' flexible and reactive behav-iors: reuse the architectural mechanisms that sup-
port an agent's own flexible/reactive behaviors in service of tracking others' behaviors. 
  to address ambiguities in real-time: quickly commit to a single interpretation  and use single-state backtracking to recover from erroneous commitments. 
  to address real-time issues in general: keep tracking firmly tied to the now  i.e.  to the present state. one key issue for future work is investigating the generality of these lessons by applying resc to other competitive and collaborative multi-agent domains. one candidate that has been suggested is a real-time multirobot domain where robots track other robots or humans to collaborate in a task by observation  rather than by communication  kuniyoshi et ai  1 . beyond agent tracking  there is some indication that resc could apply in other real-time comprehension tasks. for instance  a resc-type strategy has been previously used in a realtime language comprehension system lewis  1 . this system also commits to a single interpretation of an input sentence despite ambiguity  and attempts to repair the interpretation in real-time when faced with parsing difficulties. we hope that investigating these broader applications will lead to an improved understanding of agent tracking and comprehension. 
