 
　　a theorem proving system has been programmed f o r automating m i l d l y complex proofs by s t r u c t u r al i n d u c t i o n . one purpose was to prove propert i e s of simple f u n c t i o n a l programs w i t h o u t loops or assignments. one can see the formal system as a g e n e r a l i z a t i o n of number t h e o r y : the formal language is typed and the i n d u c t i o n r u l e is v a l i d f o r a l l t y p e s . proofs are generated by working backward from the g o a l . the i n d u c t i o n strategy s p l i t s i n t o two p a r t s :  1  the s e l e c t i o n of i n d u c t i o n v a r i a b l e s   which is claimed to be l i n k e d 
　　to the u s e f u l g e n e r a l i z a t i o n of terms to v a r i a b l e s   and  1  the generation of i n d u c t i o n subgoals  i n p a r t i c u l a r   the s e l e c t i o n and s p e c i a l i z a t i o n of hypotheses. other s t r a t e g i e s include a f a s t s i m p l i f i c a t i o n a l g o r i t h m . the prover can cope w i t h s i t u a t i o n s as complex as the d e f i n i t i o n and correctness proof of a simple compiling a l g o r i t h m f o r expressions. 
　　a f t e r an overview of the formal system and the search s t r a t e g y   the paper explains how i n d u c t i o n v a r i a b l e s are s e l e c t e d   which includes g e n e r a l i z a t i o n   and how i n d u c t i o n subgoals are generated. f i n a l l y   other s t r a t e g i e s are presented  i n c l u d ing s i m p l i f i c a t i o n . a d e t a i l e d example and t e c h n i c a l remarks c o n s t i t u t e the appendices. 
aubin  1  describes the whole system in d e t a i l . 
formal system 
　　the formal system can be thought of as a gene r a l i z a t i o n of number theory  but w i t h i m p l i c i t outermost u n i v e r s a l q u a n t i f i e r s o n l y . 
　　every term  t h a t i s   every v a r i a b l e and f u n c t i o n a p p l i c a t i o n   has a t y p e . types and cons t r u c t o r constants are h i e r a r c h i c a l l y i n t r o d u c e d . for example  
  t r u e : 	| f a l s e :   -* bool 
  z e r o : 	| s u c c : n a t    * nat 
  n i l : 	| c o n s : n a t   l i s t    + l i s t 
 atom:nat | consx:sexpr sexpr  -  sexpr 
  n u l l t r e e : 	| t i p : n a t | node: t r e e   n a t   t r e e   	t r e e 
　　variables are simply d e c l a r e d . f i n a l l y   def i n e d f u n c t i o n constants are introduced by stages 
w i t h the help of d e f i n i t i o n s by cases   b u r s t a l l 1  hoare 1 . here are some concrete examples: 

d e s c r i p t i v e terms 
　　program p r o v i n g   theorem p r o v i n g   data type  s t r u c t u r a l i n d u c t i o n   g e n e r a l i z a t i o n   s i m p l i f i c a t i o n . 
a= b | bool  = cases a   t r u e  = b | f a l s e  = t r u e   
a&b | bool   a=  bu= false  = false 

i n t r o d u c t i o n 
　　in g e n e r a l   p r o v i n g p r o p e r t i e s of programs r e quires an i n d u c t i v e argument of one s o r t or o t h e r . s t r u c t u r a l i n d u c t i o n is used in t h i s theorem proving system f o r automating m i l d l y complex proofs about programs w r i t t e n in a simple programming language w i t h o u t loops or assignments. 
　　theorem provers using such a method were w r i t ten by brotz  1  f o r number theory and boyer and moore  1  f o r a theory of l i s t s  see also moore  1  and moore  1  . the l a t t e r was applied t o p r o v i n g p r o p e r t i e s o f programs w r i t ten in a l i s p - l i k e language. the present system is an improvement over these previous works by i t s typed language and it more s o p h i s t i c a t e d use o f i n d u c t i o n . 
　　present address: department of computer science  concordia university  1 ouest  boul. de maisonneuve  montreal  quebec  h1c 1  canada. 

they introduce the function constants of implicat i o n   conjunction  and equality for terms of type nat. the computer program uses another concrete representation for type and function definitions as can be seen in appendix 1. 
　　the inference rules are those of  1  truth  1  specialization  1  d e f i n i t i o n by k-recursion  1  
modus ponens   1  s u b s t i t u t i v i t y of equality  and  1  induction. 
　　the domain of interpretation is a many-sorted word algebra generated by the empty set. a l e x i cographic ordering is defined over the domain so that the principle of structural induction holds in i t . an interpretation is given for the language which leads to a proof of soundness and weak completeness. in particular  the meaning of a function constant defined by cases is a well-defined k-recursive function. 

knowledge acq.-1: aubin 1 

   this primitive system is raised by introducing more connectives  or  not  cond  and by deriving some inference rules. terms are put in normal form by means of rules inspired from ketonen's dialect of gentzen's sequent calculus  1 . a thorough description of the formal system can be found in aubin  1 . 
search strategy 
   proving theorems can be seen as a game: the formal system sets the rules besides which we have a strategy of play. this strategy must meet three c r i t e r i a :  1  it must follow the rules of the same  a question of soundness    1  it must be a winning strategy  a question of completeness   and  1  it must use a tolerable amount of resources  a question of efficiency . 
   the search strategy of the present theorem prover works backward  reducing the original goal to subgoals  which are in turn reduced to further subgoals etc. a solution is found when there is no more subgoals to achieve. a procedure to reduce a goal to subgoals is called a tactic  gordon  milner  and wadsworth 1 . 
   a necessary and sufficient condition of soundness of this strategy is that it never reduces a nonachievable goal to only achievable subgoals. this is f u l f i l l e d if the tactics are inverses of valid inference rules  primitive or derived. in particular  the tactic corresponding to the rule of truth reduces the goal true   to no further subgoals. soundness means that when a solution is found  a proof is indeed found. 
   a necessary condition of completeness can be seen as the converse of the previous condition: an achievable goal must not be reduced to nonachievable subgoals. this is always f u l f i l l e d by tactics corresponding to rules which are actually i n vertible  either in general or in some context. as for tactics not bound to invertible rules  the theorem prover tries to find counter examples to the subgoals they generate: if it succeeds  the condition is not met and the subgoals  rejected; if it f a i l s   we cannot t e l l for sure that the condition is satisfied  but we have some ground to believe that it i s   and the subgoals are retained. 
   finding sufficient conditions for completeness is the main problem of theorem proving and the re-
maining sections of this paper w i l l describe my contribution in this direction. 
   the preceding points lead to considering the utilization of resources. a source of efficiency in the present prover is the fact that no backtracking takes place  that i s   at each stage  only one way of reducing a goal is irretrievably taken. so  it is sufficient to keep a simple stack of goals : the original goal is pushed down onto i t   each tactic reduces the goal on top  pops up the stack  and pushes down the new subgoals onto i t . but above this structure  the choice and use of tactics have a greater bearing on efficiency. this prover uses the following tactics in turn  u n t i l the goal stack is empty: 
 1  simplification  inverse of a derived rule of substitutivity of equality and rule of definition by k-recursion  
 1  splitting  inverse of a derived rule of conjunction: from t and s  infer t&s  
 1  replacement  inverse of a derived rule of substitutivity  and strengthening  inverse of a derived rule of weakening: from t  infer s = t   
 1  contraction  inverse of a derived rule of substitutivity  
 1  truth  inverse of the rule of truth  
 1  generalization  inverse of the specialization rule   induction  inverse of the induction rule   and strengthening. 
　the search is aborted if the current goal cannot be reduced by any tactic  e.g.  the goal false  . 
induction variables and generalization 
　the induction tactic has been divided into two distinct parts:  1  the selection of a l i s t of variables to induce upon and  1  the generation of the induction subgoals  given these induction variables. this section treats the f i r s t aspect. actually  i submit that selection of induction variables and generalization are i n t r i n s i c a l l y linked together; so  both w i l l be studied in this section. 
　boyer and moore  1  f i r s t put in evidence the fact that only recursion variables were suitable candidates as induction variables. i w i l l further constrain their fundamental idea by focusing on certain recursion terms of particular importance.  a recursion term is a term which occurs in the argument position of a case variable.  if we allow ourselves to talk of  symbolic  evaluation regarding the application of krecursive definitions  we may as well talk of computation rule. a computation rule t e l l s us which subterm of a term to apply the k-recursion definition rule to. nothing can be gained from a completeness point of view by introducing this notion  but it can lead to improved efficiency. 
knowledge 	acn.-1 	aubin  the call-by-need computation rule is known to be optimal for recursion equations  vuillemin 1  and can usefully be applied to our problem. the starting point is quite simple. what do we need to know about a function application in order to be able to apply the k-recursive definition rule to it  we need to know the values of i t s recursion terms  if it has any. the interesting point is that if we apply the call-by-need line of reasoning to an induction goal which has already been simplified  the process w i l l be stopped by one or more variables marking argument positions which the call-by-need evaluator must have more information about: i submit that these variable occurrences constitute excellent candidates for doing induction upon. i call them primary variable occurrences. 
   a simple example can helpfully illustrate this. take the goal: 

the i n f i x function constant  denotes the function which appends two l i s t s and is defined thus: 

   we start the chain of reasoning with the function constant -; both of i t s arguments are recursion arguments. so  we need to evaluate both of them before trying to apply the definition of =. we iterate the process: to know about we must know about and to know about we must know about but we know nothing about j; so  this primary occurrence of j makes a good induction candidate. on the right of the equali t y   to evaluate j   k  l   we must know about j again. so  this variable is undoubtedly the i n duction variable to choose according to this technique. note i t s directedness: k and 1 are never considered. and indeed  this theorem is proved automatically in one induction on   j   . 
　　the efficiency of this approach is put in evidence if we replace j by cons n j  as would be done in the generation of an induction conclusion. primary variable occurrences are the only ones which  once replaced by structures  allow evaluation to be eventually applicable to the whole goal  try with k and 1 . 
　　the interesting fact about this approach to i n duction variable selection is that generalization can be integrated to it in a natural way. which term occurrences in the goal can we consider as better candidates for induction than the primary variables  the answer is simple: the term occurrences leading to them by the call-by-need evaluation  or in other words  the term occurrences in which the primary variable occurrences appear. i call these primary term occurrences  including the primary variable occurrences. 
   the strong relation between selection of i n duction variables and generalization is theoretically supported by prawitz's results  1 . 
   here is an example with the same flavour as the previous one  the function constant rev denotes the reverse function on l i s t s   : 

we do as before except that for each term occurrence considered by the call-by-need evaluator  we ask the question: can this occurrence  may be together with others  be generalized  this is answered negatively or positively according to whether the prover can or cannot find a counterexample to the generalized subgoal. in this example  the answers are negative u n t i l we get to rev j : if we replace both occurrences of it by a variable  the new subgoal is s t i l l achievable   i t is actually the same as in the previous example  . the new variable is chosen to be the i n duction variable. 
　the advantage of this purposeful generalization is that we can meaningfully generalize only certain occurrences of a term and in particular of a variable. for example  with: 

we find that the f i r s t and the fourth occurrences of j are primary occurrences. we try to generalize them to a new variable which successfully yields: 

this subgoal can be proved in one induction on 
 k . 
　note two points:  1  a search mechanism for counter-examples is essential to such a generalization method and  1  the approach of brotz  1   and boyer and moore  1  to generalization as separated from induction variable selection leads in this example to the nonachievable subgoal 
　some pragmatic aspects must be taken into account in the implementation of this method. in particular  since searching for counter-examples is time-consuming  we limit generalization to the cases which have a better chance of success  i.e.  when the term occurrences to generalize appear on both sides of an equality or implication  boyer and moore 1  brotz 1 . in addition  the above method may propose several candidates and the system uses some tie-breaking rules to elect a unique one. 
　here is an additional example of generalization. the original goal is:. subset  k k . 
the function constant subset is defined by cases on i t s f i r s t argument. no generalization is possible  in the goal and induction is done on  k . we obtain an induction subgoal for which the induction hypothesis cannot be used: 

knowledge acn.-1: aubin 1 　the f i r s t and third occurrences of k   l   are primary and can now be generalized to yield the subgoal: 
subset 
＊  subset 
which is easily proved in one induction on 
induction subgoal generation 
　we now want to find the induction subgoals  given the l i s t of induction variables. in p a r t i cular  we need to find heuristically j u s t i f i e d instantiations for the induction hypotheses. we 
may also wish to discard some hypotheses judged useless; it should be clear that this can cut down the complexity of the subgoals considerably. 
in order to generate the induction subgoals  
boyer and moore  1  use a method which maps the structure of what they call a bomb l i s t into the required terms. the bomb l i s t of a goal contains information about how definitions f a i l to apply to the goal. in moore's later version  1   the corresponding mechanism is directly based on function definitions. i would argue that such techniques whereby induction subgoals are more or less directly constructed from function definitions do not constitute a sound approach. in particular  the soundness of the boyer-moore tactic is not provable  at least for their system. 
　in my tactic  the heuristic-part is separated from the nonheuristic part. on the one hand  a l l induction subgoals are generated on the basis of type definitions. for each of them  the conclusion and a l l the hypotheses are considered. since checking the admissibility of type d e f i n i tions is straightforward  it is easy to convince oneself of the soundness of this nonheuristic part. 
　on the other hand  the role of the definitions of the function constants appearing in the i n duction goal does not so beyond giving information about the rejection  or the acceptance and instantiation of tentative induction hypotheses  i.e. about the heuristic part. now  discarding an induction hypothesis from an i n duction subgoal is sound  by the weakening rule  and preserves the achievability of the i n duction subgoal. moreover  instantiating an i n duction hypothesis is j u s t i f i e d by the induction rule. 
　induction conclusions and hypotheses are actually represented as substitutions involving the induction variables. by applying these substitutions to the induction goal and bundling up the resulting terms with ＊  and &  we easily obtain the induction subgoals themselves. 
　the induction tactic f i r s t finds the conclusion substitutions. for each variable  the algorithm generates the structures representing a l l the values that can be assumed by the variable  the system can do induction from any number of bases . then  the conclusion substitutions are constructed by successively binding the induction variables to each of their corresponding structures. 
　as an example  take the induction goal ack n m   zero; the induction variables  n m  are selected. the function ack is defined thus: 


　next  for each conclusion substitution  we have to find zero or more hypothesis substitutions  according to our lexicographic ordering. 
　consider any conclusion substitution. we simply have to find the immediate predecessors of the l i s t c* x*  of structures bound to the induction variables  that i s   for a l l i   l   i   n     

in our examples  we get the following results: 
 1  no substitutions  since zero has no proper substructures. 

　function definitions come into the picture to serve two purposes:  1  to reject a hypothesis substitution if no use can be foreseen for i t   and  1  to find relevant instances for the free variables. roughly speaking  the strategy applies hypothesis and conclusion substitutions to the induction goal  simplifies the resulting terms  in particular  using function d e f i n i t ions    and then tries to match parts of these terms: a failure counts toward rejection of the hypothesis  while a success both counts toward i t s retention and provides instances for the free variables. 
in the ackermann's example  we have that: 
 1  there is already no hypotheses 
 1  the tentative hypothesis is discarded 

knowlfchfcft acq.-u: aubin 1 

since the definition of ack is not recursive for this case and matching cannot even be attempted. 
 1  by applying the definition of ack to the conclusion and matching  we find the i n stance succ zero  for the free variable   i . 
 1  there are two recursive calls of ack for this case; we set two matches and retain both hypotheses  letting s be ack succ n l   m l  . 
   so  f i n a l l y   the four following induction subgoals are generated: 
 1  ack zero zero    zero 
 1  ack zero succ m l      zero 
 1  ack n l  succ zero     zero 
=  ack succ n l   zero    zero 
 1  
   this method is not fool proof: it w i l l sometimes retain hypotheses which are in fact useless  as above   and sometimes discard useful hypotheses. 	but  in general  it errs on the safe side. 
other strategies 
   other strategies are not so much directly related to using the induction rule. 
indirect generalization 
in the following definition: 

the nonrecursion argument k does not stay fixed on the right  but becomes cons n k . the interest of such definitions lies in the fact that for the class of problems studied  they are l i t e r a l translations of iterative programs. such nonfixed nonrecursion arguments are called accumulators  following moore  1    since they can be considered as holding current values of computations. 
   quite often  accumulators have to be generalized when they are not variables. 	for example  
we should generalize n i l in the goal rev1a k nil  -rev k . since it w i l l not match cons n l  nil  in the simplified conclusion of an induction on  k . however  we do not have an occurrence on both sides of  . how can we massage our goal so as to make n i l recur on the right of the equality  intuitively  if we know that l     n i l   l   we can rewrite rev k  as rev k   nil. so  the goal becomes rev1a k nil  rev k   nil  and n i l occurs on both sides. what if we replace it by a new variable  we get rev1a k l =rev k   l  which is proved easily by inducing on  k   since 1 can now be replaced by cons n l  l  in the induction hypothesis. similar generalizations can be found automatically for natural numbers and l i s t s by a method using specialization as a means of achiev-
ing generalization. 
replacement and strengthening 
   these tactics are responsible for using the induction hypotheses and is an adaptation of a method already experimented with by brotz  1  and especially boyer and moore  1 . for those members of the antecedent of a goal which are equalities  it tries to replace the right by the left-hand side  or vice-versa  in one or more members of the consequent. so  grossly speaking  it reduces s=t= u t/z  to s=t= u s/z   or vice versa. a strengthening tactic is used concurrently. in effect  the antecedent members of an implication which are involved in replacement are discarded from the antecedent  i.e. s=t  u s/z  is reduced to u s/z . this is called strenghening since it is the inverse of the weakening rule. 
   these tactics are well justified and preserve achievability when they involve induction hypotheses  but replacement with equalities not constrained by induction requires a new approach. 
splitting 
   the prover splits conjunctions  that i s   it reduces a goal of the form s&t to the subgoals s and t. this tactic preserves achievability. brotz  1  used i t   but not boyer and moore  1 . 
contraction 

simplification 
   this is the most important tactic besides i n duction. the simplification problem splits into three subproblems:  1  one of logical equivalence between terms before and after simplification   1  one of complexity measure for terms  and  1  one of selection i.e.  what to replace by what in the terms to be simplified. this last question is perhaps the most interesting. 
   the method used in this prover is inspired from vuillemin's call-by-need computation rule  1 . 	applied to simplification  the rule says: select the leftmost-outermost subterm which can be simplified   i . e . cal1-by-name   but take 

knowledge acq.-1: aubln  1 

the maximum advantage of shared subterms. because a l l terms have the same internal representation  a three-field record   the tactic can deal with variables and function applications indistinguishably; moreover the program which applies a substitution does not do undue copying. so  once a term t has been f u l l y simplified  the resulting term s  whether it is a variable or not  is copied into the record of term t  whose boolean f i e l d is set to true. thus any superterm which shared term t now shares i t s simplified equivalent s. 
   the second half of the selection question concerns the order in which the various simplification rules are applied on a given term. this prover tries successively  1  pure simplification rules   1  k-recursive definitions  and  1  normalization rules. the rules are further ordered within each category according to other c r i t e r i a . 
   the gain in efficiency due especially to the sharing of structures is very important. 
conclusion 
   the strong points of this theorem proving system are  1  i t s typed language   1  i t s mechanism for selecting induction variables and generalizing   1  i t s sound way of generating induction subgoals  and  1  i t s fast simplification algorithm. 
   however  i t s formal system is s t i l l too weak: one would like to relax the restrictions on quantification  and on type and function d e f i n i tions. it is also clear that the pure backward search is too limiting and the discovery of useful lemmas on a reasonably large scale w i l l require more of the user  interactively or not . 
   two recent works have proposed solutions to such problems and others addressed in this paper. cartwright's system  1  includes axiomatically defined types constrained to denote the same domains as the type constants described in this paper. since his language is not typed  despite his claim   his induction rule is more powerful. moreover  his system accepts a l l computable functions. his search strategy is crucially dependent on interaction with a user. 
   boyer and moore  1  have a new and even more powerful system since their types can be defined axiomatically in total freedom. it can accept any total function  then one can only conclude that their language is not enumerable: . the additional typing freedom increases the complexity of the proofs found by their automatic search strategy  but they have the f a c i l i t y of specifying any useful lemmas to the prover prior to attempting a proof. 
acknowledgement s 
i am most grateful to my directors of studies  
robin milner and rod burstall  and also to bob boyer and j 	moore. 	i was supported by the commonwealth scholarship commission and the conseil national de recherches du canada. 
appendix 1 
compiling algorithm for expressions 
   in the concrete syntax used by the computer program  type and function definitions are input as pop-1 l i s t s . the following simple compiling algorithm for expressions illustrates the use of such definitions. similar algorithms can be found in burstall  1 ; in milner and weyrauch  1  and cartwright  1   who obtained a correctness proof interactively by machine; and in boyer and moore  1  who got an automatic proof as i did. note the presence of vacuously defined type and function constants. 
   we start by defining the syntax of the source language of expressions by means of type d e f i n i tions: 
 name  
 operator  
cexpress  simple name  
 compound operator express express   
   written in the form used in the body of this paper  this last type definition  for example  would read;  simple:name | compound:operator express  express  -  express. 
   type definitions are also used for the semantic domains. states are intended to map names to numbers. our first-order logic forces us to give a function which applies an object of type function to two numbers. we assume that the variables f  and m and n have been declared to be of type function and nat respectively: 
 function  
 nat  zero   socc nat   
 state  
   apply f m n  nat        
   the following semantic functions give the meaning of the syntactic constructs; mse can be said to be an interpreter. nm is a variable of type name; st  of type state; op  of type operator; and e  el  and e1  of type express: 

　　next  v/e t u r n to the t a r a e t lanouaae. synt a c t i c a l l y   it is a set of programs which are l i s t s o f i n s t r u c t i o n s . p o s t f i x e d n o t a t i o n i s used. we also give a f u n c t i o n to concatenate two t a r g e t language programs. pr  prl  and pr1 are v a r i a b l e s of type program; and i n   of type i n -
struct:  instruct  operate operatok   fetch name   
 program  nullpr   add instruct program   
   concat prl pr1  program  
 casks prl 
  nullpr  pr1  
  add in prl  
 add in  concat prl pr1      

knowledge 	acq.-1: 	auhin 1 

   we define the semantic domains for the target language  pushdowns and stores   together with selecting functions for inspecting their constituents. pd is a variable of type pushdown; and str  of type store: 

　　in other words  we get the same store if we compile an expression and execute the resulting program  given a storage  as if we interpret the expression with the state of the given store and push the result down onto the stack of the store  leaving i t s state unchanged. 
   this statement can be proved automatically by the theorem prover with the help of the lemma: 

which can be proved automatically on i t s own. 
appendix 1 note on implementation and results 
　　the prover is implemented in pop-1. the time taken for finding a proof varies from a few seconds to a few hundred seconds. this is essenti a l l y dependent on the extent in which counter-examples have to be searched for. the proof of the compiling algorithm  which does not involve any generalizations  takes only 1 seconds. this prover could prove most theorems in brotz  1  and boyer and moore  1   plus many new ones. 
these theorems were proved using only a core of basic lemmas in the subtheory of booleans. 	however  the proofs of some of the hardest theorems   e.g.  the compiler correctness  required that equalities of other subtheories be added to the set of simplification rules before they were attempted. 
bibliography 
aubin  raymond.  mechanizing structural induct i o n .   ph.d. thesis  university of edinburgh  edinburgh  1. 
boyer  robert s.  and moore  j.  a lemma driven automatic theorem prover for recursive function theory.  computer science lab. sri  1. 
boyer  robert s.  and moore j.  proving theorems about lisp functions.  j. acm 1  1  1 : 1. 
brotz  douglas k.  embedding heuristic problem solving methods in a mechanical theorem prover.  stan-cs-1  computer science dept.  stanford univ. 1. 
burstall  r.m.  proving properties of programs by structural induction.  comp.j. 1  1  1 : 1. 
cartwright  robert. 	 user-defined data types as an aid to verifying lisp programs.  	in proc. 
coll. aut. lang  and prog.  pp.1. ed. s. michaelson and r. milner. edinburgh: edinburgh university press  1. 
gentzen  gerhard. recherche sur la deduction logique. trad  et coram. r. feys et j. ladriere. paris: puf  1. 
gordon  m.; milner  r.; and wadsworth  c.  edinburgh lcf.  dept. of computer science  univ. of edinburgh  1. 
hoare  c.a.r. 	 recursive data structures.  int. j. comp. and inf. sc. 1  1 : 1. 
milner  r.  and weyrauch  r.  proving compiler correctness in a mechanized logic.  in machine intelligence 1  pp. 1. ed. b.meltzer and d. michie. edinburgh: edinburgh university press  1. 
moore  j strother.  computational logic: structure sharing and proof of program properties.  
ph.d. thesis  univ. of edinburgh  edinburgh  1. 
moore  j strother.  introducing iteration into the pure lisp theorem-prover.  ieee trans. 
soft. eng. 1  1  1 : 1. 
prawitz  dag.  ideas and results of proof theory.  in proc. 1nd scand. log. symp.  pp. 1. ed. j. fenstad. amsterdam: 
north-holland  1. 
vuillemin  jean e.  proof techniques for recursive programs.  memo aim-1/stan-cs-1  computer science dept.  standord univ. 1. 

knowledge acq.-1: aubin 1 
