 
we study a simple  general framework for search called bootstrap search  which is defined as global search using only a local search procedure along with some memory for learning intermediate subgoals. we present a simple algorithm for bootstrap search  and provide some initial theory on its performance. in our theoretical analysis  we develop a random digraph problem model and use it to make some performance predictions and comparisons. we also use it to provide some techniques for approximating the optimal resource bound on the local search to achieve the best global search. we validate our theoretical results with empirical demonstration on the 1-puzzle. we show how to reduce the cost of a global search by 1 orders of magnitude using bootstrap search. we also demonstrate a natural but not widely recognized connection between search costs and the lognormal distribution. 
1 	introduction 
we study a simple  general framework for search called bootstrap search  which is defined as global search using only local search along with some memory for learning intermediate states as subgoals. although it has not previously been studied in detail  this simple search framework with subgoal caching is fundamental to a variety of learning processes at an abstract level. for example  in terms of robot path planning  latombe  1   it is a two-level planning scheme with subgoals being landmarks of either the workspace or configuration space. in terms of concept learning  thornton  1   it corresponds to global concept approximation using only a locally-good extrapolator with subgoals as positive examples. in terms of case-based planning  kolodner  1   it is having memory for cases and a local search for plan modifications and reuse. in terms of problem solving  the framework is simply to reduce new problems to 
　*this work has been performed at sandia national laboratories and supported by the u.s. department of energy under contract de-ac1al1. 
previously solved ones using an existing resource-limited problem solver. 
　in this paper  we present a bootstrap search algorithm  and develop some theory on its performance. our presentation is in the context of problem solving. more concretely  we use the widely-studied 1-puzzle  barr and feigenbaurn  1   the tile-sliding puzzle with 1 tiles on a 1 by 1 tray  as a motivating example. thus  given the availability of a local search procedure  e.g.  a search algorithm with some maximum time cutoff   we aim to study some ways of bootstrapping it into a global search using the memory of previously solved problems. specifically  we characterize the effectiveness of the resulting global search and compare the relative capabilities. further  if the local search is parameterized by some resource bound  we also provide a way of approximating the optimum resource bound so that an optimal local search may be used to yield the best global search. 
　our work is similar to adaptive path planning  chen  1; 1   which studies the framework of improving an existing global path planner using a local path planner augmented with experience. the difference is that bootstrap search does not assume the availability of a pre-existing global searcher  which may be difficult to code or impractical to run. hence  bootstrap search can be more widely applicable  although it will require more training in general to compensate for the lack of a teacher to provide solutions when local search fails. 
　our work is superfacially related to hierarchical qlearning  kaelbling  1  in that we both use a landmark network; however  our emphasis is on deciding what goals to remember as subgoals rather than which local actions to take. our work is also related to the use of abstraction and macro-operators  korf  1b   which can be thought of as local methods. however  our emphasis is not on learning new macro-operators  but on improving the use of existing ones through new subgoals. in this respect  our work is similar to steppingstone  ruby and kibler  1   which improves problem solving by learning new subgoal sequences. however  as in adaptive path planning  steppingstone also relies on the availability of a global planner  brute-force search  to derive new subgoal sequences for a local planner  meansends analysis  to follow. moreover  steppingstone uses general subgoals that represent subspaces of states. in contrast  we restrict our subgoals to specific states to 


gain simplicity. this simplicity allows us to develop a more rigorous understanding of the learning processes within the framework  and hopefully will provide further insight into others. nevertheless  using specified states does have a more acute scalability problem because of the potentially large number of subgoals required. 
1 	algorithm 
the basic assumption in bootstrap search is that there is an efficient  though only locally effective search procedure  local  available that can transform  reduce  any state  problem  u to another v if the pair is 'near' by some metric. a greedy method is often sufficient to implement local. a problem v is directly solvable by local if v can be reduced to a goal state. to 'bootstrap' its global effectiveness  local is allowed to remember in memory u past problems that it had solved. these memory entries can be thought of as trail-markers in that each marker can be traced back to a goal state through calls of local. of course  the more effective local is  the less bootstrapping it needs. the main issue of bootstrap search is how to selectively remember past solved problems so that future problems can still be efficiently solved. 
　to illustrate  consider the two simple but infeasible algorithms in figure 1: the first one excludes the boxed fragment  and the second one includes it. we call the first .1   and the second .1 -1 . in the algorithms  l is a procedure based on local augmented with memory u. upon input problem v  c goes through u in some order  and returns the first solved problem w to which v can be reduced via local  if w exists; otherwise null is returned. to keep track of the solution paths  a back pointer source v  for each trail-marker v is maintained. 
　in .1   the strategy is to store every solved problem in the past  excluding repetition. obviously  this strategy will lead to memory explosion  let alone the utility problem  minton  1  of eventual slowdown. thus  one must stop the training of a 1  eventually and hope that it has received sufficient training. to curb the memory growth  a -1  requires that the newly solved problem v contribute to the problem solving capability of current l. given a problem x  let r x  denote the set of problems reducible to x via local; for a set of problems x  let r x  denote the union of r x  over all x in a'. 
then a -1  requires that r v  contain problems not in r u . although this requirement will decrease the redundancy within u  it nevertheless cannot be implemented in practice. hence  we can only use a -l  for theoretical comparisons. 
　we can be more sophisticated in our learning by augmenting u with a working set of trail-markers w. we can selectively remember useful markers by storing temporary markers in w and promoting them into permanent storage u only when they become useful. in figure 1  we present a m   the algorithm parameterized by m  the size of w  using this scheme. the boxed statement does not affect the problem solving capability of ♀ but should be included when solution quality is an issue. 
　in a m   c checks first through u and then through w for the first solved problem w to which the new problem v can be reduced via local. if w; exists  it is returned; otherwise  null is returned. thus  if w exists  v is solvable by c and is inserted into w. learning occurs when w e w  i.e.  when v e r w    r u . in this case  w is 'proven' useful and promoted into u. there is a forgetting process dictated by the finiteness of m  which determines the amount of reservoir for storing potentially useful entries. because v has to be reducible to some current subgoal before it can be stored  the network of trail-markers is always connected. however  the training time required may be more than that of the alternative: store new problems as potential subgoals even though they may not be currently solvable. this alternative  though  does have its problem of maintaining potentially disconnected network of subgoals. 
　in l's search through u and w  there may well be additional heuristics available for ordering the trailmarkers for a potential match. also  search time may be reduced by calling local only when the problem states are deemed sufficiently close by some heuristic measure. further  portions of u may be skipped to foster more promotions from w and hence faster learning. however  to keep the algorithm simple and unbiased so that we may understand it more thoroughly  we employ no additional heuristics. we implement u with an unbounded first-in-first-access list and w with a rotating last-infirst-access list of size m. the stipulation on u allows us to derive the theoretical predictions later. 
1 	a path planning example 
before applying our algorithm to the 1-puzzle  we first illustrate our algorithm with a simple robot path plan-
	chen 	1 
ning example. consider a point robot operating in a 1-room workcell as shown in figure 1a. the workcell is discretized at a resolution of 1 x 1; the center of the dividers is at  1 ; the lengths of both dividers are 1; and the initial robot 'home' position is at  1 . suppose that the only path planner available is local  which implements the simple 'go-straight' procedure so that it will succeed if and only if its two given points  starting and ending robot positions  are visible from each other. using local  which is obviously not complete in this workcell  the robot is to go through a sequence of goal positions drawn uniformly at random. thus  we need to increase the effectiveness of local. 
　note that in adaptive path planning  chen  1; 1   we have the luxury of having a global searcher to provide a solution whenever the current network of subgoals is inadequate to produce one. in contrast  we do not have such a teacher in bootstrap search. thus  we need to bootstrap ourselves by learning to achieve easy goals first and use these easy goals to achieve harder goals. again  the main issue is which goals achieved should be remembered. we should also note that finding a path from the current robot position s to the goal position t is equivalent to finding a path from 'home' to the goal  because s is known to be connectable to 'home' through the current network of subgoals. hence  we can view the path finding problem in the problemreduction framework by identifying the 'home' position as the goal state  and the goal position as the problem state. further  we may use whatever network learned to accomplish the task of connecting two arbitrary points by connecting them both to 'home' first. thus  different goal states may be included in the problem-reduction framework by reducing the existing subgoals to the new goal state. 
　using our algorithm  it is clearly possible to bootstrap local to total completeness because the workcell is connected under local. the deeper questions are how much training time and memory will be required. shown in figure 1b is the graph of the trail-markers learned by a random run of .1  until it becomes complete. although in general we cannot know when completeness is reached  we can in this case by testing whether each of the four corners: .{ 1    -1    -1  -1    1 -1 }  are visible from a trail-marker. the figure shows that .1  required 1 markers  even at the benefit of knowing the optimal stopping time. in contrast  figure 1c shows the sparsity of the graph learned by a random run of a 1 . the number of markers is only one more than the theoretical minimum of 1. this 1-fold reduction in memory requirement is obtained at a price  though  of a 1-fold increase in training time: from 1 goals for .1  to 1 goals for a l . 
　to study the algorithm more  we run a m  1 times each  for m = 1 1 1. we also consider different environment complexities by varying the lengths of the workcell dividers d: from d - 1 both  we decrease them both to d = 1  and increase them both to d = 1. the resulting average training time to completeness t and average size of memory at that time s are plotted in 
figure 1. clearly  for m   1  the fast drop of t as m 

increases indicates that having sufficiently large reservoir of working memory  e.g.  m   1  is important in reducing the training time  but having more than the threshold will not yield much further reduction. moreover  the lack of variation of s as m increases indicates that the size of working memory has little effect on the size of the final memory learned. 
　so far there are no surprises. however  as we compare the performance curves of a for different environments  we see an interesting phenomenon: a can actually require more training time in a seemingly easier environment. from the plot of t m d   we see that t 1    t 1    t 1   confirming the intuition that the shorter the dividers  the easier the environment is to learn. however  for m   1  the plot actually shows on the contrary that t m  1    t m  1    t m 1   suggesting that having local more powerful  wider doors implying more local successes  do not necessarily accelerate learning  but in fact can hamper learning. the reason for this behavior is due to the greedy nature of a in minimizing 1. the algorithm will promote a working marker to permanent storage only when it demonstrates usefulness. therefore  when operating in an easy environment with d - 1  the algorithm can initially learn very quickly markers that will cover most of the workcell  leaving only a small region that is now unfortunately difficult to learn because goals in that small region will be needed to promote any working marker that is visible from it. for d = 1  the chance of such initial 'overlearning' is smaller because the doors are narrower. for d = 1  the chance of over-learning is even smaller; but because the doors are now much narrower  learning is significantly slowed. therefore  as we increase the power of local  doorways widen   we will see the increase of both the chance of learning and the interplaying chance of over-learning. 

1 	theory 
in this section  we make some precise and strong statements about the performance of our algorithm through mathematical analysis; details of the proofs are provided in the appendix. we first give a sharp bound on the performance of a 1  in theorem 1 by showing that the 'average' failure probability of c using its first k trailmarkers decreases exponentially in k. next  using .1  as a reference  we show that the performance of .1  is better than that of a -l   which is better than that of .1 . the comparison is based on the 'average' failure probability  or 'average utility' in reverse order  of c using the first k markers of each algorithm for each k. finally  we present an upper bound on the 'average' search cost of a m  for all m in theorem 1. from this theorem  a technique for finding the best resource bound c on local c  is developed in section 1. 
as in the framework of pac-learning  natarajan  
1   we assume that the problems are randomly drawn from a fixed distribution - in fact  we shall assume a uniform distribution for simplicity. to facilitate our discussion  we use subscript n on a program variable to denote its value at the nth loop  and parameter m to indicate its correspondence to a m . additionally  for ib   1  let uk be the kth entry to be inserted into memory u. thus  r un{m   denotes the set of problems reducible to any of the previously solved and stored problems in u after a m  is trained with n problems. the following random variables  with parameter m omitted for notational simplicity  are important in characterizing the learning process. 
definition 1 let local c  be a local search procedure with maximum search cost c. a random uniform digraph of problems with edge probability function f c  is a set of uniformly distributed problems in which every problem using local c  is independently solvable and reducible to every other problem with probability f c . 
　thus  the probability that local  will yield a solution with search cost no greater than c is f c . 
theorem 1 the average failure probabilities of a 1  when applied on a random uniform digraph of problems of size n decreases almost geometrically in that for all k  

　with the performance of .1  as a reference  the following theorem shows that the utility of memory u l  is higher than that of u -1   which is in turn higher than that of u 1 . in other words  .1  is better than a -l  which is better than .1  in that a 1  will tend to remember a more 'compact' set of intermediate solutions than a -l   similarly for .1 . 
theorem 1 the average failure probabilities of a{    a 1  and a -1  when applied on a random uniform digraph of problems satisfy 
		 1  
for all k. consequently  the average failure probabilities of the first k memory entries of the algorithms satisfy 
		 1  
　although the preceding theorem is a strong statement about the relative performance of a 1   a 1   and a -l   nothing similar is known about a m  for m 1. 
we conjecture that
	chen 	1 

use the puzzle to demonstrate the generic applicability of bootstrap search when nothing else is available. 
　thus  suppose that we have available only a subroutine local c   which implements iterative-deepening with manhatten distance heuristic  korf  1a  and maximum search cost c  measuring the number of nodes generated. we ask the question   is it possible to bootstrap local c   with unlimited amount of training  so that the average search cost is less than that of simply applying local    we also ask   is it possible to only bootstrap local c  to a certain degree of capability and back it up with local   so that the average total search cost is less than that of simply applying local    as we shall see  the answer to both questions is yes with roughly two orders of magnitude improvement. however  c must be sufficiently large for both cases  and cannot be too large  depending on the desired capability  for the second case. to reach this conclusion  we first need to justify the random digraph model and study f c . 
　obviously  the digraph of the solvable states of the 1puzzle induced by local c  is not random. however  the number of states n = 1 is extremely large and the digraph is almost regular. hence  it is plausible to model the digraph of the 1-puzzle by the random digraph with f c  being the fraction of states solvable by local c . to study this fraction  we apply local 1  to 1 randomly generated solvable states  and record the fraction of successes with search cost less than or equal to c  against case increases to 1. since brute-force search costs tend to be exponential  we use log scale on c. we plot log c against the fraction of success under local c  to obtain the cumulative distribution function and the 

is the normal probability function. 
　having approximated f c   we next describe our bootstrap search experiment and verify our theoretical predictions before answering the questions posed in the beginning of this section. for each x = 1 1  we compare .1  and .1  by running them on 1 random problems. for reference  we also run .1   but with a pre-chosen bound b on the memory storage: b = 1 for both x = 1; 1 = 1 for x = 1; 1 = 1 for x = 1. these bounds are chosen so that storage requirements are comparable to that of .1  and a 1 . for each x  we denote the data corresponding to .1   .1   .1  as type  1 x    l x    1 x   respectively. for all three algorithms  we use the last 1 random problems to measure their properties at the end of the 1 training problems. 
　one property is the utility of the permanent trailmarkers learned. we tabulate the number of problems that require k memory accesses  and estimate a k  by the fraction of problems unsolvable with the first k markers. to allow a closer look at the failure probabilities for large k  we plot the negative log of the failure frequencies in figure 1  with reference line k log l - p x   anticipated by theorem 1. for each algorithm  the data for k up to the size of the permanent memory is plotted. several observations can be made from this figure. first  the reference lines do not approximate the  1  x  data very well  but do support all data from below. the bad approximation may be explained by the fact that the state space of the 1-puzzle is really not a random graph  but only an approximately regular graph. however  the appearance of the reference lines supporting the data from below provide evidence that the algorithms do perform more successfully than predicted under the idealized randomgraph model. another observation is that as x decreases  .1  becomes less trained than a 1 . simultaneously  the relative size of u 1  to that of u 1  also decreases  as indicated by the largest k attained for each data set. obviously  as the power of local decreases  x decreases   the more working markers is needed for faster bootstrapping. with just one working marker  a 1  cannot learn as quickly as .1  with the same number of training problems. finally  we can clearly see the higher memory utility of a 1  compared to that of a 1   as predicted by 
theorem 1. the data of type  l x  is generally higher 


figure 1: cost ratios of a m  backed up by local oo  to local oo  alone. 
than that of type  1 x . although we do not see such generality between data of types  1  x  and  1  x   we do also see that data of type  1  x  is generally higher than that of type of  1  x . these data support our conjecture that   1  for arbitrary m. 
　finally  we measure the average search cost of each algorithm backed up by local  and compare that with the theoretical predictions of theorem 1. by straightforward computation  we have for the ratio of the average search cost of a m  backed up by local to that of localalone. we plot the theoretical upper bound for a = 1 1 1 1  and the observed data points in figure 1. the data points with observed failure probability a   1 are listed in the adjacent table. careful checking shows that the observed data points are indeed consistently lower than the predicted upper bound. taking the failure probabilities into consideration  we see that a 1  with local 1  is the best of the 1 algorithms compared  yielding the lowest search cost ratio of 1. 
now suppose that the upper bounds are indeed true. 
then from h x  1   we see that after a m  becomes fully trained  i.e.  without the need of local  as a backup   it can search with average cost less than that of local as long as x   1  or c   1. however  from h x  a  for a   1  we see that if a m  is not fully trained  then the range of x for which a m   with local  as a backup  can search with average cost less than of local oo  alone necessarily has an upper bound. for example  if a = 1  then x must be greater than 1 and less than 1. this 'over-learning' phenomenon of powerful local search becoming detrimental can be explained: if a   1  then there may be problems more difficult than a m  can handle; in this case  fruitless search will be incurred by a m   causing local  alone to be better. 
1 	conclusion 
we have presented an algorithm for bootstrap search  and provided some initial theory on their performance. our theoretical analysis is based on a problem model of a random digraph. using this model  we have made some strong performance predictions and comparisons of the algorithm. further  we have illustrated some techniques for locating the optimal 'power' of the local search to be bootstrapped so as to yield the best global search. we have empirically evaluated our algorithm by applying it to both the 1-puzzle and a simple robot path planning problem. in both domains  we have 1  explained the observed performance behavior in terms of the theory developed; 1  explored the importance of having sufficiently large set of working trail-markers; and 1  identified a common detrimental phenomenon of over-learning when local search becomes too powerful. incidentally  we have also contributed some new results on solving the 1puzzle. in particular  we have made a natural but not widely recognized connection between search costs and the lognormal distribution. 
　our work is by no means complete. theoretically  we have not addressed the issue of training time  nor the issue of solution quality. within the random digraph model  there are conjectures yet to be settled. outside the random digraph model  there are plenty of other problem models that may yield further insights into the area of bootstrap search. in particular  the path planning problem addressed in this paper should provide motivation for a different model suitable for analysis. there may also be some other bootstrapping algorithms waiting to be discovered. with its broad applicability in search and learning  further research is definitely needed. 


1 	automated reasoning 

1 	automated reasoning 

1 	automated reasoning 

1 	automated reasoning 











1 	automated reasoning 

1 	automated reasoning 

1 	automated reasoning 







1 	automated reasoning 

1 	automated reasoning 

1 	automated reasoning 

