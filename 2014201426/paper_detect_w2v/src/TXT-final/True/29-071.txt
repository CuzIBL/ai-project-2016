 
bayesian belief networks have grown to prominence because they provide compact representations of many domains  and there are algorithms to exploit this compactness. the next step is to allow compact representations of the conditional probability tables of a variable given its parents. in this paper we present such a representation in terms of parent contexts and provide an algorithm that exploits this compactness. the representation is in terms of rules that provide conditional probabilities in different contexts. the algorithm is based on eliminating the variables not needed in an answer in turn. the operations for eliminating a variable correspond to a form of partial evaluation  where we are careful to maintain the probabilistic dependencies necessary for correct probabilistic inference. we show how this new method can exploit more structure than previous methods for structured belief network inference. 
1 	introduction 
probabilistic inference is important for many applications in diagnosis  perception  and anywhere there is uncertainty about the state of the world from observations. belief  bayesian  networks  pearl  1 are a representation of independence amongst random variables. they are of interest because the independence is useful in many domains  they allow  for compact representations of problems of probabilistic inference  and there are algorithms to exploit the compact representations. 
　recently there has been work to extend belief networks by allowing more structured representations of the conditional probability of a variable given its parents. this has been in terms of either causal independencies  heckerman and breese  1; zhang and poole  1  or by exploiting finer grained contextual independencies inherent in stating the conditional probabilities in terms of rules  poole  1  or trees 
    this work was supported by institute for robotics and intelligent systems  project ic-1 and natural sciences and engineering research council of canada research grant ogp1. thanks to holger hoos and mike horsch for comments. 
1 	probabilistic reasoning 
 boutilier et al  1 . in this paper we show how algorithms for efficient inference in belief networks can be extended to also exploit the structure of the rule-based representations. 
　in the next section we introduce belief networks  a rulebased representation for conditional probabilities  and an algorithm for belief networks that exploits the network structure. we then show how the algorithm can be extended to exploit the rule-based representation. we present an example in detail and show how it is more efficient than previous proposals for exploiting structure. 
1 background 
1 belief networks 
a belief network  pearl  1  is a dag  with nodes labelled by random variables. we use the terms node and random variable interchangeably. associated with a random variable x is its frame  val x   which is the set of values the variable can take on. for a variable x  let  be the parents of x in the belief network. associated with the belief network is a set of probabilities of the form   the conditional probability of each variable given its parents  this includes the prior probabilities of those variables with no parents . 
　a belief network represents a particular independence assumption: each node is independent of its non-descendents given its parents. suppose the variables in a belief network are x1 ...  xn where the variables are ordered so that the parents of a node come before the node in the ordering. then the independence of a belief network means that: 

by the chain rule for conjunctions we have 

this is often given as the formal definition of a belief network. 
example 1 consider the belief network of figure 1. this represents a factorization of the joint probability distribution: 


ables  there is a parent context 
probability of an assignment of a value to each variable is 

	poole 	1 


figure 1: a tree-structured representation of the conditional probability function for e given its parents. 
　in the first  this rule simply means the conditional probability assertion: 

　the second interpretation  poole  1  is as a set of definite clauses  with  noise  terms in the body. the noise terms are atoms that are grouped into independent alternatives  disjoint sets  that correspond to random variables. in this inter-
pretation the above rule is interpreted as the clause: 
where values 
are independent. 	this interpretation 
may be helpful as the operations we consider can be seen as instances of resolution on the logical formula. one of the main advantages of rules is that there is a natural first-order version  that allows for the use of logical variables. 
example 1 consider the belief network of figure 1. figure 1 gives a tree-based representations for the conditional probability of e given its parents. in this tree  nodes are labelled with parents of e in the belief network. the left hand child corresponds to the variable being true  and the right hand node to the variable being false. the leaves are labelled with the probability that e is true. for example irrespectively of the value for c or d. 
1 	probabilistic reasoning 
these trees can be translated into rules: 

lemma 1 if the bodies for the rules are exclusive the probability of any context on {x  ...  xn} is the product of the probabilities of the rules that are applicable in that context. 
for each xi  there is exactly one rule with xi  in the head that is applicable in the context. the lemma now follows from equation  1 . 
　in general we allow conjunctions on the left of the arrow. these rules have the obvious interpretation. section 1 explains where these rules arise. 
1 	belief network inference 
the aim of probabilistic inference is to determine the posterior probability of a variable or variables given some observations. in this section we outline a simple algorithm for belief net inference called ve  zhang and poole  1  or bucket elimination for belief assessment  beba  dechter  1   that is based on the ideas of spi  shachter et al.  1 . this is a query oriented algorithm that exploits network structure for efficient inference  similarly to clique tree propagation  lauritzen and spiegelhalter  1; jensen et al  1 . one difference is the factors represent conditional probabilities rather than the marginal probabilities the cliques represent. 
　suppose we want to determine the probability of variable x given evidence e which is the conjunction of assignments to some variables  namely 
     1 we only specify the positive rules on our examples. for each rule of the form: 

we assume there is also a rule of the form 

we maintain both as  when we have evidence  section 1   they may no longer sum to one. 

then: 
here is a normalizing factor. the problem of probabilistic inference can thus be reduced to the problem of computing the probability of conjunctions. let  and sup-
pose that the yi's are ordered according to some elimination ordering. to compute the marginal distribution  we sum out the yi's in order. thus: 

where the subscripted probabilities mean that the associated variables are assigned the corresponding values in the func-
the general idea of the structured probabilistic inference algorithm is to represent conditional probabilities in terms of rules  and use the ve algorithm with a form of partial evaluation to sum out a variable. this returns a new set of clauses. we have to ensure that the posterior probabilities can be extracted from the reduced rule set. 
　the units of manipulation are finer grained than the factors in ve or the buckets of beba; what is analogous to a factor or a bucket consists of sets of rules. given a variable to eliminate  we can ignore  distribute out  all of the rules that don't involve this variable. 
　the input to the algorithm is: a set of rules representing a probability distribution  a query variable  a set of observations  and an elimination ordering on the remaining variables. 
　at each stage we maintain a set of rules with the following program invariant: 
the probability of a context on the non-eliminated variables can be obtained by multiplying the probabilities associated with rules that are applicable in that context. moreover for each assignment  and for each non-eliminated variable there is only one applicable rule with that variable in the head. 
the algorithm is made up of the following primitive operations that locally preserve this program invariant:1 
     1 to make this presentation more readable we assume that each variable is boolean. the extension to the multi-valued case is straightforward. our implementation uses multi-valued variables. 
	poole 	1 

combining heads. 	if we have two rules: 
 1  
 1  
such that a and 1 refer to different variables  we can combine them producing: 
		 1  
thus in the context with a  1  and c all true  the latter rule can be used instead of the first two. we show why we may need to do this in section 1. 
　in order to see the algorithm  let's step through some examples to show what's needed and why. 
example 1 suppose we want to sum out b given the rules in example 1. b has one child e in the belief network  and so 1 only appears in the body of rules for e. of the six rules for e  two don't contain b  rules  1  and  1    and so remain. the first two rules that contain b can be treated separately from the other two as they are true in different contexts. vpe of rules  1  and  1  with rule  1   results in: 

summing out 1 results in the following representation for the probability of e.  you can ignore these numbers  it is the structure of the probability tables that is important.  

thus we need 1 rules  including rules for the negations  to represent how e depends on its parents once 1 is summed out. this should be contrasted with the table of size 1 that is created for ve or in clique tree propagation. 
1 compatible contexts 
the partial evaluation needs to be more sophisticated to handle more complicated cases than summing out 1  which only appears at the root of the decision tree and has only one child in the belief network. 
example 1 suppose  instead of summing out 1  we were to sum out d where the rules for d were of the form: 
 1   1  
 1  
the first three rules for e  rules  1 - 1   don't involve d  and remain as they were. variable partial elimination is not directly applicable to the last three rules for e  rules  1 - 1   as they don't contain identical contexts apart from the variable being eliminated. it is simple to make the variable partial elimination applicable by splitting rule  1  on b resulting 
1 	probabilistic reasoning 
in the two rules: 
 1  
 1  
rules  1  can be used with rule  1  in a variable partial evaluation  and  1  can be used with rule  1 . the two rules corresponding variable partial evaluation with rule  1  are: 

four other rules are created by combining with the other rules for d. 
in general  you have to split rules with complementary literals and otherwise compatible  but not identical  contexts. you may need to split the rules multiple times on different atoms. for every pair of such rules  you create the number of rules equal to the size of the union of the literals in the two rules minus the number of literals in the intersection. 
1 multiple children 
one problem remains: when summing out a variable with multiple children in the belief network  using the technique above  we can't guarantee to maintain the loop invariant. consider the belief network of figure 1. if you were to sum out y  the variables a  1  c  and d become mutually dependent. using the partial evaluation presented so far  the dependence is lost  but it is crucial for correctness. 
　to overcome this  we allow multiple variables in the head of clauses. the rules imply different combinations of the truth of the variables in the heads of clauses. 
example 1 consider a belief network with a and 1 are the only children of y  and y is their only parent  and y has a single parent z. suppose we have the following rules involving a  1  and y: 
 1  
 1  
 1  
 1  
 1  
we could imagine variable partial elimination on the rules for a with rule  1   and the rules for 1 with rule  1   resulting in: 

however  this fails to represent the dependency between a and b that is induced by eliminating y. 
　we can  however  combine rules  1  and  1  resulting in the four rules: 
 1  
 1  
 1  
 1  

similarly  we can combine rules  1  and  1   resulting in four rules including: 
		 1  
which can be combined with rule  1  giving 
		 1  
note that the rules with multiple elements in the head follow the same definition as other rules. 
1 evidence 
we can set the values of all evidence variables before summing out the remaining non-query variables  as in ve . suppose is observed. there are three cases: 
  remove any rule that c o n t a i n s w h e r e in the head or the body. 
  remove any term  in the body of a rule. 
  replace any  in the head of a rule by true. 
rules with true in the head are treated as any other rules  but we never resolve on true. when combining heads containing true  we can use the equivalence: true a = a. 
example 1 suppose d is observed. the rules for e become: 
 1  
 1  
 1  
 1  the rules  1 - 1  for d become: 
 1  
 1  
 1  
d doesn't appear in the resulting theory. 
1 	extracting the answer 
once evidence has been incorporated into the rule-base  the program invariant becomes: 
the probability of the evidence conjoined with a context c on the non-eliminated variables can be obtained by multiplying the probabilities associated with rules that are applicable in context c. 
suppose x is the query variable. after setting the evidence variables  and summing out the remaining variables  we end up with rules of the form: 
is obtained by 

1 	the algorithm 
we have now seen all of the components of the algorithm. it remains to put them together. we maintain the loop invariant of section 1. 
the top-level algorithm is the same as ve: 
to compute given elimination ordering y 1  ...  yk: 
1. set the evidence variables as in section 1. 
1. sum out in turn. 
1. compute posterior probability as in section 1 
the only tricky part is in summing out variables. 
to sum out variable yi: 
1. {rule splitting for combining heads  for each pair of rules such that &i and b1 both contain y  
and  are compatible  but not identical 
　　　split each rule on variables in body of the other rule. {following i  all rules with yi in the body that are applicable in the same context have identical bodies.} 
1. { combining heads} for each pair of rules such that b contains yi 
	and 	and 	are compatible 
replace them by the rule 
{following 1  for every context  there is a single rule with yi in the body that is applicable in that context.} 
1. { rule splitting for variable partial evaluation } for every pair of rule of the form and and b  and b1 are comparable and not identical 
　　　split each rule on atoms in body of the other rule. {following 1  all rules with complementary values for the yi  but otherwise compatible bodies have otherwise identical  bodies and identical heads  
1. { variable partial evaluation } for each set of rules: where the vk are all of the values for y  
for each set of rules such that 
create the rule 
1. {clean up} 
remove all rules containing yi . 
1 comparison with other proposals 
in this section we compare standard belief network algorithms  other structured algorithms and the new probabilistic partial evaluation algorithm. example 1 is particularly illuminating because other algorithms do very badly on it. 
　under the elimination ordering b    c  a  y  z  to find the prior on e  the most complicated rule set created is the rule set for e given in example 1 with 1 rules  including the rules for the negations . after summing out d there are also 1 rules for e. after summing out c there are 1 rules for e  and after summing out a there are 1 rules for e. observations simplify the algorithm as they mean fewer partial evaluations. 
	poole 	1 

not shown. they can be multiply connected. similarly the descendents of a and 1 are not shown. 
　suppose we were to sum out e. once e is eliminated  a and b become dependent. in ve and bucket elimination we form a factor containing all the remaining variables. this factor represents p a  b c d  f g h . one could imagine a version of ve that builds a tree-based representation for this factor. we show here how the rule-based version is exploiting more structure than this. 
　　　suppose e is only relevant to a when d is true  and e is only relevant to b when / is true. in this case  the only time we need to consider the dependence between a and 1 is when both d and / are true. for all of the other contexts  we can treat a and b as independent. the algorithm does this automatically. consider the following rules for a: figure 1: exemplar for a node with multiple children: e to  eliminate. 
　in contrast  ve requires a functor with table size 1 after b is summed out. clique tree propagation constructs two cliques  one containing y  z  a  1  c  d of size 1 = 1  and the other containing a  b  c  d  e of size 1. neither takes the structure of the conditional probabilities into account. 
　note however  that ve and clique tree propagation manipulate tables which can be indexed much faster than we can manipulate rules. there are cases where the rule-base exponentially is smaller than the tables  where added variables are only relevant in narrow contexts . there are other cases where we require as many rules as there are entries in the table  we never require more   in which case the overhead for manipulating rules will not make us competitive with the table-based methods. where real problems lie in this spectrum is still an open question. 
　boutilier et al.  present two algorithms to exploit structure. for the network transformation and clustering method  example 1 is the worst case; no structure can be exploited after triangulation of the resulting graph.  the tree for e in example 1 is structurally identical to the tree for x  1  in figure 1 of  boutilier et al  1  . the structured cutset 
conditioning algorithm does well on this example. however  if the example is changed so that there are multiple  disconnected  copies of the same graph  the cutset conditioning algorithm is exponential in the number of copies  whereas the probabilistic partial evaluation algorithm is linear. 
　this algorithm is most closely related to the tree-based algorithms for solving mdps  boutilier et al  1   but these work with much more restricted networks and with stringent assumptions on what is observable. 
1 	why not trees  
it may be thought that the use of rules is a peculiarity of the author and that one may as well just use a tree-based representation. in this section i explain why the rule-based version presented here can be much more efficient than a tree-based representation. 
　figure 1 shows an exemplar for summing out a variable with multiple children. the ancestors of c  d  f  g  and h are 
1 	probabilistic reasoning 

　finally we can now safely replace e by its rules; all of the dependencies have been eliminated. the resultant rules encode the probabilities of {a  1} in the contexts and u ..      1 rules . for all other contexts we can consider a and 1 separately. there are rules for a in the contexts 
 rule  1    	with the last two resulting from combining rule  1   and an analogous rule created by splitting rule  1   with rules  1  and  1  for 
e . similarly there are rules for b in the contexts 
the total number of rules  including 
rules for the negations  is 1. 
　one could imagine using ve or beba with tree-structures probability tables. this would mean that  once e is eliminated  we need a tree representing the probability on both a and 1. this would entail multiplying out the rules that were not combined in the rule representation  for example the distribution on a and b the contexts  this results in a tree with 1 probabilities at leaves. without any structure  ve or beba needs a table with 1 = 1 values. 
　unlike ve or beba  we need the combined effect on a and 1 only for the contexts where e is relevant to both a and b. for all other contexts  we don't need to combine the rules for a and 1. this is important as combining the rules is the primary source of combinatorial explosion. by avoiding combining rules  we can have a huge saving when the variable to be summed out appears in few contexts. 
1 	conclusion 
this paper has presented a method for computing the posterior probability in belief networks with structured probability tables given as rules. this algorithm lets us maintain the rule structure structure  only combining contexts when necessary. 
　the main open problem is in finding good heuristics for elimination orderings. finding a good elimination ordering is related to finding good triangulations in building compact 
junction trees  for which there are good heuristics  kjaerulff  1; becker and geiger  1 . these are not directly applicable to probabilistic partial evaluation  as an important criteria in this case is the exact form of the rules  and not just the graphical structure of the belief network. 
　the two main extensions to this algorithm arc to multivalued random variables and to allow logical variables in the rules. both extensions are straightforward. 
　one of the main potential benefits of this algorithm is in approximation algorithms  where the rule bases allows finegrained control over distinctions. complementary rules with similar probabilities can be collapsed into a simpler rule. this can lead to more compact rule bases  and reasonable posterior ranges  poole  1 . 
