 
the ease of learning concepts from examples in empirical machine learning depends on the attributes used for describing the training data. we show that decision-tree based feature construction can be used to improve the performance of back-propagation  bp   an artificial neural network algorithm  both in terms of the convergence speed and the number of epochs taken by the bp algorithm to converge. we use disjunctive concepts to illustrate feature construction  and describe a measure of feature quality and concept difficulty. we show that a reduction in the difficulty of the concepts to be learned by constructing better representations increases the performance of bp considerably. 
1 	introduction 
recent progress in artificial neural networks  anns  and their use in disparate domains have spurred the interests of researchers in studying various means of improving them. a n n s have shown promising results for a number of problem areas including content addressable memory  pattern recognition and association  category formation  speech production  and global optimization  kohonen  1; rumelhart et a/.  1; anderson  1; sejnowski and rosenberg  1; hopfield and tank  1 . they have also been tried with fairly good results in financial   d u t t a a n d shekhar  1  and in manufacturing applications  rangwala and dornfeld  1  among other areas. 
   back-propagation  bp  is one of the most widely used neural network algorithms due to its powerful problem solving capabilities. there have been numerous studies by researchers who are interested in the dynamics of the bp algorithm  and the network  multi-layered perceptrons  on which back-propagation is based. although bp has varied advantages  such as being able to represent/learn any function  hornik et a/.  1   an inherent problem with the algorithm is that it is very slow to converge  learn . researchers in the area have successfully 
   *this research was supported in part by national science foundation grant no. iri-1. we would like to thank 
christopher matheus and larry rendell for providing us a copy of citre. 
1 	learning and knowledge acquisition 
implemented modifications to enable fastei convergence of the neural networks using disparate means  ranging from modifications to the bp algorithm itself to appropriate implementations of the algorithm in v l s i components. in this paper  we present a novel technique for improving the learning process in a feed-forward neural network by pre-processing the input  training  data using an automated methodology. 
   a difficult concept shows a high degree of dispersion in the input representation space due to the inability of the low-level measurement attributes to describe the concepts concisely and accurately. given any feature set for data representation  the concept dispersion measure a  ragavan and rendell  1  estimates the difficulty of learning any concept using that feature set  and thus the quality of the feature set. we use feature construction  pagallo  1; matheus and rendell  1; drastal et a/.  1  to develop new features from the original set of attributes  to decrease the concept's dispersion in the constructed feature space. the constructed feature sets are then used as input to the bp algorithm. new features are constructed using c i t r e  matheus  1  and used to improve the performance of the bp algorithm considerably. 
1 reducing concept dispersion by feature construction 
difficult concepts exhibit numerous  peaks  or regions in instance space  rendell and seshu  1  if the attributes used for describing data are inappropriate. these concepts require changes in representation  for example through feature construction. good feature construction can reduce the difficulty of such concepts  by providing a more compact representation for the training data. 
   for difficult concepts  each subset of the training data formed by conditioning on a value of any attribute would contain a large number of both positive and negative examples  and consequently show high uncertainty about the concept class. entropy 1 measures this uncertainty- to measure concept difficulty  we estimate the net conditional entropy in the training data  using all the at-
1
　　 entropy of a boolean concept y is defined as h y  = where p and n are the prior proba-
bilities of finding a positive or negative instance of y. 

tributes  assumed independent  on which the concept depends. 
has a value between 1 and 1. 
　in general  the more difficult the concept  the higher its dispersion. at the other extreme  if any single feature splits the positive and negative examples cleanly  such a feature alone is sufficient to determine the concept; no uncertainty would result when the instances are conditioned on such a feature.  captures the difficulty of a 
　concept for learning using any given feature set. it can therefore be used estimate feature set quality. 
　we use feature construction  specifically using citre  matheus  1   to create new feature sets with lower values. using as an indicator of feature quality  we show that decreases in the successive feature spaces constructed by citre. more importantly  bp performance increases as  decreases. 
   the fringe  pagallo  1  module of citre constructs features iteratively from decision trees. it forms new features by conjoining two nodes at the fringe of the tree-the parent and grandparent nodes of positive leaves are conjoined to give a new feature. new features are added to the set of original attributes and a new decision tree is constructed using the maximum information gain criterion  quinlan  1 . this feature selection phase thus chooses from both the newly constructed features as well as the original attributes for rebuilding the decision tree. the iterative process of treebuilding and feature construction continues until no new features are found. splitting continues to purity  i.e.  no pruning  breiman et a/.  1  is used. the reader is referred to  matheus  1  for a more detailed discussion on citre. 
　we use three different disjunctive boolean concepts to illustrate our experiments. the three functions are: 

three different data sets were generated for the functions y1  y1and y1  and used as input to the fringe feature construction module in citre. the newly constructed features and the trees that were generated for the first function  y1  are shown in fig.l. 
　the  values of the feature sets selected by citre during the different tree generations are evaluated. results for all three disjunctive concepts are shown in fig.1. 
tree generation 1: att1 att1 att1 att1 att1 a t t 1 . 
new features: 
1 and equal att1 false  equal att1 true   1 and equal att1 true  equal att1 ialse   
　1 	and equal att1 true  equal att1 lalse   tree generation 1: 
1 1 a t t 1 . 
new features: 
　1 and equal att1 true  equal 1 true   
　1 and equal 1 true  equal att1 false   
　1 and equal 1 true  equal 1 false   tree generation 1: 
1 1 a t t 1 . 
new features: 
　1 and equal att1 false  equal f1 true   
　1 and equal 1 true  equal 1alse   tree generation 1: 
1 1. 
new features: 
　1 	and equal f1 true  equal f1 false   tree generation 1: 1 1. 
figure 1: features constructed by citre for y1. 

figure 1: quality of the features constructed by citre. 
	ragavan and piramuthu 	1 

as can be seen from this figure  the values drop significantly as new feature sets are used. we use the features from each tree as input to the bp algorithm. as we show in the next section  decreasing the concept's dispersion in the condensed feature sets in this manner speeds up the convergence of the bp algorithm greatly. 
1 	learning with back-propagation 
a number of previous studies  dutta and shekar  1; 
fisher and mckusick  1; mooney et a/.  1; weiss and kapouleas  1  have compared bp with other classification methods including statistical tree induction  with results favoring bp in terms of classification accuracy. one of the major drawbacks of bp is that it is very slow. several researchers  fahlman  1; becker and le cun  1  have worked on this problem trying to increase the convergence speed of the bp algorithm by disparate means. there are four primary means of increasing the convergence speed of bp:  1  by appropriately pre-processing the data used as input to the algorithm   1  by improving the bp algorithm itself   1  by hard-wiring the algorithm using v l s i circuits  and  1  by utilizing the inherent parallelism in the bp algorithm through implementations in parallel machines. 
   several researchers  becker and le cun  1; fahlman  1; parker  1; waltrous  1  have successfully modified the bp algorithm using second-order gradient search methods resulting in improved performance. kung  vlontos  and hwang  describe a v l s i architecture for implementing bp using a programmable systolic array. hinton  and deprit  describe the use of parallel processing computers to implement the bp algorithm with each unit in the network assigned to a processor. 
   in this study  no attempt is made to improve the bp algorithm itself as in previous studies. instead  the data used as input to the bp algorithm is pre-processed  see  piramuthu  1  . more specifically the  of the concept is reduced in the attributes used as input to bp  to enable it to learn more effectively. as discussed in section 1  feature construction can be used to reduce the   of concepts. using feature construction  a subset of the initial and newly constructed attributes that are deemed to be better for representation are used as input to the bp algorithm. the new representation has fewer concept regions per class. this makes the search space less complex  which in turn increases the convergence speed of bp. 
1 	using good feature sets for bp 
the criterion we use to categorize a newly generated feature set as  good  is that it should have small values  relative to the initial feature set. feature spaces with reduced  values have fewer concept regions  and are thus relatively easier for learning  i.e.  for separating the examples belonging to different classes. the disjunctive concepts of section 1 are used to study the effect of decrease in on the convergence speed of bp. 
   as the initial weights in the network were set randomly  we ran the bp algorithm 1 times for each set of 
1 	learning and knowledge acquisition 
table 1: results using bp for all three concepts. standard deviations are shown in parenthesis. 

features corresponding to the various trees constructed by c i t r e . the average of 1 bp runs and their standard deviations are given in table 1. the number of units in the network 1 is also shown in the table. the number of hidden units is roughly half the total number of input and output units for all the networks. the output layer always has 1 unit which classifies an example as either positive or negative. 
   in table 1  the decision trees constructed by c i t r e are indicated by t m n   for the tree constructed after the  iteration for the function y m . the identical 
entries in table 1 for the rows corresponding to the last two trees of each function  e.g.  t1 and t 1   are due to the identical final trees that c i t r e produces on convergence. 
	the 	decision 	attributes 	used 	in 	the 	final 	trees 
 t 1  t 1  t 1   are fewer than those in the initial set  1 . this reduces the number of input units  in turn reducing the hidden units that are necessary. thus the total number of units used in the network is reduced. except for a few cases  the standard deviations for each of the resulting values are low compared to their respective mean values. it can also be seen that the standard deviation values do not have any specific pattern with respect to the number of units used in the neural network. 
   we now take a closer look at the first two performance criteria listed in table 1. the number of epochs and time taken to converge by bp for the three concepts as a function of constructed feature sets  tree generations  are shown in graphically in figs. 1 and 1 respectively. 
   as fig.1 shows  the epochs required for convergence shows a slight initial increase in some cases  but then reduces considerably as better representations are con-
 and c are the number of input  hidden  
and output units respectively. 


　　　　　　　　　　　feature set # figure 1: bp epochs in the new feature spaces. 


figure 1: bp convergence time improvement. 

structed. the number of epochs taken by the final set of features  t 1   t1  t1  to converge decreases to about half the value corresponding to the original attributes  t1  t1  t1   for all three examples. 
   in fig.1  the time taken for bp to converge  for all three examples  drops precipitously as the tree generation proceeds  before finally levelling off. the time taken for the bp algorithm to converge using the final set of attributes is less than an order of magnitude compared to using the initial attributes. 
   this trend of improved performance with decreasing concept difficulty is also clear from the reducing number of connection updates  cus = # epochs x total number of units in the network  in table 1. the reduction in convergence time is substantial due to significant drop in the number of connection updates  as newer feature sets are generated. because of serial processing  the time taken per epoch depends to a large extent on the total number of units that are used in the network. this would not be the case if parallel processors  e.g.  connection machine  are used for the units. 
   neural networks require fewer epochs to learn a concept if its dispersion is decreased by using good features. by constructing new features  we reduce not only the number of effective attributes that are needed to define the concepts  but also increase the average information content at each of the constructed input units. this is achieved by considering the interaction effects of the attributes in disjunctive concept terms in addition to the main effects of the individual disjunctive terms  through 
feature construction. 
1 	discussion 
the back propagation algorithm is being successfully used in commercial applications  such as credit risk rating of companies. there have also been developments in the area of creating expert systems using neural networks. in a commercial credit risk rating situation  for example  the learning speed of the bp algorithm is critical for the firm to be able to make quicker decisions for it to remain competitive. we have shown a means of getting closer to the goal of achieving faster learning using a feed-forward neural network by automating the input feature selection process. feature construction can be used to automatically generate better feature sets  as measured by their  values  which are used as input to the bp algorithm. the proposed methodology also eliminates the least important attributes from the training data  thus facilitating efficient use of computing resources by directing attention to only those attributes important for a given classification problem. 
   advantanges of neural networks such as good performance in high feature interaction domains  indhurkya and weiss  1  are combined with advantages of decision-tree based induction by this method. incorporating a front-end like c i t r e to the bp algorithm also provides a facile technique for introducing domain knowledge in neural nets. knowledge gets compiled into the constructed features. 
   it should also be noted that the classification accuracy was a full 1 percent for feature sets corresponding to each of the trees  t1  t1  t1  t1  t1  
t1  t1  t1  t1  t1  t1  t1  t1  t1  t1  t1   in spite of the reduction in the number of attributes used as input after feature construction. by using a set of attributes with reduced  along with other means of increasing the convergence speed such as second-order gradient methods  the convergence speed of the bp algorithm can be significantly improved. performance with noisy data remains to be investigated. 
	ragavan and piramuthu 	1 

