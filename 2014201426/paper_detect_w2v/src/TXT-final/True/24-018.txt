 
delayed reinforcement learning is an attractive framework for the unsupervised learning of action policies for autonomous agents. some existing delayed reinforcement learning techniques have shown promise in simple domains. however  a number of hurdles must be passed before they are applicable to realistic problems. this paper describes one such difficulty  the input generalization problem  whereby the system must generalize to produce similar actions in similar situations  and an implemented solution  the g algorithm. this algorithm is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received. connectionist backpropagation has previously been used for input generalization in reinforcement learning. we compare the two techniques analytically and empirically. the g algorithm's sound statistical basis makes it easy to predict when it should and should not work  whereas the behavior of backpropagation is unpredictable. we found that a previous successful use of backpropagation can be explained by the linearity of the application domain. we found that in another domain  g reliably found the optimal policy  whereas none of a set of runs of backpropagation with many combinations of parameters did. 
1 	background 
delayed reinforcement learning is a framework for learning  without supervision  to act in an environment  sutton  1 . in this framework  an agent is given on each tick  in addition to  perceptual  inputs  a numerical reinforcement  which is a measure of the immediate value of the state corresponding to the inputs. the goal of the agent is to choose actions to maximize the sum of reinforcement over time. 1 
   *this work was supported by the defense advance research projects agency under nasa contract nas1. 
1
　　or  more accurately  to maximize a future-discounted sum of reinforcement  as we will explain. 
1 	learning and knowledge acquisition 
　delayed reinforcement learning is attractive due to its similarity to the problem faced by a person or other creature placed in unfamiliar surroundings and expected to act intelligently. such problems are of increasing theoretical and practical interest due to recent progress in the construction of autonomous agents such as mobile robots. though such systems have achieved new levels of performance  they generally depend on elaborate handcoded policies in computing how to act. when the environment for which they were designed is changed slightly  they may fail gracelessly; they are unable to adapt to new environments; and the process of hand-coding policies they require is slow and error-prone. agents whose action policies are developed autonomously from a reinforcement signal might transcend all these problems  chapman  1  appendix b . 
　the bulk of experience with delayed reinforcement learning methods has been in simple domains that do not stretch their capabilities. the work described in this paper began by applying existing techniques to a more difficult task domain. this domain raised technical problems that our work here addresses. 
1 	temporal difference learning 
the best-understood approach to delayed reinforcement learning is temporal difference  td  learning  codified by sutton and his colleagues  barto et al.  1a; sutton  1 . two forms of td learning have been studied in detail  the adaptive heuristic critic of sutton  and q-learning  due to watkins . several authors have compared these methods empirically and found q-learning superior  kaelbling  1; lin  1; sutton  1   so we adopted q-learning as our starting point. 
　q-learning is based on estimating the values of q i  a   which is the expected future discounted reinforcement for taking action a in input state i and continuing with the optimal policy. the discounted reinforcement is the sum of all future reinforcement weighted by how close they are; specifically 
is a discount fac-
tor close to one  and r{t  is the reinforcement received 
at time t. thus the q values say how good an action is not only in terms of immediate reinforcement  but also in terms of whether it gets the agent closer to future reinforcement; so in effect they allow the learner to dynamically create subgoals based on reinforcement that may be far in the future. given q values  we can also compute the value of a state as the value of the best action in that state: 

a 
the learning algorithm works by creating a two-dimensional table of q values indexed by actions and inputs and then adjusting the values in the table based on actions taken and reinforcement received. this process is based on the observation that q of the current  input action  pair can be computed based on the immediate reinforcement received and the value of the next input:  

for further details  see  kaelbling  1; watkins  1 . 
1 	t h e d o m a i n 
the domain used in this research was the videogame 
amazon  previously used in the system sonja  chapman  
1 . more accurately  we studied an apparently simple subproblem from this domain  which turned out to be unexpectedly difficult. 
   in amazon  the player controls an  amazon  icon which is attacked by ghosts. on each tick  the player can move the amazon one pixel in any of the eight  king's move  directions. since the amazon is fifty pixels high  the player has very fine-grained control over motion.  the otherwise similar videogame domains used by other researchers have coarse motion control  in which the player icon moves its entire width in a single tick. we'll see that this difference matters.  ghosts similarly move a single pixel on each tick. the player can cause the amazon to shoot a projectile in the amazon's direction of motion. projectiles move in a straight line  four pixels per tick; if they collide with a ghost  the ghost dies. thus  the amazon must be aligned with a ghost in one of the king's move directions to kill it. the reinforcement given is 1 when a ghost dies  otherwise -.1 if a shot is taken  otherwise 1. 
   the subproblem from amazon we studied generates a ghost at a random distance and orientation from the amazon  waits for the amazon to kill it  and then repeats. we experimented with various forms of perception  some described in  chapman and kaelbling  1   all delivering a relatively small number of input bits. 
   this domain is difficult for several reasons. first  some states are rare  and it is hard to gain enough experience with them to find the optimal policy. second  ghosts are programmed to avoid alignment w i t h the amazon. when the amazon and the ghost are unaligned  there are limited opportunities for the system to learn. in particular  the system is not  locally  reinforced for actions that lead to killing the ghost  because it is impossible to kill the ghost while unaligned. under a random strategy  the amazon and ghost will typically stay unaligned for stretches of several hundred ticks punctuated by brief 
periods of alignment terminated by ghost death. thus most experience is of very limited value  and the interesting reinforcements  ghost deaths  occur infrequently. third  there is a great variance in the value of states that the limited perception does not make available. how nearly aligned the ghost is and how close it is to the amazon determine how long it will be before it is possible to kill it. if the system gets a series of  easy  ghosts in a row it can readily come to wrong conclusions about a state value. this means that the learning rate must be sufficiently low to even such differences out; but that makes learning slow. 
　a fourth difficulty in this domain lead directly to the g algorithm. we hoped to use sonja's visual system as an input representation for q-learning  and that the learner could come to control this active visual system in which the computations performed are chosen top-down. progress along these lines has been reported by whitehead and ballard . however  this visual system provides more than a hundred bits of input  corresponding to more than 1 distinct inputs. this is a problem for two reasons  space and time. one simply cannot allocate an array one of whose dimensions is 1. even if you could  it would divide the state space up into tiny pieces  each of which would occur extremely rarely  so the system could never accumulate enough experience to gauge the value of most states. somehow a learning algorithm must guess about the value of states based on experience with similar states. but how can it know which states are similar when it has no experience with them  
1 	the g algorithm 
the g algorithm addresses this problem of input  generalization in a q-learning framework.1 although generalization over inputs is a large field of study in machine learning  delayed reinforcement learning puts special constraints on the problem that make most general techniques inapplicable. 
　we can motivate the g algorithm in two ways. the first is to see it as collapsing the exponential sized q table engendered by large numbers of input bits. in most domains  large chunks of the table should have identical entries  because many of the bits will be irrelevant to acting under certain circumstances. if we can figure out which bits are irrelevant  we can summarize a large region of the state space with only one q value  thereby saving both space  to store the values  and time  since experience with any state in the region can be used to update the single q value . 
　another way of looking at the algorithm is as a technique for incrementally constructing the sort of action selection networks that have recently been used in various situated machine agents  beer  1; brooks  1; chapman  1; connell  1; kaelbling and rosenschein  1 . these networks are digital circuits that compute what to do based on perceptual inputs. the circuits are kept shallow in order to compute quickly when 
1
　　for further discussion of the algorithm  see  chapman and kaelbling  1 . 
	chapman and kaelbling 	1 

figure 1: a g tree. internal nodes correspond to input bit splits; the algorithm collects statistics about the subspaces represented by the leaf nodes. 
implemented on slow  massively parallel hardware. typically they maintain little or no state. because the networks are shallow  they have the property that although every input bit is used in computing how to act  in most situations most bits are ignored. for example  sonja searches for the amazon whenever the visual system reports losing track of it; all other inputs are irrelevant in this case. 
   the g algorithm incrementally builds a tree-structured q table. it begins by supposing that all input bits are irrelevant  thereby collapsing the entire table into a single block. it collects q values within this block. g also collects statistical evidence for the relevance of individual bits. when it discovers that a bit is relevant  it splits the state space into the two subspaces corresponding to the relevant bit being on and off. then it collects statistics  action and relevance  within each of those blocks. these blocks can in turn be split  giving rise to a treestructured q table  figure 1 . the system acts on the basis of the q statistics in the leaf node corresponding to an input. thus the tree acts as a boolean input classification network  essentially similar to the sorts of action networks described above. 
   the incremental  one-bit-at-a-time construction of the g tree puts a constraint on the sorts of environments that g can learn in: the relevance of bits must be apparent in isolation. the algorithm will fail if groups of bits are collectively relevant but individually irrelevant. if we consider the perceptual system of an agent to be part of the  environment  of its learning system  as we must  then this constraint can be placed on that system rather than the world. in other words  we hypothesize that a well-designed perceptual system orthogonahzes inputs such that they are individually relevant. 
   the g splitting technique is related to existing algorithms  such as id1  quinlan  1  and c a r t  breiman et a/.  1   for inducing decision trees. the crucial dif-
ference is that the decision-tree algorithms are presented with input/output pairs rather than reinforcement data; for this reason  the statistical tests used to make splits must be different. also  our work has emphasized making incremental decisions with a fixed amount of computation per tick rather than learning the shallowest or 
1 	learning and knowledge acquisition 
smallest tree.1 
   the g algorithm applies to input generalization. a similar problem arises on the output side: if the number of actions is very large  the learner can not hope to try each in every state. for example  sonja's visual system has several dozen control bits that  the action policy must set on every tick. kaelbling  has described an approach to this problem. we believe  however  that the g algorithm should be directly applicable to the output generalization problem. that is  the system could keep track of the effect of individual output bits on reinforcement received in particular input blocks  and construct a tree of output bit relevance analogous to the input bit relevance tree. we have not implemented this  however. 
1 	statistics 
1 	d i s c r e t e r e i n f o r c e m e n t : d statistics 
we found the standard q technique insufficiently sensitive in the amazon domain. the problem is that q simply sums all the reinforcement it gets  without distinguishing between different reinforcement values. for example  if the system is acting at random  as it does initially  it will typically have to shoot off many pro-
jectiles before killing a ghost. as the value of killing a ghost is only 1 and the cost of shooting is - . 1   the 1 can get lost when summed with enough -.is. to solve this problem  we extended q to make more distinctions. specifically  we effectively added a third dimension to 
q i  a   keeping track of d   z   a   r     the discounted future probability of receiving reinforcement r after performing action a given input i: 

this extension separates out the various possible reinfor cement values and so gives better statistical information. the q values can be recovered with the identity 

where r is the space of reinforcements. this extension to q-learning is possible only when the reinforcement given is discrete and takes on only a relatively small number of values  though it might be possible to use buckets to apply it in cases of continuously varying reinforcement . it's not surprising that d-learning works better than q-learning; it is superior for the same reason that qlearning is better than the adaptive heuristic critic: it keeps track of more distinctions. the logical next step in this progression would be to keep track of input-actioninput triples  as  for instance  drescher  has done. this raises questions of combinatorial feasibility  however. 
1
　　utgoff's  id1 algorithm works incrementally  but a single instance can require a large amount of work if it causes a node to be  pulled up  in the tree. 
1 	the bit relevance test 
c uses a standard statistical test  the student's t test  snedecor  1   to determine when a bit is relevant. the t test tells you  given two sets of data  how probable it is that distinct distributions gave rise to them. that is  how likely is it that these two sets of data arose from the same underlying process  this is just what we need in order to determine whether an input bit is relevant: is the learner/environment interaction the same when the bit is on versus off  or is it different  
　two sorts of relevance statistics are kept: a bit may be relevant because it affects the value of a state or because it affects the way the system should act in that state. two sorts of statistics are used to determine value  corresponding to the mean immediate value of the state and its mean discounted future value. both sorts are required; immediate value is used to  bootstrap  the process by recognizing the states that themselves give large reinforcements  e. g. those in which a projectile is flying toward the ghost  and discounted value is used to find states that lie on a path toward externally reinforced states  such as those in which the ghost and the arnazon are aligned . for each bit in each state block  g keeps track of the immediate and discounted values of the state block subdivided by the bit being on and off  and compares these values with the t test. 
　a bit may also be relevant because it affects how the agent should act; for example the input bits indicating the direction to the ghost do not affect the values of 
states  but they do determine which direction the player should head in. to discover such relevance  g keeps track  for each action in each state block  of the discounted value of taking the action in that state block when the bit is on versus when it is off  and compares these values with the t test. 
- when a bit is shown to be relevant in a block  that block is split on the bit. when a block is split  all discounted statistics  both action value and relevance  must be zeroed. the reason is that a state block whose mean value is low may have a subblock whose value is high. before the split is made  this high-valued subblock is effectively invisible  and the estimated values of all states that can transition to that subblock will be too low. throwing away all experience accumulated thus far on each split seems too drastic. we are exploring ways of avoiding this  and expect that they will substantially 
increase the learning rate. 
1 	enforcing normality 
unfortunately  the t test depends on the assumption that items sampled are distributed normally. most statistical techniques make such assumptions.1 the normality assumption is violated by amazon  because the interesting reinforcement value  for killing a ghost  occurs so rarely. we found  as a result  that the test as specified in the last section often made incorrect judgements of relevance. 
　normal statistics are frequently used to examine nonnormal data  and this is often successful due to the cen-
　　1 an alternative is to use nonparametric statistics  which are unwieldy and seemed inappropriate to this domain  for reasons too complex to go into here . 

figure 1: noise bits that change slowly relative to estimated state values look relevant. 
tral limit theorem which states that the sum of a set of values from an arbitrary distribution will approach normality as the number of samples increases. we were able to eliminate most incorrect relevance judgements by delaying splitting until enough samples had been collected for their distribution to approach normality. this fix depends on a numerical threshold whose values may vary according to the domain. a better-motivated alternative would be to use statistical tests of normality  such as skew and kurtosis  snedecor  1   to decide whether enough samples have been collected to trust the data. 
1 	low frequency noise 
the techniques described in the previous sections were mostly sufficient in practice to ensure that the system did not split on irrelevant bits. an additional problem arose in some cases  however: while bits that changed rapidly presented no problems  irrelevant slowly-changing bits continued to pass the / test. figure 1 illustrates the reason. if an input bit changes slowly relative to changes in estimated state values  the statistics collected to determine the discounted value of a subspace are skewed. in the figure  the estimated value of the state starts low and converges to a higher value. initially the bit b1 is low  and later goes high. as a result  it will appear that b1 being on makes this subspace more valuable and the system will split. 
　the solution to this problem is to separate learning into action value and bit relevance phases. estimated q values are held constant while bit relevance statistics are collected. the system switches phases when values seem to have settled down  based on information about the derivatives of the statistical measures. 
1 	performance comparisons 
relevance-splitting in g has performed well in our problem domain. we have run it for well over a million ticks with ten bits of noise given in addition to the standard inputs; it never split on any of these noise bits. on the other hand  on several runs on each of several variations of the problem g has always split on all the bits that arc relevant. having done so  it has always learned the optimal policy for the domain. the total learning time for the simplest version of the problem runs around 1 ticks. 
　the system learns many times slower than q on problems with few inputs bits  because it has to find the right 
	chapman and kaelbling 	1 
splits before learning a policy. however  we have successfully run g on problems with thirty input bits  for which q could not allocate memory to store its table  much less accumulate the billions of ticks of experience neccessary to fill it out. 
　several other methods have been applied to input generalization for reinforcement learning. watkins  used the cmac algorithm. mahadevan and connell   working simultaneously with and independent of us  developed a statistical clustering method that is neatly dual to g. rather than starting from a single merged state and splitting it  they start with fully differentiated inputs and merge them when they are found to behave similarly. we hope to compare g with cmac and with this clustering technique in future work. 
　anderson  and lin  have successfully combined td methods with connectionist backpropagation  which can generalize given a large number of input bits. others  chapman  1; kaelbling  1; shepanski and macy  1  have attempted the combination and reported negative results; the combination of td and backpropagation sometimes learns very slowly and converges to poor action choices. it is hard to resolve the discrepancy analytically because backpropagation is ill-characterized; it is impossible to know how the algorithm will perform when presented with a complex problem because it often converges to bad local minima. 
　to better understand backpropagation's success  we examined more closely lin's domain  a video game in which a player collects food and avoids obstacles and enemies. it occured to us that potential field navigation  in which the direction of motion depends on a vector sum of attractive and repulsive forces  might be an adequate strategy for this domain  and furthermore that such a strategy would arise from a q i a  function that is linear in lin's retinotopic input representation. if this is the case  the good performance of backpropagation would not be surprising; by the perceptron convergence theorem  there should be no local minima to fall into. 
　we tested this hypothesis by using a linear associator in place of backpropagation. figure 1 demonstrates that the linear learner does as well as backpropagation. this suggests that this domain is unexpectedly easy  and that the success of backpropagation should not necessarily be expected to transfer to other domains in which the q function is nonlinear. 
　g would not work in this domain with the retinotopic input encoding because each of the 1 input bits is relevant in every situation. g would try to split on all of them and would soon generate too large a tree. however  as we have argued elsewhere  chapman  1   the inputs to mammalian policy learning systems are almost certainly not retinotopic  and we should not try to optimize our learning systems for such inputs. we hypothesize that  intermediate  visual inputs should be easier to learn from than retinotopic ones. 
　the g algorithm is a more direct approach to the generalization problem than is backpropagation. it is mathematically well-characterized due to a sound statistical basis  and it is therefore easier to determine when and why it should or should not work. given the difficulty in 
1 	learning and knowledge acquisition 

figure 1: typical learning curves for q plus backpropagation  solid  and linear associator  dotted  in lin's video game domain. the horizontal axis is games played and the vertical axis is the average number of pieces of food collected. 
predicting the performance of q plus backpropagation  a fair empirical comparison of the two methods would require tests on a spectrum of domains. 
　we tested q plus backpropagation on the simplest amazon problem with the same input representation used with g. backpropagation has numerical parameters that must be tuned for a domain: the learning rate and the number of hidden units. we carried out several dozen runs with a wide variety of combinations of settings of these parameters. we checked every thousand ticks to see if the system had yet found the optimal policy; the average run length was about 1 ticks  twice the time required by  *   and some were as long as 1 ticks. backpropagation never found the optimal policy. it is possible  though  that some other combination of parameter settings  or longer runs  would eventually find the solution. also  our backpropagation engine does not implement momentum  so this parameter was effectively zero in all runs. 
　the input generalization problem is one of the most important in attempting to apply temporal difference learning to complex domains. further analytic study and more detailed empirical testing  involving a spectrum of domains  is needed. 
acknowledgments 
thanks to tom dean  long-ji lin  stan rosenschein  
stuart russell  steve whitehead  and two anonymous reviewers for their various contributions to this work. 
