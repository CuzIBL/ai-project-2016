 	case-based reasoning  cbr  models  cf.  riesbeck & 
schank  1; owens  1   recognize pragmatically-useful 
　most ai simulations have modeled memory retrieval index patterns that allow retrieval of episodes  or cases  likeseparately from comprehension  even though both activly to aid their current task. cbr models generally simulate ities seem to use many of the same processes. we have reasoning of experts within a given domain  rather than gendeveloped remind  a model that performs both episoderal human reminding. as in general reminding models  most ic memory retrieval and language understanding with a 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　cbr models use hand-coded cases and operate separately single spreading-activation mechanism. this approach from the comprehension process  though a few do some rulehas a number of advantages over retrieval-only models. based reasoning to make explanations  cf.  hammond  first  because the comprehension process makes infer-
　　　　　　　　　　　　　　　　　　　　　　　　　　　　1  . most cbr models instead focus on deciding which ences about actors* plans and goals  remind is able to particular abstract indices  or features  are most useful for reget abstract remindings that would not be possible withtrieval of useful cases in different problem-solving tasks. out an integrated model. it also allows a more psycholog-
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　although separating memory retrieval from the language ically-plausible model of reminding than previous understanding process makes accounts of the phenomena approaches  since all aspects of a text's interpretation afmore manageable  it is undeniable that real-world retrieval refect what is retrieved through the spreading-activation sults from comprehension processes. we have developed reprocess  as in human reminding. an inferencing-based 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　mind 	 retrieval 	from 	episodic 	memory 	through retrieval model such as remind also has several com-
　　　　　　　　　　　　　　　　　　　　　　　　　　　　inferencing and disambiguation   lange & wharton  in putational advantages over pure retrieval models. the efpress   a model that integrates language understanding and fects of the understanding process eliminate the need for memory retrieval in a single structured spreading-activation the separate  purely structural comparisons used in most mechanism. this approach has several computational advananalogical retrieval models. further  it potentially extages over retrieval-only models and provides a more psychoplains how the explicit indexing of case-based reasoning logically-plausible model of reminding. this paper shows models can be eliminated  while retaining its benefits as several simulations that illustrate some of these advantages  an emergent property of the comprehension process. contrasting it with retrieval-only and cbr approaches. 
1 introduction 
1 overview of remind 
the most parsimonious account of comprehension and re-
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　remind is initially given a syntactic representation of a minding is that they  amount to different views of the same short text as a cue. using general knowledge stored in its mechanism   schank  1 . however  most ai models that long-term memory  remind constructs an elaborated interperform memory retrieval do so in isolation from the lanpretation of the cue and retrieves the episode that is most simguage understanding process. different retrieval models apilar to the surface and inferred features of that representation. proach varying aspects of the retrieval problem and have 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　remind's structured spreading-activation networks endifferent goals  but nearly all are given fully hand-coded repcode world knowledge about concepts and general knowlresentations of the memory episodes  or cases  they use. edge rules for inferencing in the same way as robin  lange 
most psychological models of memory retrieval simulate 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　& dyer  1   a structured connectionist model that perempirical results showing that reminding is based on both forms high-level inferencing and disambiguation for natural surface feature similarities  e.g.  shared words  and analogilanguage understanding. structured connectionist models cal similarities  e.g.  shared inferences and themes   wharton seem to be particularly well-suited to language understanding et al  in press . systems such as arcs  thagard et a/.  1  because their constraint satisfaction abilities are ideal for inand mac/fac  gentner & forbus  1  model this with tegrating contextual evidence to perform disambiguation and two-stage retrieval processes that first search for all episodes priming  while their network structure allows for the represharing surface features with the cue. they then select the epsentation of complex knowledge and parallel inferencing. isode that shares the most surface and analogical similarities 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　remind's networks also contain representations of prior by mechanisms that explicitly calculate structural isomorepisodes  such as fred put his car in the car wash before his phism  or analogical similarity  between the cue and targets. 

date with wilma  car wash  and billy put his playboy under the bed so his mother wouldn 1 see it and spank him  dirty magazine . the representations used are the actual plan/goal analysis  or interpretation  that was inferred by remind when input for them was first presented to the network. these prior episodes are indexed into the semantic comprehension network through connections with all the knowledge structures with which they were understood. to perform retrieval  remind is given a short text passage to understand and use as a cue. this understanding process often requires disambiguation and for a number of inferences to be made. consider: 
john put the pot inside the dishwasher because the police were coming.  hiding pot . 
　although it initially appears that john is cleaning a cooking pot  this sentence is disambiguated and interpreted to mean john was hiding marijuana from the police to avoid being arrested. to understand such cues  units in the network representing the cue and its syntactic bindings are clamped to high levels of activation. activation is then spread through the network. by propagating signature activation patterns  lange & dyer  1   the network makes the different possible inferences explaining the input in a manner similar to markerpassing systems  cf.  riesbeck & martin  1  . for example  one of the multiple interpretation paths that gets inferred  and activated  as a possible explanation for hiding pot is the interpretation that john was trying to hide the pot from the police to satisfy his goal of avoiding arrest. other interpretations concurrently activated include the possibilities that he was trying to clean the pot or store it. activation spreads until the network settles.the units with the most activation represent the most plausible set of inferences and the network's disambiguated plan/goal interpretation of the cue. 
　because units representing long-term memory episodes are connected within the network  episodes having concepts related to the elaborated cue also become highly activated. this includes episodes only superficially-related to the cue because of surface feature overlap  e.g.  episodes involving police or illegal drugs  and episodes related abstractly because they share similar inferred plans and goals  e.g. episodes sharing the inferences that a person was trying to avoid-detection of something to avoid a punishment . after the network settles  the episode receiving the most activation from the cue's interpretation becomes the most highly activated  and is retrieved as the best match for the cue. 
　thus  in remind  a single mechanism drives both language understanding and memory retrieval processes. the same spreading-activation mechanism that infers a single coherent interpretation of a cue also activates episodes retrieved from memory. episodic activation results from both the surface semantics of the input  i.e.  different possible word and phrase meanings  and the deeper thematic inferences made from the input. accordingly  recalled episodes depend on both surface and analogical similarities with the cue. because both inferencing and memory retrieval occur within a single integrated network  the context in which interpretations are formed affects the episodes that are retrieved  which in turn influences the context in which disambiguation and interpretation of input takes place. thus  text comprehension and memory retrieval processes are tightly coupled. 
1 cue understanding in remind 
as with robin  remind uses structured networks of simple connectionist units to encode semantic networks of frames and rules representing world knowledge  such as the scripts  plans  and goals  schank  1  necessary for understanding stories in a limited domain. figure 1 shows a partial overview of a remind network and the knowledge given to it  by hand  as in most structured models . however  it is given no specific information about episodes it will understand. 
　knowledge given to remind is used to construct the actual structure of the network before any processing begins. as with other structured connectionist models  nodes in the network represent particular frames or roles. relations between concepts are represented by weighted connections between nodes. activation on concept nodes is evidential  corresponding to the amount of evidence available in the current context. the network also has additional structure to solve connectionist networks' variable binding problem by propagating signature activation patterns representing bound concepts  lange & dyer  1 . the network makes inferences in parallel by propagating signature bindings  such as of john  marijuana  and cooking-pot  over connections between binding units that represent general knowledge rules. 
　as an example  consider how hiding pot is understood by the network. to represent john put the pot inside the dishwasher  transfer-inside is clamped to a high level of evidential activation  black box in figure 1 . the binding units of its roles  not shown  are clamped to the signature activations of its bindings  john for its actor  cooking-pot or marijuana for its object  and dishwasher for its location . 
these signature bindings then propagate  as activation  over connections to the corresponding roles of neighboring frames. this propagation allows the network to infer that the pot is inside-of the dishwasher and that it was done either because it was going to be cleaned  inside-of-dishwasher and following frames  or because it would be blocked from sight  inside-of-opaque and following frames . 
　as signatures propagate to perform inferencing  evidential activation spreads and accumulates along conceptual nodes to disambiguate between competing inferences. initially the inside-of-dishwasher path receives the most activation because of feedback between its strong stereotypical connections to cooking-pot and dishwasher. however  activation feedback between inside-of-opaque and inferences from the police coming  transfer-self...block-see  and the police-capture frames causes inside-of-opaque to end up with more activation than inside-of-dishwasher and marijuana with more activation than cooking-pot. 
　the network's final interpretation of hiding pot includes the most highly-activated path of frames in figure 1 and their network signature bindings. this interpretation includes the inferences that  a  marijuana is inside of an opaque dishwasher  inside-of-opaque  and is blocked from sight  block-see    b  john possesses illegal marijuana  possess-lllegal-obj   and  c  john is in danger of being arrested by the police  police-arrest . note that alternative interpretation paths retain activation for possible reinterpretation  since remind uses inhibition that normalizes activations rather than driving losers to zero. see lange & dyer  for further details on how the network performs such inferencing and disambiguation for hiding pot and other inputs. 


figure 1. overview of network segment after activation has settled in processing hiding pot. actual network represents frames and their roles by structured sub-networks of units holding evidential and signature activations. gray boxes represent level of evidential activation on the frames  darker = higher activation . circles above frames indicate a long-term instance of frame in an episode. episodes understood and stored here: 1  dirty magazine. 1  car wash. 1  jane shot mark with a colt-1. he died. 1  betty wanted to smoke a cigarette  so she put it on top of the stove and lit it. 1  the pleasure boat followed the whales to watch them. 1  barney put the flower in the pot  and then watered it. 1  mike was hungry. he ate some fish. 1  suzie loved george  but he died. then bill proposed to her. she became sad. 

1 memory retrieval 
in remind  memory retrieval is a side-effect of the spreading-activation understanding process. representations of previously-understood episodes are connected directly to the semantic network that understood them originally. knowledge structures activated when understanding a cue activate similar episodes that were stored in the network earlier. this direct form of  indexing  causes episodes that share many conceptual similarities with the cue to become active during interpretation. the most active episode is retrieved. 
1 network encoding of episodes 
whereas remind uses hand-coded general world knowledge  it is not given any information about the particular episodes it processes and stores in long-term memory. target episode representations are created entirely by reminds spreading-activation understanding process. input for each episode is presented to the network  which infers an interpretation by the spread of signature and evidential activation as described before. next  units and connections are added to store the episode* s entire resulting interpretation in the network. thus  each episode's representation includes all aspects of its interpretation  from its disambiguated surface features  such as the actors and objects in the story  to the plans and goals the network inferred the actors were using. 
　for example  consider how dirty magazine  billy put the playboy under his bed so his mother wouldn 't see it and spank him  is processed and encoded as a memory episode. first  input for its phrases is clamped and an interpretation inferred. as in hiding pot  the network infers that somebody is hiding something  avoid-detection  and that it is blocked from sight  block-see . here  however  the inferred signatures show that it is billy hiding a playboy-magazine rather than john hiding marijuana. several other knowledge structures involved in hiding pot  e.g. proximity-of  possessobj  punishment  are also activated by dirty magazine. 
however  there are a number of differences  e.g. frames of the 
guardian-discipline structure are part of dirty magazine's interpretation  but the police-capture frames are not. 
　once an interpretation is formed of an episode  units and connections are added to the network  by hand  to represent all of its instantiated frames and elements.  lange & wharton  in press  describes the actual units and connections added to do this; the important part is that the added units representing each instantiated frame  such as a particular instance of avoid-detection where billy was hiding a playboy-magazine  are added locally to its semantic network frame and that their connections encode their bindings. the new instance units are also all connected to an episode unit that groups all of the episode's elements. an overview of the final result is shown in figure 1  with the nodes having a circled   1   above them representing the frames inferred and encoded as part of dirty magazine's representation. other circled numbers represent elements of other stored episodes' interpretations. 

1 the retrieval process 
with episodes understood and stored in the network  retrieval is performed simply by presenting an input cue to the network to be understood. because instance units representing episodes are connected directly to the normal semantic units  they become activated by the inferencing spread of activation. the more similarities an episode shares with the inferred interpretation of a cue  the more of its instances become active and the more activation its episode unit receives. 
　figure 1 shows activations of the eight episodes from figure 1 during understanding of hiding pot. episode.1  barney put the flower in the pot  and then watered it  initially becomes highly active because it shares a number of surface features - e.g. both involve a transfer-inside  and planting-pot gets activation from pot. similarly  episode.1 and 
episodes having varying degrees of shared features become active. however  as time goes on  the hiding and punishment frames are inferred and become active. episode. 1  dirty magazine 's activation thus climbs and eventually wins  because it shares the most surface and abstract features of any episode with hiding pot's interpretation  see figure 1 . it is therefore retrieved as the episode most similar to hiding pot. 
1 experiments 
remind has been tested with a knowledge base having 1 conceptual frames and 1 inference rules. it has understood and retrieved the examples here and a number of other episodes of similar complexity. here we briefly describe three additional simulations that illustrate  1  the importance of inferences and disambiguation on retrieval   1  the strong influence of superficial feature similarities on retrieval  and  1  the effect of episodic recall on the understanding process. 
1 importance or inferencing 
an example of how strongly the inferencing and disambiguation of the model affects retrieval is shown in figure 1  which shows activations after presentation of input for john 
put the pot inside the dishwasher because company was coming  dinner party . note that although this cue differs from hiding pot by only a single word  company instead of police   the interpretation remind reaches is completely different  i.e. that he was cleaning a cooking pot to prepare for a 

figure 1. episode unit activations of figure 1 episodes and episode.1 after presenting hiding pot. 
dinner party . this causes a different episode to be recalled  episode.1  car wash   which shares the goals of cleaning something to prepare for an entertainment event. 
1 superficial similarities 
as in human reminding  remind often retrieves superficially similar episodes to a cue even when a better analogy exists. as an example  figure 1  shows the episode activations after hiding pot is presented to a network having the eight episodes of figure 1 and an additional superficially similar episode  cheech put the grass inside the hong because chong was coming  episode.1 . notice that the activation of this new episode quickly dominates the others because of the surface features it shares with hiding pot. this superficially similar  but thematically dissimilar episode is therefore retrieved even though dirty magazine is a better analogy. 
1 effect of reminding on interpretation 
remind's integration of the reminding and understanding processes has pragmatically interesting and useful effects on the understanding process. episodes that become active while inferencing feed activation back into the comprehension part of the network. this can prime and bias the interpretation remind settles on for a given input. consider: 
the star loved the plumber  but he was shot by a thief. then the astronomer proposed to her. she started to cry.  astronomer proposal . 
　two possible reasons for the movie star starting to cry are that the proposal either made her extremely happy  happyproposal  or extremely sad  unhappy-proposal . perhaps the most likely reason for her crying was that the proposal reminded her of murdered lover  therefore making her sad. remind  however  does not have the complex knowledge about memories and how they affect people's emotions thai would be necessary to make that interpretation. as shown in figure 1  remind therefore ends up interpreting the star's crying in astronomer proposal as a happy-proposal  because of its weights' strong biases that marriages are happy. as astronomer proposal illustrates  remind often arrives at counter-intuitive interpretations of stories when the biases of its connection weights are too strong or when it does not have enough knowledge to make the needed inferences for the right interpretation. however  when there is a highlyanalogous episode  or case  in memory  the influence of episodic retrieval upon text understanding can lead remind to a correct interpretation of its input. for example  consider: 
suzie loved george  but he died. then bill proposed to her. she became sad.  sad proposal  
　sad proposal is similar to astronomer proposal  but explicitly states that suzie became unhappy after the proposal. 

figure 1. activations of episodes and of happy-proposal and unhappy-proposal interpretations after astronomer proposal is presented to network containing sad proposal episode. 
this leads the network to make the correct interpretation  that the marriage-proposal after the death of her lover was an unhappy-proposal. this interpretation  including the inference unhappy-proposal.1  is stored in memory as episode.1 in figure 1. figure 1 shows the activations of sad proposal  episode.1  and the other episodes as astronomer proposal is then understood by remind. as expected  sad proposal quickly dominates the other episodes because it is so similar to astronomer proposal episode.1 becomes temporarily active because it also involves a shooting. however  sad proposal eventually wins and is retrieved. 
　the most interesting result in figure 1 is the activation levels of the competing happy-proposal and unhappy-proposal frames. as when astronomer proposal was presented to the network without any episodes in memory  happy-proposal initially has more activation than unhappy-proposal. in this case  however  episode.1 is highly active  and with it 
unhappy-proposal.1. since active episode instances feed activation back into their concepts in the understanding network  unhappy-proposal gets significant activation from unhappy-proposal.1. this added evidence allows its activation to climb over happy-proposal  which gets no added evidence from memory episodes. unhappy-proposal therefore ends up with more activation than happy-proposal  so remind's interpretation is that the proposal made the movie star unhappy. the network therefore selects the correct interpretation of astronomer proposal because of activation feedback from an analogous case in memory  sad proposal. 
　remind's use of one spreading-activation mechanism for both comprehension and memory retrieval shows how recall can subtly affect interpretation. when stored episodes are similar to a cue that remind is interpreting  they feed activation back into the inferencing network. this feedback can bias remind's interpretation to be consistent with the active episodes  a limited form of case-based reasoning. 
1 comparison to general reminding models 
remind is most directly comparable to arcs  thagard et al  1  and mac/fac  gentner & forbus  1   two other models of general  non-expert reminding. all three take into account psychological evidence showing that memory retrieval is strongly influenced by both surface and thematic similarities between a cue and episodes in memory. in contrast to remind  arcs and mac/fac do not model the language understanding process  concentrating solely on retrieval of hand-coded targets. this allows them to retrieve more complicated episodes than remind can currently understand. on the other hand  remind's inferencing-based theory of retrieval has two significant advantages over retrieval-only models:  i  it shows how understanding and reminding can be modelled with a single spreading-activation mechanism  and  1  that this integration eliminates the need for the separate structural comparison mechanisms used to by arcs and mac/fac to allow analogical retrieval. 
　a major criticism of arcs and mac/fac is that neither model specifies how the representation of its input cues and episodes is formed or what kinds of knowledge those representations should generally include. should the cue representations include only the surface propositions directly stated in a cue's text  or should they include a fully elaborated interpretation of the cue  including a complete causal plan/goal analysis of the text and any abstract themes it involves  as in remind  memory retrieval often cannot be performed without such inferences  as illustrated by examples in this paper and in cbr models. however  even if retrieval-only models such as arcs and mac/fac were given fully elaborated cues  we believe that not modeling the process by which these representations are formed misses important features of reminding. understanding of text varies depending on whether it is simply being skimmed or is being read carefully for its deeper ramifications. thus  when the comprehension process is not modeled  there is no way to simulate the specific circumstances under which understanders infer and can use planning or thematic information in probing memory  cf.  seifert et a/.  1  . in contrast  remind explicitly models the cue interpretation process  and so can potentially explain when elaborated inferences are available to affect reminding. 
　remind is fundamentally different from arcs and mac/fac in how it models the influence of analogical similarity on retrieval. both arcs and mac/fac model analogical reminding by explicitly factoring in the degree of structural isomorphism  or relational consistency  between the cue and targets into their best match computation. isomorphism can best be explained by an example from  thagard et al.y 1  for the cue the dog bit the boy and the boy ran away from the dog  boy run . compare this to the analogs 
fido bit john and john ran away from fido  john run  and rover bit fred and rover ran away from fred  rover run . 
john run is structurally isomorphic with boy run  because mapped objects play the same roles in mapped predicates. in both cases  the dog did the biting and the person it bit did the running. in rover run  however  it was the dog that ran from the person it bit. john run is more isomorphic to boy run than is rover run  and is therefore a better analog. 
analogical similarity is hypothesized by arcs and mac/ 
fac to exert its effect on memory retrieval as a direct result of a specifically computed degree of syntactic isomorphism 

between cues and memory episodes. remind  in contrast  never explicitly computes the degree of isomorphism. instead  relationally consistent targets are retrieved over relationally inconsistent targets only indirectly  when the different syntactic structure of each input leads to different inferences. for example  if presented with john run  remind would infer that the boy ran away because he was afraid that dog would continue its attack. however  if presented with rover run  remind would infer that the dog ran away because it feared retaliation from the boy. because of these different interpretations of the two episodes  remind would also retrieve john run when presented with boy run as a cue. unlike arcs and mac/fac  however  remind does so without having to go through a separate stage to explicitly compute the degree of syntactic isomorphism. 
　we believe that the effects of syntactic isomorphism and relational consistency on memory retrieval can be fully explained by the understanding process. relationally consistent episodes tend to have more similar inferences  interpretations  and themes than relationally inconsistent episodes. in remind  explicit measures of syntactic isomorphism are unnecessary  since analogical reminding occurs as a natural side-effect of interpreting and disambiguating an input text. 
1 cbr models and indexing 
most case-based reasoning models are meant to be models of expert reminding and problem-solving in given domains. unlike remind  they are not meant to be models of general  non-expert human reminding. an advantage of case-based reasoning models over remind is that their use of symbolic processing abilities allows them to handle longer and more complex episodes than remind  and connectionist models in general  can currently handle. on the other hand  as a model of comprehension and general reminding  remind is better able to explain psychological results such as the relatively high prevalence of remindings based on superficial similarities and on how the reminding and language understanding processes interact and effect each other. 
　there are many similarities between retrieval of episodes in cbr models and in remind. one of the major goals of cbr researchers is to find the indices that will enable retrieval of the cases most likely to help their current task  the indexing problem . pragmatically useful indices usually include features such as abstract plans  goals  themes  explanations  and anomalous situations  depending upon the problem being solved. although remind does not currently recognize all these types of indices  all are features that an ideal model would have to recognize and that would therefore be used for comprehension and retrieval in future versions of remind. 
　one of the advantages of remind's approach to storing episodes is that it avoids cbr models' indexing problem. because episodes are simply stored  indexed  under all of the features that played a part in understanding them  there is no need for a separate computation stage to determine precisely which features they should be indexed under for best retrieval. the network's massively-parallel comprehension process eliminates the need to limit the number of indices used in order to constrain search time. further  this approach has the advantage of allowing episodes to be retrieved in contexts and situations other than those the problem-solver  or index evaluator  originally considered useful. 
　many of the desirable features of cbr indexing methods emerge from the dynamics of the spreading-activation process and how episodes are learned over time. for example  one important feature of a useful index is how unique it is. although remind indexes its episodes under all of their features  relatively unique features affect retrieval more than common ones simply because they activate fewer episodes  compare possess-obj to the more abstract avoid-detection and punishment frames in figure 1 . another important aspect of the spreading-activation process is that particularly salient features receive the most activation and therefore automatically act as stronger retrieval indices. while it does not currently approach the problem-solving of many cbr models  an extension of remind that did so would also focus activation on the types of problems and failures being examined  therefore naturally emphasizing useful indices. such a model might show how the benefits of explicit indexing in cbr models can fall out of the comprehension process. 
1 conclusions 
remind's use of a single spreading-activation mechanism to perform both comprehension and retrieval ensures that the features inferred from a cue during understanding will access episodes in memory that share similar inferences. this integration of comprehension and retrieval is a more psychologically-plausible way of producing analogical reminding than previous models. it also has several computational advantages. because the inferencing process activates abstract plans and themes  the explicit structural isomorphism computations needed to allow retrieval of analogies in retrieval-only models such arcs and mac/fac are unnecessary. remind's massively-parallel approach to comprehension and encoding also potentially explains how the explicit indexing of cbr models can be eliminated  while retaining its benefits as an emergent property of the comprehension process 
