 
given a set of numbers  and a set of bins of fixed capacity  the np-complete problem of bin packing is to find the minimum number of bins needed to contain the numbers  such that the sum of the numbers assigned to each bin does not exceed the bin capacity. we present two improvements to our previous bin-completion algorithm. the first speeds up the constant factor per node generation  and the second prunes redundant parts of the search tree. the resulting algorithm appears to be asymptotically faster than our original algorithm. on problems with 1 elements  it runs over 1 times faster. furthermore  the ratios of node generations and running times both increase with increasing problem size. 
1 introduction and overview 
given a set of numbers  and a fixed bin capacity  the bin-packing problem is to assign each number to a bin so that the sum of the numbers assigned to each bin does not exceed the bin capacity. an optimal solution uses the fewest number of bins. for example  given the set of numbers 1  1  1  1  1  1  and a bin capacity of 1  we can assign 1  1  and 1 to one bin  and 1  1  and 1 to another  for a total of two bins. this is an optimal solution to this instance  since the sum of all the numbers  1  is greater than 1  and hence at least two bins are required. an example application is given a set of orders for wire of varying lengths  and a standard length in which it is manufactured  how to cut up the minimum number of standard lengths to fill the orders. 
　bin packing was one of the earliest problems shown to be np-complete garey & johnson  1 . the vast majority of the literature on this problem concerns polynomial-time approximation algorithms  such as firstfit decreasing  ffd  and best-fit decreasing  bfd   and the quality of the solutions they compute. first-fit decreasing sorts the numbers in decreasing order  orders the bins  and assigns each number in turn to the first bin in which it fits. best-fit decreasing sorts the numbers in decreasing order and then assigns each number in turn to the fullest bin in which it fits. first-fit decreasing requires three bins to pack the set of numbers above  while best-fit decreasing packs them into two bins. both algorithms run in o nlogn  time. 
　in this paper we are concerned with finding optimal solutions  for several reasons. in applications with small numbers of bins  even one extra bin is relatively expensive. in addition  being able to find optimal solutions to problem instances allows us to more accurately gauge the quality of approximation algorithms. furthermore  an anytime algorithm for finding optimal solutions  such as that presented in this paper  can make use of any additional time available to find better solutions than those returned by polynomial algorithms. finally  optimal bin packing is computationally challenging  and may lead to insights applicable to other problems. 
　first we review previous algorithms for optimal bin packing  including our bin-completion algorithm  korf  1 . we then describe two improvements to bin completion. the first is an algorithm for generating undominated bin completions more efficiently  reducing the constant time per node generation. next we describe a method to reduce the branching factor of the search  by eliminating branch points that are dominated by previous branches. in experiments on millions of problem instances with uniformly distributed random numbers  our new algorithm appears to be asymptotically faster than our original bin-completion algorithm. this is based on the observation that both the ratio of nodes generated by the two algorithms  and also their running times  grow with increasing problem size. on problems of size 1  our new algorithm runs over 1 times faster than our original bin-completion algorithm. we also report mixed results on a common set of benchmark problems. 
1 previous work 
1 martello and toth 
a well-known algorithm for optimal bin packing  martello & toth  1a; 1b  is based on depth-first branch-and-bound. the numbers are first sorted  and are considered from largest to smallest. it first computes an approximate solution as an initial upper bound  using the best solution among first-fit  best-fit  and worst-fit 
　
decreasing. then  for each number  the algorithm places it in each partially-filled bin that it fits into  or in an empty bin. thus  the algorithm branches on the different bins that a number can be placed in. it also uses a lower-bound function to prune the search. 
1 	m o r e recent or approaches 
an anonymous reviewer pointed out some more recent references on optimal bin packing from the operations research literature  based on integer programming formulations  with linear-programming relaxations.  valerio de carvajho  1; scholl  klein  & jurgens  1; vanderbeck  1; degraeve & schrage  1; vance et a/.  1; vance  1  while we have not yet fully digested this work  we report below some comparison results on a standard set of benchmarks. 
1 	b i n c o m p l e t i o n 
in  korf  1   we described our bin completion algorithm  which uses a branching structure different from that of martello and toth. a feasible set is a set of numbers whose sum does not exceed the bin capacity. initially  we sort the numbers in decreasing order of size. 
we then generate feasible sets that include the largest number. if there is more than one such set  the search may branch at that point. each node of the search tree  except the root node  represents a complete assignment of numbers to a particular bin. the children of the root represent different ways of completing the bin containing the largest number. the nodes at the next level represent different feasible sets that include the largest remaining number  etc. the depth of any branch of the tree is the number of bins in the corresponding solution. bin completion is also a branch-and-bound algorithm. it starts with the best-fit decreasing solution as an upper bound  and applies a lower-bound heuristic function to prune the search. rather than assigning numbers one at a time to bins  it branches on the different feasible sets that can be used to complete each bin. bin completion appears asymptotically faster than the martello and toth algorithm  and outperforms it by a factor of a thousand on problems of size 1. the key property that makes it more efficient is a dominance condition on the feasible completions of a bin that allows us to only consider a small subset of them. 
1 	set d o m i n a n c e 
some sets of elements assigned to a bin cannot lead to solutions that are any better than those achievable by assigning other sets of elements to the same bin. we begin with some simple examples of these dominance relations  and then consider the general formulation. 
　first  consider two elements x and y whose sum is exactly the bin capacity c. assume that in one solution  x and y are in different bins. in that case  we can swap y with all other elements in the bin containing x  without increasing the number of bins. this gives us an equally good solution with x and y in the same bin. thus  given a problem with two values x and y such that  = c  we can always put x and y in the same bin  resulting in a smaller problem  gent  1 . unfortunately  this does not extend to three or more elements that sum to exactly the bin capacity. 
　as another example  consider an element x such that any two remaining elements added to x will exceed c. in other words  at most one additional element can be added to the bin containing x. let y be the largest remaining element such that  then  we can place y in the same bin as x without sacrificing solution quality. the reason is that if we placed any other single element z with x  then we could swap y with z  since 

　as a final example  again assume that y is the largest remaining element that can be added to x such that x + y c  and that y equals or exceeds the sum of any set of remaining elements that can be added to x without exceeding c. in that case  we can again put x and y in the same bin  without sacrificing solution quality. the reason is that any other set of elements that were placed in the same bin as x could be swapped with y without increasing the number of bins. 
　to illustrate the general form of this dominance relation  let a and b be two feasible sets. if the elements in b can be partitioned into subsets  and the subsets can be matched to the elements of a such that the sum of the elements in each subset doesn't exceed the corresponding element of a  then set a dominates set b. in other words  if the elements of b can be packed into bins whose capacities are the elements of a  then set a 

　given all the feasible sets that contain a common element x  only the undominated sets need be considered for assignment to the bin containing x. the reason is that if we complete the bin containing x with a dominated set  then we could swap each subset of numbers in the dominated set with the corresponding element of the dominating set  and get another solution without increasing the total number of bins. 
　martello and toth use this dominance relation to some extent. in particular  they take each element x  starting with the largest element  and check if there is a single completion of one or two more elements that dominates all feasible sets containing x. if so  they place x with those elements in the same bin  and apply the reduction to the remaining subproblem. they also use dominance relations to prune some element placements as well. 
　our bin-completion algorithm  however  makes much greater use of this dominance condition. in particular  when branching on the completion of any bin  it only considers undominated completions. 
1 finding undominated completions 
the first contribution of this paper is a faster algorithm to generate all the undominated completions of a bin. 
　
1 previous algorithm 
the simplest algorithm is to generate all feasible sets that include a particular element  and then test each pair of sets for dominance. our original implementation improved on this by only generating a subset of all feasible completions  and testing for pairwise dominance only among those. in particular  if only one additional number can be added to a bin  only the largest such number is added. if only two additional numbers can be added to a bin  the set of two-element undominated completions can be found in linear time  as follows. 
　each undominated two-element completion must have a sum greater than the largest feasible single number  and less than or equal to the remaining capacity of the bin. the remaining values are kept in sorted order  with two pointers  one initially assigned to the largest value and the other to the smallest value. if the sum of these two numbers exceeds the remaining capacity  the pointer to the larger number is moved to the next smaller number. if the sum of the two numbers is less than or equal to the largest single feasible number  the pointer to the smaller number is moved to the next larger number. if the sum of the two numbers is within this range  the pointer to the smaller number is increased to the largest number for which the sum of the two is still in range  and these two values form an undominated two-element completion. then the pointer to the larger number is moved to the next smaller number  and the pointer to the smaller number is moved to the next larger number. this process continues until the two pointers meet. 
　if the sum of the numbers in two feasible sets are unequal  only the one with the larger sum can dominate the other. if two sets have the same sum  only the one with the smaller cardinality can dominate the other. once a subset of the feasible completions is found  each pair was tested to see if either dominates the other. this was done by trying to pack the numbers of the potential dominated set into bins whose capacities were the numbers of the potential dominating set. these small bin-packing problems were solved by brute-force search. 
　there are two drawbacks to this approach. the first is the time to generate and then test the dominated feasible sets. the second is the memory needed to store the dominated feasible sets before they are pruned. 
1 generating undominated sets faster 
ideally  we would like to generate all and only undominated bin completions  without pairwise testing of feasible sets for dominance. we describe such an algorithm in three stages: 1  how to generate all subsets of a universe  1  how to generate only feasible subsets  and 1  how to generate only undominated feasible subsets. 
　the easiest way to generate all 1n subsets of n elements is to recursively traverse a binary tree  where each node represents a collection of subsets. the root node represents the entire power set  and the leaf nodes represent individual subsets. each interior level of the tree corresponds to a different element. at each node  the left branch includes the corresponding element in all subsets below it  and the right branch excludes the same element from all subsets below it. the tree is traversed depth-first using only linear memory. 
　to generate feasible completions of a bin containing a number x  we add an upper bound on the sum to each recursive call  which is initially the residual capacity  or the bin capacity minus x. we sort the remaining unassigned numbers in decreasing order of size. we only include numbers that are less than or equal to the upper bound. when we include a number by taking the left branch from a node  we subtract it from the upper bound of all recursive calls below that branch. when the upper bound drops to zero  we prune the tree below that node  since no further numbers can be added. this generates all feasible completions of a bin containing x. 
　generating undominated feasible completions requires a little more work. when we exclude a number that exceeds the upper bound  by taking the right branch from the corresponding node  nothing additional is required  since it can't be a member of any feasible set below that node. if we exclude a number that equals the upper bound  we can terminate that branch of the binary tree immediately  because any feasible subset of included elements below that node cannot sum to more than the excluded element  and any subset with a smaller sum would be dominated by the excluded element. 
　what happens when we exclude a number that is less than the upper bound  to prevent the excluded clement from dominating any included subset below it  the sum of the numbers in any such subset must exceed the excluded element. this generates a lower bound on the sum of the elements in any included subset below this node  which is equal to the excluded element plus one  assuming that the numbers are integers. as with the upper bound  the lower bound is reduced by any subsequently included elements in the recursive calls below the corresponding nodes. the lower bound generated by an excluded element is only used if it exceeds the current lower bound on that node. 
　thus  we perform a depth-first traversal of the binary tree representing all possible subsets of numbers remaining to be assigned. this traversal is pruned by the upper and lower bounds propagated down the tree as described above  and generates complete subsets at the leaf nodes. these will include all undominated feasible sets  but may include some dominated feasible sets as well. 
　to eliminate these  we perform an additional test on the feasible bin completions generated. the residual capacity r of a bin is the bin capacity c minus the largest number in the bin  which is common to all feasible completions of a given bin. let t be the sum of all the numbers in a feasible set a  excluding the common largest number. the excluded numbers are all remaining numbers less than or equal to r that are not included in a. set a will be dominated if and only if it contains any subset whose sum s is less than or equal to an excluded number x  such that replacing the subset with x will not exceed the bin capacity. this will be the case if and only if  thus  to guarantee that a feasible set a is undominated  we check each possible subset sum s  and each excluded number x  to verify that  
　this algorithm generates feasible sets and immediately tests them for dominance  so it never stores multiple dominated sets. it tests for dominance by comparing subset sums of included elements to excluded elements  rather than comparing pairs of sets for dominance. 
　pseudo-code for this algorithm is given below. the feasible function takes a set i of included elements  a set e of excluded elements  a set r of remaining elements  a lower bound / and an upper bound u. it generates all feasible sets of remaining elements whose sum is within the two bounds  and calls the test function on each. in the initial call  i and e are empty  r contains all remaining elements less than or equal to the residual capacity r of the bin  u is set to r  and i is set to the largest single element that can feasibly be added to the bin  plus one. test is the test described above  and is a function of the included elements j  the excluded elements e  and the residual capacity r. 

　to improve this algorithm  we use the same optimizations used in our original algorithm to generate feasible sets. namely  if only one more number can be added to a bin  we only add the largest such number  and if only two more numbers can be added  we generate all undominated two-element completions in linear time. this algorithm speeds up the generation of all undominated sets  without affecting the number of bin completions considered. for that  we turn to our next contribution. 
1 pruning the search space 
consider a number w in a bin  with a capacity of c. assume that two undominated feasible completions of the 

our search explores bin completions in decreasing order of subset sum  so in this case we consider before 
       furthermore  assume that after exhausting the subproblem below the assignment and while 
exploring the subproblem below the assignment we find a solution that assigns x and y to the same bin  
 since  and we could swap z with x and y  resulting in a solution with the same number of bins  but including the bin assignments  however  all possible solutions below the node representing the bin assignment  have already been explored. thus  this solution is redundant  and doesn't need to be considered again. in particular  below the  branch of the search tree  any solution that assigns x and y to the same bin will be redundant and can be pruned. 
　in general  given a node with more than one child  when searching the subtree of any child but the first  we don't need to consider bin assignments that assign to the same bin all the numbers used to complete the current bin in a previously-explored child node. more precisely  let be a set of brother nodes in the search tree  and let be the sets 
of numbers used to complete the bin in each node  excluding the first number assigned to the bin  which is common to all the brother nodes. when searching the subtree below node nt for i  1  we exclude any bin assignments that put all the numbers in sj in the same bin  for j   i. thus  no bin completion below node ni can have as a subset the numbers in  by rejecting these bin assignments as redundant  the number 
of node generations is reduced. 
1 current implementation 
our current implementation of this pruning rule propagates a list of nogood sets along the tree. after generating the undominated completions for a given bin  we check each one to see if it contains any current nogood sets as a subset. if it does  we ignore that bin completion. 
　to keep the list of nogood sets from getting too long  occupying memory to store them and time to test them against bin completions  we prune the list as follows. whenever there is a non-empty intersection between a bin completion and a nogood set  but the nogood set is not a subset of the bin completion  we remove that nogood set from the list that is passed down to the children of that bin completion. the reason is that by including at least one but not all the numbers in the nogood set in a bin completion  we've split up the nogood set  guaranteeing that it can't be a subset of any bin completion below that node in the search tree. 
　this implementation could probably be improved with more sophisticated data structures for representing arid manipulating sets of elements. 
1 experimental results 
we tested our algorithm on large sets of problems with uniformly-distributed high-precision numbers  and on a set of benchmark problems of relatively low precision. 
1 uniform high precision numbers 
we compared our original algorithm to our new bincompletion algorithm on the same problem instances and on the same machine. since high-precision numbers are often more difficult to pack than low-precision numbers  we used a bin capacity of one million  and random numbers uniformly distributed from one to one million. given the enormous variation in the difficulty of individual problem instances  we ran one million instances of each problem size  which ranged from 1 to 1 numbers  in increments of 1. table 1 shows the results. 
　
 n optimal original without pruning with nogood pruning ratios bins time nodes time nodes time nodes time 1  1 
1 1 
1 .1
.1 1 	1 
1 .1 
.1 1 
1 1 
1 1 1
1 1 1 1 .1 1 .1 1 1 1 1 1 1 .1 1 .1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 | 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1.1 1 1 1 1 1 1 1 1 1.1 1 1 1 1 1 1 1 1 1.1 1 1.1 1 1 1 1 1 1 1.1 1 1.1 1 1 1 1 1 1 1.1 1 1.1 1 1 1 1 1 1 1 1.1 1 1 1 1 1.1 1 1 table 1: experimental results for uniformly distributed  high-precision numbers 
　
　the first column gives the problem size  which is the number of values being packed. the second column shows the average number of bins needed in the optimal solution. since the numbers range uniformly from zero to the bin capacity  the expected value of any number is half the bin capacity  and the expected value of the sum of the numbers is the half the bin capacity times the number of values. as expected  the average minimum number of bins is slightly more than half the number of values  due to the inevitable wasted space in the bins. 
　the third column gives the average running time of our original bin-completion algorithm  korf  1   in microseconds. this is also the total time in seconds to solve all one million problem instances. all implementations are on a 1 megahertz sun ultra 1 workstation. 
　the next two columns  labelled  without pruning   give the average node generations and running times in microseconds for our implementation of bin completion with our new method of generating undominated feasible sets  but without pruning nogood sets. while this program considers the same number of candidate solutions as our original one  the node generations differ from those reported in our earlier paper  korf  1 . the reason is that we define a node as a recursive call to the search routine  and our current implementation checks terminating conditions before making a recursive call  rather than at the beginning of the search function. our new program outperforms our original one by a factor of up to 1 in running time. we didn't run it on problems of size 1 or 1 due to the time that would be required. 
　the next two columns  labelled  with nogood pruning    give the average number of nodes generated and average running time in microseconds for our full algorithm  including nogood pruning. comparing the number of node generations to the corresponding column without pruning shows the effect of nogood pruning. 
　the last two columns give performance ratios of our best program  including nogood pruning. the node ratio is the number of nodes generated without nogood pruning  divided by those generated with nogood pruning. the time ratio is the running time of our original program  divided by our current best program. 
　as problem size increases  nogood pruning generates increasingly fewer nodes than without pruning. on the largest problems we ran both algorithms on  the ratio of node generations is about a factor of 1. the fact that the node generation ratio increases with increasing problem size suggests that nogood pruning reduces the asymptotic time complexity of bin completion. 
　the ratios of the running times displays a similar trend  although the values are less than the ratios of node generations. this is due to the increased overhead of nogood pruning. on the larger problems  our new algorithm is over an order of magnitude faster than our original algorithm. problems of size 1 take an average of only 1 seconds per problem to solve optimally. 
variation in individual problem difficulty 
there is tremendous variation in the difficulty of individual problems. for example  in 1% percent of the one million problems with 1 numbers  the best-fit decreasing solution uses the same number of bins as the lower bound  solving the problem without any search. among the same problems  however  twenty instances generated more than a billion nodes  three of those generated more than ten billion nodes  and one of those generated more than a hundred billion nodes. our program solved over 1 problems of size 1 in about a day  and then failed to solve the next problem in over 1 days. what 
　
distinguishes the hard problems from the easy ones  
　among all problems of size 1  the average number of bins in the optimal solution is 1. among the twenty hardest problems  however  the average optimal number of bins is only 1. intuitively  this makes sense  since fewer bins means more items per bin  and hence more undominated feasible sets to consider. 
　on the other hand  problems with a relatively small number of bins in the optimal solution are not necessarily difficult. the reason is that with smaller numbers  approximation algorithms like best-fit decreasing are more accurate. for example  eleven problems of size 1 required 1 or fewer bins. seven of those required no search  and the average number of nodes generated to solve all eleven was only 1  compared to an average of 1 nodes for all one million problem instances. 
　thus  the hard problems tend to use fewer bins  but problems that use fewer bins are not necessarily hard  since often the lower bounds agree with the solutions returned by approximation algorithms. 
1 	b e n c h m a r k p r o b l e m s 
we also ran our best algorithm on eight sets of twenty benchmark problems each  from the operations research library maintained by j.e. beasley at imperial college  london. these problem instances were originally generated by falkenauer  falkenauer  1   and have been used by a number of other researchers  valerio de carvalho  1; vanderbeck  1 . we compare our results to  valerio de carvalho  1   since he reports the most detailed results. he ran his experiments on a 1 mhz pentium  compared to our 1 mhz sun workstation. 
uniform problems 
the first four problem sets  called uniform problems  consist of numbers chosen uniformly from 1 to 1  with a bin capacity of 1. each set contains 1 problems  which are of size 1  1  1  and 1 numbers each. 
　on the uniform problems of size 1  our best algorithm took 1 seconds on problem 1  1 seconds on problem 1  and solved the rest instantly  meaning in less than a second. in eleven of these problems  either the best-fit decreasing solution  or the first solution found by bin completion  matched our lower bound  requiring no search.  valerio de carvalho  1  reports an average of 1 seconds for these problem instances. 
　for the uniform problems of size 1  our algorithm solved all but three instantaneously  solved problem 1 in 1 seconds  but failed to solve problems 1 and 1 in over ten minutes each. in twelve of these problems  the first solution found by bin completion matched our lower bound.  valerio de carvalho  1  reports an average of 1 seconds for these problems. 
　on uniform problems of size 1  our algorithm solved 1 problems instantly  took 1 and 1 seconds on problems 1 and 1  respectively  but failed to solve problems 1  1  1  and 1 in ten minutes each. in eleven of these problems  the first solution found by bin completion matched our lower bound.  valerio de carvalho  1  reports an average of 1 seconds on these instances. 
　for uniform problems of size 1  our algorithm solved problems 1  1  1  1  1  1  1  1 and 1 instantly  and solved problems 1 and 1 in 1 and 1 seconds  respectively. in seven of these problems  the first solution found by bin completion matched our lower bound. it failed  however  to solve the remaining 1 problems in ten cpu minutes each.  valerio de carvalho  1  reports an average of 1 seconds for these problems. 
triplet problems 
the other four problem sets are called triplets  since each bin contains exactly three elements in the optimal solution. the bin capacity is 1  with numbers in the range 1 to 1. the first number in each bin was chosen uniformly from 1 to 1  the second was chosen from 1 to one-half the size of the first number  and the third element was chosen so that the sum of the three is exactly 1. thus  no space is wasted in the optimal solution. 
　our algorithm solved all 1 triplets of size 1 instantly  while  valerio de carvalho  1  reports an average of 1 seconds on these problems. 
　on triplets of size 1  our algorithm solved all but four problems instantly  and required 1  1  1  and 1 seconds on problems 1  1  1  and 1  respectively  for an average time of 1 seconds.  valerio de carvalho  1  reports an average of 1 seconds on these instances. 
on triplets of size 1  our algorithm solved instances 
1  1  1  1  and 1 instantly  required 1  1  1  1  1  
1 and 1 seconds on problems 1  1  1  1  1  1  and 1  respectively  but failed to solve the remaining 1 problems after 1 minutes each.  valerio de carvalho  1  reports an average of 1 seconds on these problems. 
　we were unable to solve any of the triplets of size 1  with ten cpu minutes each.  valerio de carvalho  1  reports an average of 1 seconds on these instances. 
discussion of benchmark results 
on the uniform problems of size 1  and on the triplets of size 1 and 1  our algorithm outperformed that of  valerio de carvalho  1  in average running time  taking into account the different clock speeds of our machines. on the remaining sets of problems  our algorithm would require a longer average running time  since it failed to solve some instances in ten minutes. on uniform problems of size 1 and 1  however  our algorithm took less time than that of  valerio de carvalho  
1  on most problem instances  1 out of 1 for size 1  and 1 out of 1 for size 1. on uniform problems of size 1  our algorithm took less time on 1 out of 1 problems  and on triplets of size 1 our algorithm was faster on 1 out of 1 problems. overall  our algorithm performed worse than that of  valerio de carvalho  1  on the largest problem sets  however. 
　one possible reason for this difference in performance is that we designed our algorithm with high-precision numbers in mind  ranging from one to a million in our experiments. with these values  there are no duplicate numbers in the same problem instance  and no pairs of 
　
numbers that sum to exactly the bin capacity. thus  we didn't consider optimizations that are only possible with duplicate numbers. in the uniform benchmark datasets  however  the values range from 1 to 1  and in the triplets datasets they range from 1 to 1. as a result  these problem sets contain many identical numbers. 
　while high-precision values are usually more difficult to pack  this is not the case with the triplet datasets considered here. in particular  if they were generated in the same way  but using high-precision values instead  the first solution found by bin completion would be optimal  since there would be only one way to fill each bin completely  and that completion would be considered first. furthermore  it would be immediately recognized as optimal  since there is no extra space in any of the bins. 
1 conclusions 
we presented two improvements to our original bincompletion algorithm. the first is an algorithm for generating all undominated bin completions directly  without testing pairs of completions for dominance. more importantly  we presented an algorithm to identify and eliminate redundant bin completions  which prunes the search space. combining these two improvements yields an algorithm that appears to be asymptotically faster than our original  and runs over 1 times faster on problems of size 1. for numbers uniformly distributed from zero to the bin capacity  we can solve a million problems of size 1 optimally in an average of 1 seconds per problem instance. 
　there is enormous variation in individual problem difficulty  with most problems being solved instantly  but some running for days or weeks. one problem of size 1 ran for 1 days without verifying an optimal solution. 
　on a set of standard benchmark problems  the results were mixed. our algorithm outperformed that of  valerio de carvalho  1  on the smaller problem instances  but did worse on the largest problem instances. these benchmarks problems contain relatively low-precision values  which may provide further opportunities for improving the performance of bin completion. 
　it is important to note that most of the algorithms described in this paper are anytime algorithms. in other words  they produce an approximate solution immediately  and as they continue to run they produce better solutions  until they find and eventually verify the optimal solution. furthermore  the gap between the approximate solution and the lower bound is often only a single bin  and always known throughout the computation  allowing the user to decide how much effort to expend in trying to achieve this improvement. 
1 acknowledgments 
thanks to the anonymous reviewers  particularly for pointing out the recent work on this problem in the operations research community. this work was supported by nsf under grant no. eia-1  by nasa and jpl under contract no. 1  and by the state of california micro grant no. 1. 
