 
　a simulation model is described for the acquisition of the control of syntax in language generation. this model makes use of general learning principles and general principles of cognition. language generation is modelled as a problem solving process involving prmciply the decomposition of a lobe-communicated semantic structure into a hierarchy of subunits for generation. the syntax of the language controls this decomposition. it is shown how a sentence and semantic structure can be compared to infer the decomposition that led to the sentence. the learning processes involve generalizing rules to classes of words  learning by discrimination the various contextual constraints on a rule application  and a strength process which monitors a rule's history of success and failure. this system is shown to apply to the learning of noun declensions in latin  relative clause constructions in french  and verb auxiliary structures in english. 
introduction 
　this research has its background in past work on language acquisition  for reviews  see anderson. 1; pinker. 1-see also langley. 1   especially in my previous work on las  language acquisition system-see anderson. 1 . for various reasons that will be explained  there were problems with las and a more general concept of human cognition was developed called act  anderson  1 . the system to be reported here is an attempt to merge the ideas in the act project and the las project. it is called alas for act's language acquisition system. first in this paper i will review those aspects of the las and act systems that are relevant to understanding the current project and then i will turn to describing the alas system. 
the las system 
　las accepted as input strings of words  which it treated as sentences  and scene descriptions encoded as associative networks. when learning  the program attempted to construct and modify augmented transition networks which described the mapping between sentence and scene descriptions. this assumption  that the program has access to sentence-meaning pairings  is the basic assumption underlying most of the recent attempts at language acquisition. this assumption might be satisfied in the circumstance where the child is hearing a sentence describing a situation he is attending to. even here it is likely that the child will represent aspects of the situation not described and fail to represent aspects described. in las we worked out mechanisms for filtering out the non-described aspects of the meaning representation by comparison with the sentence. in the current alas system there is a discrimination mechanism for bringing in aspects of the situation not initially 
　 this project supported by contract n1-1-c-1 from the office of ni research i would like up thank steven pinker. lynne reder and mirim schunuek their commons on earlier drafts of this i thought by the learner to be part of the sentence. so. we have worked out mechanisms for achieving sentence-meaning pairings in simple ostensive learning situations. however  much of what a child must learn about language will lack simple ostensive referents. for instance  most of the verb auxiliary system refers to non-ostensive meaning. how a child  or any system  would come up with sentence-meaning pairings in these situations is not clear and remains an issue for future research. 
　a major assumption of the las model that is maintained in the current system is that the system already knows the meaning of a base set of words. las was unable to learn the meaning of any words in context while the current system can; however the basic learning algorithm in both still requires that a substantial number of words in the sentence have their meanings previously learned. in principle  see anderson  1 . it would be possible to call to bear statistical learning programs to extract the meaning of the base set of words from a sufficiently large sample of meaning-sentence pairings. however  the evidence  mcwhinney  1  is that children accomplish their initial lexicalization by having individual words paired directly with their referents. 
　identifying phrase structure: 	the graph deformation condition 
　a major problem in language learning is to identify the phrase structure of the sentence. there are a number of reasons why inducing the syntax of language becomes easier once the phrase structure has been identified:  1  much of syntax is concerned with placing phrase units within other phrase units.  1  much of the creative capacity for generating natural-language sentences depends on recursion through phrase structure units.  1  syntactic contingencies that have to be inferred are often localized to phrase units  bounding the size of the induction problem by the size of the phrase unit.  1  natural language transformations are best characterized with respect to phrase units as the transformational school has argued.  1  finally  many of the syntactic contingencies are defined by phrase unit arrangements. so. for instance  the verb is inflected to reflect the number of the surface structure subject 
　a major mechanism for identifying phrase structure in las  and which is continued in alas  is use of the graphdeformation condition. the idea is to use the structure of a sentence's semantic referent to place constraints on surface structure. the application of the graph deformation condition is illustrated in figure 1. in part  a  we have a semantic network representation for a series of propositions and in part  b  we have a sentence that communicates this information. the network structure in  a  has been deformed in  b  so that it sits above the sentence but all the node-to-node linkages have been preserved. as can be seen  this captures part of the sentence's surface structure. at the top level we have the subject clause  node x in the graph   give  book  and the 
1 

recipient  node y  identified as a unit the two noun phrases are segmented into phrases according to the graph structure. for instance  the graph structure identifies that the words lives and house belong together in a phrase and that big  girl  lives  and house belong together in a higher phrase. 
   the graph deformation in part  b  identifies the location of the terms for which meanings are possessed in the surface structure of the sentence. however  terms like the before big girl remain ambiguous in their placement. it could either be part of the noun phrase or directly part of the main clause. thus  there remains some ambiguity about surface structure that will have to be resolved on another basis. in las the remaining morphemes were inserted by a set of ad hoc heuristics that worked in some cases and not in others. one of the goals in alas was to come up with a better set of principles for determining the boundaries of phrases. 
　the graph deformation condition is violated by certain sentences which have undergone structure-modifying transformations that create discontinuous elements. examples in english are: 
1. the news surprised fred that mary was pregnant. 
1. john and bill borrowed and returned  respectively  the lawnmower. 
transformations which create discontinuous elements are more common in languages that use word order less than english. however  the graph deformation condition remains as a correct characterization of the major tendency m all languages. the general phenomena has been frequently commented upon and has been called behaghers first law  see clark & clark  1 . a problem with las was that it had no means of dealing with exceptions to the graph deformation conditions or of learning transformations in general. another goal for the alas current enterprise is to be able to delect sentences that violate the graph deformation condition and to use these as opportunities for learning transformations. 
　a major source of my dissatisfaction with las is that its processing discipline and learning mechanisms are specific to language and it was hard to imagine how they would relate to other types of skill learning. while many people believe the principles underlying language acquisition are unique  1 do not i think the other problems with the las enterprise could be repaired but 1 felt a fresh start was needed if we were to show that general skill acquisition principles could plausibly apply to natural language as a special case. this led to the development of the act theory  anderson  1; anderson  kline  & lewis  1  and to a set of learning principles for that theory. 
act 
　as originally formulated. act was s production system without any commitment to the mechanisms of skill organization or skill acquisition. however  a set of principles have emerged in our more recent work  anderson. kline. & beasley. 1; anderson & kline. 1: anderson. greeno. kline. sl neves. 
1  and it is these developments which are essential for the current application. these ideas have been developed id nonlinguistic domains-schema abstraction  acquisition of proof skills in geometry  and most recently in the acquisition of programming skills. 
　we see any skill as being hierarchically organized into a search of a problem space in which there is a mam goal  which is decomposed into subgoals. and so on until the decomposition reaches achievable subgoals. much of what is distinctive about a particular skill is the way in which the problem space is searched for a solution. in our model of language generation  this is seen as a simple top-down generation of subgoals  corresponding to phrases  where there is no real search needed unless transformations have to be applied. we will illustrate this application to language shortly. 
　in simulating language acquisition we have focused on the learning mechanisms concerned with operator selection: generalization  discrimination  and strengthening. generalization takes rules developed from special cases and tries to formulate more general variants. discrimination is responsible for acquiring various contextual constraints to delimit the range of overly general rules. strength reflects the success of a rule in the past and controls its probability of future application. in combination  these mechanisms function like a statistical learning procedure to determine which problem features are predictive of a rule's success. they have been extensively documented in our efforts to model the literature on schema abstraction  anderson & kline  1: eho sl anderson  in revision   but they have had a richer application to acquisition of proof skills  anderson  submitted: anderson  greeno. kline  & neves. 1 . 1 will sketch their application to the language acquisition domain  but the reader should go to these other sources  and particularly anderson  kline  & beasley. 1  for a fuller development 
current framework for language learning 
　the language learner is characterized as having the goal of communicating a particular set of propositions. this set of propositions is organized into a mam proposition and subpropositions. so. for instance  the goal behind the generation of the girl kicks the boys might be a communication structure which we can represent as  kick  girl x   boy y   where x is tagged as singular and y is tagged as plural. to achieve the goal  the learner tries to decompose this higher level goal into subgoals. according to the units of the overall communication structure. so. he will decompose this into the subgoals of communicating kick  of communicating  g1rl x   and of communicating  boy y . he looks to his language for some means of organizing these subgoals. so. he might have learned a rule of the form: 
if the goal is to communicate  lvrelation lvobjectl 
　　　lvobject1  and lvrelation is in the verbx class 
then set as the subgoals to communicate lvobjectl say the morpheme for lvrelation say  s  and to communicate lvobject1 or we might more compactly denote this rule: 
	 1 	1 	1  --  1 + 1* + 1 + 1 	if 1 in verbx 
1 

in the above  the 1  1  and 1 match the three elements in the meaning structure-klck   girl x   and  boy y . the right side of the arrow specifies their order in the sentence and the insertion of mophemes like s. the star above the 1 indicates its lexical form is to be retrieved. the other elements will have to be further unpacked. 
　if it is early in the language learning history and the learner does not have a rule for realizing this construction  then he might try to invent some principle. he may only produce a fragment  e.g.. girl hit  or a non-allowed order  e.g.  girl boy hit . there is some evidence in first language acquisition that children will use word orders not frequent in adult speech  clark  1; de villiers & de villiers. 1; mcwhinney. 1 . for instance  there is a tendency to prefer agents first even when one's language does not. also  it is well known that second language learners fall back on their first language word orders when knowledge of word order fails. 
　the embedded subgoals are unpacked into actions or further subgoals in the same way that the top level structure is unpacked. for instance  if the object to be communicated were  girl x  like x  sailor z     the top level of this structure might be communicated by the rule: 
 1 	1 	1   the + 1 + 1 	if 	1 is a noun 
where   like x  sailor z   is item 1 in the above and would be communicated by the rule: 
 1 1   who + 1  + 1 if 1 is a verb and the construct i o n is embedded figure 1 illustrates the hierarchy of subgoals id the generation of s relatively complex sentence: the young policeman sees the lawyer whom the crook paid. it should be clear that if sentences are generated by setting subgoals to reflect the structure of the referent  then the graph deformation condition will tend to be satisfied in natural language. 

　the learning that occurs in alas is basically learning by doing. the learner generates an utterance and it is assumed that he has access to feedback about the correctness of the construction he generated and perhaps information about what the correct utterance should have been if he has made an error. there are many ways this can happen. the learner may generate a sentence and be corrected by a teacher. he may generate a sentence and remember a sentence or sentence fragment heard earlier. he may hear a sentence  infer its meaning  and compute how he would express the meaning. by whatever means the learner sometimes identifies some fragment of his generation to be in error and sometimes has a hypothesis as to the correct utterance. this is the stimulus for learning. in the actual simulations that will be reported  the program is given a model sentence along with each meaning and the program compares its generation with the model sentence. no doubt this is an unrealistically ideal assumption and results in a considerable speed up of the learning process in alas  however  the same learning mechanisms would apply in more psychologically realistic situations where the program was given only occasional information and often fragmentary information about what the correct target sentences were. 
formation of initial rules 
　the initial rules that the system acquires are  of course  quite specific. so  for instance  consider the rules it might form upon receiving a pairing of the latin sentence   equ i  agricol as port ant  and the meaning representation  carry  horse x  farmer y  . with a partially complete lexicalization  alas knows the meaning of equ is horse  the meaning of agricol it farmer  and the meaning of port is carry . alas then formulates the following rules: 
	 1 	1 	1    	1 + 1 + 1 + ant 	if 1 = carry 
	 1 	1  1- + i 	if x = horse 
	 1 	1  1 ' + as 	if 1 = farmer 
thus  its acquired rules are exact encodings of the relations at each level in the meaning hierarchy. the evidence is that children also start out with rules specific to individual words  mac whinney. 1; maratsos & chalkley  1  and indeed the nature of natural language makes this a wise policy in that rules are quite specific to various lexical items  bresnan  1; maratsos & chalkley  1; pinker  1 . this also is exactly how learning proceeds in other areas to which we have applied act. initially  the system acquires rules that encode the exact goal structure of specific examples. later  generalizations are formed. 
　while  on one hand  these rules are too specific  on the other hand  they are too general. the inflections associated with the nouns and verbs are only correct for the specific case and number combinations but these rules do not reflect that constraint. the system will have to acquire discriminating features that will properly constrain the range of application of these rules. again that corresponds to child language. children initially use words with a single inflection in all situations and only later acquire the contextual constraints. it also corresponds to our other learning endeavours where contextual constraints on goal decomposition are acquired through discrimination. 
discrimination 
　to illustrate the discrimination process consider again the rule for realizing farmer: 
	 1 	1  	-  	1* + as 	if 1 ＊ farmer 	 a  
suppose the system encounters a second instance of farmer in the meaning-sentence. pairing  call  farmer u   girl v     agricol a   puell am  voc at . it would detect a conflict between its generation of agricol+as and the target agricol*a. in this case it would look for differences between the context of its current application and the previous. the relevant differences are: 	       
1. y in the previous application is lagged as plural while u in the above structure is singular 
1. the object structure was in third position in the embedded clause of the first meaning structure  but now it is in second position. 
however  there are any number of other potential differences such as 
1. the previous verb was port and the current voc 
1. the second position of the embedded clause was plural and the current is singular. 
1. the current sentence involves a feminine object. las has an ordering of distance  to-be-explained  such that 1 and 1 above would be definitely less preferred but there is no clear basis for choosing 	1 	and 1 over 	1. 	a feature to discriminate upon is chosen at random and a new rule is formed 	such 	as: 	 b  
 1  --  1*+ a if 1 * farmer  b   1  --  and 1 is singular 
1 

note that tins if a discrimination for the current context  not the previous. alas can also form a rale for the old context 
 1 	1  	- 	  	1' 	+ 	as 	if 1 = farmer 	 c  
and 1 is plural 
but only if the old rule  a  exceeds a threshold of strength to indicate that it has applied successfully more often than not and is therefore not a pure mistake. 
　the correct rules above need another round of discrimination before they pick up the semantic position feature. then they will become 
 1 1  -   v   a if 1 = fanner and 1 is singular' and this 
occurs in second position in the semantic referent  d   1 1  -   1'   as if 1 = fanmer and 1 is plural and this occurs in third position in the semantic referent  e  　the set of possible features for discrimination is defined by a network that includes the semantic referent the goal structure  and any properties tagged to terms in the semantic referent or the goal structure. the program does a breadth first search out from the current position in this network looking for features that distinguish between current and past applications of the rule. it chooses the features it first finds in that search. this means that the system is sensitive to both syntactic and semantic contingencies of the context of application. 
generalization 
　let us consider what would happen if the currently implemented act generalization process were to apply to rule  e  from before and to the following rule that the system might derive in a similar manner: 
 1  	-  1* + as 	if 1 = girl 
　　　　　　　　　　　and 1 is plural and this occurs in the third position in the sonantic referent act would generalize these two rules by simply dropping the constraint that 1 be farmer or girl. this would yield: 
 1 	1  	- 	  	1* 	+ 	as 	if 1 is plural 
　　　　　　　　　　　　　and this occurs in third position of the semantic referent this would lead to an enormous overgeneralization in that the above rule u only valid for first-declension nouns. 
　thus  we have had to assume that generalization cannot occur in language by the wholesale replacement of a constant by a variable. rather what we assume is that generalization occurs by replacing a constant by s word class. so. the proper form of the above rule becomes 
 1  -   l* + as if 1 is in class x 
and 1 is plural and this occurs in third position in the semantic referent 
where class x will contain firmer and girl among others. it is unclear at present whether this is s true instance of where language acquisition differs from other cognitive learning or whether the generalization mechanism should be set up to produce constrained variables in all situations. 
　a major issue in alas concerns when words should be merged into the same class. it is not the case that this occurs whenever there is the potential to merge two rules as above. the existence of overlapping declensions and overlapping conjugations in many languages would result in disastrous overgeneralizations. rather we have brought to bear an extension of our schema abstraction ideas  anderson & kline. 1 . what alas does is look at the pattern of rules that individual words appear in. it will merge two words into a single class when 
j. the total strength of the rules for both words exceeds a threshold indicating a satisifactory amount of experience 
   1. a traction  currently 1  of the rules that have been formed for one word  as measured by strength  have been formed for the other word. when such a class is formed  the rules for the individual words can be generalized to that class. also  any new rules acquired for one word will generalize to the other. once a class is formed new words can be merged with the class according to the same criteria  1  and  1  for merging words. further  two classes can be merged together  agsin according to the same criteria. thus  it is possible to gradually build up large classes like first declension. 
　the word-specific rules are not lost when the class generalizations appear. furthermore  one form of discrimination is to propose that there is a rule special to a word. because of the specificity ordering in production selection  these wordspecific rules will be favored when applicable. this means that the system can live with a situation where a particular word  such as dive  can be in a general class but still maintain some exceptional behavior. 
　thus  the system begins with a lot of word-specific rules which gradually expand in their scope of application. this is basically the development observed in child language. 
　it should be noted that there is another dimension in which the system's behavior starts out very general. the rules for communicating a particular construction  such as an object construction  eg. noun phrase  or qualifying proposition  e.g.. a relative clause   are assumed to apply in every location. thus  the system automatically assumes rules are recursive and does not need  as did las  to verify such points of recursion. rather  the learning here takes the form of constraining this assumption where overgeneral- as we have discussed correspondingly  children seem not reluctant to venture old constructions in new syntactic contexts. 
phrase structure segmentation 
　up to this point we have assumed that the target sentences were segmented into phrase structure units. the graph deformation condition can be used to assign the words whose meaning u known to phrase units but this leaves unspecified the other morphemes. to take an example from my work with latin consider the following meaning-sentence pairing: 
 praise  friend u  have  man v  u    field x 
　　　　 have  farmer y  x    	1 amic us vir l ager os agricol ae laud at 1 
 translated: the man's friend praises the farmer's fields . 
clearly  the semantic structure indicates vtr  man  associates with amic  friend  as a modifier and not with ager  field  since man is contained in the same meaning unit as friend. however  the semantic structure provides us no way of deciding whether the non-meaning-'bearing morpheme us associates with vir or emit. similarly  it is ambiguous how to locate the other noun inflections: /. os  and a*. on the other hand  et occurring at the end of the sentence definitely must associate with laud. thus  by means of the graph deformation coodition and only taking unambiguous cases  we get the following hierarchical organization for the latin string: 
       amic   vir     ager   agricol    laud at  1 where the indeterminate morphemes are left out. at one point in its application of the graph deformation condition alas calculates just this structure. if nothing more can be done  this is the form of the string provided to the learning systemi.e.  with the ambiguous morphemes deleted. 

1 

　how can this string be improved upon to insert the nonmeaning bearing morphemes  in the literature there are three suggestions. first  there may be pauses in the speech signal to indicate the correct associations. there would be no ambiguity if there were long pauses after us  /  os  and as in the above message. normal speech does not always have such pauses in correct places and sometimes has pauses in wrong places. still  this basis for segmentation would be correct more often than not and alas's error correcting facilities have the potential to recover from the occasional missegmentaion. also  it is argued that parent speech to children is much better segmented than adult speech to adults  see de villiers & de villiers. 1 . in alas pausing is used when given  but the system does not require pause segmentation. 
　a second suggestion is to use past instances of successful segmentation to segment in the current case. thus  if the system has previously identified agricol+se as associating together it can assume they associate together now. the past experience could derive from bearing the word in isolation or from other sentences where some other basis could be applied for segmentation. memory for words spoken in isolation is a particularly useful solution to the problem of identifying which morphemes belong together to define a word. the evidence is quite clear that children do bear many words in isolation  mcwhinney  1 . this is less helpful in identifying phrase boundaries for structures like noun phrases or relative clauses- both because these structures are less likely to be spoken in isolation and because the same word sequence is rarely repeated. this may explain why missegmentalion of morphemes within words is rare in child speech relative to missegmentalion of words with phrases  slobm  1 . although we could in principle use this strategy  our simulation that attempted to segment without pause structure was not given words in isolation. 
　the third basis for segmentation relies on the use of statistics about morpheme-to-morphenie transitions. for instance  the segment ae will more frequently follow agricol with which it is associated than it will precede laud with which it is not. the differences in transitional frequencies would be very sharp in a language like latin with a very free word order but they also exist in english. thus  alas can associate ae with the agricol if it has followed agricol more frequently than it has preceded laud. this requires keeping statistics about word-toword transitions. currently  the system will favor one association of a morpheme over another if there is a difference in frequency of two. this might seem a rather small threshhold but i have gotten satisfactory performance out of alas  partly because alas can recover from occasional missegmentations. again the evidence is that children do occasionally missegment  mcwhinney. 1  and. of course  recover eventually. it strikes some as implausible to suppose that people could keep the statistical information required about word to word transitions. however  hayes and clark  1  have shown that subjects in listening to nonsense sound streams can use differential transition probabilities as a basis for segmentation. such information has also proven useful in computational models of speech recognition  lesser  hayes-roth  birnbaum. & cronk  1 . 
　it is possible and frequently has been the case that none of the alas segmentation mechanisms could apply to assign a morpheme to a level in the phrase structure. in such cases the non-assigned morpheme was simply omitted from the phrase structure. thus  the initial utterances produced by alas  like the utterances produced by young children  arc telegraphic in character. that is. they are missing many functors. 
latin: the issue of segmentation 
　our first endeavour was to learn a fragment of latin that involved first and second declension nouns  inflected for the nominative  accusative  and genitive cases and for plural and singular. an example of the input to alas is 
agricol ae puel am legat i laud ant  praise  farmer x   girl u  have  lieutenant v  u   where x is plural  u and v are singular 
that is  the input was a string of latin morphemes that comprised the target sentence and a hierarchical representation of the meaning of this sentence. the program was provided with a long sequence of such pairings. over the sequence all syntactic possibilities were realized. with each pairing  alas consulted its rules to see if they would map the meaning structure onto the target string. its learning principles were evoked to modify the rules if they failed to produce the right mapping. in this simulation  and the others  we provide the strings segmented into morphemes. acquisition of morpheme segmentation is thus being ignored. the verbs used were 1 first-conjugation verbs; the nouns were 1 first-declension nouns and 1 second-declension nouns. one of the things our simulation was going to get at was the adequacy of our class heuristics to separate our first and second declension nouns. we performed two simulations over this target language subset. in the first we provided the system with no information about segmentation and it was forced to use the graph-deformation condition and transitional probabilities to segment into surface structure units. in the second simulation we provided pause information to indicate with which words the inflections were associated. 
　to avoid any possible biasing in input order  the sentencemeaning pairs were generated by a randomization program. the simulation without the pause information required 1 pairings before it has identified all the needed grammatical rules and ran a criterion 1 pairings with no mispredictions of the target strings. with pause information  only 1 sentences were required to reach the same criterion. figure 1 illustrates the mean number of errors for the two conditions plotted as a function of the logarithm of number of pairings experiences. an error was defined as a misordering of elements at any phrase level  the insertion of an incorrect morpheme  or the ommission of a morpheme. 
figure 1 

log  number of pairings  
　in the case where the system was not given information about pause structure  it had to use transitional frequency to segment. after the first 1 sentences it was correctly associating about 
1% of the noun inflections with the nouns. most of the remaining 1% were failures to insert the morphemes but there were occasional missegmentations. despite the fact that it was correctly segmenting over half of the input to the learning program after the 1th trial  it was only after 1 trials that any learning of inflections showed up in its performance  i.e.. it started using these inflections with significantly greater than chance accuracy . even after 1 sentences alas is failing to 

1 

segment some nouns in 1% of the sentences. the difficulty in segmentation is what is accounting for the slow learning of the program. the examples that follow present first the latin morpheme string that the program generated to express a meaning structure  not shown  and  second  the target string that was correct. i have given a non-random selection of these to give the reader a sense of the progress of the system throughout the course of the 1 pairings: 

　the class formation heuristics worked quite well in these simulations. both with and without pause information  the two declensions were identified as two word classes and all the verbs were brought together into another word class. figure 1 illustrates the history of discrimination that led to correct use of inflections for the second declension in the simulation with pause information. time goes to the right and down in the figure. it turned out that on four occasions the system proposed an unconstrained rule for the us inflection. this is reflected in the horizontal dimension. going down we have the history of discrimination for each rule. arrows lead from a rule to a discriminated rule. the label on the arrow indicates the feature added in the discrimination. thus  for instance  a1 is a rule that calls for the us inflection  appropriate for nominative singular . it was used incorrectly in an accusative plural situation and an os rule. a1  was formed with the discriminating test that the noun be in accusative case  i.e.. third position in the semantic structure . this rule misapplied in an accusative singular situation and so a singular feature was added. rules in boxes are ones that were so weakened by misapplication that they were removed. figure 1 

　note that there are four rules with all the necessary features: a1 for accusative singular  a1 for genitive plural. a1 for accusative plural  and a1 for nominative plural. on the other hand. a1 for genitive singular only tests that it is in a possessive context and not for number. however  because of the specificity ordering on production selection  the more specific genitive plural rule  a1  will apply whenever applicable leaving a1 only the genitive singular situations in which to apply. similarly  a1 which has no discriminating features will only apply when no other rule is applicable-which is to say it will apply only the nominative singular case for which it gives the appropriate inflection. 
　we ran another simulation training alas on the same subset of french that las  anderson  1  had been trained on. this subset is interesting because it introduces the recursive properties of natural language through relative clause recursion. alas successfully learned this subset the detail* of this simulation are being omitted because of space constraint*. however  it is mentioned here to note that alas can reproduce the success of the previous program. information about this simulation can be obtained by writing to the author. 
verb auxiliaries 
   the third simulation was an attempt to have alas learn the verb auxiliary system of english. this is one of the standard language fragments used to introduce and motivate transformational grammar  e.g.  cuhcover. 1 . this is interesting because the verb auxiliary system does not involve any violations of the graph deformation condition and should be learnable by alas without resorting to transformations. the models we used were cm  could  should  would  will  and may with corresponding meaning components of present-able  past-able  obligation  intention  future  and possibility. these meaning components were not assigned to the terms but rather had to be learned from context the sentences were also marked for tense and  optionally for perfect  progressive  and stative  we used sets of four adjectives  eight nouns  six transitive verbs  and four intransitive verbs. among the verbs were hit  shoot  and run which all have irregular inflections. therefore  another problem for the simulation will be to learn the special inflections associated with these terms. we provided these strings with the pause structures to permit segmentation. space limitations prevent a detailed specification of the semantic representation  but the author can be written to for a fuller report 
　figure 1 plots the performance of the system for 1 pairings which is the number of pairings required to reach 1 trials of errorless performance. examples of sentences it generated are: 
	sentence 1: 	jump angry debutante 
	sentence 1: 	a tall lawyer s could jump ed 
sentence 1: some smart actress have tickle ed the sailor s 
	sentence 1: 	being smart a angry lawyer 
	sentence 1: 	the sailor s were dance ing 
	sentence 1: 	a smart sailor tickle ing a bad lawyer 
sentence 1: the farmer may have shoot ed some arab s 


1 

sentence 1: the fat doctor s should dance ed 
sentence 1: a fat lawyer can be tall ed 
sentence 1: some smart lawyer s should be tickle ing the angry actress s 
sentence 1: a sailor are tickle ed by some good lawyer s 
sentence 1: some sad ed lawyer s have run 
sentence 1: the sad doctor s are kick ed by the angry farmer s 
sentence 1: some lawyer s were being hit ed. 
　these sentences illustrate one of the unexpected developments in the simulation. alas collapsed adjectives  transitive verbs  and untransitive verbs into a single word class over time because all these are involved in numerous similar auxiliary structures. this accounts for the appearance of constructions like  sad ed lawyers  and  can be tall ed  where the  ed  inflection has generalized from the verbs to adjectives. then alas had to go through a number of discriminations in which it used the action-quality property distinction between verbs and adjectives to properly restrict the rules. 
　an important feature of the verb auxiliary system is that  if we consider the verb matrix sequenced tense-modal ity-perfectprogresstve-verb  tense coaditions an inflection in the term that immediately follows it  perfect an inflection in the term that follows it  and similarly progressive. this is interesting because the modality  perfect  and progressive terms are all optional. this means that the term inflected for tense or perfect will vary. so. for instance  depending on the verb matrix we inflect perfect  has/had   progressive  is/was   or verb  kicks/kicked  for tense. this is handled in standard transformational analysis by a transformation called affix hopping. this is handled in our simulation by making the prior term part of the rule. so  for instance. alas learned the rule: 
1 + 1 --  1 + s   1 if 1 is progressive and the context is present and the syntactic subject is singular 
it is not a simple matter to judge whether the affix hopping transformation  together with its many support rules  provides a more parsimonious characterization of verb auxiliary structure or whether our context-sensitive rules do. however  the alas rules seem much easier to learn. this is one illustration of many where learning considerations can be used to guide linguistic description. 
