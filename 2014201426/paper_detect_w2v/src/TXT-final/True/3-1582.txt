
we present a machine learning approach to identifying and resolving one-anaphora. in this approach  the system first learns to distinguish different uses of instances of the word one; in the second stage  the antecedents of those instances of one that are classified as anaphoric are then determined. we evaluated our approach on written texts drawn from the informative domains of the british national corpus  bnc   and achieved encouraging results. to our knowledge  this is the first learningbased system for the identification and resolution of one-anaphora.
1 introduction
the word one is a frequently used word in english: in the 1-million-wordbritish national corpus  bnc   it accounts for 1% of all the words  ranking as the 1th most common word  and the 1th most common pronominal form  more frequent than the pronoun us.1 not all of these instances of one are anaphoric; the most common use is as a number  as in one book   and there are a number of other uses. however  any natural language processing  nlp  system which tries to process anaphora in unrestricted text will need to be able to determine whether a particular instance of one is being used anaphorically  and when this is the case  what the antecedent of the anaphor is.
　in brief  a one-anaphor is an anaphoric noun phrase  np  headed by the word one  as in the following example:
 1  her greater sympathy for the atlantic connection over the european one is not widely shared by colleagues.
one-anaphora is sometimes referred to as  identity-of-sense anaphora   in contrast to the more common pronominal  identity-of-reference  anaphora  hankamer and sag  1 . in processing a pronominal reference  we are generally looking for an antecedent noun phrase that refers to the same entity as the anaphoric form; however  one-anaphoric forms are generally used to refer to something of the same kind as something mentioned before. hence  in order to interpret an instance of one-anaphora  we must identify the antecedent textual material that provides the semantic content alluded to by the use of one. in example  1   correctly interpreting the european one requires inferring that it has a relationship to the noun phrase the atlantic connection. the range of semantic relationships that can hold between an one-anaphoric form and its antecedent is broad and complex; for present purposes  we are interested in identifying the antecedent noun phrase that contains the relevant semantic content.
　we designed two systems that are applied consecutively to identify and resolve one-anaphora. in the case of example  1   our one-expression classifier first determines that the european one is in fact a one-anaphor; then our one-anaphora resolver would identify the noun phrase the atlantic connection as the antecedent of the one-anaphor. in this paper  an antecedent is defined as the noun phrase from which the head sense of the one-anaphor can be determined.
　the pervasiveness of anaphoric reference in general means that anaphora resolution is recognized as an important subtask in natural language processing; one-anaphora resolution  however  has been relatively neglected. as noted  the frequency of occurrence of the word one means that any real nlp system cannot just ignore it; however  the knowledge sources  typically  features of the linguistic context  used in state-of-the-art noun phrase coreference resolution systems  e.g.   soon et al.  1; ng and cardie  1   are not applicable to one-anaphora. consequently  one-anaphora resolution is a task that requires special attention.
　the rest of this paper is organized as follows. we first introduce the various uses of the word one in english and the different types of antecedents of one-anaphora. then we give an overview of how our one-expression classifier and oneanaphora resolver are applied consecutively to identify and resolve one-anaphora  followed by two separate sections giving a detailed description and evaluation of the two systems. in the final two sections  we describe related work and conclude.
1 classes of one-expressions
in this section  we introducethe various uses of one inenglish and the different types of antecedents of one-anaphora  along with some statistics of their distribution in the bnc.
1 uses of one and corpus annotation
the taxonomy of uses of the word one adopted in this section is based on existing theories of the various uses of one  halliday and hasan  1; webber  1; dahl  1; luperfoy  1  and our own study of one-expressions. we divided uses of one into six classes: numeric  partitive  anaphoric  generic  idiomatic  and unclassifiable.
1. numeric one: modifies a head noun to indicate singularity as in example  1 ; this is the only adjectival use of one.
 1  john has one blue t-shirt.
1. partitive one: selects an individual from a set. it is followed by an of-prepositional phrase headed by a plural noun or pronoun as in example  1 .
 1  a special exhibition of books for children forms one of the centrepieces of the 1st annual frankfurt book fair.
1. anaphoric one: relates a set of properties to the set of properties mentioned by the antecedent. there are three types of one-anaphors  distinguished by the type of their antecedents. first  the antecedent may be a kind  as in the noisy cameras of example  1 ; second  the antecedent may be a set of entities  as in the set of two world bank men of example  1 ; and third  the antecedent may refer to a single instance  as in this book of example  1 .
 1  i have an aversion to noisy cameras  and this one rings several decibels before it's done with winding on the film.
 1  the two world bank men  one german and one british  strode across the tarmac.
 1  would you like this book  yes  i would like that one.
1. generic one: a pronominal use that refers to a generic person or to the speaker of a sentence; often used in subject position followed by a modal verb and a main verb that takes an animate subject.
 1  one must think a little deeper to discover the underlying social roots of the problem.
1. idiomatic one: conventionalized uses whose semantics appear not to be based on general use  but rather on idiomatic patterns  as in example  1 .
 1  it would be perfect to have a loved one accompany me in the whole trip.
1. unclassifiable one: inevitably  there are instances which are difficult to classify as any of the above  as in example  1 .
 1  cursed be every one who curses you.
for the present study  we randomly selected 1 one expressions from the bnc 1  and manually annotated these with the six classes above. the distribution of each class in the annotated corpus is shown in table 1  and it mirrors the distribution of one expressions in naturally occurring text.
classfrequency%numeric1.1partitive1.1anaphoric1.1generic1.1unclassifiable1.1idiomatic1.1total11table 1: distribution of uses of one in the annotated corpus.

figure 1: how training and test data are used.
1 annotating antecedents of one-anaphora
we annotated the 1 examples of one-anaphors in our corpus a second time  marking the antecedent in each case. not every one-anaphor has an explicit noun phrase antecedent as shown in the previous anaphoric examples; in such cases  the reader has to infer the nature of the antecedent from the available text. we label those cases where there is no explicit antecedent as  one-anaphors with implicit antecedents . the remaining one-anaphors are  one-anaphors with explicit antecedents . of the 1 one-anaphors  1% had explicit antecedents.
1 an overview of our approach
before we describe our two machine learning systems for one-expression classification and one-anaphora resolution  we first provide an overview of how the two systems are applied consecutively to accomplish the overall task of identifying and resolving one-anaphora. as an example  we use one trial in the 1-fold cross validation to illustrate the process. here  the identification of one-anaphora is step 1  s1   and the resolution of one-anaphora is step 1  s1 .
　as shown in figure 1  a gold standard corpus is divided into two sets  containing 1% and 1% of the data respectively. the larger set is used to train the s1 classifier. in the s1 training data  there is a set of anaphoric examples  ana. this set is used to train the s1 classifier.
　the smaller set of 1% of the examples is used for testing. first  the test examples are passed to the s1 classifier  which identifies a set of anaphoric examples  ana*. this set may contain errors in identification  a one-anaphor not classified as anaphoric  or a non-anaphoric example classified as

ceived as literary or creative  are not considered because they contain a large amount of dialog  which makes one-anaphora resolution a harder task. this restriction follows the genre of muc texts  muc-1  1   a widely used data set in noun phrase coreference research  which also only included written newspaper texts.
anaphoric . ana* is in turn passed to the s1 classifier  which outputs the antecedents it finds  denoted as antecedent*. when calculating accuracy  ana* and antecedent* are used to compare with the gold standard annotation to measure both the s1 accuracy and the overall accuracy of the combined systems.
1 one-expression classification
our one-expression classifier classifies a given oneexpression into one of the six classes listed in table 1.
1 the features
we devised a set of features that is useful in determining which of the six classes a given one-expression belongs to. we focus on the classes numeric  partitive  anaphoric  and generic  since 1% of the instances are from these classes.
　the numeric use of one is the simplest case  since it is the only adjectival use in english. it can be readily identified by the part-of-speech  pos  tag of the word one. partitive use of one can be identified by checking the syntactic context  i.e.   one  of nppl   .
　discriminationbetween the anaphoric and generic classes is more complex. we used the three features issubj isanimateverb  and ismodalverb to identify instances of the generic class  in line with the observations mentioned in the definition of generic one . we also noticed that the relative position of one in its host np and the category of the word immediately preceding one both give strong hints as to the function of one in its host np. therefore  we added two more features  positioninnp and w pos  to assist in classification.
　we experimented with a total of 1 features as described below. each instance in the training and test data set is thus represented as a feature vector of seven values. the feature values are acquired by running the charniak parser  charniak  1  over the corpus  in combination with information about verbs of cognition from wordnet  fellbaum  1 :
1. w pos is the pos tag of one assigned by the charniak parser. its possible values are cd  nn  nnp  prp.
1. isofplural checks whether one is followed by of prepositional phrase with a plural head noun/pronoun. its possible values are plural  notplural  or na when the word one is not followed by a pp headed by of. for example:
 a  plural: one of the patients survived.
 b  notplural: the problem was the unusual one of a warmish  wet spring.
1. issubj checks whether the word one is in subject position. this feature value is inferred from the parse tree structure; its possible values are true or false.
1. isanimateverb checks whether the lemmatized verb following subject one is a verb of cognition according to wordnet: some examples are think  judge  analyze  and doubt. the possible values for this feature are true  false  or na when one is not the subject. example  1  would get true for this feature.
1. ismodalverb checks whether the verb phrase following a subject one contains a modal verb  i.e.  a word with
num in
data setnum
identifiednum
correctrpf1num11.1.1.1par11.1.1.1ana11.1.1.1gen11.1.1.1sub
total111111unc111-idio11--total111111table 1: accuracy of one-expression classification.
md pos tag  such as must . the possible values for this feature are true  false  or na when one is not the subject. example  1  would get true for this feature.
1. positioninnp indicates the position of the word one in its host np. it has four possible values: singleone when the np only consists of the word one; leftmost when the np has multiple words and one is the leftmost word; rightmost when the np has multiple words and one is the rightmost word; and middle otherwise. for example:
 a  singleone:  one  might suppose that ...
 b  leftmost: i was concerned with  one thing  only.
 c  rightmost:  the european one 
 d  middle: on  the one hand 
1. w pos is the pos tag of the word immediately preceding one. its possible values are the 1 pos tags in penn treebank tagset and na when one is the first word of the sentence.
the learning algorithm used in our one-expression classification system is c1  quinlan  1 . this is a commonly used decision tree learning algorithm and may be considered as a baseline against which other learning algorithms can be compared.
1 evaluation and error analysis
evaluation
table 1 gives the 1-fold cross validation results for the oneexpression classifier for each of the 1 classes.
　the accuracy of identifying one-anaphora is 1%  which is not as high as the numeric  partitive  and generic classes. the remaining two classes  idiomatic and unclassifiable  are poorly discriminated; when counted in  these pull down the overall classification accuracy from 1% to 1%.
error analysis
table 1 provides a matrix of the number of misclassifications in each class. row 1 and column 1  highlighted  show that the major confusion with the anaphoric class comes from the generic and numeric classes  with partitive making a smaller contribution to the erroneous classifications.
　our classifier's performance is highly impacted by the accuracy of pos tagging and parsing: failures here caused most of the confusion with numeric and partitive. for example  one in  1  is classified as numeric because rings is wrongly
classifie
as	*d num*par*ana*gen*unc*idio*numna111par1na11ana1na11gen11na1unc11na1idio111natable 1: one-expression classifier error matrix.
tagged as nns; and partitive one in the phrase one or more of them is classified as anaphoric because it is wrongly parsed as   one  or  more of  them   .
　the confusion with the generic class is mainly caused by occurrences of anaphoric one as subject and generic one in non-subject position. such cases can also be confusing for human readers: in a sentence like one is usually shunting around in the yard  we might think we have a generic use  but the one may refer to a previously mentioned locomotive.
1 one-anaphora resolution
our one-anaphor resolver attempts to identify the np in the preceding linguistic context that provides the semantic content required for interpretation of a one-anaphor; we refer to this np as the antecedent np.
1 experimental data
we trained and tested a one-anaphor resolution classifier using a set of positive/negative antecedent-anaphor pairs. a pair is positive when the candidate antecedent in this pair is the actual antecedent of the anaphor; otherwise  it is negative.
training data
in each trial of the 1-fold cross validation  the actual oneanaphora instances in the gold standard training corpus of step 1 are used to create step 1 training instances in the corresponding trial as shown in figure 1.
　creation of training instances consisted of three steps. first  each sentence containing one and its three preceding sentences 1 were processed by rm np chunker  ramshaw and marcus  1  to carry out np chunking. second  each pair of an anaphor and its actual antecedent were used to create a positive training instance. lastly  to generate negative training instances  anaphors were paired with each of the nps that appeared between the anaphor and its real antecedent.
　in our experiment  we further adjusted the ratio of positive to negative instances in the training data by controlling the number of negative instances randomly picked from the whole set of negative instances. we decided to set the ratio at 1  which introduces no preference for either one of the two assignments to the classifier. this procedure produced a set of 1 antecedent-anaphor pairs in total  of which 1  1%  were positive instances. as already noted  each trial used roughly 1% of this set as training data.
test data
in each trial  instances of one identified as being anaphoric in the test data of step 1 were used to create step 1 test instances in the corresponding trial as shown in figure 1.
　creation of test instances consisted of two steps. the first step is the same as that for the training data; in the second step  every base np preceding the instance of one is a potential antecedent  so each of these nps was paired with one.
　when doing testing  the one-anaphora resolution algorithm starts from the immediately preceding base np and proceeds backward in the reverse order of the nps in the context until there is no remaining np to test  or an antecedent is found
 i.e.  the classifier returns true .
1 the features
to decide whether the anaphor in a given antecedent-anaphor pair refers to the candidate antecedent  we need features that show a preference for good candidates.
　as with pronominal anaphora  an intuitivelyappealing feature to use is whether the candidate antecedent np is in focus  sidner  1; vieira and poesio  1 . in anaphora resolution  two commonly used features for approximating the notion of focus are syntactic role and recency: a candidate that fills a salient syntactic role such as subject  or one that is very recent  is often the focus of the discourse. we used four features to measure this type of information: anteissubj  anteinrelclause  anteisnearestnp  and bothinpp. the feature bothinpp allows us to take syntactic parallelism into consideration. the pos tag of the head word of the candidate antecedent is also a good feature in filtering out improper candidates: a proper noun should not be the antecedent of a oneanaphor  dahl  1 .
　we used a total of 1 features as described below. their values are acquired from the charniak parser output of the training/test data set.
1. hwposofante is the pos tag of the head word of the candidate antecedent. its possible values are the set of pos tags of head words that occur in our training/test data. in this paper  the head word of the candidate antecedent is defined as the rightmost noun  or the rightmost word if no noun is found  in the base np candidate antecedent.
1. bothinpp checks whether the candidate antecedent and the one-anaphor are both in prepositional phrases  pp   and identifies the types of pps they are in. it has five possible values: na when the candidate antecedent is not in pp; onlyanteinpp when the candidate antecedent is in pp  but the one-anaphor is not in pp; sharepp when the candidate antecedent and the one-anaphor are in the same pp; commonpreposition when both candidate antecedent and one-anaphor are in different pps  but the prepositions of the two pps are the same; and differentpreposition when bothcandidate antecedent and oneanaphor are in different pps with different prepositions.
1. anteissubj checks whether the candidate antecedent is in subject position. its possible values are true or false.
1. anteisnearestnp checks whether the candidate antecedent is the nearest np preceding the one-anaphor. its possible values are true or false.
1. anteinrelclause checks whether the candidate antecedent is in a relative clause. its possible values are true or false.
　again  the learning algorithm used in our one-anaphora resolution engine is c1.
1 evaluation and error analysis
evaluation
in order to evaluate the overall performance of our approach to one-anaphora identificationand resolution  we conducted a 1-fold cross validation of the one-anaphora resolver  where each trial is performed based on the result of our oneexpression classifier in step 1. the overall recall  precision  and f-measure are presented in table 1  together with the accuracy of one-anaphora identification from step 1.
　the  s1 correct  in table 1 column 1 is the sum of two values. the first value is the number of hits in correctly identifying the explicit antecedent of a one-anaphor. a hit of this type occurs when a one-anaphor with an explicit antecedent is correctly identified as anaphoric in step 1 and the actual antecedent is found in step 1. the second value is the number of hits returning no antecedent for a one-anaphor when it has no annotated antecedent. a hit of this type occurs when a oneanaphor without an explicit antecedent is correctly identified as anaphoric in step 1  and none of the candidate antecedents is accepted in step 1.
　the overall recall is the sum in  s1 correct  divided by the total number of one-anaphors with or without explicit antecedent in the data set  column 1 ; the overall precision is the sum in  s1 correct  divided by the total number of one-anaphors with or without explicit antecedent identified in step 1  column 1 . the f-measure finally achieved is 1%. we compared two baseline heuristics with our accuracy: nearestnp and nearestsubj  which always assign the nearest np or subject preceding the one-anaphor as its antecedent. the two baseline accuracies are calculated by applying the two heuristics on the set of anaphoric ones identified in step 1; nearestnp  nearestsubj  heuristics achieved 1%  1%  accuracy  considerably lower than the overall accuracy of our system.
error analysis and future improvement since we perform one-anaphora resolution on the output of our one-anaphora identification system  the 1% overall accuracy is a combination of both step 1 and step 1 performance. errors introduced in step 1 were never remedied in step 1  and they directly affected both overall recall and precision. in other words  given a perfect step 1 classifier to work on the current one-anaphora identification output  the highest overall f-measure achievable is 1%. therefore  any further improvement in step 1 performance would significantly improve the overall accuracy. this could be done by adding features to improve the performance of step 1  or adding a remedy strategy in later processing.
　our system finds it difficult to locate actual antecedents that are far away from their one-anaphors: preference is wrongly given to closer and more salient candidates. in example  1   the director has a strong syntactic preference and it is considered before moustache  so the system wrongly returns the director even before checking the actual antecedent.
 1  he has this ridiculous moustache. ken russell  the director  insisted i grew one of my own  rather than wear a false one  so that i looked completely convincing.
　we expect that such mistakes could be corrected by using semantic features.
1 the contribution of the features
to evaluate the relative contributionof the various knowledge sources to the overall accuracy of one-anaphora identification and resolution  we ran a series of leave-one-out classifiers  where we first used all step 1 features and disabled one step 1 feature at a time; then we used all step 1 features and disabled one step 1 feature at a time. the contribution of the features measured in terms of the overall f-measure is shown in table 1 column 1.
　the critical features which cause a substantial reduction of overall f-measure when disabled are step 1 features positioninnp  isofplural  and w pos  as well as step 1 features hwposofante and bothinpp. the remaining features only have a small impact on the overall f-measure.
1 related work
the literature on one-anaphora is small; we cited the most significant works in the area in section 1. most of the existing literature is more concerned with describing the phenomenon than in determining how it might be handled automatically.
　there is  of course  an extensive literature on computational techniques for resolving pronominal anaphora  going back to at least the 1s. of the more recent research in the area  important work is that of lappin and leass  and kennedy and boguraev   who provided heuristics that could be used to determine the antecedents of pronominal forms. soon et al.  and ng and cardie  used a machine learning approach for coreference resolution. however  most of the linguistic features used in the work on pronominal anaphora are not applicable to the one-anaphora problem. markert et al.  focused on the related phenomenon of other-anaphora.
1 conclusion
in this paper  we have presented a machine learning approach to the identification and resolution of one-anaphora and achieved encouraging results; to our knowledge  this is the first learning-based system for resolving one-anaphora. there is scope for refinement to improve both the accuracy of identifying anaphoric uses of one  and of identifying the antecedent noun phrase that contains the semantic content required for interpreting the one-anaphor. beyond this goal  there are more complex challenges awaiting in terms of oneanaphora interpretation.
num
of
ana
in data setstep1: 1 features  f1-f1 step1:	1 features  f1-f1 overallbaselines1
features1
sysout
as anas1
corrects1 rs1 ps1 f1s1
features1
correctrpf1n-npn-subj1all1111all111111no f1+1.1.1.1no f1+1.1.1.1no f1+1.1.1.1no f1+1.1.1.1no f1+1.1.1.1no f11.1.1.1all111111no f11.1.1.1all111111no f11.1.1.1all111111no f11.1.1.1all111111no f11.1.1.1all111111no f11.1.1.1all111111no f11.1.1.1all111111table 1: overall accuracy and baseline of identification and resolution of one-anaphora & contribution of the features. step1 features are
1.positioninnp  1.isofplural  1.w pos  1.isanimate  1.w	pos  1.issubj  1.ismodalverb. step1 features are 1.hwposofante  1.both-inpp  1.anteinrelclause  1.anteisnearestnp  1.anteissubj
