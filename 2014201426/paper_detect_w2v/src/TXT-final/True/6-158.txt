 
 in most real-world settings  due to limited time or other resources  an agent cannot perform all potentially useful deliberation and information gathering actions. this leads to the metareasoning problem of selecting such actions. decision-theoretic methods for metareasoning have been studied in ai  but there are few theoretical results on the complexity of metareasoning. we derive hardness results for three settings which most real metareasoning systems would have to encompass as special cases. in the first  the agent has to decide how to allocate its deliberation time across anytime algorithms running on different problem instances. we show this to be atp-complete. in the second  the agent has to  dynamically  allocate its deliberation or information gathering resources across multiple actions that it has to choose among. we show this to be afp-hard even when evaluating each individual action is extremely simple. in the third  the agent has to  dynamically  choose a limited number of deliberation or information gathering actions to disambiguate the state of the world. we show that this is afp-hard under a natural restriction  and hard in general. 
1 introduction 
in most real-world settings  due to limited time  an agent cannot perform all potentially useful deliberation actions. as a result it will generally be unable to act rationally in the world. this phenomenon  known as bounded rationality  has been a long-standing research topic  e.g.   1  . most of that research has been descriptive: the goal has been to characterize how agents-in particular  humans-deal with this constraint. another strand of bounded rationality research has the normative  prescriptive  goal of characterizing how agents should deal with this constraint. this is particularly important when building artificial agents. 
　characterizing how an agent should deal with bounded rationality entails determining how the agent should deliberate. 
   *the material in this paper is based upon work supported by the national science foundation under career award iri-1  grant iis-1  itr iis-1  and itr ns-1. 
because limited time  or other resources  prevent the agent from performing all potentially useful deliberation  or information gathering  actions  it has to select among such actions. reasoning about which deliberation actions to take is called metareasoning. decision theory  1  provides a normative basis for metareasoning under uncertainty  and decisiontheoretic deliberation control has been widely studied in ai  e.g.   1-1 1-1-1  . 
　however  the approach of using metareasoning to control reasoning is impractical if the metareasoning problem itself is prohibitively complex. while this issue is widely acknowledged  e.g.   1-1    there are few theoretical results on the complexity of metareasoning. 
　we derive hardness results for three central metareasoning problems. in the first  section 1   the agent has to decide how to allocate its deliberation time across anytime algorithms running on different problem instances. we show this to be np-complcte. in the second metareasoning problem  section 1   the agent has to  dynamically  allocate its deliberation or information gathering resources across multiple actions that it has to choose among. we show this to be mvhard even when evaluating each individual action is extremely simple. in the third metareasoning problem  section 1   the agent has to  dynamically  choose a limited number of deliberation or information gathering actions to disambiguate the state of the world. we show that this is np-hard under a natural restriction  and pspace-hard in general. 
　these results have general applicability in that most metareasoning systems must somehow deal with one or more of these problems  in addition to dealing with other issues . we also believe that these results give a good basic overview of the space of high-complexity issues in metareasoning. 
1 allocating anytime algorithm time across problems 
in this section we study the setting where an agent has to allocate its deliberation time across different problems-each of which the agent can solve using an anytime algorithm. we show that this is hard even if the agent can perfectly predict the performance of the anytime algorithms. 
1 	motivating example 
consider a newspaper company that has  by midnight  received the next day's orders from newspaper stands in the 1 
cities where the newpaper is read. the company owns a fleet of delivery trucks in each of the cities. each fleet needs its vehicle routing solution by 1am. the company has a default routing solution for each fleet  but can save costs by improving  tailoring to the day's particular orders  the routing solution of any individual fleet using an anytime algorithm. in this setting  the  solution quality  that the anytime algorithm provides on a fleet's problem instance is the amount of savings compared to the default routing solution. 
   we assume that the company can perfectly predict the savings made on a given fleet's problem instance as a function of deliberation time spent on it  we will prove hardness of metareasoning even in this deterministic variant . such functions are called  deterministic  performance profiles  1 1 1 . each fleet's problem instance has its own performance profile.1 suppose the performance profiles are as shown in fig. 1. 

figure 1: performance profiles for the routing problems. 
then the maximum savings we can obtain with 1 hours of deliberation time is 1  for instance by spending 1 hours on instance 1 and 1 on instance 1. on the other hand  if we had until 1am to deliberate  1 hours   we could obtain a savings of 1 by spending 1 hours on instance 1. 
1 	definitions and results 
we now define the metareasoning problem of allocating deliberation across problems according to performance profiles. 
definition 1  performance-profiles  we are given a list of performance profiles   f i   f1         fm   where each f  is a nondecreasing function of deliberation time  mapping to nonnegative real numbers   a number of deliberation steps n  and a target value k. we are asked whether we can distribute the deliberation steps across the 
　　'because the anytime algorithm's performance differs across instances  each instance has its own performance profile  in the setting of deterministic performance profiles . in reality  an anytime algorithm's performance on an instance cannot be predicted perfectly. rather  usually statistical performance profiles are kept that aggregate across instances. in that light one might question the assumption that different instances have different performance profiles. however  sophisticated deliberation control systems can condition the performance prediction on features of the instance-and this is necessary if the deliberation control is to be fully normative.  research has already been conducted on conditioning performance profiles on instance features  1 1  or results of deliberation on the instance so far  1 1 1 .  
problem instances to get a total performance of at least k;  with 
   a reasonable approach to representing the performance profiles is to use piecewise linear performance profiles. they can model any performance profile arbitrarily closely  and have been used in the resource-bounded reasoning literature to characterize the performance of anytime algorithms 
 e.g.  . we now show that the metareasoning problem is afp-complete even under this restriction. we will reduce from the knapsack problem.1 

theorem 1 performance-profiles is np-complete even if each performance profile is continuous and piecewise 
linear.1. 
proof: the problem is in mv because we can nondcterministically generate the  in polynomial time  since we do not need to bother trying numbers greater than n   and given the  we can verify if the target value is reached in polyno-
mial time. to show np-hardness  we reduce an arbitrary 
knapsack instance to the following performance-


1 	resource-bounded reasoning 

so we have found a solution to 
the performance-profiles instance. on the other hand  suppose there is a solution to the performance-

1 dynamically allocating evaluation effort across options  actions  
in this section we study the setting where an agent is faced with multiple options  actions  from which it eventually has to choose one. the agent can use deliberation  or information gathering  to evaluate each action. given limited time  it has to decide which ones to evaluate. we show that this is hard even in very restricted cases. 
1 	motivating example 
consider an autonomous robot looking for precious metals. it can choose between three sites for digging  it can dig at most one site . at site a it may find gold; at site b  silver; at site c  copper. if the robot chooses not to dig anywhere  it gets utility 1  for saving digging costs . if the robot chooses to dig somewhere  the utility of finding nothing is 1; finding gold  1; finding silver  1; finding copper  1. the prior probability of there being gold at site a is 1  that of finding silver at site b is 1  and that of finding copper at site c is 1. 
　in general  the robot could perform deliberation or information gathering actions to evaluate the alternative  digging  actions. the metareasoning problem would be the same for both  so for simplicity of exposition  we will focus on information gathering only. specifically  the robot can perform tests to better evaluate the likelihood of there being a precious metal at each site  but it has only limited time for such tests. the tests are the following:  1  test for gold at a. if there is gold  the test will be positive with probability if there is no gold  the test will be positive with probability 1. this test takes 1 units of time.  1  test for silver at b. if there is silver  the test will be positive with probability 1; if there figure 1: tree representation of the action evaluation instance. 
ing at the root represents not having done a test yet  whereas being at a left  right  leaf represents the test having turned out positive  negative ; the value at each node is the expected value of digging at this site given the information corresponding to that node. the values on the edges are the probabilities of the test turning out positive or negative. we can subsequently use these trees for analyzing how we should gather information. for instance  if we have 1 units of time  the optimal information gathering policy is to test at b first; if the result is positive  test at a; otherwise test at c.  we omit the proof because of space constraint.  
1 	definitions 
in the example  there were four actions that we could evaluate: digging for a precious metal at one of three locations  or not digging at all. given the results of all the tests that we might undertake on a given action  executing it has some expected value. if  on the other hand  we do not  yet  know all the results of these tests  we can still associate an expected value with the action by taking an additional expectation over the outcomes of the tests. in what follows  we will drop the word  expected  in its former meaning  that is  when talking about the expected value given the outcomes of all the tests   because the probabilistic process regarding this expectation has no relevance to how the agent should choose to test. hence  all expectations are over the outcomes of the tests. 
　while we have presented this as a model for information gathering planning  we can use this as a model for planning 

 computational  deliberation over multiple actions as well. in this case  we regard the tests as computational steps that the agent can take toward evaluating an action.1 
　to proceed  we need a formal model of how evaluation effort  information gathering or deliberation  invested on a given action changes the agent's beliefs about that action. for this model  we generalize the above example to the case where we can take multiple evaluation steps on a certain action  although we will later show hardness even when we can take at most one evaluation step per action . definition 1 an action evaluation tree is a tree with 
  a root r  representing the start of the evaluation; 
  for each nonleafnode w  a cost kwfor investing another step of evaluation effort at this point; 
  for each edge e between parent node p and child node c  a probability  of transitioning from p to c 
upon taking a step of evaluation effort at p; 
  for each leaf node 	value 
　according to this definition  at each point in the evaluation of a single action  the agent's onlv choice is whether to invest further evaluation effort  but not how to continue the evaluation. this is a reasonable model when the agent does evaluation through deliberation and has one algorithm at its disposal. however  in general the agent may have different information gathering actions to choose from at a given point in the evaluation  or may be able to choose from among several deliberation actions  e.g.  via search control  1 . in section 1  we will discuss how being able to choose between tests may introduce drastic complexity even when evaluating a single thing. in this section  however  our focus is on the complexities introduced by having to choose between different actions on which to invest evaluation effort next. 
　the agent can determine its expected value of an action  given its evaluation so far  using the subtree of the action evaluation tree that is rooted at the node where evaluation has brought us so far. this value can be determined in that subtree by propagating upwards from the leafs: for parent p with a set of children c  we have 
　we now present the metareasoning problem. in general  the agent could use an online evaluation control policy where the choices of how to invest future evaluation effort can depend on evaluation results obtained so far. however  to avoid trivial complexity issues introduced by the fact that such a contingency strategy for evaluation can be exponential in size  we merely ask what action the agent should invest its first evaluation step on. 
definition 1  action-evaluation  we are given i action evaluation trees  indexed 1 through i  corresponding to i different actions.  the transition processes of the trees are independent.  additionally  we are given an integer n. we are asked whether  among the online evaluation control policies that spend at most n units of effort  there exists one that takes its first evaluation step on action j  and gives maximal expected utility among online evaluation control policies that spend at most n units of effort.  if at the end of the deliberation process  we are at node for tree  then our utility is max  because we will choose the action with the highest expected value.  
1 results 
we now show that even a severely restricted version of this problem is np-hard.1 
theorem 1 action-evaluation is np-hard  even when all trees have depth either 1 or i  branching factor 1  and all leaf values are -i  1  or i. 

vations about the constructed action-evaluation instance. first  once we determine the value of a action to be 1  choosing this action is certainly optimal regardless of the rest of the deliberation process. second  if at the end of the deliberation process we have not discovered the value of any action to be 1  then for any of the trees of depth 1  either we have discovered the corresponding action's value to be -1  or we have done no deliberation on it at all. in the latter case  the expected value of the action is always below 1  is carefully set to achieve this . hence  we will pick action 1 for value 1. it follows that an optimal deliberation policy is one that maximizes the probability of discovering that a action has value 1. now  consider the test set of a policy  which is the set of actions that the policy would evaluate if no action turned out to have value 1. then  the probability of discovering that a action has value 1 is simply equal to the probability that at least one of the actions in this set has value 1. so  in this case  the quality of a policy is determined by its test set. now we observe that any optimal action is either the one that only evaluates action 1  and then runs out of deliberation time   or one that has action 1 in its 

test set.  for consider any other policy; since evaluating action 1 has minimal cost  and gives strictly higher probability of discovering a action with value 1 than evaluating on any other action besides 1  simply replacing any other action in the test set with action 1 is possible and improves the policy.  now suppose there is a solution to the knapsack instance  that is  a set s such that and 
　　　　　then we can construct a policy which has as test set 	 evaluating all these actions costs at most deliberation units.  the probability of at least one of these actions having value 1 is at least the probability that exactly one of them has value 1  which is 
using our previous observation we 
can conclude that there is an optimal action that has action 1 in its test set  and since the order in which we evaluate actions in the test set does not matter  there is an optimal policy which evaluates action 1 first. on the other hand  suppose there is no solution to the knapsack instance. consider a policy which has 1 in its test set  that is  the test set can be expressed as for some set then we must have and since there is no solution to the knapsack instance  it follows that but the probability that at least one of the actions in the test set 

that the policy of just evaluating action 1 is strictly better. so  there is no optimal policy which evaluates action 1 first. 
　we have no proof that the general problem is in is an interesting open question whether stronger hardness results can be obtained for it. for instance  perhaps the general problem is -complete. 
1 dynamically choosing how to disambiguate state 
we now move to the setting where the agent has only one thing to evaluate  but can choose the order of deliberation  or information gathering  actions for doing so. in other words  the agent has to decide how to disambiguate its state. we show that this is hard.  we consider this to be the most significant result in the paper.  
1 	motivating example 
consider an autonomous robot that has discovered it is on the edge of the floor; there is a gap in front of it. it knows this gap can only be one of three things: a staircase  1   a hole  i/   or a canyon  c   assume a uniform prior distribution over these . the robot would like to continue its exploration beyond the gap. there are three courses of physical action available to the robot: attempt a descent down a staircase  attempt to jump over a hole  or simply walk away. if the gap turns out to be a staircase and the robot descends down it  this gives utility 1. if it turns out to be a hole and the robot jumps over it  this gives utility 1  discovering new floors is more interesting . if the robot walks away  this gives utility 1 no matter what the gap was. unfortunately  attempting to jump over a staircase or canyon  or trying to descend into a hole or canyon  has the disastrous consequence of destroying the robot  utility  . it follows that if the agent cannot determine with certainty what the gap is  it should walk away. 
　in order to determine the nature of the gap  the robot can conduct various tests  or queries . the tests can determine the answers to the following questions:  1  am i inside a building  a yes answer is consistent only with s  a no answer is consistent with 1  h  c.  1  if i drop a small item into the gap  do i hear it hit the ground  a yes answer is consistent with s  h  a no answer is consistent with h  c.  1  can 1 walk around the gap  a yes answer is consistent with s  h; a no answer is consistent with s  h  c. 
　assume that if multiple answers to a query are consistent with the true state of the gap  the distributions over such answers are uniform and independent. note that after a few queries  the set of states consistent with all the answers is the intersection of the sets consistent with the individual answers; once this set has been reduced to one element  the robot knows the state of the gap. 
　suppose the agent only has time to run one test. then  to maximize expected utility  the robot should run test 1  because the other tests give it no chance of learning the state of the gap for certain. now suppose that the agent has time for two tests. then the optimal test policy is as follows: run test 1 first; if the answer is yes  run test 1 second; otherwise  run test 1 second.  if the true state is 1  this is discovered with probability  if it is h  this is discovered with probability so total expected utility is starting with test 1 or test 1 can only give expected utility 
1 	definitions 
we now define the metareasoning problem of how the agent should dynamically choose queries to ask  deliberation or information gathering actions to take  so as to disambiguate the state of the world. while the illustrative example above was for information gathering actions  the same model applies to deliberation actions for state disambiguation  such as image processing  auditory scene analysis  sensor fusing  etc. . 
definition 1  state-disambiguation  we are given 
of possible world states;1 
a probability function p over 
　　　1if there are two situations that are equivalent from the agent's point of view  the agent's optimal course of action is the same and the utility is the same   then we consider those situations to be one state. note that two such situations may lead to different answers to the queries. for example  one situation may be that the gap is an indoor staircase  and another situation may be that the gap is an outdoor staircase. these situations are considered to be the same state  but will give different answers to the query  am i inside  . 

a utility function where gives the utility of knowing for certain that the world is in state at the end of the metareasoning process;  not knowing the state of the world for certain always gives utility 
 a query set 	where each  is a list of subsets of 	each such subset corresponds to an answer to the query  and indicates the states that are consistent with that answer. we require that for each state  at least one of the answers is consistent with it: that is  for any  we have when 
a query is asked  the answer is chosen  uniformly  randomly by nature from the answers to that query that are consistent with the world's true state  these drawings are independent ; 
 an integer n; a target value g. 
we are asked whether there exists a policy for asking at most n queries that gives expected utility at least g be the probability of identifying the state when it is the expected utility is given by 
1 	results 
before presenting our hardness result  we will first present a relatively straightforward hardness result for the case where for each query  only one answer is consistent with the state of the world. this situation occurs when the states are so specific as to provide enough information to answer every query. our reduction is from 
definition 1  set-cover  we are given a set a collection of subsets and a positive integer m. we are asked whether any m of these subsets cover that is  whether there is a subcollection such that and 
theorem 1 state-disambiguation is hard  even when for each state-query pair there is only one consistent 
answer. 
proof: we reduce an arbitrary set-cover instance to the following state-disambiguation instance. let 
	let 	be uniform. let 	and for any 
let 1. q 
we claim the instances are equivalent. 
　　　1therc arc several natural generalizations of this metareasoning problem  each of which is at least as hard as the basic variant. one allows for positive utilities even if there remains some uncertainty about the state at the end of the disambiguation process. in this more general case  the utility function would have subsets of  as its domain  or perhaps even probability distributions over such subsets . in general  specifying such utility functions would require space exponential in the number of states  so some restriction of the utility function is likely to be necessary; nevertheless  there are utility functions specifiable in polynomial space that are more general than the one given here. another generalization is to allow for different distributions for the query answers given. one could also attribute different execution costs to different queries. finally  it is possible to drop the assumption that queries completely rule out certain states  and rather take a probabilistic approach. 
　first suppose there is a solution to the set-cover instance  that is  a subcollection such that 
m and then our policy for the state-
disambiguation instance is simply to ask the queries corresponding to the elements of  in whichever order and unconditionally on the answers of the query. if the true state is in 1  we will get utility  regardless. if the true state is each query will eliminate the elements of the corresponding  from consideration. since  is a set cover  it follows that after all the queries have been asked  all elements of s have been eliminated  and we know that the true state of the world is  to get utility 1. so the expected utility is  so there is a solution to the state-disambiguation instance. 
on the other hand  suppose there is a solution to the 
state-disambiguation instance  that is  a policy for asking at most n queries that gives expected utility at least g. because given the true state of the world  there is only one answer consistent with it for each query  it follows that the queries that will be asked  and the answers given  follow deterministically from the true state of the world. since we cannot derive any utility from cases where the true state of the world is not 1  it follows that when it is b  we must be able to conclude that this is so in order to get positive expected utility. consider the queries that the policy will ask in this latter case. each of these queries will eliminate precisely the corresponding  since at the end of the deliberation  all the elements of 1 must have been eliminated  it follows that these in fact cover s. hence  if we let  be the collection of these this is a solution to the set-cover instance. 
	we are now ready to present our 	hardness re-
sult. the reduction is from stochastic satisfiability  which is complete . 
definition 1  stochastic-sat  ssat   we are given a 
boolean formula in conjunctive normal form  with a set of clauses   over variables 	we 
play the following game with nature: we pick a value for subsequently nature  randomly  picks a value for y1  whereupon we pick a value for  after which nature picks a value for  etc.  until all variables have a value. we are asked whether there is a policy  contingency plan  for playing this 
game such that the probability of the formula being eventually satisfied is at least 
	now we can present our 	hardness result. 
theorem 1 state-disambiguation is 	hard. 
proof: let 	where 	consists of the 
elements of an upper triangular matrix  that is  
. p is 
uniform over this set. u is defined as follows: for a l l f o r 
all where nans q  is the number of possible answers to the queries are as follows. for every 
there is a query additionally  for each variable there are the following two queries: letting 
 that is  row i in the matrix   and letting we have 
resource-bounded reasoning 

                                                   we may assume that the policy that achieves the target value asks one of the former two in this case as well. it folquery 	we say 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　lows that the part of this policy that handles the cases where this corresponds to nature selecting 
no answers have been either one of the corresponds 

the following contingency plan for asking queries: 
  start by asking the query corresponding to how the first variable is set in the ssat instance  that is  is set to true  is set to false '  
  so long as all the queries and answers correspond to variables being selected  we follow the ssat contingency plan; that is  whenever we have to ask a query  we ask the query that corresponds to the variable that would be selected in the ssat contingency plan if variables so far had been selected in a manner corresponding to the queries and answers we have seen; 
  if  on the other hand  we get as an answer  we proceed to ask in that order; 
  finally  if we get c as an answer  we simply stop. 
   we make two observations about this policy. first  if the true state of the world is one of the we will certainly discover this.  upon asking query which is or we will receive answer v1 and switch to qk queries; then if query will be we will receive answer 
and know the state; whereas if we will eliminate all the other elements of with queries 1 through and know the state.  second  if the true state is b  for any 
　　　query will be either this will certainly eliminate all the   so we will know the state at the end if and only if we also manage to eliminate all the clauses. but now notice that each query-answer pair eliminates exactly the same clauses as the corresponding variable selections satisfy. it follows that we will know the state in the end if and only if these corresponding variable selections satisfy all the literals. but the process by which the queries and answers are selected is exactly the same as in the ssat instance with the solution policy. it follows we discover the true state with probability at least 1. hence  our total expected utility is at least 
g. so there is a policy that achieves the goal. 
　now suppose there is a policy that achieves the goal. we first claim that such a policy will always discover the true state if it is one of the for if a policy does not manage this  then there is some such that for some combination of answers consistent with   the policy will not discover the state. suppose this is indeed the true state. since each consistent answer to query occurs with probability at least it follows that the unfavorable combination of an-
swers occurs with probability at least it follows 
that even if we discover the true state in every other scenario  exactly to a valid ssat policy  according to the correspondence between queries/answers and variable selections outlined earlier in the proof. but now we observe  as before  that if the true state is  the probability that we discover this with the state-disambiguation policy is precisely the probability that this ssat policy satisfies all the clauses. this probability must be at least  in order for the state-
disambiguation policy to reach the target expected utility value. so there is a solution to the ssat instance. 
　the following theorem allows us to make any hardness result on state-disambiguation go through even when restricting ourselves to a uniform prior over states  or to a constant utility function over the states. 
theorems every state-disambiguation instance is equivalent to another state-disambiguation instance with a uniform prior and to another with a constant utility function 	moreover  these equivalent instances can be constructed in linear time. 
1 conclusion and future research 
in most real-world settings  due to limited time or other resources  an agent cannot perform all potentially useful deliberation and information gathering actions. this leads to the metareasoning problem of selecting such actions carefully. decision-theoretic methods for metareasoning have been studied in ai for the last 1 years  but there are few theoretical results on the complexity of metareasoning. 
   we derived hardness results for three metareasoning problems. in the first  the agent has to decide how to allocate its deliberation time across anytime algorithms running on different problem instances. we showed this to be complete. in the second  the agent has to  dynamically  allocate its deliberation or information gathering resources across 

resource-bounded reasoning 	1 

multiple actions that it has to choose among. we showed this to be hard even when evaluating each individual action is very simple. in the third  the agent has to  dynamically  choose a limited number of deliberation or information gathering actions to disambiguate the state of the world. we showed that this is hard under a natural restriction  and hard in general. 
   our results have general applicability in that most metareasoning systems must somehow deal with one or more of these problems  in addition to dealing with other issues . the results are not intended as an argument against metareasoning or decision-theoretic deliberation control. however  they do show that the metareasoning policies directly suggested by decision theory are not always feasible. this leaves several interesting avenues for future research: 1  investigating the complexity of metareasoning when deliberation  and information gathering  is costly rather than limited  1  developing optimal metareasoning algorithms that usually run fast  albeit  per our results  not always   1  developing fast optimal metareasoning algorithms for special cases  1  developing approximately optimal metareasoning algorithms that are always fast  and 1  developing meta-mctareasoning algorithms to control the meta-reasoning  etc. 
