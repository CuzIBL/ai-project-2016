 
direct memory access translation  dmtrans  is a theory of translation developed at cmt of cmu in which translation is viewd as an integrated part of cognitive processing. in this paradigm  understanding in source language is a recognition of input in terms of existing knowledge in memory and integration of the input into the memory. context of sentences are established as what is left in memory after understanding previous sentences  or a preceding part of a sentence . decisions made during translation are influenced by what is dynamically modified in memory through preceding recognitions. since knowledge in memory is directly shared with the rest of cognition  during translation other cognitive processes such as inference can dynamically participate in the translation process. 
	i. 	introduction 
the direct memory access translation  dmtrans  is a new approach to machine translation currently researched at the center for machine translation  cmt  of cmu. we claim that every part of cognition dynamically participates in translation  as in any other cognitive process  through shared memory  and that a translation system aiming at fully-autonomous machine translation should be designed with this in mind. this project is an experimental project currently being developed at the c m t as a new generation mt system and should not be confused with the ongoing c m u - m t project  tomita&carbonell . 
     the current implementation of dmtrans uses the spreading activation model as a simulated parallel memory search1 to recognize input in terms of the existing knowledge in memory. similar approaches to understanding languages are found in quillian l1   collins  fahlman  riesbeck&martin. related past works in this area include hirst  hahn  yokoyama&hanakata  and charniak. we prefer this method  because translation is performed directly througjh the network of memory  which makes dynamic interaction with other memoryrelated processes possilble  and because all previously created memory structures can potentially participate in translation. dmtrans extends and integrates theories of direct memory access understanding into translation with consideration of 
   *a significant part of this research was done at the computer science department of yale university as i multi-lingual natural language module of the project ivy  case-base medical consultation system supported by the national library of medicine under oram no. 1-r1-lm1 . 
   1 the author is supported by the fulbright scholarship under he designation dob  no. 1. 
   1  a guided spreadmg activation is performed directly on the memory net and no modular syntactic analysis  birnbaum 1d is done. 
1 	natural language 
cross-cultural questions that accompany the attempt. we view translation as locating existing memory structures under the source language that the text is referring to and generating text that refers to these memory structures in the target language1. often  a single memory structure is not shared by different languages and in that case  use of similar existing memory structures and explanation by surrounding memory structures replace direct generation from identified memory structures. currently  the system is developed to translate between english and japanese and is capable of understanding and generating fairly complex sentences between the two languages. 
	ii. 	where most mt systems fail 
a. 	ambiguation - no choice over others 
because most mt systems do not understand what they are translating  they are incapable of making decisions based on the content of the material they are translating. for example  the famous structurally ambiguous examples such as  i saw a man with a telescope'' and  the man left by the door rotten  are handled by current systems by representing multiple interpretations of the input; however  this does not mean these systems are capable of handling garden path sentences  since none of these systems are capable of choosing the most correct interpretation over the others. since inputs are translated sentence by sentence  virtually no contexual help is available during translation. this makes an autonomous translation extremely unlikely  because very often sentences can have multiple interpretations  most of which  humans are unaware of ; without human assistance  such systems are incapable of selecting one interpretation over others1. thus  being able to generate all possible interpretations of an input sentence does not automatically mean the system is capable of handling syntactically ambiguous sentences. we claim that the system should be able to select the correct interpretation  what speaker intended  in order to claim that it handles'* such a sentence. unfortunately  most current mt systems fail in this task. by the same token  most mt systems fail in handling semantically ambiguous sentences. consider the examples: ''the quality of this paper is terrible  and  john gave mary a punch  . in the former example  the interpretation of paper should be different  for example  japanese for 'thesis' ana 'a sheet of paper' is different  according to what has been said before  or perhaps  visual perception of the situation may supply help . in the latter sentence  interpretation should be different again due to 
　　1  since understanding is done as accomodating input with already existing knowledge in memory  or past cases  we can also view dmtrans as a kind of case-based translation theory. 
　　1this problem is conspicuous when a sentence has a fairly complex structure including conjuncts. consider  show me the picture of lung with small cell carcinoma with magnification of ten and the brain with squamous cell carcinoma with magnification of five**. 

the context  japanese for punch as propel and punch as a drink is different . again  being able to generate multiple interpretations of sentences does not mean the system is capable of handling semantically ambiguous sentences. the system instead should be able to choose appropriate interpretations. 
b. ellipses  anaphora  indirect speech act 
in most mt systems  ellipsis in a sentence results in either no parse at all or output with missing slots. for example  in translating  kouryo suu to ittaga  totemo shinjigatai     he  said   he  will consider  it   but  i  can hardly believe  it   which is a typical japanese sentence with missing subjects  most mt systems simply fail in filling in missing information1. another example is  how often does squamous cell carcinoma metastasize to the brain  lung  large cell carcinoma  . unless mt systems perform some strong inference at run-time  it is beyond their capacity to handle this phenomenon. 
     anaphoric expressions are another kind of phenomenon that most mt systems fail to handle. consider the example of  musashi threw an arrow at the giant rat. it ate it.  current mt systems arc satisfied with translating 'it' as 'it'1; however  this often creates problems: for example  japanese does not prefer 'sore'  it  for animate objects whereas english refers to both animate and inanimate objects with 'it'. in some languages  the morphology of 'it' changes according to what it is referring to. in this sense  anaphora is another phenomenon most mt systems avoid 
     indirect speech acts also cause failure in most mt systems. at best  these systems output two possible interpretations of the utterance: the primary illocution and the secondary illocution  however  no preference for one over the other is made. a conference interpreter will take  can you move over a little  your shoulder is blocking the picture  almost undoubtedly to be a request instead of a question. without knowledge of what is it that the interpreter is translating  such an automatic choice is impossible. 
iii. what dmtrans can do 
dmtrans outperforms most systems in choosing an appropriate interpretation of sentences over others in accordance with contexts. dmtrans docs not even realize many of the unlikely interpretations of the text  just like humans do not realize unlikely interpretations of an input text . this is possible because sentences arc always recognized in context in dmtrans  by performing strong predictions based on what has been recognized previously. 
a. 	contexual recognition of concepts 
first  a brief view of the dmtrans marker passing mechanism is in order. we have three kinds of markers1 that are spread around in the memory network: the activationmarker  a-marker   the prediction-marker  p-marker   and the context-marker  c-marker . the a-marker is to mark concepts  and the abstractions  that are being identified with inputs. the p-marker is used to predict the next likely concept to be recognized  through knowledge of the possible sequence of concepts. the c-marker is used to mark concepts that are likely to be input under a given context. when a word comes in  the word sends activation to  put an a-marker on  a concept that the word is attached to and the activation is sent above the abstraction hierarchy in the network. the a-marker contains the source of activation to indicate which concept originated the activation. the p-marker contains the origin of prediction of a concept that the p-marker is put on. predictions are initially made  p-markers are put  on all the first elements of concept sequences1  and if a predicted concept receives activation  when a-marker and p-marker meet  then the next element of the concept sequence is predicted. a concept sequence is a sequence of concepts that represents an order of concepts that is unique to a language and is stored in root concepts1. when the last element of a concept sequence is activated  then the concept sequence is accepted and the associated root concept is recognized. when this happens  dmtrans searches for  or creates if it does not exist yet  some concept underneath the root concept in the abstraction hierarchy that represents the specific input concept sequence1 and activates that concept  another spreading activation . the c-marker is stored in concepts  not necessarily root concepts  that influence the context of the text  and is sent to associated concepts when this concept is activated1. when activation is spread upward in the abstraction hierarchy and if more than one route exist 
 such as two meanings for a word   then the route through the c-marked concepts are chosen unless the route hits a higher level concept that indicates a contrary preference. 
     in order to demonstrate this mechanism  let us examine a short translation of a semantically  word-sense  ambiguous sentence:  john is at ucai-1. he said the quality of the paper is terrible   figure 1 . initially  all the first elements of concept sequences  indicated by  ...   are predicted. the first word  john'* comes in and activates the concept 'john'  put a-marker on it  then the a-marker is sent upward until it hits the concept 'person' which is predicted by 'at-person-loc' as the first element of the sequence. then the prediction is sent to 'is' which gets activated by receiving a-marker from next input word  is . then 'at' is predicted as the third element of the sequence which meets activation from the input  at . then the prediction for 'ijcai-1' is made. when the word  ijcai-1  comes in  and activates 'ijcai-1' which was predicted as the last element of the concept sequence  person is at location this concept sequence is accepted and the root-concept 'at-person-loc' gets activated. then the search is performed to find a specific concept under the root concept that indicates the input1  and a concept refinement is conducted to get to 'at-john-ijcai-1'. if this is not found  dmtrans creates this concept as a specific episode of 'atperson-loc'. at the same time  since academic-conference'  activated by 'ijcai-1'  is a contexual-root concept it sends c-markers to 'person-present-thesis'  'person-criticize-thesis'  
'thesis'  'proceedings'  etc.. when the next word  he  comes in  it sends activation upward and finds that the only male per-
1
　　we use the term 'concept sequence' to represent some known sequence of concepts such as  feature  physical-object which includes sequence of abstract concepts as in mop components and also low level phrasal templates such as described by becker  wilensky  and hovy. 
   1 root concept is a concept that packages another concept in a structure  ie  mop in dmtrans. verbs in a case-frame based lexicon are comparable structures. 
　 this called 'concept refinement'. lytinendiscusses a rule-based version of this scheme. 
　 concepts that receive the c-marker include: participants of a mop  concepts representing events  explanation-patterns attached to a mop. 
1
   concept refinement in dmtrans is performed as a search for a node that packages the input recognized concept with links parallel to the links from the accepted root node to the elements of the accepted concept sequence. 
	tomabtchl 	1 

'printer' respectively1. however  since   ink  and  printer  both come after  ijcai-1  in both cases  'paper' is prefered over 'thesis' in both cases  and it gets activated. unless these activations meet contradicting hypotheses elsewhere  'paper' becomes the contextual interpretation of  paper . 
b. 	explanatory generation 
we have two different concept sequences stored in each root concept  one for english and one for japanese. especially because they represent texts from different language families  the sequences are rarely the same; however  the roles are shared  it is because memory structures are independent of languages and the types of roles are inherent in the root concepts  not in the languages. similar approaches are taken in lytinenf 1 's moptrans and cmu's current mt system. both systems take advantage of shared memory structures for translation  the former using mops as the shared structure and the latter using case frames as the shared structure. 
　　generation begins with the result of memory activation parsing from input in one language. for each concept refined nodes left in memory  we do the following. 1  check at the lexical node for the refined concept in the target language and if a lexical entry is found  generate in accordance with templates stored with the concept and we are done. 1  if not  which is the often the case   we generate according to the stored concept sequence for the target language. that is to generate from the first element of the concept sequences  go back to 1 with the first element of the concept sequence . 1  since not all concepts have sequence attached to it  search the abstraction hierarchy upward for abstraction of the refined concept which has concept sequences attached to it. 1  get the sequence from this abstraction and then instantiate with the roles in the refined-concepts. then from the first element of the instantiated concepts sequence  repeat from looking up lexical node again. if not found  repeat from the 1 again to explain this concept. 


 he said the quality of the paper is terrible.** 
figure 1: concept refinement and context marking 
son activated in memory is 'john'  and activates 'john' again; 'person' gets re-activated  which is predicted as the first element of 'mtrans-event  then  said  comes in and fits as the second element of the concept sequence attached to 'mtransevent  likewise   the quality of this paper  is accepted  being identified with the sequence  feature-type of object is featurevalue  attached to 'object-description'. one thing that happens is that when  paper  which is attached both to  paper' and  thesis' comes in  only 'thesis' sends activation upward because 'thesis' was c-marked by 'academic-conference' and 'paper' was not marked. this choice is not challenged when 'mtransevent* is accepted and is concept-refined to 'person-criticizethesis-event'  since this concept also supports the contexual interpretation of  paper 1. this way  understanding is left as activated memory structures representing 'at-john-ijcai-1' and 'john-criticize-quality-of-thesis-event* that are instances of the refined concepts under accepted root concepts. 
　　also  if two conflicting choices of a concept are marked by two c-markets. the c-marker put by the concept activated more recently gets preference. for example  in  john was writing a letter on a plane to ijcai-1. the ink smeard. he said the quality of this paper is terrible  and in  john was printing a paper for ijcai-1. the printer jammed he said the quality of this paper is terrible   both 'paper' and 'thesis' are c-marked by and 'ink'  'ijcai-1' and 
 1c-marked by the same contexual root concept as 'thesis*. 
1actually. c-marked by 'academic-conference* which was activated by 
1 	natural language 
　　one sample short translation is translating the japanese sentence:  gionshoja no kane no koe  shogyomujo no hibiki ari  which is translated to be:  the sound of the bell at gionshoja has the tone of 'shogyomujo'  impermanence of all phenomena in the world  . note that dmtrans outputs 'shogyomujo' as 'shogyomujo'  and adds the explanation of the word in parentheses. this is because an english lexical entry for the concept representing 'shogyomujo' was not found in memory. since a concept may not be shared across languages  this type of explanation happens often  especially in a cross-cultural context1. if a lexical entry for the target language is not found  most mt systems simply halt excution. in contrast  dmtrans outputs the explanation of the concept in the target language. this is possible through the explanatory generation mechanism described above. explanation is performed by generation using the surrounding concepts with lexical entries in the target languages. 
'ucai-1'. l1theae three concepts trigger  activate  contexual-root concepts. 
　 this is the inherent uniqueness of the dmtrans system  that the system does not halt even if the lexical entry is not found in the target language; instead dmtrans tries to explain the concept through surrounding concepts in the memory network that have lexical entries in the target language. 
     1   the described explanatory generation mechanism works effectively in translation between english and japanese  where a one to one match of concepts is often difficult to find due to the difference in the cultural contexts. even words such as  river  and  kawa   japanese for river  which are normally substituted for one another without any further consideration  reveal difference in concepts attached to them  ie  the japanese word  kawa  is normally associated with images of clear rapid streams. what about  kou  in chinese  

c. dynamic interactions with the rest of cognition 
since translation is performed by directly accessing the memory network  other faculties of cognition can dynamically participate in translation. one example sentence here is  john threw an apple at the giant rat it ate it . whenever  a pronoun comes in as an input  dmtrans tries to identify the object that is referred to1. in this example  the concept 'animal-ingest-object-event gets activated by the input  it ate it   'animal-ingest-object-event* is a mop structure which is a kind of 'ingest-event' it has two roles to be filled: actor and object. in order to determine the actor  the inference mechanism is activated and it looks for activated concepts in memory that can be an actor and finds 'giant-rat' to be a 
candidate given restrictions set forth by the mop structure1. then a search is made for concepts previously activated in memory that fit the requirements for objects and 'apple' is selected to be an acceptable object of 'ingest-event  this example only requires a minimum amount of work for deciding objects; however  this architecture allows for deeper inferences if necessary  such as utilizing causal relations stored in mops and explanation patterns associated with higher level structures  schank 1. 
d. 	a translation system that learns 
dmtrans is capable of creating new concepts while translating  and is capable of learning new vocabulary for newly created concepts in a multi-lingual context. when a concept refinement is performed  if a specific concept representing the input sentence is not found underneath the accepted root concept  a new specialization is created. also  the user of the system is asked to input the english and japanese names  words  for the concept  or input phrase can simply be stored as a phrasal lexicon . by the same token  we can simply assert facts to be translated by dmtrans and the system stores the assertion as well as it translates it as long as it is not incompatible with what it already knows. at the same time  the acquired concept is accessible from different contexts because of the hierarchical organization of memory  schank  that implements mop structures. this way dmtrans implements dynamic memory as its memory network and is capable of learning while translating. 
	iv. 	conclusion 
from a practical point of view  dmtrans may be interesting because a lexically guided spreading activation mechanism is parallel in nature  and recent availability of massively parallel machines makes it an appealing theory for machine translation  utilizing such parallel architectures. however  the impact of this theory is that translation is performed as an integrated part of cognition  cooperating with other faculties through memory. most mt systems have failed in tackling contexually ambiguous sentences; however  in dmtrans  with use of episodic and thematic memory  and also the c-marker passing  performance with ambiguous sentences is significantly improved. 
explanatory generation handles culturally sensitive translations more effectively  especially when lexical entries in the target language are not available. also  the dynamic participation of an inference mechanism contributes in handling phenomena such as anaphora  ellipsis  and indirect speech acts. a future possibility is that we may supplement dmtrans with other input output channels to make the system's abilities closer to those of human interpreters in handling questions of pragmatics. in our understanding  memory is shared by all parts of cognition  and any cognitive task including translation should be dynamically assisted by every faculty with direct access to the memory. 
acknowledgments 
thanks are due to members of the project ivy  ie  
lawrence hunter and alain rostain of yale ai project and dr. jerry silbert of west haven veterans administration hospital. dr. christopher riesbeck motivated me in the dma paradigm. also  many thanks to members of cmt especially to eric nyberg for valuable comments  and to professor jaim carbonell and dr. masaru tomita who are providing me an excellent opportunity to devote myself in machine translation research at the center for machine translation. 
