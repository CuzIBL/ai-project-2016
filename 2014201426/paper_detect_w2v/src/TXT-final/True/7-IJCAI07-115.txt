
jean is a model of early cognitive development based loosely on piaget's theory of sensori-motor and pre-operational thought. like an infant  jean repeatedly executes schemas  gradually transferring them to new situations and extending them as necessary to accommodate new experiences. we model this process of accommodation with the experimental state splitting  ess  algorithm. ess learns elementary action schemas  which comprise controllers and maps of the expected dynamics of executing controllers in different conditions. ess also learns compositions of action schemas called gists. we present tests of the ess algorithm in three transfer learning experiments  in which jean transfers learned gists to new situations in a real time strategy military simulator.
1 introduction
one goal of the jean project is to develop representations and learning methods that are based on theories of human cognitive development  particularly piaget's theories of sensorimotor and pre-operational thought. piaget argued that infants acquire knowledge of the world by repeatedly executing action-producing schemas  piaget  1 . this activity was assumed to be innately rewarding. piaget introduced assimilation of new experience into extant schemas and accommodation of schemas to experiences that don't quite  fit  as the principal learning methods for infants. this paper gives a single computational account of both assimilation and accommodation.
　although it seems to be a relatively recent focus in machine learning  transfer of learned knowledge from one situation or scenario to another is an old idea in psychology and is fundamental to piaget's account of cognitive development. this paper demonstrates that the schemas learned by jean can be transferred between situations  as any piagetian schema should be.
　jean learns action schemas and gists. an action schema comprises a controller  a representation of the dynamics of executing the controller  and one or more criteria for stopping executing the controller. gists are compositions of action schemas for common tasks; for instance  push involves the sequence move-to  contact and apply-force. jean's learning method is a kind of state splitting in which state descriptions are iteratively refined  split  to make the transitions between states as predictable as possible  giving jean progressively more control over the outcomes of actions.
　the following sections introduce jean's action schemas  sec. 1   and then establish correspondences between action schemas and the more familiar states and finite-state machines  sec. 1 . the state splitting algorithm is described in section 1. its application to transfer learning and empirical results with a small-unit tactical military simulator are presented in sections 1 and 1  respectively.
1 action schema components
action schemas have three components: controllers  maps  and decision regions. controllers control jean's behavior. for example  the  move-to jean obj  controller moves jean from its current location to the specified object. jean has very few innate controllers - move-to  turn  rest  apply force. it learns to assemble controllers into larger plan-like structures called gists as described in section 1.
　as jean moves  or executes any other controller  certain variables change their values; for instance  the distance between jean's current location and obj usually decreases when jean executes the move-to controller. similarly  jean's velocity will typically ramp up  remain at a roughly constant level  then ramp down as jean moves to a location. the values of these variables ground out in jean's sensors  although some variables correspond to processed rather than raw sensory information.
　these variables serve as the dimensions of maps. each execution of a particular schema produces a trajectory through a map - a point that moves  moment by moment  through a space defined by distance and velocity  or other bundles of variables. each map is defined by the variables it tracks  and different maps will be relevant to different controllers.
　each invocation of a controller creates one trajectory through the corresponding map  so multiple invocations will create multiple trajectories. figure 1 shows several such trajectories for a map of distance for the move-to controller. one can see that move-to has a  typical  trajectory  which might be obtained by taking the mean of the trajectories in the map. also  when operating in jean's environment  move-to produces trajectories that lie within some distance of the mean trajectory. in a manner reminiscent of quality control jean can assess whether a particular trajectory is  going out of bounds. 
　the idea that sensori-motor and pre-operational development should rely on building and splitting maps or related dynamical representations was anticipated by thelen and smith  thelen and smith  1  and has been explored by other researchers in developmental robotics and psychology  e.g.  removed-for-blind-review  siskind  1; barsalou  1; mandler  1  .

figure 1: trajectories through a map of distance between jean and a simulated cat. sometimes  when jean gets close to the cat  the cat moves away and the distance between them increases  in which case a trajectory may go  out of bounds. 

figure 1: this schematic of a map  for the  move-to jean obj  controller  has one decision region associated with the distance between jean and the object obj being zero. the other decision region bounds the area in which jean cannot reach loc even moving at maximum speed.
　every map has one or more decision regions within which jean may decide to switch from one controller to another. one kind of decision region corresponds with achieving a goal; for example  there is a decision region of the move-to map in which distance to the desired location is effectively zero  e.g.  the thin  horizontal grey region in fig. 1 . another kind of decision region corresponds to being unable to achieve a goal; for instance  there is a region of a timedistance map from which jean cannot move to a desired location by a desired time without exceeding some maximum velocity  e..g.  the inverted wedge-shaped region in fig. 1 .
these regions are sometimes called envelopes   gardiol and
kaelbling  1  and removed-for-blind-review
　jean is not the only agent in its environment and some maps describe how relationships between jean and other agents change. the upper two panels of figure 1 illustrates how distance  relative velocity  heading  and contact change in an environment that includes jean and another agent  called the  cat   an automaton that moves away from jean if jean moves too close  too quickly.
1 action schemas as finite state machines
it will help to draw parallels between jean's maps and the more familiar elements of finite state machines  fsms . conventionally  states in fsms represent static configurations  e.g.  the cat is asleep in the corner  and arcs between states represent actions  e.g.   move-to jean cat  . for us  arcs correspond to the intervals during which jean executes controllers  i.e.  actions   and states correspond to decision regions of maps. that is  the elements of action schemas are divided into the intervals during which a controller is  in bounds   the arcs in fsms  and the intervals during which jean is thinking about what to do next  the states . both take time  and so require a rethink of the conventional view that states in fsms persist and transitions over arcs are instantaneous. however  the probabilistic semantics of fsms are retained: a controller invoked from a decision region  i.e  an action invoked in a state  will generally take jean into one of several decision regions  i.e.  states   each with some probability. figure 1 redraws figure 1 as a finite state machine. starting from a decision region of some action schema a  the move-to controller will with some probability  say .1  drop jean in the decision region associated with achieving its goal  and  with the complementary probability  in the region associated with being unable to achieve the goal in time.

figure 1: the action schema from figure 1 redrawn as a finite state machine in which arcs correspond to the execution of controllers and states correspond to decision regions.
　there is one special case: sometimes the world changes when jean is doing nothing. we model this as a schema in which no controller is specified  called a dynamic schema to distinguish it from an action schema. dynamic schemas do have maps  because variables such as distance between jean and another agent can change even when jean does nothing; and they have decision regions  because jean may want to invoke a controller when these variables take particular values  e.g.  moving away when another agent gets too close . the fsms that correspond to dynamic schemas have no controller names associated with arcs but are otherwise as shown in figure 1.
1 learning action schemas and gists
jean has a small set of innate controllers and a few  empty  maps  and it  fills in  these maps with trajectories and learns decision regions. jean also learns compositions of action schemas called gists for their story-like or plan-like structure. one principle underlies how jean learns decision regions. it is to maximize predictability  or minimize the entropy of  what's next.  within an action schema  what's next is a location in a map. at the boundaries of action schemas  that is  in decision regions  or states  what's next is the next state. we call the entropy of what's next the boundary entropy. jean learns decision regions that minimize boundary entropy between states. interestingly  this criterion tends to minimize boundary entropy within maps as a side effect.
1 experimental state splitting algorithm
we give a formal outline of the experimental state splitting  ess  algorithm in this section. jean receives a vector of features ft = {f1 ... fn} from the environment at every time tick t. some features will be map variables  others will be inputs that have not yet been associated with maps. jean is initialized with a goal state sg and a non-goal state s1. st is the entire state space at time t. a is the set of all controllers  and a s    a are the controllers that are executed in state s （ s. typically a s  should be much smaller than a. h si aj  is the boundary entropy of the state si in which controller ai is executed. a small boundary entropy corresponds to a situation where executing controller aj from state si is highly predictive of the next observed state. finally  p si aj sk  is the probability that executing controller aj in state si will lead to state sk.
　for simplicity  we will focus on the version of ess that only splits states; an alternative version of ess is also capable of learning specializations of parameterized controllers. the ess algorithm follows:
  initialize state space with two states  s1 = {s1 sg}.
  while -optimal policy not found:
- gather experience for some time interval τ to estimate the transition probabilities p si aj sk .
- find a schema feature f （ f  a threshold θ （ Θ  and a state si （ s to split that maximizes the boundary entropy score re-
1	1 result from splitting si using feature f and thresh-
and sk1 = {s （
	i	.
- split si （ st into sk1 and sk1  and replace si with new states in st+1.
- re-solve for optimal plan according to p and st+1
　finding a feature f and the value on which to split states is equivalent to finding a decision region to bound a map.
　without heuristics to reduce the effort  the splitting procedure would iterate through all state-controller pairs  all features f （ f  and all possible thresholds in Θ  and test each such potential split by calculating a reduction in boundary entropy. this is clearly an expensive procedure.
　ess uses a simple heuristic to find threshold values for features f and  thus to split a state: states change when several state variables change more or less simultaneously. this heuristic is illustrated in figure 1. the upper two graphs show time series of five state variables: headings for jean and the cat  in radians   distance between jean and the cat  and their respective velocities. the bottom graph shows the number of state variables that change value  by a set amount  at each tick. when the number of state variables that change simultaneously exceeds a threshold  jean concludes that the state has changed. the value of the schema f at the moment of the state change is likely to be a good threshold for splitting f. for example  between time period 1 and 1  jean is approaching the cat  and the heuristic identifies this period as one state. then  at time period 1  several indicators change at once  and the heuristic indicates jean is in a new state  one that corresponds to the cat moving away from jean.

figure 1: new states are indicated when multiple state variables change simultaneously.
1 transferring learning
although ess can learn action schemas and gists for new situations from scratch  we are much more interested in how previously learned policies can accommodate or transfer to new situations. gists capture the most relevant states and actions for accomplishing past goals. it follows that gists may be transferred to situations where jean has similar goals and the conditions in the situation are similar.
　the version of ess that we described above is easily modified to facilitate one sort of transfer: after each split we remove the transition probabilities on all action transitions between each state. this allows the state machine to accommodate new experience while maintaining much of the structure of the machine  see removed-for-blind-review for a previous example of this idea . in the experiments in the next section we explore the effects of transfer using this mechanism in several conditions.
1 experiments
to measure transfer we adopt a protocol sometimes called b/ab: in the b condition the learner learns to perform some tasks in situation or scenario b. in the ab condition  the learner first learns to perform tasks in situation or context a and then in b. by comparing performance in situation b in the two conditions after different amounts of learning in situation b one can estimate the effect of learning in a and thus the knowledge transferred from situation a to situation b. for instance  a might be tennis and b squash  and the b/ab protocol compares learning curves for squash alone  the b or control condition  with learning curves for squash after having learned to play tennis  the ab or transfer condition . we say positive transfer has occurred between a and b if the learning curves for squash in the transfer condition are better than those in the control condition.
　in general  better  learning curves means faster-rising curves  indicating faster learning   or a higher value after some amount of learning  indicating better performance   or higher value after no learning  indicating that knowledge from a helps in performing b even before any learning has occurred in b . in our experiments  better learning performance means less time to learn a gist to perform a task at a criterion level. thus  a smaller area beneath the learning curve indicates better learning performance  and we compare conditions b and ab by comparing the area beneath the learning curves in the respective conditions. a bootstraprandomization procedure  described shortly  is used for significance testing.
　we tested jean's transfer of gists between situations in the 1-d real time strategy game platform isis. isis can be configured to simulate a wide variety of military scenarios with parameters for specifying different terrain types  unit types  a variety of weapon types  and multiple levels of unit control  from individual soldiers to squad-level formations .
　in each of three experiments  jean controlled a single squad at the squad level  with another squad controlled by an automated but non-learning opponent. jean's squad ranged in size from 1 to 1 units while the opponent force ranged from 1 units. although the opponent was smaller  it could move faster than jean's forces. in each experiment  jean's goal is to move its units to engage and kill the opponent force.
　jean is provided four innate action schemas: run  crawl  move-lateral  and stop-and-fire. it must learn to compose these into gists that are appropriate for different engagement ranges  possible entrenchment of the opponent  and some terrain features  mountains .
　all experiment scenarios were governed by a model of engagement ranges that determined how the squads interact and how the opponent controller would respond to jean's actions. engagement ranges are defined as follows:
iv. outer range  beyond 1 meters : as long as the opponent is within line of sight  i.e.  not obscured by terrain features   no matter what the distance  jean can locate the opponent. however  beyond 1 meters  the opponent cannot see jean's forces.
iii. visual contact  up to 1 meters : at this range  if jean's forces are standing they will be sighted by the opponent. if jean's forces are crawling  and they have not begun firing  then jean's forces won't be sighted  until they are within range i .
ii. firing range  up to 1 meters : within this range  either force can fire on the other. once jean's forces have fired  they are considered sighted  even if crawling  and the opponent can return fire with the same effectiveness as jean's forces.
i. full contact  up to 1 meters : at this range  even if jean's forces are crawling  they will be sighted by the opponent. direct fire has full effect.
　each experiment had a transfer condition ab and a control condition  b. in the former  jean learned gists to accomplish its goal in a scenario designated a and then learned to accomplish its goal in scenario b. in the latter  control condition  jean tried to learn in scenario b without benefit of learning in a.
the experiments differ in their a and b scenarios:
experiment 1 : all action takes place in open terrain. a scenarios all have jean's forces starting near enemy forces. b scenarios are an equal mix of starting near the enemy or far away from the enemy.
experiment 1 : all action takes place in open terrain. a scenarios all have jean's forces starting far from the enemy forces. b scenarios are an equal mix of starting near the enemy or far away from the enemy.
experiment 1 : the terrain for a scenarios is open  whereas the terrain for b scenarios has a mountain that  for some placements of jean's and the enemy's forces  prevents them seeing each other.  the advantage goes to jean  however  because jean knows the location of the enemy forces.  the a scenario is an equal mix of starting near or far from the enemy  the b scenario is an equal mix of starting near and far from the enemy in the mountain terrain.
1 metrics and analysis
we plot the performance of the jean system in the various experimental scenarios as learning curves over training trials. better learning performance is indicated by a smaller number of training instances required by jean to achieve a criterion level of performance. thus  a smaller area beneath the learning curve indicates better learning.
　given n learning curves for the b and ab conditions  we test the null hypothesis of  no transfer  as follows: let b and ab denote the sets of n learning curves in the b and ab conditions  respectively  x be the mean learning curve for a set of learning curves x  and area x  be the area under learning curve x. we measure the benefit of transfer learning with the transfer ratio
.
values greater than one indicate that the area under the learning curves in the control condition b is larger than in the transfer condition  or a positive benefit of transfer.
　the null hypothesis is r = 1  that is  learning proceeds at the same rate in the control and transfer conditions. to test whether a particular value of r is significantly different from 1 we require a sampling distribution for r under the null hypothesis. this is provided by a randomization-bootstrap procedure  cohen  1 . first  we combine the sets of learning curves  c = b “ ab. then  we randomly sample  with replacement  n curves from c and call this a psuedosample b ; and again randomly sample  with replacement  another n curves from c to get pseudosample ab . then  we compute  and store its value in the sampling distribution of r. repeating this process a few hundred times provides an estimate of the sampling distribution. for one-tailed tests  the p-value is simply the proportion of the sampling distribution with values greater than or less than r  depending on the direction of the one-tailed alternative hypothesis . because we expect learning rates in the transfer condition to be higher  our alternative hypothesis is r   1 and the p value is the proportion of the sampling distribution with values greater than r.  n.b.: when there is a possibility of negative transfer  in which the knowledge acquired in a actually impedes learning in b  it might be more appropriate to run two-tailed tests. 
　to obtain a confidence interval for r  as opposed to a p value  we again construct a sampling distribution  but this time  instead of first combining the two sets b and ab  we sample b  with replacement from b  and ab  similarly from ab . then we calculate. repeating this process yields a sampling distribution for r. a 1% confidence interval around r is the interval between the 1% and 1% quantiles of this distribution  cohen  1 .
1 results
let us start with a qualitative assessment of what jean learned. in experiment 1  jean learned in scenario a to run at the enemy and kill them. in scenario b  jean learned a gist that included a conditional: when starting near the enemy  use the gist from scenario a  but when starting far from the enemy crawl - don't run - until one is near the enemy and then use the gist from scenario a. the alternative  running at the enemy from a far starting location  alerts the enemy and causes them to run away. state splitting did what it was supposed to do: initially  jean's gist for scenario b was its gist for a  so jean would always run at the enemy  regardless of starting location. but through the action of state splitting  jean eventually learned to split the state in which it ran at the enemy into a run-at and a crawl-toward state  and it successfully identified the decision region for each. for instance  the decision region for the crawl-toward state identifies a distance  corresponding to being near the enemy   from which jean makes a transition to the state in which it runs at and shoots the enemy.
　similar results are obtained in experiment 1  where jean learns to run at the enemy from a far starting location as long as the mountain prevents the enemy from sighting jean  otherwise to crawl.
　in experiment 1  jean learned nothing in scenario a and was no more successful in scenario b. this is due to the difficulty of the scenario. jean is always initialized far away from the enemy units  and must learn a policy for killing them by exploring a continuous  high-dimensional feature space using her four available actions. many of these actions result in the enemy soldiers detecting jean's presence and running away  thus reducing jean's chances of ever reaching her goal by simple exploration. since jean does not learn anything useful in the a scenario  her performance in the ab transfer condition is no better than in the control condition b.
　the learning curves for the a and b scenarios in experiment 1 are shown in figure 1. note that the vertical axis is  time to achieve the goal   in the scenarios  so a downward-sloping curve corresponds to good learning performance. jean learned a good policy in the a scenario where the enemy units are initially close to jean's position. jean receives a significant benefit when it learns in the b scenario after having learned in the a scenario  i.e.  in the transfer condition ab . the ab learning curve starts out immediately with much better performance than the b learning curve. in fact  it takes 1 trials for learning in the b condition  in which no transfer happens  to reach the level of performance observed at all levels of training in the transfer condition ab. however  the ab curve is roughly flat  which means learning in scenario a provides a boost in performance in b but has no impact on the rate of learning in b. we rejected the null hypothesis that b and ab have the same mean learning curve with a p value of 1; the ab condition has significantly better learning curves.

figure 1: learning curves for learning in scenario a  in scenario b  and in scenario b after having first learned in scenario a. each point is averaged over eight replications of the experiment. error bars are two standard deviations wide. each point of each curve is the average of ten fixed test trials. test trials are conducted every 1 training trials. the x-axis plots the number of training trials that the agent has completed.
　in contrast  in experiment 1 jean does not succeed in achieving any transfer. jean does not ever learn anything useful in the a condition because  as noted above  when jean starts far from the enemy the search space of possible gists is too large. the transfer ratio r = .1 is nearly equal to the expected value under the null hypothesis of no transfer  and the p value is accordingly high  p = .1.
　in experiment 1  jean transfers learned knowledge from the a scenario  which involves open terrain to  jump-start  performance in the b scenario  which includes a mountain.

figure 1: experiment 1. learning curves for the a  ab  and b conditions  averaged over eight replications of the experiment. the x-axis plots the number of training trials that jean has completed.
rp1% quantile1% quantileexpt. 1.1.1.1.1expt. 1.1.1.1.1expt. 1.1.1.1.1table 1: the transfer ratio r  the p value  and lower and upper bounds of a 1% confidence interval around r in all three experiments.
figure 1 shows the average learning curves we observe in this experiment. note the wide error bars. while learning in the transfer condition is significantly more successful than in the control condition  r = 1 p = 1  the data do not support the conclusion that jean's learning in the b scenario is accelerated by learning in a. as in experiment 1  it appears that the curve for learning in b after a is quite flat and the benefit of knowledge learned in a is felt immediately in domain b.
　table 1 summarizes the data we observed from the three experiments and provides confidence intervals for the transfer ratios.
1 discussion
the experimental state splitting algorithm splits an undifferentiated gist comprising just a start state and goal state into a sequence of states that follow each other with high predicability. to do this  ess finds new decision regions for action schemas  that is  it finds values of map variables that indicate jean should switch from one action schema to another. ess often reduces the entropy within action schemas  that is  it reduces the entropy of trajectories in maps and  tightens up  its expectations of the dynamics associated with executing a controller. this is because trajectories lead to decision regions and wildly varying trajectories lead to large numbers of decision regions  and  thus  to highly entropic distributions of next states. splitting states to reduce the entropy of the distributions of next states is equivalent to reducing the number of decision regions associated with a state and thus the variability of trajectories in a map.
　interestingly  this hints at an unexplored aspect of the relationship between the piagetian learning mechanisms of assimilation and accommodation. assimilation means incorporating experiences into known schemas  which  for jean  means adding trajectories to the maps of action schemas. accommodation means modifying schemas when experiences don't fit. for jean  this means state splitting. we have learned that the impetus for accommodation  and thus for maintaining narrow bounds on what's assimilated  is a desire to predict the next state.
　we also have preliminary evidence that gists can transfer between situations by a relatively simple mechanism  namely  keeping the structure of a gist's fsm but deleting its transition probabilities.
　much work remains to be done. further transfer experiments are already underway. better heuristics for proposing states to split and feature values as splitting thresholds are being developed. perhaps most challenging  we want ess to be a truly experimental kind of state splitting  an algorithm that proposes and carries out experiments to assess the causal influences of features on map trajectories.
