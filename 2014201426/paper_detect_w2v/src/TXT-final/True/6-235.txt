 
distance-based methods in pattern recognition and machine learning have to rely on a similarity or dissimilarity measure between patterns in the input space. for many applications  euclidean distance in the input space is not a good choice and hence more complicated distance metrics have to be used. in this paper  we propose a parametric method for metric learning based on class label information. we first define a dissimilarity measure that can be proved to be metric. it has the favorable property that between-class dissimilarity is always larger than within-class dissimilarity. we then perform parametric learning to find a regression mapping from the input space to a feature space  such that the dissimilarity between patterns in the input space is approximated by the euclidean distance between points in the feature space. parametric learning is performed using the iterative majorization algorithm. experimental results on realworld benchmark data sets show that this approach is promising. 
1 introduction 
the notion of similarity or dissimilarity plays a fundamental role in pattern recognition and machine learning. a promising direction to pursue is to learn good  dis similarity measures from data. recently  learning distance metrics from data has aroused a great deal of interest from machine learning researchers. one typically wants to embed patterns in a  possibly non-metric  input space into a feature space  in which the euclidean distance between points accurately reflects the dissimilarity between the corresponding patterns. therefore the  linear or nonlinear  mapping from the input space to the feature space corresponds to feature extraction. alternatively  the feature space may be a low-dimensional space for data visualization. 
　in this paper  we propose a parametric distance metric learning method in the supervised setting. the main ideas 
   this research has been partially supported by the research grants council of the hong kong special administrative region under grants dag1.eg1  hkust1e and hkust1e. 
of our method are summarized as follows. using class label information  we define a similarity measure  and hence also the corresponding dissimilarity measure  between patterns in the input space. the dissimilarity measure implicitly induces a metric space for embedding the original patterns. to explicitly represent the mapping from the input space to the feature space  we then approximate the mapping by a regression model to embed the original patterns in an euclidean space. the regression parameters are estimated from data with the objective that the dissimilarity between patterns in the input space is approximated by the euclidean distance between points in the feature space. once the regression model has been found  any new pattern can be mapped to its corresponding location in the feature space. distance-based methods  such as k-means clustering  nearest neighbor classifiers and support vector machines  can then be applied in the feature space for clustering or classification applications. 
　the rest of this paper is organized as follows. a modified metric incorporating class label information is proposed in section 1. section 1 outlines our regression model for metric learning and the corresponding optimization method. experimental results are presented in section 1  and the last section gives some concluding remarks. 
1 modified metric with label information 
denote the input space by rq and the set of all c possible class  target  labels by t. a training set has n patterns where u = r if pattern i belongs to class r. here  each pattern is assumed to belong to only one class. in general  a number of similarity measures can be defined on these patterns  gower and legendre  1 . in this paper  we utilize also the label information in defining the similarity sij between patterns xi and xj: 
 1  
where 	denotes the euclidean norm and b   1 is a width parameter. the corresponding dissimilarity sij is then: 
		 1  

1 	poster papers 

　as illustrated in figure 1  this  dis similarity measure enjoys some nice properties for pattern discrimination. for example  the  dis similarity between any two patterns in the same class is always larger  smaller  than that between any two patterns belonging to different classes. moreover  the larger the euclidean distance between the patterns is  the smaller is the within-class similarity while the larger is the between-class similarity. 
　in recent years  finite metric spaces and their embeddings have received much attention  indyk  1; linial et al  1 . among embedding into normed spaces  embedding into an euclidean space is the most popular. 
　given the dissimilarity matrix  we are interested in the question of whether and how the dissimilarity matrix can be embedded. in other words  for the original points we attempt to find a configuration of points in some euclidean space  such that the squared distances between these points will be equal to 
the so-defined dissimilarities  
	the following theorem confirms that 	can be embedded. 
theorem 1 define d ij ~ with as in  1 . the matrix d - is metric. in other words  satisfies the following properties: 

the proof of this theorem can be found in izhang et al  
1 . the subsequent task is then to find the embedding  i.e.  points such that the inter-point distance is equal to dij in general  obtaining an exact solution for these xi's is difficult. nevertheless  because d is metric  an approximate solution can be easily obtained by using principal coordinate analysis or other multidimensional scaling  mds  methods  cox and cox  1 . we will return to this problem in section 1. 
　notice that the resultant euclidean embedding will still incorporate information from both the input space representation  xi  of the patterns and their corresponding class labels  tl . moreover  the distance metric dtj  like the associated enjoys those nice properties useful for pattern discrimination. 
1 metric learning with regression model 
as mentioned in section 1  an approximate solution for  can be obtained by using mds. however  this 
may be intractable for large data sets. moreover  for new patterns with unknown labels  the problem then is on how to determine sl1 in the first place. following  koontz and fukunaga  1; cox and ferry  1; webb  1   we attempt to find a mapping from x  in the original input space to in the embedded euclidean space one possibility is to first obtain a mds configuration  and then construct a regresicox and ferry  1 . however  
this mapping is not determined as part of the mds procedure  webb  1 . in the following  we will follow the approach of  webb  1. 
denote the mapping from the original input space to the by f -   f 1   . . .   fi '. assume 
 1  
where 	let x be the target con-
                                       is defined in  1 . using the iterative majorization algorithm  we then minimize the squared error 

1 experiments 
in this section  we perform experiments on six benchmark data sets  table 1  from the uci repository  murphy and aha  1 . the distance metric is learned using a small subset of the labeled patterns  with / = p = q   = x and the width  in  1  set to the average distance of the labeled patterns to the class means. the remaining patterns are then used for testing. 
　table 1 shows the classification results by the nearest mean and nearest neighbor classifiers  with both the euclidean and learned metrics. as can be seen  the learned metric almost always outperforms the original metric. 
　next  we perform clustering experiments using the a;means clustering algorithm  with the value of k set to the true number of clusters in each data set. the clustered patterns are assigned labels and the clustering accuracy is measured by comparing these labels with the true labels  as in classification problems . as these cluster labels can be permuted 

poster papers 	1 

table 1: the six uci data sets used in the experiments. 
data set total # of patterns  # patterns for metric learning class size pima indians diabetes  diabetes  1 
1 1 
1 1 
1 soybean 1 
1 
1 
1 1 
1 
1 1 
1 
1 
1 wine 1 
1 
1 1 
1 1 
1 wisconsin breast cancer  wbc  1 
1 1 1 
1 ionosphere 1 
1 1 1 
1 iris 1 
1 
1 1 
1 
1 1 
1 
1 without changing the clustering solution  results reported here arc based on the labeling with the highest clustering accuracy. as can be seen from table 1  the learned metric outperforms that with the original metric on all data sets. 
table 1: classification accuracies on the uci data sets  numbers in bold indicate the better results . 
data set 1 	nearest mean | 	nearest neighbor 　euclidean 1 	metric learned 
metric 　euclidean 1 	metric learned 
metric diabetes 1 1 1 1 	1 1 soybean 1 1 1 1 wine 1 1 1 1 wbc 1 1 1 1 ionosphere 1 1 1 1 iris 1 1 1 1 1 concluding remarks 
in this paper  we proposed a new parametric method for distance metric learning based on class label information. experiments on uci data sets show promising results. 
the current work can be extended in several directions. 
first  nonlinear basis functions can be used to improve the approximation power of the regression mapping. second  although theorem 1 states that the dissimilarity measure induces a metric  it is not clear whether the matrix is also euclidean. if this is the case  a new kernel can then be defined on the joint space of the input space and class label space  scholkopf  1 . third  in addition to using label information  we will also incorporate manifold structure between neighboring patterns into our metric learning process. 
1 
table 1: clustering accuracies on the uci data sets  numbers in bold indicate the better results . 
data set euclidean metric learned metric diabetes 1 1 soybean 1 1 wine 1 1 wbc 1 1 ionosphere 1 1 iris 1 1 