 
　　　　a new parsing algorithm is described. it is intended for use with advice-taking  or augmented  phrase structure grammars of the type used by woods  simmons. heidorn and the author. it has the property that it is guaranteed not to propose a phrase unless there exists a continuation of the sentence seen thus far  in which the phrase plays a role in some surface structure of that sentence. the context in which this algorithm constitutes a contribution to current issues in parsing methodology is discussed  and we present a case for reversing the current trend to ever more complex control structures in natural language systems. 
	l 	i n t r o d u c t i o n 
　　　lingol  linguistics oriented programming language  is a language to facilitate the writing of natural language processing programs. in a previous paper  pratt 1 the lingol system was described  examples of output from several small-scale lingol programs were given  and the reader was led through a console session as a trivial-scale french translator was developed. that paper sought to establish lincol's credentials as a pedagogical device for newcomers to the art of writing natural language  front ends . 
　　　lingol was originally conceived as a language intended for use by serious researchers due to its author's preoccupation with more mathematical pursuits during the past few years  lingol has not been exercised until recently with anything but small-scale student-generated programs since the appearance of  pratt 1   several more such small-scale lingol programs have been written. more recently  the author has begun work on a large-scale program to see whether lingol really can be used as the research tool it was originally designed to be  without compromising those features that made it attractive to beginners  ease of use and low resource consumption  in this paper we concentrate on the implementation of the current version of lingol. this discussion will complement the one in  pr.tt 1   which concerned the motivation for the lingol language  that part of the lingol system accessible to the user. primarily  we shall present a new context-free parsing algorithm that  paradoxically perhaps  is responsible for the efficiency of the very context-dependent parser in lincol's cognitive component. the idea behind this algorithm is one that may be of value in other structure-eliciting type problems besides parsing 
　　　an issue that still seems to haunt computational linguistics is that of the syntax/semantics dichotomy. quite clearly  lingol takes approximately the same point of view as  woods 1   that surface structure is worth eliciting  and that context-free grammars  or their transition-net equivalent  can play a non-trivial role in parsing. in section 1 we discus a relatively new 
perspective on this issue which puts this style of parsing in a more favorable liight than people have been willing to view it of late  riesbeck 1  marcus 1 
1     t h e   c u r r e n t l l g o l . l m p l x m x n t a t 1 q k 
1-g eneral qverview 
　　　we have already discussed elsewhere  pratt 1  the rationale for those features of lingol available to the user. in this section we shall talk about what goes on behind the scenes the distinction being drawn here is exactly that of the programmer's manual for a language versus the implementation of a particular compiler for that language in the case of lingol the author has found  on occasion  people who are unwilling or unable to draw the distinction  the result is a misconception of what the lingol user has to put up with in writing his programs  as opposed to what performance he may expect when running his programs. the distinction has in fact to be drawn even more 
carefully for lingol than for conventional programming 
languages because for the latter  the usuat choice of operations  arrays  lists  block structure  arithmetic and other operations with well-understood implementations  suggests  to within details of little interest to the programmer  the appropriate implementation of the run-time support since the programs one writes for lingol are highly non-deterministic  and are organized as modules  actors  to use a term in vogue  that do not know with whom they will be communicating until lingol connects them together during the processing of a sentence  the lingol run-time system's task is appreciably more difficult than that of fortran or algol. the burden of control has been shifted from the grammarian to the system  leaving the grammarian free to concentrate on linguistics. so far  no one has proposed an  obviously  good way to tackle this problem for english  and lingol users should be prepared to accept radical changes in lincol's internal operation  i.e. parsing algorithm  as progress is made in this area  since the only legitimate effect of such changes is to improve overall resource consumption  not to compromise correctness of the user's program  this is not really a burden on the user.  
　　　for the benefit of non-readers of  pratt 1  we describe briefly the overall organization of lingol. the lingol system is envisaged as a translator from some natural source language to some target language  natural or artificial  of the user's choice  not necessarily a different language from the source language. there are two phases  one that elicits the surface structure of a sentence and one that produces the desired translation s . the intention is that issues relevant to determining the intended surface structure versus those of translation should be separated out this corresponds to the recommended practice when translating from english to french  say  of understanding each sentence  naturally taking into account previous sentences  before attempting the translation. no restrictions are made on where the lingol programmer draws this boundary  or to what extent 

1 

the information in the two components is duplicated in fact  he can omit the generative component entirely and put everything in the cognitive component  though at some cost in resource consumption at run time  no firm commitment is made by the cognitive component to a particular choice of surface structure of an ambiguous sentence  allowing the generative component to pick and choose when the cognitive component has not had enough information to decide at present lincol users are encouraged to try to make their cognitive component intelligent enough to make the right decision  and so far no lincol programs have attempted disambiguation in the generative component. one would expect this to change as people attempt more sophisticated programs. 
　　　a lincol program is a set of rules each having three components: a context-free rule  a cognitive function and a generative function their respective roles are as follows the cf rule specifies a general english construction  the cognitive component  or  critic   supplies the expertise about that construction and the generative component supplies the information about the target language that may be relevant to this english construction.  our tacit assumption of english as the source language reflects 
lingol's applications to date  
　　　it is fashionable these days to want to avoid alt reference to context-free grammars beyond warning students of computational linguistics that they are unfit for computer consumption as far as computational linguistics is concerned in lincol  as in atn's  woods 1   their role is different from that in  say  the harvard predictive analyzer  kuno 1 . instead of being used to encode all information about english  they form the basis of a pattern-directed non-deterministic programming language. this strategy has several advantages 
 i  it allows the programmer to structure his program as a set of relatively self-contained modules  thereby decreasing the number of things he has to keep in his head at once when looking at a particular part of his program. 
 ii  it eliminates much of the testing-for-cases control structure the programmer would need in a non-pattern-driven language. 
 iii  flow of control between modules is confined to the surface structure  radically simplifying the controlling and tracing of computations this is in contrast to systems that require the user to supply considerably more information to control the flow of computation  riesbeck 1. marcus 1 the apparent subtlety of this advantage belies its importance  and we discuss it further in section 1 
 iv  instead of having to identify each possible source of ambiguity and think up a way to deal with it  the user writes  critics  of individual situations and lets lincol compare the results of the criticisms as applied to competing situations when an ambiguity arises. this reduces the order of magnitude of programming effort in the resolution of ambiguity from possibly order n1 to 
order n  where n is the number of situations that may need to be compared. this is in contrast to the notion of 
 differential diagnosis  presented in  marcus  1  
 v  lingol can optimize the user's program much more effectively if it can identify the context-free component by itself if this component were to be incorporated into the cognitive component  a popular practice these days  the system would not be able to do its own optimization as effectively  and the burden would fall back on the user 
　　　the reader wanting more information on items  i  to  iii  is referred back to  pratt 1  the worked example illustrates each of these advantages the remaining two items are covered in the following sections 
	1 . 1   t h e - c o g n i l i v e 	component 
　　　in this section we describe the lingol parser we first present the algorithm on which it is based  and then show how to use this algorithm to assemble the user's modules and set up communication between them. 
       before immersing ourselves in the technical details of the algorithm  let us consider the options open to us the goal for the parser is to build the surface structure intended by the sentence's speaker all it has to go on is the top and bottom of this tree  and the rules  grammar  constraining plausible surface structures a decision must be made as to where growth should begin two extremes are the top-down approach  in which the tree is grown from its root  and the bottom-up  where growth begins from the bottom  that is  from the words of the sentence. methods with the flavor of either  or both  of these extremes inherit their name s . these methods have other aliases in the literature any scheme that claims to be doing  predictive  analysis  that is  that has expectations about what is coming next and uses those expectations as hypotheses to  drive  the program is essentially a top-down method a program that finds substructures  say conceptual dependency structures  schank 1   and uses them to build bigger structures is a bottom-up method. terms suggested to the author by r. moore are  hypothesis-driven  for  top-down  and  data-driven  for  bottom-up  these concepts transcend phrase-structure grammars and may be applied to any system responsible for building an hierarchically organized structure 
       no matter where the construction begins  we do not know how to carry it out in any straightforward way  even if we want nothing more than to satisfy the context-free rules of our grammar. we always run the risk of letting the construction wander down blind alleys for some grammars and some sentences  the top-down method is less likely to run into cul-de-sacs  but the dual case can also arise it is hardly surprising  given this state of affairs  to hear people wish that they could build structures both top-down and bottom-up in a way that somehow reduced the overhead. one form of this wish is to request a single algorithm that builds no node a bottom-up method would not consider  nor anything a top-down method would not build. an alternative desideratum might be that no node n be built unless that part of the text seen to date is part of som  sentence having a surface structure in which n participates. for a backup-less parser like lingol's present one  this is the strongest possible thing one could ask for as far as exploring cul-de-sacs is concerned one might add to the above the requirement that the parser be able to cooperate 
1 
with other processes such as tuuiines written by the user 
　　　the current implementation of lingol achieves all of the above goals to be more precise  every node it builds is built by both the cocke-kasami-younger bottom-up algorithm and the ingenious earley top-down algorithm  the two algorithms cited in  aho and ullman 1  as the canonical methods for parsing general context-free languages that is  the work performed n the intersection of the work done by each of these methods  at least with respect to proposed phrases moreover  as each phrase is discovered  lingol is able to accept advice from other sources  namely the user's cognitive component  and use it to guide the parse 
　　　roughly speaking  the minimization of searching is accomplished by running the cocke-kasami-younger algorithm and as each phrase is discovered asking an  oracle  whether the earley algorithm would have discovered it the remarkable thing is that this question can be answered in time independent of the length of the input  without having to go and actually run the earley algorithm to see what it would have done  the time is proportional to the size of the grammar  but in the lingol implementation  asking the question involves no more than forming the logical and of n/1-bit vectors for a grammar of n non-terminals. unlike the large grammars kuno worked with  a large lingol grammar should have only from 1 to 1 non-terminals  lingol grammars are not expected to have much information encoded in the context-free component. we are at present exploring a dichotomy for non-terminals  known only to lingol's internals and not to the grammarian  that would permit having goals for only very few non-terminals  thereby ensuring that n/1 would remain negligible.  
　　　before discussing the construction of the oracle  let us sketch the version of the cocke-kasami-younger algorithm we shall use we assume that all rules are of the form either a -  b or a ＊  b c . where a . b and c are non-terminals  or of the form a -  a where a is a terminal. the presence of a -  b means that this is not really chomsky normal form  and allows either the user or some preprocessor to turn an arbitrary grammar into this form using only the trick of replacing all but the first item on the right side of a rule having three or more items by a non-terminal which is itself rewritten to be the replaced non-terminals  and so on until all rules have right sides of length i or 1. the most recent version of lingol incorporates a preprocessor for this task  so this normal form ts now a feature solely of the implementation  not of the user's language  in  earley 1  the notion of  state  is introduced  which elegantly plays the role of these introduced non-terminals in em ley's notation  the state ab.cde plays the role of the nonterminal that replaces the cde everything we say in terms of our restricted grammars can be rephrased more elegantly in terms of earley's states the mam advantage of our notation is that the description of the algorithm is less complicated if the reader is not required to think about arbitrarily  long  states  with this form of grammar we can talk about the left and right sons of binary nodes and the  only  sons of unary nodes. for our purposes it will be convenient to refer to only sons as left sons. 
in the following version of the algorithm  we use 

will print all pairs of numbers summing to 1. this avoids cluttering up the algorithm with details of searching that the programming reader will have no difficulty filling in. 
　　　we shall employ  between-word  notation for positions rather than  at-word  that is  rather than saying that the first word in the sentence is at position i  we will say it lies between positions 1 and i. this avoids any possible ambiguity when referring to the string lying between positions i and j  and also simplifies naming the common boundary of two concatenated strings. it is also the preferred notation in more recent string processing papers. 

　　　a more detailed description of this algorithm appears in  aho and ullman 1  we are concerned here with extending the algorithm 
　　　the above suffices for context-free recognition. for parsing  nodes must record  in addition to the three items type  start and end  two additional items  namely which rule was invoked when noting that node  and which phrases are its sons. the former allows us to access the cognitive and generative components associated with the rule at a later date  while the latter allows us to recover the surface structure  having the rule present makes the syntactic category of the node redundant  and in fact lingol omits it.  
　　　we now introduce the oracle. two things are required to construct this oracle a readily accessible representation of the left-most-character relation  and the notion of a goal we first deal with the goals. associated with each position in the sentence is a set of goats. a goal is a desired non-terminal. if a phrase of the same type as some goal is discovered starting in the position 
1 

affirmative when a phrase thai appears in the tree is 
proposed to the oracle  because every phrase is the left-most character of either some non-left-son or of the 
root  and all such possible goals will have been created by the time we propose this phrase hence the phrase is correctly built. 
　　　to see that no node is built thai could not participate in some surface structure of some completion of the sentence  we must assume that for each non-terminal of the grammar there exists a derivation starting with that non-terminal and ending with a string of all terminals. this follows immediately if we require that every non-terminal appear in at least one surface structure of some sentence of the language  a perfectly reasonable requirement. suppose that we have just built some node 
 b.i.j  then there exists some goal  a.i  such that 
ar*b holds hence there exists a tree with a at the root and b on the left edge we can therefore extend the sentence so that the part starting with the b reduces to a one more reduction is now possible  using the rule that gave rise to the goal in the first place we continue up the tree in this fashion  progressively extending the sentence and satisfying more goals  until we satisfy the sentence goal at this point we have the desired sentence. this completes the proof of the claim that every node built has a 
chance of being used in the fina.1 surface structure. 
　　　what does this fancy algorithm buy as far as the practically minded user ts concerned' one thing we do not claim is any improvement over the traditional 1 n   speed limit for parsing sentences of length n.  see 
 valiant 1  for an improvement to this situation - he 
           1 offers 1 n    though practical considerations make it much worse than most 1 n   algorithms with 
respect to both speed and ability to take advice when parsing  typical  english sentences.  however  we do claim what amounts to an even better improvement in practice than 
going from  namely an 
improvement proportional to the number of non-terminals in the grammar. that is  there exist grammars for which the earley algorithm may generate large state sets in its operation when our algorithm builds very much fewer nodes.  however  there do not exist any grammars where ear ley's algorithm runs in time  while ours runs in time 
 there do exist such grammars when 
comparing our algorithm with the cocke-kasami-younger algorithm  
　　　in even more practical terms  how does this affect parsers working with a purported grammar of english  we 
　　　to see that the algorithm as modified does no less conducted an experiment to compare our algorithm with the than it has to  i.e that it overlooks nothing   suppose we 
                                                                     cocke-kasami-younger algorithm by the simple expedient of have an initial segment of a sentence of the language of the suppressing the test in the procedure  note  that makes our given grammar then in any tree for this sentence  we claim algorithm different from the other working with the grammar that all phrases in the tree contained within the initial of english used in the  1-hour  french translator exhibited segment will have been proposed by the time we have reached in  pratt 1 . we found an improvement of a factor of five the end of the segment we also claim that every right son in the number of nodes built altogether! in fact  with the in the tree will have been generated as a goal just before new algorithm almost all of the nodes built were used in the the parser reached the starting position of that phrase final surface structures of the sentences we tried lack of 
 the induction proofs of these claims are messy and probably a local implementation of earley's algorithm has prevented inappropriate for this paper - the interested reader is us from comparing it with ours in an actual machine simulation. encouraged to fill in his own details.  it follows from 
                                                                     however  it does nor require a machine simulation to see these two claims that the oracle will always answer in the 
1 

that the sort of thing that makes our parser better than ear ley's in some grammars is exactly what arises in english grammars. for example  if one has the rules sentence -  np vp  sentence -  wh vp and sentence -  vp  then earley's algorithm will generate states corresponding to each of these rules even when the sentence begins with  say   why.  earleys algorithm is not smart enough to realize that the first and third rules can be ruled out here  we are making some obvious assumptions about what the rest of this simple grammar might look like . 
1 . . t a k i n g 	advice 
　　　one attractive feature of the above technique is that we did not need to  compile  the grammar; we retained the interpretive nature of the earley and cocke algorithms. this makes it simple for the user to contribute to the operation of the parser  since all the parser is doing at each step is recognizing that some combination of phrases forms a new phrase the user is given the opportunity at each step to look at the constituents of those phrases  to consult his model of the world  or to perform deductions his conclusions are summarized numerically for lingol's benefit  and in pursuing any particular structure  lingol accumulates these numbers as a measure of its confidence in that structure. these confidence numbers are used to choose between alternative ambiguous structures. the winning structures are made readily available to the generative component while the losing structures are kept around  on the end of a list of alternatives  in case the generative component becomes dissatisfied with the choice made by the cognitive component and wants to try some of the others. 
　　　the style of programming used in the cognitive component is analogous to that described in  pratt 1  for the generative component the primary difference is that  since the structures are being built bottom-up at the time the cognitive component is being built  it is not possible to declare variables high up in the tree for use by routines lower down  a facility that gives the generative component considerable power this inability is inherent in the nature of any system that wants to do criticism on the spot without waiting for the rest of the sentence if you don't know what's coming  you can't  other than by guessing  make assumptions about what the higher nodes using the one in question will look like  this is not altogether true - if the relation r* were represented explicitly as a collection of paths in r  it might be possible to set up variables on high as the goals are being generated  provided nodes being discovered below set their own sights on only one goal it is likely that this would add substantially to the overhead of the system  however  
1-using  	the.algorith.m as 	a contcal device 
　　　the emphasis of this section is on nsight rather than mechanics the program paradigm discussed in  pratt 1  p 1  supplies the mechanical details of how modules are assembled and how they communicate. briefly  the surface structure chosen by the algorithm is taken to be the skeleton for a program whose substantive components are lisp functions  the generative component . these functions are associated with the nodes of the tree the tree is then itself taken to be a large expression  and is evaluated. 
two types of communication are provided for: functions may  return  values  which are received by their immediate superiors in the tree  and variables  declared local to some subtree  may be used as  mailboxes  for communication up  down or sideways w i t h i n t h a t subu.ee  reflecting an apparent locality in many linguistic phenomena  a more detailed account of these mechanics is in  pratt  1  
　　　we have here a somewhat unusual programming environment the user is told that he may write arbitrary lisp code for the function at each vertex; provided the original sentence can be reconstructed from the surface structure information  he is no worse off in principle than if he started from scratch however  this easily made point is not the real issue rather  the user is supposed to assume that the surface structure the parser found is what he thought it would be the insight is that  although a considerable amount of enguistic processing of the sentence may still remain to be done after the structure is found  that processing will be of the form  what to reply when you see a ...  rather than  where to search for a.  that is  there is no longer the emphasis on control  backup techniques  depth-first vs breadrh-first issues  passing environments around  and so on  that characterizes many papers on parsing instead  the user has to decide  for each of many local situations  what the answer is 
       it should be obvious from the above that we are advocating: 
 i  uniform control of search  using some linguistic information  the context-free component plus the cognitive component  plus a smart parsing algorithm; 
 u  non-uniform modular treatment of the remainder of the user's linguistic information 
the second of these reflects the structure of a typical grammar written in english for consumption by humans  at least those grammars written before linguistics became confused with mathematics . the phenomena are treated one at a time in the grammar  and the notions of procedure  control  for-loops  recursion  searching and so on never appear everything is very modular  even isolated much of the time  by  isolated  we mean that the phenomenon does not depend on some other phenomenon for its full explanation   modular  only refers to the degree of organization and does not exclude inter-module communication.  
　　　it is our hope that this modularity is what makes it easy for grammarians to write large grammar books for human consumption  rather than that the grammars are written in english if so  it may make more efficient the process of telling english to computers  which has been proceeding slowly to date 
　　　all this assumes  of course  that we have a reliable structure finder we snnd by our claim in  pratt 1. p. 1  that considerably less linguistic information is needed to get the surface structure than for subsequent processing. 
1 

1 	role of the context-free component. 
　　　since the trend these days is away from explicit context-free grammars and towards encoding all syntactic information in other ways  it is reasonable to ask why have a separate context-free component the issue is one of efficiency  among other things it has yet to be demonstrated that english is easy to parse as far as we know  a program with a lot of expertise about english is going to discover a lot of things to say abou' a sentence  most of them presumably being of the form  it can't be this interpretation because. * there is much wishful thinking these days  marcus 1. riesbeck 1  about being able to ignore entirely the sorts of parses discovered by the harvard predictive analyzer  kuno 1  for reasonable english sentences to this author's knowledge  no such wishful thinking has been realized as a program having the linguistic competence of  say  sager's system  sager j1 . or for that matter the predictive analyzer the problem may be that one cannot dismiss these obscure parses on trivial grounds without also eliminating perfectly good parses of other sentences where the corresponding structure is not so peculiar. unfortunately  there is no evidence to support this one way or the other  and the author wishes to sit on the fence for the time being as far as whether the above wishful thinking can be put into practice  he would like to put it into practice himself  but along with the rest of the world has no idea how.  
　　　given that one's program can be expected to encounter many competing interpretations of a sentence  and that in many cases it will have to pass non-trivial judgment on rhese cases  it can be very difficult to write a program to deal with much of english lingol allows the user to organize his program so that the burden of the book-keeping associated with discovering and comparing all these possibilities is shifted to the system  allowing the user to concentrate on writing code to criticize individual situations m marcus has suggested to the author that in so doing he is allowing the user to concentrate on the competence aspects of english by supplying him with packaged performance the context-free rules and the critics encode the sort of information one finds in a grammar book  which is competence  while the system knows about good parsing strategies  which is performance 
　　　a long range goal in this regard is to develop a high level language version of lingol such that grammars may be given to either computers or people then if the computer  understood  english on the bas's of that grammar  and if people could read it painlessly. it would make an ideal theory of english since people read procedures painfully slowly if at all. the high level language will have to be considerably less procedural than at present 
       in our approach  the cf rules function as a crude approximation to english that permits lingol to rapidly select from a huge set of possible structures for the sentence a smalt plausible set for more detailed  and expensive  criticism by the cognitive component. the context-free representation for the  crude approximation  is chosen partly because it is not difficult to construct quite good approximations using context-free grammars  and partly because there exist remarkably efficient algorithms for exploring the space of possible surface structures for sentences of context-free grammars  yet that can accept advice on a step by-step basis 
　　　this situation of having the system do one's book-keeping was what obtained in the hey-day of context-free parsers  of course one side-effect of the later disenchantment with and abandonment of context-free grammars was to throw the baby out with the bath-water by reverting to doing much or all of the book-keeping oneself not only does this require an indefinitely larger programming effort  it a ho requires of the programmer considerable sophistication in parsing techniques if his code is to operate as efficiently as one of the better context-free parsing algorithms  especially when that algorithm can cooperate effectively with the user's code in assisting it to reduce the search space even further. 
　　　a comparison of our system with that of woods  is inevitable where woods has augmented transition networks  we have augmented context-free grammars. since basic  i.e. unaugmented  tn's are exactly equivalent to context-free grammars in strong generative capacity  there should in principle be no difference in practice  there are a number of differences one difference is in our parsing algorithm  a property of the implementation rather than of the lingol language  which assumes a larger responsibility for determining the flow of control than does woods'. another difference is in the language - we take a static view of english inasmuch as we use the seemingly declarative cf notation  whereas woods uses the seemingly procedural transition networks the procedural flavor becomes substantive when the augments are introduced. our augments are intended to be mere grammatical critics; woods' have considerably more to say about the flow of control. as argued earlier  we feel that the static view is more conducive to efficient writing of grammars  so long as the system can take over the efficiency considerations one notable difference is that the lingol language has far fewer primitive concepts than does woods'  without losing any of the features of woods' system the idea here is that the constructs provided explicitly in woods system have lisp analogues  so why duplicate them  another difference is our separation of the cognitive and generative components  which we feel is a plus since then the target language issues can be cleanly separated from the cognitive issues this separation does not preclude the sort of interaction with world knowledge advocated in  winograd 1  
	1 . s y n t a x 	a n d efficiency: 
       lingol has had a chronic identity crisis over the issue of whether it should predominantly rely on syntax in its initial phase this brief section addresses the issue of whether syntax is a necessary part of a computational linguistics program we have in mind here r. schank's claim that  syntax is not needed to do parsing  for some variety in the usual replies to this sort of claim  we propose that even if schank is right  thts assumption is local to this section  syntax may be of value in improving the efficiency of the parsing process this point can be easily overlooked both in informal introspection about whether one felt one needed any syntax to parse the sentence and in formal experiments  eg. with the tachistoscope  designed to show that scrambled sentences shown briefly are recalled in their unscrambled form. what is being overlooked here is how long it took to unscramble the sentence 
1 
　　　given grammars language's sentence's initial segment assumed given in order to see no less overlooked than necessary  translation: to see that no less is overlooked than necessary  assume we are given an initial segment of a sentence of the language of the given grammar.  if you stumbled over this sentence then perhaps it is because the syntax is not there to speed things up for you. conventional groupings of words have been rearranged in relatively unfamiliar  though not entirely ungrammatical or meaningless  ways and some  noise  words have gone. nevertheless  with a tittle extra effort you should at least be able to parse the sentence correctly  and after a few passes you will begin to wonder why you ever had any trouble with it at all. moreover  there seems no obvious reason why 
a program that could handle the original sentence could not equally welt handle the above version.  i had difficulty restraining myself from replacing it with a more difficult sentence by the time i had typed it up.  the claim is that in the original version  part of the reason why you had less trouble with it was that it was phrased in a very conventional style that you have encountered frequently  allowing you to go straight to the places where you expect to find the information in the sentence there are  noise  words all along the way  but to see that they are not really all that noisy  try replacing them with other noise words; the effect wilt be somewhat like switching all the street signs when navigating in one's car. 
　　　thus white it is conceivable that one can get by without syntax  though what this means exactly is surely open to debate   even if one does so one is faced with culling out the structure desired from a huge choice without the benefit of syntactic information to reduce the search space. 
1 
