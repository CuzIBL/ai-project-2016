
the predictive representations hypothesis holds that particularly good generalization will result from representing the state of the world in terms of predictions about possible future experience. this hypothesis has been a central motivation behind recent research in  for example  psrs and td networks. in this paper we present the first explicit investigation of this hypothesis. we show in a reinforcement-learning example  a grid-world navigation task  that a predictive representation in tabular form can learn much faster than both the tabular explicit-state representation and a tabular history-based method.
1 introduction
a predictive representation is one that describes the world in terms of predictions about future observations. the predictive representations hypothesis holds that such representations are particularly good for generalization. a good representation is one that captures regularities of the environment in a form useful to the learning agent; and in a reinforcement-learning task  something is  useful  if it increases the agent's ability to receive rewards. thus  representations generalize well when the regularities they capture allow an agent to learn more efficiently how to increase its cumulative reward.
　why do we believe that predictive representations might generalize particularly well  good generalization tends to result when similar situations have similar representations. in interactive tasks  two situations are similar when executing an action sequence in one leads to about the same results as executing the same action sequence in the other  regardless of what the action sequence is. the more similar the results tend to be  the more similar the situations are  and  therefore  the more similar their predictive representations will be.
　the predictive representations hypothesis makes an especially broad claim  and as a result it is especially resistant to testing or proof. in particular  questions regarding representation and generalization often involve the confounding issues of function approximation and representation acquisition - issues that we have developed specific measures to avoid. in reinforcement learning  for example  generalization is commonly achieved through function approximation: by learning weightings from a set of features over the agent's past and present perceptions. good generalization occurs when the features are well chosen and are roughly independent of each other. predictive representations  too  are generally used in conjunction with function approximation  where each prediction is treated as a feature.
　yet function approximation is itself a complex issue  requiring choices of method  optimization of learning parameters  etc.  each of which can influence the results in its own way. to avoid such confusion and test the predictiverepresentations hypothesis most directly  we have controlled for the possible confounding effects of function approximation by developing a strictly tabular form of predictive representation. furthermore  our focus is on how predictive representations affect generalization  not on how these representations are acquired  and so we assume they have already been acquired by some other process.
　we illustrate the power of predictive representations in a grid-world navigation task where the agent's action and observation space is chosen to be particularly impoverished  thus imposing a high degree of perceptual ambiguity. with only a small subset of the predictions necessary for a full representation of the environmental state  a reinforcementlearning agent can learn quickly and still achieve nearly optimal performance.
1 predictive representations
several recently introduced methods for modeling dynamical systems have been inspired by the predictive-representations hypothesis. among these are predictive state representations  psrs   littman et al.  1  and temporal-difference networks  td networks   sutton and tanner  1 . current research on predictive representations has focused principally on the acquisition of the representation  though their use for control is also beginning to be explored  james et al.  1; izadi and precup  1  .
　psrs are based on the concept of a core test-a sequence of actions followed by an observation-similar to the tests of rivest and schapire . an agent records the outcome of each core test as either a success or failure depending on whether the predicted observation matches the actual observation. the agent maintains the probability of success for each core test  and this value becomes a feature in the agent's representation of the world. knowledge of the world can be expressed as a function  generally a linear combination  of these features. core tests are continually acquired until the features form a sufficient statistic  which means that they capture all relevant  knowable information about the environment and can maintain such information from one time step to the next.  more will be said about sufficient statistics below. 
　td networks consist of two conceptually separate networks: a question network and an answer network. the question network poses questions about possible future observations. the answer network learns to predict answers to those questions. the questions are therefore analogous to the core tests of psrs and the answer network is analogous to the function that computes the probabilities of core-test successes. however  the units of the question network can make predictions about not just the agent's observations  but also about the values of other network units  and these predictions of predictions allow the td network to represent many possible core tests in a compact form. furthermore  these predictions are generally  but not necessarily  action conditional.
　the two approaches just outlined share the common aspiration of representing the world as a set of predictions about future observations. while research into predictive representations is still in its nascent phases  we attempt to make a prediction of our own about possible future observations. in this spirit we pose the following question: if a method does indeed prove successful at acquiring predictive representations  will these representations be good at generalization 
1 tabular predictive representations
in order to focus exclusively on prediction as a basis for generalization  unclouded by issues involving representation acquisition  we assume the agent has already acquired the ability to make correct predictions. to eliminate issues involving function approximation  we consider deterministic tasks with a single  binary observation  though our method can also be applied to multiple binary observations  and we describe an extension in the future-work section for accommodating continuous observations or stochastic environments . to further focus our tests  we choose to look only at predictions that are contingent upon the agent's actions  though this restriction is not a requirement of predictive representations in general.
1 identically predictive classes
we start with a set of binary tests resembling psr core tests. each test is of length n  meaning that it is a sequence of n actions followed by a single observation bit. we construct all possible tests of length 1 through n and produce a panel of test results in each state  one entry per test. given n binary tests  the panel may take on 1n distinct configurations of outcomes  assuming the environment is deterministic . if the agent has a actions available  then there are at most an distinct tests of length n  and the number of tests of length n or less is:
.
if two states cannot be distinguished by any of the n tests  then the panel of results is identical in both states  and we say these states are identically predictive for that value of n and belong to the same identically predictive class  similar to the states of rivest and schapire's simple-assignment automata   but constrained by n .
　if for a given n an environment has c identically predictive classes  we arbitrarily number them 1 through c  and when an agent visits an environmental state  it observes the number for that state's identically predictive class. this number is therefore the agent's predictive representation in tabular form.
as an example  suppose that a = 1  and n = 1  then n =
1  and the panel consists of three tests  one for each action . each test can result in an observation of 1 or 1  so there are 1 possible panel configurations and c = 1  at most . every state is aggregated into one of these eight classes  labeled 1 through 1  and the agent observes a 1 1 1 1  or 1 in each state that it visits.
　in general  as n increases  both n and c increase  so there are fewer states per class on average and the agent's representation of its environment becomes more expressive.
　however  not all 1n panel configurations are necessarily represented in the environment: clearly  there can never be more classes c then there are environmental states s  so if n   s then some tests must be equivalent  meaning there are no environmental states where the tests yield different results . furthermore  as n increases  n increases exponentially  yet c tends to increase quite slowly in environments with even a moderate amount of regularity.
　eventually  increasing n no longer increases c  and the classes represent a sufficient statistic. at that point c may still be less than s if there are environmental states that cannot be distinguished by any test of any length.
1 sarsa 1  with identically predictive classes
all agents in this paper are trained using the reinforcementlearning algorithm known as episodic tabular sarsa 1   sutton and barto  1 . in the traditional markov case- where the agent directly observes the environmental state- an action-value function is learned over the space of environmental states and actions. in this algorithm  the estimated value q s a  of each experienced state-action pair s a is updated based on the immediate reward r and the estimated value of the next state-action pair; i.e 
 q s a  = α r + q s1 a1    q s a   
where α is a learning-rate parameter.
　episodic tabular sarsa 1  is implemented over the predictive state space by mapping environmental states to their corresponding identically predictive classes  as described in the previous section. the function c ，  provides this mapping  and the resulting classes are then treated by the sarsa agent as though they were environmental states:
	 q c s  a  = α r + q c s1  a1    q c s  a  	 1 
because no distinction is made between the states within a class  the learning that occurs in one environmental state applies to all states mapped to the same class.

figure 1: an agent in this world can have one of 1 orientations in each of the 1 grid squares. its observation consists of one  nose touch  bit that senses only whether there is a wall directly in front of it. it chooses actions from {rotate 1  right  rotate 1  left  advance}. there are therefore 1 distinct environmental states  but a predictive representation will identify  at most  1 identically predictive classes.
1 performance and generalization
in episodic tasks the performance of a reinforcement-learning agent can be quantified by the total reward it receives per episode. given infinite time  agents that observe the  markov  state of the environment can achieve optimal performance.
　in practice  however  learning an optimal policy may take arbitrarily long for an arbitrarily large state space  and these days one can rarely find enough time  let alone an infinite or even an arbitrary amount of it. it is therefore desirable in many cases to use methods that speed learning  even if the final solution is not absolutely optimal.
　we anticipate a trade-off between the agent's asymptotic performance and its speed of learning. as c increases  the classes more closely resemble the markov state of the environment and the agent's asymptotic performance approaches optimal. as c decreases  there are fewer distinct cases to learn about and learning speed improves  but there is a risk that the states comprising a class will disagree about the optimal action for that class. this disagreement can lead to sub-optimal action selection  and in extreme cases the conflict may be catastrophic  precluding the discovery of any reasonable policy. therefore  a good representation will find low values of c while minimizing the amount of disagreement within classes. the predictive-representations hypothesis specifically holds that the classes formed through predictive representations will do just that.
1 predictive classes and sufficient statistics
an agent's representation of the world is a sufficient statistic if it cannot be improved through any further experience with the world. in the case of predictive representations  a sufficient statistic implies that there are no additional predictions that can add knowledge or improve performance. on the other hand  adding more predictions to a sufficient statistic can never worsen asymptotic performance.
　in the tabular case  if a sufficient statistic consists of c
　classes  no number of additional tests will increase c.  if by adding a test we were to end up with c1 classes  where c1   c  then our original representation could not have been a sufficient statistic  since it did not represent the knowledge captured by the newest c1  c class distinctions.  nor will any further tests ever reduce the number of classes or change the way states have been assigned to classes.
　but an agent whose predictive representation is a sufficient statistic may still not distinguish all environmental states. this idea is illustrated in figure 1 where there are 1 distinct environmental states  four orientations in each grid cell .
　given an agent that has three actions  rotate 1  right  rotate 1  left  advance  and that observes only whether or not there is a wall directly in front of it  each arm of the cross will produce exactly the same sets of predictions. to the predictive agent  the arms are identical because there is no test that can distinguish a state in one arm from the corresponding states in the other three arms  even with infinite-length tests. therefore  for this environment there are a maximum of 1 identically predictive classes  four orientations in four arm cells  plus only one in the center because all orientations in the center are identically predictive .
　this example also illustrates how an agent's learning speed can be improved through the use of predictive representations. if the task is to navigate to the cell marked x  an agent observing identically predictive class numbers will learn an optimal policy much faster than an agent observing environmental state  because the predictive-class agent has only a quarter of the situations to learn about and therefore requires far less experience to learn about all possible situations.
1 tabular history-based representations
the obvious competitors to predictive representations are history-based representations. of these  the fixed-length or markov-k approaches  ring  1; mccallum.  1; mitchell  1  most clearly lend themselves to a tabular format. to promote a fair comparison between representations  we define a k-length history to be an observation followed by k action-observation pairs. in tabular format  each possible history is uniquely labeled.
　predictive representations and fixed-length history representations offer fundamentally different kinds of generalization. specifically  most environmental states can be reached via multiple different paths  each corresponding to a different history; conversely  a single action sequence  history  may reach multiple environmental states by starting from different states. therefore  the mapping between environmental states and fixed-length history sequences is many to many. but the mapping between environmental states and identically predictive classes is many to one.
　conceptually  the reason for the difference is that an agent can take many paths to arrive in a state  but once there  has only one set of possible futures; and the set of possible futures is absolutely fixed for each environmental state. for example  if k = 1 the agent has at least two ways of reaching every state in figure 1-rotating right or rotating left. if k = 1 there are at least 1 different histories for each state. the number of fixed-length history representations that can lead to each environmental state increases exponentially with k  so as k increases  there are an exponential number of cases that the agent must learn about.  this problem does not automatically disappear by using function approximation methods. 
in contrast  the set of possible futures available from each

figure 1: the  office layout  grid-world used for the navigation task. the agent starts an episode in one of the six top rooms  and finishes in the square marked g.
state is the same no matter how that state is reached  and as a result  the number of predictive classes is never more than the number of distinct environmental states. the nature of this mapping makes generalization in the predictive case far more intuitive  and to some extent the predictive-representations hypothesis is based on this clearer intuition.
1 experiment design
we examined the predictive-representations hypothesis by first designing a grid-world with a large degree of regularity  figure 1 . we then tested sarsa 1  agents with four different methods of representation on a task in this environment to see how well each method was able to exploit the environment's regularities.
　the agent's task was to navigate to the goal cell  marked g  from any randomly chosen square in one of the top six rooms.  the environment was designed to resemble a typical office layout  and the task can be likened to finding the quickest route to the staircase.  representations that generalize well should allow their respective agents to exploit the regularities in the environment to improve their speed of learning.
　as in figure 1  the agent has a one-bit observation and three possible actions. the rewards for the task are +1 for reaching the goal state and  1 on all other timesteps. all transitions in the environment are deterministic and the environment has a total of 1 states  1 of which are possible start states. on average  there are 1 steps along the optimal path from start to goal. the task is formulated to be undiscounted and episodic; thus the agent is transported to a randomly chosen starting position upon reaching the goal.
　in every case actions were chosen according to an -greedy policy;  was set to 1  and α was set to 1  which are typical values for sarsa agents in episodic tasks.
the four representational schemas tested were:
  markov
  n-depth predictive classes
  k-step fixed-length histories
  random state aggregation
　in the markov case the agent directly observed its current environmental state  each state being represented by a unique
unique% environmentalagentobservationsstatesmarkov-1%predictiven 11%1111111111fixed-historyk 11%11111 1.11 1.1randomcase 1 1%1 11 1figure 1: the four representational schemas in their tested instantiations and the degree of state aggregation in each case. in the predictive case  the  unique observations  column represents the number of identically predictive classes  c. for fixed-length history  this column represents the number of unique histories that occurred during training.
label. in the predictive case the agent observed a label corresponding to the identically predictive class  as described in section 1  for six different values of n. for each value of n  states were assigned to classes as described in section 1. in the fixed-history case  the agent observed a label corresponding to the k-step history it had just experienced  section 1  for five different values of k. to see whether our results in the predictive case were merely due to beneficial properties of state aggregation  we tried randomly assigning states to classes and then training according to equation 1.
　figure 1 shows the vital stats for each of the representational methods tested. the amount of state aggregation that occurs in each method is shown in terms of the ratio of unique observations to environmental states.
1 results
performance results for the different representational schemas given in figure 1 are graphed in figure 1  with the exception of the random state aggregation method  which performed too poorly to be graphed meaningfully.
　each point in the graph represents the average number of steps per episode over the previous 1 episodes. the curves are averaged over 1 trials  each trial being 1 episodes. at the end of each trial  the agent's action values are reset  and learning begins from scratch. over the course of 1 episodes  the markov case shows a smooth  steadily improving curve  which by the 1th episode is performing very close to optimal.

episodes
figure 1: a comparison of three representational methods on the task of figure 1. the representational schemas shown are: explicit environmental state  history-based representations for k = 1 through 1  and identically predictive classes for n = 1 through 1. the vertical axis shows average steps taken per episode over the past 1 episodes of training. note that the axes are logarithmic and the vertical axis intersects the horizontal axis at the optimum.　though fixed-length history representations learn more slowly than the other methods  their learning speeds are fastest for small values of k and their final results are best for large values of k  demonstrating the trade-off between representational expressiveness and learning speed. the number of histories increases  exponentially  with k  which negatively impacts learning speed but positively impacts the final results of learning. interestingly  on individual trials the curves for the history representation resemble a step function. apparently  the -greedy action selection chooses poorly for many episodes until the agent suddenly learns some key piece of information that dramatically improves performance. as k grows  the agent takes longer to discover this piece of information but then converges to a better solution.
　the results look promising for predictive representations. they allow both speedy learning and convergence to a good policy. in general  the results for the identically predictive representations are similar to those for the fixed-history representations in that convergence speed decreases and convergence quality increases as n increases. however  in contrast to the fixed-history representation  the number of identically predictive classes increases quite slowly with n and the generalization benefit of the predictive classes is clear. the representation effectively aggregates similar states  allowing the agent to converge to near-optimal solutions. this result closely matches our intuition and expectations. only in the n = 1 case was there no improvement in learning speed  indicating there may be a maximum degree of state aggregation beyond which learning speed is not improved-also an expected and intuitive result from section 1. results in the n = 1 and n = 1 cases match these trends but are not shown so as to keep the graph at least vaguely readable.
the case of random state aggregation is not shown in the graphs because these agents performed so poorly. with only 1% state aggregation  1% of the states not aggregated   random state aggregation brought about catastrophic failure in over 1% of the agents tested  meaning that they show no progress in their training. to avoid catastrophic failure in 1% of the agents  the amount of state aggregation had to be 1% or less  meaning that only one state in a hundred shared a class with another state.
　in contrast  there were no cases of catastrophic failure in any of the agents using predictive representations.
　it might be argued that the predictive representations benefit unfairly from the preprocessing done to create the classes  which the history-based methods did not have  and which an actual learning agent may not reasonably be expected to acquire. or it might be argued that history-based methods could somehow be augmented to combine all ways of reaching a state into equivalence classes as our tabular representations do for predictions. but these objections miss two fundamental aspects of this research and of predictive representations in general. first  it is not our intention here to prove the superiority of one particular learning algorithm to any other  but only to test out  in advance  whether the research direction of predictive representations looks promising.  the evidence collected clearly gives reason for optimism.  second  acquisition of a predictive representation is always directed toward acquisition of a sufficient statistic. once an agent acquires the predictions necessary for a sufficient statistic  then it can in principle create the class mappings used here.  in fact  those mappings would fall directly out of a td network representation  being merely a subset.  it is not clear what the equivalent of a sufficient statistic might be for history-based methods  nor whether it could be reasonably acquired.
　another objection is that performance could be improved even more by hand-coding a mapping of states to classes. but  don't be so sure! while it is obvious that an optimal policy could be encoded by just three classes  one for each action   finding a mapping that encourages learning is a completely different matter  and it is difficult to design a mapping by hand that  a  aggregates the states into a small enough number of classes to allow good generalization   b  does not risk catastrophic failure by mapping two states into a class that makes learning impossible  and  c  is not essentially based on our intuitive notions of prediction.
1 conclusions and future work
this paper makes an initial attempt to answer the question: if there were a learning system that could represent the world in terms of predictions about possible future experience  would that representation turn out to be useful for generalization in reinforcement-learning tasks  while this question is itself quite broad  fully answering it might require thorough testing in the presence of many confounding factors   our results are clear and lend weight to the possibility of a yes answer.
　in comparison to other representational schemas  our tests suggest that predictive representations may generalize a state space very well  allowing faster learning without obvious risk of catastrophic failure through poor state aggregation.
　but there is another significant conclusion to be reached. it is fairly common in reinforcement-learning tasks to provide agents with the environmental state in tabular form. though it was our intention only to examine predictive representational methods  the particular tabular method we developed shows significant advantages over the markov representation often seen in the reinforcement-learning literature. we suggest it may have unintended practical application to tasks with large state spaces when the environmental state is readily available. mapping states into identically predictive classes may speed learning and still allow nearly optimal performance to be achieved. this certainly deserves further investigation.
　we would also like to test predictive representations in non-deterministic tasks and in environments with continuousvalued observations. in the discrete  deterministic case we can make binary predictions of observations  but in environments with continuous-valued observations  the predictions would also be continuous  and in the non-deterministic tasks the predictions would be continuous-valued probabilities. one solution is to use tile coding to discretize the continuous values  reducing them to binary values that can be treated as described above. a second approach is to view the panel of test results as vectors and measure their similarity with a metric such as euclidean distance. vectors within a certain distance would belong to the same identically predictive class.
　it should be noted that we have avoided the issue of poor or malicious placement of rewards. what would happen  for example  if the goal in figure 1 were placed in the middle of one of the arms  the agent  not able to distinguish between one arm and the others  would actually be hindered by using a predictive representation. the solution is to incorporate reward s  into the predictive model of the world. the ability to predict rewards would increase the expressiveness of the representation and could assist the integration of predictive representations into reinforcement-learning algorithms.
　finally  there is an obvious enhancement to the classification method we employed in this paper. since small values of n speed learning the most but large values of n converge to a better policy  it would make sense to gradually increase n  splitting up the identically predictive classes into smaller and smaller groups whenever learning begins to converge. since all the states in each newly derived class would come strictly from the same parent class  the action values for the new classes could be initialized to those of the parent class  and all training done up to that point would be preserved.
acknowledgments
we thank anna koop for her participation in our early discussions on this paper and for being a good sport. thanks to mark lee for help with the simulation code. this research was supported in part by grants from icore  and nserc  and by darpa grant hr1-1.
