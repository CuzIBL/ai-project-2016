
subset selection problems are relevant in many domains. unfortunately  their combinatorial nature prohibits solving them optimally in most cases. local search algorithms have been applied to subset selection with varying degrees of success. this work presents compset  a general algorithm for subset selection that invokes an existing local search algorithm from a random subset and its complementary set  exchanging information between the two runs to help identify wrong moves. preliminary results on complex sat  max clique  1 multidimensional knapsack and vertex cover problems show that compset improves the efficient stochastic hill climbing and tabu search algorithms by up to two orders of magnitudes.
1	introduction
the subset selection problem  ssp  is simply defined: given a set of elements e = {e1 e1 ... en} and a utility function u : 1e 1↙ r  find a subset s   e such that u s  is optimal. many real-life problems are ssps  or can be formulated as such. classic examples include sat  max clique  independent set  vertex cover  knapsack  set covering  set partitioning  feature subset selection  classification  and instance selection  for nearest-neighbor classifiers  to name a few.
﹛since the search space is exponential in the size of e  finding an optimal subset without relaxing assumptions is intractable. problems associated with subset selection are typically np-hard or np-complete. local search algorithms are among the common methods for solving hard combinatorial optimization problems such as subset selection. hill climbing  simulated annealing  kirkpatrick et al.  1  and tabu search  glover and laguna  1  have all been proven to provide good solutions in a variety of domains. the general technique of random restarts is applicable to all of them  yielding anytime behavior as well as the pac property  probability to find the optimal solution converges to 1 as the running time approaches infinity .
﹛the problem with these local search algorithms is  however  that they are too general. the only heuristic they have about the domain is in a form of a black-box  the utility function which they try to optimize. it is therefore common to have modifications to these algorithms that trade being general for additional domain-specific knowledge. the sat domain is full of such variants  gent and walsh  1; hoos and stutzle  1“   but they are common in other domains as well  khuri and back  1; evans  1“  .
﹛in this paper we present a general modification to known local search algorithms which improve their performance on ssps. the idea behind it is to exploit attributes that are specific to search spaces of subset selection. knowing that it is a subset search space allows us to infer which moves were likely to be wrong. by reversing these moves and trying again  we start from a new context  and the probability to repeat the mistake is reduced. in experiments performed on complex sat  max clique  1 multidimensional knapsack and vertex cover problems  the new method has shown to significantly improve the underlying search algorithm.
1	the compset algorithm
subset selection can be expressed as a search in a graph. each node  state  in the graph represents a unique subset. edges correspond to adding or removing an element from the subset  thus there are n = |e| edges to every node. the following figure shows the search graph for 1 elements.

a state s can be represented by a bit vector where bit s i  is 1 iff ei ﹋ s. moving to a neighboring state in the graph is equivalent to flipping one bit.
﹛each state is associated with a utility value  which is the value of u on the subset it represents. local search algorithms typically start from a random state and make successive improvements to it by moving to neighboring states. they vary from each other mainly in their definition of neighborhood  and their selection method.
﹛using this representation  all local search algorithms are also applicable to subset selection. however  being general  they overlook the specific characteristics of subset selection. compset guides a given local search algorithm using knowledge specific to subset selection.
1 characteristics of subset selection in a selection problem of n elements  there are n operators: f = {f1 f1 ... fn} where fi is the operator of toggling the membership of element i in a set  if it was in the set remove it   or add it if it was out . applying fi is equivalent to flipping the ith bit in the bit vector representation. throughout the following discussion we assume a single optimal subset  solution  which we donate s .
we make the following observations about subset search:
observation 1. let s1 be an arbitrary state  subset . from any state s  there exists a subset of the operators  考 s s1    f  that when applied to s results in s1.
proof. since s1  as s  is an n bits long vector  there are at most n bits of s1 that do not agree with s and need to be flipped using an operator.	
observation 1. let s be the complementary state of s i.e.  the state derived from s by flipping all its bits. the subset of operators 考 s s   leading from s to the solution s   is the complementary subset of 考 s s   in f. that is  考 s s   ﹍ 考 s s   = f and 考 s s   ﹎ 考 s s   =  .
proof. we need to show  that for every fi﹋f either fi ﹋
 then fi ﹋/ 考 s s    it does not need to be flipped . moreover  if necessarily  since in s all bits are flipped. thus  fi ﹋/ 考 s s   ↙ fi ﹋ 考 s s  . the same goes for the other possible case  in which. 
﹛the inherent problem in finding 考 s s   using local search  is that operators are applied successively and their effect is not necessarily of monotonic improvement due to interdependencies between elements. such non-monotonic behavior of u confuses local search algorithms and often makes them trapped in local optima. in such a case  there are two options: either the search is progressing on the correct path to the solution  but the algorithm does not see a way of continuing   or it is off the correct path altogether. it would be beneficial to distinguish between these two scenarios.
consider two independent hill climbing runs  one from
and one from s. given that the optimal solution is not found  the two runs have stopped in local optima  ls and ls respectively. we consider the subsets of operators leading from s
to ls and from s to ls. by observation 1  it is not possible that an operator fi appears in both operator subsets if they are both on the path to s . if we do observe the same operator in both  it is a clear sign that one of them is wrong. this is the idea behind compset  which is described next.
1	description of the compset algorithm
interdependencies between elements are distracting when searching for good solutions. had all elements been independent  a simple linear search  which adds element after element as long as the utility value improves  would suffice. local optima are an example where such interdependency brings the search to a full stop. it is likely that by applying the operators in a different order  or eliminating some  the local optimum would have been avoided. compset uses the above observations to try and identify wrong invocations of operators. it then cancels them  reverses their effect  and resumes the run towards another local optimum or  hopefully  the solution.
procedure compset s  localsearchalg 
s1 ↘ s ; s1 ↘ s ; agree ↘ false loop until agree
ls ↘ localsearchalg s1 
ls ↘ localsearchalg s1 
c ↘ 考 s ls  ﹎ 考 s ls 
if c is empty then
agree ↘ true
 // apply all operators in c on ls
return the better between ls and ls
end
given a start state s  compset initiates two runs of the given local search algorithm  one from s and one from
its complementary s. once two local optima are achieved 
the series of operators that has led to each one is examined. every operator that appears in both series must be wrong in one of them. we do not know which of the runs went wrong  so we reverse the effect of such operators in both local optima. once all obviously wrong operators are undone  the local search is continued. the process repeats upon encountering the next pair of local optima. when no conflicting operators exist  and the optimal solution was not found  compset ends and returns the better of the two local optima in hand.
the rational behind compset is illustrated here:

the solution s  is reachable by applying all operators from
 in any order   and by applying all operators
from 考 s s   to s. the underlying search algorithm from s is in a correct path if it is using only operators from 考 s s  . however  a search can easily divert from this path  and it is often necessary for its overall convergence to the solution. diversion occurs if an operator from 考 s s   is applied to s  or an operator from 考 s s   is applied s. the problem is  that since we do not know 考 s s   or 考 s s    it is difficult to detect such diversions. however  what we do know  is that the solution s  conforms to 考 s s   ﹎ 考 s s   =  . we can use this fact to try and identify diversions.
once stopped in local optima ls from s and ls from s 
we check 考 s ls  ﹎ 考 s ls . if the intersection is not empty then by definition either ls or ls are off the path. note  that if the intersection is empty  the local optima might still be off path  because either 考 s ls  uses an operator
from 考 s s    or 考 s ls  uses an operator from 考 s s  . however  考 s ls  ﹎ 考 s ls  1=   means that for sure at least one of the local optima is off path.
﹛in general it is possible that the search algorithm would continue applying operators and finally return to the path  but being in a local optimum means that it has essentially  given up . elimination of all conflicting operators from both sides brings us to s1 and s1  ls ↙ s1 and ls ↙ s1. since all common operators were eliminated  考 s s1  ﹎ 考 s s1  =   and thus s1 and s1 are on a possibly correct path. it is still possible that 考 s s1  contains operators from 考 s s   or that 考 s s1  contains operators from 考 s s   thus still being an obstacle for reaching the solution.
﹛an important point to notice  is that s1 and s1 are not necessarily states that the algorithm has visited before. groups of operators are simultaneously eliminated  an operation which interdependencies would prohibit had the operators were successively eliminated. compset effectively switches to another context which is mostly correct  in which the eliminated operators can be tried again.
1	empirical evaluation
the following algorithms were considered:
  stochastic hill climbing  shc  - starts from a random subset  iteratively picks a neighboring subset  differs in exactly one element  in random and moves there if it has a better or equal utility value. the simplicity of shc often misleads; several works  mitchell et al.  1; baluja  1  showed that shc does not fall from the complex ga mechanism. in the sat domain such stochastic local search  sls  methods have been shown to be comparable with state-of-the-art domain-specific algorithms  hoos and stutzle  1“  .
  tabu search  ts   glover and laguna  1  - examines the neighborhood of the current state for the best replacement. it moves to the chosen state even if it does not improve the current state  which might result in cycles. to avoid cycles  ts introduces the notion of a tabu list that stores the last t  tabu tenure  operators used. ts is prevented from using operators from the tabu list when it generates the neighborhood to be examined  unless certain conditions called aspiration criteria are met. in this paper we use a common aspiration criterion that allows operators which lead to better state than the best obtained so far.
  simulated annealing  sa   kirkpatrick et al.  1  - begins at high temperature which enables it to move to arbitrary neighboring states  including those which are worse than the current one. as the temperature declines  the search is less likely to choose a non-improving state  until it settles in a state which is a local minimum from which it cannot escape in low temperatures.
﹛to test the effectiveness of compset we have applied it to shc and ts. compset is not applicable to sa since sa begins with a high temperature at which it randomly moves far from the initial state. the concept of compset is to set new start points for the underlying algorithm and by randomly moving away from them sa defeats its purpose.
the algorithms were tested in the following domains:
  propositional satisfiability  sat  - the problem of finding a truth assignment that satisfies a given boolean formula represented by a conjunction of clauses  cnf  c1 ＿ ... ＿ cm. sat is a classic ssp since we look for a subset of variables that when assigned a true value  makes the entire formula true. the utility function is the number of unsatisfied clauses when assigning true to all variables in s:
u s  √ |{ci|ci is false unders 1 ≒ i ≒ m}|
the global minimum for u is 1  for satisfied formulas. a search algorithm using this utility function will attempt to maximize the number of satisfied clauses  which is a generalization of sat called max-sat. problem instances for sat were obtained from the satlib  hoos and stutzle  1“   repository of random 1-sat. we use problems from the solubility phase transition region1  cheeseman et al.  1 .
  max clique - another classic ssp  where the goal is to find the maximum subset of vertices that forms a clique in a graph. given a graph g =  v e  and a subset s   v   we define the following utility function:
1
	|v |   |s|	s is a clique
u s  √
	|v |   |s| + |v | + |s| ﹞  |s|   1    |es|	else
 a clique should be maximized but our implementation always minimizes u  therefore we use |v |   |s|. incomplete solutions are penalized by the number of additional edges they require for being a clique  |s| ﹞  |s|   1    |es|   plus a fixed value |v | that is used to separate them from the legal solutions. by striving to minimize u  the search algorithm finds feasible solutions first  and then continues by minimizing their size. the global minimum of u corresponds to the maximum clique. problem instances were obtained from the dimacs  benchmark for maximum clique.
  1 multidimensional knapsack  mkp  - the problem of filling m knapsacks with n objects. each object is either placed in all m knapsacks  or in none of them  hence  1  . the knapsacks have capacities of c1 c1 ... cm. each object is associated with a profit pi and m different weights  one for each knapsack. object i weighs wij when put into knapsack j. the goal is to find a subset of objects yielding the maximum profit without overfilling any of the knapsacks.
knapsack j is overfilled in state.
let k be the number of overfilled knapsacks. we define:
﹛k=1 k	k   1
the utility of feasible subsets is simply their profit  with minus sign for minimization purposes . infeasible solutions are penalized for each knapsack they overfill. problem instances for mkp were obtained from the or-library  beasley  1 .
  vertex cover - the goal is to find the smallest subset of vertices in a graph that covers all edges. given a graph g =  v e   we define:
1
	|s|	s covers all edges
u s  √
	|s| + |v | + |e es|	else
for legal vertex covers  u takes values less than or equal to |v |. incomplete solutions are penalized by the number of edges they do not cover  plus a fixed value |v | that is used to separate them from the legal solutions. the global minimum of u corresponds to the optimal vertex cover.
the complementary graphs of the instances from the original dimacs benchmark were taken  so that the known maximum clique sizes could be translated to corresponding minimum vertex covers1.
1	experimental methodology
we have tested five algorithms: shc  ts  sa  with t = 1 汐 = 1   compset over shc and compset over ts. ts was used with t = 1 for all domains other than sat  and t = 1 for sat. each run was limited to 1 u evaluations.
﹛all algorithms use random restart to escape from local optima when they have still not exhausted their evaluations quota. they use random restart also when there is no improvement over the last k steps. we use k = 1 for domains other than sat  and k = 1 for sat. sat is characterized by wide and frequent plateaus  frank et al.  1  therefore we chose higher values of t and k for it.
﹛1 runs of each algorithm were performed on each problem in the test sets. each run started from a random state  that was common to all algorithms. we measured the number of u evaluations needed to obtain the optimal solution in each run  as well as the time taken.
1	results
the results are summarized in tables 1  1  1 and 1. for brevity  we did not include the timing information in these tables. the considered algorithms do not introduce a significant overhead  so the execution time is a linear function of the number of u evaluations. the tables show the characteristics of the problem instance  followed by the number of successful runs  columns titled #ok  and the average number of u evaluations for each algorithm. a successful run is a run in which the algorithm has found the optimal solution within the limit. we tested the statistical significance of the improvement introduced by compset using the wilcoxon matched-pairs signed-ranks test with the extension by etzioni and etzioni  to cope with censored data1. a  +  sign in the sg. column between shc and compset/shc indicates that compset improved shc with p   1. a  -  sign indicates that shc performed better with p   1. a     sign indicates that the difference is not significant. whenever it is not possible to draw definitive conclusions since there is too much censored data  n/a appears. the same holds for the sg. column between ts and compset/ts.
﹛the superiority of compset over the other algorithms is striking  both in the number of evaluations  and the number and difficulty of instances solved. in the sat domain  the best performing algorithms are compset/ts and compset/shc. the average success ratio of compset/ts is 1% and of compset/shc is 1%. for comparison  the success ratios for sa  ts and shc are 1%  1% and 1% respectively. the speedup factor gained by using compset is as large as 1 for shc  instance uf1  and as large as 1 for ts  instance uf1 . note that these are lower bounds since shc and ts were terminated because of resource limit for some of the runs.
﹛the best performing algorithms in the max clique domain are compset/shc and shc with average success ratios of 1% and 1% respectively. the speed up factor of compset/shc over shc is as large as 1  instnace sanr1.1 .
in the knapsack domain  the best performing algorithm is
compset/shc with an average success ratio of 1%. for comparison  the success ratios for ts  sa  compset/shc and shc are 1%  1%  1% and 1% respectively. the speedup factor gained by using compset is as large as 1 for ts  instance weish1  and as large as 1 for shc  instance weish1 .
﹛the best performing algorithms in the vertex cover domain are compset/shc and shc with average success ratios of 1% and 1% respectively. sa is relatively close with 1% but ts is far behind with 1%  improved by compset to 1%. the speedup factor gained by using compset is as large as 1 for ts  instance hamming1  and as large as 1 for shc  instance sanr1.1 .
﹛another interesting statistics is the number of random restarts required by the underlying search algorithm and compset  as well as the number of operator eliminations performed by compset and how many operators they spanned. we have collected this data throughout 1 runs on the p hat1 vertex cover problem  a graph of 1 vertices. shc required 1 random restarts on average  in each run   while compset required only 1. compset has performed 1 operator eliminations  reversing the effect of 1 operators each time.
1	conclusions and future work
in this paper  we have provided useful insights into the domain of subset selection. we have realized that using local search  paths from complementary subsets to the solution must be distinct in terms of the operators used. this has led us to conclude that if the paths contain common operators  it may serve as an indication of a mistake. to test our conjecture  we introduced compset  a new guiding policy for local search algorithms in the context of ssp. the results show a significant improvement over both ts and shc by up to two orders of magnitudes.
﹛we currently in the process of running compset on other subset selection domains  progressing towards a better understanding of its behavior. one interesting direction is to research for ways to incorporate knowledge of the entire search
paths  instead of only the local minima at their end. in addition  it is beneficial to find out how characteristics of a
specific problem affect its performance. overall  the general idea of incorporating such ssp specific insights seems to be a promising lead to better subset selection algorithms.
1-sat instancesshcsg.compset/shctssg.compset/tssanamevarsclauses#okevals#okevals#okevals#okevals#okevalsuf111+11 11 1uf111 11 11uf111 11 1 11 1uf111 11 11 1uf111 11 11 1uf111 1+1 11+1 11 1uf111 1+1 11 1 11 1uf111 1+1 11+1 11 1uf111 1+1 11+1 11 1uf111 1+1 11+1 11 1uf111-+1 11 1+1 11 1uf111-+1 11 1+1 11 1uf111-+1 11 1+1 11 1uf111-+1 11 1+1 11 1uf111-+1 11 1+1 11 1uf111-+1 1-+1 11 1uf111-+1 1-+1 11 1uf111-+1 1-+1 11 1uf111-+1 1-+1 11 1uf111-+1 1-+1 11 1uf111-+1 1-+1 11 1uf111-+1 1-+1 11 1uf111-n/a1 11-+1 11 1uf111-n/a1 11-n/a1 11 1uf111-+1 11-+1 11 1uf111-+1 11-+1 11 1uf111-n/a1 11-n/a1 11 1uf111-+1 11-+1 11 1uf111-n/a1 11-+1 11 1uf111-n/a1 11-n/a1 11 1uf111-n/a1 11-n/a1 11 1uf111-n/a1-1-n/a1 11 1uf111-n/a1 11-n/a1 11 1uf111-n/a1 11-n/a1 11 1uf111-n/a1 11-n/a1 11 1table 1: sat: average over all 1 runs  including censored data
graphsshcsg.compset/shctssg.compset/tssaname|v |opt#okevals#okevals#okevals#okevals#okevalsbrock111 1+1 11-n/a1-1 1hamming111 11 1+1 11hamming111 11 1 1 11hamming111 1 1 11 1n/a1 11 1hamming111 1+1 11 1n/a1 11 1hamming111 1 1 11 1n/a1 11 1hamming111 1n/a1 11-n/a1-1 1johnson1-111 11 11johnson1-111+11 1+1 11 1johnson1-111 11 1 1 11johnson1-111 1 11 1+1 11 1p hat111 1+1 11 1n/a1 11 1p hat111 1+1 11 1n/a1 11 1p hat111 1+1 1-n/a1-1 1p hat111 1+1 11 1n/a1 11 1p hat111 1+1 1-n/a1-1 1p hat111 1+1 11-n/a1-1 1p hat111-n/a1 11-n/a1-1-p hat111 1+1 1-n/a1-1 1p hat111 1+1 11-n/a1-1 1sanr1.111 1+1 1-n/a1 11 1sanr1.111 1+1 1-n/a1-1 1sanr1.111 1+1 11-n/a1-1 1sanr1.111 1+1 11-n/a1-1 1table 1: maximum clique: average over all 1 runs  including censored data