 
     representing and reasoning about the knowledge an agent  human or computer  must have to accomplish some task is becoming an increasingly important issue in artificial intelligence  ai  research. to reason about an agent's beliefs  an ai system must assume some formal model of those beliefs. an attractive candidate is the deductive belief model: an agent's beliefs are described as a set of sentences in some formal language  the base sentences   together with a deductive process for deriving consequences of those beliefs. in particular  a deductive belief model can account for the effect of resource limitations on deriving consequences of the base set: an agent need not believe all the logical consequences of his beliefs. in this paper we develop a belief model based on the notion of deduction  and contrast it with current ai formalisms for belief derived from hintikka/kripke possible-worlds semantics for knowledge.1 
1. introduction 
     as ai planning systems become more complex and are applied in more unrestricted domains that contain autonomous processes and planning agents  there are two problems  among others  that they must address. the first is to have an adequate model of the cognitive state of other agents. the second is to form plans under the constraint of resource limitations: i.e.  an agent does not always have an infinite amount of time to sit and think of plans while the world changes under him; he must act. these two problems are obviously interlinked since  to have a realistic model of the cognitive states of other agents  who are presumably similar to himself  an agent must reason about the resource limitations they are subject to in reasoning about the world. 
in this paper we address both problems with reference to 
ai planning system robots and one part of their cognitive state  namely beliefs. our goal is to pursue what might be called robot psychology: to construct a plausible model of robot beliefs by examining robots' internal representations of the world. the strategy adopted is both descriptive and constructive. we examine a generic ai robot planning system  from now on we use the term agent  for commonsense domains  and isolate the subsystem that represents its beliefs. it is then possible to form 
'this paper describes results from the author's dissertation research. the work presented here was supported by grant n1-c-1from the office of naval research. an abstraction of the agent's beliefs  that is  a model of what the agent believes. this is the descriptive part of the research strategy. among the most important properties of this model is an explicit representation of the deduction of the consequences of beliefs  and so we call the model one of deductive belief. 
     it is assumed that the beliefs of the agent are about conditions that obtain in the planning domain  e.g.  what  physical  objects there are  what properties they have  and what relations hold between them. thus the descriptive model of deductive belief has an obvious shortcoming. although agents can reason about the physical world  they don't have any method for reasoning about the beliefs of other agents  or their own . by taking the descriptive model to be the way in which agents view other agents' beliefs  we can construct a more complex model of belief that lets agents reason about others' beliefs. this is the constructive part of the research strategy. 
     there are two main sections to this paper. in the first  the concept of a belief subsystem is introduced  and its properties are defined by its relationship to the planning system as a whole. here we discuss issues of deductive closure  completeness  and the resource limitations of the belief subsystem. we also characterize the constructive part of the model by showing how to expand a belief subsystem to reason about the beliefs of other agents. in the second section  we formalize the deductive belief model for the propositional case by introducing the belief logic b  and compare it with other formalizations of knowledge and belief. because the treatment here must be necessarily brief  throughout the paper proofs established by the author  but not yet published  are referenced. 
1. deductive belief 
     what is an appropriate model of belief for robot problemsolving systems reasoning about the world  which includes other robot problem-solving systems  in this section we discuss issues surrounding this question and propose a model of deductive belief as a suitable formal abstraction for this purpose. 
1 planning and belief: belief subsystems 
     a robot planning system  such as strips  must represent knowledge about the world in order to plan actions that affect the world. of course it is not possible to represent all the complexity of the real world  so the planning system uses some abstraction of real-world properties that are important for its task  e.g.  it might assume that there are objects that can be 

1 k. konolige 

stacked on each other in simple ways  the blocks-world domain . it is helpful to view the representation and deduction of facts about the world as a separate subsystem within the planning system; we call it the belief subsystem. in its simplest  most abstract form  the belief subsystem comprises a list of sentences about a situation  together with a deductive process for deriving 
consequences of these sentences. it is integrated with other processes in the planning system  especially the plan derivation process that searches for sequences of actions to achieve a given goal. 
     in a highly schematic form  figure 1 sketches the belief subsystem and its interaction modes with other processes of the planning system. the belief system is composed of the base sentences  together with the belief deductive process. belief deduction itself can be decomposed into a set of deduction rules  and a control strategy that determines how the deduction rules are to be applied and where their outputs will go when requests are made to the belief subsystem. 
¡¡¡¡¡¡there are two types of requests that result in some action in the belief subsystem. a process may request the subsystem to add or delete sentences in its base set; this happens  for example  when the plan derivation process decides what sentences hold in a new situation. although this process of belief updating and revision is a complicated research problem in its own right  we do not address it here  see doyle   l | for related research . the second type of request is a query as to whether a sentence is a belief or not. this query causes the control strategy to try to prove t hat the sentence is a consequence of the base set  using the deduction rules. it is this process of belief querying that we model in this paper. 
     we list here some further assumptions about belief subsystems. the internal language of a belief subsystem is a 
     formal language  which must include a  modal  belief operator  e.g.  a propositional or first-order modal language would be 
appropriate. it is assumed that there is a tarskian semantics for the language  that is  sentences of the language are either true or false of the real world. the belief subsystem doesn't inherently support the notion of uncertain beliefs  although this idea could be introduced if the internal language contained statements about uncertainty  e.g.  statements of the form p is true with probability 1. 
     the deduction rules of a belief subsystem are assumed to be sound  with respect to the semantics of the internal language   effectively computable  and to have bounded input. in particular  this forces deduction rules to be monotonic. it is our view that nonmonotonic or default reasoning should occur in the belief updating and revision process  rather than in querying beliefs. 
     the process of belief derivation is assumed to be total. this means that the answer to a query will be returned in a finite amount of time; i.e.  the belief subsystem cannot simply sit and 
continue to perform deductions without returning an answer. 
     it is possible to define several types of consistency for beliefs. deductive consistency requires that no sentence and its negation be simultaneous beliefs. logical consistency requires that there be a world in which all the beliefs are true. note that deductive consistency does not entail satisfiability  because the deductive process may not be complete. that is  a set of beliefs may be unsatisfiable and thus logically inconsistent  but  because of resource limitations  it may be impossible for an agent to derive a contradiction. deductive consistency is 
the appropriate concept for belief subsystems. the assertion that rational agents are consistent is compatible with  but not required by  the model. it gives rise to a slightly different axiomatization  see section 1 . 
     the results of this paper depend only on the most general features of a belief subsystem as depicted in figure 1: namely  that there is a formal internal language in which statements about the world are encoded; that there is a finite set of base beliefs in this language; and that there is some process of belief deduction that applies sound and effectively computable deduction rules to the base sentences at appropriate times  in response to requests by other processes in the planning 
system. 	a belief subsystem with these properties  along with the amplifications and restrictions given above  is a model of belief for planning agents  which we call deductive belief. 
1 resource limitations and deductive cloture 
     one of the key properties of belief deduction that we wish to include is the effect of resource limitations. if an agent cannot deduce all the logical consequences of his beliefs  
then we say that his deductive process is incomplete. logical incompleteness arises from two sources: an agent's deduction rules may be too weak  or his control strategy may perform only a subset of the derivations possible with the deduction rules. both these methods can be  and are  used by ai systems confronted with planning tasks under strict resource bounds. for several reasons  both conceptual and technical  we do not include incomplete control strategies in the deductive belief model. instead  we make the following assumption: 
closure property. the sentences derived in a belief subsystem are closed under its deduction rules. 
     one advantage of requiring that beliefs be closed under deduction is conceptual clarity and predictability. if beliefs are not closed  then there is some control strategy that guides 

the deductive process  making decisions to perform or not to perform deductions. if this control strategy uses a global effort bound  then behavior of such a subsystem is hard to predict. theoretically there may be a derivation of a sentence  but the control strategy in a particular case decides not to derive it  because it tried other derivations first. closed systems  on the other hand  behave more dependably. they are guaranteed to arrive at all derivations possible with the given deduction rules. 
     the concept of  belief is also complicated by the introduction of control strategy issues. for example  it makes a difference to the control strategy as to whether a sentence is a member of the base set  or obtained at some point in a derivation. one cannot simply say   agent s believes p* because such a statement doesn't give enough information about p to be useful. if p is derived at the very limit of deduction resources  then nothing will follow from it; if it is a base sentence  then it might have significant consequences. 
     in terms of formalizing the model of deductive belief  the assumption of closure is technically extremely useful. consider the task of formalizing a belief subsystem that has a complex global control strategy guiding the deductive process. to do this correctly  one must write axioms that describe the agendas  proof trees  and other data structures used by the control strategy  and how the control process guides deduction rules operating on these structures. reasoning about the deductive process involves making inferences using these axioms to simulate the deductive process  a highly inefficient procedure. by contrast  the assumption of closure leads to a simple formalization of belief subsystems that incorporates the belief deductive process in a direct way  the deductive belief logic  b  is presented in the next section . we have found complete proof techniques for b that involve running an agent's deductive system directly  in a manner similar to the semantic attachment methods of weyhrauch . 
     having argued that control strategies that use a global effort bound are undesirable  we now show that weak  but closed  deduction can have the same effect as control strategies with a local effort bound. we define a local bound as a restriction on the type of derivations allowed  without regard to other derivations in progress  i.e.  all derivations of a certain sort are produced. an example of this sort of control strategy is level-saturation in resolution systems. here we give a simpler example. 
     suppose an agent uses modus ponens as his only deduction rule  and has a control strategy in which only derivations using fewer than k applications of this rule are computed; this is a local effort bound. to model this situation with a closed belief subsystem  consider transforming the base set so that each sentence has an extra conjunct tacked onto it  the predicate dd 1   dd stands for  derivation depth  . instead of modus ponens  the belief subsystem has the following modified deduction rule: 

mp1 is sound and effectively computable  so it is a valid deduction rule for a belief subsystem. the closure of the base set of sentences of the belief subsystem under mp1 will be the 
k. konolige 1 
same  modulo the dd predicate  as the set of sentences deduced by the nonclosed control strategy of the agent. 
     the closure property  together with the assumption of totality for the belief derivation process  imply that the deduction rules are decidable for all base sets of sentences. 
1 views 
     up to this point  we have specifically assumed that agents don't have any deduction rules dealing with the beliefs of other agents. now  however  we form the constructive part of the deductive belief model: adding to the belief subsystem model so that an agent can reason about its own and other belief subsystems. 
     we can arrive at deduction rules that apply to beliefs by noting that the obvious candidate for the intended interpretation of the belief operator is another belief subsystem. that is  the modal sentence  s ¦Á is intended to mean  the sentence a is derivable in agent s's belief subsystem.  the new deduction rules that apply to belief operators will be judged sound if they respect this intended interpretation. for example  suppose a deduction rule states that  from the premise sentences  s p and  s} p q   the sentence  s q can be concluded. this is a sound rule if modus ponens is believed to be a deduction rule of s's belief subsystem  since the presence of p and pz q in a belief subsystem with modus ponens means that q will be derived. 
     we summarize by postulating the following property of deductive belief: 
recursion property. the intended model of the belief operator in tbe internal language of a belief 
subsystem is another belief subsystem. the intended model for an agent's own beliefs is his own belief subsystem. 
the recursion property of belief subsystems leaves a large amount of flexibility in representing nested beliefs. each agent might have his own representational peculiariaties for other agents' beliefs. an agent john might believe that sue has a set of deduction rules r1  whereas he believes that kim's rules are r1. in addition  john might believe that sue believes that kim's rules are r1. we call a belief subsystem as perceived through a chain of agents a view  and use the greek letter v to symbolize it. for example  john's perception of sue's perception of kim's belief subsystem is the view v = john  sue  kim. 
     obviously  some fairly complicated and confusing situations might be described with views  in which agents believe that other agents have belief subsystems of varying capabilities. some of these scenarios would be useful in representing situations that are of interest to ai systems  e.g.  an expert system tutoring a novice in some domain would need a representation of the deductive capabilities of the novice that would initially be less powerful and complete than its own  and could be modified as the novice learned about the domain. 
     having slated the recursion property  we now ask if there is a way to implement it within the confines of belief subsystems. at first glance it would seem so: suppose the agent s wishes to know whether he believes some statement p  i.e.  whether  s p is one of his own beliefs. if we assume he can query his belief subsystem  he simply submits p to it; if it answers  yes   he 

1 k. konolige 
1
1
undecidable. 
	k. konolige 	1 
agent's beliefs are also beliefs; a possible-worlds model cannot take into account resource limitations that might be present in an agent's belief system. the propositional modal logic that formalizes the possible-worlds model of belief is weak 1  that is  1 without the condition that all beliefs are true. we have proven that b reduces to this system under the following conditions: 
   j. the propositioned rules r v   for each view v are complete  and 
	1. 	belief recursion is unbounded. 
     in addition  if a modified form of b1 is used in which an agent doesn't know everything he doesn't believe  then under the same conditions b reduces to weak 1. thus  under the assumption of deductive completeness and an infinite resource bound  the b reduces to more familiar belief logics. 
1. conclusion 
     we have introduced the concept of robot belief subsystems parameterized by a finite set of base sentences and a set of deduction rules. this deductive belief model is a viable alternative to possible-worlds models of belief and has the attractive property of taking resource limitations into account in deriving consequences of beliefs. we have formalized the deductive belief model for the propositional case with the logic b  which is sound and complete with respect to our model. 
references 
	 l  	doyle  	j.  	 truth maintenance 	systems for problem 
solving   artificial intelligence laboratory technical report 
1   massachusetts institute of technology  cambridge  massachusetts  1 . 
 levesque  h. j..   a formal treatment of incomplete knowledge bases   flair technical report no. 1   
fairchild  palo alto  california  1 . 
 moore  r. c   reasoning about knowledge and action   artificial intelligence center technical note 1   sri international  menlo park  california  1 . 
 sato  m.  a study of kripke-type models for some modal 
logics by gentzen '$ sequential method  research institute for 
mathematical sciences  kyoto university  kyoto  japan  july 1. 
 smullyan  r. m.  first-order logic  springer-verlag  new york  1. 
 weyhrauch  r.   prolegomena to a theory of mechanized formal reasoning   artificial intelligence 1  1 . 

knowing intensional individuals  
and reasoning about knowing intensional individuals 
anthony s. maida 
center for cognitive science 
box 1  brown university 
providence  rhode island 1  usa 

abstract 
¡¡¡¡¡¡this paper outlines an approach toward computationally investigating the processes involved in reasoning about the knowledge states of other cognitive agents. the approach is fregean and is compared with the work of mccarthy and creary. we describe how the formalism represents the knowing of intensional individuals  coreferentiality  iterated propositional attitudes  and we describe plans to test  the scheme in the domain of speech act recognition. 
i introduction 
¡¡¡¡¡¡humans quite effectively reason about other humans' knowledge states  belief states  and states of wanting. unfortunately  the processes by which humans do this are not well understood. this paper outlines an approach toward computationally investigating these processes. this approach involves two components  the f i r s t of which involves ade-
quately representing knowledge about others' knowledge; and the second of which involves describing implementable processes by which it is possible to reason about such knowledge. our approach is fregean to the extent that the kind of cognitive system 
we propose puts emphasis upon the representation of fregean senses. however  the approach is not entire  y fregean because we do not represent denotations. this contrasts with the purely fregean 
approaches of mccarthy  1  and creary  1 . 
a. mccarthy's approach 
¡¡¡¡¡¡mccarthy begins with the simple example of pat knowing mike's phone number which is incidentally the same as mary's phone number  although pat does 
not necessarily know this. this example immediately exposes one of the d i f f i c u l t i e s of reasoning 
about knowledge  namely  the problem of inhibiting substitution of equal terms for equal terms in referentially opaque contexts. mccarthy's approach toward solving this problem involves e x p l i c i t l y representing senses and denotations. 
b. creary's extension 
¡¡¡¡¡¡creary extended mccarthy's system to handle iterated propositional attitudes. mccarthy's system f a i l s for iterated propositional attitudes because propositions are represented but not their concepts. creary's extensions involve introducing a hierarchy of typed concepts. thus for individuals such as the person mike  this scheme would have the person mike  the concept of mike  the concept of the concept mike  and so forth. the higher concept is the fregean sense of the lower concept  which reciprocally is the denotation of the higher concept. a similar situation holds for propositions. the hierarchy would consist of a truth value  the proposition which denotes the truth value  the concept of that proposition  and so on. this scheme allows for the representation of iterated propositional attitudes because all objects in the domain of discourse  most notablv propositions  have senses. 
c. the maida-shapiro position 
¡¡¡¡¡¡our starting point is the observation that knowledge representations are meant to be part of the conceptual structure of a cognitive agent  and therefore should not contain denotations. the thread of this argument goes as follows: a cognitive agent does not have direct access to the world  but only to his representations of the world. for instance  when a person perceives a physical object such as a tree  he is really apprehending his representation of the tree. hence  a knowledge representation that is meant to be a component of a  mind  should not contain 
denotations. a more elaborate statement of this position can be found in maida and shapiro  1  and the system for representing knowledge  called lambda net  described in the remainder of this paper is described in maida  1 . for our purposes  refraining from representing denotations achieves two goals: 1  the problem of substitution of equal terms for equal terms goes away because distinct terms are never equal; and 1  we can represent iterated propositional attitudes without 
invoking a hierarchy of types. 
ii lambda net 
a. intensional individuals 
¡¡¡¡¡¡there is a class of intensional individuals for which it can be said that they have a value as seen in assertions such as: 
a  john-bear knows where irving-bee is. 
b  john knows mike's phone number. 
c  john knows the mayor's name. 

a. maida 1 
what does john know in each of 	these sentences  

he knows the value of some i n t e n s i o n a l i n d i v i d u a l . we can c h a r a c t e r i z e these 	i n d i v i d u a l s by observing that 	they each involve a two-argument r e l a t i o n ; 	namely  	l o c a t i o n - o f   	phone-no-of  	and name-of  	r e s p e c t i v e l y . 	in each case  one argument is s p e c i f i e d ; namely: 	irving-bee  mike  and 	the 
mayor. the other argument is u n s p e c i f i e d . we make the assumption that context uniquely d e t e r mines the value of the unspecified argument. this value is the value of the i n t e n s i o n a l express i o n . the expressions themselves can now be represented as: 

b. 	knowing i n t e n s i o n a l 	i n d i v i d u a l s 
       since each of these expressions has a value  someone can know t h e i r values. we w i l l express t h i s via a r e l a t i o n c a l l e d  know-value-of  which takes a c o g n i t i v e agent and an intensional i n d i -
v i d u a l as arguments. to represent  john knows mike's phone number   we w r i t e : 
	g  	 know-value-of 	john 
	 the 	 lambda 	 x  	 phone-no-of mike x         
observe that we t r e a t p r o p o s i t i o n a l a t t i t u d e s   and a t t i t u d e s toward i n t e n s i o n a l i n d i v i d u a l s   as being r e l a t i o n a l and not as intensional operators. knowing is viewed as correct  but not necessarily j u s t i f i e d   b e l i e f . 
¡¡¡¡¡¡the meaning of  know-value-of  e n t a i l s that if john knows the value of mike's phone number  
and the value of mike's phone number is 1  then john  knows-that  the value of mike's phone 
number 	is 1. 
c. i t e r a t e d p r o p o s l t i o n a l a t t i t u d e s 
       reasoning about the knowledge states of others necessarily 	involves 	i t e r a t e d 	p r o p o s i t i o n a l a t t i t u d e s because the c o g n i t i v e agent 
doing the reasoning is generating b e l i e f s about another agent's knowledge s t a t e which i t s e l f may c o n t a i n b e l i e f s about the b e l i e f s of other cogn i t i v e agents. thus it is u s e f u l to show how lambda net represents such a s s e r t i o n s . creary  1  o f f e r s three semantic i n t e r p r e t a t i o n s of the ambiguous sentence: 
h  pat b e l i e v e s that mike wants to meet jim's w i f e . 
he suggests that the task of representing these i n t e r p r e t a t i o n s provides a strong 	t e s t of 	the r e p r e s e n t a t i o n . 	in order to allow the reader to compare the lambda net scheme w i t h creary's we l i s t the representations below. 	in each case  we give a rendering of 	the 	i n t e r p r e t a t i o n in e n g l i s h   our r e p r e s e n t a t i o n   and creary's r e p r e s e n t a t i o n . 
1  pat b e l i e v e s that mike wants to meet jim's w i f e as such. 
the reader should r e f e r to the o r i g i n a l papers  creary  1  and maida  1   to make the proper comparison. one of creary's goals is to stay w i t h i n the confines of a f i r s t - o r d e r l o g i c . lambda net does not have that c o n s t r a i n t . 
d. knowing c o r e f e r e n t i a l i n t e n s i o n a l 	i n d i v i d u a l s 
       to assert that two i n t e n s i o n a l i n d i v i d u a l s are c o r e f e r e n t   we w r i t e : 
i    	 equiv 	i n d i v i d u a l - 1 	l n d i v i d u a l - 1   
the 	r e l a t i o n   e q u i v   	is mnemonic f o r e x t e n s i o n a l equivalence  	and is the only reference to e x t e n s i o n a l i t y 	used in lambda net. 	one of our performance goals is to design a system which reacts a p p r o p r i a t e l y to a s s e r t i o n s of coreference. 	this involves s p e c i f y i n g a method -to t r e a t 	transparent 
and opaque r e l a t i o n s a p p r o p r i a t e l y . a r e l a t i o n   or v e r b   such as   d a i l   or   v a l u e - o f   is transparent whereas a r e l a t i o n such as  know  is opaque w i t h respect to i t s complement p o s i t i o n . we can express t h i s as: 
	 transparent 	d i a l   
	 transparent 	v a l u e - o f   
	  c o n d i t i o n a l l y - t r a n s p a r e n t know 	l s t - a r g 	1nd-arg  
  d i a l   and   v a l u e - o f   	are u n e q u i v i c a l l y 	transpare n t   whereas  know  	  e i t h e r know-that or knowv a l u e - o f   	is transparent on the c o n d i t i o n that 	the 1 a. maida agent doing the knowing also knows that two e n t i -
t i e s are c o r e f e r e n t . 	we can p a r t i a l l y express 
e. 	axiom of r a t i o n a l i t y 
       a system that reasons about the b e l i e f s of another c o g n i t i v e agent must make assumptions about the r a t i o n a l i t y of that agent in regard to 
what he considers l e g i t i m a t e r u l e s of inference. we s h a l l assume that a l l c o g n i t i v e agents u t i l ize the same set of inference schema. this is the axiom of r a t i o n a l i t y and we f u r t h e r assume that t h i s set of schema is e x a c t l y the set given in t h i s paper. a statement of the axiom of r a t i o n -
a l i t y 	i s : 
axiom of r a t i o n a l i t y - if a c o g n i t i v e agent knows or is capable of deducing a l l of the premises of a v a l i d i n f e r e n c e   then he is capable of deducing the conclusion of that i n f e r e n c e . 
the axiom of r a t i o n a l i t y enables one c o g n i t i v e agent to determine by i n d i r e c t s i m u l a t i o n whether another c o g n i t i v e agent is capable of i n f e r r i n g something. it i m p l i e s     i f i f i g u r e d it out and he knows what 1 know  then he can also f i g u r e 
it out if he thinks long enough.  we w i l l assume that the s i t u a t i o n s involved in knowing about t e l -
ephone numbers are simple enough to make p l a u s i ble the stronger r u l e     i f 1 f i g u r e d out and he knows what i know  then he has d e f i n i t e l y f i g u r e d i t o u t .   
t i n c t i n t e n s i o n a l i n d i v i d u a l s ; and  	1  the system 
must f e l i c i t o u s l y represent that another c o g n i t i v e 
agent can know the value of some i n t e n s i o n a l i n d i v i d u a l without the system i t s e l f necessarily knowing the value. lambda net has these c h a r a c t e r i s t i c s j u s t as creary's  1  does. however  lambda net o f f e r s the advantage of not invoking a h i e r a r chy of conceptual types in order to achieve these performance c h a r a c t e r i s t i c s . 
b. current 	work 
       we are implementing t h i s system to process speech acts using the general strategy described by a l l e n  1 . this approach views speech acts as communications between c o g n i t i v e agents about obstacles and p o t e n t i a l s o l u t i o n s to achieving some g o a l . therefore  comprehending and a p p r o p r i a t e l y r e a c t i n g to a speech act n e c e s s a r i l y requires the capacity to reason about another c o g n i t i v e agent's goals  wants   planning s t r a t e g y   and knowledge 
s t a t e s . 
