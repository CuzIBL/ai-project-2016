 
vision systems that have successfully supported nontrivial tasks have invariably taken advantage of constraints derived from the task and environment to increase reliability and lower the complexity of perception. we propose that it is possible to build a general purpose vision system  that is  one that can support a wide variety of tasks  and take advantage of such constraints. the central idea within our proposed architecture is the reactive skill. skills are concurrent control routines assembled at run time using instructions from a symbolic execution system. visual modules are used as resources in the construction of these skills. skills control the agent as continuous feedback loops but are constructed using discrete  symbolic instructions. the key to general-purpose vision is the ability to parametrize the primitive elements of the vision system and to compose visual and control routines in a variety of ways. we demonstrate the architecture in the context of an implemented example task of a robot collecting trash off a floor and depositing it in a garbage can. 
1 	introduction 
a longstanding goal in computer vision has been to develop general vision that can support a wide variety of goals  and inform the observer of the relevant ways in which the world does not meet its expectations  marr  1; tarr and black  1; brown  1 . on the other hand  it is advantageous for working systems to make as much use of the task and domain constraints as possible; vision systems that have successfully supported nontrivial tasks have invariably done so  see e.g.  horswill  1  
 dickmanns and graefe  1  . our goal is to build a 
　*this work was supported in part by onr contract n1-1  arpa contract n1-1 and nsf grant no. iri-1-a1. 
1 	action and perception 
system that is able to take advantage of task and environmental constraints  and yet is flexible enough to respond to dangerous situations and failed expectations  and can be quickly reprogrammed to do a different task or work in a different environment. the key to doing so is to build re-usable components that can be assembled in a task and environment specific way. 
we call our proposal the animate agent architecture. 
in the animate agent architecture  visual processing is encapsulated in modules called visual routines  after  ullman  1   that can be invoked as needed  and which are in turn composed of combinations of primitives called visual operators. at run-time  visual routines are paired with action routines designed to run in a tight servo loop with perceptual routines to meet the real-time constraints of the task. the resulting process is termed a reactive skill  slack  1; firby  1 . skills are invoked and can be terminated by a higher level system  that has access to the agent's knowledge of the task and environment  and is therefore well-suited to selecting the appropriate set of control and perceptual routines. for this component  we use the rap system  firby  1a; 1 . in contrast to the tight link between the control routines and visual routines  the rap system communicates with the skills via a relatively low bandwidth message-passing interface; we call the messages signals. we illustrate the architecture in the context of a trash collection task - one of the tasks of the 1 aaai robot competition. 
　our design seeks to take advantage of as many constraints as possible in any given situation and encode the constraints that change from situation to situation at a high level  where they are easily changed. these constraints allow us to sidestep many of the difficulties that increase the complexity and lower the accuracy of computer vision algorithms. for example  when looking for a trash can  the robot is able to avoid searching over all possible views  or  alternatively  using only features that are independent of perspective  because we have a good idea of the view from which the robot will see the trash can  assuming it is sitting upright on the floor . we are also able to limit the search to only locations in the scene consistent with an object sitting on the floor  and to combine the results of different algorithms to increase the speed and reliability of the search. 
　within this framework  not all sensing is task-driven. for instance  a visual operator monitoring possibly approaching objects to warn of possible collisions  such as nelson's directional divergence operator  nelson  1   can raise a signal if it senses motion that appears dangerous. if the taking avoidance action is of higher priority than the ongoing task  the rap system will interrupt the ongoing task to respond to the danger. if the response is known by the rap system not to conflict with the ongoing task  it is possible to run the processes in parallel. while the current implementation does not implement nelson's algorithm  it does have equivalent signals raised by the sonar sensors. 
　the entire trash collection task is described below  after a description of the proposed architecture and the robot that accomplishes the task. 
1 the animate agent architecture 
the overall system design for the animate agent architecture consists of two components: the rap reactive plan executor  and a control system based on reactive skills. the executor executes sketchy plans  that is  plans that leave many steps to be chosen as the actual situation unfolds. it makes these choices at run time when the true state of the world can be sensed. the output from the executor is a detailed set of instructions for configuring the skills that make use of the agent's sensors and effectors. 
1 the rap execution system 
the rap system expands vague plan steps into detailed instructions at run time by choosing an appropriate method for the next step from a preexisting library. by waiting until run time  knowledge of the situation will be direct rather than predicted and much of the uncertainty and incompleteness plaguing the planner will have disappeared. however  some uncertainty will always remain and incorrect methods will sometimes be chosen. the rap system copes with these problems by checking to make sure that each method achieves its intended goal and  if it doesn't  choosing another method and trying again. 
　in the rap system a task is described by a reactive action package  rap  which is effectively a context sensitive program for carrying out the task. the rap can also be thought of as describing a variety of plans for achieving the task in different situation. 
　the rap system  firby  1; 1b  carries out tasks using the following algorithm. first  a task is selected for execution and if it represents a primitive action  it is executed directly  otherwise its corresponding rap is looked up in the library. next  that rap's check for success is used as a query to the situation description and  if satisfied  the task is considered complete and the next task can be run. however  if the task has not yet been satisfied  its method-applicability tests are checked and one of the methods with a satisfied test is selected. finally  the subtasks of the chosen method are queued for execution in place of the task being executed  and that task is suspended until the chosen method is complete. when all subtasks in the method have been executed  the task is reactivated and its completion test is checked again. if all went well the completion condition will now be satisfied and execution can proceed to the next task. if not  method selection is repeated and another method is attempted. 
1 reactive skills 
the rap system refines tasks into detailed discrete actions at run time. however  one of the primary lessons of both recent ai research into robot control  agre and chapman  1  and recent vision research into active perception  bajcsy  1  is that actions must be dynamic processes tolerant of sensor error and changing surroundings: they must be reactive. it is not useful for a planning system to control a robot with steps like  move forward 1 feet  because the location of the robot may not be known exactly  the world model may be inaccurate  or an obstacle such as a person may suddenly appear  and so on. instead the goal must be given relative to something that can be sensed in the environment  and flexibility must be built in to the action in order to avoid unanticipated undesirable states. experience also shows that a fruitful way to think of such a control system is as a collection of concurrent  independent behaviors  brooks  1; slack  1 . such behaviors can be enabled in sets that correspond to the notion of discrete  primitive steps  that will reliably carry out an action in the world over some period of time. the rap system produces goaldirected behavior using this idea by refining abstract plan steps into a sequence of different configurations for a process-based control system. 
　the resulting interface between reactive execution and continuous control is illustrated in figure 1. it consists of instructions to enable  disable  and set the parameters of individual routines. thus  the rap system simply configures the control system and cannot change the routines available or the attributes that connect them. 
　control routines communicate with the rap system by sending signals. signals are typically low bandwidth messages such as  i've reached the position i was supposed to get to  or  i haven't made any progress in a while.  each routine will have a set of signals it sends under various circumstances. since routines do not typically know the reason they have been enabled  i.e.  the goal enabling the routine is known to the rap system but not the control system   the messages they send carry very little information when taken out of context. it 
firby  et al 


is up to the rap system to interpret these messages in light of the goals it is pursuing and the routines it has enabled. for example   i cannot move forward   might mean failure because there is an obstacle in the way if the robot is trying to cross a room; on the other hand  the same signal might mean success if the robot is supposed to move down a long hallway until it reaches the end. the task expansion structure built up by the rap system during execution is ideal for this interpretation. 
　for example  suppose the next routine to be carried out is to move to a trash can. this routine can be implemented using the fairly generic action routine  move in a given direction while avoiding obstacles.  the argument to this routine is the direction to move and that can be updated in real time by an active sensing routine that continually tracks the direction to the can. possible routines might track the trash can's shape  color  or motion  depending on the situation. it is up to the rap system to choose which is appropriate at run time. once the action and tracking routines have been enabled  the control system will move the robot towards the can whatever gets in the way. this control state will last indefinitely without further intervention as long as the tracked feature stays in view and the path toward it stays reasonably clear. 
1 	visual routines 
the visual system is based on a modular set of taskspecific modules  termed visual routines  designed to sense the information required by the reactive skills. these modules are in turn composed of a sequence of primitive visual processing steps called visual operators. the operators and routines are designed with re-use in mind. the intermediate representations passed between operators are designed to be as generic as possible  so that they may be shared by multiple operators. with a common language of intermediate representations  it 
1 	action and perception 

is posssible to compose many different routines with a much smaller number of primitive operators. while the routines are designed to be task-specific  as much as possible the task constraints are input as parameters  rather than being coded into the routine. 
　the base representation generated by the early visual system provided to the routines is a three-band  r g b  color gaussian pyramid. visual routines may access a rectangular subset of any level of the pyramid. all further representations are created on demand by visual operators. 
1 	chip 
the robot we have constructed as a testbed for our ideas is shown schematically in figure 1. it uses a three-wheeled omni-directional mobile base built by real world interface  rwi . stereo color cameras are mounted on top of the robot on a two degree of freedom computer-controlled pan-tilt platform. video signals are processed off-board by datacube image processing hardware attached to a host sun sparcstation. the robot sensors also include sonar sensors and infrared proximity sensors. the sonars can be turned to face the surface nearest the robot to minimize problems due to specular reflection. the infrared sensors are well equipped for detecting obstacles near the robot  their signal drops off quickly after about ten centimeters   and have failure modes that are complementary to the sonar sensors. for manipulation  chip is equipped with a heathkit hero arm and manipulator. force and contact sensors on the gripper provide tactile feedback. 
　microprocessors mounted on the robot run the sensors  do initial processing of the data they create  and skills that do not require visual processing  which is done offboard. the microprocessors share a small amount of memory to allow communication and synchronization. the on-board microprocessors talk via radio modem to a macintosh computer  which runs some control routines and the reactive planner. 
　the robot can be run in either a tethered mode or a non-tethered mode in which the cameras transmit video to the datacube and the macintosh workstation communicates with the on-board microprocessors via a radio link. 
the robot has enough freedom of movement to allow 

it to navigate through a considerable area within the computer science department. because people work within the robot's range of movement  the robot's world is always changing. 
chip's vision processing is distributed over a sun 
sparc-1 workstation and a datacube mv-1 image processing computer with a digicolor digitizer. the datacube is a pipelined processor that can perform many operations including convolutions and histogramming in real-time. the datacube server  kahn  1  allows multiple processes to access the datacube simultaneously from any machine on the network. the server allows us to implement our visual routines as separate processes potentially distributed across multiple machines while still allowing each visual routine to access data from the datacube. 
　there is a set of low-level visual operations that are widely used among the visual routines and/or are timecritical and so have been implemented on the datacube. these operations include the creation of color gaussian and laplacian pyramids  extracting subregions from a level of one of the pyramids  convolution  and histogramming  color histogram backprojection and motion detection. visual routines may access a rectangular subset of any level of the pyramid. all further representations are created on demand by visual operators. 
　during each visual routine's initialization a socket connection with the datacube server is established. when a visual routine requires a computation to be performed on the datacube it sends requests to the datacube server. the datacube server performs the requested operation and returns the output. the programming interface to the datacube server makes all server requests look like normal function calls. using this interface we are able to write visual routines that use the datacube without worrying about its internals. 
　visual routines communicate with the rest of the system through a mechanism called the message hub  kahn et a/.  1 . the message hub provides a reliable socket 
based communication link between processes with a simple programming interface. 
1 	experiment: the trash collection task 
in one of the 1 aaai events  robots competed to tidy up an office area littered with soda cans  styrofoam cups  and paper wads  collecting and depositing them in any nearby trash can. although chip did not win the event  it was the only robot to actually pick up a piece of trash and put it in a trash can. the rap system made it easy to decompose the task into a set of simpler vision  navigation  and manipulation goals  and to add new context-dependent methods for achieving them. at the top level  chip's goal in this example is  put-trashin-can . to an outside observer  the behavior looks like the sequence: 
 sequence 
 tl  scan-for-object trash    t1  go-to-object trash 1   
 t1  orient-to-object trash 1    t1  pickup-object trash   
 t1  scan-for-object trash-can   
 t1  go-to-object trash-can 1   
 t1  orient-to-object trash-can 1   
 t1  drop-held-object       
figure 1: method for achieving the top-level goal puttrash-tn-can  the numbers indicate distances from the 
denoted objects  in cm . 
find trash nearby - move to trash - align with trash - pick up trash - find trash can - travel to trash can - align with trash can - dump trash in can - turn back toward where trash was found - repeat 
　the method for achieving this goal is the sequence of subgoals in figure 1. all these goals lead to further subgoals; ultimately  primitive goals enable the robot's perceptual and behavioral routines. in total  there are 1 raps  five of which are specific to the trash collection task. the raps invoke a total of 1 reactive skills  five of which involve visual processing. the visual processing is made specific to the task by the models that are passed to the routines. the raps are described in more detail in  firby et at.  1 . 
　we'll examine two steps of the method to demonstrate chip's visual routines  and cooperative on-board/offboard perceptual and motor processes. for the subgoal scan-for-object  chip identifies and locates a nearby ob-
ject that can be segmented from the floor. the subgoal orient-to-object consists of precisely lining up with it to 
grab it. 
　when searching  chip alternates simple exploratory movements  e.g.  scanning its head  with calls to a visual routine  the small object finder  which finds and classifies small objects lying on surfaces with only fine  high spatial frequency  texture or none at all. the routine uses the datacube to grab an image  smooth it  and extract edges. the edges are closed into segments  which are then classified with a set of thresholds on size  aspect ratio  edge density  and gray-scale brightness. the process typically takes under four seconds for a high resolution image of a cluttered scene. accuracy is currently at 1 percent on a database of challenging test images  and almost perfect in actual runs of the task. 
　the distance and bearing of the object are computed from a single image using triangulation from the robot's known height and assuming that the object is on the floor. this calibration is cached for particular tilt angles of the camera. 
　for the goal orient-to-object  chip aligns itself directly in front of the target  at a distance of less than one 
firby  etal 

figure 1: image processing steps for identifying trash. upper left: high-resolution test image; upper right: edges from sobel operator after gaussian smoothing; lower left: edges closed into segments; lower right: small and large segments removed. 
meter. a visual tracking routine locates a piece trash repeatedly and generates a target location with which to align. the results are time-averaged to provide a stable and accurate reference point with which chip can align. 
　the tracking routine operates exactly like the object finder described above  except that the identification procedure operates only on a much smaller window  figure 1   at greater than one update per second. the 1d frame-to-frame image displacements of the tracked ob-
ject's centroid are fitted to a model of smooth image motion that determines placement of the next processing window  prokopowicz et a/.  1 . if the object is not in the window  the entire scene is searched. the primary advantage of tracking in this situation is to avoid confused behavior that can result from identifying different items at different stages of the routine. 
　when the rap system encouters the goal orient-toobject   it selects a method for achieving the goal accord-
ing to whether the object involved is a piece of trash or a trash can. the method for orienting to a piece of trash expands into two parallel goals: primitive-track-smallobject and primitive-orient-to-goal. the rap method for primitive-orient-to-goal enables a routine to watch for violations of a safety zone in front of the robot  using sonars  and a routine to orient the robot's position according to an input goal location. the method for primitive-track-small-object enables the trash-tracking visual routine to update that goal location continuously. the routines enabled for these goals run as separate processes. the orient-to-goal routines run on-board the robot using odometry to drive and turn to a certain dis-
1 	action and perception 

figure 1: chip's view of a can during alignment. inset: the can as it is tracked. only the image area in the white box is processed. 

figure 1: left: chip rolls forward after aligning with the can; right: chip's view as it picks up a can. 
tance and direction from the goal location. the trashtracking visual routine runs off-board and repeatedly computes a relative distance to the target which is turned into a new goal location and passed to the on-board orient-to-goal routine. to orient to the trash can  a different method is chosen by orient-to-object: enabling the same action routines used for orienting to trash but a different visual routine for updating the goal location using visual operators better for finding trash cans. 
　once chip has lined up with the object to be picked up  it rolls forward blindly until the led beam between its fingers is broken by an object  or nothing is encountered within a meter. chip is able to align itself and roll forward accurately enough to pick up trash almost every time  figure 1 . failures are noticed and retried. 

figure 1: edge models  taken from distances of 1 meters to 1 meters. 　the trash finder is designed to find easily-segmented objects known only by their class membership  e.g. pop cans of all types  crumpled paper of various shapes . we have developed a different routine for locating and tracking known objects that cannot necessarily be segmented from their background. we have had some success in this direction using a hausdorff model-matcher developed at cornell  huttenlocher and rucklidge  1 . we assume: 
1. color and edge models of the object from likely views 
 figure 1  
1. the object sits upright on the floor  and is shorter than the robot 
1. the floor is flat 
1. the robot's view of the the object suffers at most minor occlusion 
this last assumption is less limiting than one might expect  since the robot is free to roam about the room in search of the object. 
　using this approach  we are able to avoid searching over all possible views. we search locations in the scene consistent with an object sitting on the floor  and we combine the results of different algorithms to speed the search. 
the solution is as follows: 
1. using raps  scan the room from left to right  changing view enough to leave a reasonable overlap between views 
1. for each view  enable a visual routine that: 
 a  finds all regions of interest using color  swain and ballard  1l  
 b  runs a hausdorff edge matcher  huttenlocher and rucklidge  1  in each region of interest 
the robot takes advantage of the assumption that the object is on the floor by restricting the region searched vertically  according to the height of the object and the distance to the nearest model in the set. it also takes advantage of this assumption to restrict the range of scale searched by the edge matcher within each region of interest returned by the color matcher. doing so relies on the distance calibration over the image for the floor at the angle of view of the camera during the search. 
　the hausdorff metric allows one to measure the match of a template to an image  allowing minor deformations. the cornell software searches efficiently over all translations and scalings of the template  within controllable limits  to find the best fit. we have found that the process works somewhat more reliably if the range of scales is limited  to between 1 and 1 percent  in our case  and more models are used to span the scales actually encountered. we have had good results with match criteria that at least 1 percent of the scaled and translated model points lie within one pixel  across or diagonally  of some image point. a reverse match criteria that at least half the image edge points covered by the model lie within one pixel of some model point reduces false matches in dense parts of the edge image. 
　as efficient as the cornell software is  we have found it too slow over large  cluttered scenes  except at very low resolutions or with unrealistically tight match criteria. the relatively high complexity of the edge matching motivated our use of color  to screen possible locations prior to edge matching. the histogram back-projection technique assigns to each color pixel a rating according to how well it indicates the presence of the model. the histogram is computed in an 1 bit hue  saturation  value color space  with 1 bits resolution of hue and saturation  and 1 of value. the pixel saliency ratings are then averaged over 1 neighborhoods  and the peaks above a threshold are found. an area around the peak is cropped until the smoothed saliency value falls under 1 percent of the peak value. most of the image has values at or near zero  figure 1l   so the edge matching phase is greatly speeded up. the back-projection and edge-detection is computed on the datacube  and region cropping is done on the workstation  taking together less than 1 seconds. in a typical scene as shown here  matching against all models of a target  we use up to six  takes about 1 seconds. 
　in table 1 the search times and results are given as the task constraints are added one by one. in all cases  the 
firby  et al 


figure 1: top left: back-projected color model of a trash can into medium-resolution image; top right: peaks in image extracted for matching to edge models; bottom: model found. 


table 1: cost and accuracy of a 1-degree high resolution search for a trash can. each search used 1 overlapping views. image grabbing and color processing were done on a datacube image processor; edge matching on a sparc 1. times shown are elapsed seconds  including the time to re-orient the camera. 
trash can was correctly identified  but with fewer constraints the search took longer and resulted in more false positives. the technique is reliable  requires only modest computing resources  and can be extended to arbitrary objects - provided the task constraints hold  which are that the object is of restricted size  and sits on the floor in one of a small number of possible orientations. 
　the distance to an object found with the hausdorff matcher can be easily computed using the known distance to the original picture of the model and the scale at which the model matched the image. we compare this distance to the triangulated distance judged from the location of the model in the image  assuming that the object is on the floor with the robot. matches whose distances don't agree within 1 percent are rejected. 
　the robot has successfully found  picked up  and thrown away hundreds of pieces of trash  at rate of about 1 pieces per hour. we have been able to demonstrate increasing if not perfect reliability. one major impediment to high reliability in our current system is its rudimentary navigation abilities. chip does not try to remember where trash has been seen before and ignores small objects on the floor while moving around. these capabilities are being added now. 1 action and perception 

figure 1: chip's view of a trash-can during alignment. inset: match made to an edge model of the trash-can. 
1 related work 
kosecka and bajcsy  kosecka and bajcsy  1  have studied synthesizing complex behaviors from simpler components. they model their components using discrete events systems  des  theory  which allows them to predict the controllability of composite behaviors  that is  whether or not it is possible to reach a goal state from a given state in the system. we leave the decision of how to combine multiple concurrent skills to the programmer  and instead concentrate on languages to conveniently express which skills should be run in sequence or in parallel  and to build natural interfaces that allow visual operators and control routines to be parametrized and composed to perform a variety of tasks. aloimonos  aloimonos  1; 1  has been an outspoken advocate of what he calls the purposive approach to achieving general-purpose vision  which is generally consistent with the approach we have taken here. 
1 conclusion and future work 
this paper describes the design of an implemented architecture that both enables the use of context from the task 

and environment to simplify vision problems  and allows the components  visual operators and control routines  to be reconfigured to solve a variety of tasks with minimal reprogramming. it provides a framework for building vision systems that are robust and timely  and can be utilized for a variety of tasks. we have demonstrated an implemented system  important parts of which are available to others  that works on a trash collection task in an office environment  and have explained why the components can be re-used in another task or environment. 
　we are using the architecture to encode a number of different robot skills  including fetching tools and other objects under human guidance  and delivering items to people's desks. but one of the first tasks is to prepare for the next robot competition  in which the task will be to expanded to sorting recyclables from the trash. reprogramming chip to perform this task will involve merely acquiring models for the recycling bin  and modifying the put-trash-in-can ra p to choose a destination depending on the identity of the object picked up. 
