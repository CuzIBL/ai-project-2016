 
it is argued that in applications of concept learning from examples where not every possible category of the domain is present in the training set  i.e.  most real world applications   classification performance can be improved by integrating suitable discriminative and characteristic classification schemes. the suggested approach is to first discriminate between the categories present in the training set and then characterize each of these categories against all possible categories. to show the viability of this approach  a number of different discriminators and characterizers are integrated and tested. in particular  a novel characterization method that makes use of the information about the statistical distribution of feature values that can be extracted from the training examples is used. the experimental results strongly supports the thesis of the paper. 
1 	introduction 
most algorithms that learn from examples  e.g.  decision tree algorithms such as id1  quinlan  1  and c1 
 quinlan  1   nearest neighbor algorithms such as the ib family  aha et al.  1  and the backpropagation algorithm  rumelhart et al.  1    learn discriminative category descriptions. that is  they learn to discriminate between the categories present in the training set. as a consequence  the classification schemes computed by these algorithms suffer from an inability of detecting instances of categories not present in the set of training examples. instead  such instances are assigned to one of the categories actually represented in the training set  resulting in undesired misclassifications. although this problem often is ignored in machine learning research  it arises in most real world applications as these typically can be characterized as open domains  hutchinson  1 . we simply do not have complete information about the domain  e.g.  the actual number of categories is not known. 
1 	learning 
   for example  consider the decision mechanism in a coin-sorting machine of the kind often used in bank offices. its task is to sort and count a limited number of different coins  e.g.  a particular country's   and to reject all other coins. supposing that this decision mechanism is to be learned  it is for practical reasons impossible to train the learning system on every possible kind of coin  genuine or faked. rather  the system should be trained only on the kinds of coins it is supposed to accept. another example are decision support systems  for instance in medical diagnosis  where the cost of a misclassification often is very high - it is better to remain silent than to give an incorrect diagnosis. 
   as pointed out by smyth and mellstrom   it is necessary to learn characteristic descriptions to solve the problem. such a description characterizes the category with regard to all possible categories  regardless of their occurrence in the training set   and are thus  at least potentially  able to reject instances of categories not present in the training set. we will here call algorithms that learn characteristic descriptions characterizers and algorithms that learn discriminative descriptions discriminators. holte et al.  have shown that the cn1 algorithm can be modified to learn characteristic descriptions in the form of rule-based maximum specific descriptions. besides simple memorization of the training instances  without doing any generalization   constructing maximum specific description is the simplest way to learn characteristic descriptions; for each category  or disjunct  one just compute the minimum and maximum value of each feature from the training instances belonging to the category and then construct a description that only accepts instances within the multidimensional space delimited by these values.  for nominal features  allow only the values present in the training instances belonging to the category.  
　it is possible make distinctions between different kinds of characterizers in terms of how much they generalize. on one end we have those that learn the most general descriptions that do not cover any description of other concepts  e.g.  aq1  michalski and larson  1   which in some cases degenerates into discriminators  and on the 

other end we have those that  just as the above mentioned modification of cn1  learns the most specific descriptions possible. however  in most applications both these extreme approaches are inadequate as they tend to over- and under-generalize respectively. moreover  these approaches are very static in the sense that there is no way of controlling the degree of generalization. it is the author's belief that the ability to control the degree of generalization is essential in most real world applications. there is a trade-off between the number of re-
jected and misclassified instances that must be balanced in accordance with the constraints of the application. in some applications the cost of a misclassification is very high and rejections are desirable in uncertain cases  whereas in others the number of rejected instances belonging to categories present in the training set are to be kept low and a higher number of misclassifications are accepted. of course  it is always desirable to reject as many as possible of the instances of categories not present in the training set. 
　another disadvantage of current characterizes is that they are not as good as current discriminators to discriminate between the categories actually present in the training set. the reason is simply that often they do not take into account the information that can be extracted from the training data about those categories  or do so only to a certain extent  cf. aq1 . 
1 	integrating discrimination and characterization 
the weaknesses of algorithms learning discriminative descriptions correspond closely to the strengths of those that learn characteristic descriptions and vice versa. that is  discriminative descriptions are in general good at discriminating between categories present in the training set but are unable to identify instances of categories not present in the training set whereas characteristic descriptions are able to do this but are not as good to discriminate between the categories present in the training set. as a consequence  it would be useful to try to combine their strength and at the same time reduce their weaknesses. a general method for doing this is to first apply an algorithm that discriminates between the categories present in the training set  and then another one that characterizes these categories separately against all possible categories.1 thus  the learning is divided into two separate phases: 
1. discrimination between all known categories 
1. characterization of these categories against all possible categories. 
     1  there are obvious reason for doing it in this order. for instance  we do not know which entities  i.e.  disjuncts  to characterize before we have made the discrimination. also the classification would be less computationally efficient if we first test all the characteristic description and then discriminate between those that accepted the instance. 
　one instantiation of this general method would be to apply a discriminator  e.g.  id1  in the first phase and then compute the maximum specific description for each category/disjunct  i.e.  for each leaf  in the id1 case . classification  on the other hand  would consist of first applying the discriminative description to get a preliminary classification  and then apply the maximum specific description to decide whether to reject or accept the instance. in what follows we will refer to the multidimensional subspace of the instance space defined by a characteristic description  in this case the maximum specific description  as the acceptance region. thus  each category/disjunct has its own acceptance region. if the instance to be classified is outside the acceptance region  it will be rejected whereas if it is inside the acceptance region  it will be classified according to the preliminary classification made by the discriminator. 
　now  which discriminators and characterizers are suitable for integration  as indicated above  it is possible to use existing components  e.g.  id1 as discriminator and compute maximum specific descriptions for characterization. since most research on learning from examples has focused on creating discriminative descriptions  many good discriminators besides id1 have been invented. the problem is with the characterization phase. as pointed out earlier  the maximum specific description method is rather static in the sense that there is no way of controlling the degree of generalization  i.e.  the size of the acceptance regions. moreover  the method is very sensitive to noise. in the next section an example of a more noise-tolerant approach to characterization in which it is possible to control the degree of generalization will be described. 
1 	a novel approach to characterization 
the central idea of this method is to make use of statistical information concerning the distribution of the feature values hidden in the training data. two versions of this approach  below referred to as the sd approach  have been developed: one univariate that computes separate and explicit limits for each feature  just as methods based on maximum specific descriptions do  which is suitable for application to  e.g.  decision tree algorithms 1 and one multivariate method  able to capture covariation among two or more features  suitable for integration with discriminators that do not use rule- or tree-based representation  e.g.  nearest neighbor. 
1 	univariate version 
for every feature  and every category/disjunct  we compute a lower and an upper limit so that the estimated probability that a particular feature value  of an instance 
     1 the explicit limits make it possible to retain the tree structure induced by the discrimination algorithm  e.g.  id1  augmenting it with a subtree at each leaf that characterize the category/disjunct represented by that leaf. 
	davidsson 	1 
 belonging to this category/disjunct  belongs to the interval between these limits is  in this way we can control the degree of generalization and  consequently  the above mentioned trade-off by choosing an appropriate value. the lesser the  is  the more misclassified and less rejected instances. thus  if it is important not to misclassify instances and a high number of re-
jected instances are acceptable  a high value should be selected. 
　it turns out that only very simple statistical methods are needed to compute such limits. assuming that x is a normally distributed stochastic variable  we have that: 
where m is the mean  a is the standard deviation  and is a critical value depending on 
　in order to follow this line of argument we have to assume that the feature values of each category  or each disjunct if it is a disjunctive concept  are normally distributed. as indicated by the experiments below  this assumption seems not too strong for most applications. moreover  as we cannot assume that the actual values of m and  are known  they have to be estimated. a simple way of doing this is to compute the mean and the standard deviation of the training instances belonging to the category/disjunct.1 
1 	multivariate version 
to implement this method we will make use of multivariate methods to compute a weighted distance from the instance to be classified to the  center  of the category/disjunct. if this distance is larger than a critical value  dependent of   the instance is rejected. assuming that feature values are normally distributed within categories/disjuncts we have that the solid ellipsoid of x values satisfying 
has probability 	is the covariance matrix  x1 is the chi-square distribution  and 
p is the number features.  for more details  see for instance  johnson and wichern  1 .  thus  we have to estimate two parameters for each category/disjunct:  i   which contains the mean for each feature  and  ii  
which represents the covariance for each pair of features. in analogy with the univariate case  we estimate these parameters by computing the observed mean vector and covariance matrix of the training instances belonging to the category/disjunct. 
　the main limitation of both versions of the sd approach is that they are only applicable to numerical features. however  as the max approach is applicable also 
     1 it can be argued that this is a rather crude way of computing the limits. a more elaborate approach would be to compute confidence intervals for the limits and use these instead. this was actually the initial idea but it turned out that this only complicates the algorithm and does not increase the classification performance significantly. 
1 	learning 
to ordered and nominal features  it is possible to make sd more general by combining it with max to form a hybrid approach able to handle all kinds of ordered features. we would then use sd for numerical features and max for the rest of the features. 
1 	empirical results 
the ideas behind the sd approach emerged when working on a real world application concerning the problem of learning the decision mechanism in coin sorting machines described in the introduction. the application of the sd method to this problem was very successful and the results are presented briefly below. a more detailed presentation of this application can be found in  davidsson  1b . 
　in order to properly evaluate the sd approach it have to be applied to other data sets as well. at the uci repository of machine learning databases there are no examples of data sets of the desired kind  i.e.  data sets consisting of separate training set  with x categories  and test set  with more than x categories . however  we can simulate such data sets in the following way: take any data set  but leave out one  or more  category during training. then test the algorithm on instances from all of the categories. at best  the algorithm will classify all the instances belonging to categories present in the training set correctly and reject all those belonging to categories not present in the training set. this approach may at first sight seem somewhat strange as we actually know that there are  for instance  three categories of irises in the famous iris data set  anderson  1 . but how can we be sure that there only exist three categories  it might exist some not yet discovered species of iris. in fact  as pointed out earlier  in most real world applications it is not reasonable to assume that all relevant categories are known in advance and can be represented in the training set. 
　due to shortage of space  only the results of one experiment besides the coin sorting application will be presented here. although better classification performance was achieved using other data sets  i have chosen the iris data set because it is so well-known.  as discrimination between the three categories is considered relatively easy  it was somewhat surprising that this task turned out to be quite difficult.  results from several different data sets using different discrimination algorithms can be found in  davidsson  1a . we will begin with studying the behavior of the sd approach and then compare it with other approaches to learning characteristic descriptions. 
1 	the behavior of the sd approach 
in our experiments we have used re-implementations of two different discriminators  id1 and ibi  a basic nearest neighbor algorithm  aha et al.  1    and combined them with both the univariate and the multivariate version of sd. we will here only describe the results of 

figure 1: classification performance of id1-sduni as a function of  performance on categories present in the training set are shown in the left diagram and the category not present in the training set in the right diagram.  for each value  averages in percentages over 1 runs.  
id1 integrated with the univariate version  id1-sduni  and ib1 integrated with the multivariate version  ib1sdmulti   which are the most natural combinations. 
coin sorting application 
in the experiments two databases were used  one describing canadian coins contains 1 categories  1  1  1  1  1 cent  1 and 1 dollar   and one describing hong kong coins that also contains 1 categories  1  1  1  1 cent  1  1  and 1 dollar . all of the 1 attributes  diameter  thickness  permeability  and two kinds of conductivity  are numerical. the canada and hong kong databases were chosen because when using the manufacturer's current method for creating the rules of the decision mechanism  which is manual to a large extent   these coins have been causing problems. in each experiment 1 
 1  instances were randomly chosen for training and 1  1x1  for testing. 
　figure 1 shows the classification results of id1-sduni for some different value when training only on hong kong coins  which is the most difficult case . to begin with  we can see that all foreign coins  i.e.  the canadian coins  are rejected. however  there were some problems with misclassifications of hong kong coins. in this particular application there are some demands that must be met: at most 1% rejects of known types of coins and very few misclassifications  not more than 1% . for id1sduni  these requirements are met when is between 1 and 1. this clearly illustrates the advantage of being able to control the degree of generalization. 
　the results when applying ibl-sdmulti are even more pleasing  see figure 1 . in fact  it does not misclassify any instances! note  however  that very small a-values must be used to achieve excellent classification behavior. 
the iris database 
the iris database contains 1 types of iris plants  setosa  
versicolor or virginica  of 1 instances each. in each experiment the data set was randomly divided in half  with one set used for training and the other for testing. thus  1  1  instances were used for training and 1 figure 1: classification performance of ibl-sdmulti. 

figure 1: classification performance of id1-sduni. 
 1  for testing. figure 1 shows the classification results when the algorithms were trained on instances of iris setosa and iris virginica  the most difficult case . we see that by varying the it is possible to control the trade-off between the number of rejected and misclassified instances. it is possible to achieve almost zero misclassifications if we choose  =1  but then we get a rejection rate of over 1% also for the two known categories. 
figure 1 shows some more encouraging results for i b l -
sdmulti. compared to id1-sduni we see that by selecting an appropriate value it is always possible to achieve both more correct classifications and less misclassifications using the multivariate approach. since sd-multi creates acceptance regions that closer match the distribution of feature values  i.e.  regions that are smaller but still cover as many instances  fewer instances of categories not present in the training set are misclassified. 

figure 1: classification performance of ibl-sd-multi. 
	davidsson 	1 

table 1: results from training set containing hong kong coins  averages in percentages over 1 runs . 
1 	c o m p a r i s o n t o o t h e r a l g o r i t h m s 
we have compared six different algorithms: 
  the basic id1 algorithm  only discriminator  
  the basic ib1 algorithm  only discriminator  
  id1 combined with the maximum specific descrip-tion algorithm  id1-max  
  id1 combined with the univariate version of sd 
  ib1 combined with the multivariate version of sd 
  aq1c 
aq1  wnek et al.  1  is a system that learns decision rules from examples  and counterexamples . it has the ability to learn discriminative or characteristic descriptions depending on how parameters are set. it is by many considered as one of the best algorithms for learning characteristic descriptions. to get aq1c to learn characteristic descriptions a parameter called trim is set to spec  which means that the rules learned  are as specific as possible  involving the maximum number of extended selectors  each with a minimum of values  
 wnek et al.  1  page 1   
　when using the plain aq1c algorithm it is not possible to control the degree of generalization. however  for each classified instance aq1c also outputs a value between 1 and 1 reflecting how confident it is in the classification. thus  the decision to accept or reject an instance could be based on the degree of confidence  e.g.  accept if confidence   1 and reject if confidence   1. 
　in the following experiments two  or more  results of aq1c are presented:  i  the plain aql1c and  ii  the version s  using the confidence which seems to balance the trade-off best  i.e.  we try to present aq1c as favorable as possible . also  one or two a-values for each of the sd algorithms that performs  better  than  i  and  ii  respectively has been chosen. 
coin sorting application 
the results from the coin sorting application are presented in table 1. first  we see that of the discriminators  ib1 is slightly better than id1. as a result  the combined 
1
aq1c is a c language re-implementation of aq1. 
1 	learning 
table 1: results from training set containing instances of iris setosa and iris virginica  averages over 1 runs . 
algorithms using ib1 performs better than those using id1. both discriminators misclassify  of course  all foreign coins. the plain aq1c algorithm suffers from the same problem as id1-max  it rejects far too many instances of the categories present in the training set  the hong kong coins . however  when decreasing the confidence factor the number of misclassification becomes much too high. in short  the sd algorithms are clearly superior to both id1-max and aq1c for this problem. 
　the only confidence values  except from 1 that corresponds to plain aq1c  produced by aq1c were 1  1  1  1 and 1. therefore  these are the only relevant candidates. this is another disadvantage with aq1c - you cannot control the generalization completely  you are limited by the confidence values it produce and cannot choose a value in between. using sd on the other hand  you are able to choose any degree of generalization you want. 
　note also that the number of misclassifications for categories present in the training set is reduced when the discriminator is combined with a characterizer. 
the iris database 
table 1 shows the classification results when the algorithms were trained on instances of iris setosa and iris virginica. also in this experiment does ib1 discriminate slightly better than id1. we see that ib1-sdmulti 1 outperforms id1-max in every respect and that aq1c is not adequate for this task - it misclassifies far too many of the class not present in the training set. it is impossible to achieve less than 1% misclassifications. the only confidence values produced by aq1c were 1  1  1  1 and 1. 
1 	how to choose discriminator and characterizer 
in theory it is possible to combine an arbitrary discriminator with an arbitrary characterization algorithm. in practice  however  it is often necessary to take some constraints into consideration  such as  the representation language of the category descriptions. for instance  if 

you need to retain the tree structure constructed by a decision tree algorithm  you are forced to chose sduni or max for characterization rather than sdmulti. 
　another issue to take into consideration is whether the categories in the domain are likely to be of a dis-
junctive nature or not. if a category consists of clearly separate clusters of instances and we use a discriminator that does not learn explicitly disjunctive descriptions  e.g.  backpropagation and nearest neighbor algorithms  only one acceptance region is computed for the category. this acceptance region will then be unnecessarily large implying too many misclassified instances. 
1 	conclusions and future work 
we have argued that for most real world applications of learning from examples classification performance can be improved if discriminative and characteristic classification schemes are integrated. the former is used to discriminate between the categories present in the training set  and the latter to characterize these categories against all possible categories. a novel approach for characterization  the sd approach  was suggested. an important property of this approach is its ability to control the degree of generalization continuously which is crucial for most real world applications. finally  some experimental results were presented that supported the claims that classification performance can be improved by integrating discriminative and characteristic classification schemes and that the sd approach often is a good choice when selecting a characterizer. 
　some important issues for future work are:  i  developing strategies for selection of discriminators and characterizers   ii  developing characterizers in which it is possible to control the degree generalization also for nonnumeric features  and  iii  trying to shed some light on the underlying tension between discrimination and characterization. 
1 	acknowledgment 
i wish to thank r.s. michalski and e. bloedorn at the mlic for providing the aq1c software. 
