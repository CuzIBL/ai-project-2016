
recently  substantial efforts have been devoted to the subspace learning techniques based on tensor representation  such as 1dlda  ye et al.  1   dater  yan et al.  1  and tensor subspace analysis  tsa   he et al.  1 . in this context  a vital yet unsolved problem is that the computational convergency of these iterative algorithms is not guaranteed. in this work  we present a novel solution procedure for general tensor-based subspace learning  followed by a detailed convergency proof of the solution projection matrices and the objective function value. extensive experiments on realworld databases verify the high convergence speed of the proposed procedure  as well as its superiority in classification capability over traditional solution procedures.
1 introduction
subspace learning algorithms  brand  1  such as principal component analysis  pca   turk and pentland  1  and linear discriminant analysis  lda   belhumeur et al.  1  traditionally express the input data as vectors and often in a high-dimensional feature space. in real applications  the extracted features are usually in the form of a multidimensional union  i.e. a tensor  and the vectorization process destroys this intrinsic structure of the original tensor form. another drawback brought by the vectorization process is the curse of dimensionality which may greatly degrade the algorithmic learnability especially in the small sample size cases.
¡¡recently substantial efforts have been devoted to the employment of tensor representation for improving algorithmic learnability  vasilescu and terzopoulos  1 . among them  1dlda  ye et al.  1  and dater  yan et al.  1  are tensorized from the popular vector-based lda algorithm. although the initial objectives of these algorithms are different  they all end up with solving a higherorder optimization problem  and commonly iterative procedures were used to search for the solution. a collective problem encountered by their solution procedures is that the iterative procedures are not guaranteed to converge  since in each iteration  the optimization problem is approximately simplified from the trace ratio form argmax to the ratio
trace form argmaxuk tr   uktskuk  1 uktskpuk    in order to obtain a closed-formsolution for each iteration. consequently  the derived projection matrices are unnecessary to converge  which greatly limits the application of these algorithms since it is unclear how to select the iteration number and the solution is not optimal even in the local sense.
¡¡in this work  by following the graph embedding formulation for general dimensionality reduction proposed by  yan et al.  1   we present a new solution procedure for subspace learning based on tensor representation. in each iteration  instead of transforming the objective function into the ratio trace form  we transform the trace ratio optimization problem into a trace difference optimization problem maxuk tr ukt skp ¦Ësk uk  where ¦Ë is the objectivefunction value computed from the solution  of the previous iteration. then  each iteration is efficiently solved with the eigenvalue decomposition method  fukunaga  1 . a detailed proof is presented to justify that ¦Ë  namely the value of the objective function  will increase monotonously  and also we prove that the projection matrix uk will converge to a fixed point based on the point-to-set map theories  hogan  1 .
¡¡it is worthwhile to highlight some aspects of our solution procedure to general subspace learning based on tensor representation here:
1. the value of the objective function is guaranteed tomonotonously increase; and the multiple projection matrices are proved to converge. these two properties ensure the algorithmic effectiveness and applicability.
1. only eigenvalue decomposition method is applied foriterative optimization  which makes the algorithm extremely efficient; and the whole algorithm does not suffer from the singularity problem that is often encountered by the traditionalgeneralized eigenvaluedecomposition method used to solve the ratio trace optimization problem.
1. the consequent advantage brought by the sound theoretical foundationis the enhanced potentialclassification capability of the derived low-dimensional representation from the subspace learning algorithms.
¡¡the rest of this paper is organized as follows. section ii reviews the general subspace learning based on tensor representation  and then we introduce our new solution procedure along with the theoretical convergency proof in section iii. by taking the marginal fisher analysis  mfa  algorithm proposed in  yan et al.  1  as an example  we verify the convergency properties of the new proposed solution procedure and the classification capability of the derived lowdimensional representation is examined with a set of experiments on the real-world databases in section iv.
1 subspace learning with tensor data
in this section  we present a general subspace learning framework by encoding data as tensors of arbitrary order  extended from the one proposed by  yan et al.  1  and taking the data inputs as vectors. the concepts of tensor inner production  mode-k production with matrix  and mode-k unfolding are referred to the work of  yan et al.  1 .
1 graph embedding with tensor representation
denote the sample set as x =  x1 x1 ... xn  xi ¡Ê rm1¡Ám1¡Á...¡Ámn i = 1 ...n  with n as the total number of samples. let g = {x s} be an undirected similarity graph  called an intrinsic graph  with vertex set x and similarity matrix s ¡Ê rn¡Án. the corresponding diagonal matrix d and the laplacian matrix l of the graph g are defined as
		 1 
the task of graph embedding is to determine a lowdimensional representation of the vertex set x that preserves the similarities between pairs of data in the original highdimensional feature space. denote the low-dimensional embedding of the vertices as y =  y1 y1 ... yn   where
y is the embedding for the vertex xi  with the assumption that yi is the mode-k production of xi with a series of column orthogonal matrices 
y
where is anidentity matrix. to maintain similarities among vertex pairs according to the graph preserving criterion  yan et al.  1   we have
	= argmin	 1 
	= argmin 	 1 
uk|nk=1
where is a function that poses extra constraint for the graph similarity preserving criterion. here uk|nk=1 means the sequence u1  u1 to un and so for the other similar representations in the following parts of this work. commonly   may havetwo kinds of definitions. one is for scale normalization  that is 
	 	 1 
where b is a diagonal matrix with non-negative elements. the other is a more general constraint which relies on a new graph  referred to as penalty graph with similarity matrix sp  and is defined as

¡¡without losing generality  we assume that the constraint function is defined with penalty matrix for simplicity; and for scale normalization constraint  we can easily have the similar deduction for our new solution procedure. then  the general formulation of the tensor-based subspace learning is expressed as
p
	|k=1	=	=1
¡¡recent studies  shashua and levin  1   ye  1   ye et al.  1   yan et al.  1  have shown that dimensionality reduction algorithms with data encoded as high-order tensors usually outperform those with data represented as vectors  especially when the number of training samples is small. representing images as 1d matrices instead of vectors allows correlations between both rows and columns to be exploited for subspace learning.
¡¡generally  no closed-form solution exists for  1 . previous works  ye et al.  1   yan et al.  1  utilized iterative procedures to search for approximate solutions. first  the projection matrices u1 ... un are initialized arbitrarily; then each projection matrix uk is refined by fixing the other projection matrices u1 ... uk 1 uk+1 ... un and solving the optimization problem: k 
	u	= argmax	 1 
tr uktskpuk 

	= argmaxuk tr uktskuk 	 1 
where yik is the mode-k unfolding matrix of the tensor y i = xi ¡Á1 u1 ... ¡Ák 1 uk 1 ¡Ák+1 uk+1 ... ¡Án un and sk =

¡¡the optimization problem in  1  is still intractable  and traditionally its solution is approximated by transforming the objective function in  1  into a more tractable approximate form  namely  ratio trace form 
	uk  = argmaxk tr  uktskuk  1 uktskpuk  	 1 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡u which can be directly solved with the generalized eigenvalue decomposition method. however  this distortion of the objective function leads to the computational issues as detailed in the following subsection.
1 computational issues
as the objectivefunction in each iteration is changed from the trace ratio form  1  to the ratio trace form  1   the deduced solution can satisfy neither of the two aspects: 1  the objective function value in  1  can monotonously increase; and 1  the solution  u1 u1 ... un  can converge to a fixed point. in this work  we present a convergent solution procedure to the optimization problem defined in  1 .
1 solution procedure and convergency proof
in this section  we first introduce our new solution procedure to the tensor-based subspace learning problems  and then give the convergency proof to the two aspects mentioned above.
¡¡as described above  there does not exist closed-form solution for the optimization problem  1   and we solve the optimization problem also in an iterative manner. for each iteration  we refine one projection matrix by fixing the others and an efficient method is proposed for this refinement. instead of solving a ratio trace optimization problem  1  for an approximate solution  we transform the trace ratio optimization problem  1  into a trace difference optimization problem defined as
	uk  = argmax	 	 1 
where ¦Ë is the value of objective function  1  computed from the projection matrices of the previous iteration.
¡¡though the iterative proceduremay converge to a local optimum for the optimization problem  1   it can monotonously increase the objective function value as proved later  which directly leads to its superiority over the ratio trace based optimization procedure  since the step-wise solution of the latter is unnecessarily optimal for  1 .
¡¡we iteratively refine the projection matrices  and the detailed solution procedure to solve the tensor-based general subspace learning problem is listed in algorithm 1.
1 analysis of monotonous increase property
rewrite the objective function of  1  as

and then we have the theory as below:
¡¡theorem-1. by following the terms in algorithm-1 and eqn.  1   we have
g ut1 ... utk 1 utk 1 utk +1 ... utn 1  ¡Ü g ut1 ... utk 1 utk utk +1 ... utn 1 .  1 
proof. denote g u  = tr ut skp   ¦Ësk u  where
 
then we have
.
algorithm 1 . procedure to tensor-based subspace learning
1: initialization. initialize as arbitrary column orthogonal matrices. 1: iterative optimization.
for t=1 ... tmax  do for k=1 ... n  do
1. set.
1. compute sk and skp as in  1  based on the projection matrices ut1 ... utk 1 and.
1. conduct eigenvalue decomposition:
 
where vj is the eigenvector corresponding to the jth largest eigenvalue ¦Ëj.
1. reshape the projection directions for the sake of orthogonal transformation invariance:
 a  set;
 b  let   where
xik is the mode-k unfolding of the tensor xi;
 c  conduct eigenvalue decomposition as
	svui = ¦Ãi ui.	 1 
1. set the column vectors of matrix utk as the leading eigenvectors  namely .
end
if is set to 1 in this work   then break.
end
1: output the projection matrices uk=utk k=1 ... n.

moreover  from  it is easy to prove that
.
from algorithm 1  we have  and hence
.
then . as matrix sk is pos-
itive semidefinite 1  we have

that is 

¡¡¡¡¡¡1though sk may have zero eigenvalues  tr utktskutk  will be positive when is larger than the number of the zero eigenvalues.

figure 1: the value of the objective function  1  vs. iteration number.  a  usps database  b  orl database  and  c  cmu pie database. here the traditional method means the solution procedure based ratio trace optimization.¡¡from theorem-1  we can conclude that the value of the objective function monotonously increases.
1 proof of convergency
to prove the convergency of the projection matrices u1 u1 ... un  we need the concept of point-to-set map. the power set   ¦Ö  of a set ¦Ö is the collection of all subsets of ¦Ö. a point-to-set map ¦¸ is a function: ¦Ö ¡ú   ¦Ö . in our solution procedure to tensor-based subspace learning  the map from    can be considered as a point-to-set map  since each utk is invariant under any orthogonal transformation.
¡¡strict monotony. an algorithm is a point-to-set map ¦¸:¦Ö ¡ú   ¦Ö . given an initial point x1  an algorithm generates a sequence of points via the rule that xt ¡Ê ¦¸ xt 1 . suppose j : ¦Ö ¡ú r+ is continuous  non-negative function  an algorithm is called strict monotony if 1  y ¡Ê ¦¸ x  implies that j y  ¡Ý j x   and 1  y ¡Ê ¦¸ x  and j y  = j x  imply that y = x.
¡¡let set ¦Ö be the direct sum of the orthogonal matrix space   that is  the data space ¦Ö =
 	then the algo-
rithm 1 produces a point-to-set algorithm with respect to j x  = g uk|nk=1   and it can be proved to be strictly monotonic as follows.
¡¡theorem-1. the point-to-set map from algorithm 1 is strictly monotonic.
	proof.	from theorem-1  we have 	¡Ü
  and hence the first condition for strict monotony is satisfied. for the second condition  we take u1 as an example to prove that this condition is also satisfied. if
  then from the proof of theorem1  we have g ut1  = g ut1  with 
and sk skp computed from . from the proof of theorem-1  we can have that there only exists one orthogonal transformation1 between ut1 and ut1. as shown in algorithm 1  this kind of orthogonal transformation has been normalized by the reshaping step  hence we have ut1=ut1.
similarly  we can prove that  hence the second condition is also satisfied and the algorithm 1 is strictly monotonic.
¡¡theorem-1  meyer  1 . assume that the algorithm ¦¸ is strictly monotonic with respect to j and it generates a sequence {xt} which lies in a compact set. if ¦Ö is normed  then
.
¡¡from theorem-1  we can have the conclusion that the obtained  will converge to a local optimum  since the ¦Ö is compact and with norm definition.
1 experiments
in this section  we systematically examine the convergency properties of our proposed solution procedure to tensor-based subspace learning. we take the marginal fisher analysis  mfa  as an instance of general subspace learning  since mfa has shown to be superior to many traditional subspace learning algorithms such as linear discriminant analysis  lda ; more details on the mfa algorithm is referred to  yan et al.  1 . then  we evaluatethe classification capability of the derived low-dimensional representation from our solution procedure compared with the traditional procedure proposed in  ye et al.  1  and  yan et al.  1 . for tensor-based algorithm  the image matrix  1rd tensor  is used as input  and the image matrix is transformed into the correspondingvector as the input of vector-based algorithms.
1 data sets




figure 1: the step difference of the projection matrices vs. iteration number.  a d  usps database   b e  orl database  andthree real-world data sets are used. one is the usps handwritten dataset 1 of 1-by-1 images of handwritten digits with pixel values ranging between -1 and 1. the other two are the benchmark face databases  orl and cmu pie 1. for the face databases  affine transform is performed on all the samples to fix the positions of the two eyes and the mouth center. the orl database contains 1 images of 1 persons  where each image is normalized to the size of 1-by-1 pixels. the cmu pie  pose  illumination  and expression  database contains more than 1 facial images of 1 people. in our experiment  a subset of five near frontal poses  c f  cmu pie database.
 c1  c1  c1  c1 and c1  and illuminations indexed as 1 and 1 are used and normalized to the size of 1-by-1.
1 monotony of objective function value
in this subsection  we examine the monotony property of the objective function value from our solution procedure compared with the optimization procedure that step-wisely transforms the objective function into the ratio trace form. the usps  orl and pie databases are used for this evaluation. the detailed results are shown in figure 1. it is observed that the traditional ratio trace based procedure does not converge  while our new solution procedure guarantees the monotonous increase of the objective function value and commonly our new procedure will converge after about 1 iterations. moreover  the final converged value of the objective function from our new procedure is much larger than the value of the objective function for any iteration of the ratio trace based procedure.
1 convergency of the projection matrices
to evaluate the solution convergencyproperty compared with the traditional ratio trace based optimization procedure  we calculate the difference norm of the projection matrices from two successive iterations and the detailed results are displayed in figure 1. it demonstrates that the projection matrices converge after 1 iterations for our new solution procedure; while for the traditional procedure  heavy oscillations exist and the solution does not converge. as shown in figure 1  the recognition rate is sensitive to the oscillations caused by the unconvergent projection matrices and the classification accuracy is degraded dramatically.
table 1: recognition error rates  %  on the orl database.
methodg1g1g1w/o dr.111lda111mfa rt111mfa tr111tmfa rt111tmfa tr111table 1: recognition error rates  %  on the pie database.
methodg1g1g1w/o dr.111lda111mfa rt111mfa tr111tmfa rt111tmfa tr1111 face recognition
¡¡¡¡¡¡¡¡1 in1	1
iteration number
 a 11	1
iteration number
 b 1figure 1: recognition error rate  %  vs. iteration number.  a  orl database g1   and  b  cmu pie database g1 .in this subsection  we conduct classification experiments on the benchmark face databases. the tensor marginal fisher analysis algorithm based on our new solution procedure  tmfa tr  is compared with the traditional ratio trace based tensor marginal fisher analysis  tmfa rt   lda  ratio trace based mfa  mfa rt  and trace ratio based mfa  mfa tr   where mfatr means to conduct tensor-based mfa by assuming n=1. to speed up model training  pca is conducted as a preprocess step for vector-based algorithms. the pca dimension is set as n-nc  n is the sample number and nc is the class number   which is equivalent to the case for fisherface algorithm  belhumeur et al.  1 . the same graph configuration with nearest neighbor k = 1 for the intrinsic graph and kp = 1 for the penalty graph is adopted for all the mfa based algorithms. since the traditional tensor subspace learning algorithms do not converge  we terminate the process after 1 iterations.
¡¡for comparison  the classification result on the original gray-level features without dimensionality reduction is also reported as the baseline  denoted as 'w/o dr.' in the result tables. in all the experiments  the nearest neighbor method is used for final classification. all possible dimensions of the final low-dimensional representation are evaluated  and the best results are reported. for each database  we test various configurations of training and testing sets for the sake of statistical confidence  denoted asfor which x images of each subject are randomly selected for model training and the remaining y images of each subject are used for testing. the detailed results are listed in table 1 and 1. from these results  we can have the following observations:
1. tmfa tr mostly outperforms all the other methods concerned in this work  with only one exception for the case g1 on the cmu pie database.
1. for vector-based algorithms  the trace ratio based formulation  mfa tr  is consistently superior to the ratio trace based one  mfa rt  for subspace learning.
1. tensor representation has the potential to improve theclassification performance for both trace ratio and ratio trace formulations of subspace learning.
1 conclusions
in this paper  a novel iterative procedure was proposed to directly optimize the objective function of general subspace learning based on tensor representation. the convergence of the projection matrices and the monotony property of the objective function value were proven. to the best of our knowledge  it is the first work to give a convergent solution for general tensor-based subspace learning.
1 acknowledgement
the work described in this paper was supported by grants from the research grants council of the hong kong special administrative region and dto contract nbchc1 of usa.
