 
this paper presents research spanning the fields of computer vision  computer graphics  and artificial life  with implications for ai. although the modeling of all aspects of living systems is a worthwhile endeavor in its own right  the emphasis here will be on the modeling of animals  including humans  for computer vision. first  i present a new breed of artificial animals in a physics-based virtual marine world  whose muscle-actuated bodies harbor brains with motor  perception  behavior  and learning centers. in these mobile autonomous agents  sensorimotor control for the purposes of perceptuallyguided navigation employs on-board. active computer vision systems that continually analyze the visual world. second  turning my attention to human animals  i describe new algorithms that can construct artificial human heads with expressive faces. through the use of range scanners  generic biomechanical facial models may be automatically personalized to individuals. currently  artificial faces support a model-based approach to facial image analysis. in the future  it should be possible to incorporate brains and some degree of intelligence into them as well. 
1 	introduction 
modeling has been a central theme of artificial intelligence. this is very evidently so in computer vision  where the modcling of objects has preoccupied researchers since the dawn of the field some three decades ago . a good strategy for progress on the vision problem is through physics-based modeling  see  e.g.   . the idea is to incorporate principles of physical dynamics into conventional geometric models in order to be able to represent not only the shapes of objects  but their physical behaviors as well. indeed  many of the products of the physics-based modeling movement in computer graphics  see  e.g.    are also useful in vision. this fuels our long-standing philosophy in the visual modeling group at the university of toronto that vision and graphics are mutually converse problems  and that the two fields should advance synergistically. 
　in this paper  i will demonstrate that we have now taken an important next step and introduced some artifical life into the vision-graphics marriage. this enables us to set our sights on the most complex objects known-objects that are alive. i will demonstrate how recent advances in the emerging field of a-life are inspiring fresh approaches to computer vision.1 these advances center around the idea of artificial animals  or  animats  a term coined by wilson   in particular  i will review two of our ongoing research projects  which relate to the modeling of living systems for computer vision. the first involves the modeling of complete animals of nontrivial complexity on the evolutionary ladder  such as teleost fishes in their natural habitats  fig. 1 . the second project  involves the modeling of faces  a vitally important communicative medium of the most highly evolved living systems known-human beings  fig. 1 . 
　the presentation is in two parts. sections 1 and 1 present our work on artificial fishes and animal vision. the basic idea in a nutshell is to implement  entirely in software  realistic artificial animals and to give them the ability to locomote  perceive  and in some sense understand the realistic virtual worlds in which they are situated so that they may achieve both individual and social functionality within these worlds. to this end  each animat is an autonomous agent possessing a muscle-actuated body that can locomote and a mind with motor  perception  behavior  and learning centers. the animat is endowed with functional eyes that can image the dynamic 1d virtual world onto its 1d virtual retinas. the perceptual center of the animat's brain exploits  active vision  algorithms to continually process the incoming retinal image stream in order to make sense of what it sees and  hence  to purposefully navigate its world. i hope to begin to convince the uninitiated reader that it is now within our means to implement realistic virtual worlds inhabited by artificial animals rich enough to support some serious computer vision  and ai  research. 
　in the second part of the paper  sections 1 and 1 briefly present our work on physical and anatomical modeling of human faces. our goal has been to develop artificial faces that arc capable of synthesizing realistic facial expressions. at different levels of abstraction  these hierarchical models capture knowledge about facial expression from psychology  facial anatomy and facial tissue histology  and continuum biomechanics. 1 will show that a generic facial model of this sort can be  personalized   or made to conform closely to 
   'for an engaging survey of the a-life field  see  e.g.  s. levy  artificial life  pantheon  1 . journals such as artificial life and adaptive behavior document the state of the art. 
	terzopoulos 	1 

figure 1: artificial fishes in their physics-based virtual world as it appears to an underwater observer  monochrome version of original color images . top: the 1 reddish fish  center  are engaged in a mating ritual  the greenish fish  upper right  is a predator hunting for small prey  the remaining 1 fishes are feeding on plankton  white dots . dynamic seaweeds grow from the ocean bed and sway in the current. bottom: a predator shark stalking a school of prey. 

figure 1: an artificial face  monochrome version of original color images . the functional face model was constructed automatically from an rgb/range laser scan of an individual   george . artificial george is shown here engaged in synthesizing various facial expressions and pretending to read a technical paper about how he was constructed. 
individuals once the geometry and photometry of their faces has been captured by a range sensor. finally  i will describe how this sophisticated model can be used in a model-based analysis-by-synthesis strategy to analyze facial images and image sequences  an important computer vision problem that relates to visual communication. 
　section 1 concludes the paper with a brief discussion of where i hope that our approach will lead us and a preview of future work. 
1 artificial fishes 
imagine a virtual marine world inhabited by a variety of realistic fishes  fig. 1 . in the presence of underwater currents  the fishes employ their muscles and fins to swim gracefully around immobile obstacles and among moving aquatic plants and other fishes. they autonomously explore their dynamic world in search of food. large  hungry predator fishes stalk smaller prey fishes in the deceptively peaceful habitat. the sight of predators compels prey fishes to take evasive action. when a dangerous predator appears in the distance  similar species of prey form schools to improve their chances of survival. as the predator nears a school  the fishes scatter in terror. a chase ensues in which the predator selects victims and consumes them until satiated. some species of fishes seem untroubled by predators. they find comfortable niches and feed on floating plankton when they get hungry. driven by healthy libidos  they perform elaborate courtship rituals to attract mates. 
   each artificial fish is an autonomous agent with a deformable body actuated by internal muscles. the body also harbors eyes and a brain with motor  perception  behavior  and learning centers  as fig. 1 illustrates. through controlled muscle actions  artificial fishes are able to swim through simulated water in accordance with hydrodynamics. their functional fins enable them to locomote  maintain balance  and maneuver in the water. thus the artificial fish model captures not just 1d form and appearance  but also the basic physics of the animal and its environment. though rudimentary compared to real animals  the minds of artificial fishes are nonetheless able to learn some basic motor functions and carry out perceptually guided motor tasks. in accordance with their perceptual awareness of the virtual world  their minds arbitrate a repertoire of piscatorial behaviors  including collision avoidance  foraging  preying  schooling  and mating. 
　the details of the artificial fish model are presented in  1; 1 . i will summarize its main features in the remainder of this section. 
1 	motor system 
the motor system comprises the dynamic model of the fish including its muscle actuators and a set of motor controllers  mcs . fig. 1 a  illustrates the biomechanical body model which produces realistic piscatorial locomotion using only 1 lumped masses and 1 elastic elements. these mechanical components are interconnected so as to maintain the structural integrity of the body as it flexes due to the action of its 1 contractile muscles. 
tual fluid which induces local reaction forces normal to the 
1 	invited speakers 　artificial fishes locomote like real fishes  by autonomously contracting their muscles. as the body flexes it displaces virfigure 1: control and information flow in artificial fish. 
body. these hydrodynamic forces generate thrust that propels the fish forward. the model mechanics are governed by lagrange equations of motion driven by the hydrodynamic forces. the system of coupled second-order ordinary differential equations are continually integrated through time by a numerical simulator.1 
　the model is sufficiently rich to enable the design of motor controllers by gleaning information from the fish biomechanics literature. the motor controllers coordinate muscle actions to carry out specific motor functions  such as swimming forward  swim-mc   turning left  1 eft-turn-mc   and turning right  right-turn-mc . they translate natural control parameters such as the forward speed or angle of the turn into detailed muscle actions that execute the function. the artificial fish is neutrally buoyant in the virtual water and has a pair of pectoral fins that enable it to navigate freely in its 1d world by pitching  rolling  and yawing its body. additional motor controllers coordinate the fin actions. 
1 	perception system 
artificial fishes are aware of their world through sensory perception. the perception system relies on a set of on-board virtual sensors to gather sensory information about the dynamic environment. as fig. 1 b  suggests  il is necessary to model not only the abilities but also the limitations of animal perception systems in order to achieve natural sensorimotor behaviors. the perception center of the brain includes a perceptual attention mechanism  see  for a review of attention 
   1 the artificial fish model achieves a good compromise between realism and computational efficiency. for example  the implementation can simulate a scenario with 1 fishes  1 food particles  and 1 static obstacles at about 1 frames/sec  with wireframe rendering  on a silicon graphics r1 indigo1 extreme workstation. more complex scenarios with large schools of fish  dynamic plants  and full color texture mapped gl rendering at video resolution can take 1 seconds or more per frame. 
mechanisms  which allows the artificial fish to sense the world in a task-specific way  hence filtering out sensory information superfluous to its current behavioral needs. for example  the artificial fish attends to sensory information about nearby food sources when foraging. our early artificial fishes-those that are not equipped with the active vision system described in section 1-employ simulated perception  a  perceptual oracle  which satisfies the fish's sensory needs by directly interrogating the 1d world model-the fish can directly access the geometric and photometric information that is available to the graphics rendering engine  as well as object identity and dynamic state information about the physics-based world model. 
1 	behavior 
the behavior center of the artificial fish's brain mediates between its perception system and its motor system. an intention generator  the fish's cognitive faculty  harnesses the dynamics of the perception-action cycle. the innate character of the fish is established by a set of habits that determine if it is male or female  predator or prey  etc. at each simulation time step  the intention generator takes into account the habits of the fish and the incoming stream of sensory information to generate dynamic goals for the fish  such as to avoid an obstacle  to hunt and feed on prey  or to court a potential mate. it ensures that goals have some persistence by exploiting a single-item memory. persistence makes sustained behaviors such as foraging  schooling  and mating more robust. the intention generator also controls the perceptual attention mechanism. at every simulation time step  the intention generator activates behavior routines that attend to sensory information and compute the appropriate motor control parameters to carry the fish one step closer to fulfilling its current intention. the behavioral repertoire of the artificial fish includes primitive  reflexive behavior routines  such as obstacle avoidance  as well as more sophisticated motivational behavior routines such as schooling and mating whose activation depends on the dynamic mental state of the fish as represented by hunger  fear  and libido mental variables  see  for the details . 
terz1ul1s 

1 	learning 
the learning center of its mind enables the artificial fish to learn how to locomote through practice and sensory reinforcement. through optimization  the motor learning algorithms discover muscle controllers that produce efficient locomotion. muscle contractions that produce forward movements are  remembered . these half-success then form the basis for the fish's subsequent improvement in its swimming technique. their brain's learning center also enable these artificial animals to train themselves to accomplish higher level sensorimotor tasks  such as maneuvering to reach a visible target  see 1; 1  for the details . 
1 	modeling form and appearance 
of course  we want our animats to capture the form and appearance of real fishes with considerable visual fidelity. visual fidelity is especially important in the application of our animats to computer vision  which i will describe shortly. to this end  photographs of real fishes  such as the one shown in fig. 1 a   are converted into 1d spline  nurbs  surface body models  fig. 1 b  . the digitized photographs are analyzed semi-automatically using a  snake-grid  tool which is demonstrated in fig. 1 d-e  on a different fish image. the grid of snakes  floats freely over an image. the border snakes adhere to. intensity edges demarcating the fish from the background  and the remaining snakes relax elastically to cover the imaged fish body. this yields a smooth  nonuniform coordinate system  fig. 1 e   for mapping the texture onto the spline surface to produce the final texture mapped fish body model  fig. 1 c  . 
1 	animat vision 
the psychologist j.j. gibson stressed in pre-computational terms the importance of modeling the active observer situated in the dynamic environment . versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of bajcsy  and ballard  under the names of active perception and animate vision  respectively.1 
     1  animat  vision should not be confused with ballard's  animate  vision; the latter does not involve artificial animals. 
1 	invited speakers 
the active vision approach has developed into a prevailing paradigm  1; 1; 1; 1 . 
1 	problems with the  hardware vision  mindset 
as active vision is practiced in most labs today  however  it is in reality little more than technology-driven  hardware vision . to be sure  applications-minded researchers have legitimate reasons for building robot vision systems  but the necessary hardware paraphernalia-ccd cameras  pan-tilt units  ocular heads  frame-rate image processors  mobile platforms  manipulators  controllers  interfaces  etc.-can be expensive to fabricate or acquire commercially and a burden to maintain in good working order. 
　the animat vision methodology that we propose in  can potentially liberate a significant segment of the computer vision research community from the tyranny of robot hardware. it addresses the needs of scientists who are motivated to understand and ultimately reverse engineer the powerful vision systems found in higher animals. these researchers are well aware that animals do not have ccd chip eyes  electric motor muscles  and wheel legs. that is to say  they realize that readily available hardware systems are terrible models of biological animals. for lack of a better alternative  however  they have been struggling with inappropriate hardware in their ambition to understand the complex sensorimotor functions of real animals. moreover  their mobile robots typically lack the compute power necessary to achieve real-time response within a fully dynamic world while permitting active vision research of much significance. 
　artificial animals such as the fish are active  vehicles  in the sense of braitenberg . we believe that they are as appropriate for grounding active vision systems as are the hardware  mobots  that have come out of the situated robotics work of brooks and his group  1; 1  and have been an inspiration to numerous other robotics groups  see  e.g.  the compilation  1   . undeniably  however  efforts to equip realtime mobile robots with general-purpose active vision systems have been hampered by the hardware and the relatively modest abilities of on-board processors. 
1 	geta-life! 
i will now describe a zoomimetic approach to vision  which is made possible by the confluence of 
1. advanced physics-based artificial life modeling of animals  as i presented it in the previous section 
1. photorealistic computer graphics rendering  especially as implemented in modern 1d graphics workstations  and 
1. active computer vision algorithms. 
our animat vision approach offers an alternative strategy for developing biologically inspired active vision systems that circumvents the aforementioned problems of hardware vision. the animat vision concept is realized with realistic artificial animals and active vision algorithms implemented entirely in software on 1d graphics workstations. animat vision offers several additional advantages: 
  one can slow down the  cosmic clock  of the virtual world relative to the cycle time of the cpu on which it is being simulated. this increases the amount of computation that each agent can consume between clock ticks without unduly retarding the agent's responses relative to 
the temporal evolution of its virtual world. this in turn permits the development and evaluation of new and/or computationally complex active vision algorithms that are not presently implementable in real-time hardware. 
  the quantitative photometric  geometric  and dynamic information that is needed to render the virtual world is available explicitly. generally  the animats are privy to none of this environmental ground truth data  but must glean visual information the hard way-from their retinal image streams. however  the readily available ground truth can be extremely useful in assaying the effective accuracy of the vision algorithms or modules under development. 
　our challenge has been to synthesize a prototype active vision system for the fish animat which is based solely on retinal image analysis . the vision system should be extensible so that it will eventually support the broad repertoire of individual and group behaviors of artificial fishes. it is important to realize that we need not restrict ourselves to modeling the perceptual mechanisms of real fishes. in fact  the animat vision paradigm applies to animats that model any animal- even a human being-to the level of fidelity that the artificial fish models a real fish. indeed  the animat vision system that we have developed does not attempt to model fish perception . given a piscine animat that is an active observer of its world  we have found it interesting and challenging to endow the animat with human-like retinal imaging capabilities! 
1 active vision system 
the basic functionality of the animat vision system starts with binocular perspective projection of the color 1d world onto the animat's 1d retinas. retinal imaging is accomplished by photorealistic graphics rendering of the world from the animat's point of view. this projection respects occlusion relationships among objects. it forms spatially variant visual fields with high resolution foveas and low resolution peripheries. based on an analysis of the incoming color retinal image stream  the visual center of the animat's brain supplies saccade control signals to its eyes and stabilize the visual fields during locomotion  to attend to interesting targets based on color  and to keep a moving/deforming target fixated. the artificial fish is thus able to approach and track other artificial fishes using visual feedback. eventually its arsenal of active vision algorithms will enable it to forage  evade predators  find mates  etc. 
　fig. 1 is a block diagram of the active vision system showing two main modules that control foveation of the eyes and retinal image stabilization. 
eyes and retinal imaging 
the artificial fish has binocular vision. the movements of each eye are controlled through two gaze angles  1   1  which specify the horizontal and vertical rotation of the eyeball  respectively. the angles are given with respect to the head coordinate frame  such that the eye is looking straight ahead when1 = 1 = 1＜. 
　each eye is implemented as four coaxial virtual cameras to approximate the spatially nonuniform  foveal/peripheral imaging capabilities typical of biological eyes. fig. 1 a  shows an example of the 1 x 1 images that are rendered by the four coaxial cameras  using the gl library and sgi 

figure 1: the animat vision system. the flow of the algorithm is from right to left. a: update gaze angles  1  1  and saccade using these angles  b: search current level for model target and if found localize it  else search lower level  c: select level to be processed  see text   f: reduce field of view for next level and render  m: compute a general translational displacement vector  u  v  between images i t - 1  and i t } s: scale the color histogram of the model for use by the current level. 
graphics pipeline  of the left and right eye. the level / = 1 camera has the widest field of view  about 1＜  and the lowest resolution. the resolution increases and the field of view decreases with increasing /. the highest resolution image at level / = 1 is the fovea and the other images form the visual periphery. fig. 1 b  shows the 1 x 1 binocular retinal images composited from the coaxial images at the top of the figure. to reveal the retinal image structure in the figure  we have placed a white border around each magnified component image. 
　the advantages of the multiresolution retina are significant. vision algorithms which process the four 1 x 1 component images are 1 times more efficient than those that process a uniform 1 x 1 retinal image. 
foveation by color object detection 
the mind of the fish stores a set of color models of objects that are of interest to it. for instance  if the fish is by habit a predator  it would possess models of prey fish. the models are stored as a list of 1 x 1 rgb color images in the fish's memory. 
　to detect and localize any target that may be imaged in the low resolution periphery of its retinas  the animat vision system of the fish employs an improved version of a color indexing algorithm proposed by swain . since each model object has a unique color histogram signature  it can be detected in the retinal image by histogram intersection and 
terzopoulos 


1 	invited speakers 

terz1ul1s 

1 	artificial faces 
i will now shift gears and discuss the modeling of human animals  focusing on the important and challenging problem of modeling human faces. 
　the human face has attracted much attention in several disciplines  including psychology  computer vision  and computer graphics. psychophysical investigations clearly indicate that faces are very special visual stimulii. psychologists have studied various aspects of human face perception and recognition  1; 1 . they have also examined facial expression-the result of a confluence of voluntary muscle articulations which deform the neutral face into an expressive face. the facial pose space is immense. the face is capable of generating on the order of 1 distinguishable facial expressions with about 1 semantic distinctions. studies have identified six primary expressions that communicate anger  disgust  fear  happiness  sadness  and surprise in all cultures. ekman and friesen's  facial action coding system   facs  provides a quantification of facial expressions . the facs quantifies facial expressions in terms of 1  action units   au  involving one or more muscles and associated activation levels. 
1 a functional facial model 
we have developed a hierarchical model of the face which provides natural control parameters and is efficient enough to run at interactive rates . conceptually  the model decomposes into six levels of abstraction. these representational levels encode specialized knowledge about the psychology of human facial expressions  the anatomy of facial muscle structures  the histology and biomechanics of facial tissues  and facial skeleton geometry and kinematics: 
1. expression. at the highest level of abstraction  the face model executes expression  or phoneme  commands. for instance  it can synthesize any of the six primary expressions within a specific time interval and with a specified degree of emphasis. 
1. control a muscle control process  a subset of ekman and friesen's facs  translates expression  or phoneme  instructions into a coordinated activation of actuator groups in the facial model. 
1. muscles. as in real faces  muscles comprise the basic actuation mechanism of the model. each muscle model consists of a bundle of muscle fibers. when fibers contract  they displace their points of attachment in the facial tissue or the jaw. 
1. physics. the face model incorporates a physical approximation to human facial tissue. the tissue model is a lattice of point masses connected by nonlinear elastic springs. large-scale synthetic tissue deformations  sub-
ject to volume constraints  are simulated numerically by continuously propagating through the tissue lattice the stresses induced by activated muscle fibers. 
1. geometry. the geometric representation of the facial model is a non-uniform mesh of polyhedral elements whose sizes depend on the curvature of the neutral face. muscle-induced synthetic tissue deformations distort the neutral geometry into an expressive geometry. 
1 	invited speakers 
1. images. after each simulation time step  standard visualization algorithms implemented in dedicated graphics hardware render the deformed facial geometry in accordance with viewpoint  light source  and skin reflectance information to produce the lowest level representation in the modeling hierarchy  a continuous stream of facial images. 
　the hierarchical structure of the model encapsulates most of the complexities of the underlying representations  relegating the details of their computation to automatic procedures. at the higher levels of abstraction  our face model offers a semantically rich set of control parameters which reflect the natural constraints of real faces. 
　our synthetic facial tissue model is motivated by histology and tissue biomechanics. human skin has a nonhomogeneous and nonisotropic layered structure consisting of the epidermis  dermis  subcutaneous fatty tissue  fascia  and muscle layers. the synthetic tissue is a deformable lattice  an assembly of discrete finite elements  see  for the details . 
1 personalizing the functional model 
we have developed a highly automated approach to constructing realistic  functional models of human heads . these physics-based models are anatomically accurate and may be made to conform closely to specific individuals. currently  we begin by scanning a subject with a laser sensor which circles the head to acquire detailed range and reflectance information. next  an automatic conformation algorithm adapts a triangulated face mesh of predetermined topological structure to these data. the generic mesh  which is reusable with different individuals  reduces the range data to an efficient  polygonal approximation of the facial geometry and supports a high-resolution texture mapping of the skin reflectivity. 
　the conformed polygonal mesh forms the epidermal layer of a physics-based model of facial tissue. an automatic algorithm constructs the multilayer synthetic skin and estimates an underlying skull substructure with a jointed jaw. finally  the algorithm inserts synthetic muscles into the deepest layer of the facial tissue. these contractile actuators  which emulate the primary muscles of facial expression  generate forces that deform the synthetic tissue into meaningful expressions. to increase realism  we include constraints to emulate tissue incompressibility and to enable the tissue to slide over the skull as real skin does. 
　fig. 1 illustrates the aforementioned steps. the figure shows a 1＜ head-to-shoulder scan of a woman   heidi   acquired by a cyberware color 1d digitizer. the data set consists of a radial range map  fig. 1 a   and a registered rgb photometric map  fig. 1 b  . the range and rgb maps are high-resolution 1 x 1 arrays in cylindrical coordinates  where the x axis is the latitudinal angle around the head and the y axis is vertical distance. fig. 1 c  shows the generic mesh projected into the 1d cylindrical domain and overlayed on the rgb map. the triangle edges in the mesh are elastic springs  and the mesh has been conformed automatically to the woman's face using both the range and rgb maps . the nodes of the conformed mesh serve as sample points in the range map. their cylindrical coordinates and the sampled range values are employed to compute 1d euclidean space coordinates for the polygon vertices. in addition  the nodal coordinates serve as polygon vertex texture map coordinates 

into the rgb map. fig. 1  d  shows the 1d facial mesh with the texture mapped photometric data. 
　once we have reduced the scanned data to the 1d epidermal mesh of fig. 1 d   we can assemble a physics-based face model of heidi  including the synthetic skin  fig. 1 e  shows a skin patch undergoing deformation  and muscles  fig. 1 e  shows the contractile muscles  vectors  underneath the epidermal mesh . fig. 1 l g-j  demonstrates the resulting facial model producing animated expressions by contracting facial muscles. the same technique was applied to animate the facial model of george shown in fig. 1. 
1 model-based facial image analysis 
facial image analysis and synthesis is necessary for numerous applications. among them is low bandwidth teleconferencing which may involve the real-time extraction of facial control parameters from live video at the transmission site and the reconstruction of a dynamic facsimile of the subject's face at a remote receiver. teleconferencing and other applications require facial models that are computationally efficient and also realistic enough to accurately synthesize the various nuances of facial structure and motion. we have argued that the anatomy and physics of the human face  especially the arrangement and actions of the primary facial muscles  provide a good basis for facial image analysis and synthesis 1 . 
　the physics-based anatomically motivated facial model has allowed us to develop a new approach to the analysis of dynamic facial images for the purposes of estimating and resynthesizing dynamic facial expressions . part of the difficulty of facial image analysis is that the face is highly deformable  particularly around the forehead  eyes  and mouth  and these deformations convey a great deal of meaningful information. techniques for tracking the deformation of facial features include  snakes'1 . motivated by the anatomically consistent musculature in our model  we have considered the estimation of dynamic facial muscle contractions from video sequences of expressive faces. we have developed an analysis technique that uses snakes to track the nonrigid motions of facial features in video. features of interest include the eyebrows  nasal furrows  mouth  and jaw in the image plane. we are able to estimate dynamic facial muscle contractions directly from the snake state variables. these estimates make appropriate control parameters for resynthesizing facial expressions through a generic face model at real-time rates. 
　fig. 1 shows a plot of the estimated muscle contractions versus the frame number. they are input to the physics-based model as a time sequence. the model resynthesizes the facial expression. three rendered images are shown in fig. 1 c . 
	1 	where do we go from here  
i have presented two of our ongoing research projects that span the fields of computer vision  artificial life  and computer graphics. the projects are related in that they involve the development of nontrivial models of living systems. the models are founded upon computational physics. 1 have demonstrated applications of each of the models to computer vision. 
　to summarize  on one front  we have made significant progress over the past two years in developing a model which captures the essential features of most living systems- biomechanics  locomotion  perception  behavior  and learning. we are now using this piscatorial model as a situated 
	terz1ul1s 	1 


figure 1: dynamic facial image analysis and expression resynthesis. sample video frames with superimposed deformable contours tracking facial features;  a  intensity images with black snakes   b  image potentials with white snakes   c  facial model resynthesizes surprise expression from estimated muscle contractions. 
virtual robot for active vision research. it is my hope that the active vision systems that we are synthesizing in this way will be relevant in whole or in part to physical robotics. it seems to me that virtual animats in their dynamic world can serve as an useful proving ground for theories that profess sensorimotor competence in animal or robotic situated agents.1 
　on another front  we have been able to generate functional models of people's heads and use them for model-based facial image analysis. at the biomechanical and anatomical levels  the face models are as faithful to human faces as the fish models are to real fishes. unlike artificial fishes  however  the disembodied artificial heads do not yet have a much of a brain-just a simple motor center that blinks eyelids  moves the eyes  flexes the neck  and coordinates the facial muscles to produce meaningful expressions. 
　we would like to construct a brain model for the artificial heads that is as at least as comprehensive as the brain of the artificial fish  in the sense that it should be capable of dynamic perception and cognitively motivated behavior depending on environmental influences. we can get to this goal in an interesting way. 
an artificial mermaid: a logical next step  given what we have implemented already  would be to couple the artificial head model to the posterior of the artificial fish model with an anthropomorphic torso to create an artificial mermaid. the mermaid will be able to locomote through its virtual underwater world as the fishes now do. using an animat vision system  it will be able to perceive and interact with fish and other mermaids. unlike fish  however  the mermaid will have some of the expressive and behavioral capabilities of a human. far from being frivolous  this virtual creature could serve to smoothly bridge the gap between the humble cognitive abilities of an artificial fish and-maybe some day-human level 


figure 1: estimated facial muscle contractions plotted as time series. 
intelligence. 
the artificial life of a virtual human: naturally  an exciting long-term goal that should elicit little controversy within the al community is to develop an intelligent artificial human that is at least as convincing on virtual terra firma as our artificial fishes are in their virtual seas. it seems to me that we are well on the way to this end. to achieve this goal  however  more progress will obviously be necessary on several challenging problems  not excluding the ai problem. 
to be continued... 
explore  accumulate points  and avoid being killed. 
1 	invited speakers      1 doom vision: as a further test of the animat vision paradigm  we are developing an active vision system  similar to the one in artificial fishes  within an autonomous agent situated in a  doom  world   doom  is an amazingly popular video game . the challenge is for this agent to assume the role of the human doom player  given the same dynamic graphical image s  that a human player would see displayed on the screen. the agent's  brain  will interpret the incoming retinal image stream and generate motor commands  analogous to the keyboard commands a human player would issue  to locomote through the amusingly hostile doom world. a successful doom agent would be able to do what a skilled human player does- 
acknowledgements 
i would like to thank my students for their important contributions to the research described herein. xiaoyuan tu developed the artificial fish animat. tamer rabie developed the animat vision system. radek grzeszczuk developed the animat learning algorithms. yuenchen lee developed the physics-based face model. i thank keith waters for our years of collaboration on facial modeling. i also thank the many persons who have discussed and debated with me some of the ideas contained herein  especially john tsotsos  who also provided valuable comments on a draft   geoffrey hinton  and allan jepson. range/rgb facial data were provided courtesy of cyberware  inc.  monterey  ca. the research described herein was supported by grants from the natural sciences and engineering research council of canada and by the ark  autonomous 
robot for a known environment  project  which receives its funding from precarn associates inc.  industry canada  the national research council of canada  technology ontario  ontario hydro technologies  and atomic energy of canada limited. the author is a fellow of the canadian institute for advanced research. 
