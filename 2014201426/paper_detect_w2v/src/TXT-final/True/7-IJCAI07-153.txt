
a formula in first-order logic can be viewed as a tree  with a logical connective at each node  and a knowledge base can be viewed as a tree whose root is a conjunction. markov logic  richardson and domingos  1  makes this conjunction probabilistic  as well as the universal quantifiers directly under it  but the rest of the tree remains purely logical. this causes an asymmetry in the treatment of conjunctions and disjunctions  and of universal and existential quantifiers. we propose to overcome this by allowing the features of markov logic networks  mlns  to be nested mlns. we call this representation recursive random fields  rrfs . rrfs can represent many first-order distributions exponentially more compactly than mlns. we perform inference in rrfs using mcmc and icm  and weight learning using a form of backpropagation. weight learning in rrfs is more powerful than structure learning in mlns. applied to first-order knowledge bases  it provides a very flexible form of theory revision. we evaluate rrfs on the problem of probabilistic integrity constraints in databases  and obtain promising results.
1 introduction
recent years have seen the development of increasingly powerful combinations of relational and probabilistic representations  along with inference and learning algorithms for them. one of the most general representations to date is markov logic  which attaches weights to first-order formulas and views them as templates for features of markov random fields  richardson and domingos  1 . while markov logic may be the language of choice for many applications  its unification of logic and probability is incomplete. this is because it only treats the top-level conjunction and universal quantifiers in a knowledge base as probabilistic  when in principle any logical combination can be viewed as the limiting case of an underlying probability distribution. in markov logic  disjunctions and existential quantifiers remain deterministic. thus the symmetry between conjunctions and disjunctions  and between universal and existential quantifiers  is lost  except in the infinite-weight limit .
　for example  an mln with the formula r x  … s x  can treat worlds that violate both r x  and s x  as less probable than worlds that only violate one. since an mln acts as a soft conjunction  the groundings of r x  and s x  simply appear as distinct formulas.  mlns convert the knowledge base to cnf before performing learning or inference.  this is not possible for the disjunction r x  ‥ s x : no distinction is made between satisfying both r x  and s x  and satisfying just one. since a universally quantified formula is effectively a conjunction over all its groundings  while an existentially quantified formula is a disjunction over them  this leads to the two quantifiers being handled differently.
　this asymmetry can be avoided by  softening  disjunction and existential quantification in the same way that markov logic softens conjunction and universal quantification. the result is a representation where mlns can have nested mlns as features. we call these recursive markov logic networks  or recursive random fields  rrfs  for short.
　rrfs have many desirable properties  including the ability to represent distributions like noisy dnf  rules with exceptions  and m-of-all quantifiers much more compactly than mlns. rrfs also allow more flexibilty in revising firstorder theories to maximize data likelihood. standard methods for inference in markov random fields are easily extended to rrfs  and weight learning can be carried out efficiently using a variant of the backpropagation algorithm.
　rrf theory revision can be viewed as a first-order probabilistic analog of the kbann algorithm  which initializes a neural network with a propositional theory and uses backpropagation to improve its fit to data  towell and shavlik  1 . a propositional rrf  where all predicates have zero arity  differs from a multilayer perceptron in that its output is the joint probability of its inputs  not the regression of a variable on others  or  in the probabilistic version  its conditional probability . propositional rrfs are an alternative to boltzmann machines  with nested features playing the role of hidden variables. because the nested features are deterministic functions of the inputs  learning does not require em  and inference does not require marginalizing out variables.
　the remainder of this paper is organized as follows. we begin by discussing mlns and their limitations. we then introduce rrfs along with inference and learning algorithms for them  compare them to mlns  and present preliminary experimental results.
1 markov logic networks
a markov logic network  mln  consists of a set of first-order formulas and weights  { wi fi }  that serve as a template for constructing a markov random field. each feature of the markov random field is a grounding of one of the formulas. the joint probability distribution is therefore:

where ni is the number of true groundings of the ith formula given this assignment  and z is a normalization constant so that the probabilities of all worlds sum to one. richardson and domingos  show that  in finite domains  this generalizes both first-order logic and markov random fields.
　another way to think about a markov logic network is as a  softening  of a deterministic knowledge base. a first-order knowledge base can be represented as a conjunction of all groundings of all of its formulas. mlns relax this conjunction by allowing worlds that violate formulas  but assigning a per-grounding penalty for each violated formula. worlds that violate many formulas are therefore possible  but less likely than those that violate fewer. in this way  inconsistent knowledge bases can still be useful.
　however  while mlns soften conjunctions and universals  disjunctions and existentials remain deterministic. just as in mlns the probability of a world decreases gradually with the number of false groundings of a universally quantified formula  so the probability should increase gradually with the number of true groundings of an existentially quantified formula. rrfs accomplish this.
1 recursive random fields
a recursive random field  rrf  is a log-linear model in which each feature is either an observable random variable or the output of another recursive random field. to build up intuition  we first describe the propositional case  then generalize it to the more interesting relational case. a concrete example is given in section 1  and illustrated in figure 1.
1 propositional rrfs
while our primary goal is solving relational problems  rrfs may be interesting in propositional domains as well. propositional rrfs extend markov random fields and boltzmann machines in the same way multilayer perceptrons extend single-layer ones. the extension is very simple in principle  but allows rrfs to compactly represent important concepts  such as m-of-n. it also allows rrfs to learn features via weight learning  which could be more effective than current feature-search methods for markov random fields. the probability distribution represented by a propositional rrf is as follows:

where z1 is a normalization constant  to ensure that the probabilities of all possible states x sum to 1. what makes this different from a standard markov random field is that the features can be built up from other subfeatures to an arbitrary number of levels. specifically  each feature is either: fi x  = xj  base case   or
		 recursive case 
　in the recursive case  the summation is over all features fj referenced by the  parent  feature fi. a child feature  fj  can appear in more than one parent feature  and thus an rrf can be viewed as a directed acyclic graph of features. the attribute values are at the leaves  and the probability of their configuration is given by the root.  note that the probabilistic graphical model represented by the rrf is still undirected. 
　since the overall distribution is simply a recursive feature  we can also write the probability distribution as follows:
p x = x  = f1 x 
　except for z1  the normalization of the root feature  f1   the per-feature normalization constants zi can be absorbed into the corresponding feature weights wki in their parent features fk. therefore  the user is free to choose any convenient normalization  or even no normalization at all.
　it is easy to show that this generalizes markov random fields with conjunctive or disjunctive features. each fi approximates a conjunction when weights wij are very large. in the limit  fi will be 1 iff the conjunct is true. fi can also represent disjuncts using large negative weights  along with a negative weight for the parent feature fk  wki. the negative weight wki turns the conjunction into a disjunction just as negation does in de morgan's laws. however  one can also move beyond conjunction and disjunction to represent m-of-n concepts  or even more complex distributions where different features have different weights.
　note that features with small absolute weights have little effect. therefore  instead of using heuristics or search to determine which attributes should appear in which feature  we can include all predicates and let weight learning sort out which attributes are relevant for which feature. this is similar to learning a neural network by initializing it with small random values. since the network can represent any logical formula  there is no need to commit to a specific structure ahead of time. this is an attractive alternative to the traditional inductive methods used for learning mrf features.
　an rrf can be seen as a type of multi-layer neural network  in which the node function is exponential  rather than sigmoidal  and the network is trained to maximize joint likelihood. unlike in multilayer perceptrons  where some random variables are inputs and some are outputs  in rrfs all variables are inputs  and the output is their joint probability. in other ways  an rrf resembles a boltzmann machine  but with the greater flexibility of multiple layers and learnable using a variant of the back-propagation algorithm. rrfs have no hidden variables to sum out  since all nodes in the network have deterministic values  making inference more efficient.
1 relational rrfs
in the relational case  relations over an arbitrary number of objects take the place of a fixed number of variables. to allow parameter tying across different groundings  we use parameterized features  or parfeatures. we represent the parameter tuple as a vector  g  whose size depends on the arity of the parfeature. note that g is a vector of logical variables  i.e.  arguments to predicates  as opposed to the random boolean variables x  ground atoms  that represent a state of the world. we use subscripts to distinguish among parfeatures with different parameterizations  e.g.  represent different groundings of the ith parfeature.
each rrf parfeature is defined in one of two ways:
	 base case 
  recursive case 
the base case is straightforward: it simply represents the truth value of a ground relation  as specified by x . there is one such grounding for each possible combination of parameters  arguments  of the parfeature. the recursive case sums the weighted values of all child parfeatures. each parameter gi of a child parfeature is either a parameter of the parent feature  gi  or a parameter of a child feature that is summed out and does not appear in the parent feature  gi  .  these parameters are analogous to the parameters that appear in the body but not the head of a horn clause.  just as sums of child features act as conjunctions  the summations over g parameters act as universal quantifiers with markov logic semantics. in fact  these generalized quantifiers can represent m-of-all concepts  just as the simple feature sums can represent m-of-n concepts.
　the relational version of a recursive random field is therefore defined as follows:
p x = x  = f1 x 
where x is the set of all ground relations  e.g.  r a b   s a    x is an assignment of truth values to ground relations  and f1 is the root recursive parfeature  which  being the root  has no parameters . since f1 is a recursive parfeature  it is normalized by the constant z1 to ensure a valid probability distribution.  as in the propositional case  all other zi's can be absorbed into the weights of their parent features  and may therefore be normalized in any convenient way. 
　any relational rrf can be converted into a propositional rrf by grounding all parfeatures and expanding all summations. each distinct grounding of a parfeature becomes a distinct feature  but with shared weights.
1 rrf example
to clarify these ideas  let us take the example knowledge base from richardson and domingos . the domain consists of three predicates: smokes g   g is a smoker ; cancer g   g has cancer ; and friends g h   g is a friend of h . we abbreviate these predicates as sm g   ca g   and fr g h   respectively.
　we wish to represent three beliefs:  i  smoking causes cancer;  ii  friends of friends are friends  transitivity of friendship ; and  iii  everyone has at least one friend who smokes.  the most interesting belief from richardson and domingos   that people tend to smoke if their friends do  is omitted here for simplicity.  we demonstrate how to represent these beliefs by first converting them to first-order logic  and then converting to an rrf.
　one can represent the first belief   smoking causes cancer   in first-order logic as a universally quantified implication:  g.sm g    ca g . this implication can be rewritten as a disjunction:  sm g  ‥ ca g . from de morgan's laws  this is equivalent to:   sm g  …  ca g    which can be represented as an rrf feature:

where w1 is positive  w1 is negative  and the feature weight w1 is negative  not shown above . in general  since rrf features can model conjunction and disjunction  any cnf knowledge base can be represented as an rrf. a similar approach works for the second belief   friends of people are friends. 
　the first two beliefs are also handled well by markov logic networks. the key advantage of recursive random fields is in representing more complex formulas. the third belief   everyone has at least one friend who smokes   is naturally represented by nested quantifiers:  g. h.fr g h  … sm h . this is best represented as an rrf feature that references a secondary feature:

note that in rrfs this feature can also represent a distribution over the number of smoking friends each person has  depending on the assigned weights. it's possible that  while almost everyone has at least one smoking friend  many people have at least two or three. with an rrf  we can actually learn this distribution from data.
　this third belief is very problematic for an mln. first of all  in an mln it is purely logical: there's no change in probability with the number of smoking friends once that number exceeds one. secondly  mlns do not represent the belief efficiently. in an mln  the existential quantifier is converted to a very large disjunction:
 fr g a  … sm a   ‥  fr g b  … sm b   ‥ ，，，
if there are 1 objects in the database  then this disjunction is over 1 conjunctions. further  the mln will convert this dnf into cnf form  leading to 1 cnf clauses from each grounding of this rule.
these features define a full joint distribution as follows:

 g
		1
figure 1: comparison of first-order logic and rrf structures. the rrf structure closely mirrors that of first-order logic  but　figure 1 diagrams the first-order knowledge base containing all of these beliefs  along with the corresponding rrf. connectives and quantifiers are replaced by weighted sums.
1 inference
since rrfs generalize mlns  which in turn generalize finite first-order logic and markov random fields  exact inference is intractable. instead  we use mcmc inference  in particular gibbs sampling. this is straightforward: we sample each unknown ground predicate in turn  conditioned on all other ground predicates. the conditional probability of a particular ground predicate may easily be computed by evaluating the relative probabilities when the predicate is true and when it is false.
　we speed this up significantly by caching feature sums. when a predicate is updated  it notifies its parents of the change so that only the necessary values are recomputed.
　our current implementation of map inference uses iterated conditional modes  icm   besag  1   a simple method for finding a mode of a distribution. starting from a random configuration  icm sets each variable in turn to its most likely value  conditioned on all other variables. this procedure continues until no single-variable change will further improve the probability. icm is easy to implement  fast to run  and guaranteed to converge. unfortunately  it has no guarantee of converging to the most likely overall configuration. possible improvements include random restarts  simulated annealing  etc.
　we also use icm to find an initial state for the gibbs sampler. by starting at a mode  we significantly reduce the burnin time and achieve better predictions sooner.
1 learning
given a particular rrf structure and initial set of weights  we learn weights using a novel variant of the back-propagation algorithm. as in traditional back-propagation  the goal is to efficiently compute the derivative of the loss function with respect to each weight in the model. in this case  the loss function is not the error in predicting the output variables  but rather the joint log likelihood of all variables. we must also consider the partition function for the root feature  z1. for these computations  we extract the 1/z1 term from f1  and use f1 refer to the unnormalized feature value.
　we begin by discussing the simpler  propositional case. we abbreviate fi x  as fi for these arguments. the derivative of the log likelihood with respect to a weight wij consists of two terms:

the first term can be evaluated with the chain rule:

from the definition of fi  including the normalization zi :

from repeated applications of the chain rule  the   log f1 / fi term is the sum of all derivatives along all paths through the network from f1 to fi. given a path in the feature graph {f1 fa ... fk fi}  the derivative along that path takes the form f1wafawbfb ，，，wkfkwi. we can efficiently compute the sum of all paths by caching the per-feature partials   f1/ fi  analogous to back-propagation. the second term    log z1 / wij  is the expected value of the first term  evaluated over all possible inputs x. therefore  the complete partial derivative is:

where the individual components are evaluated as above.
　computing the expectation is typically intractable  but it can be approximated using gibbs sampling. a more efficient alternative  used by richardson and domingos   is to instead optimize the pseudo-likelihood  p    besag  1 :

where mbx xt  is the state of the markov blanket of xt in the data. pseudo-likelihood is a consistent estimator  but little else is known about its formal properties. it can perform poorly when long chains of inference are required  but worked quite well in our test domain.
　the expression for the gradient of the pseudo-loglikelihood of a propositional rrf is as follows:

we can compute this by iterating over all query predicates  toggling each one in turn  and computing the relative likelihood and unnormalized likelihood gradient for that permuted state. note that we compute the gradient of the unnormalized log likelihood as a subroutine in computing the gradient of the pseudo-log-likelihood. however  we no longer need to approximate the intractable normalization term  z.
　to learn a relational rrf  we use the domain to instantiate a propositional rrf with tied weights. the number of features as well as the number of children per feature will depend on the number of objects in the domain. instead of a weight being attached to a single feature  it is now attached to a set of groundings of a parfeature. the partial derivative with respect to a weight is therefore the sum of the partial derivatives with respect to each instantiation of the shared weight.
1 rrfs vs. mlns
both rrfs and mlns subsume probabilistic models and first-order logic in finite domains. both can be trained generatively or discriminatively using gradient descent  either to optimize log likelihood or pseudo-likelihood. for both  when optimizing log likelihood  the normalization constant z1 can be approximated using the most probable explanation or mcmc.
　any mln can be converted into a relational rrf by translating each clause into an equivalent parfeature. with sufficiently large weights  a parfeature approximates a hard conjunction or disjunction over its children. however  when its weights are sufficiently distinct  a parfeature can take on a different value for each configuration of its children. this allows rrfs to compactly represent distributions that would require an exponential number of clauses in an mln.
　any rrf can be converted to an mln by flattening the model  but this will typically require an exponential number of clauses. such an mln would be intractable for learning or inference. rrfs are therefore much better at modeling soft disjunction  existential quantification  and nested formulas.
　in addition to being  softer  than an mln clause  an rrf parfeature can represent many different mln clauses simply by adjusting its weights. this makes rrf weight learning more powerful than mln structure learning: an rrf with n + 1 recursive parfeatures  one for the root  can represent any mln structure with up to n clauses  as well as many distributions that an n-clause mln cannot represent.
　this leads to new alternatives for structure learning and theory revision. in a domain where little background knowledge is available  an rrf could be initialized with small random weights and still converge to a good statistical model. this is potentially much better than mln structure learning  which constructs clauses one predicate at a time  and must adjust weights to evaluate every candidate clause.
　when background knowledge is available  we can begin by initializing the rrf to the background theory  just as in mlns. however  in addition to the known dependencies  we can also add dependencies on other parfeatures or predicates with very small weights. weight learning can learn large weights for relevant dependencies and negligible weights for irrelevant dependencies. this is analagous to what the kbann system does using neural networks  towell and shavlik  1 . in contrast  mlns can only do theory revision through discrete search.
1 experiments: probabilistic integrity constraints
integrity constraints are statements in first-order logic that are used to detect and repair database errors  abiteboul et al.  1 . logical statements work well when errors are few and critical  but are increasingly impractical with noisy databases such as those that arise from the integration of multiple databases  information extraction from the web  etc. we want to make these constraints probabilistic  so we can statistically infer the types of errors and their sources. to date  there has been very little work on this problem.  see  andritsos et al.  1; ilyas et al.  1  for two early approaches.  this is a natural domain for mlns and rrfs  since both use first-order formulas to construct probability distributions over worlds  or databases.
　the two most common types of integrity constraints are inclusion constraints and functional dependencies. inclusion constraints are of the form:
 x.  y.r x y       z.s x z  
for example  in company x  r could be the relation  projectlead   x is in charge of project y  and s could be the relation  managerof   x manages employee z . this constraint says that every project leader manages at least one other worker. of course  some employees could manage other employees without being the lead on any project.
　to evaluate mlns and rrfs on inclusion constraints  we generated domains consisting of 1 people and 1 projects. with probability 1  a person x is a project leader  and leads  or co-leads  each project y with probability 1. for each project x leads  x manages employee z with probability 1. additional managing relationships are generated with a probability that we vary from 1 to 1. however  the actual leadership and management relationships are unobserved: we only see noisy versions  which are corrupted with a probability that we vary from 1 to 1.
　we converted the constraint formula into an mln and an rrf as described in previous sections and added the implications projectlead x y    projectlead and managerof x z    managerof  where the predicates with primes are the observed ones. we found that both mlns and rrfs worked better when given both directions of the integrity constraint  since project leaders manage people and managers lead projects. we learned weights to optimize pseudo-log-likelihood in all models. the results are shown in figure 1. each data point is an average over 1 train/test set pairs. rrfs show a consistent advantage over mlns  because they can better represent the fact that an employee who manages many people is probably a project leader  while an employee who manages few people may not be.
　the second type of integrity constraints  functional dependencies  are of the form:
 x y1 y1.  z1 z1.r x y1 z1  … r x y1 z1     y1 = y1

figure 1: log-pseudo-likelihood of rrfs and mlns for the inclusion constraints and functional dependencies. for inclusion constraints  we vary the probability of corrupting each observed relation  projectleadand manages far left  and the probability of adding extra manages x z  tuples  mid-left . for functional dependencies  we vary the probability of corrupting the company name of a supplier  mid-right  and the number of equivalent names per company  far right .in a functional dependency  each x determines a unique y  or an equivalence set of ys . for example  suppose company x has a table of parts suppliers represented by the relation supplier taxid companyname parttype . a supplier that supplies multiple types of parts may appear multiple times  but each tax id should always be associated with the same company name or set of equivalent company names  e.g.   microsoft    microsoft  inc.   and  msft  are all equivalent . if there is noise in the database  logic alone cannot say which company names are equivalent and which are errors.
　for evaluating functional dependencies  we generated a database with 1 tax ids  1 company names  partitioned into equivalence sets of size k   and 1 part types. each tax id is associated with one set of equivalent company names  uniformly selected from the 1/k name groups. for each company name a tax id uses  we generated part types z that the company supplies with probability 1. this simulates the merging of k distinct databases  each with its own company naming scheme but with shared identifiers for the other fields. as a final step  we randomly corrupted company names with a probability that we varied from 1 to 1.
　the task was to predict which pairs of company names were equivalent  given the supplier table. the results are shown in figure 1. with multiple databases at moderate levels of noise  rrfs outperform mlns. we attribute this to rrfs' ability to represent the fact that two company names are more likely to be equivalent if they both appear multiple times with the same tax id  and to learn the appropriate form of this dependence.
1 conclusion
recursive random fields overcome some salient limitations of markov logic. while mlns only model uncertainty over conjunctions and universal quantifiers  rrfs also model uncertainty over disjunctions and existentials  and thus achieve a deeper integration of logic and probability. inference in rrfs can be carried out using gibbs sampling and iterated conditional modes  and weights can be learned using a variant of the back-propagation algorithm.
　the main disadvantage of rrfs relative to mlns is reduced understandability. one possibility is to extract mlns from rrfs with techniques similar to those used to extract propositional theories from kbann models. another important problem for future work is scalability. here we plan to adapt many of the mln optimizations to rrfs.
　most importantly  we intend to apply rrfs to real datasets to better understand how they work in practice  and to see if their greater representational power yields better models.
acknowledgments
this work was supported by a national science foundation graduate research fellowship awarded to the first author  darpa grant fa1-1  managed by afrl   darpa contract nbch-d1  nsf grant iis-1  and onr grant n1-1. the views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  either expressed or implied  of darpa  nsf  onr  or the united states government.
