
we introduce a new technique for counting models of boolean satisfiability problems. our approach incorporates information obtained from sampling the solution space. unlike previousapproaches our method does not require uniform or near-uniform samples. it instead converts local search sampling without any guarantees into very good bounds on the model count with guarantees. we give a formal analysis and provide experimental results showing the effectiveness of our approach.
1 introduction
boolean satisfiability  sat  solvers have been successfully applied in a range of domains  most prominently  in ai planning and hardware and software verification. in these applications  the basic task is to decide whether a sat encoding of the underlying problem domain is satisfiable or not. given the tremendous progress in state of the art sat solvers and their applications  researchers have become interested in pursuing other questions concerning sat encodings to further extend the reach of sat technology. for example  can one randomly sample from the set of satisfying assignments  or can one count the total number of satisfying assignments  in both a formal complexity theoretic sense as well as in practice  these computational questions are much harder than  just  determining satisfiability versus unsatisfiability of a formula. formally  counting the number of solutions is a #p-complete problem  making it complete for a complexity class at least as hard as the polynomial-time hierarchy  ph   toda  1 .1 on the positive side  efficient methods for sampling and counting would open up a wide range of new applications  e.g.  involving various forms of probabilistic reasoning  darwiche  1; roth  1; littman et al.  1; park  1; sang et al.  1 .
¡¡uniform sampling and model counting are closely related tasks. in particular  jerrum  valiant  and vazirani  showed that for many combinatorial problems  if one can sample uniformly from the set of solutions  then one can use samples to obtain a highly accurate count of the total number of solutions.1 we will explain this strategy below. this approach was exploited in the work on approxcount  wei and selman  1   where a model sampling procedurecalled samplesat  wei et al.  1 was used to provide samples of satisfying assignments of the underlyingsat instance. in this setting  the counting is only approximate - with no guarantees on the accuracy - because samplesat does in general not sample purely uniformly; it uses markov chain monte carlo  mcmc  methods  madras  1; metropolis et al.  1; kirkpatrick et al.  1   which often have exponential  and thus impractical  mixing times. in fact  the main drawback of jerrum et al.'s counting strategy is that for it to work well one needs  near- uniformsampling  which is a very hard problem in itself. moreover  biased sampling can lead to arbitrarily bad under- or over-estimates of the true count.
¡¡the key new insight of this paper is that  somewhat surprisingly  using sampling with a modified strategy  one can get very good lower-bounds on the total model count  with high confidence guarantees  without any requirement on the quality of the sampling. we will provide both a formal analysis of our approach and experimental results demonstrating its practical effectiveness.
¡¡our strategy  samplecount  provides provably  probabilistic  guaranteed lower-bounds on the model counts of boolean formulas using solution sampling. interestingly  the correctness of the obtained bounds holds even when the sampling method used is arbitrarily bad; only the quality of the bounds may go down  i.e.  the bound may get farther away from the true count on the lower side . thus  our strategy remains sound even when a heuristic-based practical solution sampling method is used instead of a true sampler.
¡¡samplecount can also be viewed as a way of extending the reach of exact model counting procedures  such as relsat  bayardo jr. and pehoushek  1  and cachet  sang et al.  1 .1 more specifically  samplecount first uses sampling to select a set of variables of the formula to fix. once a sufficient number of variables have been set  the remaining formula can be counted using an exact model counter. from the exact residual model count and the number of fixed variables  we can then obtain a lower-bound on the total number of models. as our experiments will show  these bounds can be surprisingly close to optimal. moreover  on many problem classes our method scales very well. to mention just one example  we consider a circuit synthesis formula from the literature  called 1bitadd1.cnf. this formula can be solved quite easily with a local search style sat solver  such as walksat  mcallester et al.  1 . however  it is out of reach for all but the most recent dpll style solvers  minisat  ee¡än and s¡§orensson  1  takes almost two hours to find a single solution . consequently  it is completely out of reach of exact model counters based on dpll. the original formula has nearly 1 variables. samplecount sets around 1 variables in around 1 minutes  with the remaining formula solved by cachet in under two minutes. the overall lower-bound on the model count - with 1% confidence - thus obtained is an astonishing 1 models. we see that the formula has a remarkable number of truth assignments  yet exact counters cannot find any models after over 1 hours of run time.
1 main idea
to provide the reader with an intuitive understanding of our approach  we first give a high-level description of how sampling can be used to count. we will then discuss how this can be made to work well in practice.
 a  from sampling to counting  jerrum et al.  1 . con-

sider a boolean formula f with m satisfying assignments. assuming we could sample these satisfying assignments uniformly at random  we can measure the fraction of all models that have x1 set to true  m+  by taking the ratio of the number of assignments in the sample that have x1 set to true over the sample size. this fraction will converge with increasing sample size to the true fraction of models with x1 set positively  ¦Ã = m+/m.  for now  assume that ¦Ã   1.  it follows immediately that m =  1/¦Ã m+. we will call 1/¦Ã the  multiplier     1 . we have thus reduced the problem of counting the models of f to counting the models of a simpler formula  f+. we can recursively repeat the process  leading to a series of multipliers  until all variables are assigned or until we can count the number of models of the remaining formulas with an exact counter. for robustness  one usually sets selected variable to the truth value that occurs more often in the sample. this also avoids the problem of having¦Ã= 1 and thereforean infinite multiplier.  note that the more frequently occurring truth value gives a multiplier of at most 1. 
 b  approxcount  wei	and	selman 	1 .	in

approxcount  the above strategy is made practical by using a sat solution sampling method called samplesat  wei et al.  1 . unfortunately  there are no guarantees on the uniformity of the samples from samplesat. although the counts obtained can be surprisingly close to the true model counts  one can also observe cases where the method significantly over-estimates or under-estimates. our experimental results will confirm this behavior.
 c  samplecount. our approach presented here uses a

probabilistic modification of strategy  a  to obtain true accurate counts in expectation. the key strength of samplecount is that it works no matter how biased the model sampling is  thereby circumventing the main drawback of approach  a . instead of using the sampler to select the variable setting and compute a multiplier  we use the sampler only as a heuristic to determine in what order to set the variables. in particular  we use the sampler to select a variable whose positive and negative setting occurs most balanced in our set of samples  ties are broken randomly . note that such a variable will have the highest possible multiplier  closest to 1  in the samplecount setting discussed above. informally  setting the most balanced variable will divide the solution space most evenly  compared to setting one of the other variables . of course  as we noted  our sampler may be heavily biased and we therefore cannot really rely on the observed ratio between positive and negative settings of a variable. interestingly  we can simply set the variable to a randomly selected truth value and use the multiplier 1. this strategy will still give - in expectation - the true model count. a simple example shows why this is so. consider our formula above and assume x1 occurs most balanced in our sample. let the model count of f+ be 1m/1 and of f  be m/1. if we select with probability 1 to set x1 to true  we obtain a total model count of 1¡Á1m/1  i.e.  too high; but  with probability 1  we will set the variable to false  obtaining a total count of 1¡Ám/1  i.e.  too low. overall  our expected  average  count will be exactly m.
¡¡technically  the expected total model count is correct because of the linearity of expectation. however  we also see that we may have significant variance between specific counts  especially since we are setting a series of variables in a row  obtaining a sequence of multipliers of 1   until we have a simplified formula that can be counted exactly. in fact  in practice  the total counts distributions  over different runs  are often heavy-tailed  kilby et al.  1 . to mitigate the fluctuations between runs  we use our samples to select the best variables to set next. clearly  a good heuristic would be to set such  balanced  variables first. we use samplesat to get guidance on finding such balanced variables. the random value setting of the selected variable leads to an expected model count that is equal to the actual model count of the formula. we show how this property can be exploited using markov's inequality to obtain lower-bounds on the total model count with predefined confidence guarantees. these guarantees can be made arbitrarily strong by repeated iterations of the process.
¡¡we further boost the effectiveness of our approach by using variable  equivalence  when no single variable appears sufficiently balanced in the sampled solutions. for instance  if variables x1 and x1 occur with the same polarity  either both
positive or both negative  in nearly half the sampled solutions and with a different polarity in the remaining  we randomly replace x1 with either x1 or x1  and simplify. this turns out to have the same positive effect as setting a single variable  but is more advantageous when no single variable is well balanced.
¡¡our experimental results demonstrate that in practice  samplecount provides surprisingly good lower-bounds - with high confidence and within minutes - on the model counts of many problems which are completely out of reach
of current exact counting methods.
1 preliminaries
let v be a set of propositional  boolean  variables that take value in the set {1}. we think of 1 as true and 1 as false. let f be a propositional formula overv. a solution or model of f  also referred to as a satisfying assignment for f  is a 1 assignment to all variables in v such that f evaluates to 1. propositional satisfiability or sat is the decision problem of determining whether f has any models. this is the canonical np-complete problem. in practice  one is also interested in finding a model  if there exists one. propositional model counting is the problem of computing the number of models for f. this is the canonical #p-complete problem. it is theoretically believed to be significantly harder than sat  and also turns out to be so in practice.
¡¡the correctness guarantee for our algorithm relies on basic concepts from probability theory. let x and y be two discrete random variables. the expected value of x  denoted e x   equals ¡Æx xpr x = x . the conditional expectation of x given y  denoted e x |y   is a function of y defined as follows: e x |y = y  = ¡Æx xpr x = x |y = y . we will need the following two results whose proof may be found in standard texts on probability theory.
proposition 1  the law of total expectation . for any discrete random variables x and y  e x  = e e x | y  .
proposition 1  markov's inequality . for any random variable x  pr x   ke x     1/k.
1 model counting using solution sampling
this section describes our sample-based model counting algorithm  samplecount  see algorithm 1 . the algorithm has three parameters: t  the number of iterations; z  the number of samples for each variable setting; and ¦Á  the  slack  factor  which is a positive real number. the input is a formula f over n variables. the output is a lower-bound on the model count of f. samplecount is a randomized algorithm with a high probability of success. we will shortly formalize its success probability quantitatively as theorem 1.
¡¡samplecount is formally stated as algorithm 1 and described below. samplecount performs t iterations on f and finally reports the minimum of the counts obtained in these iterations. within an iteration  it makes a copy g of f and  as long as g is too hard to be solved by an exact model counting subroutine 1 does the following. it calls samplesolutions g z  to sample up to z solutions of g; if the sampling subroutine times out  it may return fewer than z samples.1 call this set of solutions s. getmostbalancedvar s  selects a variable u of g whose positive and negative occurrences in s are as balanced as possible  i.e.  as close to |s|/1 each as possible. ties are broken randomly. similarly  getmostbalancedvarpair s 
params: integers t z; real ¦Á  1
	input	: a cnf formula f over n variables
output: a lower-bound on the model count of f
begin
mincount ¡û 1n for iteration ¡û 1 to t do
until g becomes feasible for exactmodelcount do s ¡û s+1
s ¡û samplesolutions g z  u ¡û getmostbalancedvar s 
 v w  ¡û getmostbalancedvarpair s  r ¡û a random value chosen uniformly from {1} if balance u  ¡Ý balance v w  then set u to r in g
if r = 1 then replace w with v in g else replace w with  v in g
simplify g  s ¦Á¡¤ exactmodelcount g 
if count   mincount then mincount ¡û count
return lower-bound: mincount
end
algorithm 1: samplecount
selects a variable pair  v w  whose same and different occurrences are as balanced as possible. here  same occurrence  in a solution means that either both v and w appear positively or they both appear negatively. if u is at least as balanced as  v w   samplecount uniformly randomly sets u to 1 or 1 in g. otherwise  it uniformly randomly replaces w with either v or  v  forcing a literal equivalence. now g is simplified by unit propagating this variable restriction  it is tested again for the possibility of exact counting  and  if necessary  the sampling and simplification process is repeated. once g comes within the range of the exact counting subroutine after s simplification steps  the number of remaining models in g is computed. this  multiplied with 1s ¦Á  is the final count for this iteration. after t iterations  the minimum of these counts is reported as the lower-bound.
¡¡by setting variables and equivalences  the original formula is reduced in size. by doing this repeatedly  we eventually reach a formula that can be counted exactly. samplecount provides a practically effective way of selecting the variables to set or the equivalence to assert. each time it looks for the most evenly way to divide the set of remaining solutions. by picking one of the two subsets randomly  we obtain real guarantees  see section 1  and the formula is eventually sufficiently simplified to enable an exact count. the sampling is used to attempt an as evenly as possible division of the solution space. the better this works  the better our lower-bound. but we don't need any guarantees on the sampling quality; we will still obtain a valid  non-trivial lower-bound.
1 correctness analysis of samplecount
we now analyze samplecount as a randomized algorithm and show that the probability that it provides an incorrect lower-bound on an input formula decreases exponentially to zero with the number of iterations  t  and the slack factor  ¦Á.
somewhat surprisingly  as we discuss below  the bound on the error probability we obtain is independent of the number z of samples used and the quality of the samples  i.e.  how uniformly they are distributed in the solution space .
theorem 1. for parameters  t z ¦Á   the lower-bound returned by samplecount is correct with probability at least 1 ¦Át independent of the number and quality of solution samples.
proof. let 1s  s    1  be the true model count of f. suppose samplecount returns an incorrect lower-bound  i.e.  mincount   1s . this implies that count   1s  in all of the t iterations. we will show that this happens in any single iteration with probability at most 1 ¦Á. by probabilistic independence of the t iterations  the overall error probability would then be at most 1 ¦Át  proving the theorem.
¡¡fix any iteration of samplecount. when executing the algorithm  as variables of g are fixed or replaced with another variable within the inner  until-do  loop  g is repeatedly simplified and the number of variables in it is reduced. for the analysis  it is simpler to consider a closely related formula ¦Ñ g  over all n variables. at the start of the iteration  ¦Ñ g  = g = f. however  within the inner loop  instead of fixing a variable u to 1 or 1 or replacing w with v or  v and simplifying  as we did for g   we add additional constraints u = 1  u = 1  w = v  or w = v  respectively  to¦Ñ g . clearly  at any point during the execution of samplecount  the solutions of ¦Ñ g  are isomorphic to the solutions of g; every solution of g uniquely extends to a solution of ¦Ñ g  and every solution of ¦Ñ g  restricted to the variables of g is a solution of g. in particular  the number of solutions of g is always the same as that of ¦Ñ g . further  every solution of ¦Ñ g  is also a solution of f.
¡¡let g denote the final formula g on which the subroutine exactmodelcount g  is applied after s variable restrictions. note that s itself is a random variable whose value is determined by which variables are restricted to which random value and how that propagates to simplify g so that its residual models may be counted exactly. consider the variant of g defined on all n variables  is a random formula determined by f and the random bits used in the iteration. finally  the variable count for this iteration is a random variable whose value is 1s ¦Á times the model count of . we are interested in the behavior of count as a random variable.
¡¡recall that every solution of ¦Ñ g  is also a solution of f. for every solution ¦Ò of f  let y¦Ò be an indicator random variable which is 1 iff ¦Ò is a solution of g. then  count = 1s ¦Á¡Æ¦Òy¦Ò. we will compute the expected value of count using the law of total expectation: e count = e e count | s  .
¡¡fix s. for each ¦Ò  pr y¦Ò = 1 | s  equals the probability that each of the s variable restrictions within the inner loop is consistent with ¦Ò  i.e.  if ¦Ò has u = 1 then u is not restricted to 1  if ¦Ò has v = w then w is not replaced with  v  etc. because of the uniformly random value r used in the restrictions  this happens with probability exactly 1 in each restriction. note that the s restrictions set or replace s different variables  and are therefore probabilistically independent with respect to being consistent with ¦Ò. consequently  pr y¦Ò = 1 | s  = 1 s.
this implies that the conditional expectation of count given s is e count | s  = e 1s ¦Á¡Æ¦Òy¦Ò | s  = 1s ¦Á¡Æ¦Òe y¦Ò | s  =
1. since f
 has 1s solutions  we have e count | s  = 1s  ¦Á. applying the law of total expectation  e count  = e e count | s   = s  ¦Á.
finally  using markov's inequality  prcount
e count /1s  = 1 ¦Á. this proves that the error probability in any single iteration is at most 1 ¦Á  in fact  strictly less than 1 ¦Á . from our argument at the beginning of this proof  the overall probability of error after t iterations is less than 1 ¦Át. since we did not use any property of the number or quality of samples  the error bound holds independent of these. 
¡¡we end with a discussion of the effect of the number of samples  z  on samplecount. it is natural to expect more samples to lead to a  better  bound at the cost of a higher runtime. note  however  that z does not factor into our formal result above. this is  in fact  one of the key points of this paper  that we provide guaranteed bounds without making any assumptions whatsoever on the quality of the sampling process or the structure of the formula. without any such assumptions  there is no reason for more samples to guide samplecount towards a better lower-bound. in the worst case  a highly biased sampler could output the same small set of solutions over and over again  making more samples futile. however  in practice  we do gain from any sampling process that is not totally biased. it guides us towards balanced variables whose true multipliers are close to 1  which reduces probabilistic fluctuations arising from randomly fixing the selected variables. indeed  under weak uniformity-related assumptions on the sampler and assuming the formula has a mix of balanced and imbalanced variables  a higher number of samples will reduce the variation in the lower-bound reported by samplecount over several runs.
1 experimental results
we conducted experiments on a cluster of 1 ghz intel xeon machines with 1gb memory per node running linux. the model counters used were samplecount  relsat version 1 with counting  cachet version 1 extended to report partial counts  and approxcount version 1. both
samplecount and approxcount internally use samplesat for obtaining solution samples. samplecount was created by modifying approxcount to ignore its sample-based multipliers  fix the most balanced variables at random  and analyze equivalences  and by creating a wrapper to perform multiple iterations  t  with the specified slack factor ¦Á.
¡¡in all our experiments with samplecount  ¦Á and t were set so that ¦Át = 1  giving a correctness confidence of 1   1 = 1%  see theorem 1 . t ranged from 1 to 1 so as to keep the runtimes of samplecount well below two hours  while the other model counters were allowed a full 1 hours. the number of samples per variable setting  z  was typically chosen to be 1. our results demonstrate that samplecount is quite robust even with so few samples. of course  it can be made to produce even better results with more samples of better quality  or by using a  buckets  strategy that we will
table 1: performance of samplecount compared with exact counters and with an approximate counter without guarantees.
samplecountexact countersapproxcount	instance	true count  1% confidence models 
time   relsat modelstime   cachet modelstime without guarantees models 
time¡¡circuit synth. 1bitmax1.1¡Á1¡Ý 1¡Á1 sec1¡Á1 sec1¡Á1 sec¡Ö 1¡Á1 sec1bitadd 1	-¡Ý 1¡Á1 min-1 hrs-1 hrs¡Ö 1¡Á1 minrandom k-cnf wff-1.1.1¡Á1¡Ý 1¡Á1 min1¡Á1 hrs1¡Á1 min¡Ö 1¡Á1 secwff-1.1.1¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs1¡Á1 hrs¡Ö 1¡Á1 secwff-1.1	-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 seclatin square
ls1-norm1¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 secls1-norm1¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 secls1-norm1¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 secls1-norm1¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minls1-norm-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minls1-norm-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minls1-norm-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minls1-norm-¡Ý 1¡Á1 min-1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 hrsls1-norm-¡Ý 1¡Á1 min-1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1.1 hrslangfor
lang-1d probs.
1¡Á1¡Ý 1¡Á1 min1¡Á1 min1¡Á1 hrs¡Ö 1¡Á1.1 minlang-11¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minlang-11¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minlang-11¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minlang-11¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minlang-11¡Á1¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs¡Ý 1¡Á1 hrs¡Ö 1¡Á1 minlang-1-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs-1 hrs¡Ö 1¡Á1.1 hrslang-1-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs-1 hrs¡Ö 1¡Á1 hrslang-1-¡Ý 1¡Á1 min¡Ý 1¡Á1 hrs-1 hrs¡Ö 1¡Á1 hrsbriefly outlinein section 1. on the other hand  approxcount often significantly under- or over-estimated the number of solutions with 1 samples. we therefore allowed it around 1 samples per variable setting in all our runs  except for the very easy instances  circuit synthesis and random formulas  where it used 1 samples. other parameters of approxcount were set so as to obtain the desired number of samples in a reasonable amount of time from samplesat. a local search  cutoff between 1 and 1 was sufficient for most problems  while the langford instances required a cutoff of 1 to obtain enough samples. finally  both samplecount and approxcount were set to call cachet when typically between 1 and 1 variables remained unset.
¡¡table 1 summarizes our main results  where we evaluate our approach on formulas from four domains: circuit synthesis  random k-cnf  latin square  and langford problems. we see that samplecount scales well with problem size and provides good high-confidence lower-bounds close to the true counts  in most cases within an hour. it clearly outperforms exact counters  which almost always time out after 1 hours  providing counts that are several orders of magnitude below those of samplecount. we also report results on approxcount even though the comparison is not really meaningful as approxcount does not provide any correctness guarantees. indeed  it  for example  under-estimates the count by at least 1 on 1bitadd1  and over-estimates by 1  with an increasing trend  on the latin square formulas.  of course  there are also classes of formulas where approxcount appears to count quite accurately when given good quality samples.  we discuss the results in detail below. the circuit synthesis formulas are for finding minimal size circuits for a given boolean function. these are known to quickly become very difficult for dpll style procedures. the instance 1bitmax1 is still easy for exact model counting procedures  and samplecount also gets a very good lower-bound quickly. 1bitadd1  on the other hand  was only recently solved for a single solution using minisat in about 1 seconds. 1bitadd1 is certainly far beyond the reach of exact counting. the total solution count for this formula is astonishing; samplecount reports a lowerbound of 1 ¡Á 1. note that the formula has close to 1 variables and therefore the solution set is still an exponentially small fraction of the total number of assignments. samplecount sets around 1 variables in around 1 min-
table 1: comparison of samplecount with mbound  both with 1% confidence.
instance samplecount modelstime¡¡¡¡mbound modelstimerelsat modelscachet modelstimeramsey-1-1¡Ý 1¡Á1.1 min¡Ý 1¡Á1 min¡Ý 1¡Á1¡Ý 1¡Á1 hrsramsey-1-1¡Ý 1¡Á1 min¡Ý 1¡Á1 min¡Ý 1¡Á1¡Ý 1¡Á1 hrsschur-1¡Ý 1¡Á1 min¡Ý 1¡Á1 min¡Ý 1¡Á1¡Ý 1¡Á1 hrsschur-1-1 hrs¡Ý 1¡Á1 hr--1 hrsfclqcolor-1-1¡Ý 1¡Á1.1 min¡Ý 1¡Á1 sec¡Ý 1¡Á1¡Ý 1¡Á1 hrsfclqcolor-1-1¡Ý 1¡Á1 min¡Ý 1¡Á1 min¡Ý 1¡Á1¡Ý 1¡Á1 hrsutes  with the remaining formula solved by cachet in under two minutes. finally  approxcount seriously underestimates the true count  reporting only 1¡Á1.
¡¡our random formulas are selected from the underconstrained area  i.e.  with clause-to-variable ratios below the sat-unsat threshold. as noted by bayardo jr. and pehoushek   such formulas have a large number of assignments and are much harder to count than formulas of the same size near the phase transition. the table gives results on three such formulas: wff-1-1  wff-1-1  and wff-1-1. samplecount comes close to the true counts within minutes  while cachet takes up to 1 hours. approxcount again under-estimates the counts.
¡¡our third domain involves the problem of counting the number of normalized latin squares of a given order. a normalized latin square is a latin square with the first row and column fixed. the exact counts for these formulas are known up to order 1. we see that samplecount scales nicely as n increases  giving good bounds in a relatively small amount of time. we used averaging over buckets of size two for better bounds  cf. section 1 . both relsat and cachet consistently time out with partial or no counts. interestingly  approxcount over-estimates the counts by several orders of magnitudefor the harderformulaswhose true countis known.
¡¡our final domain  langford's problem  is parameterized by two values  k and n. in our instances  k = 1 and the following problem is encoded: produce a sequence s of length 1n such that for each i ¡Ê {1 ... n}  i appears twice in s and the two occurrences of i are exactly i apart from each other. this problem is satisfiable only if n is 1 or 1 modulo 1. we see that samplecount  with buckets of size 1  scales well as n increases  quickly giving good lower-bounds on the true count  which is known for some of our instances  cf. http://www.lclark.edu/ miller/langford.html . again  approxcount over-estimates the counts by many orders of magnitude  while relsat and cachet produce significant under-counts in 1 hours of cpu time.
¡¡in table 1  we compare the performance of samplecount with an xor-streamlining based model counting method that we recently proposed  called mbound  gomes et al.  1 . these two approaches are very different in spirit. mbound was designed for challenging combinatorial problems for which even finding a single solution is often computationally difficult. samplecount  on the other hand  is targeted towards problems for which multiple solutions can be repeatedly sampled efficiently. while mbound adds randomly chosen  xor  constraints to the formula  potentially making it harder for sat solvers  samplecount adaptively eliminates variables  simplifying the formula. samplecount has fewer parameters than mbound and is easier to use in practice. as we will see  on formulas where enough samples can be obtained easily  samplecount outperforms mbound. however  when it is hard to find samples  mbound wins. this shows that the two techniques are complementary.
¡¡all formulas considered for the comparison in table 1 are beyond the reach of current exact model counting methods  and are also challenging for approxcount  see  gomes et al.  1  for details . we see that on both the ramsey and the clique coloring problems  samplecount provides much stronger lower-bounds than mbound  at 1% confidence for both approaches . for the schur problems  samplecount dominates on the easier instance  but is unable to sample solutions at all for the harder instance.
¡¡finally  we demonstrate that although in expectation accurate model counts are still obtained even when samples are not used in the form of a guiding heuristic  techniques based on randomly selecting and fixing variables can suffer from a highly undesirable heavy-tail effect. consider the following settings for samplecount: ¦Á=1 t =1  and an exact counter is called only when all variables are fixed  so that it returns either 1  inconsistency  or 1 as the residual count . we use two variations of this  the first where variables are selected at random and the second where sampling is used to select variables likely to be more balanced.
¡¡figure 1 shows the result on the latin square formula ls1-normalized. it plots the cumulative average of the number of solutions obtained using the two variants over 1 independent runs. the values plotted for run i are the averageof the first i model counts for the two variants  respectively. theoretically  both of these cumulative averages must convergeto the true count for this formula  1¡Á1  shown as a horizontal line   after sufficiently many runs. it is clear from the figure that when balanced variables are used even by considering only 1 solution samples  the obtained model count approaches the true count significantly faster than when variables are selected at random. this fast convergenceis key to samplecount's good performance in practice.
¡¡note that in the random variable selection case  the model count happened to start quite low in this experiment  keeping the cumulative average down. the sudden upward jumps
model counts for ls1-normalized.cnf

number of runs
figure 1: convergence to the true model count: random variable selection vs. balanced variables using solution sampling
in the plot correspond to excessively high model counts that compensate for all runs up till that point and bump up the cumulative average. here  these over-estimated counts are 1¡Á1 in run 1 and 1¡Á1 in run 1  both over 1 times the true count. extremes of this nature  and even more drastic  occur fairly often when selecting variables at random.
1 extending samplecount with buckets
for randomized algorithms  more computational resources and more iterations typically result in improved behavior. while this is true for samplecount as well  it is not immediately obvious. in fact  the more iterations we perform i.e.  the higher the t   the worse lower-bound on the model count we are likely to get because samplecount takes the minimum over these t iterations. this apparent paradox is resolved by noting that as t grows  the minimum count may reduce a bit but the probability of obtaining an incorrect bound  as given by theorem 1  converges exponentially to zero. thus  more iterations directly translate into a much higher confidence level in the correctness of samplecount.
¡¡samplecount can in fact be extended so that more iterations translate into a trade-off between higher bounds and higher confidence. the idea is to perform t = bt iterations and group these into t buckets or groups of size b each. the lower-bound in this extended version is obtained by computing the average of the b counts within each bucket and taking the minimum over these t averages.
¡¡for b = 1  this bucket strategy is identical to samplecount. as b increases while t remains unchanged  we are likely to get lower-bounds even closer to the true count. to see this  fix ¦Á = 1 so that the count obtained in each iteration is a random variable whose expected value is precisely the true count. over different iterations  this count varies from its expected value  lowering the value of the minimum over all counts. by taking averages over several iterations within a bucket  we stabilize the individual count for each bucket  and the minimum over these stabilized counts increases. one can show that the correctness guarantee for this bucket strategy is exactly the same as theorem 1. we leave experimental evaluation for a full version of the paper.
1 conclusion
we presented samplecount  a new method for model counting which capitalizes on the ability to efficiently draw  possibly biased  samples from the solution space of problems using sat solvers. a key feature of this approach is that it gives probabilistic correctness guarantees on the obtained bounds on the solution counts without assuming anything at all about the quality  i.e.  uniformity  of the sampling method used. in practice  samplecount provides very good lowerbounds on the model counts of computationally challenging problems  and scales well as problem complexity increases.
