 
concept learning is inherently complex. without severe constraint or inductive  bias   the general problem is intractable. while most learning systems have been designed with built-in biases  these systems typically work well only in narrowly circumscribed problem domains. here we present a model of concept formation that views learning as a simultaneous optimization problem at three different levels  with dynamically chosen biases guiding the search for satisfactory hypotheses. in this model  the partitioning of events into classes occurs through dynamic interactions among three layers: event space  hypothesis space  and bias space. this view of the induction process may help clarify the problem of learning and lead to more general and efficient induction systems. to test this model of meta-knowledge  a variable bias management system  vbms  has been designed and partly implemented. the system will dynamically alter evolving hypotheses  concept representation languages  and concept formation algorithms by monitoring progress and selecting biases based on characteristics of the particular induction problems presented. vbms is designed to learn the best biases for different types of induction problems. thus it is robust  effective and efficient in many domains . the system can learn incrementally despite noisy data at any level. 
	i. 	introduction 
in theory the important problem of concept formation is simple: identify and describe useful classes of objects. unfortunately  this inductive problem is inherently complex  watanabe  1 . in practice  all inducers  both human and mechanical  must be able to reduce the number of hypotheses by choosing the proper constraints or biases  mitchell  1 . 
a. mechanised concept formation 
     class formation partitions a universe of objects  instances  or events into subsets.1 a class is described by a concept a candidate concept is a hypothesis  mitchell  1 . given a domain  the universe of possible events is divided into those events that are consistent with the description of some underlying concept and those that are not. this target concept  e.g. a boolean function over all events  may be discovered by searching  hypothesis space   or by building and modifying hypotheses in  event space   michalski  1 . 
　　1. to confound the problem  new objects or descriptions must sometimes be constructed  and it is the resulting universe that must be classified. this  problem of new terms  is discussed in  michalski ! 1; rendell. 1 . 
1 	knowledge acquisition 
     while this  crisp  view of concept formation is prevalent in ai  a more refined view is useful in real-world environments. since data are often uncertain  events may have degrees of class membership. thus concept formation becomes the partitioning of the universe of events into graded utility classes  and instead of being a boolean function  a concept becomes a multivalued  often probabilistic function  rendell  1a; zadeh  1 . moreover  since description languages are often deliberately constrained to control the difficulty  the concept may be imperfect; consequently hypotheses have degrees of credibility. so instead of being absolutely right or wrong  a hypothesis may also be assessed probabilistically  rendell  1b; watanabe  1 . 
b. combinatorial complexity 
     the practical problem faced in automated concept formation is managing the combinatorial explosion of hypotheses. the space of all hypotheses for a given problem contains every possible class description expressible in the language. consider  for example  a 1 x 1 grid of bits which can be used to encode different symbols. each bit on the grid can be on or off independently of the others  yielding a total of 1 possible patterns or events. if we want to learn a symbol  there are 1  possible classifications or hypotheses to choose from. a naive  generate and-test  inductive system would consider each one of the hypotheses; even algorithms such as candidate elimination  mitchell  1  cannot tractably solve this problem. 
     we can reduce the combinatorial complexity by replacing the 1 primitive pixels by a reduced set of abstract features. for example  one feature might indicate the presence or absence of standard curves and strokes. if there were  for instance  only 1 features of 1 values each  there would be 1 or about 1 possible configurations. yet even with this drastic simplification  the number of possible hypotheses is 1 - still well beyond the limits of practical computation. 
     to contain the combinatorial complexity  all learning systems employ one or more means of reducing the space of hypotheses. one simple method is to restrict the universe of events  as in our example  but there are several other ways. techniques for pruning hypotheses provide inductive  bias.  
c. inductive bias 
     mitchell  1  defined bias as  any basis for choosing one generalization over another  other than strict consisiency with the observed training instances'  cf. watanabe's  1   inductive ambiguity  . bias encompasses all extra-evidential choices made to reduce the complexity of a learning problem. 
these choices are often  hard coded  with the result that learning systems rarely perform well outside of narrowly circumscribed problem domains  such systems are  brittle  . this limitation can be overcome only by designing systems capable of dynamically altering their biases to accommodate new problems  as utgoff  1  has begun to do. 
　　one distinction among biases is their strength  mitchell  1; utgoff and mitchell  1; utgoff  1 . strong biases cause an induction systems to exclude a relatively large proportion of possible hypotheses. rendell  1b  has quantified the notion of bias strength and used the measure to analyze some learning systems and the kinds of concepts they can manage; haussler  1  has formalized a relationship between bias strength and learnable concepts. in general  strong biases mean fast concept acquisition but they may miss the target concept; weak biases are more likely to include the concept but they usually retard learning to the point of infeasibility. 
　　although the notion of bias strength is a valuable one  our model of learning refines the problem along a different dimension: when and how to vary the bias. while utgoffs  1  variable-bias system dynamically invokes a weaker bias when a strong one fails  the decision is based on immediate experience. in contrast  we are concerned with accumulated experience and the connection between problems and the particular biases used to solve them  this is  meta-knowledge  . 
d. scope of this paper 
　　this paper will examine the problem of dynamically variable bias and the tools needed to address it. we shall detail the problem of bias from this perspective in the next section. section iii will describe our  three space  model of concept formation which facilitates a solution to the problem. section iv will use this framework to develop the variable-bias management system  and section v will present some preliminary results which support the model. 
ii. bias flexibility and binding times 
concept learning systems have implemented inductive bias with varying degrees of flexibility and power. depending on system design  bias may be decided by the user  or it may be  partially  determined by the program. when bias selection is mechanized  its degree of automation may vary with respect to the range of bias choices and the flexibility of their selection. 
a. fixed bias 
　　the most common approach is to use a fixed bias  which is  built-in  at system design time and cannot be altered without changing the program. one common fixed bias excludes many hypotheses by abstracting secondary objects to be the objects from which the induction system generalizes. in samuel's  1  checkers program  the primary or  primitive  objects are board configurations  but the abstract or learned objects are k-tuples of highly descriptive features  such as piece advantage  which compress the information. another way to supply fixed bias is to restrict the language for hypothesis representation. for example  a logic-based language may confine hypotheses to those having few disjuncts  michalski 1 . by restricting the language of representation  systems limit expressible concepts; hence these systems speed processing but are applicable to some problems only. 
b. parameterized bias 
　　some systems allow the user to instruct them to ignore or disfavor certain types of hypotheses. because it can be altered at the beginning of a run  this is parameterized bias. in the aq systems  michalski  1   biases such as hypothesis simplicity are parameterized in the form of a  lexicographic evaluation functional   lef . an expression of hypothesis quality or preference  the lef may be input by the user of aq at run-time. for instance  a user might specify a preference for hypotheses having few disjuncts. the lef thus biases the system towards  desirable  hypotheses while downgrading the less desirable. lenat  1  uses a slightly different approach to parameterized bias in eurisko. in eurisko a user can temporarily suspend the processing in order to fine-tune a system parameter. 
c. dynamic bias 
　　a still more flexible induction system may be capable of altering its biases during the course of execution. this form of bias is  dynamically  variable. unlike a parameterized bias  a variable bias does not require the user to make decisions; rather the system will  set  its own bias according to its experience. one step toward mechanizing the selection of a 
　　bias is utgoffs system called  search for a better bias   or stabb  utgoff  1 . while solving a given problem  stabb is capable of forming a new disjunct to add to the hypothesis representation language. this expansion takes place if hypotheses using fewer disjuncts have been rejected by evidential criteria. 
d. more powerful variable bias management 
　　despite its flexibility  stabb is limited to altering one kind of bias. insofar as the control strategy involves a fixed ordering of choices  we might say the hypotheses are ordered by design-time criteria. in contrast  more powerful variations on the variable-bias scheme may not rely on a fixed ordering strategy. instead  they would be capable of learning through experience just when different biases are appropriate. not only would dynamic-bias management choose biases according to problem type  these schemes could have a greater range of choices. by using accumulated knowledge  they would systematically alter their biases  which would include not only aspects of representation such as features for event description  but also aspects of the inductive algorithm itself  e.g.  hypothesis transformation operators might be selected to compose an algorithm dynamically . the result would be an inductive system that is adaptive  efficient  and robust  i.e. effective over a wide variety of problem domains . 
　　this is the motivation for the variable-bias management system  vbms . biases are dynamically located and adjusted according to problem characteristics and past experience with similar problems. through exposure to different types of problems  vbms induces problem classes and identifies techniques  biases  appropriate for each class. with experience  vbms will evolve into a general learning system capable of identifying and effectively learning diverse classes of problems. the fundamental idea behind vbms is the use of multiple layers of learning  see buchanan et al.. 1 . 
	rinded  sheshu  and tcheng 	1 

hi. a three - space model for learning 
in this section  we shall view concept learning as a parallel inductive search through multiple spaces. the model will be used to explore interrelationships between spaces  to clarify the problem of concept learning  and to explain and develop vbms  section iv . 
　　the layers in our model are associated with three distinct spaces: event space  hypothesis space  and bias space. event space orders the events to which the learning system might be exposed. hypothesis space describes all possible partitions of the events into concept classes. bias space specifies the potential combinations of constraints that may be imposed upon a hypothesis space. 
a. event space 
　　inductive systems learn by extracting information from a set of events  which are the  ground objects  or basic cases to be formed into classes and described by the target concept. events are provided to a learning system as descriptions  usually consisting of a list of features  such as shapes of figures or strategic board positions . the set of all features over which a system operates constitutes a feature space in which each feature forms a distinct dimension. each point in this space represents a possible event. since this space defines all possible events which the system might ever encounter  the space is called event space or e-space. 
　　we can consider similarity of events within e-space as a 
　　relevant criterion for assessing the ultimate  utility'' of events. by utility  we mean the degree of concept membership. we can view a concept as a surface over e-space  where the shape of the surface is defined by the utility function. if a point in event space has a high utility value  we may expect neighbors of that point also to be of relatively high utility. 
　　e-space proximity underlies many efficient induction methods such as hyperrectangle creation  discriminant analysis  conceptual clustering  etc.  rendell  1b . proximity in event space is useful  however  only if the utility surface is regular. a regular space can be visualized as a smooth surface without abrupt deformations. operationally  a regular utility surface means that generalization and specialization operators may proceed more efficiently  rendell . 
b. hypothesis space 
　　the purpose of a concept learning system is to form a description extending the set of positive events to predict others. but for any given set of events  there are many potential extensions. if the utility is simply binary  and if e-space has only 1 possible events  1 of which are known to be positive  then there are 1 - 1  ~ 1 valid extensions. 
a given system will attempt to select and describe the 
 'correct  extension  where correctness depends upon the goals of the system. each description is a hypothesis for what the correct one might be. some hypotheses  however  are more  credible  than others  i.e. they more closely approximate the correct  target  concept. thus  each hypothesis can be assigned a graded value called its credibility  which is analogous to the utility of an event in e-space. in our example of 1 events  the induction problem is to guess the binary utility value of the 1 unknown events. the greater the number of correct guesses  the better the  evidential  credibility. 
1 	knowledge acquisition 
　　each hypothesis may be viewed as a function u. the domain of u is the set of points in e-space; the range of u is a real number between 1 and 1 reflecting the degree of concept membership - u is the utility function. each hypothesis is a complete surface over e-space which maps every point in espace to a specific utility. a hypothesis can therefore be viewed as a surface over e-space. the credibility of a 
　　hypothesis depends upon how closely the hypothetical utility surface matches the correct utility surface of the concept. 
just as we may think of the set of events as constituting 
e-space  we may also think of the set of hypotheses as being organized into its own space. given a particular representation for concepts and hypotheses  the space covering all possible hypotheses is called hypothesis space or h space. 
　　inductive operators which generalize  specialize  or otherwise transform hypotheses allow systems to move from one h-space point  hypothesis  to another. for example  in an hspace based on logic  applying an operator that replaces a constant with a variable results in movement to a more general hypothesis. 
　　the size and structure of h-space depends on the event and hypothesis description languages. consider the problem of symbol recognition. if the experimenter uses 1 primative features  e.g. pixels   then an impractically large 1' yet highly expressive h-space results. to reduce the enormous hspace resulting from a detailed event language  the experimenter could  e.g.  restrict the hypothesis language to some logic function over a limited number of pixels. if instead  the experimenter used abstract features  e.g. x1 - number of lines  x1 - number of curves   then a much smaller h-space would result. even so  feasible search requires techniques such as hill climbing  which take advantage of regularity or smoothness in the function being optimized. 
regularity in h-space can be defined like regularity in 
e-space  except that we use a credibility surface instead of utility. in a regular h-space  we may say something about the credibility of a hypothesis' neighbors once we know something about the hypothesis' credibility. regular credibility surfaces in h-space mean that we can move to neighboring hypotheses without experiencing significant variations in credibility. if  on the other hand  there is no discernible regularity within the h-space  then a new bias may be employed to reevaluate hypothesis credibility. 
c. bias space 
　　the process of concept learning is equivalent to a search through hypothesis space -p the goal is to pick the  correct  hypothesis to classify the set of events. the choice of what constitutes the correct hypothesis depends on several factors. evidential or e-space information will heavily influence the decision. just as important  though  are the extra-evidential factors that constrain the potential form of the hypotheses. these factors are the system's biases. different systems have different sets of biases and thus have different hypothesis spaces in which they operate. 
　　1. since the time required to discover evidential credibility is usually extreme  a learning system is usually designed to estimate an additional component that is faster to compute - the  extraevidential  credibility. since refined comparisons are helpful  the credibility should be graded rather than all-or-none  for other reasons see rendell  1b . 
　　in the symbol recognition problem  many possible biases could constrain the event or hypothesis description language. to describe grids  suppose we have 1 available representations: 
 1  binary pixel values   1  shape frequencies  e.g. number of lines  etc.   and  1  line angle distributions. suppose also that we have 1 inductive algorithms available which variously represent hypotheses as  1  linear discriminant functions   1  prototypes  or  1  utility regions. assuming the event descriptions are compatible with the inductive algorithms  our possible choices of event and hypothesis representation languages define a simple 1  bias space.  
　　in the same way that we proposed an h-space over all hypotheses  we can envision a third space over all biases. the collection of all possible biases make up this new space  called bias space or b-space. each bias exists as a point in bspace.1 just as we viewed a hypothesis as both a point in hspace and a utility surface over e-space  we may view a particular bias as both a point in b-space and as a credibility surface over h-space. a credibility surface over h-space is a function that takes a hypothesis from h-space and maps it onto a credibility value. in other words  each hypothesis defines a utility surface over e-space  and each bias defines a credibility surface over h-space. operationally  the credibility surface over h-space actually represents an ordering of all potential hypotheses to be evaluated in h-space. this credibility surface is also  by definition  a point in bias space. 
　　induction systems that are capable of dynamically altering biases work with a further measure. just as a hypothesis has a credibility  a bias has a belief associated with it. belief is the induction system's estimate of the  goodness  of a bias  just as the credibility is an estimate of the  goodness  of a 
　　hypothesis  and the utility is an estimate of the  goodness  of an event. 
d. relationships between spaces 
　　every point in e-space represents an event that the system might possibly encounter; every point in hypothesis space is a utility function over the events in e-space; every point in bias space is a credibility function over hypothesis space. choosing a particular point in hypothesis space is tantamount to characterizing the utility of every point in event space. choosing a particular point in bias space is tantamount to characterizing the credibility of every point in hypothesis space. 
　　many induction systems use the credibility surface over hypothesis space to guide the search for hypotheses  rendell  1 . similarly  induction systems vvith variable biases should be able to use a belief surface over bias space to guide the search for new algorithms and/or representation languages. thus  in a manner analogous to identifying correct hypotheses through a search of hypothesis space  the  correct  bias for a given problem domain may be found by searching bias space. 
　　to say that bias space is regular means that the average difference in belief between neighboring points is small. in that case  a proper distance measure can allow a variable-bias system to perform what is essentially hill-climbing through 
　　1.  bias  will be used to refer to either a single bias or a set of biases. thus  while a system may assume several biases  the set of these biases will be called the system's bias. 
bias space. unfortunately  two biases which may be very hose together in the context of one problem may not yield sufficiently similar results in other domains. even small variations in problem characteristics may produce cases in which two biases perform similarly in one situation but differently in another. however  if the experimenter  or the induction system  knows or infers that two problems are sufficiently similar  then hill-climbing in bias space can be valuable. in that case  knowledge of the proximity of different biases in the context of one problem can provide the basis for hill-climbing in the second problem. 
　　in all three spaces  the same essential phenomenon permits the same basic techniques. regularity or smoothness in a certain function allows efficient methods such as hill-climbing. depending on the level of learning  the function may be called utility  credibility  or belief  but the important general phenomenon of proximate similarity is responsible for efficiency at all three levels. these ideas are expanded and analyzed in  rendell  1b  1 ; the learning system using them is developed below. 
iv. variable-bias management 
in section ii we saw that to avoid brittleness and extend efficacy  we need more flexible learning. according to the three-space model  flexible learning can be viewed as a parallel search across event  hypothesis  and bias spaces. the variable-bias management system vbms is a realization of this concise multiple-space model. by controlling movement between the three spaces  the vbms is designed to learn the most effective techniques of induction for a wide range of 
problem classes. 
　　ideally  a learning system should be able to select its own biases. biases  including representations  algorithms  and components of each  should depend on problem characteristics. although this ability can be  hard -coded  into the system by the experimenter  doing so yields a system that performs well for a few problems familiar to the researcher but with no ability to learn from mistakes or adapt to new classes of problems. a more flexible approach is to let the system learn the concept of appropriate bias selection from scratch - as a direct result of problem solving experience. the vbms begins with no knowledge of the appropriateness of biases  but gradually induces problem classes along with the corresponding biases most useful for the effective learning of the problems in each class. to explain the operation of the vbms we begin with a discussion of a simplified  naive approach. 
a. naive bias management 
　　one naive approach is to try all available biases for a given problem  calculate the average effect of each bias  over all problems encountered   and then use this general knowledge bias space to select future biases. such a naive approach might consist of the following components:  i  a set of bias points  to determine a space of biases ;  ii  a general belief table  to assign a belief 1 to each bias point ; and  iii  a credibility measure  to judge the correctness of hypotheses . 
　　a point in bias space is a choice of inductive algorithm  representation language  and any relevant parameters to the algorithm or language  e.g. number of disjuncts  splitting criterion  etc. . the system records its belief b in each bias in a 
　　general belief table  gbt  which is simply a list of bias points 
	rendoll  sheshu  and tcheng 	1 
and corresponding b's. for example a simple bias space might have two-dimensions:  1  representation  e.g. number of disjuncts   and  1  algorithm  e.g. generalization or specialization . the general belief table might gradually assign greater beliefs to few disjuncts  perhaps because learning is faster and 
just as accurate. 
　　one natural measure of 1 is the credibility of the best hypothesis produced by a particular bias choice  relative to the credibility of hypotheses produced by other biases. many credibility measures exist  both evidential and extraevidential   and the only restriction in choosing a credibility metric is that it be uniformly applicable to any hypothesis generated by the system. for instance  if the problem is character recognition  an appropriate credibility measure would be how quickly and accurately the hypothesis classifies characters. 
　　a bias management system operates in two modes: learning and performance. in the learning mode  the system tests all available biases on each given problem. testing a bias entails running the associated inductive algorithm  measuring the credibility of hypotheses generated  and retaining the most credible ones. after ail biases have been tried  the system scales the credibilities of retained hypotheses to fit in the interval  1   with the better hypotheses earning values closer to 1. these normalized credibilities represent the system's belief  1 in each bias point  relative to a specific problem. 
　　as outlined in the three space model  section iii   these bias points and b's can be visualized as forming a surface over bias space with peaks representing relatively good biases. since initial implementation of a bias management system would involve only a limited number of bias points  compared to the space of all possible biases   this  surface  is more conveniently represented as a belief table. a problem belief table  pbt  contains all biases explored for a specific problem and their estimated b's. similarly the average of all pbt's created in the system's lifetime forms a general belief table  gbt . 
　　in the performance mode  the naive system simply tries the biases in its gbt in descending order of their estimated belief until an acceptable hypothesis is found or the rate of credibility improvement per bias tried falls below some threshold. 
　　this bias management is more flexible than utgoff's stabb  which has a limited set of biases and does not adjust their order. on the other hand  our naive algorithm is still limited. its major fault is that it tries biases according to their average performance over all problems experienced. this approach makes no use of problem characteristics when selecting biases. its behavior is analogous to a doctor who always prescribes aspirin regardless of the symptoms because he has found the drug to be effective in most situations. even if this system were to encounter a new problem identical to a previously solved problem it would still use its general belief table rather than the more appropriate problem belief table. 
b. improved bias management 
　　while naive bias management might be useful for certain applications  it would be too coarse. a cure is to perform induction on bias space itself  and allow the system to learn how to apply different biases to different types of problems. this meta-level learning is the basis of the variable bias management system vbms. 
1 	knowledge acquisition 
　　intuitively  any similarity between a given problem and previously solved problems should influence bias selection. if the similarity is high  we are tempted to give some weight to the pbt  problem belief table  generated from related past problems when selecting biases for the new problem. if the similarity is low  the system may be justified only in using its general knowledge of bias space found in its gbt. the use of similarity to select biases assumes that similar problems will have similar solutions. thus the appropriate choice of a similarity measure is crucial to the operation of a more flexible bias management system. the vbms uses a dynamic similarity measure that evolves with experience. this dynamic assessment of similarity is a novel feature of the vbms and should result in great flexibility. 
　　to associate problem characteristics with effective biases  we need to introduce the idea of a problem space. problem space is similar to event space except that its dimensions are global features of the problem rather than descriptions of events. problem characteristics are user-defined and should be applicable to all problems presented to the system. for instance  if the vbms consists of algorithms that process feature vectors  then potentially useful problem characteristics include the number of training events  the reliability of training events  the number of features per event  the grain size of features  and other properties of features. 
　　over time  the system partitions the points in problem space into regions  problem classes  whose problem points have similar solutions  pbt's . in other words  problem points in the same region have similar bias beliefs or pbt's. the formation of these regions involves a region belief table  rbt  to store regional beliefs. like the gbt of the naive system  an rbt is an average of all pbt 's belonging to problems within the region. for example  regions of problem space where the grain size is large might have a strong belief in few disjuncts. problems within the same region are considered  similar  with respect to their solutions and bias beliefs.  similarities in values  tables  or functions can be used to form and modify regions; see rendell  1.  
　　every new problem attempted by the vbms is associated with a point in problem space. when selecting biases  the system finds the region containing the new point in problem space  and uses the region belief table to select initial biases. biases in the rbt are tried in order of decreasing belief until an acceptable hypothesis is found or the rate of credibility improvement falls below some threshold. if this approach fails  the gbt is then used to select a bias. 
　　at first vbms exhaustively searches bias space for each new problem and learns only general knowledge of the belief surface  i.e. a gbt . as more problems are attempted  vbms gradually learns relationships between problem characteristics and effective biases. this knowledge resides in the problem space regions and their associated rbt s. 
　　the reliance of vbms on multiple iterations of concept formation suggests inefficiency. the system might be too slow if it had to construct a new region belief table for each new learning problem. but since this meta level knowledge  rbt  is accumulated for all problems encountered  the procedure is reasonable  as section v begins to show . a more complete description of the vbms algorithm appears in  rendell et al.  1b . 

v. implementation and experiment 	b. initial implementation 

we have begun to implement and test the vbms. to explain details  let us first consider some concrete forms of bias. 
a. kinds of bias 
　　from the standpoint of computer implementation there are two basic manifestations of bias: representational  for description of events and concepts   and algorithmic  for construction  transformation  and verification of hypotheses . 
1. representational bias 
　　given a language such as dnf logic  the specific elements of the language  e.g. function symbols or attributes  and constraints on it  e.g. few disjuncts  are the biases that the user or system may control. the manifestation of bias as a variable number of disjuncts  utgoff  1  is a special case of an imposed model: in uncertain environments   number of disjuncts  becomes the number of peaks in the utility function. additional constraints may confine the functional form  e.g. a linear combination of features - see rendell  1 . while functional forms such as the number of disjuncts are straightforward to modify  not all representational biases are easy to implement. for example  the choice of features is a complex research problem  porter  1 . 
1. algorithmic bias 
　　given a hypothesis language having a particular representational bias  a concept learning algorithm is designed to search in an associated space for credible hypotheses that approximate the target concept. since the learned concept may depend on details of the search  algorithms and algorithm components are also biases. just as hypotheses space contains the desired concept  'algorithm space  contains the desired algorithm for finding it.  we could think of algorithm space as being the subpace of the entire bias space that has to do with algorithms only.  just as the target concept for the current domain problem is extracted from hypotheses space  a well behaved algorithm for the current learning problem is extracted from algorithm space. 
　　depending on our representation of algorithm space  it could be more or less grainy. a very grainy algorithm space might contain a few fixed concept learning systems  such as aq  id1  pls1  etc.  michalski  1; quinlan  1; rendell  1 . in contrast  a refined algorithm space might contain system components  such as operators for hypothesis transformation . in the simple  grainy  case  a completed algorithm would be selected as a unit; in the complex  refined  ease  the algorithm would be constructed from its components. 
　　to initiate testing of variable-bias management  we wanted to begin with one of the simpler kinds of bias. we decided to begin with algorithmic biases and have the vmbs try to choose an entire learning system from a coarse algorithm space. unlike utgoff's  1  stabb  vmbs bases its choices on characteristics of the problem domain  although the characteristics so far are simple and syntactic . 
1. first experiment: bias as algorithm 
　　in the first experiment vmbs selects one of three learning systems aq1  assistant  and pls1  which are dissussed in rendell et al.  1a . the choice is based on the behavior of these three systems as a function of number of training events and number of features. in other words  problem space here is only two-dimensional: the number of events is on one axis  and the number of features in each event is on the other. 
　　table i shows results involving a lymphography data base  using the learning systems aq1  assistant  and plsl. initially  vbms tries each algorithm on each problem  until sufficient experience is accumulated  at this time the criterion for  sufficient experience  is user-supplied  although full automation will simply be information-theoretic . 
　　the credibility measure should reflect performance  and should be include concept accuracy and processing time  measures for these are standard - - see rendell et al.  1a . to keep the first experiment simple  our uniform measure of credibility is just the number of user-cpu-seconds  on a vax1 running under unix - all programs were written in pascal . in this case we normalize credibility /i by just taking the quotient: number of seconds used by the fastest algorithm over the number of seconds used by the given algorithm. in this case  with only three algorithms  a triplet of u-values is associated with each point in problem space. call these values u   aq1   u1  assistant   and u1  plsl   and the resulting uvector u. 
　　vbms divides problem space by splitting it into orthogonal rectangles  making the splits having the highest dissimilarity rating  like the basic plsl algorithm . to calculate the dissimilarity rating  vbms first averages the u's for each of the three algorithms in the proposed regions. call these aver-
ages for two tentative subrectangles next  vbms calculates 

 this is based on an information-theoretic measure and should generally include an error term: see rendell  1.  

*  time  it user time in seconds. the  normalised  u is the ratio of the given algorithm's performance to the performance of the fastest algorithm 
	rendell  sheshu  and tcheng 	1 

1. results of initial experiment 

vi. summary and conclusions 
since induction is so complex  concept formation is feasible only by reducing the search space through the use of selective biases. biases appear in many different forms  e.g. abstracted features  acceptable concept descriptions  etc.  and are usually fixed into the design of a system. since any fixed bias is too restrictive for some problems and too slow for others  biases should be dynamic - for generally efficient and effective  robust  learning  we need better methods for managing bias mechanically. 
previous work on dynamic bias has been quite limited. 
utgoffs  1  stabb includes a variable bias  but only in a form that does not learn to associate bias with knowledge about problems. 
　　we have outlined a robust multiple space model of learning and proposed a design for a variable bias management system vbms. in this model of knowledge and meta-knowledge  learning can be viewed as a parallel search across event  hypothesis  and bias spaces. the vbms is specifically designed to learn biases and to induce their relationships to classes of problems  and thereby to support flexible learning across different problem domains. unlike most other learning systems  the vbms learns at different levels. 
　　an inductive algorithm performs better when it outputs hypotheses of higher credibility using fewer resources. even in initial implementation involving a coarse  algorithm space   we have shown that vbms performs better any of its subsidiary algorithms alone. by dynamically adjusting its bias  vbms can select and use a strong bias appropriate to a given induction problem. thus it can combine the efficiency of a strong bias with the generality of a weak one. 
1 	knowledge acquisition 
　　vbms is a robust framework for probabilistic  multilayered learning. extensions include elaboration of problem space and bias space using analogy and other semantic information. these and other refinements  and algorithms for the system  are discussed in  rendell et al.  1b . 
