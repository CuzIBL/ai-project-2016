 
an interpretation system finds the likely mappings from portions of an image to real-world objects. an interpretation policy specifies when to apply which imaging operator  to which portion of the image  during every stage of interpretation. earlier results compared a number of policies  and demonstrated that policies that select operators which maximize the information gain per cost  worked most effectively. however  those policies are myopic - they rank the operators based only on their immediate rewards. this can lead to inferior overall results: it may be better to use a relatively expensive operator first  if that operator provides information that will significantly reduce the cost of the subsequent operators. 
this suggests using some lookahead process to compute the quality for operators non-myopically. unfortunately  this is prohibitively expensive for most domains  especially for domains that have a large number of complex states. we therefore use ideas from reinforcement learning to compute the utility of each operator sequence. in particular  our system first uses dynamic programming  over abstract simplifications of interpretation states  to precompute the utility of each relevant sequence. it does this off-line  over a training sample of images. at run time  our interpretation system uses these estimates to decide when to use which imaging operator. our empirical results  in the challenging realworld domain of face recognition  demonstrate that this approach works more effectively than myopic approaches. 
1 	introduction 
interpretation is the process of finding the likely mapping from portions of an image to real-world objects. it is the basis for a number of imaging tasks  including recognition   is objectx in the image    and identification   which object is in the image     as well as several forms of tracking   find all moving objects of typex in this sequence of images    etc.  pl1; hr1 . it is important that an interpretation system  is  be efficient as well as accurate. any is should have 
vision 
russell greiner 
department of computing science university of alberta edmonton  ab t1g 1 canada greiner cs.ualberta.ca 
access to an inventory of  imaging operators  - like edge detectors  region growers  corner locators  etc.  each of which  when applied to any portion of the image  returns meaningful tokens  like circles  regions of the same color  etc. . an  interpretation policy  specifies which operator to apply to which portion of the image  during each step of the interpretation process. such policies must  of course  specify the details: perhaps by specifying exactly which bottom-up operators to use  and over what portion of the image  if and when to switch from bottom-up to top-down  which aspects of the model to seek  etc. 
　our earlier work llgolb  considered various types of policies  towards demonstrating that an  information theoretic policy   which selects operators that maximize the information gain per unit cost of the imaging operator  work more effectively than others. however  the figolb  policies evaluate each operator o -  myopically - i.e.  independent of the cost and effectiveness of subsequent operators that would be applied  after performing this o . . to see why this is problematic  assume the task is to determine whether there is an airplane in an image  by seeking the various parts of an airplane - e.g.  the fuselage  wings  engine pods  tailpiece  etc. now consider the oep operator that detects and locates the engine pods. as the engine pods are small and often partiallyoccluded  oep is probably expensive. however  once these parts have been located  we expect to find the associated wings very easily  and then the remaining parts required to identify the entire airplane. a myopic policy  which evaluates an operator based only on its immediate cost  would miss this connection  and so would probably prefer a cheaper operator over the expensive oep. a better policy would consider  operator interactions   here  relating oep to  say  the  wing finder operator   when deciding which operator to apply. the data in section 1  related to the complex task of face recognition  shows that such non-myopic policies can be both more accurate  and more efficient. 
　of course  non-myopic policies must use some type of lookahead to evaluate the quality of its operators; this can be combinatorially expensive to compute. we address this concern by  1  dealing with an abstract version of the  interpretation state space   and then by  1  using dynamic programming techniques over this abstracted space  pre-computing many relevant  utility   sb1  values off-line  bdb1 . in particular  our system computes the utility of imaging operator se-

quences in each state s encountered  based on data from a set of training examples. it produces a policy  which maps each state s to the operator that appears to be the most promising  incorporating this lookahead. at run-time  our system finds the best matching state and applies the operator associated with that situation. our empirical results  in the domain of face recognition  show that such policies work effectively - better than the best results obtained by any of the earlier myopic systems. 
　section 1 presents relevant related work  to help frame our contributions. section 1 discusses the salient features of interpretation strategies. section 1 then presents our approach  of using reinforcement learning in image interpretation  within the domain of face recognition  using operators that each correspond to types of eigenfeatures. section 1 provides empirical results that support our claims. 
1 	related work 
we produce an interpretation of an image by applying a sequence of operators  where each operator maps the current state  typically a partial interpretation  to a new state. as the effects of an operator may depend on information not explicitly contained in the state description  this mapping is stochastic. moreover  each operator has a cost  and the state associated with the final interpretation has a  quality   i.e.  its accuracy as an interpretation . as such  we view this imageinterpretation task as a  markov decision problem   mdp . this means we can apply the host of reinforcement learning techniques  sb1  to this problem. this specific application area  of image interpretation  follows the pioneering work of draper  bdboo   which shows that dynamic control policies can outperform hand-coded policies. we extend their work by addressing and exploiting the issue of operator interactions and by doing a systematic analysis of the cost and accuracy tradeoffs in face recognition. 
　while we could encode each state as the entire image  this would be too unwieldy. boutilier et al.  bdh1  survey several types of representations in planning problems and discuss ways to exploit them to ease the computational cost of policies or plans. their work focuses on abstraction  aggregation and decomposition techniques. we use abstractions to reduce the size of each state  and hence of the state space we are exploring; moreover  we are not concerned with the general planning problem. our system is similar to the  forest inventory management system   bdl1 . in their system  the output of some operators provide the input to some other operator  while in our system the result of one operator is used to narrow the search for some other operator. moreover  our system can exploit the structure specific to our task to bound the lookahead depth. 
1 framework 
as suggested above  our overall objective is to produce an effective interpretation policy - e.g.  one that efficiently returns a sufficiently accurate interpretation  where accuracy and efficiency are each measured with respect to the underlying task and the distribution of images that will be encountered. this section makes this framework more precise. subsection 1 specifies our particular task; subsection 1 lists the strategies we will evaluate; subsection 1 outlines our performance domain  face recognition; and subsection 1 describes the specific operators we will use. 
1 	input to the interpretation system 
we assume that our interpretation system  /s  is given the following information: 
* the distribution d of images that the is will encounter  encoded in terms of the distribution of objects and views that will be seen  etc. for our face recognition task  this corresponds to the distribution of all people the system will see  which varies over race  gender and age  as well as poses and sizes. we approximate this using the images given in the training set. see figure 1. 
  	the task t -  includes two parts: 
first  v specifies the objects that the is should seek  and what is should return. second  the task specification also provides the  evaluation criteria  for any policy  which is based on both the expected  accuracy  and the maximum interpretation  cost  which the is should not exceed. 
　here  specifies our task is to identify the person from his/her given test image  wrt the people included in the training set   subject to the accuracy  and cost requirements 
* the set of possible  operators   includes  say  various edge detectors  region growers  graph matchers  etc. 
for each operator o1  we must specify 
  its input and output  
  its  effectiveness   which specifies the accuracy of the output  as a function of the input. 
  its  cost   as a function of  the size of  its input and pa-rameter setting. 
we will use a specific set of operators in our face recognition task; we describe these in detail in section 1. 
　borrowing from the mdp literature  we view the main input to each operator as its  state . as we are dealing with scenes  we could use the entire pixel image as  part of  the state. however  for reasons of efficiency  we will often use an abstracted view  of  our interpretation of the  the scene s. section 1 presents the specific abstraction we are using. 
1 	strategies 
strategy infogain: selects the operator that provides the largest information gain  per unit cost  at each step. this myopic strategy first computes the expected information gain  for each possible operator and argument combination o  as well as the cost it then executes the operator that maximizes their ratio   
　we focus on this strategy as a number of earlier empirical studies have demonstrated that it was the best of all of the  myopic  strategies considered  across a number of task-contexts and domains  including simple blocks world  car recognition  identifying the make and model of a car based on its tail light assembly as well as the face recognition system considered here  
strategy bestseq: selects an operator  that appears to be the most promising in the current abstract state f s   as 
vision 
given by the utility  is the  optimal  mapping from state to actions. see section 1 for details. 
1 	face recognition task 
we investigate the efficiency and accuracy of the strategies listed above in the domain of face recognition  tp1; pms1; pwhr1; ec1 . this section briefly discusses the prominent  eigenface  technique of face recognition that forms the basis of our approach; then presents our framework  describing the representation and the operators we use to identify faces; and finally presents our face interpretation algorithm  for identifying a person from a given image of his/her face. 
eigenface and eigenfeature method: many of today's face recognition systems use principal component analysis  pca   tp1: given a set of training images of faces  the system first forms the covariance matrix c of the images  then computes the main eigenvectors of c   eigenfaces  . every training face is then projected into this coordinate space 
  facespace    producing a vector  
　during recognition  the test face is similarly projected into the facespace  producing the vector which is then compared with each of the training faces. the best matching training face is taken to be the interpretation  tp1 . 
　following  pms1   we extend this method to recognize facial features - eyes  nose  mouth  etc. - which we then use to help identify the individual in a given test image. we partition the training data into two sets  for constructing the eigenfeatures and for collecting statistics  where each set contains at least one face of each of the people.  feature regions for the training data t were extracted using the operators explained in 1 and verified manually for correctness . letting denote the person whose face is 
given by 	we have 
each remaining 	and 	also maps to  
　we use pca on the mouth regions of each image  to produce a set of eigenvectors; here eigen-mouths. for each face image let be the  feature space  encoding of  
mouth-region. we will later compare the feature space encoding of a new image against these vectors  with the assumption that suggests that htcst is really person i - i.e.  finding that is small 
should suggest that 	 note 	refers to the  
norm  aka euclidean distance.  to quantify how strong this 
belief should be  we compute values where each is the euclidean distance between the  eigen-mouth encodings  of and 
 using the training data  we can learn a mapping from these values to probabilities   which we can use to estimate 
 see  igolb  for details.  
　we compute similar estimates for the other facial features  such as nose  left eye  le  and right eye  we then use the nai've-bayes assumption  dh1   that features are independent  given a specific person  1 to compute a cumulative 
'of course  this assumption is almost assuredly false in our sit-
vision 
 face probability  from these  feature probabilities : the valur e /   corresponding t o a set of values  one for each individual i . 
we then compute 
  1 
1 	operators 
we 	use 	four 	classes 	of 	operators  	o 	-
 to 
detect respectively  left eye    right eye    nose  and  mouth . 	each specific operator also takes several parameters: 	{1 1 1} specifies the number of eigen-vectors being considered; the other parameters and 	specify the space this operator will sweep  looking for this feature: it will look in the rectangle 	i.e.  sweep 	pixels  
centered at  
　each instantiated operator takes as input the image of a test face and returns a probabilistic distribution over the individuals. it has three subtasks: locates the feature from within the entire face here we use a simple template matching technique in which we search in the fixed region of size pixels  centered at pixel then projects the relevant region of the test image into the feature space - c o m p u t i n g o f dimension . subtask#1 uses this to compute first the values for each person i and then to com-
pute the probability 	for each person 
i. we use equation 1 to update the distribution when considering the 1nd and subsequent features; see  igolb . for each eigenspace dimension we empirically determined the cost 
 in milliseconds  of the four operators - 
where  	is the size of the range. while increas-
ing the dimensionality of the feature space should improve the accuracy of the result  here we see explicitly how this will 
 
increase the cost. 
1 use of dynamic programming 
this section briefly overviews mdps  presents  state abstraction  in face recognition and shows how dynamic programming can be used to compute the utility of operators in abstracted states. we also discuss operator interaction and describe how it can be exploited in face recognition. 
uation. however  this classification has been found to work well in practice  mit1 . 

1 	markov decision problem 
a markov decision problem can be described as a 1-tuple is a finite set 
     is a finite set of acis the state transition probais the probability that 
taking action a in situation s leads to being in state  and is the reward an agent gets for taking an action a in state s  s. the markov property holds if the transition from state to using action a depends only on st and not the previous history. a policy : s a is a mapping from states to actions. for any policy 1r  we can define a utility function  such that 

which corresponds to the expected cumulative rewards of executing the apparently-optimal action in state s  then following policy after that. 
given an mdp  we naturally seek an optimal policy 
i.e.  a policy that produces the cumulative reward for each state  dynamic programming provides a way to compute this optimal policy  by computing the utilities of the best actions. of course  given the values of action at each state s is simply  

　this can be challenging in the general setting  where sequences of actions can map one state to itself; much of the work in reinforcement learning  sb1  is designed to address these issues. in our current case  however  we will see there is a partial order on the states  meaning no sequence of actions can map a state to itself. here  we can use dynamic programming to compute the optimal utility for each  final step   then use these values to compute the optimal action  and utility  for each penultimate state  and so forth. 
1 	state abstraction 
an mdp involves  states . in our face recognition task  any attempt to define states in terms of the pixel values of an image would be problematic  as there will be far too many states to enumerate. following  bdboo; bdl1   we use the notion of  abstract states   which basically redefines an original state in a much more compact form  using only the certain aspects of the state. since we recognize a person using the features  we define an abstraction function 
  1  
where s is the actual complex state as present in the image and denotes the location  center  of feature  left eye  right eye  nose or mouth  in the image.  here c is the total cost we have spent so far  and  is the current posterior distribution over the possible faces  based on the current evidence the location of some feature may not yet be known in an image; here  we use the value of for both and further  we say two abstractions are equivalent   written  
  si and  1 have located the same set of features 	i.e.  and 
- i.e.  when the distance  in pixels  between the centers of feature in and .s-1 is small  
as we are using normalized images that contain only faces  denoting the distance in absolute pixel values is not an issue. 
in general  is a small  predefined constant; we used 1 
　the result of abstraction is that a large number of complex states can be described by a small number of compact state descriptions. of course  the same abstract state can represent 
multiple states 	when 	 
we use a lookahead algorithm to compute the ♀/-values. 
since we consider only four features  we need a lookahead of only depth four. moreover  we can do this during the training phase. we will use the reward function 
 1  
that penalizes each operator a by a times the time it required  in seconds  as well as a positive score for obtaining the correct interpretation  here  represents that correct identity of the person in the image.  we used  = 1. during the training phase  our system finds the optimal operator sequence 
i.e.  the one that has the maximum 
value  based on the abstracted state  
1 	dynamic programming 
this section shows how to compute u{s  utility values. 
tree expansion: we use four classes of operators o 	- each 
with 1 different values of 	for a 
total of 1 operators  see section 1 . the parameters 	-
       and specify the rectangular area where the operator will search. our system does not have to search over their values; instead  it directly computes their values from the locations of the features  we have already detected; see section 1. 
　our system expands the operator tree exhaustively using a depth-first search. of course  the depth of the tree is at most 1  not 1   as we only consider at most one instance of each operator along any path. 
computing u  s : we compute the various utility values using a dynamic programming approach: 
　 i  we first apply every sequence of 1 different operators to the original image  to produce the set of all possible s 1  leaf states. then s e t w h e r e i s the correct interpretation  and is the observed locations of the various features. 
　 ii  now consider each state that involves some set of 1 operators. for each  we can consider all 1 + 1 possible actions: either or apply the remaining operator  with one of the 1 possible k values . we can trivially compute the utility of each option: as for the  action  and if the action a takes time t a  and produces 
　　1  for this value of we found that there were about 1 entries of totally. 
vision 

the  variance  - i.e.  the size of the search window around 

retrieving the most promising operator during interpretation: this entire procedure of computing the optimal policy  is done off-line  during the training phase. during inter-
pretation  performance phase   in any state  we  i  find the  nearest neighbour   such that the sum of the distances between the corresponding features in  is 
the minimum over all possible entries; and  ii  return the operator which is either  or an instantiated operator - say  1   1    1  . if not-stop  the run-time system then executes this operator to locate another feature  which produces a new state  updating the total cost and posterior distribution as well . it then determines the next action to perform  and so forth. 
　there is one place where our system might perform differently from what the policy dictates: as we are maintaining the actual cost so far  and the actual posterior distribution over interpretations  our run-time system will actually terminate if either the actual cost has exceeded our specs  or if the highest probability is above the minimal acceptable value. 
　notice  given our specific set-up - e.g.  only 1 types of operators that can only be executed once  and which always succeed  etc. - the policy obtained can be viewed as a  straightline  policy: seek one specific feature  than another  until achieving some termination condition; see figure 1 for an example. in general  this basic dynamic programming approach could produce more complex policies. 
1 	operator dependencies 
our face recognition system will exploit a certain type of operator interactions  using the result of one operator to simplify a subsequent one. that is  after detecting some feature  say the left eye   we expect to have some idea where to find 
vision 
this expected position. to be more precise  let us assume that the search window for some feature is of size pixels. after detecting feature we instead search for f1 in a smaller r e g i o n c e n t e r e d at the location that we computed from 's location  using that linear function. 
　after observing several features  say nose and left eye   we will have several estimates for the location of the current feature  here right eye . here we take the smallest bounding box and use that as the search area for locating the current feature. this  factoring  means we need only consider a set of 1 x 1 transformations  of each of the four feature versus the other three features   rather than deal with all possible subsets. 
　to make these ideas more concrete: initially  given no other information  we would look for the right eye in the region 
 however  after finding the left eye at locaour system knows that  if asked to look for 
the right eye  it should search in the region 
 we are not committing to looking for the right eye at this time; just indicating where to search  if requested.  notice this region is different from the one we would consider if we had not located the left eye; moreover  if we also knew that the nose was at location  =  1   as well as left eye   we would use a yet more refined region - here  

1 	image interpretation policies 
during interpretation  infogain and  the policy produced by  bestseq each iteratively select an operator  recall that the values of r and ♀ are determined from the context  of other detected features ; we therefore do not need to 
specify them here.  infogain chooses an instantiated op-
erator  that has the maximum  

value. bestseq selects an operator where a is the current state and 
from state to actions. in each case  the operator is applied to the appropriate region in the given test face image and the distribution is updated...until one face is identified with sufficiently high probability or the system fails  by exhausting all the possible operators  or cost  
1 	experiments 
we used face images of 1 different people  each 1 x 1 pixels. we assigned 1 images to the set t  another 1 images to s and used another 1 as test images.1 as shown in figure 1  the faces are more or less in the same pose  facing front   with some small variation in size and orientation.1 we considered all 1 operators based on the four features listed above and  {1 1 1} for each feature. the bestseq interpretation policy decides precisely what operator to apply on which portion of the image  given each state  which here corresponds to a specific set of information gathered ; see figure 1. 
basic experiment: we set  
 i.e.  no upper limit on identification cost .1 in each  set-up   we assigned a random probability to each person  corresponding to drawing a sample with replacement . on each run  we picked one face randomly from the test set as the target  and identified it using each of the policies. we repeated this for a total of 1 runs per set-up  then changed the probability distribution and repeated the entire process again  for a total of 
1 set-ups. the accuracy of identification for infogain and bestseq are 1%and 1% respectively.  recall both were required to obtain at least pmin = 1%.  the average cost of identification was 1 1 and 1 1 seconds1 for infogain and bestseq respectively. hence  we see bestseq took much less time than infogain to obtain this level of accuracy. 
the first two rows of table 1 shows how often infogain 
 resp.  bestseq  used one  resp.  two  three  four  operators before terminating  over these 1 runs. we observe the i n fogain applied slightly more operators than bestseq on the average. more importantly  as bestseq can exploit simple operator interactions  the search area it uses for the subsequent operators can be narrower than infogain's. these two factors result in a much lower cost for bestseq. 
bounding the cost: in many situations we need to impose a hard restriction on the total cost; we therefore considered var-
　　1  we assume that any test face-image belongs to one of the people in the training set  but probably with a different facial expression or in a slightly different view  and perhaps with some external features not in the training image  like glasses  hat  etc.   or vice versa. 
1
 1  these 	faces 	were 	downloaded 	from 	the and 
 1  this work assumes the head has 
already been located and normalized; if not  we can use standard techniques  tp1  first. 
   1 the other part of the task  is  identifying people ; this is true for all of the experiments discussed here. 
   1  all the experiments reported in this paper were run on a pentium 1 mhz. pc with 1 mb ram ninning linux os 1.1. 
in the range  1-1  seconds. we then 
picked one face randomly from the test set  and identified the test image for each of these maximal costs  using infogain and bestseq. 
　as always  we terminate whenever the probability of any person exceeds  or if the cost exceeds  returning the most likely interpretation. 
　we repeated this experiment for a total of 1 set-ups  each with a different distribution over the people  and with 1 random runs  target face images  per set-up. we found that 
1% of the policies produced matched the policy shown in figure 1. 
　figure 1 a  plots the accuracy  the percentage of correct identifications  for the policies found for various values of 
 in general  the policies produced by bestseq had better accuracy than the ones produced by infogain. bestseq found different policies for different costs. the bottom 1 rows of table 1 show how often bestseq used one  resp.  two  three  four  operators for different values of before terminating. 
　for low cost values  under 1 seconds   bestseq performs much better than infogain  accuracy of 
versus  as bestseq was able to exploit operator interactions  it was able to apply a second  and probably a third  operator by using a smaller sweep area  within the allotted time. since infogain does not exploit this  it needed significantly more time. 
varying the minimum accuracy: in this experiment  we varied  from 1 to 1. for each of these values  we chose a face randomly from the test set as the target and identified it using each of the two policies. during the process  the first person in the training set for which  is returned  or if cost the most probable face is returned . we repeated this for 1 different faces  runs  per set-up  and repeated the entire process for a total of 1 different set-ups. about 1% of these policies matched figure 1. 
　we evaluated the results in two different ways. first  figure 1 b  compares the percentage of wrong identifications of each policy  for each  value. we see no significant difference in error between bestseq and infogain. the second graph  figure 1 c   compares the average cost of each policy  for each  value. here we see that bestseq has much lower cost than infogain. again  this is because bestseq can use operator interactions to sweep a narrower region. 
vision 


figure 1:  a  cost vs. accuracy 	 b  min. accuracy vs. error 	 c  min. accuracy vs. cost 

1 	conclusions 
future work: this paper explored one type of operator interaction: where the output of one operator can be used to help a subsequent operator- here by reducing the time it will require to search. moreover  our operators were relatively simple  as they always succeeded  or at least  think that they succeed   and they could be run in any order. as a consequence  our policy was quite simple - just straight-line sequence of operators. 
　there are many ways to extend our analysis. first  it might be useful to allow each operator to return not just a position  but also other information  such as confidence  which would become part of the state . the best policy  then  could use that information when deciding on the proper future action to take. if the abstracted state also included other information about the image  such as average intensity  the policy could use that information as well. this could result in a policy that was more complicated and  presumably  more accurate. 
　finally  it would be interesting to consider operators that have some explicit dependency - e.g.  one cannot run the  connect edgep operator unless we have already run some  produce edgel  operator. these precedence constraints would add yet other challenges to our framework. 
contributions: this paper shows how dynamic programming can be used to build efficient interpretation systems. we argue that computing the utility of operator sequences  which incorporate the benefits of operator interactions  can play a significant role towards building efficient interpretation systems. however  in several real world situations  such systems may have  i  to deal with complex and unwieldy states  and  ii  to explore the operator space to find the most promising operator sequence  which is expensive. we addressed the first issue by using simplified abstract states in face recognition  and addressed the second by using an off-line limited lookahead search to find the most promising operator sequence. 
　we framed the image interpretation task as a markov decision problem and used ideas from reinforcement learning to compute the utility of imaging operators non-myopically over a finite lookahead depth in an operator space. we built an interpretation system that uses these concepts and compared its performance with a successful myopic system in face recognition. our results show that the former has a much better performance in various experiments that address the cost and accuracy tradeoffs. vision 
acknowledgments 
the authors gratefully acknowledges the generous support from canada's natural science and engineering research council and the alberta ingenuity centre for machine learning. 
