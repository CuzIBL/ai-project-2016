 
a complete system of learning spelling-tophoneme conversion of english words consists of three major processes: alignment  mapping learning  and grapheme generation. such a system can be used to construct prototypes of reading machines for english or other languages quickly and automatically. this paper focusses on the alignment process  which is critical to mapping learning and grapheme generation. we present several novel alignment algorithms which learn alignment without supervision. the basic alignment algorithm is a hillclimbing algorithm. several improvements of it are studied and tested. in addition  a method that overcomes the pitfall in hill-climbing algorithms is designed. our best alignment algorithm produces very impressive results: only 
1% of error rate. 
1 introduction 
reading english text aloud has been studied successfully for many years with numerous laboratory systems and some commercial systems  see  e.g.  allen  1; allen  
hunnicutt  & klatt  1; kurzweil  1; klatt  1  
1 . in this paper  we focus on only one aspect of reading aloud: isolated word text-to-phoneme conversion  ignoring visual recognition  text analysis  intonation and stress analysis  speech synthesis  and so on . our attention is on automated learning of text-to-speech conversion  rather than  for example  conversions specified by manually designed rules. our learning system can be used to construct automatically in a very short time prototypes of reading machines for english or other languages. 
　given a set of words  each with an orthographic representation  spelling  and a phonological representation  pronunciation   the basic learning task is to learn a mapping from the orthographic representation to the phonological representation  and to predict the pronunciation of unseen words with a high accuracy. a natural approach to model the complete learning process of single-word spelling-to-phoneme conversion requires 
1 	learning 
three major steps  see section 1 for other approaches and their weaknesses . the first step is to align the orthographic representation with the phonological representation. the second step is to learn the mapping from the orthographic to the phonological representations  and the last step is to generate graphemes. these three processes are intimately tied together  and are very complicated to model. most previous work models only one of the three processes. for example  models by sejnowski and rosenberg  1   seidenberg and mcclelland  1  and plaut  mcclelland  seidenberg  and patterson  1  only deal with the mapping learning task; the alignment was done manually. 
　we assume that the starting point of orthographic representation is 1 letters  plus two marks for the beginning and ending of the word . the phonological representation  on the other hand  is a small set of about 
1 standard phonemes as the sound building blocks for english. below are some examples of letter-to-phoneme mappings of single words: speech -  spets  thought -  t*t  and t h r i l l -  t r i l . 
　the first task of aligning the orthographic representation with the phonological representation is necessary because often n letters in spelling of a word maps to m phonemes in pronunciation with  for example  the word thought  which has 1 letters in the orthographic representation  has only 1 phonemes  t*t  in the particular phonological  phoneme  representation that we use.1 therefore  the mapping from the orthographic to the phonological representations is not oneto-one 1 to make the second  learning mapping  and 
     1  there are four cases where a single letter maps to more than one phoneme. they are x as in box  maps to ks in 
boks   j as in just  maps to dz in 
to  and u as in fuel to simplify learning  these  macro  phonemes are replaced by single letters not used in the original phonological representation. 
1
　　 in the phoneme string t*t  t represents the sound th in thought  * represents the sound ough in thought  and t represents the sound t in thought. 
1
　　in sejnowski and rosenberg's nettalk  a silent phoneme is inserted so the alignment is done before learning. taking the mapping t h r i l l -  t r i l as an example  it would be t h r i l l -  t ril ; so the mapping is always one-to-one in 

third tasks  grapheme generation  possible  we have to properly align letters with phonemes so that the learning programs  and children  know which letter  or letter combination  maps to which phoneme. for thought  the first alignment below is correct 1 while the second and third are not. 
thought thought thought t * 	t t*t 	t* 	t  clearly  there is a total of  j  = 1 different ways of inserting 1 spaces in t*t  or 1 ways of aligning thought with t*t. 
　the second task is to learn a complicated mapping from letters to phonemes from pairs of letter strings to phoneme strings which have been aligned. such a mapping is only quasi-regular with many exceptions  and is more complicated than some other language-learning tasks  such as the mapping from verb stems of english verbs to their past tenses. the mapping learning has been studied with symbolic learning algorithms  e.g.  dietterich  hild  & bakiri  1  and connectionist learning algorithms  e.g.  sejnowski & rosenberg  1; seidenberg k mcclelland  1; plaut et al.  1 . 
　it is important to realize that the result of alignment directly affects mapping learning: each possible alignment combination represents one potentially possible mapping to be learned. as mentioned  the mapping 
is only quasi-regular  with many exceptions. it could be that  if thought is aligned with t*t   t maps to t  h maps to *  o maps to t  and the rest of the letters ught map to a blank phoneme - this might be a legitimate mapping to be learned. that is  a bad alignment also 
constitutes a mapping. since many words in the training set have more than one alignment  e.g.  thought has 1   the combination of possible alignments of all the words in the training set is huge  but each represents one potentially possible mapping to be learned. as an example  the data set we use in our study originated from  seidenberg & mcclelland  1   and it contains a total of 1 words  and the combination of all of these alignments of the corpus is estimated to be over 1. the question is  with so many possibilities  how can a learning program learn the correct alignment of all words and mapping based on the newly-learned alignment ef-
fectively  
　alignment should be included as a part of the learning task  instead of being manually derived as in most previous work. however  while mapping learning after alignment is supervised  alignment learning is unsupervised. this work solves this critical problem. lawrence and kaye  1  designed a stand-alone alignment algorithm  but it is not a learning method  see section 1 for more details . 
nettalk. 
1
　　there are other correct alignments of thought with t*t  such as -t* t or -t-* t. in general  as long as t is aligned with one letter in th  and * with one letter in ough  the alignment is regarded as correct. 
　the task of performing simultaneously pattern alignment and pattern learning exists in other areas of language learning as well  such as reading continuous text  word matching in long stretches of speech  pinker  1  page 1   and meaning matching of words in speech  pinker  1  page 1 . the alignment algorithms presented in section 1 should be applicable to other problems of this type. 
　the alignment process is crucial since the correct alignment determines the suitable mapping learning and proper grapheme generation. we have built a decisiontree learning system that models all of these three processes. however  due to limited space and the complexity of the system  we describe only the alignment algorithms in sufficient detail in this paper. 
1 learning mappings with automatic alignment 
in this section  we present several alignment algorithms that utilize c1 as the mapping learning algorithm. c1  quinlan  1  is one of the most popular machine learning algorithms. we first discuss the representation issues in the task of reading aloud  and then describe several algorithms for learning mapping and alignment simultaneously. 
1 	representation issues 
the representation of learning to read aloud used in this 
paper is similar to that in the nettalk  sejnowski & rosenberg  1  - an n-to-1 sliding-window representation  where n is usually called the window size which determines how far the neighbouring letters that may be used in the decision-making process. siding-window representation converts the mapping between n letters to m phonemes into n-to-1 mappings; that is  it becomes a classification problem. thus  only one phoneme is predicted at a time from the letter at the center of the window and its left and right neighbouring letters. the window slides over the letters of the word and predicts the corresponding phonemes one by one. in this paper  we choose the window size as 1 which is enough for the data set we have. thus  a phoneme is predicted based on the middle letter  1 left neighbouring letters and 1 right neighbouring letters. 
　the classifier learns a hypothesis  and uses it to predict all phonemes  including the blank phoneme -  one by one when given the spelling of a new word for testing. more specifically  one phoneme is predicted from the corresponding letter and 1 left and 1 right neighbouring letters at a time  and these phonemes are concatenated to be the phoneme string of the word  with the blank phoneme removed . if one or more phonemes are predicted incorrectly  the whole word is regarded as predicted incorrectly. 
　we use the most popular machine learning classification algorithm c1  quinlan  1  with its default parameter setting as our mapping learning mechanism. c1 is an improved implementation of the id1 learning 
	ling & wang 	1 
algorithm  cf. quinlan  1 . c1 induces classification rules in the form of decision trees from a set of classified examples. it uses information gain ratio as a criterion for selecting attributes as roots of the subtrees. the divide-and-conquer strategy is recursively applied in building subtrees until all remaining examples in the training set belong to a single concept  class ; then a leaf is labeled as that concept. the information gain guides a greedy heuristic search for the locally most relevant or discriminating attribute that maximally reduces the entropy  randomness  in the divided set of the examples. the use of this heuristic usually results in building small decision trees instead of larger ones that also fit the training data. 
1 	t h e basic a l i g n m e n t a l g o r i t h m 
as we discussed in section 1  learning mapping while aligning words is a challenging task  alignment learning is unsupervised  since each possible alignment represents a potentially possible mapping to be learned  and there is a huge number of possible alignments  1 in our data set . the key idea in solving this difficult problem is that most of the 1 mappings do not contain much regularity at all. that is  if many words are aligned incorrectly or inconsistently  there is little regularity to be learned  and it becomes almost impossible to predict the phonemes of a new word. therefore  our basic alignment algorithm is based on the fact that the proper alignment should be consistent among words  and the prediction based on aligned words should be consistent with the correct alignment of the new word. 
　the basic alignment algorithm is a hill-climbing algorithm that gradually builds up the set of aligned words. from a set of words that have already been aligned  we call this set a converged set   a decision tree is built using c1  and it is used to choose the best alignment of an unaligned word. the best alignment is then added into the converged set. more specifically  an unaligned word from the unconverged set  containing all unaligned words  is aligned in the following way: whenever the new word has more than one possible alignment  that is  the word is an n -  m mapping with n   m   a prediction of the word using the decision tree built on the current converged set  of aligned words  is produced first. as we discussed earlier  the prediction based on aligned words should be consistent with the correct alignment of the new word. the prediction is thus compared with all possible alignments of the word  and the alignment most consistent1 with the prediction is chosen as the correct one. the chosen alignment  which hopefully is correct  is then added into the converged set  the decision tree is updated1 with the inclusion of the newly aligned word  and the process is repeated. as the set 
1
　　the consistency between two words  i.e.  prediction and alignment  is determined simply by the number of different phonemes in the two words at the corresponding positions. 
1
　　currently c1 is applied to the enlarged training set directly. however  id1  utgoff  1  could be used and the decision tree would only be updated. the resulting decision 
1 	learning 
of the aligned words increases  the decision tree algorithm learns more varieties of mappings from letters to phonemes  the alignment of the new words becomes more and more accurate. the basic alignment algorithm is presented in table 1. it is very similar to the one proposed by bullinaria  1  for the connectionist model  except that the difference between two words is calculated on the output units in his model. 
conv = empty 	 * converged set  empty to begin with *  unconv ＊ l i s t of training examples 	 * unconverged set * repeat 
take one word w from unconv 
if w is n -  n   no alignment is needed *  then add w to conv  delete w from unconv update the tree t based on conv 
else insert space in w at different places obtaining possible alignments w i  w 1 ... use t to predict w  the prediction is u compare u with w i  let e i ＊ difference u  w i  let e k - min e i  
w.k is chosen as the correct alignment of w insert w k to conv  delete w from unconv 
update the tree t based on the new conv 
until the unconv set is empty 
table 1: the basic alignment algorithm. 
　let us see an example. if we have a set of words that have been properly aligned  and we want to align a new word speech mapping to spec. since this is a 1 -  1 mapping  there are 1 ways of inserting two blank phonemes in spec  or 1 possible alignments. for example  spec  s-p ec  spe-c-  and so on. obviously  the third one is the correct alignment. if the set of aligned words  in the converged set  contains words with letters to phoneme mappings for s  p  ee  and ch  then the prediction of speech from the current decision tree would be spe-cl  which is correct. in this case  the prediction of the new word is the same  consistent  as one of the possible alignments  hence it  i.e.  spe c   is taken as the correct alignment and is added into the converged set of the aligned words. however  when the current training set does not contain enough varieties of words  the prediction may not be entirely correct. for example  the prediction of speech from the current decision tree could be spe kh  i.e.  the ch part has not been learned yet. still  we find spe c   or spe c  among other 1 possible alignments is closest to the prediction spe-kh  since there are only two errors  the last two phonemes  between them; while there are  for example  four errors between spe kh and s p ec. in this case  we take the best alignment  still spe c   or spe c   as the correct alignment  and add it into the converged set of aligned words. this time  the chosen alignment is correct. 
tree would be equivalent to the one obtained by applying c1  id1  on the enlarged training set. 
1 improvements on the basic alignment algorithm 
we found that the basic alignment algorithm makes an excessive number of misalignments: the error rate1 is over 1%. thus  we study several extensions and improvements of it in the following subsections. these improvements include incorporating a tie breaking policy  ordering the words from easy to complex  employing a conservative criterion for accepting aligned words  and correcting previously misaligned words. 
tie breaking policy 
in an initial implementation of the basic alignment algorithm  we found that ties often occur: several possible alignments have the same closest distance to the prediction produced by the decision tree.  in the example of the previous subsection  two alignments spe-c  and spe c tie with the prediction spe kh with two errors . breaking tie randomly ends up with many incorrect and inconsistent alignments. a tie breaking policy is introduced: when there is a tie in alignment  the word is put back at the end of the current list of unconverged words. that is  if there is no unique best alignment  the decision is delayed until more knowledge of alignment is learned. 
　when the alignment algorithm  with the tie breaking policy  takes words from the training set in a random order  it still produces quite a few misaligned words - a total of 1 misaligned words among 1 words in the training set.1 the reason for this is that before a large body of alignment knowledge is accumulated  the prediction of a more complex word  such as an n -  n-1 word  contains many errors. in this case  the difference between the prediction and every possible alignment of the word is large; thus  even if the best alignment is unique  it is often incorrect. misalignments in the converged set spread - they cause  in turn  more misalignments in the training set. 
learning from easy to complex 
clearly  if a word is an n -  n mapping  then there is only one possible alignment. the word can be added directly into the converged set for learning the mapping from letters to phonemes. therefore  n -  n words are regarded as  easier  words in learning alignment than n - -   n - 1   than n -  n - 1 words  and so on. hence  if n -  n words are learned first  then the predictive accuracy of n -  n - 1 words would be high  since the mapping of all phonemes except one  to which two letters 
1
　　since no  teacher  provides correct alignment to the learning program  this is essentially an unsupervised learning task. the error rate here is thus the testing error rate instead of the training error rate in supervised learning. 
1
　　these 1 misaligned words are too many to list in the paper. note that there are normally several correct alignments for a word  and the 1 words are those certainly misaligned. also note that misalignment does not necessarily imply an incorrect prediction. however  a training set with many misaligned words constitutes a much more complicated mapping  and thus  predictive accuracy of new words would be lowered. 
map  have likely been learned already in n -  n words. in this case  the correct alignment will more likely be found. 
　the implementation of this improvement algorithm is the same as the one in section 1 except the initial list of unconv contains words ordered from easy to complex. that is  n -  n words go first  n -  n - 1 words next  and then n -  n - 1 words  n -  n - 1 words  and n -  n - 1 words. the number of misaligned words produced by the algorithm is much smaller; only 1 among a total of 1 words  only about 1% of error . some examples of misaligned words are breast -  bres-t  should be bre st   shed -  se d  should be s ed   says -  sez   should be se-z   and shall -  s al  should be s a l or s al  . however  some mistakes are consistently made for several words  such as days  jays and says . this indicates again a pitfall in the hill-climbing algorithms - since there is no backtracking mechanism for correcting previously made mistakes  mistakes are likely to spread further. 
learning more conservatively 
another improvement over the basic alignment algorithm described in section 1  which takes words in random order  is to adopt a conservative policy that restricts words accepted into the converged set. this conservative policy reflects the idea that  if the current learned knowledge does not produce an answer close enough to the correct one  its alignment decision is delayed until more knowledge of alignment is learned. as we have noted  if the difference between the prediction and every possible alignment of a word is large  it signals the possibility that not enough alignment is learned  and that the alignment decision on this word may be premature. this conservative policy puts a bound on such a difference; only when the difference between the prediction and the best alignment is within the bound  is it chosen as the correct alignment. this bound is increased gradually  from 1 to 1 . 
　the results of this algorithm  which takes words in random order  are very impressive. the number of alignment mistakes is very small; among a total of 1 words  there are only 1 misaligned words. this represents less than 1% of error rate in alignment. again  since this is an unsupervised learning task  the error rate is the testing error rate  instead of the training one. 
　the misaligned words by our algorithm are listed in table 1. as we can see  several types of errors occur consistently in more than one word. five words have an alignment error in i f f part  three words in ays part  and two words in ugh part. this again points out the pitfall in the hill-climbing strategy used in all of these alignment algorithms. in the next section we present a method of overcoming this pitfall. 
correcting misalignments 
from the results of the misaligned words in the previous subsections we observed one phenomenon: certain mistakes occur consistently among several similar words  e.g.  days  jays  and says . this reflects the pitfall of 
	ling & wang 	1 

table 1: misaligned words; training with a conservative policy. only one correct alignment is listed for each word. 

the general hill-climbing algorithm - there is no backtrack mechanism. wherever for some reason a mistake is made at an early stage  it is likely to propagate  and no correction is performed. admittedly  mistakes should be allowed  for this is part of the life in human learning as well. people  however  after learning more knowledge  often realize mistakes made earlier and correct them. we present a correction algorithm  which corrects alignment mistakes made in the hill-climbing algorithms. 
　the correction algorithm takes as input the list of aligned words  output containing misalignments from one of the previous alignment algorithms . since the knowledge of alignment can now be built on a large number of aligned words and thus is more complete  mistakes  misalignments   especially early mistakes based on a small set of the converged words  may now be corrected. the correction of such words may help in correcting other misaligned words. 
　given a list of aligned words ordered by the output of the alignment algorithm  the correction algorithm takes out the first 1% of the words  those aligned very early when little about alignment and mapping had been learned   learns the mapping based on the remaining 1% of the words  and re-aligns that first 1% of the words. that is  words aligned earlier are more likely to have alignment mistakes  and they are re-aligned by the large number of more recently aligned words  in the hope that some earlier  premature mistakes can be corrected. those 1% re-aligned words are then added to the end of the list  and the next 1% of the words are taken out for correction  by the rest of the words  including previously re-aligned words . after one round of 1 correction sessions  every word in the list has been re-aligned once. the correction algorithm terminates if no correction has been made after one round. 
　we apply the correction algorithm to the list of the aligned words from the algorithm with the tie break-
1 	learning 
ing policy in section 1 with a total of 1 incorrectly aligned words 1 and the result is presented in table 1. numbers listed under the column  correction  in the table represent the numbers of words that have been corrected. that is  they are misaligned before correction  and properly aligned after correction  such as boss -  b os before  boss -  bo s after . numbers listed under the column  miscorrection  represent the mistakes made by the correction algorithm. these are the words that are properly aligned before correction  but misaligned after correction  such as h i l l -  hijl before  h i l l -  h il after . this happens because the misaligned words in the 1% of the words can affect the re-alignment of the 1% of the words. numbers under the column  improvement  are simply the difference between  correction  and  miscorrection . they represent the net improvement accomplished by the correction algorithm. there are several other outcomes of the correction algorithm: both words  before and after correction  are correct  both words are correct but the alignments are different  or both words are incorrectly aligned. however  these results are not reflected in the table since they do not affect the net improvement of the correction algorithm. 
table 1: the outcomes of the correction algorithm. 
correction miscorrection improvement   1st 1% 1 1 1 1nd 1% 1 1 1 1rd 1% 1 1 1 1th 1% 1 1 - 1 1th 1% 1 1 1 1th 1% 1 1 1 1th 1% 1 1 1 1th 1% 1 1 1 1th 1% 1 1 1 1th 1% 1 1 1 total 1 1 1 　from table 1 we can see that  in general  the number of corrections is high for the early part of the list  especially the first 1% . this confirms our expectation that early learning is less mature and more prone to errors. overall  there is a marked improvement after one round 
 1 correction sessions  of correction  i.e.  each word is re-aligned once . the net improvement is 1 for the first round. in the second round  details not shown here   the total number of  corrections  is 1  and  miscorrections  is 1. overall  there are 1  1 + 1  improvements after two rounds  thus reducing the total number of misaligned words from 1 to 1  a 1% reduction. 
1
    results from subsequent improvements contain too few misalignments to show the effect of the correction algorithm. 

1 relation to past work 
much research in text-to-speech conversion has been done with a few commercial systems  see klatt  1  for an excellent review . however  most commercial systems are not based on the learning or automated knowledge acquisition that we study in this paper. our learning system can be used to construct prototypes of reading machines for english or other languages quickly and automatically. 
　some past work on learning to read aloud came from the connectionist researchers. sejnowski and rosenberg  1  first designed a connectionist model for text-tospeech conversion  but their model only solved the mapping learning problem - it did not deal with the alignment problem and grapheme generation. phonemes of words had already had a special symbol inserted by hand for the silent phoneme. we adopted the sliding-window representation from nettalk in our system. 
　bullinaria  1  recently extended sejnowski and rosenberg  1 's nettalk by adding an alignment algorithm. our basic alignment algorithm described in section 1 is inspired by his method. however  the basic alignment algorithm produces an excessive number of misalignments. we have improved it in several directions  see section 1 . the error rate of our best alignment algorithm using the same dataset as his is very low  1% . bullinaria's model does not deal with grapheme generation. 
　lawrence and kaye  1  designed a stand-alone alignment algorithm  but it is not a learning method. a table of phonological-to-orthographic correspondences is designed by hand and given to the alignment algorithm. the table is actually quite large - it has 1 entries. when testing this method on 1 words  
1 words were misaligned  representing an error rate of 1%. our alignment algorithms learn the alignment without supervision. 
　the decision-tree learning algorithm id1 had been applied to the nettalk data previously  dietterich et al.  1   but the method was applied to the nettalk data set  and thus the alignment problem was not studied. 
1 conclusions 
we describe several methods for aligning letters with phonemes. alignment is critical to the mapping learning and grapheme generation. our best alignment algorithm produces very impressive results: less than 1% of a total of 1 words are misaligned. we also discuss a 
correction method for correcting previously misaligned words. the idea can be used in other hill-climbing search algorithms to improve their results. in future  we plan to use our method to construct prototypes of reading machines for other languages. 
a c k n o w l e d g m e n t s 
we like to thank gratefully john bullinaria for providing with us the data set used in his study  and for numerous discussions on the topic. the data set originally came from  seidenberg k mcclelland  1 . discussion with david plaut has also been helpful. reviewers also provided useful suggestions to the paper. 
reference 
allen  j.  1 . synthesis of speech from unrestricted text. in proc. ieee 1  pp. 1. 
allen  j.  hunnicutt  s.  k klatt  d.  1 . from text to speech: the mitalk system. cambridge u.p.  cambridge  uk. 
bullinaria  j.  1 . representation  learning  generalization and damage in neural network models of reading aloud. submitted to psychological review. 
dietterich  t.  hild  h.  k bakiri  g.  1 . a comparative study of id1 and backpropagation for english text-to-speech mapping. in proceedings of the 1th international conference on machine learning  pp. 1. morgan kaufmann. 
klatt  d.  1 . review of text-to-speech conversion for english. journal of the acoustic society of america  1   1. 
klatt  d.  1 . the klattalk text-to-speech system. in proc. int. conf acoust. speech signal process. icassp-1  pp. 1. 
klatt  d.  1 . how klattalk became dectalk: an academic's experiences in the business world. speech tech  1  1. 
kurzweil  r.  1 . the kurzweil reading machine: a technical overview. in reden  m.  k schwandt  w.  eds.   science  technology and the handicapped  pp. 1. 
lawrence  s.  k kaye  g.  1 . alignment of phonemes with their corresponding orthography. computer speech and language  1  1. 
pinker  s.  1 . the language instinct. william morrow and company  inc. 
plaut  d.  mcclelland  j.  seidenberg  m.  k patterson  k.  1 . understanding normal and impaired word reading: computational principles in quasiregular domains. psychological review  1  1. 
quinlan  j.  1 . induction of decision trees. machine learning  1   1 - 1. 
quinlan  j.  1 . c1: programs for machine learning. morgan kaufmann: san mateo  ca. 
seidenberg  m.  k mcclelland  j.  1 . a distributed  developmental model of word recognition and naming. psychological review  1  1. 
sejnowski  t.  k rosenberg  c.  1 . parallel networks that learn to pronounce english text. complex systems  1  1 - 1. 
utgoff  p. e   1 . incremental induction of decision trees. machine learning  1  1 - 1. 
	ling &' wang 	1 
