 
we apply reinforce merit learning methods to learn domain-specific heuristics for job shop scheduling a repair-based scheduler starts with a critical-path schedule and incrementally repairs constraint violations with the goal of finding a short conflict-free schedule the tem poral difference algorithm  is applied to tram a neural network to learn a heuristic evaluation function over states this evaluation function is used by a one-step lookahead search procedure to find good solutions to new scheduling problems we evaluate this approach on synthetic problems and on problems from a nasa space shuttle pay load processing task the evaluation function is trained on problems involving a small number of jobs and then tested on larger proble ms the td sclied uler performs better than the best known existing algorithm for this task-zwehen s iterative repair method based on simulated annealing the results suggest that reinforcement l arn mg can provide a new method for constructing high-performance scheduling systems 
1 	introduction 
many problems of commercial interest-including job shop scheduling-are instances of np-complete problems hence  there is little hope of finding generalpurpose solutions to these problems however in any particular application setting there are usually domainspecific constraints and regularities that can be exploited to construct fast  domain-specific heuristic algorithms while such domain-specific heuristics can be engineered bv hand  the process is expensive and time-consuming i he goal of the research described in this paper is to explore the possibility of applying reinforcement learning algorithms lo discover good domain-specifie heuristics automatically 
　reinforcement learning algorithms learn policies for state-space problem-solving tasks for each state  the policy specifies what action should be performed during learning  the learning system receives a reinforcement signal  called a ' reward'   after each action the 
1 	learning 
	thomas g 	diettench 
department of computer science oregon state university 
	corvalhs  oregon 	1-1 
u s a 
goal of tht learning system is to find a policy that maxi mizes the expected reinforce ment over future actions in the context of job shop scheduling   he policy tells what scheduling action to make next in order to maximize some measure of the quality of the final schedule 
　in this paper we focus on the application domain of space shuttle payload processing for nasa the. goal is to schedule a set of tasks a set of temporal and resource constraints while also seeking to minimize the total duration  makespan  of the schedule of particular interest to nasa are scheduling methods that can also be used to re pur a schedule when some unforeseen diffi cult  arises in previous work on this task zweben and colleagues  zwehen  developed an iterative repair-based scheduling procedur  that combines a set of heuristics with a simulaled annealing se arch procedure the resulting scheduling system provides an efficient and flexible facility for scheduling space shuttle ground operations it is in regular use at the kennedy space center  deale ti al   1  the challenge tor a 1 earning approach is to discover scheduling heuristics that ian match or exceed the quahtv- and efficiency of t ins itera five repair method 
　in the remainder of the paper we describe tht scheduling task in greater detail we then briefly describe 1weben s iterative repair-based scheduler follow ing this we review the reinforcement learning method known as  and describe how the scheduling task can be formulated so that  can be applied we then describe our experiments on simulated problem sets and discuss the results these results indicate that reinforcement learning can outperform the iterative repair scheduler on realistic scheduling tasks furthermore the knowledge learned through reinforcement learning can be applied to scheduling problems that are larger and more complex than the ones that were studied during training these initial results suggest that reinforcement learning has an important role to play in developing high-performance ai scheduling systems 
1 the nasa domain and the iterative repair method 
the nasa space shuttle payload processing  sspp  domain requires scheduling the various tasks that must be performed to install and test the payloads that are 
placed in the cargo bay of the space shuttle 	in jobshop scheduling terminology each shuttle mission is a 
job each job consists of a partially-ordered set of tasks that must be performed each task has a duration and a list of resource requirements for example  the task mission -sequence -test has a duration of 1 and requires two quality-control officers  two technicians  one ate one spcds  and one hits there are 1 different tvpes of resources there may be many units of a resource available for example  there are 1 quality control officers available and 1 technicians however these available resources may be split into resource pools so that for example  the 1 quality control officers might be subdivided into three pools of size 1  and 1 if a task requires two quality control officers they must both be drawn from the same pool resource pools model multiple work shifts and multiple physical locations a complete schedule must specify the start time of each task and the resource pool by which each resource require ment of each task is satisfied 
　a typical sspp problem involves the simultaneous scheduling of between two and six shuttle missions t cash mission involves between 1 and 1 tasks hencr th  sspp domain requires solving scheduling problems con taming several hundred tasks most of these tasks must be performed prior to launch but some also take place after the shuttle has landed fach shuttle mission has a fixed launch date but no starting date or ending date hence tasks prior to launch have deadlines but no ready times tasks after landing have readv times but no deadimes a key goal of the scheduling system is to minimize the total duration of the schedule this is much more challenging than simply finding a/eaiime schedule 
　 weben et al 1 developed the followingiteralive re pair method for solving this scheduling problem first a critical path schedule is constructed bv working backward and forward from the launch and landing dates fach task prior to launch is scheduled as late as the l  mporal partial order will permit each task after landing is scheduled as earl  as the temporal partial ordtr will permit resource constraints are ignored  resource requests are randomly assigned to resource pools this critical path schedule can be constructed verv efficiently  and it provides the starting state for the scheduling problem space in each state of this problem space there are two possible operators that can be applied the r f a s s i g n p o o l operator changes the pool assignment for one of the resource requirements of a task it is only applied when the pool reassignment would allow the resource re=quirement to be successfully satisfied the m o v f operator moves a task to a different time and then reschedules all of the temporal dependents of the task using the critical path method  leaving the resource pool assignments of the dependents unchanged  the m o v l operator is only applied to move a task to the first earlier or the first later time at which the violated resource requirement can be satisfied 
　these two operators are applied by the iterative repair method as follows at each step  the earliest constraint violation  l e   where a resource pool is over-allocated  is identified if a reasslgn-pool operator can be applied 
to reduce this over-allocation  then it is applied if not  then the move operator is applied to move one of the offending tasks to an earlier or later time if several different pool re assignment a are possible  one is chosen at random if both an earlier and a later move are possible  then one is chosen at random of the several tasks involved in the resource violation one is chosen at ran dom based on a heuristic that prefers to move the task that  a  requires an amounl of resource nearly equal lo the amount that is over allocated  b  has few temporal dependents and  c  needs to be moved onlv a short distance lo satisfy the resource request 
　the overall control structure of the algorithm applies simulated annealing to minimize the number of resource pool violations after each operator is applied the num her of violations m the resulting sche dule is computed if this has decreased  the resulting schedule is accepted as the  current' schedule if it has increased the resuiting schedule is accepted only willi probability ' * ' / r t   where  is the change in tin number of violations and 1 is the current temperature the temperature is grddu ally  decreased sean h proceeds until no constraints are violated to obtain a short schedule the algorithm is run sveral tunes and the shortest resulting schedule is selected 
1 	reinforcement learning  temporal difference learning  and scheduling 
he mforcement learning methods learn a policy for selecting actions in a problem space the pohev tells for   ach state which action is to be performed in that state after an action a is chosen and tpplied in state the problem space shifts lo slate  ' and the learning system receives reinforcement 
　to view the scheduling problem as a reinforcemc nt learning problem we must describe the problem space and the reinforcement function ft wt employ the same problem space as zweben et al the starting statt sn is lhe critical path schedule as discusstd above we define the reinforcement function to give a reinforcement of -1 for each schedule ' that still contains constraint violations this asse-sscs a small penalty for each scheduling action  reassign-pool or movl  and it is intendi d to encourage reinforcement learning to prefer actions that quickly find i good schedule for any schedule s' that is free of violations  the re mforcenient is the negative of the resource dilation factor 
 the rdf attempts to provide a scale independent measure of the length of the schedule and this final reinforcement is intended to tncourag* reinforcement lea iiing to find short final schedules because the reinforcement function depends only on the resulting stat    we will write it as . 
the rdf is defined as follows l  i capacity{i  be the 
 fixed  capacity of resource type i-that is the combined capacity of all resource pools of resource ty pe i at each time t in the schedule let  be the current utilization of resources of type i ifu it  ' capacity i   then the resource of type i is overallocated at time t  no matter how we assign resource requests to resource pools of this type  we define the resource utilization indez 
zhang and dietterich 

for resource type t at time t to be 
if the resource is not over-allocated  	oth-
erwise it is the fraction of overallocation 
　the total resource uttltttzatton index  trui  for a schedule of length / is the sum of the resource utilization index taken over all n resources and all i times 

given these definitions  the resource dilation factor is defined as 

　to understand the rationale behind this formula  first note that in the final schedule s   is just n times the length of the schedule this is because in the final 
schedule  no resource is overallocated  
hence  we could have used the neg ative of this value as the reinforcement function  but reinforcement learning is easier if the reinforcement function is independent of the difficulty of the scheduling problem a very difficult problem  e g   with man  jobs that have simultaneous deadlines  would require a very long schedule  whereas a simple problem would require a much shorter schedule the total resource utilization index of the initial schedule  measures the amount of overallocation of resources in the initial state  and hence  provides a crude measure of the difficulty of the scheduling problem hence we use this to normalize the final schedule length to produce the resource dilation factor 
　now that we have specified how to view repair-based scheduling as a reinforcement learning problem  we turn our attention to the learning algorithm suppose at a given point in the learning process we have developed policy which says that in state s the best action to select is a = we can define an associated function /t   called the value function  such that tells the cumulative reward that we will receive if we follow policy from state 1 onward formally  where n is the number of steps until a conflict-free schedule is found 
   as in most reinforcement learning work  we will attempt to learn the value function of the optimal policy  denoted   rather than directly learning once we have learned this optimal value function  we can transform it into the optimal policy via a simple one-step lookahead search to choose the best action in state s we compute the state a s  that would result from applying each possible action a to state s for each such action  we compute the value of the resulting state   and choose the action a that maximizes this value note that this approach requires that we know the effects of our operators-which is certainly true for repair based scheduling operators 
　to learn the value function  we can apply the method of temporal difference learning known as  developed by sutton 1 in  the value function is 
1 	learning 

and updates the weights of the network according to 

here  is a smoothing parameter that combines previous gradients with the current gradient in ej  and a is the 
learning rate 
　the  algorithm was designed to learn the value function for a stationary markov random process such as would result from following a fired policy in reinforcemenl learning however  we want to apply it to learn the value function of the optimal policy starting with an initial random pohey to do this  we employ a form of value iteration  is applied online to the sequences of states and reinforcements that result from choosing actions according to the current estimated value function / at each state s during learning  we conduct a onestep lookahead search using the current estimated value function j to evaluate the states resulting from applying each possible operator we then select the action that maximizes the predicted value of the resulting state ; ' after applying this action and receiving the reward we update our estimate o f / to reflect the difference between the value of  and the more informed value 
  we actually employ a slightly more com plex procedure described below   this means that the policy is continually changing during the learning process fortunately   will still converge under these conditions  sutton  1  
　there are five further modifications that we made to this algorithm based on preliminary experiments first for any reinforcement learning algorithm it is critical to perform some kind of exploration to discover new and better ways of getting from the start 1tate to the goal we employed the following simple exploration strategy at each state  with probability  we choose a random action instead of the action recommended by the current value function and policy initially  is set to 1 after each action  is decreased by an amount . until il reaches a final value of 1  the values used for are given below   
　second  we do not perform weight updates m the neural network after each action instead  we remember the sequence of states visited along the path from the start ing state to the final conflict-free schedule then we update the network starting with the final action and working backward to the start of the action sequence experimentally  this works better than simple online training  because the values being backed up are more up-to-date 

　third  we employ lin's experience replay method during learning  the best sequence of moves from start to goal is remembered  and after every four training sequences  we update the network using this best training 
sequence this improved learning and performance significantly 
　fourth we do not emplo  a full one-step lookahcad starch to select  actions  because the branching factor in this problem space is typically 1 and it is costly to compute the value of each of these 1 successor stites instead  we employ random sample greedy search which generates a random 1ubse1 of the possible operators and evaluates their resulting states the best of these operators is then chosen the size of the random sample is determined incrementally an initial sample of four actions is chosen based on the resulting computed values and a permitted amount of error t and desired confidence  we can compute the probability that the value of  ik best sampled action is within e of the best possible action we continue sampling possible actions until this probabihtv 'e xceeds 
random sample greedy search is employed during both training and execution 
　the final change in the learning algorithm is that we tlo not use the actual stales of the scheduling process as input to the neural network the neural network can accept only a fixed vector of feature values describing each state  i e each current schedule  schedules on the other hand are variable length objects hence it was necessary to define a set of useful features that extract important aspects of the current schedule that the neural network can use to predict ihe value of the stale we defined the following features  based on a v  ry modest amount of experimentation  
　mean and standard deviation of the free pool capacity for bottleneck pools simple experiments showed that only the technician logistics electrical en gineer  mechanical engineer  and quality control officer resource types became major bottleneck resources for each bottleneck pool  the number of unallocated units  the free capacitv   is measured over the whole schedule period and the mean and standard deviation of this quantity provide two features for each pool 
	mean and standard deviation of slaeks 	the 
slack time between a task and one of its temporal prerequisites is the difference between the end time of the prerequisite task and the scheduled start time of the task we measure the minimum slack for each task  and all of its temporal prerequisites  and the average slack for each task the mean and standard deviation of these two quantities taken over all tasks provide four features 
　modified r d f we used a slightly modified version of the resource dilation factor of the current schedule the numerator of the modified rdf is computed using the capacity and allocation of individual resource-pools 
rather than of resource types 
　over-allocation index this is the total number of units of over-allocated resources in the current schedule divided by the total number of units of over-allocated resources in the starting schedule 
	percentage of windows in violation 	a window 
is defined to be a maximal period of time during which the set of currently scheduled tasks does not change a schedule can be segmented into a sequence of windows we compute the percentage of windows that contain a constraint violation we also find the earliest window lhat contains a constraint violation and compute the per centage of the following 1 windows that have violations 
　percentage of windows in violation that can be resolved by pool reassignment this is the fraction of those windows having constraint violations where the total amount of resources assigned is actually less than the total capacity  so that-if the resources were not subdivided into pools-the resource requirements could be met 
　percentage of time units in violation this is measured over the whole schedule pterod 
　first violated window index  normalized  let wa be the index of the earliest window that has a violation let w be the total number of windows then this feature is as violations arc repaired  this value decreases to zero if no window has a violation  we 
　fach of these features was developed by studying small scheduling problems to find quantities that had some ability to predict rdt however we believe that these features can be improved substantially and this is a goal of our ongoing research 
　a consequence of using these features instead of the full state is that the learned policy may enter infinite loops we have taken two steps to detect and prevent these loops first the randomness introduced by the random sample greedy procedure and by the random exploration process tends to avoid loops because even when the same statf is revisited  the same action may not be chosen second  all states visited while solving a particular problem are recorded and checked to detect loops when a loop is detected we apply the learned value function to compute the second best action and choose it if a loop is detected again at the same state  we backtrack to the preceeding state and again take the second best action if this were to create a loop also  we would continue backtracking to earlier states 
1 	m e t h o d s 
we briefly describe the methods applied to generate the training and lest problems  the network architecture and tht parameters employed in the learning algorithm 
1 	p r o b l e m sets 
we constructed two problem sets an artificial problem set and a problem set based on specifications for the nasa sspp problem the artificial problems were generated as follows first we generated a pool of 1 jobs from these  we constructed scheduling problems by choosing random subsets of these jobs this was in tended to model the nasa setting where there are only a limited number of possible shuttle cargo-bay configura tions  l e   jobs   but where each scheduling problem is a unique combination of such shuttle missions more generally  this models a job shop where each new scheduling 
zhang and dietterich 

interval requires scheduling a unique mix of more-or less standard jobs 
　to generate a synthetic job we choose the number of tasks randomly in the range 1 to 1 a set of temporal constraints among these tasks is then randomly generated such that approximately 1% of all possible pairwise precedence constraints are asserted 
　next  resource requirements are determined for each task there are two types of resources each resource has two pools-one pool has a capacity of 1 units and the other has a rapacity of 1 units resource requirements are randomlv assigned to each task uniformly in the range from 1 to 1 units for each resource tvpe 
　once the pool of 1 jobs is generated in this way  1 training problems and 1 test problems are constructed to generate a problem  we first choose the number of jobs in the problem to be either 1 or 1  with equal prob ability  the desired number of jobs is selected randomly with replacement from the 1-job pool each job is assigned a completion deadline with the deadlines randomly separated by between 1 and 1 time units 
　sixteen input features are computed to represent schedules for these problems h pool capacity features for the 1 pools 1 slack features and features describing the modified rdf percentage of windows and time units in violation  and percentage of violated windows in which the violation can be resolved by pool reassignment 
　during training 1 of the 1 training problems were held out as a validation set to determine when to halt training the remaining 1 problems were repeatedly processed to train the value function networks 
　in addition to the 1 test problems  we generated a second test set of 1 larger problems to evaluate the ability of the learned value functions to scale up to larger scheduling problems each of these larger problems was generated in the same way as the smaller problems except that the number of jobs was chosen uniformly between 1 and 1 
　for the space shuttle pay load processing task  a prob lem consists of a set of shuttle missions with launch dates one to three months apart lach mission can have one or two pavloads we considered three kinds of pay loads long module  lm   mission peculiar equipment support structure  mpess   and pallet and igloo  pallet & igloo  these have 1  and 1 tasks  respectively there are 1 types of resources of which only five are major bottleneck resources 
　we randomly generated a training set of 1 problems and a test set of 1 problems the training problems each contained between two and four shuttle missions of the 1 training problems.  1 were held out for validation to determine when to stop training the test problems each contained 1 to 1 shuttle missions the test problems thus assess the ability of the learned policy to 
scale up to larger problems 
for the shuttle problems  1 input features are used 
1 features for pool capacity  1 slack features  modified rdf 1 features describing windows in violation  percentage of time units in violation  index of firat violated window and the overallocation index 
1 	learning 
1 	network architecture and training procedure 
to represent the value function  we trained feed-forward networks having 1 sigmoidal hidden units and 1 sigmoidaloutput unite the 1 output units encode the predicted rdf using the technique of overlapping gaussian ranges  pomerleau  1  as follows each output unit represents one assigned rdf value  for the artificial problems  these rdf values are 
for the sspp problems  the 
rdf values are v  	  during training  the target output activation for each output unit is set to be 
where is the standard normal probability densi ty function with mean and standard deviation a 
during testinr the predicted rdf value is computed as 
where aci1 is tht actual output 
activation for output unit j 
　for each problem 	we lrained eight different networks  using all combinations of the following parameters learning rate 	exploration schedule and 	 preliminary experiments showed that 	did not perform as well   	the training set problems are processed in round-robin fashion each problem is solved using one of the networks to obtain a sequence of states and actions that network is then updated  via barkpropaga. tion with  by processing t he st at e sequence working backward from the final state after every 1 passes through the training set a cross-validation test is conducted to compute the average rdf of the final schedules produced over all cross-validation problems 	the best network found during cross-validation  for each of the eight parameter sets  is retained for each network training continues until the cross-validated rdf of that network is worse than the previous nine m  asured values for cross-validated rdf 
　six networks are chosen for testing as follows the three best networks found during cross validation are retained along with their corresponding final networks we retain the final networks to compensate for variance m the cross-validation measurements 
　for the simulated annealing component of tht iterative repair method  we set the starting temperature to 1 for the synthetic scheduling task and to 1 for the sspp task after every 1 accepted repairs to the schedule the temperature is reduced according to 
1 	results 
figure 1 shows the average cross-validation rdf for the four value function networks trained with  the horizontal axis gives the number of training sequences processed this figure shows that the performance of the trained networks is improving on the cross-validation problems figure 1 plots the number of repair actions for these same networks this shows that there is some reduction in the number of actions required to convert the starting schedule into a conflict-free final schedule 
　figures 1 compares the performance of temporal dif ference  td  scheduling with the iterative repair  ir  


figure 1 	average number of repairs over 1 
    problems 


figure 1 performance comparison of td toir on 1 medium scale problems 

method of zweben the vertical axis is the rdf of the best conflict-free schedule found so far the horizontal axis is a machine-independent proxy for the amount of cpu game consumed by each method for ir the horizontal axis gives the number of restarts of the simulatid annealing procedure and the vertical axis records the h d t of the best conflict-free schedule found so far the longer ir is run  the better its performance 
　for the td scheduler the horizontal axis represents the number of neural network evaluation function;s employed when k networks are used to soive a scheduling problem  the problem is sohed k times once with cach network and the schedule having the best r d f is returned as the answer the beet k networks  as deter mined by cross-vahdation are used the curves stop at k = 1  because only six networks were used  once each  
　some care must be taken in interpreting the horizontal axis as a measure of cpu time fach step of lht td scheduler requires more cpu time than a step of the ir scheduler  because the td scheduler must perform the. random sample lookahead search and check for loops on the average  td spends 1 times as much c pu time per step as ir on the other hand td requires fewer steps to find a conflict-free schedule the average sequence length for an iteration of td is 1% as long as an average ir sequence the net effect is that one iteration of td is equivalent to approximately 1 iterations of ir 
　bearing this in mind  the key point to notice is that the curve for the td scheduler alwav s lies below the curve for iterative repair this means that given the same amount of cpu time  td always finds a better schedule  l e   with lower rdf  for example  with 1 networks td ob tains an r d f of 1 compared to ir s rdf of 1  at 1 1 = 1 iterations  this is a 1% improvement  which in a schedule lasting a year is a savings of 1 days  and thousands of dollars  the curve also shows that iterative repair alwajs requires much more time  1 lter ations vs 1  to find a schedule whose quahtv matches the rdf found by td 
　figure 1 shows a similar comparison for td and ir on the 1 larger test problems here the difference between the algorithms is even more pronounced temporal dlf 
ference scheduling scales belter lo larger problems even though it has onlv been trained on smaller problems 
　figure r  shows analogous results for temporal differ ence and iterative repair on the 1 test-set sspp problems here tin horizontal axis is log cpu lime we see that id maintains a constant factor advantage over iterative repair temporal difference scheduling finds better schedules faster than iterative repair 
　note however that this figure just gives the average rdp over the whole test set because of the random components of both algorithms  this hides considerable variation figure 1 reveals this variation let us say that td wins  on a particular problem if the rdf of its best schedule computed so tar is belter than the rdf of the best ir schedule computed with the same amount of cpu time the two algorithms will be said to  tie' if they find schedules with identical rdi- values i igurc fi plots the fraction of td wins  and td  wins + ties as a function cf log cpu time we see that at low cpu costs  td wins on almost every problem eventually  as cpu time becomes larger  td still wins or ties slightly more than 1% of the time 
1 	discussion and concluding remarks 
these results show that temporal difference  td  methods outperform the best previous algorithm for scheduling space shultle payload processing jobs furthermore  
zhang and dietterich 

there are clearly many ways that the td methods can be improved for example  the current set of features needs to be improved so that the learning procedure can capture more domain specific knowledge there is also some evidence to suggest that the training procedure could be improved 
several authors  bradtke  1  thrun and schwartz  
1  boyan and moore 1  schraudolph et al  1  have shown that there are pitfalls associated with using neural networks  and other function approximation schemes  to represent value functions in reinforcement learning however  the results of this paper and the notable success of tesauro's  td backgammon system show that in some situations  these pitfalls are nol encountered an important open question is to understand why  works in this and other applications 
　we suspect that the success of td methods in this domain results from two factors first  there are probably many good solutions to each scheduling problem certainly there are many good solution paths because the search space is highly redundant second  td is essentially a technique for smoothing adjacent estimates of the final rdf this smoothing can remove local minima even if it does a poor job of predicting the final rdf these two ptoperties may permit a simple greedy algorithm to find good schedules 
　these same two properties may explain why the iterative repair method with simulated annealing also succeeds in this domain simulated annealing ib a stochastic method for locally smoothing an objective function as applied in this domain  simulated annealing is not run long enough to find a global optimum  but it may be able to escape local minima and find an acceptable solution 
1 	learning 
in spite of this 
　industrial scheduling problems abound and general purpose solutions to these problems probably do not exist this research has shown that reinforcement learning methods have the potential for quickly finding highquality solutions to these scheduling problems the goal of future research must be to improve these learning methods bo that they can be applied with a minimum of domain-specific engineering to produce a new  cost effective scheduling technology 
acknowledgements 
i he authors thank rich sutton and monte zweben for several helpful discussions the authors gratefully acknowledge the support of nasa grant nag 1 from nasa ames research center additional support was provided by nsf grants cda-1 and 1-1 
