 
the task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using bayesian statistics. within this framework  and using various mathematical and algorithmic approximations  the autoclass system searches for the most probable classifications  automatically choosing the number of classes and complexity of class descriptions. simpler versions of autoclass have been applied to many large real data sets  have discovered new independently-verified phenomena  and have been released as a robust software package. recent extensions allow attributes to be selectively correlated within particular classes  and allow classes to inherit  or share  model parameters though a class hierarchy. 
1 introduction 
the task of supervised classification - i.e.  learning to predict class memberships of test cases given labeled training cases - is a familiar machine learning problem. a related problem is unsupervised classification  where training cases are also unlabeled. here one tries to predict all features of new cases; the best classification is the least  surprised  by new cases. this type of classification  related to clustering  is often very useful in exploratory data analysis  where one has few preconceptions about what structures new data may hold. 
　we have previously developed and reported on autoclass  cheeseman et al.  1a; cheeseman et a/.  
1b   an unsupervised classification system based on bayesian theory. rather than just partitioning cases  as most clustering techniques do  the bayesian approach searches in a model space for the  best  class descriptions. a best classification optimally trades off predictive accuracy against the complexity of the classes  and so does not  overfit  the data. such classes are also  fuzzy ; instead of each case being assigned to a class  a case has a probability of being a member of each of the different classes. 
 research institute for advanced computer science 
1 	learning and knowledge acquisition autocla1 iii  the most recent released version  combines real and discrete data  allows some data to be missing  and automatically chooses the number of classes from first principles. extensive testing has indicated that it generally produces significant and useful results  but is primarily limited by the simplicity of the models it uses  rather than  for example  inadequate search heuristics. autoclass iii assumes that all attributes are relevant  that they are independent of each other within each class  and that classes are mutually exclusive. recent extensions  embodied in autoclass iv  let us relax two of these assumptions  allowing attributes to be selectively correlated and to have more or less relevance via a class hierarchy. 
　we begin by describing the bayesian theory of learning  and then apply it to increasingly complex classification problems  from various single class models up to hierarchical class mixtures. finally  we report empirical results from an implementation of these extensions. 
1 bayesian learning 
bayesian theory gives a mathematical calculus of degrees of belief  describing what it means for beliefs to be consistent and how they should change with evidence. this section briefly reviews that theory  describes an approach to making it tractable  and comments on the resulting tradeoffs. in general  a bayesian agent uses a single real number to describe its degree of belief in each proposition of interest. 
1 	theory 
let e denote some evidence that is known or could potentially be known to an agent; let h denote a hypothesis specifying that the world is in some particular state; and let the sets of possible evidence e and possible states of the world h each be mutually exclusive and exhaustive sets. 
　in general  p ab cd  denotes a real number describing an agent's degree of belief in the conjunction of propositions a and 1  conditional on the assumption that propositions c and d are true. more specifically    is a  prior  describing the agent's belief in h before  or in the absence of  seeing evidence e   is a  posterior  describing the agent's belief after observing some particular evidence e  and l e h  is a  likelihood  em-

   in theory  all an agent needs to do in any given situation is to choose a set of states h  an associated likelihood function describing what evidence is expected to be observed in those states  a set of prior expectations on the states  and then collect some evidence. bayes' rule then specifies the appropriate posterior beliefs about the state of the world  which can be used to answer most questions of interest. 
1 	p r a c t i c e 
in practice this theory can be difficult to apply  as the sums and integrals involved are often mathematically intractable. here is our approach. 
   rather than consider all possible states of the world  we focus on some smaller space of models  and do all of our analysis conditional on an assumption s that the world really is described by one of the models in our space. this assumption is almost certainly false  but it makes the analysis tractable. 
   the parameters which specify a particular model are split into two sets. first  a set of discrete parameters t describe the general form of the model  usually by specifying some functional form for the likelihood function. for example  t might specify whether two variables are correlated or not  or how many classes are present in a classification. second  free variables in this general form  such as the magnitude of the correlation or the relative sizes of the classes  constitute the remaining continuous model parameters v. 
   we generally prefer a likelihood 1  which is mathematically simple and yet still embodies the kinds of complexity relevant in some context. 
	similarly  	we 	prefer 	a 	simple 	prior 	distribution 
 vt s  over the model space  allowing the resulting v integrals  described below  to be at least approximated. we also usually prefer a relatively broad and uninformative prior  and one that gives nearly equal weight to different levels of model complexity  resulting in a  significance test . adding more parameters to a model then induces a cost  which must be paid for by a significantly 
l
　　a variable like v in a probability expression stands for the proposition that the variable has a particular value. better fit to the data before the more complex model can be preferred. 
   the ioint can now be written as   and  for a reasonably-complex 
problem  is usually a very rugged distribution in v t   with an immense number of sharp peaks distributed widely over a huge high-dimensional space. because of this we despair of directly normalizing the joint  as required by bayes' rule  or of communicating the detailed shape of the posterior distribution. 
instead we break the continuous v space into regions 
r surrounding each sharp peak  and search until we tire for combinations rt for which the  marginal  joint 

is as large as possible. the best few such  models  rt found are then reported  even though it is usually almost certain that more probable models remain to be found. 
   each model rt is reported by describing its marginal joint m{ert s   its discrete parameters t  and estimates of typical values of v in the region r  such as the mean estimate of v: 

or the v for which  is m a x i m u m in r. while these estimates are not invariant under reparameterizations of the v space  and hence depend on the syntax with which the likelihood was expressed  the peak is usually sharp enough that such differences don't matter. 
   a weighted average of the best few models found is used to make predictions. almost all of the weight is usually in the best few  justifying the neglect of the rest. 
   even though the sums and integrals can be difficult  and large spaces must be searched  bayesian theory offers the advantages of being theoretically well-founded and empirically well-tested  berger  1 ; one can almost  turn the crank   modulo doing integrals and search1  to deal with any new problem. disadvantages include being forced to be explicit about the space of models one is searching in  and occasional ambiguities regarding what an appropriate prior is. also  it is not clear how one can take the computational cost of doing a bayesian analysis into account without a crippling infinite regress. 
   we will now illustrate this general approach by applying it to the problem of unsupervised classification. 
1 	single class models 
for all the models to be considered in this paper  the evidence e will consist of a set of i cases  an associated set k of attributes  of size1 k  and case attribute values1 
1
　　the joint probability provides a good local evaluation function for searching though. 
1
　　 we use script letters like k to denote sets  and matching ordinary letters k to denote their size. 
1
　　 nothing in principle prevents a bayesian analysis of more complex model spaces for relational data. 
hanson  stutz  and cheeseman 

	1 	independent attributes - si 
we now introduce some notation for collecting sets of indexed terms like xik. a single such term inside a {} will denote the set of all such indexed terms collected 
1
　　if the fact that a data value is unknown might be informative  one can model  unknown  as just another possible denotes 1 when u - v and 1 otherwise. 
1
 discrete  data value; otherwise the likelihood for an unknown 	t y  is the gamma function  spiegel  1 . 
value is just a sum over the possible known values. 	1 actually  log  weight  is more normally distributed. 
1 	learning and knowledge acquisition 

　1  we choose it because it is easy  fits well with our model of class hierarchy  and allows full dependence. 
hanson  stutz  and cheeseman 

1 class mixtures 
1 	flat mixtures - sm 
the above model spaces sc = sy or s i c a n be thought of as describing a single class  and so can be extended by considering a space sm of simple mixtures of such classes  d.m.titterington et al  1 . with sc = st  this is the model space of autoclass iii  and figure 1 shows how it can fit a set of artificial real-valued data in five dimensions. 
in this model space the likelihood  
 sums 	over 	products 	of 	 class 
weights    c   each giving the probability that any case would belong to class c of the c classes  and class likelihoods describing how members of each class are distributed. in the limit of large c this model space is general enough to be able to fit any distribution arbitrarily closely  and hence is  asymtotically correct . 
	the parameters 	and 
combine parameters for each class and parameters describing the mixture. the prior is similarly broken down 

the choice of class were another discrete attribute  except that a c  is added because classes are not distinguishable a priori. 
   except in very simple problems  the resulting joint dj evt s  has many local maxima  and so we must now distinguish regions r of the v space. to find a local maxima we use the  em  algorithm  dempster et al.y 1  which is based on the fact that at a maxima the class parameters vc can be estimated from weighted sufficient statistics. relative likelihood weights wic =  satisfying 
give the probability that a particular case i is a member of class c. using these weights we can break each case into  fractional cases   assign these to their respective classes  and create new  class data  with new weighted class sufficient statistics obtained by using weighted sums instead of sums ♀v. for example and substituting these statistics into any previous class likelihood function l e vctcsc  gives a weighted likelihood and associated new estimates and marginals. 
at the maxima  the weights wic should be consistent 

a maxima we start out at a random seed and repeatedly use our current best estimates of v to compute the 	wic  and then use the to re-estimate the v  stopping after 1 - 1 iterations when they both predict each other. 
　integrating the joint in r can't be done directly because the product of a sum in the full likelihood is hard to decompose  but by using fractional cases to approximate the likelihood  while holding the wu fixed  we get an approximate marginal: 
j  
our standard search procedure begins each converging trial from classes built around c random case pairs. the number of classes c is chosen randomly from a lognormal distribution fit to the cs of the 1 - 1 best trials seen so far  after trying a fixed range of cs to start. we also have developed alternative search procedures which selectively merge and split classes according to various heuristics. while these usually do better  they sometimes do much worse. 
　the marginal joints of the different trials generally follow a log-normal distribution  allowing us to estimate during the search how much longer it will take on average to find a better peak  and how much better it is likely to be. 
　in the simpler model space smi where sc = si the computation is order  where averages over the search trials. n is the number of possible peaks  out of the immense number usually present  that a computation actually examines. in the covariant space smv where sc = sv this becomes  
1 	class hierarchy and inheritance - sh 
when there are many attributes and each class must have its own set of parameters for each of these attributes  multiple classes are strongly penalized. attributes which are irrelevant to the whole classification  like a medical patient's favorite color  can be particularly costly. to reduce this cost  one can allow classes to share the specification of parameters associated with some of their independent blocks. 
   rather than allow arbitrary combinations of classes to share blocks  it is simpler to organize the classes as leaves of a tree. each block can be placed at some node in this tree  to be shared by all the leaves below  farther from the root than  that node. in this way different attributes can be explained at different levels of an abstraction hierarchy. for medical patients the tree might have viral-infectionsneai the root  predicting fevers  and some more specific viral disease near the leaves  predicting more disease specific symptoms. irrelevant attributes like favorite-color would go at the root. to make predictions about a member of a leaf class  one first inherits down the attribute descriptions from classes above it. 
　therefore the above class mixture model space sm can be generalized to a hierarchical space sh by replacing the above set of classes with a tree of classes  and using the tree to inherit specifications of class parameters. from the view of the parameters specified at a class  all of the classes below that class pool their weight into one big class. figure 1 shows some sample trees  and figure 1 shows how a class tree  this time with sc - sv  can better fit the same data as in figure 1. 
	a tree of classes has one root class r. 	every other 

1 	learning and knowledge acquisition 

class c has one parent class pc  and every class has jc child classes given by ccj  where the index j ranges over the children of a class. each child class has a weight  relative to its siblings  with 	and an 	searching in this most complex space shv is challeng-
absolute weight 	with 	 	ing. there are a great many search dimensions where one 
	each class has an associated set of attributes 	can trade off simplicity and fit to the data  and we have 
which it predicts independently through a likelihood and which no class above or below 
it predicts. 	to avoid having redundant trees which only begun to explore possible heuristics. blocks can be merged or split  classes can be merged or split  blocks can be promoted or demoted in the class tree  em itera-describe the same likelihood function  only 	can be 	tions can be continued farther  and one can try a random 
empty  and non-leaves must have  	restart to seek a new peak. but even the simplest ap-
　we need to ensure that all attributes are predicted somewhere at or above each leaf class. so we call 
the set of attributes which are predicted at or below each class  start with  and then recursively partition each ac into attributes kc  kept  at that class  proaches to searching a more general model space seem 
to do better than smarter searches of simpler spaces. 
1 	results and hence predicted directly by it  and the remaining attributes to be predicted at or below each child for leaves a = k . 
　expressed in terms of the leaves the likelihood is again a mixture  
figure 1: autoclass iv finds class tree x 1 better lists of attribute numbers denote covariant blocks within each class  and the ovals now indicate the leaf classes. 
we have built a robust software package  autoclass iii  around the flat independent model smi  with utilities for reading data  controlling search  and viewing results  
in the general case of shv   where sc = sv  computa-
tion again takes except that the  is now also an average of  for each k the number of classes in the hierarchy which use that k  i.e.  have 
since this is usually less than the number of leaves  the model sh is typically cheaper to compute than sm for the same number of leaves. and have released it for general use. it is written in commonlisp  and runs on many different machines. we  and others  have applied this system to many large and real databases. when applied to infrared stellar data we found new  independently verified  phenomena  goebel et a/.  1 . others have successfully applied it to protein structure  hunter and states  1 . hanson  stutz  and cheeseman 

   we axe now developing autoclass iv around the full hierarchical block covariant model space shv   as hoped  it gives a significant improvement over the previous version when applied to real data  though we have only tried it on small problems so far. on the standard iris flowers data  1 cases  1 attributes  we find a model over 1 1 times more probable than the independent model  i.e.  with a marginal joint  absolute value 1~ 1 1   that much larger. 1 this compares with typical improvements of 1 for doubling the search time or for using a smarter merging search. 

figure 1: each new feature allows a better model 
the marginal probabilities improve by large factors as we fit each new class tree. 
   figures 1  1  and 1 illustrate a similar gain on an artificial data set with 1 cases and 1 real attributes. figure 
1 shows how the data is distributed in attributes #1 and # 1   and in attributes #1 and #1  attribute #1 is not shown . superimposed upon this is the best result found with autoclass i i i . 
   since the data is dominated by a covariance between attributes #1 and # 1   the independent model tries to model this by stringing classes along the 1 - 1 covariance axis. there is too little data to justify any more structure in this model class  so the # 1   #1 and #1 axes are basically ignored. the 
   figure 1 shows a progression of models between this model and the current best answer from autoclass i v   given in figure 1. each new model adding one new feature and gains an improved marginal joint. moving from a flat independent model to a flat fully covariant model  the best model found is over 1 1 times more probable  w i t h 1 classes each of which have significant covariances in 1 pairs of attributes. this large gain comes from combining a much better fit to the data  despite extra costs paid for the added class and covariance parameters. but  
1
　　cross-validation would probably be a better test here  since our priors are fairly crude. 
1 	learning and knowledge acquisition 
on inspection one finds that the four covariant classes are virtually the same in the 1 - 1 projection and show little correlation with the other attributes. we therefore split the covariance blocks and raise the   1   block to the common root for another relative increase of 1. this process can be repeated on pairs of the  1 1  blocks to get the 1 level tree shown in figure 1  gaining another 1 in relative probability. we have thus gained a total factor of over 1 in relative marginal probability over the best classification found using the independent model. of that total  about 1 1 comes from the fact that the tree now requires fewer parameters to specify a similar likelihood. 
   these preliminary results support previous indications that the ability to represent various kinds of structures in the data is the major limiting factor of such a system. 
