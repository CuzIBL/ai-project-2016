
in this paper we revisit the classical nlp problem of prepositional phrase attachment  ppattachment . given the pattern v  np1 p np1 in the text  where v is verb  np1 is a noun phrase  p is the preposition and np1 is the other noun phrase  the question asked is where does p   np1 attach: v or np1  this question is typically answered using both the word and the world knowledge. word sense disambiguation  wsd  and data sparsity reduction  dsr  are the two requirements for pp-attachment resolution. our approach described in this paper makes use of training data extracted from raw text  which makes it an unsupervised approach. the unambiguous v  p  n and n1  p  n1 tuples of the training corpus teach the system how to resolve the attachments in the ambiguous v   n1   p   n1 tuples of the test corpus. a graph based approach to word sense disambiguation  wsd  is used to obtain the accurate word knowledge. further  the data sparsity problem is addressed by  i  detecting synonymy using the wordnet and  ii  doing a form of inferencing based on the matching of v s and ns in the unambiguous patterns of v  p  np  np1 p  np1. for experimentation  brown corpus provides the training data and wall street journal corpusthe test data. the accuracy obtained for pp-attachment resolution is close to 1%. the novelty of the system lies in the flexible use of wsd and dsr phases.
1 introduction
correct pp-attachment is essential for syntactic and consequent semantic analysis of a sentence. for example: in the sentence i lifted the girl with a crane  with crane is the prepositional phrase  pp   and the pp-attachment could either be lifted with a crane  machine sense of crane  or girl with a crane  bird sense of crane . word sense disambiguation and pp-attachment mutually affect each other for correct and unambiguous assignment of senses and pp-attachment respectively  as is evident from this example. many previous researches  ratnaparkhi  1; donald hindle and rooth  1  have not considered the properties of nouns inside the pps  but these make a difference. for example: in sentence  i ate rice with salad/spoon  the attachments are rice with salad and ate with spoon. in our approach  we consider the contribution of noun within pp for pp-attachment. further  since most of the previous methods have focused only on v   n1   p   n1 structures  they can not consider the attachments of far away pps  which are mostly adjuncts. we deal with such situation in our unsupervised approach.
¡¡even with large corpora  the biggest challenge is data sparsity. we propose a simple  yet effective method of data sparsity reduction  dsr - using wordnet1. dsr helps smooth  generalize  the low probability counts. the correct ppattachment is basically determined by the semantic property of the lexical items in the context of the preposition. we use an iterative graph-based unsupervised word sense disambiguation  wsd  method  simlar to  mihalcea  1    which exploits the global semantic dependency and interaction of the word senses and ranks the senses of each word. this helps in correct pp-attachment.
¡¡the remainder of the paper is organized as follow: section 1 describes a graph based iterative unsupervised word sense disambiguation method. section 1 explains the data sparsity reduction process. section 1 details the unsupervised pp-attachment method. section 1 presents the experimental results and section 1 concludes the paper.
1 word sense disambiguation for pp-attachment
in this section  we describe a word sense disambiguation  wsd  method in the context of prepositional phrase attachment  pp-attachment . the approach is an iterative graph based algorithm which performs random walk on the sense dependency graph of the words involved in the context. our approach to word sense disambiguation for pp-attachment is based on two hypotheses:  1  prepositions are semantic carriers. most of the semantic relations are derived in association with prepositions  1  exploitation of the global semantic dependency among the words  preferably in the proximity of a preposition  will help wsd. during the completion of this module of our work  a similar work on a generic graph-based approach for sequence data labeling and its application for wsd was published  mihalcea  1 .
1 graph based algorithm for wsd
we perform a random walk on the graph for each context  i.e  a sequence of words that are supposed to disambiguate each other. for us the sequence is v   n1   p   n1. since p is not a content word  the interaction is basically among v   n1 and n1. a typical graph for a v  n1  p  n1 sequence is shown in figure 1. we formally define the graph as follow: let the given sequence of words be w = w1 w1 w1 .... wn  and let each word wi has nwi senses. assume the senses for the word. we construct a labeled graph g =  v e  in which each vertex represents a word sense and is labeled with the sense number swi. in g  there is a vertex v for every possible sense of a word. the dependency edges between a pair of vertices are labeled with the sense dependency representingthe sense similarity of the concepts  which will be explained later. the word senses of the same word are not connected within themselves. rather they are connected to labeled sense vertices of other words. however  not all labeled pairs can be related by dependency.

figure 1: partial labeled graph with senses assigned to words in the sentence he wrote a book on literature
1 labeling the graph edges
consider the sentence he wrote a book on literature. the v  n1 p  n1 tuple is wrote book on literature and words to be disambiguated are write  book  and literature. the labeled graph for this v n p n tuple is constructed as described in section 1. figure 1 shows the labeled graph structure for this sentence. all the words in the sentence have multiple meaning. the words write  book  and literature have nine  ten and four wordnet senses respectively. for the simplicity of presentation  only a few sample labeled sense dependency edges are shown in the graph. if the sense dependency similarity is zero  no edge is placed. in figure 1  it can be observed that the graph-based iterative algorithm performing random walk on the context sense dependency graph assigns the highest weight to sense #1 for write  sense #1 forbook and sense #1 for literature. the process of assigning weights to vertices are given in subsection 1. however  it can be observed that the differences between the first and the third sense of write and all the senses for the word literature are slight. for example  all the sense in case of literature can be interchangeably used.
1 sense similarity
the similarity between the two words is computed using the following criteria  which is motivated by the variations of original lesk algorithms  lesk  1 :  1  the longer the sequence of words that match  the more is the similarity   1  stop words such as as  a  the  be  is  shall and will are filtered out. prepositions are not filtered out  since our words to be sense disambiguated are associated with prepositions. if the same preposition appears in the definitions of the words that match  extra weight is assigned to the semantic similarity. if the sequence of words match along with the preposition then the similarity gets more weight   1  normalization of the weights is done based on the length of the definition to counter the effect of long definitions   1  pronouns of similar forms  such as {i  we  he  she  they  you}  are treated as a single entity   1  words havingdifferentderivationalmorphological forms such as published and publication are considered similar.
1 labeling process for graph vertices
the basic idea is motivated by the pagerank algorithm  brin and page  1 .
1. more the number of links are connected to a vertex more important is the vertex.
1. if the incoming link is from a very important vertex  thenlink itself carries more weight  accordingly the vertex receiving the link is highly weighted.
for the labeled graph  given a set of weights wab associated with the edge connecting vertices va and vb the weighted page rank score is determined as given in equation 1:

 1 
 in equation 1  the wp va  value indicate the stationary probability of a particular sense of a word.
1 iterative algorithm for vertex ranking
the algorithm consists of three main steps:  1  building of the labeled dependencies graph  1  scoring of vertices using graph-based ranking algorithms  1  label assignment. the iterative algorithm for vertex ranking using equation 1 is similar to described in  mihalcea  1   with the difference in the way we find the similarity between the senses of words. the steady state weights are the relative weights of the senses. since the graph-based algorithm ranks the weights  we have experimented with assigning multiple senses to each word.
1 data sparsity reduction  dsr  process
words in general are polysemous. much of the data sparsity in this context can be attributed to non-exploitation of paradigmatic relationships among words. it is rather infeasible to collect all possible combinations of training examples even from a large corpus  if paradigmatic and contextual relations are not exploited. consider the following sentences as a sample of the corpus.
1. he painted the wall with colour.  va- meaning verb atachment 
1. he paints the wall with red color.  va 
1. he coated the wall with colours.  va 
1. he will paint the room with medieval scenes.  va 
1. they coated the building with cracks.  na  meaning noun attachment 
1. i coloured the house with distemper.  va 
the corresponding pp-attachment tags are given in parenthesis.
¡¡in the given data set  many of the words are variants of inflectional morphology  such as painted  paints  paint and a few of them are synonyms such as colour  color and building  house  room. we should exploit these observations. moreover  if we can establish a relation between the verbs painted and coated  given the preposition with and att=va  then possibly we can find higher probability in equation 1  even though such an instance is not available in the training corpus.
	p n1 = scene|v = coated p = with att = va 	 1 
the data sparsity reduction  dsr  procedure is described next.
1 data sparsity reduction  dsr 
use is made of lemmatisation  synset replacement  paradigmatic substitution  and inferencing based on syntagmatic context. the updation process for verb attachment has been given below; a similar process is followed for noun attachment by substituting n1 for v. updation process for verb attachment:
the following steps are applied on the original training corpus for each preposition for modeling pr n1|p v va . when the attachment is to v   n1 is independent of n1  and n1 statitics is not changed.
1. lemmatisation and morphing:  a lemmatisation of dependent noun: after lemmatising if tuples become similar  add to the frequency counts of the tuples and  b morph verb: after morphing if tuples become similar  add to the frequency counts of the tuples.
1. synset replacement: for each tuple in the corpus  create new tuples with weights proportional to empirical counts  for each word in the synsets of first two senses of v and n1. suppose  if v has l1 = synwcont v   number of synonymous words and n1 has l1 = synwcont n1  number of synonymous words  then l1   l1 number of tuples are generated  each having weight of the empirical count of original tuple. update the appropriate frequency counts with the counts of l1   l1 newly generated tuples.
1. inferencing: the third step involves inferencing among the tuples in the v  p  n1 syntagmatic context based on matching partly or fully  which either may generate new tuples not available in the training corpus or may increase the frequency count of the existing tuples. for example  if v1  p  n1 and v1  p  n1 exist as also do v1  p  n1 and v1  p  n1  then if v1  p  ni exists  i = 1   we can infer the existence of v1pnj where  with frequency count of v1pni added appropriately.
the above three steps are applied in the given order. we have observed significant reduction dsr.
1 unsupervised prepositional phrase attachment method
in this section  we propose an unsupervised pp-attachment method which does not require any annotated data. the proposed method directly collects training examples from the text. our approach is based on the hypothesis that unambiguous attachment cases of training data teach how to resolve the ambiguous attachment cases of the test data. our approach is motivated from the work of ratnaparkhi  ratnaparkhi  1  and is to some extent similar in terms of the statistical modelling and the extraction of training examples. however  we have introduced the graph based word sense disambiguation and data sparsity reduction  which is a point of difference. another point of difference is the employing of the training data refinement process described in subsection 1.
¡¡we resolve attachment in the ambiguous v  n1 p  n1 test instances using the extracted unambiguous v   p   n1 and n1   p   n1 cases from the training data. for example  the ambiguous ate rice with spoon  can be interpreted as correct unambiguous ate with spoon and incorrect unambiguous rice with spoon instances. the extracted v   p   n1s are more reliable. n1 p  n1s are not reliable- particularly the n1pn1s in which p   n1s are the pp-adjuncts which can appear far away from verbs but actually are attached to verbs.
1 collecting training data from raw text
we first annotate the text with part-of-speech tags using the lt pos1. the the noun phrases and verb phrases are chunked using our own simple chunker. after chunking we replace each chunk with their head words. extraction heuristics are then applied to extract the unambiguous v   p   n1s and n1   p  n1 training instances given in subsection 1. the whole process of tagging  chunking extraction of training examples are given in table 1.
table 1: the process of extracting training data from raw text

raw textthe professional conduct of the doctors is guided by indian medical association.pos taggerthe dt professional jj conduct nn of in thedt doctors nns is vbz guided vbn by in indian nnp medical nnp association. nnp . .chunkerconduct nn of in doctorsnns guidedvbn by in associationextraction heuristic n1 = conduct  p= of  n1= doctors 
 v = guided  p = by  n1 = association morphology n1 = conduct  p = of  n1 = doctor 
 v = guide  p = by  n1= association synset addition n1 = conduct  p = of  n1 = doctor 
 n1 = behavior  p= of  n1= physician  similarly we can have 1= 1 combinations  and
 v = guide  p= by  n1= association 
 v = direct  p= by  n1= association  similarly we can have 1= 1 combinations1 heuristic extraction of unambiguous training data
the extraction heuristic exploits the idea that an attachment site of a preposition is usually within a few words to the left of the preposition. the heuristic has the following parameters:
window size s: this is the maximum distance in words between a preposition p and n1  v or n1. we use w = 1 in our experiments. we extract:
1. v   p   n1  if the parsed segments satisfy:
  p is a preposition
  v is not a form of the verb to be
  v is the first verb that occurs within w words to the left of p
  no noun occurs between v and p
  n1 is the first noun that occurs within w words to the right of p
  no verb occurs between p and n1
1. n1   p   n1  if the parsed segments satisfy:
  p is a preposition
  n is the first noun that occurs within w words to the left of p
  no verb occurs within w words to the left of p. if it appears  it must be ensured that a preposition  subordinating conjunction or a wh-type conjunction appears between n and the new verb seen
  n1 is the first noun that occurs within w words to the right of p
  no verb occurs between p and n1
since the unambiguous instances v   p   n1s and n1  
p   n1s are extracted using heuristics  these- particularly n1   p   n1s- are not always correct  and hence call for refinement.
1 refinement of training data
we filter out the incorrect v   p   n1 and n1   p   n1 instances by applying the graph based wsd algorithm discussed in 1. the features considered are one word to the right of n1 and one word to left of v or n1. only the nouns and verbs are disambiguated. three strategies are used to refine the data:
1. set of heuristics for reliable unambiguous n1   p   n1s. these are based on syntactic heuristics which pick almost reliable npns. for example   1  n1   p   n1 as subject: like tube through doorway in the sentence the tube through the doorway disturbs the people and
 1  n1 p  n1 as predicate in the b part of a sentence of the form a  form of 'be'  b: as in item in program in the sentence it is an important item in the program.
1. tuples after step 1 are further refined using strong conditions. we use wordnet to find semantic properties of words such as place  time  group etc.
1. finally slightly weaker conditions are applied throughlimited statistical inferencing to give a set of highly correct v   p   n1 and n1   p   n1 tuples.
1 training method
our goal is to resolve the ambiguous pp-attachment instances using the learnt knowledge of unambiguous pp-attachment. the ambiguous tuple are of the form v n1 p n1. the unambiguous training tuples are of the form v   p   n1 and n1   p   n1. we define our classier as in equation 1.
att v n1 p n1  = arg max pr v n1 p n1 a 
a¡Ê{n v}
 1  we can factor pr v n1 p n1 a  as follows:
		 1 
the factors pr n1  and pr v   are independent of the attachment a and need not be computed. the estimation of pr a|v n1   pr p|a v n1   and pr n1|p a v n1  is difficult  because in the training data both n1 and v do not occur together. for this reason  these factors are computed using the approximation in equation 1:
pr a = n1|v n1  ¡Ö prz a =v nn1|n  1 
	pr a=v|v 	 1 
	pr a = v |v n1  ¡Ö	z v n1 
where  z v n1  = pr a = n1|n1  + pr a =
v |v  . similarly  we approximate pr p|a v n1  and pr n1|p a v n1  as given in equations 1 and 1 respectively. the reasons for these approximations are to avoid using counts of  v n1  together  since they are never seen together in the extracted data.
pr p|a = n1 v n1  ¡Ö pr p|a = n1 n1 
 1 
pr p|a = v v n1  ¡Ö pr p|a = v v  
pr n1|p a = n1 v n1  ¡Ö pr n1|p a = n1 n1 
pr n1|p a = v v n1  ¡Ö pr n1|p a = v v  
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ 1  the approximated probabilities are computed from the training data as in  medimi and bhattacharyya  1 . we used a variant of backed-off technique in order to smooth the probability computation.
1 experiments  results and analysis
training data:we used brown corpus for collecting the unambiguous training examples. the corpus size is 1mb  consisting of 1 sentences  and nearly 1 million 1 thousand words. the most frequent prepositions are in  to  for  with  on  at  from. the preposition of which is highly biased towards noun attachment is not considered. the extracted unambiguous distinct n1  p n1 and v p n1 tuples number 1 and 1 respectively.
testing data: for testing  we used penn treebank wall street journal data by ratnaparkhi  ratnaparkhi et al.  1   which is a standard benchmark data for pp attachment used by many groups  ratnaparkhi et al.  1; collins and brooks  1; stetina and nagao  1 . baseline: we consider the unsupervised approach by ratnaparkhi  ratnaparkhi  1  as the baseline system for our performance evaluation. we name it as base-rp. further  we tested the performance of our system on the extracted unambiguous samples. we name this process as base-ms.
¡¡we name our proposed system a flexible unsupervised pp-attachment or in short flxuppattch. we experimented on the performance of flxuppattch in two stages:  i  dsr without wsd  we call it dsr-wo-wsd  and  ii  dsr with wsd  we call it dsr-with-wsd . the stages and their names appear in table 1.
table 1: the naming of flxuppattch systems utilizing different dsr stages and the graph based wsd
stages of data sparsity reductionmorphinginferencingsynsetsynset & inferencingdsr-wo-wsdmorphinferwnsynsyn-infdsr-with-wsdmorphwsinferwswnsynwssyn-infws¡¡further  since our wsd method provides ranks to all the senses of each word  we experimentedwith different schemes of sense assignment
1. gwsrnk1- assign the first ranked sense
1. gwsrnk1- first two highest ranked senses
1. gwsrnk1- first three highest ranked senses
1. gwsrnk1-c1- first sense always  1% times the random assignment of the second sense and 1% times the random assignment of the third sense
1. gwsrnk1-c1- 1% times the first sense  1% times the second sense  and 1% times the third sense
with different senses being assigned  we observed the performance of our system with respect to different stages of dsr process particulrly after synsets and after synsets and inferencing. this performance comparision is given in figure 1. in case of gwsrnk1  the precision is low. this is because  though the coverages of the tuples increases due to   it introduces noise through wrong lexical entries  which has a net negetive effect on the precisison. the best performing combination is  gwsrnk1-c1    this may be due to the fact

figure 1: performance summary of flxuppattch dsr-withwsd with combination of senses assigned to words
that  it makes a trade-off between coverage and precision. in case of  gwsrnk1-c1    though it increases the precision  it decreases the coverage proportionately. it may also be observed that inferencing consistently increases the precision of the system.
¡¡we also compared the the performance of our system with baseline using wsd and applying the dsr in stages. figure 1 shows the performance variation.

figure	1:	comparison	of	baselines	with
flxuppattch gwsrnk1-c1  at different stages of dsr
¡¡the performance of our system after morphing is better than performance of ratnaparkhi  ratnaparkhi  1   i.e base-rp. this gives an indication of the better accuracy of extraction heuristics. the comparative performance of our best performing system with the state-of-the-art systems are shown in table 1
1 conclusion
we presented an unsupervised method for pp-attachment which compares favourably with the existing state of the art approaches. the method makes use of lexical semantics and inferencing through the use of wordnet. we employ
wsd in a limited way and make use of the unambiguous
pp-attachments to learn the resolution of pp-attachment in case of the ambiguous v   n1   p   n1 tuples. since the starting point is raw corpora  the method is usable even when annotation is not available. clearly the efficacy of the method depends on the richness of the presence of v   p   n and n   p   n tuples. obvius future work consists in refining

¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡ stetina and nagao  1  j. stetina and m. nagao. corpus table 1: comparison of flxuppattch  gwsrnk1-c1  with based pp attachment ambiguity resolution with a semantic state-of-the-art-systems dictionary. in proceedings of the fifth workshop on very
large corpora  pages 1  beijing and hong kong 
	pp-attachment systems	precision % 
1.
human without context1ratnaparkhi  ratnaparkhi et al.  1 11mitchell 1.1use of wordnet back off1stetina and nagao  stetina and nagao  1 11li and abe 1.1flxuppattch  gwsrnk1-c1 1use of thesaurus back off1pantel and lin  pantel and lin  1 11mclauchlan 1.1zhao and lin 1.1wsd and thereby improving the performance of attachment  and also dealing with the more difficult case of n   p   n tuples.
