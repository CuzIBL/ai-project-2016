 
we discuss the use of case-based reasoning  cbr  to drive an information retrieval  ir  system. our hybrid cbr-ir approach takes as input a standard frame-based representation of a problem case  and outputs texts of relevant cases retrieved from a document corpus dramatically larger than the case base available to the cbr system. while the smaller case base is accessible by the usual case-based indexing  and is amenable to knowledge-intensive methods  the larger ir corpus is not. our approach provides two benefits: it extends the reach of cbr  for retrieval purposes  to much larger corpora  and it enables the injection of knowledge-based techniques into traditional ir. our system works by first performing a standard hypo-style cbr analysis  and then using texts associated with certain important cases found in this analysis to  seed  a modified version of inquery's relevance feedback mechanism in order to generate a query. we describe our approach and report on experiments performed in two different legal domains. 
1 	introduction 
one forte of case-based reasoning  cbr  systems is their ability to reason about a problem case and  in particular  to retrieve highly relevant cases. however  this ability is limited by the availability of cases actually represented in a cbr system's case base. among current cbr systems there are few with large case bases  say  larger than 1 cases  and fewer still with both large case bases and large-sized cases  although all cbr systems use symbolic representations of cases and many perform highly sophisticated reasoning  kolodner  1 . 
　on the other hand  full-text information retrieval  ir  systems are not hampered by any lack of available cases  in textual form . there are huge case bases and individual cases are often very large  e.g.  tens of pages of text ; however  the level of representation is shallow at best  i.e.  the text itself   and the indexing is weak  e.g.  based on statistics of the collection  salton  1   
　'this research was supported by nsf grant no. eec-1  state/lndustry/university cooperative research on intelligent information retrieval  digital equipment corporation and the national center for automated information research. 
　thus we have two well-developed technologies  each with its own strengths and limitations. a natural approach is to form a hybrid system to produce results or functionalities 
unachievable by either individually. 	* 
　our goal in this project is to take advantage of the highly articulated sense of relevance used in cbr and the broadly applicable retrieval techniques used in ir in order to retrieve documents that are relevant to a problem case from commonly available large text bases  without the need for creating a symbolic case representation for every document. therefore  a central question in our research is: can we automatically 
formulate good queries to an ir system based on information 
derived by a cbr system  
　instead of a user composing a query to initiate a retrieval  in our approach a user inputs facts of a problem case in a standard frame-based representation  e.g.  a case template filled by facts . what the user gets back is a set of relevant texts retrieved from a document corpus many times larger than the case base available to the cbr system. any further analysis of these retrieved texts  for instance  for the purpose of making a case-based argument  is up to the user. 
　our hybrid cbr-ir system works by first performing a standard hypo-style cbr analysis  iashley  1 ; irissland and ashley  1    and then using the results to cause the inquery ir system fcallan et al.  1 to generate and act on a query. this is done by applying a modified version of inquery's relevance feedback  rf  mechanism to the documents associated with important cases found during the cbr analysis  such as most on-point cases. from this small set of  seed  documents  the rf mechanism selects and weights terms to form a query to the larger text corpus. this use of relevance feedback  in effect  tells the ir component that the cases found through the cbr analysis are highly relevant and that inquery should retrieve more like them. 
　the cbr analysis is performed with respect to the relatively small case base available to the cbr component. relevance feedback is based on a set of noteworthy cases selected from this analysis; this set is smaller than those usually used in relevance feedback. the ir can be performed on a text collection of arbitrary size. in one of our application domains  an area of tax law  the full-text collection is 1 times larger than the cbr module's case base; in the other  an area of bankruptcy law  it is about 1 times larger. thus  the retrievals can be 
done from corpora much larger than is usual in cbr. 
　our hypothesis is that the quality of documents retrieved via this hybrid system is better than via ir methods alone. 
this hypothesis has been borne out in our experiments. our hybrid approach achieves a very fine level of performance  as measured by standard measures of precision and recall. 
　in the next section  we give further background on our task. in sections 1 and 1  we present an overview of the architecture of our hybrid system  and give an example. in section 1  we provide some background on the mechanics of query formation and on the domains explored. in section 1  we discuss the experiment and in section 1 analyze the results. we summarize in section 1. 
1 	background 
even though cbr partly ameliorates the knowledge acquisition bottleneck by taking advantage of problem cases as they arise  it is still time-consuming to build a case corpus of significant size if cases are represented in any depth. if the case base is constructed after the fact from pre-existing archives of textual materials  the task can be daunting. 
　most cbr systems that have represented large numbers of cases have used fairly simple case representations  e.g.  mbrtalk  stanfill and waltz  1   pace  creecy et al  
1   johnny  stanfill  1   anapron  golding and rosenbloom  1   or have used representations easily derived from solved problems  veloso  1 . in a very few situations  large case bases have been constructed through a combination of case acquisition as a side-effect of customer service and follow-up knowledge engineering by a team specifically tasked with creating a case base  shimazu et ai  1 . our own cbr systems  which use detailed case representations-hypo  ashley  1   rissland and ashley  1   cabaret  rissland and skalak  1   frank  rissland et al.  1  bankxx  rissland et al  1a   rissland et ai  1b -have typically had case bases in the range of three to five dozen cases. 
　text-based ir can be used to access many extensive and widely-used commercial text collections in a variety of domains  such as commerce  medicine  and the law. for instance  all the cases decided in the supreme court and other federal courts since their beginnings  in 1  and most state courts over at least the last 1 years are available through either west publishing company's westlaw ror mead data cental's lexis r systems. these massive on-line corpora represent a tremendous resource and investment of capital. 
　however  users of current ir systems  even those accepting queries in natural language  must know how to manipulate them in order to get back truly relevant information. often users are not even aware of the difficulties because nothing appears to go wrong. for instance  one study found that although many users felt that they had retrieved most of the right documents  i.e.  that recall was high   in fact  they had retrieved only a mere 1% of the relevant texts  blair and maron  1 . 
　the other typical problem is that of retrieving too much information  only some of which is relevant. for example  if one were gathering precedents to be used in writing a brief for a personal  chapter 1  bankruptcy case involving the legal question of court approval of the plan proposed by the debtor  westlaw could be used to query its collection of bankruptcy cases  for instance  with the query  1 a    the cite to the relevant section of the bankruptcy statute . even with an additional restriction to cases decided between 1 and 1  this query produces 1 cases; far too many to be looked over by even the most dedicated legal researcher or research team. a more restricted query  1 a  1   the cite to the subsection addressing the narrower  good faith  requirement for plan approval  retrieves 1 cases; still too many. adding information about the case at hand  e.g.  profession of debtor  amount of debts  duration of plan  or placing further restrictions on date and jurisdiction would be ways to narrow down further the set of cases retrieved. 
　while traditional ir systems can access huge document bases  users of ir systems make the implicit assumptions that not all the relevant documents will be retrieved  i.e.  recall will not be perfect  and that not all of those retrieved are relevant  i.e.  precision will not be perfect . users of cbr systems  on the other hand  often assume higher  it not perfect  levels of precision and recall. our goal is to extend case-based retrieval to the ir context without sacrificing recall and precision and without enlisting the aid of an army of knowledge engineers to re-tool existing text collections. 
　by bringing in specifics of the case at hand-exactly the sort of information used by cbr systems-it is possible to retrieve a workable set of truly relevant cases  not just those that happen to share a particular statutory cite. this is what an experienced user does. in addition to facts of the current case  information from known relevant precedents  past successful approaches to similar retrieval problems  particular knowledge of the domain  etc. can also be used. by being smart about query formation  one can drive a retrieval engine to produce better results. 
　in our approach  knowledge about the problem case is input directly by the user. knowledge about what makes one case similar to another-particularly what makes one case a good precedent to appeal to in making a legal argument about another-is embedded in hypo-style cbr. knowledge of the mechanics of forming a query is handled by the relevance feedback mechanism of inquery knowledge about the domain  e.g.  personal bankruptcy law  is used in the cbr module  and knowledge about text  e.g.  word frequencies  is used in the ir module. thus we enhance traditional ir with knowledge through inclusion of cbr. 
1 	system overview 
our system takes as input a problem case given in the form of a generic case-frame filled in with specific features. it outputs a set of documents considered relevant to the problem case. 
 see figure 1.  
　we did not design new case representations for this project. rather  we used pretty much as is the representations developed in two past cbr projects from our lab: cabaret 
 rissland and skalak  1  and bankxx  rissland et at.  1a   rissland et al.  1b . we only added one additional slot to each case: the document identifier of the case's opinion in the text collection. the same case representation is used for representing a problem case and cases in the cbr module's case-knowledge base or ckb. 
　we use a standard hypo-styled cbr module to perform the case-based reasoning  ashley  1    rissland and ashley  1 . it analyzes a problem case with respect to the cases in its ckb and generates a data structure called a claim lattice  which represents a sorting of cases relevant to the 
	rissland and daniels 	1 
problem case according to how on-point they are. from the claim lattice  our system selects certain special classes of cases to use in relevance feedback. we call the subset of the ckb cases selected via cbr analysis and used in relevance feedback the rf-ckb. 
in brief  the cbr analysis is done as follows. first  the 
cbr module determines the relevant cases: these are cases with the hope of achieving improved recall and precision. 
that share at least one dimension in common with the problem 	ordinarily inquery would not engage in relevance feed-
case. dimensions address important legal aspects of cases back until a retrieval  based on user input  had been made and are used both to index and compare cases. next  the and a set of documents retrieved  examined  and tagged by relevant cases are sorted according to how relevant or on- the user. however  since the cbr analysis already provides 
point they are. this is done by examining the intersection of the system with a set of relevant documents  there is no need each case's set of applicable dimensions with those applicable for an initial user-provided query nor user-provided relevance in the problem case.  cases with no shared dimensions-that judgments. 
is  irrelevant cases-are not considered.  in this sorting  which results in a partial order  case a is considered more on-point 1 	example than case b if the set of applicable dimensions it shares with 
the problem case contains those shared by b and the problem to illustrate the workings of our system we run through the case. maximal cases in this ordering are called most on-point following scenario. a client approaches a lawyer about his 　an rf module uses a selection metric to extract a set of terms from the relevant texts. the top n terms are then weighted according to another metric. for our experiments  we apply the selection and weighting metrics used in a similar application  croft and das  1 . a query consists of a weighted sum of terms. 

cases or mope's. the resulting sort of relevant cases can be shown in a so-called claim lattice.  see figure 1 for an example.  cases just below the root are the mope's. 
　we use the inquery retrieval engine as our ir component. inquery uses an inference network model  turtle and croft  1   specifically  a bayesian probabilistic inference net. it uses a directed acyclic graph with a query node at the root  document nodes at the leaves  and a layer of query concept nodes and a layer of content representation nodes in between. nodes that represent complex query operators can be included between the query and query concept nodes. the inquery model allows for the combination of multiple sources of evidence  beliefs  to retrieve relevant documents. 
　full-text versions of the opinions for cases selected for inclusion in the rf-ckb are passed to a modified version of inquery's relevance feedback module. relevance feedback is a widely-used method for improving retrieval. it can improve precision significantly  salton  1 . in relevance feedback  a user tags texts as to their relevance. using information derived from the texts tagged as relevant  an rf algorithm alters the weights of the terms used in the original query  and/or adds additional query terms  to produce a modified query. the new query is then submitted to the ir engine attempt to take a tax deduction for his home office. the internal revenue service has questioned the deduction  but the client  a college professor  believes that he is entitled to take it. he tells his lawyer various facts concerning his problem. she inputs these to the cbr-ir system. 
　suppose the lawyer has knowledge of a set of previously decided home office deduction cases  for instance  cases she knows about from her own tax practice  and these make up the ckb used by the system. to be specific  suppose the problem case is the weissman 1 home office deduction case and that the lawyer's ckb contains cases originally used in cabaret. figure 1 shows the top two layers of the resulting claim lattice. drucker  gomez  honan  and meiers are the mope's. 
　using the lawyer's ckb  the system analyzes mr. weissman's problem and uses various important cases to seed a search for additional relevant cases from a larger corpus  say the westlaw federal taxation case law collection. suppose the lawyer asks the system to use the set of cases in the top two layers of the claim lattice as the rf-ckb because she knows these are always very relevant. this set contains all 1 cases shown in figure 1. the indices for the texts associated 
1  weissman v. comm. 1  f.1d 1  1d cir. 1 . 


　some of these terms  like 1a  are perfectly obvious. it is not hard to imagine how others might have been found. for instance  focal is from the phrase focal point test  the name for a particular legal approach to the home deduction issue and dwell is the stem of dwelling  a term used frequently in the language of section 1a of the irs code concerning deductions of various expenses in connection with business use of a home  rental of vacation homes  etc.; these are often quoted in case opinions. others are not obvious at all  such as  opera  which no doubt comes from the drucker case which concerned a musician in the metropolitan opera orchestra. 
　even an experienced user would be unlikely to use some of these terms if she needed to compose the query herself. case names for cases that are not known or memorable  like curphey  would surely not be used.  presumably memorable cases would be included in the ckb . in fact  from our own observations  most users of inquery tend to use only one or two individual terms in their queries even though inquery allows ample natural language input. a typical user in our scenario would probably use the single term 1a. 
　finally  our system returns to the lawyer those texts retrieved with the system-generated query. these include cases  like drucker  bale  etc. from the top two layers  which the lawyer already knew about  and new cases like dudley  about a married couple  both of whom are college professors  which she didn't. 
　the lawyer now has a larger set of relevant documents for her research on mr. weissman's problem. it has located new cases unknown to the cbr module. of course  she  herself  has to read and analyze these. however  without any need for formulating queries or cleverly manipulating the retrieval engine directly  she has been able to access a massive on-line document collection in a problem-based manner and discover relevant cases she might not have considered otherwise. 
1 methodology 
in this section  we describe briefly domains of application  how we defined baselines and answer keys  and the main parameters varied in our experiments. 
1 domains 
we have used two domains in our work thus far: 
1. the home office deduction domain  used originally in our cabaret project  rissland and skalak  1 ; 
1. the good faith bankruptcy domain  used in our bankxx project  rissland et al  1b . 
cabaret's original case base consisted of 1 real and hypothetical cases concerning the home office deduction  as specified in section 1a c  l  of the internal revenue code. for this project  we re-used 1 of these cases as the ckb of our cbr-ir system in the first domain. bankxx's original case base consisted of 1 cases concerning the  good faith  issue for the approval of plans for  individual  debtors under chapter 1 of the bankruptcy code  as specified in section 1 a  1 . for this project  we re-used 1 of these. 
1 problem cases 
in each domain  we have run a series of experiments by submitting problem cases  chosen from the ckb  to the cbr-ir system  which then treats it in a de novo manner by  temporarily  deleting it from the ckb and treating it as a new case. that is  we run the system on a problem case in a minus-one manner against a ckb consisting of the other cases. so far we have run experiments with 1 home office deduction and 1 bankruptcy cases. 
1 building the corpus 
to test our approach  we constructed two test document collections: 
1. hod-corpus consists of over 1 legal case texts addressing a variety of legal areas; 
1. bankruptcy-corpus consists of over 1 legal case texts addressing the issue of approval of a debtor's plan  as specified in section 1 a   and the sub-issue of good 
faith from section 1 a  1 . 
　the hod-corpus contains cases addressing a great many legal questions. it was built by adding approximately 1 cases to another already existing  nearly 1 document collection  called the west or fsupp collection  haines and croft  1    turtle  1 . the additional texts are for cases contained in the cabaret ckb and cases found when the query home office was posed to the on-line westlaw federal taxation case law database. we restricted the query to cases decided between january 1 and november 1. we added in these cases  with redundant cases removed  to build our hod-corpus. all 1 of the cabaret cases are contained in the resulting collection. the hod-corpus contains 1 texts in total. of these  only about 1%  1 cases  discuss the home office deduction  1a c  l   issue we are interested in. 
　we established a baseline for the hod-corpus by using the of the answer key and the retrieved items  to the total simple one-term query 1a. it is realistic query given that it is number of relevant items. 
the relevant statutory cite and the one keyword that most users   precision measures the percent of retrieved items that would probably start with. it does very well: 1% average are relevant. it measures accuracy. it is the ratio of the 
precision. {average precision is defined in section 1 . this 	number of relevant retrieved items to the total number of represents a baseline for retrieval performance using ir alone. 	retrieved items. by contrast  the bankruptcy-corpus contains cases dealing only with the specific issue of debtor plan approval  as speci-	  average precision is the average of the precision scores fied in section 1 a . we built this corpus by downloading 	achieved at 1 levels of recall: 1%  1%  1% ... 1%. all the cases that were found with the query 1 a  to the 	since we know what the correct answer is  we can determine westlaw federal bankruptcy case law database. we re-	when a given level of recall is achieved by the system and then stricted the query to cases decided between 1 and 1. it 	calculate the precision at this level. when we use 1 levels of 
contains all but the 1 earliest cases from the original 1-case recall  it is called ii-point average precision. bankxx ckb. in this corpus about 1%  1 cases  make specific reference to the narrower  good faith  issue. thus  1 experiments 
this corpus is very focussed. 	in this section  we discuss our experiments with different rf-
　for the bankruptcy domain  we established a baseline by us- ckb sets and different numbers of terms that are used in the ing the simple one-phrase query good faith on the bankruptcy-resulting query. corpus. this baseline query  which uses ir alone  achieves 1% average precision. this high value indicates that a high proportion of  good faith  cases actually use that phrase and that cases on other issues do not. 
　both text collections were built using the standard ir procedure of removing predefined  stop  words  that is  high frequency words that do not represent content and add little value for discrimination between documents  e.g.  and  but  the  a   and stemming  that is  removing suffixes  to get at the root form of a word. what remains in a document constitute the terms that are used as the  inverted  indices for the document. the same stopping and stemming procedures are used by the rf module on the rf-ckb texts to produce a list of terms that may constitute a query. in addition  the rf module also gives each term a weight that represents its relative importance in the query. 
　figure 1 gives the total number of unique terms in the various rf-ckb's from our experiments with the weissman 
case  the average number of unique terms for a text  and the average document size for each rf-ckb. the figures for the original fsupp collection are taken from  haines and croft  1. 
1 answer keys 
for each problem  we constructed an  answer key  that specifies the documents to be considered as relevant. in these 
experiments  we used a very broad sense of relevance. 
　in the home office deduction domain  any of the 1 cases from the hod-corpus that actually concerns a taxpayer trying to take the home office deduction is considered relevant. in the bankruptcy domain  any of the cases from the bankruptcycorpus that discusses the  good faith  issue is considered relevant. thus  all problem cases in a given domain were assigned the same set of texts as the correct answer. for the most part  our answer keys contain cases that cabaret or bankxx would have considered relevant. 
　answer keys are used to calculate precision and recall statistics. 
  recall measures the percent of those items that should have been retrieved by the query that actually were. it measures coverage. it is the ratio of the number of relevant retrieved items  i.e.  items in the intersection 
1 system parameters varied 
for each problem case  we varied the following: 
1. the rf-ckb used to seed the rf mechanism; and 1. the number of terms used in the inquery query. 
　we did not vary other parameters used in relevance feedback  such as the weighting metric. for our experiments  there is no  original query  per se. instead  the rf module is given a null query and the rf-ckb as its set of relevant documents. because there is no original query to modify  some concerns of relevance feedback  such as re-weighting of terms  do not apply. 
　for each rf-ckb  the relevance feedback module selected  weighted  and formed a query with the top 1  1  1  1  1  
1  1  1  1  1  1  1  and 1 terms found in the rf-ckb. the maximum length query was 1 terms because of a limitation of the rf module. therefore  longer queries  such as all the terms from within a rf-ckb  were not tested. 
1 rf-ckb's - documents for seeding relevance feedback 
for the home office deduction domain  we selected 1 cases to use as problem cases. the weissman case  discussed in our example  was the first problem case with which we experimented. we examined the queries and resulting precisionrecall results derived from six different types of rf-ckb's: 
　1. rf-ckb1 consists solely of the set of mope's. for the weissman problem  there are 1 such cases. coincidentally  these happen to be pure in the sense that there are no other issues under consideration in them besides that of the home office deduction. an impure case discusses the home office deduction and one or more other issues. of the 1 cases in the cbr module's ckb  1 are pure. of the other 1 home office deduction cases in the hod-corpus  more than 1 were pure. in figures 1 and 1  this rf-ckb is referred to as mope/pure. 
　1. rf-ckb1 consists of only impure cases; a random selection of 1 of them from the weissman claim lattice. rfckb1 tests the ability of relevance feedback to discriminate important terms from non-relevant ones within noisy texts. 
　1. rf-ckb1 is the union of rf-ckb 1 and rf-ckb1 and so has both pure and impure texts. rf-ckb1 has the 

1 	results 
for each rf-ckb used on a problem case  we calculated 1point average precision scores. figure 1 lists the scores for the six rf-ckb's used on the weissman case with different numbers of terms used to form a query. 
　rf-ckb 1 takes the longest to find a good set of terms and weights. it is not until there are between 1 and 1 terms that a query achieves an average precision that exceeds the baseline of 1%. rf-ckb1 achieves this average between 1 and 
1 terms  while rf-ckb1 needs 1 or less terms. overall  rfckb1 achieves the best set of average precisions  rf-ckb1 next  and rf-ckb1 the worst. 
　every rf-ckb results in significant improvement over the baseline average precision of 1% by the time the queries have included 1 terms. the relative improvement over the baseline is nearly 1% in many cases. thus  the hybrid cbr-ir method significantly out-scores straight ir alone. 
　there is a large jump in the average precisions for most of the rf-ckb's. for example  within rf-ckb 1  the jump is from 1% to 1% and occurs between 1 and 1 terms. for rf-ckb1  the jump is from 1 to 1% and happens with the addition of terms 1 to 1. this may be explained by examining the set of terms that are added to the longer 
queries. it turns out that whenever the jump occurs  both 1a and dwell are new terms. no such large jump is apparent with rf-ckb1 and both terms can be found in all queries. 
　we did not expect that the mope rf-ckb would do the worst among the set of rf-ckb's. in fact  we had hypothesized that it would perform the best. its failure to do better may be due to the limited number and size of the documents in it since these are the texts from which the rf module draws and weights terms. for instance  rf-ckb 1 had only 1 documents  but rf-ckb1 had 1 and rf-ckb1 had 1. also  the average document in rf-ckb1 is approximately twice as large as that in rf-ckb 1. the average rf-ckb1 document is not quite twice as large. 
　rf-ckb 1 may also do poorly because its ability to select high-value terms may be handicapped by the purity of its texts. its cases discuss only the home office deduction. although its texts contain lots of terms highly descriptive for the home 
office deduction issue  their discriminatory power is probably undervalued by the rf mechanism because so many of them occur across all four texts. by contrast  discriminating high-value terms within the impure and mixed rf-ckb's is probably easier because they comprise a smaller proportion of each text  which may help the selection metric. the impure documents may provide the  noise  necessary for these high-value terms to stand out. a totally impure rf-ckb like rf-ckb1 and rf-ckb1 might contain too much noise however. 
　thus the query to the ir system is find me cases that look like this where similarity for the ir engine is defined by the terms generated from the rf-ckb that is used. different 
rf-ckb's provide different senses of similarity for the ir 

engine. 
　because the top two layers of the claim lattice did so well  and knowledge about the  purity  of a text would generally not be known to the cbr system  we decided to continue experiments with the following rf-ckb's: 
1. rf-ckb1: the set of mope's for a problem case. 
1. rf-ckb1: the top two layers of a problem case's claim lattice. 
　we ran a similar set of experiments for three other cases from the home office deduction domain. these were honan  
meiers  and soliman. 1 
  extend the range of retrievals to materials outside the scope of the cbr system; 
  improve the recall and precision of ordinary information retrieval; 
  leverage the strengths of each; 
  achieve robust  decent results with minimal effort; 
  require no human in the loop  other than case entry; 
  be reproducible across a variety of problem cases. 
feedback  in which we have no initial query to modify  and 
these results were similar to those found with weissman. a small number of well-chosen full-text documents  we can 
most of the queries generated using rf-ckb1 exceeded the automatically and easily produce a query that achieves good baseline by the time 1 or fewer terms are used. further  results. queries generated using rf-ckb1 always exceeded the base-
　　　　　　　　　　　　　　　　　　　　　　　　　the results are generally best when we use 1 or more line within 1 or fewer terms and achieved better overall terms. note that since the sets of terms are generated autos results than those using rf-ckb1. matically  and efficiently  by the relevance feedback module  
　within the bankruptcy domain we selected three problem the only added cost is that of inquery's evaluation of the cases and again used these two same rf-ckb's. at this query  which is linear in the number of terms . this is in point  the bankruptcy term results do not appear to be as contrast to the situation where the user must input terms or spectacular. the cbr-ir system achieved average precisions even natural language. even if we are restricted to small set ranging from 1 to 1%. better average precision occurs with of short texts that all discuss the same issue  we achieve good higher numbers of terms  1 to 1 . once again  when results. the system uses rf-ckb1  composed of the top two layers 
　　　　　　　　　　　　　　　　　　　　　　　　　within the home office deduction domain  the majority of of the claim lattice of a problem case  it outperforms rfmope rf-ckb's exceeded the baseline  and all of the top-
ckb1  composed of the mope's. random sets of four or five two-layers rf-ckb's did  generally by nearly 1%. using a documents achieved average precisions in the same range. it large number of terms  1  does not degrade the query should be noted that the total number of documents used by as much as might be expected. in fact  in most instances the rf mechanism was still very small; the largest rf-ckb our system achieved results as good as or better than with contained only 1 documents. note however  that we restricted queries with fewer terms. thus  not only is there limited cost our queries to simple terms  but that the baseline query was associated with using this many terms  there is no detrimental composed of a phrase. phrases can be much more descriptive effect. of a text's content. 
our results stand in contrast to those of croft and das  
we are in the process of evaluating our cbr-ir approach 
　　　　　　　　　　　　　　　　　　　　　　　　 croft and das  1   who claimed that relevance feedback with a change in the rf module that allows for the selection of may not be beneficial when using only a small set of relevant pairs of terms found in proximity of each other. these pairs documents. we found this not to be the case. their doubts can be loosely thought of as phrases since we can specify are due to the potential lack of concept coverage by a small how close the terms must be to each other. in on-going work  set of documents. however  their documents were relatively we are evaluating a more problem-specific sense of a  right  short; they used abstracts whereas we used full-length legal answer: a case is listed in the answer key only if the court cases. furthermore  our rf-ckb's are drawn from the top opinion of the problem case actually cites it. 
portion of the claim lattice and hence the terms generated in our approach are probably more descriptive. 
1 	conclusion 
　　　　　　　　　　　　　　　　　　　　　　　　　both case-base reasoning and information retrieval have the goal of this project is to create a system that provides their strengths and weaknesses. we are seeking to exploit the access to more cases than usually afforded by a cbr sys- strengths  and remediate the weaknesses  of each  by pursuing tem and with a more precise sense of relevance than provided a hybrid cbr-ir approach. our preliminary results show by traditional ir systems. in our hybrid cbr-ir approach  that cbr and ir indeed lend themselves to beneficial cross knowledge-intensive reasoning is performed on a  small  cor- fertilization. pus of cases represented in a cbr system  and important cases we have shown that using a modified version of relevance 

selected from this analysis are used to drive a traditional textbased ir engine on a large corpus. we use the cbr system to locate good examples of the kind of cases we want and the ir system to retrieve more of the same. in this two-stage approach  the first stage is knowledge-intensive and depends on a highly articulated cbr notion of similarity; the second uses weak but easily applied text-based notions. 
in summary  our approach integrates cbr with ir to: 
1 	acknowledgements 
we thank michelle lamar for the use of her relevance feedback code and providing assistance in building the text collections. we also thank jamie callan  david aha  and the anonymous referees for their comments and advice. 
1honan v. comm.  t.c. memo. 1; meiers v. comm.   ashley  1  kevin d. ashley. modeling legal argument: 1 f.1d 1  1th cir. 1 ; soliman v. comm.  1 f.1d 1  1th 	reasoning with cases and hypotheticals. m.i.t. press  
cir. 1 . 	cambridge  ma  1. 