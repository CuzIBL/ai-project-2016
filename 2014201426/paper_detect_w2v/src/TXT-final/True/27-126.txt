ions. this view is shared by another well-known author  allen newell  newell  1   who contemplates the possibility of viewing computer programs at a level more abstract than 
1 	temp1rai 	reasoning 
that of the programming language  which he calls the knowledge-level. 
¡¡the notion of a mental state is useful because it is abstract. models at more specific levels  e.g.  mechanical and biological models  are difficult to construct. they require information that we often do not have  such as the mechanical structure of the agent  or its program. on the other hand  mental-level models can be constructed based on observable facts the agent's behavior - together with some background knowledge. in fact  as mccarthy points out  we might sometimes want to use these models even when we have precise lower level specifications of the agent  e.g. c code. we might do this either because the mental-level description is more intuitive or because computationally it is less complex to work with. 
¡¡we present a formalism that attempts to make these ideas more concrete and that will hopefully lead to better understanding of how the ascription of mental state could be mechanized. motivated by work in decisiontheory  luce and raiffa  1  and work on knowledge ascription  halpern and moses  1; rosenschein  1   we suggested in  brafman and tennenholtz  1b  a specific structure for mental-level models  consisting of beliefs  desires and a decision criterion. this model showed how these elements act as constraints on the agent's action  and how these constraints can be used to ascribe beliefs to the agent. we would like to use this model in a particular prediction context  where we observe an agent performing part of a task  we know its goal  and we would like to predict its next actions. we use the following process: first  we ascribe beliefs to the agent based on the behavior we have seen so far. next  we update the ascribed beliefs based on observations the agent makes  e.g.  new information it has access to or the outcomes of its past actions. then  in order to predict the agent's next action  we examine what action would be perceived as best by an agent in this mental state. 
¡¡in order to perform this prediction process  we must understand how beliefs can be ascribed  how they should be updated  and how they should be used to determine the best perceived action. we have examined the first and the last question in  brafman and tennenholtz  1b   although not in the context of prediction . in this paper  we wish to concentrate on the second question  that of modeling the agent's belief change. 
¡¡

¡¡the reader should not confuse this last question with another important question which has received much attention: how should an agent change its beliefs given new information   for example  see  levesque  1; 
friedman and halpern  1; katsuno and mendelzon  
1; del val and shoham  1; alchourron et a/.  1; goldszmidt and pearl  1 .  in our work we are concerned with externally modeling the changes occurring within the agent rather than saying how that agent should update its beliefs. although that agent may be implementing one of the above belief revision methods  it is quite possible that it has no explicit representation of beliefs and that its  idea  of update is some complex assembler routine. 
   our discussion of the problem of prediction will be in the context of the framework of mental-level modeling and belief ascription investigated in  brafman and tennenholtz  1b . this framework is reviewed in section 1. in section 1 we discuss the problem of prediction. we suggest a three-step process for prediction and highlight the importance of the ascription of a belief change operator to this process. in section 1 we introduce a particular belief change operator and show that it has desirable properties from a decision-theoretic perspective. moreover  we show that under minimal assumptions  this belief change operator can always be ascribed to an agent. 
1 	the framework 
we start by establishing a structure for mental-level models. our framework  discussed in  brafman and tennenholtz  1b   is motivated by the work of halpern and moses  halpern and moses  1  and rosenschein  rosenschein  1  on knowledge ascription  and by ideas from decision-theory  savage  1; luce and raiffa  1 . to clarify the concepts used  we will refer to the following example. 
e x a m p l e 1 we start with a robot located at an initial position. the robot is given a task of finding a small can located in one of three possible positions: a  b  or c the robot can move in any direction and can recognize a can from a distance of 1 meters.  see figure 1 . 
1 	t h e a g e n t - basic d e s c r i p t i o n 
an agent is described by a set of possible  local  states and a set of possible actions. the agent functions within an environment  which may also be in one of a number of states. we refer to the state of the system  i.e.  that of both the agent and the environment as a global state. w.l.o.g.  we will assume that the environment does not perform actions. the effects of the agent's actions are a  deterministic  function of its state and the environment's state.1 this effect is described by the transition function. together  the agent and the environment constitute a state machine with two components  with transitions at each state corresponding to the agent's possible actions. it may be the case that not all combinations of an agent's local state and an environment's state are possible. those global states that are possible are called possible worlds. 

example 1  continued : suppose that our robot has imperfect sensing of its position. thus  its local state would include its position reading as well as whether or not it has observed the can; its actions correspond to motions in various directions. the state of the environment describes the actual position of the can and each possible world describes  1  the robot's position   s the can's position  1  the robot's position reading and  1  whether it has observed the can. the transition function describes how each motion changes the global state of the system. there are three initial states. in each the position of the robot is the given initial position and the can is located in one of positions a  b  or c. 
¡¡we say that an agent knows some fact if in all the worlds the agent should consider possible  this fact holds. the worlds an agent should consider possible are those in which its information  as represented by its local state  would be as it is now. 
definition 1 the set of worlds possible at /  pw l   is {w € s : the agent's local state in w is i}. the agent knows ¦µ at w € s if ¦µ holds in all worlds in pw l   where i is its local state at w. 
example 1  continued : let us assume from now on that the robot's position reading is perfect. in that case  the robot knows its position  since a position reading r is part of its local state and r can only be obtained in worlds in which the actual position of the robot is r. however  unless the robot has observed the can  it does not know the can's position  since it has possible worlds in which the can's position is different. 
¡¡the agent's observed  or programmed behavior is formally captured by the notion of a protocol. 
1
¡¡¡¡a framework in which the environment does act can be mapped into this framework using richer state descriptions and larger sets of states  a common practice in game theory. 
1
¡¡¡¡though context is an overloaded term  its use here seems appropriate  following  fagin et al.  1 . 
	brafman and tennenholtz 	1 
¡¡

1 	temporal reasoning 
¡¡

¡¡while utilities are easily compared  it is not a priori clear how to compare perceived outcomes  thus  how to choose among protocols. a strategy for choice under uncertainty is required. this strategy could depend on  for example  the agent's attitude towards risk. this strategy is represented by the decision criterion  a function taking a set of perceived outcomes and returning the set of most preferred among them. 
d e f i n i t i o n 1 a decision criterion p is a function which maps each set of tuples  of equal length  of real numbers  to a subset of it. 
¡¡two examples of decision criteria are maximin  which chooses the tuples in which the worst case outcome is maximal  and the principle of indifference which prefers tuples whose average outcome is maximal. a fuller discussion of decision criteria appears in  luce and raiffa  
1; brafman and tennenholtz  1a .  
¡¡we come to a key definition that ties all of the components we have discussed so far. 
d e f i n i t i o n 1 the agency hypothesis; at each state the agent follows a protocol whose perceived outcome is most preferred  according to its decision criterion  among the set of perceived outcomes of all possible-
protocols.1 
¡¡the agency hypothesis takes the view of a rational balance among the agent's beliefs  utilities  decision criterion and behavior. it states that the agent chooses actions whose perceived outcome is maximal according to its decision criterion. 
¡¡since given a fixed utility function  the decision criterion induces a choice among actions  we will often use the term 'most preferred protocol' in place of 'the protocol whose perceived outcome is most preferred'. 
1 	a s c r i b i n g b e l i e f 
the various elements at the mental-level are related through a rational balance. we can exploit this relation to ascribe a mental state to an agent. we use the available information  such as observed behavior and background information  to constrain the possible values of the unknown mental state. we now show how belief can be ascribed according to our framework. 
¡¡belief ascription requires certain information regarding the agent. this information should specify some of the other elements of the rational balance we have just 
1
¡¡¡¡the possible protocols are implicitly denned by the set of actions aa  cf. def. 1 . 

discussed. using this information we look for belief assignments confirming the agency hypothesis. that is  suppose the entity modeled satisfies the agency hypothesis and that its utilities and decision criterion are as given  then such beliefs would lead us to act as was observed. this is a process of constraint satisfaction. thus a formal statement of belief ascription is the following: given a context c for an agent a  find a belief assignment b such that b together with the agent's behavior  its utility function and its decision criterion confirm the agency hypothesis. 
example 1  continued : we will try to ascribe beliefs to the robot. we assume that the decision criterion used is maximin.  what follows applies also to the principle of indifference . suppose we observe the robot moving along the path described in figure 1. what can we say about its beliefs  we must see under what beliefs the path observed would yield the highest utility. it is easy to see the ascribed plausible worlds are {a b}. if the robot believed only one state to be plausible it would head directly to it. similarly  if {b c} was believed the robot's path would head more toward them. if the robot believes {a b c}  a better path would be along the middle  rather than the left-hand side. 
¡¡our ability to ascribe belief in the framework of the mental-level model just presented is discussed in  brafman and tennenholtz  1b . 
1 	p r e d i c t i o n s 
we wish to explore the application of mental-level models in a particular form of prediction: we observed an agent taking part in some activity; we know its goals; and we wish to predict its next actions. in what follows we try to examine what problems this task raises and how we might solve them. we will concentrate on one particular issue  belief change. we will soon see how it relates to our task. the approach we suggest underlies some of the work done in belief and plan ascription. we believe that a formal approach will aid in understanding this task better and in detecting the implicit assumptions made in predicting an agent's future behavior. 
¡¡to predict an agent's next action  we go through three steps  illustrated in figure 1 :  1  construct a mentallevel model of the agent based on actions performed until now;  1  revise the agent's ascribed beliefs  if needed  based on the observations it made after performing the last action;  1  predict the action which has the most preferred perceived outcome based on these beliefs. 
	brafman and tennenholtz 	1 
¡¡
example 1  continued : in the previous section we saw an example of belief ascription. this corresponds to the first stage: constructing a mental-level model based on observations and background knowledge. the robot's beliefs were {a b}. based on these beliefs we can predict that the robot will continue to move in its current direction until it can observe whether the can is in a or b. suppose the can was observed to be in b. in that case  the beliefs of the robot are revised to contain only b. given these beliefs  we expect the robot to turn to the right  i.e. toward b . 
¡¡our human experience shows that models of mentalstate are useful in predicting human behavior  and we believe they are also likely to succeed with human-made devices  hence the agency hypothesis: the device acts as an agent of its designer  echoing its goals and beliefs . thus  using mental-level models seems to make heuristic sense. however  when is this really appropriate  moreover  when is the particular formalism suggested here appropriate  reexamining the three-step prediction process we see two major implicit assumptions: 
  we can model the observed behavior of an agent using a mental-level model. 
  we can assume some methodical belief change process. 
¡¡we discussed the first among these issues in our previous work  brafman and tennenholtz  1b . in particular  we have shown a class of agents that can be ascribed the mental-level model discussed in section 1. we devote the rest of this paper to the second issue. 
1 	b e l i e f c h a n g e 
suppose we have constructed a mental-level model based on past behaviors. to use it in predicting future behavior  we must make an additional assumption  that there is some temporal coherence of beliefs. consider the example of the robot that accompanied the preceding sections. we observe the robot move along a certain path and ascribe it the belief assignment {a b}. at a certain stage  it is near enough to a and b to be able to see whether the can is in one of these two positions. we expect this new information to affect the behavior of the robot. in our ascribed model of the robot  we expect this information to be manifested in terms of belief change. however  unless the new belief can be somehow constructed from the old beliefs and the observation  we will have very little ability to predict future behavior. 
¡¡we first suggest a restriction on the relationship between beliefs in different states. later on  we will show that this restriction is both natural and useful. 
1 	admissibility 
consider the following restriction: if my new information is consistent with some of the runs i previously considered plausible  i will now consider plausible those runs previously considered plausible that are consistent with this new information. 
¡¡let na{s  = t va s    i.e.  the state that will follow s when a performs the action specified by its protocol  and na{t  = {na s  s e t}. 
1 temporal rfasoning 
d e f i n i t i o n 1 a belief assignment b  for agent a  is admissible  if for local states 1' such that i' follows i on some run: whenever na b 1   n pw l'  1 then b{v  = na b l   d pw{ li j otherwise v is called a revision state and b v  can be any subset of pw v . 
if worlds corresponded to models of some theory  then  in syntactic terms  admissibility corresponds to conjoining the new data with the existing beliefs  whenever this is consistent. it is closely related to the probabilistic idea of conditioning beliefs upon new information. 
¡¡it turns out that admissible belief assignment can be viewed in a different way. as the following theorem shows  an admissible belief assignment is equivalent to a belief assignment induced by a ranking of the set of initial states  that is  a belief assignment which assigns to every local state those worlds in pw l  that originate in initial states whose rank is minimal. intuitively  we associate minimal rank with greater plausibility. 
t h e o r e m 1 assuming perfect recall 1 let i. -p  denote the initial state of the  w v  run prefix. a belief assignment b is admissible iff there is a ranking function r  i.e.  a total pre-order  on the possible initial worlds i  such that b l  = {w € pw{1  : i   w p . is r-minimal}. 
1 	w h y a d m i s s i b i l i t y 
the fact that admissible beliefs have a nice representation seems encouraging. it suggests a refinement to our model in which beliefs have the additional structure provided by a ranking over possible worlds. however  this by itself is no reason to accept this restriction. remember that we want to show that mental-level models are abstractions that are grounded in lower level phenomena. the kind of support we need would look like  under assumption x on the agent's behavior  a ranked belief assignment can be ascribed to it . in this section  we would like to present results of this nature. once these questions are answered  we would be able to make justified predictions based on the approach presented in the previous section. on our way to this goal  we will also get some interesting results from a decision-theoretic perspective. 
¡¡recall the agency hypothesis. the agent was viewed as choosing among protocols based on the utility of the runs they generate1 and its beliefs. however  there is an alternative way for choosing among actions given the agent's beliefs  called backwards induction. 
definition 1 a backwards i n d u c t i o n  bi  protocol for an agent a is defined inductively as follows: for local states i  all of whose children are final states  assign a most preferred action at that state.  in this case an action determines a run suffix.  inductively  assign to each local state an action that is most preferred given the choices for its descendents. 
1
¡¡¡¡proofs are omitted due to lack of space  and will appear in a longer version of this paper. 
1
¡¡¡¡an agent is said to have perfect recall if its local state contains all previous local states. 
¡¡¡¡1 this section assumes that there are only a finite number of possible local states  that runs are finite  and that the agent has perfect recall. 
¡¡
¡¡backwards induction is considered the rational way of choosing actions according to classical decision-theory. another decision-theoretic concept we will use is the following  where o denotes vector concatenation : 
d e f i n i t i o n 1 a decision criterion satisfies the suret h i n g principle if v o v' is at least as preferred as u o u' whenever v is at least as preferred as v' and u is at least as preferred as u'. 
that is  suppose the agent has to choose between two actions  a and a'. it prefers a over a' when the plausible worlds are b. it also prefers a over a' when the plausible worlds are b'. if this agent satisfies the sure-thing principle it should also prefer a over a' when the plausible worlds are b u b'. in what follows we assume that the agent satisfies the sure-thing principle. 
¡¡one final note: when we compare protocols at an initial state we only care about their outcome on states that are plausible; we are indifferent to what actions we take in revision states. we can view this as a choice among partial protocols  defined only on the plausible worlds. however  once we get to a revision state we will have to choose among a new set of partial protocols  depending upon our beliefs in the revision state. since we assume perfect recall  these choices are independent. hence  it will be convenient for us to think about the agent making all these choices initially. that is  at the initial state it chooses not only what to do on the plausible worlds  but also what to do on revision states  if it ever gets to them. this way we view the agent as choosing among full protocols  and the notion of most preferred protocol will be defined accordingly. 
¡¡given the above machinery  we first look at normative reasons for accepting admissibility. 
t h e o r e m 1 let a be an agent with admissible beliefs. its most preferred protocols at the initial local state remain most preferred at all the following states. 
¡¡therefore  agents of theorem 1 can choose a protocol once and for all at the initial state based on its perceived outcome. when beliefs are not admissible  a counter example can be constructed where protocols most preferred at the initial states are not most preferred later on. 
¡¡another nice property associated with admissible beliefs is given by the following theorem and corollary. 
t h e o r e m 1 let a have admissible beliefs  v is a bi protocol for a iff it is most preferred at the initial state. 
corollary 1 assume a has admissible beliefs. there is a utility function on states such that a can be viewed as executing the best local action at each state. 
¡¡our claim that admissibility is a good modeling assumption is supported by the following result: 
t h e o r e m 1 	let v be the observed protocol of an entity. 
if this entity can be ascribed beliefs at the initial state and at subsequent revision states based on this protocol  it can be ascribed an admissible belief assignment at all local states. 
¡¡thus  admissibility is free if we can ascribe beliefs at the initial and revision states. 
¡¡the previous results imply that admissible beliefs are useful for ascription and prediction. in fact  the results can be even further improved. the fact that we associate utilities with runs rather than states complicates our life when we try to ascribe beliefs. to ascribe beliefs we must at each state compare whole protocols and the run suffixes they produce. it would be much easier if we could only look at single actions and their immediate outcomes. doing this would require defining a utility function over the set of states  rather than the set of run suffixes. indeed  this is possible: 
t h e o r e m 1 let *p be the observed protocol of an agent  and suppose that this agent can be ascribed beliefs at the initial state and at all subsequent revision states based on this protocol. then  it can be ascribed an admissible belief assignment at all local states and a local utility function over states such that its observed action has the most preferred perceived outcome according to the local utility function. 
1 	discussion 
the following question motivates much of the research in belief and belief change: given that we can make better programs by equipping them with large amounts of knowledge  how should this knowledge be represented  and how should it be updated   for example  see  levesque  1; friedman and halpern  1; katsuno and mendelzon  1; del val and shoham  1; alchourron et al  1; goldszmidt and pearl  1 .  that work often attempts to capture our intuitive notion of belief and belief change. in addition  it often implicitly assumes that we  the designers  are those who will supply the agent with its knowledge  at least initially. 
¡¡we are concerned with a more specific question of representation and ask: how should an agent represent its information about another agent in a way that will facilitate explaining and predicting the other agent's behavior  moreover  we assume that the bulk of an agent's knowledge about other agents comes from a particular source  observation of these agent's behavior. thus  we are more concerned with modeling agent's ascribed beliefs than with designing them. 
¡¡an important related work that shares some of our perspective is levesque's  levesque  1   which is concerned with treating computers as believers. however  his work describes the beliefs of one particular class of agents whose actions are answering queries. our work attempts to address a more general class of agents  whose actions are arbitrary. 
modeling data is a central task of machine-learning. 
much like our work  these models are constructed to help make predictions  e.g.  a decision tree helps us predict what class an instance belongs to. our work brings to this task a special bias in the form of the agencyhypothesis: machines are agents of their designers; they are usually designed with a purpose in mind and with some underlying assumptions; therefore  they should be modeled accordingly. with this motivation in mind  this work and  brafman and tennenholtz  1b  attempt to understand the basis for modeling entities as if they have a mental state. the central issues are: what elements should such a model contain  how should we 
	brafman and tennenholtz 	1 
¡¡
use observable information to construct it  and  under what assumptions is our modeling  bias  justified  
¡¡an important issue for future work involves predicting an agent's behavior at revision states. currently  we do not know how to model an agent's belief revision process and cannot predict an agent's action after an unexpected observation. past actions do not tell how to ascribe belief in that case. we believe some form of an inductive leap is required  which should exploit additional structure  not present in our current model. such structure could be obtained by e.g.  augmenting our purely semantic construction with an interpretation of a suitable language over the possible states.1 
¡¡this paper complements our previous work on belief ascription  brafman and tennenholtz  1b  and supplies initial answers to the above-mentioned questions. in this paper  we reviewed our proposed structure for mental-level models and their construction  and explained how they can be used to predict an agent's future behavior. in order to use mental-level models in making predictions  we must constantly update them. a key component in this update process is the ability to model the belief change of other agents. we suggested 
admissibility as a belief change operator  examined its properties and showed that we can accept it under rather weak modeling assumptions. putting these ingredients together  we get a theory of action prediction using a mental-level model  which consists of the three-step process  a theory of belief ascription  discussed in  brafman and tennenholtz  1b    and a study of belief change modeling.1 
a c k n o w l e d g m e n t : we thank yoav shoham for useful discussions relating to this work. the first author was partially supported by arpa and afosr through grants af f 1-1 and af f1-j-1. 
