
we introduce a rule selection algorithm called
roccer  which operates by selecting classification rules from a larger set of rules - for instance found by apriori - using roc analysis. experimental comparison with rule induction algorithms shows that roccer tends to produce considerably smaller rule sets with compatible area under the roc curve  auc  values. the individual rules that compose the rule set also have higher support and stronger association indexes.
1	introduction
classification rule learning can be defined as the process of  given a set of training examples  finding a set of rules that can be used for classification or prediction. almost all classification rule learning algorithms belong to one of two families  namely separate-and-conquer and divide-and-conquer algorithms. the two families share a number of characteristics  most notably the assumption that the example space contains large continuous regions of constant class membership. the major differences are outlined below.
　in the separate-and-conquer family of classification rule learning algorithms  furnkranz  1：    the search procedure is generally an iterative greedy set-covering algorithm that on each iteration finds the best rule  according to a search criterion  and removes the covered examples. the process is repeated on the remaining examples until all examples have been covered or some stopping criterion has been met. in order to build a classifier  the rules found in each iteration are gathered to form either an ordered rule list  a decision list  or an unordered rule set. in the former case the classification is given by the first rule in the list that fires  while in the latter case the predictions of rules that fire are combined to predict a class.
　this approach contrasts with the divide-and-conquer family of learning algorithms  quinlan  1   where a global classifier is built following a top-down strategy by consecutive refinements of a partial theory. the result is generally expressed as a decision tree  which completely divides the instance space into non-overlapping axis-parallel hyperrectangles.
　as decision tree induction necessarily builds complete disjoint models  in some complex domains with highdimensional feature spaces these models can be quite complex. in such cases  individual rule learning algorithms may be preferable since they are capable of inducing overlapping and simpler rule sets  van den eijkel  1 . however  one of the problems of set-covering rule learning is that rules are found in isolation  although they are used in the context of the others inside the classifier. this issue can pose several problems for a rule learning algorithm. first  from the learning perspective  fewer and fewer examples are available as covering progresses. in latter stages of induction  this may lead to fragmented training sets and rules with insufficient statistical support  domingos  1 . furthermore  each new rule is constructed in complete ignorance of the examples already covered by the previously induced rules. if a bad rule has been introduced in the rule set  there is no chance of finding a better rule for those examples  there is no backtracking .
　in this work we present a new rule learning algorithm named roccer  aimed to overcome such problems. the main idea of the algorithm is to construct a convex hull in roc space. we evaluate roccer on a broad set of benchmark domains from the uci repository  blake and merz  1  and compare it with other rule induction methods. the paper is organized as follows. in section 1 we discuss the background related to this work. section 1 presents our proposed algorithm  and section 1 contains the experimental evaluation. in section 1 we make some concluding remarks.
1	related work
several approaches have been proposed in the literature to overcome the fragmentation problem.  liu et al.  1  decouple the rule generation from the covering step. the basic idea is to use an association rule algorithm to gather all rules that predict the class attribute and also pass a minimum quality criterion into the rule set. although this approach might overcome some of the problems of the separateand-conquer approach  and its performance was reported to outperform standard rule learning algorithms in some domains  its main drawback is related to the number of generated rules. often they considerably outnumber the examples  implying serious difficulties from a knowledge discovery point of view  since the understandability and usability of the generated model would decrease and the risk of overfitting would increase. this idea has been extended in the aprioric and apriorisd algorithms  javanoski and lavrac  1;・ kavsek・ et al.  1  by adding an additional filtering step to remove some of the redundant rules. however  aprioric still tends to build large rule sets and apriorisd has been developed mainly for subgroup discovery.
　a different approach is to use weighted covering  which has been independently proposed by  cohen and singer  1  and  weiss and indurkhya  1 . instead of completely removing the examples covered by the best rule on each iteration  their weights are decreased  and in each iteration the covering algorithm concentrates on highly-weighted  i.e.  infrequently covered  examples.  lavrac・ et al.  1  also discuss the use of weighted covering in a subgroup discovery context. alternative methods to remove redundant rules are based on pruning  furnkranz and widmer  1;： cohen  1 .
　some authors propose the use of confidence thresholds for classification.  gamberger and lavrac  1・   include only rules with high confidence in the rule set. the classifier then refuses to classify a new instance if none of the rules cover it.  ferri et al.  1  extends this idea by retraining a new classifier on the unclassified examples.
1	the roccer rule selection algorithm.
our approach relies on using roc analysis for selecting rules instead of using a classical covering algorithm. roughly speaking  a roc graph is a plot of the fraction of positive examples misclassified - false positive rate  fpr  - on the x axis against the fraction of positive examples correctly classified - true positive rate  tpr  - on the y axis. it is possible to plot in a roc graph either a single rule  a classifier  formed by a rule set  or not  or even a partial classifier  formed by a subset of a rule set  for instance .
　for a threshold-based classifier one can obtain several pairs of points  fpri tpri  by varying the threshold. if we trace a line connecting those points we obtain a curve in the roc space that represents the behaviour of the classifier over all possible choices of the respective threshold. in a rule learning context   furnkranz and flach  1：   show that rule learning using a set covering approach can be seen as tracing a curve in roc space. to see why  assume we have a empty rule list  represented by the point  1  in roc space. adding a new rule rj to the rule list implies a shift to the point  fprj tprj   where fprj and tprj is the tpr and fpr of the partial rule list  interpreted as a decision list  containing all rules already learnt including rj. a curve can be traced by plotting all partial rule lists  fprj tprj   for j varying from 1 to the total number n of rules in the final rule list in the order they are learnt. a final default rule that always predicts the positive class can be added at the end  connecting the point  fprn tprn  to the point  1 .
　our approach is based on this observation  and the fact that the points which represent the optimum thresholds lie on the upper convex hull of the roc curve  provost and fawcett  1 . rules come from a external larger set of rules  in our implementation  we use the apriori association rule learning algorithm  fixing the head of the rules to each of the possible class values  allowing us to deal with multi-class problems  and we perform a selection step based on the roc curve. the basic idea is to only insert a rule in the rule list if the insertion leads to a point outside the current roc convex hull  the current roc convex hull is the upper convex hull of the rules that are already in the rule list . otherwise the rule is discarded. for a better understanding of how our algorithm works  we first describe it using an example.
　rules are selected separately for each class  and are kept in an ordered rule list. let's label the class we are selecting rules for positive; the label negative represents the  conjunction of  examples in the other class es . first  we initialize the rule list with a default rule  rdefault  which always predicts positive. the current roc convex hull is formed then by 1 points   1  and  1  which means  ignore the default rule  classify everything as negative  or use the default rule  classify everything as positive  . we use fprri  tprri to refer to the rule ri's true and false positive rates  and tpri  fpri to refer to a point i in the roc curve  representing the corresponding rule list's true and false positive rates. suppose now we are inserting a new rule r1. as the actual convex hull is formed only by the line segment  1   1   r1 will only be inserted if the point formed by the rule's  fprr1 tprr1  is above the convex hull. let's say r1 is inserted. the current convex hull is then updated  and contains the points  1   fpr1 tpr1   1   where fpr1 = fprr1 and tpr1 = tprr1. this process is depicted in figure 1.

figure 1: r1 leads to a point outside the current convex hull  main diagonal  and is therefore inserted in the rule list.
　suppose we are now trying to insert a second rule r1. as we have said before  in the standard set-covering approach the learning of a new rule does not take into account the rules already learnt. in our approach  we try to overcome this issue using the roc graph to analyze interactions among rules. we do this by comparing the rule we are trying to insert with the slopes of each of he line segments in the current convex hull in the roc graph. in our example  if the slope of the point formed by the origin and  fprr1 tprr1  is above the line from the origin to the point  fpr1 tpr1   we say that r1  improves in relation to  r1 and insert r1 in the rule list. this is similar to if r1 had been learned before r1 in the set-covering approach. by comparing with the rules which are already in the rule list  our algorithm provides a kind of backtracking. this insertion will  of course  produce changes to the other points in the roc curve. the first non-trivial point in the roc curve changes to  fprr1 tprr1 . if r1 and r1 do not have any overlap  the second point will be  fprr1+ fprr1 tprr1+ tprr1 . if r1 and r1 do have some overlap  however  we should discount the examples covered by both rules to calculate the second point.
　if r1 is not inserted at the first iteration  we proceed by comparing with the remaining line segments in the roc convex hull. before we compare with the next line segment  we update r1's fpr and tpr by removing the examples covered by r1  the examples which evaluate to true for r1's antecedent . in fact  we are  interpreting  r1 as  r1…r1. if the updated position of r1 is above the line from r1 to rdefault  r1 is inserted in the rule list after r1. this process is depicted in figure 1. otherwise  since no further rules remain  r1 is discarded. the pseudo-code of this algorithm is shown in algorithm 1.

 a  r1 does not improve the  b  ...but it does when comconvex hull when compared to pared with rdefault.
r1...
figure 1: finding the right point to insert r1.
algorithm 1: the roccer algorithm.
data: rsin: a  large  rule set for a given class
result: rsout: a  smaller  rule list containing the selected rules
rsout = {rdefault}; foreach rule （ rsin do
   trytoinsertrule rule  endfch return rsout procedure trytoinsertrule rule  ruletocompare = first rule in rsout; repeat
if rule's  fpr tpr  is outside convex hull then
   insert rule into rsout before ruletocompare else
/* shift to a new origin */
remove all the examples from rule which are covered by ruletocompare;
actualize rule's  fpr tpr ;
   ruletocompare = next rule in rsout endif
until ruletocompare 1= rdefault; if rule is not inserted then
   discard rule endif

due to overlapping coverage among rules  this process does not necessarily lead to a convex curve. the insertion of a new rule in the rule list may introduce concavities before or after the point of insertion. if concavities occur after an insertion  the inserted rule covers examples originally covered by subsequent rules  which decreases the latter's precision. in this case  we remove the rules where the concavity occurs. alternatively  if the concavity occurs before the insertion point  both rules share a region where they misclassify some examples. in this case  it is unreasonable to use the partial rule list including the first rule but excluding the second  because the corresponding roc point is under the convex hull. therefore  we construct the disjunction of the two rules and treat it as a single rule.
　in our implementation  rules presented to roccer are initially ordered  using the euclidean distance to the point  1  in the roc space. however  due to the possibility to remove a selected rule and the  implicit  backtracking  the order dependence in selecting rules is lower than a decision list in inducing rules. experiments with another orderings would be an interesting issue for further work  though.
　this concludes the description of the training phase. for classification  we also use a roc-based method. bayes' theorem states that the odds that a classifier correctly classifies an instance  posterior odds  is given by the likelihood ratio times the odds of the instance being of the predicted class  prior odds . in roc space  the likelihood ratio can be interpreted as tpr/fpr. recall that we selected rules separately for each class. thus  we have a roc convex hull for each class. to classify a new instance we also consider each class separately  and  for each class  we determine the first rule that fires in the respective roc convex hull. this rule has an associated  tpr fpr  in the roc curve  which yields a likelihood ratio. the posterior odds is then converted back to a probability  for ranking   or we select the class with maximum posterior odds  for classification .
1	experimental evaluation
　in order to empirically evaluate our proposed approach  we performed a experimental evaluation using 1 data sets from uci  blake and merz  1 . we used only data sets without missing values  as apriori  the association rule algorithm we use to generate the rules for subsequent selection by roccer  can't handle them. table 1 summarizes the data sets employed in this study. it shows  for each data set  the number of attributes  #attrs   the number of examples  #examples   and percentage of examples in the majority class  %majclass  - although roccer can handle more than two classes  in order to calculate auc values we restricted our experiments to twoclass problems. for data sets having more than two classes  we chose the class with fewer examples as the positive class  and collapsed the remaining classes as the negative.
　roccer's results were compared with those obtained by the following rule learning systems:
cn1 this algorithm is a classical implementation of the separate-and-conquer rule learning family. in its first version  clark and niblett  1  cn1 induces a decision list using entropy as a search heuristic. it has later
#data set# attrs# examples%majclass1breast111bupa111e.coli111flag111german111glass111haberman111heart111ionosphere111kr-vs-kp111letter-a111new-thyroid111nursery111pima111satimage111vehicle11table 1: uci data sets used in our experiments.
been modified to incorporate the induction of unordered rule sets and laplace error correction as evaluation function  clark and boswell  1 .
ripper  cohen  1  proposed ripper in the incremental reduced error pruning  irep   furnkranz and widmer ： 1  context. it has features such as error-based pruning and an mdl-based heuristic for determining how many rules should be learned.
slipper this algorithm is a further improvement of ripper which uses a weighted set-covering approach  cohen and singer  1 .
c1  quinlan  1 's c1 is almost a standard in empirical comparison of symbolic learning algorithms. it is a member of divide-and-conquer family. c1 uses information gain as quality measure to build a decision tree and a post-pruning step based on error reduction. we can consider each branch in a decision tree as a rule.
ripper and slipper were used with -a option to generate rules for both classes. cn1 was used in its two versions  ordered  cn1or  and unordered  cn1 . we also evaluated both pruned  c1  and non-pruned  c1np  trees induced by c1. all other parameters were set to default values. in order to calculate the auc values we estimated probabilities of each rule using laplace correction. for the unordered version of cn1  probabilities were estimated using all fired rules. auc values were estimated using the trapezoidal rule. we used  borgelt and kruse  1 's implementation of apriori to generate the large rule sets used by roccer. the parameters were set to 1% of confidence and 1 of the percentage of minority class as support. for roccer  the probabilities were estimated by the posterior odds  described in section 1 . we also compare with a bagging of all rules generated by
apriori.
　we ran the experiments using 1-fold stratified crossvalidation. the experiment is paired  i.e.  all inducers were given the same training and test files. the averaged auc values  and respective standard deviations in brackets  are shown in table 1. we also perform a two-tailed dunett multiple comparison with a control procedure1 using roccer as control  the other results against roccer . cells having auc values statistically better than roccer are represented in dark gray while light gray is used to represent cells statistically worse than roccer  both with 1% confidence level.
　table 1 shows relatively few statistically significant differences. comparing against c1  roccer achieved 1 wins and 1 loss. this is the same score if we compare roccer against ripper. against slipper  the results are 1 wins and no losses. a comparison of roccer against c1 without pruning  cn1 and cn1 ordered yield 1 losses and no wins. we believe these two losses are due to the high degree of class skew in those two datasets  they are the most skewed in our study . in order to allow apriori to find rules for both classes in these domains  the support parameter used in apriori is very low. in these cases we have both a small number of rules generated for the minority class and a large number of rules generated for the majority class. further improvements should be made in roccer to cope with such situations  for instance  it would be interesting to introduce different minimum support for each class . taking into account all the rule learning algorithms  the score is 1 wins and 1 losses  all losses are concentrated in the two skewed domains  though . comparing with all the generated rules  roccer produced 1 wins and no losses. we also compute auc value on selecting k  the same number as roccer random rules and rules with higher individual auc values. due to lack of space results are not shown in this paper  but they are in most of the cases significantly worst and never better than roccer. this indicates that roccer's selection procedure is responsible for a gain of performance over all the presented rules.
　the good results with both versions of cn1  and the relatively poor auc figures for ripper and slipper  are worth noticing  and may be explained by the absence of pruning mechanisms. it has already been reported in the literature that non-pruned trees are better for probability prediction and thus produce higher auc values  provost and domingos  1 . it might be expected that a similar phenomenon also would occur with algorithms from the separate-and-conquer family. ripper and slipper are - at least conceptually - similar to cn1 but incorporate  respectively  rule pruning and weighted coverage.
　table 1 presents the average size  in number of rules  of the rule sets for each algorithm. size 1 means that the classifier is formed only by the default rule. the picture here is more clear. apart from some exceptions  roccer produces smaller rule sets than c1 without pruning  cn1  both ordered and unordered   and slipper. on the other hand  ripper produced  significantly  smaller rule sets in 1 out of 1 domains  and there were 1 draws and 1 win. a further investigation involving the data sets where most of the algorithms produced smaller rule sets than roccer  breast 
heart  ionosphere and kr-vs-kp  might produce some in-
#roccerc1c1npcn1cn1orripperslipperall1.1.1 1 1 1 1 1 1 1 1 1 1 1  1 1 1 1
1.1.1 
1 1 1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 1  1 1 1 1 1 1 1 1  1 1 1 1.1.1  1 1  1 1  1 1  1 1  1 1  1  1  1 1  1.1.1  1 1  1 1  1 1  1 1  1 1  1  1  1 1 
1.1.1 	1 1 	1 1 	1 1 	1 1 	1 1 	1  1 	1 1 
1.1.1  1 1  1 1  1 1  1 1  1 1  1  1  1 1  1.1.1  1 1  1 1  1 1  1 1  1 1  1  1  1 1 
1.1.1 	1 1 	1 1 	1 1 	1 1 	1 1 	1  1 	1 1 
1.1.1 	1 1 	1 1 	1 1 	1 1 	1 1 	1  1 	1 1 
1.1.1 	1 1 	1 1 	1 1 	1 1 	1 1 	1 1 	1 1 
1.1.1  1 1  1 1  1 1  1 1  1 1  1  1  1 1  1.1.1  1 1  1 1  1 1  1 1  1 1  1 1  1 1 
	1.1.1 	1 1 	1 1 	1 1 	1 1 	1 1 	1  1 	1 1 
1
1.1.1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 1  1 
1  1 1 1 1 1 1 1 avg11111111table 1: auc values estimated with 1-fold cross-validation on the 1 uci data sets described in table 1  obtained with
roccer  c1  c1 without pruning  cn1 unordered  cn1 ordered  i.e.  learning decision lists   ripper  slipper  and bagging all rules found by apriori. numbers between brackets indicate standard deviations; dark gray indicates significant wins over roccer  and light gray indicates significant losses against roccer.sights for improvements to our approach.
　we conclude from tables 1 and 1 that roccer combines  in a sense  the best of both worlds: it achieves auc values that are comparable to those of unpruned decision trees and cn1  but without the large number of rules induced by those systems. finally  table 1 presents statistics of the individual rules that comprise the rule sets  which demonstrates another advantage of the roccer approach. support ranges from 1 to 1% and is a measure of the relative coverage of each rule. weighted relative accuracy  wracc  ranges from 1 to 1 and assesses the significance of a rule  in terms of difference between the observed and expected numbers of true positives. the odds ratio ranges from 1 to ± and is a measure of strength of association. it can clearly be seen that the rules selected by roccer have considerably higher values for all measures. this means that the rules are more meaningful in isolation  without reference to the other rules in the rule set. thus  roccer successfully overcomes one of the main drawbacks of the set-covering approach.
support  % wraccodds ratioroccer1  1 1  1 1  1 c11  1 1  1 1  1 c1np1  1 1  1 1  1 cn1.1  1 1  1 1  1 cn1or1  1 1  1 1  1 ripper1  1 1  1 1  1 slipper1  1 1  1 1  1 all1  1 1  1 1  1 table 1: support  weighted relative accuracy and odds ratio averaged over all learned rules.
　a final word should be said regarding computational complexity. roccer is  of course  computationally more expensive than the other algorithms. in the worst case  the complexity is o n1   where n is the number of rules used as input. however  on average  the number of iterations is   mn   where m is the number of rules selected by the algorithm. due to lack of space we will not report runtime statistics for all data sets. for most datasets the runtime ranges from a few seconds to 1 minutes per fold  on a pentium 1.1ghz machine with 1mb of ram . for these datasets  the number of rules used as input is up to 1. for some domains  krvs-kp and satimage  the number of rules generated by apriori is very high  more than 1 . the runtime in these cases is on average nearly 1 hours per fold.
1	conclusion
we presented roccer  a rule selection algorithm based on roc analysis. roccer operates by selecting rules from a larger set of rules by maintaining a roc convex hull in the roc space. featuers of roccer's approach include implicit backtracking and discovery of pairs of related rules. experimental results demonstrate auc values that are compatible with the best probability predictors such as unpruned decision trees  achieved with considerably smaller rule sets. the rules that compose the rule sets induced by roccer have also higher values of support  weighted relative accuracy and odds ratio  and thus are more meaningful as individual rules.
acknowledgments
this work is partially supported by the brazilian research concil capes  process no. bex1-1   and was carried out while the first author was visiting the university of
bristol.
