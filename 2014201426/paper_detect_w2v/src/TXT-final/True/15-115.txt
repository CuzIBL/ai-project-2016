 
　　　we present here a learning technique which is both statistic and syntactic  by using simultaneously logical operators and counting procedures. its 
modular structure makes it usable for creating the necessary redundancy for controlling the generalization of the formulas. 
1. introduction 
　　　in data analysis  a learning problem is generally stated as  either a discrimination problem  
or a regression one. some inductive methods  use metric concepts. but most of the artificial intelligence methods  1  are purely syntactic  i.e. they use concepts of formal logic. 
　　　then a problem arises  which is the generalization one   i.e. how to apply the found logical rules to examples which are not in the training set. this problem is more and more studied  and interesting solutions have been found  based upon the idea 
of controlling the generalization  1 . 
　　　we describe in this work a learning technique which builds rules to describe a given training set. each of them can be used as an  opinion  about the training set. then a  rule storming  is perfor-
med to complete the generalization. 
1. outline of the method 
　　　we consider a set of objects  called the training set. this set is described by several  describers   which are questions  or binary variables  with a value for every object. 
　　　let us take one of these describers and call it the  variable to forecast . the aim of the method presented here is to find a combination of the others describers identical to the variable to forecast. 
　　　the method we propose consist on the iteration of an algorithm made of 1 modules : expansion  selection  compression. 
　　　expansion step consist on combining each couple of describers with a logical operator  in order to obtain a new set of describers. 
　　　selection consists on eliminating the describers which are not  similar  enough to the variable to forecast. 
　　　compression step then classifies the describers according to their similarities  and summarizes each class by one or a few  describer. 
we shall now describe each of these parts. 
1. expansion 
　　this step consist on combining each couple of describers. then  the operator must be associative  so that the combinations of 1 or more describers are nothing but successive 1 by 1 combinations. on the other hand  the permutation of 1 and 1   true  and  false   must not disturb the result of expansion. 
　　these constraints suggest two possible operators : the logical conjunction  and   and the logical equivalence  id . 
　　with the conjunction  1 describers are built for each couple of initial describers : 
a and b ;  non a  and b ; a and  non b  ;  non 
a  and  non b . 
　　after that  we have to check the consistency  i.e. if  d  is in on formula  then  non d  is not in another one. 
　　for the logical equivalence  we have the following properties : 
a id b =  non a  id  non b  ; 
 non a  id b = a id  non b  = non  a id b . 
　　according the remarks made at the beginning of this paragraph  it is only necessary to build  a id b . 
　　it is easy to check the fitting of these operators with the constraints described at the beginning of this paragraph. 
1. selection 
　　the aim of the selection step is to compare a describer to the variable to forecast  in order to select the describers which can be frucfully expanded again. the most natural way of comparison between 1 binary variables is to look at the list  n1 n1 n1 nil  of the co-occurence frequencies for the different values of the variables  i.e. of the describer to be selected and the variable to forecast. 
　　several criteria are then possible for the selection. we consider here two kinds : overlapping criteria and information theory criteria. 
　　an overlapping means that the describer has one value for at least a part of the objects from one 
class and the other value for at most a part of the objects of the other class. the corresponding thresholds are given by the user. 
       the information theory criteria are different of the previous one  in the sense that they are not used in the same way. they are information measure-
ments on the describers  which are then ordered according this measure  and the k best ones are selected  for a value k which is choosen by the user. 
       several criteria are possible. for instance  we can use the kullback's divergence  the mahalanobis distance or the contingency-khi 1 criterion. 
1. compression 
       this step consists on summarizing the set of the selected describers  regarding their inter correlations  which are measured by a given function. 
       then  we have to perform an automatic clustering of the describers into k groups. 
1 . 1 . optimization of a clustering 
       in this paragraph  we state the problem of optimizing  for a given criterion  a k-class clustering. as we previously noticed  we suppose that a distance has been chosen to measure the decorrelation between the describers. 
       then  the criterion to optimize is the sum of the distances of the describer i to the describers which are in the same class. let d i j  be the sum of distances between describer i and the describers belonging to class j. let j  i  be the class which contains describer i. let us state : 
w i  = d i j   i     - min  d i j  j=l k  
w io  = max  w i  i = l   . . .   
d io jo  = min  d io j  j=l k  
       then  the algorithm deletes io from its class and appends it to the class jo. 
       this procedure is repeated until the obtained classification is invariant  i.e. w i =1 for all the describers. this algorithm is actually a local optimization one  because we can easily proof that the criterion w i1  is descreasing at each step. 
       the i n i t i a l clustering may be randomly chosen  or given by the user. we shall now use the later possibility to define a strategy for compression. 
1. the compression algorithm 
       an interesting aspect of the previous algorithm is that it works even if one class is empty. this remark suggests a strategy for compression  which needs only to input the maximum number of classes. 
       the algorithm starts with a one-class clustering. obviously  the value of the criterion is zero in this case. 
       when the best  k-l -class clustering has been found by the algorithm we described in the 
	j. quinqueton and j. sallantin 	1 
previous paragraph  an empty class is created and 
we look for the best k-class classification. 
　　the algorithm stops when k is the given maximum value nmc. we can notice that  if the sum of intraclass distances is zero for k   nmc  then the  nmc-k  other classes w i l l remain empty. 
     once we have obtained the desired classificatior each class is summarized by one  or a small number  of its elements. we shall describe this point in the following paragraph. 
1. summary of a compression 
     the summary of a compression must depend upon the chosen distance. the purpose of the distance is to compare 1 describers. then  it w i l l be defined with the l i s t of co-occurences of values of the describers  n1 n1 n1 n1 . 
in the case of the equivalence distance min 
 n1+n1 n1+n1   we can choose any element of each cluster  because they are supposed to be logically equivalent  except for a few objects. 
     in the case of the comparability distance min  n1 n1 n1 n1   the characteristic of the elements of the same cluster is to be comparable to each other  except for a small number of objects. this kind of relationship may be summarized by ordering the describers  and then using a dichotomic decision tree to compress. 
     then  this kind of compression can be viewed as an  unfolding   dimensionality reduction  of the training set  and we studied it in some previous works   1   . 
1. end criterion 
　　as the aim is to find a formula which i s   on the training set  logically comparable to the variable to forecast  there are several ways of stopping conveniently the algorithm : 
- maximum number of iterations   i t is careful  
- emptiness of the describers l i s t   i t may happen after a bad choice of the selection parameters  
- one  or several of the built describers is sufficiently correlated to the variable to forecast. 
     these criteria are applied in the previous order. we can add some more  in order to detect the case when it is useless to continue. for instance  if the describers l i s t remains identical after a new expansion  selection and compression   stability  c r i t e r i o n     it is clear that further iterations w i l l give the same result. 
1. generalization by  rule storming  
     the generalization consists on decision making outside the training set. then  the logical rules built with the previous algorithm can be considered as  opinions  about the training set  each onebeing related to the choice of a particular describer as variable to forecast. 
1 j. quinqueton and j. sallantin 
       the generalization itself consist on performing a  rule storming  on these opinions. let us summarize this idea. according michalski   a generalization is a filter  in the topological sense   on the space of objects. 
let w be an object and 1p i  the i-th opinion 
then  1p i w  is the new object produced by applying the rule 1p i  to w. several cases are possible : 
1p i w  = w  at least on the training set . 1p i w  = w  then op  i w'  = 1p i w  1p i w  = 1  the rule is not applicable . 
then  the filter is : 
v i  = {w} u  u 1p i w  v p  = {w} u  u 1p i v p-l   . 
       the rule storming is then made by a vote at a given level of this filter. 
       an advantage of this technique is that we actually build a topology  then we need not a discrimination problem to generalize. 
1. results and discussion 
       this technique has been tested on real problems : learning of animal behavior  control problems on nuclear plants  forecasting earthquakes  learning meta-rules for expert systems  decision 
making in psychology. 
       in all these applications  only a few  1 to 1  iterations of the expansion-selection-compression were necessary to find the rules. for the generalization  the good decision was made at level v l  or v 1   but never more. 
1. conclusion 
       the algorithm that we presented in this paper in a first draft of a tool for learning problems. we work now on its enhancement and integration in a complete learning system. we think that the presented results show reasonable efficiency and computing costs. the theoretical background can be found in the field of non classical logic   more precisely non distributive logics. 
