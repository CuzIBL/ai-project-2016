 
sage.1 is a production system that improves its search strategies with practice. the program incorporates four different heuristics for assigning credit and blame  and employs a discrimination process to direct its search through the space of move-proposing rules. the system has shown its generality by learning search heuristics in five different task domains. in addition to improving its search behavior on practice problems  sage.1 was able to transfer its expertise to scaled-up versions of a task  and in one case transferred its acquired search strategy to problems with different initial and goal states. 
introduction 
　the ability to search is central to intelligence  and the ability to direct search down profitable paths distinguishes the expert from the novice. since all experts begin as novices  the transition from one to the other should hold great interest for artificial intelligence. in this paper we examine sage.1. a program that implements one approach to learning effective search heuristics. below we present an overview of this system and summarize its operation in various domains. however  before turning to these matters  let us review the nature of the strategy learning task by discussing its components. 
　within any system that improves its strategies as a function of experience  we can identify three distinct components. first  such a system must be able to search  so that it can generate behaviors upon which to base its learning. second  a strategy learning system must be able to assign credit and blame to components of the system responsible for good and bad behaviors. one approach  1  1  involves finding a solution to a problem  and then using the solution path to distinguish good moves from bad moves. an alternative approach  explored by anzai  and ohlsson   involves assigning credit while the search is in progress. finally  a strategy learning program must be able to use credit information to modify its performance component so that behavior improves over time. such modification often involves determining the conditions under which various operators should be applied. given good and bad examples of these operators  heuristically useful conditions can be determined in three possible ways: through a process of generalization; through mitchell's  version space method; or  as we shall see shortly  through a process of discrimination. now that we have reviewed the components involved in learning search heuristics  let us examine how these components are implemented in the present system. 
　this research was supported by contract n1k-1 from the office of naval research. i would like to thank stephanie sage  steve smith  and john laird for comments on an earlier version of this paper. 
an overview of sage.1 
　like most other strategy learning programs  sage.1 is stated as a production system. in other words  it is cast as a set of relatively independent condition-action rules or productions  and learning occurs through the addition of new productions. below wo discuss the system's search process  its credit assignment heuristics  and its mechanisms for altering its search strategy in the light of experience. 
the search process 
　sage.1 represents states as elements in working memory and operators as productions that match against these states. each production has an associated weight. move proposing rules with the same weight can be applied in parallel  so more than one move may be proposed at a time. initially  these rules contain only the legal conditions for applying the associated operators. thus  the system carries out a breadth first search until credit can be assigned and improved move-proposing rules can be learned to direct the search process. 
when a new rule is first created  it is assigned a low weight. 
since a rule's weight is increased every time it is relearned  this number can be viewed as a measure of each rule's success  with preference being given to more successful rules. search becomes much more selective as sage.1 begins to prefer productions that have been learned many times  and to shun those that have led to errors in the past. however  it retains the ability to consider multiple paths  as long as those paths are proposed by rules with the same weights. 
assigning credit and blame 
　sage.1 can operate in two modes: it can assign credit based only on a complete solution path  or it can do so during the search process. in the first method  exhaustive breadth-first search continues until one or more solution paths are found. the system then marks all moves lying on these paths as good instances of the rules that proposed them  and labels all moves leading off the paths as bad instances of the responsible rules. 
　the second method does not require knowledge of complete solution paths  relying instead on several rules for recognizing undesirable moves. one of these notes when the system reaches some state that was visited earlier; this rule detects loops as well as unnecessarily long paths. another applies when a state is found from which no moves can be made  a deadend . these rules are relatively domain-independent  but less general rules can be employed as well. for instance  one can write problemspecific rules for recognizing when an illegal state has been reached. when any of these rules labels a move as undesirable  sage checks to see if it has made any move from the same state that is not labeled as bad. if such a move exists  and if both moves were proposed by the same rule  sage passes them to the learning component as good and bad instances of that rule; if not  then the learning component cannot be applied. 
   
1 p. langley 
learning conditions through discrimination 
   as good and bad instances of the move-proposing rules are identified  they are passed to sage's discrimination process. this mechanism searches for differences between what bundy and silver  have called the selection context  the state of memory during the good application  and the refection context  the state during the bad application . differences take the form of sets containing one or more working memory elements that were present in one context but not the other. for each difference that it finds  sage creates variants of the overly general rule by including these differences  with certain terms replaced by variables  as one or more extra positive or negated conditions on that rule. thus  the system begins with overly general rules  and gradually learns more specific versions with heunstically useful conditions that direct search down the desired paths. many spurious variants are created along with useful ones  but since useful rules are relearned more often  their weights are increased and they come to be preferred. 
   the discrimination method has two important advantages over other methods. generalization based systems  including the version space technique  find conditions that are held in common by all positive instances  and so are biased toward learning conjunctive rules. the approach can be extended to handle disjuncts. but this involves expensive backtracking through the rule space. in contrast  the discrimination method compares a single good instance to a single bad instance  enabling it to discover disjunctive rules as easily as conjunctive ones in addition  the discrimination method employs the weighting method to direct search through the space of rules. thus  sage learns more slowly than generalization-based systems  since it must gather statistics on the usefulness of competing variants. however  this approach makes the system quite robust with respect to noise  so that if an occasional error in made during credit assignment  the system will still be able to learn useful rules. 
examples of sage.1 at work 
   our overview of sage.1 is now complete  but to gain a better understanding of how the system learns search strategies  we must examine its workings in specific domains. below we discuss the program's learning sequence on the tower of hanoi puzzle  both when it uses only complete solution paths to assign credit and when it learns during the search process. we also summarize the programs behavior in four other task domains. 
sage.1 on the tower of hanoi puzzle 
   in the tower of hanoi puzzle  n disks of decreasing size are placed on one of three pegs. the goal is to move all of them to one of the other two pegs. disks are moved one at a time  with two constraints. first  only the smallest disk on a peg can be moved. second  a disk cannot be moved to a peg on which a smaller one resides. these constraints limit the set of legal moves  so that for the three-disk puzzle  only 1 states are possible. however  the number of connections between these states is high  making for a challenging problem. 
   using the complete solution path heuristic for credit assignment  sage learns only after a solution is reached. on its first pass through the problem space  sage.1 made 1 moves in a breadth-first search before it found a solution. at this point  the system examined the two  symmetrical  solution paths  assigning credit and applying the discrimination mechanism. through this process  five variants on the original operator  let us call it move  were created. one variant. move-1  was built each time the system looped back to a previous state inn the search tree. this rule contained a condition that prevented it from undoing a move that the system had just made. other variants with useful conditions were learned from other errors. however  move-1 emerged as the strongest contender  since the looping error that led to its creation was more frequent than other errors. 
   on the second run  the system's performance improved considerably  since move 1's weight had come to exceed that of move. in this case  1 moves were made before the solutions were reached. move 1 prevented backing up  but other errors still occurred  leading to the construction of three variants on this rule. of these  move 1  which included a condition to prevent moving the same disk twice in a row  was learned most often. thus  on the third run  the system neither made looping errors  nor moved the same disk twice in a row. the few remaining errors led to the creation of move-1  which prevented moving a disk back to the position it had occupied two moves earlier. this heuristic was sufficient to eliminate search on the tower of hanoi task  and when the problem was presented a fourth time  sage reached the solutions without taking any false steps. after mastering this simple version of the problem  the system was able to solve the scaled-up four-disk task as well. however  sage.1 is not at present able to transfer its learning to versions of tower of hanoi with different initial and goal states. 
   in learning while doing  sage followed a very similar learning sequence. during the initial breadth-first search  the revisited state heuristic noted a number of loops in the search tree  and labeled the responsible moves as undesirable. as before  these led to the creation of the variant move-1  which proposes only non looping moves. in later runs  the variants move 1  based on unnecessarily long paths  and move-1  based on dead-ends  were created  with the latter eventually eliminating search. 
applying sage.1 to other domains 
   one important dimension on which al systems are judged is their generality  and the most obvious test of a program's generality is to apply it to a number of different domains. accordingly  we have tested sage in four additional task domains  which we discuss below. 
   in the slide-jump puzzle  one is presented with n quarters and n nickels placed in a row and separated by a blank space. legal moves include sliding into a blank space or jumping over one other coin into a blank space  while the goal is to exchange the positions of the quarters and nickels. sage.1 was given two initial move-proposing rules - one for suggesting slide moves and the other for suggesting jumps. the system learned a search heuristic that proposed sliding a coin into a blank space only if another coin of the same type had just been jumped from that space. in addition  this rule never exceeded the original jump rule in weight  so jumps were made whenever possible. in the learning while doing runs  the system proceeded in a very similar manner  learning from both revisited states and dead ends. 
   ohlsson  has described the tiles and squares puzzle  which involves n numbered tiles and n + 1 squares on which they are placed. only one legal move is possible: moving a tile from its current position to the blank square. the goal is simple: get all of the tiles from their initial positions to some explicitly specified goal positions. on this task  sage.1 acquired two simple heuristics for directing search. the first of these states that if possible  one should move a tile into its goal position; the 
   
other states that one should never move a tile out of its goal position once it has been placed there. note that these rules are disjunctive; neither heuristic is sufficient to completely direct the search process by itself  but taken together they eliminate search. thus  the ability of the discrimination process to consider disjunctive heuristics shows its potential in this puzzle. another interesting characteristic of this problem is that sage.1 incorporated information about the goal state in the conditions it discovered. as a result  the heuristics the system learned could be applied not only to more complex problems  but to problems with differing initial and goal states. in learning while doing on this task  sage learned from both loops and unnecessarily long paths. 
   in the mattress factory puzzle  the goal is to fire all n employees at a factory  using two operators. the least senior worker may be hired or fired at any time. however  other workers may only be hired or fired if the person directly below them in seniority is currently employed  and provided that no other person below them is also employed. sage.1 was given rules for proposing both types of moves. after finding the single solution path  it arrived at variants of both rules that avoided simple loops. in addition  the variant of the first rule achieved a greater weight  so that it was preferred. in learning while doing  the system learned mainly from loops in its search tree  though one dead end also occurred. 
   in piaget's length seriation task  the child is presented with a set of blocks in a pile  and is asked to line them up in order of ascending height. for this problem  sage.1 was given a single operator for moving a block from the pile to the end of the current line. also  the program was given a domain-specific rule for determining illegal states. this stated that if a taller block had been set to the right of a shorter block  the resulting state was undesirable. in learning from complete solution paths  the system generated one useful heuristic  which proposed moving a block only if there was no other block in the pile that was taller than that piece. in learning while doing  both the rule for noting illegal states and the dead-end heuristic came into play  generating the same variant move-proposing rule. 
generality of the heuristics 
   as we have seen  sage.1 has learned rules for directing search in five different domains  suggesting that the system is a 
   relatively general one. however  each of the learning heuristics can be examined on this dimension as well. since the discrimination strategy played a central role in each of the runs described above  we can safely infer the generality of this method. however  the situation with respect to the credit assignment heuristics is somewhat more complex. the complete solution path heuristic is very general  and was applied on each of the tasks. the other heuristics were less useful  but still showed evidence of generality. both the revisited state rule and the dead-end rule led to learning in four of the five domains. the illegal state detector was stated in a domain-specific manner and was used only in the seriation task. however  one can imagine versions of the tower of hanoi  mattress factory  and slidejump puzzles in which the conditions for legal moves must be learned along with the conditions for good moves. thus  we can conclude that sage's various learning heuristics are general ones  and should prove useful in domains other than those examined here. 
	p. langley 	1 
conclusions 
   before closing  let us consider sage.1's relation to other systems  and whether it has advanced our understanding of the strategy acquisition process. the system shares anzai's  revisited state heuristic for assigning credit  but it is considerably more general than the earlier system  which was tested in a single domain. like brazdil's elm  and mitchell  utgoff  nudel  and banerji's lex   our system can also assign credit based on complete solution paths. however  elm and lex had access to only this heuristic  while sage.1 incorporates a number of interacting credit assignment methods. also  these systems focused on transfer between problems of similar complexity  while sage is also able to transfer to scaled-up problems. the discrimination technique used by sage bears a strong resemblance to elm's learning method  but sage has shown how this approach can be applied to learn disjunctive heuristics. in summary  sage has addressed a number of issues that have been overlooked in the earlier work. 
   although progress has been made  our understanding of the strategy learning process is far from complete. for example  sage.1 was not in general able to transfer its acquired expertise to problems with different initial and goal states from those on which it practiced. however  the system was able to make such a 
   transfer on the tiles and squares puzzle by explicitly representing the conditions for goal satisfaction in working memory. hopefully  by augmenting sage's representation for other tasks  it will be able to make similar transfers. a second extension should enable the system to learn in much more complex domains such as chess. if sage.1s search control were altered to allow the setting of subgoals  then the heuristic for assigning credit based on complete solution paths could be applied whenever a particular subgoal had been achieved. variants learned from this path would be specific to that subgoal; that is  they would include a description of the current subgoal as an extra condition  in addition to the other conditions found through discrimination. thus  while more research remains to be done  the results we have obtained so far are encouraging  indicating that effective search strategies can indeed be learned using simple and general mechanisms. 
