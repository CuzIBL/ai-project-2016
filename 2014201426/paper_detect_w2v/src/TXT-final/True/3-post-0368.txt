ed as a task graph  with specification of computation  storage and communication requirements. more in detail  the worst case execution time  wcet  is specified for each task and plays a critical role whenever application real time constraints  expressed here in terms of minimum required throughput  are to be met. in fact  tasks are scheduled on each processor based on a time-wheel. the sum of the wcets of the tasks for one iteration of the time wheel must not exceed time period rt  i.e.  the minimum task scheduling period ensuring that throughput constraints are met   which is the same for each processor since the minimum throughput is an application  not single processor  requirement.
　the problem we are facing is a scheduling problem with alternative resources. each task should be allocated to one of the processors  node i in figure 1 . each task also needs 1 memory areas for executing  that should be allocated to a memory device: internal task state and program data can be allocated either on the local scratchpad memory or on the remote on-chip memory  while communication queue  the memory area used by the tasks to communicate one other  must be allocated on the local scratchpad. tasks duration depends on where memory slots are allocated; tasks need time to access the bus and use remote memory. clearly  tasks should be scheduled in time subject to real time constraints  precedence constraints  and capacity constraints on all unary and cumulative resources. however  on a different perspective  the problem decomposes into two problems: the allocation of tasks to processors and the memory slots required by each task to the proper memory device; a scheduling problem with static resource allocation.
　the objective function of the overall problem is the minimization of communication cost. this function involves only variables of the first problem. in particular  we have a communication cost each time two communicating tasks are allocated on different processors  and each time a memory slot is allocated on a remote memory device. once the communication cost has been minimized  among feasible schedules we prefer those having minimum makespan.
　the allocation problem is difficult to solve with constraint programming  cp . cp has a naive method for solving optimization problems: each time a solution is found  an additional constraint is added stating that each successive solution should be better than the best one found so far. if the objective function is strongly linked to decision variables  cp can be effective  otherwise it is hopeless to use cp to find the optimal solution. in case the objective function is related to a single variable  like for makespan in scheduling problems  cp works quite well. however  if the objective function is a sum of cost variables  cp is able to prune only few values  deep in the search tree since the connection between the objective function and the problem decision variables is weak. if the objective function relates to pairs of assignments the situation is even worse. this is the case of our application where the objective function depends on pairs of assignments. in fact  a contribution to the objective function is added when two communication tasks are allocated to different processors.
　on the contrary  integer programming  ip  is extremely good to cope with these problems  while is weak in coping with time. scheduling problems require to assign tasks to time slots  and each slot should be represented by an integer variable  and the number of variables increases enormously. cp  instead  is very effective to cope with time constraints.
　therefore  the first problem could be solved with ip effectively  while for the second cp is the technique of choice. the question is now: how do these problems interact 
　we solve them separately  the allocation problem first  called master problem   and the scheduling problem  called subproblem  later. the master is solved to optimality and its solution passed to the subproblem solver. if the solution is feasible  then the overall problem is solved to optimality. if  instead  the master solution cannot be completed by the subproblem solver  a no-good is generated and added to the model of the master problem  roughly stating that the solution passed should not be recomputed again  it becomes infeasible   and a new optimal solution is found for the master problem respecting the  set of  no-good s  generated so far. being the allocation problem solver an ip solver  the no-good has the form of a linear constraint.
　a similar method is known in operations research as benders decomposition  benders  1   where the overall problem can be decomposed in two parts connected by some variables. some related approaches are  hooker  1  and  grossmann and jain  1 .
　we show that this method is extremely effective if compared to the approaches considering the problem as a whole.
　the methodology proposed in this paper has been applied to a video signal processing pipeline  wherein each task processes output data of the preceding task in the pipeline. functional pipelining is widely used in the domain of multimedia applications. task parameters have been derived from a real video graphics pipeline processing pixels of a digital image. the proposed allocation and scheduling techniques can be easily extended to all applications using pipelining as workload allocation policy  and aim at providing system designers with an automated methodology to come up with effective solutions and cut down on design time. to do that  we schedule several repetitions of the pipeline processes in order to achieve a working rate configuration.
　to validate the strength of our approach  we now compare the results obtained using this model  hybrid in the following  with results obtained using only a cp or ip model to solve the overall problem. actually  since the first experiments showed that both cp and ip were not able to find a solution  except for the easiest instances  within 1 minutes  we simplified these models removing some variables and constraints. in cp  we fixed the activities execution time not considering the execution time variability due to remote memory accesses; in ip  we do not consider all the variables and constraints involving the bus: we do not model the bus resource and we therefore suppose that each activity can access data whenever it is necessary.
　we generate a large variety of problems  varying both the number of tasks and processors. all the results presented are the mean over a set of 1 problems for each task or processor number. all problems considered have a solution. experiments were performed on a 1ghz pentium 1 with 1 mb ram. we used ilog cplex 1 and ilog solver 1 as modelling and solving tools.
hybrid
ipcp	1	1	1	1
number of tasks
figure 1: comparison between algorithms search times for different process number.number of tasks. times are expressed in　in figure 1 we compare the algorithms search time for problems with a different
seconds and
the y-axis has a logarithmic scale. for space reasons we omit the search time figure varying the number of processors  but it has very similar behaviours to figure 1.
　although cp and ip deal with a simpler problem model  we can see that these algorithms are in general not comparable with hybrid and  as the number of tasks grows  ip and cp performances worsen and their search times become orders of magnitude higher w.r.t. hybrid. furthermore  we considered in the figures only instances where the algorithms are able to find the optimal solution within 1 minutes  and  for problems with 1 tasks or more  ip and cp can find the solution only in the 1% or less of the cases.
