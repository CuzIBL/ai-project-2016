 
real-time monitoring calls for decision making capabilities in reaction to observed events. associative models provide efficiency by matching the observed situation to a recorded pattern equipped with an accurate decision. we rely on a decision tree accounting for the context and temporal chronicles expressing dynamic patterns. in highly reactive domains  i.e. when actions get as frequent as observations  the decision must anticipate the complete recognition of a pattern  comparing possible evolutions. this paper focuses on the on-line decision process  a game against nature in the general case: a timed game automaton gathers the possible next steps with associated goodness values  and uses an opportunistic algorithm to compute a temporally expressive decision  maximizing its utility  i.e. the chances of  winning . 
1 introduction 
monitoring and supervision deal with dynamic systems  or artefacts   that evolve across time. a  human or automated  agent is in charge of observing  through sensors  what happens  recognizing the typical behaviour  and acting towards the artefact  through activators   for instance to put it back into a normal state  or to process some safety procedure. this on-line process needs realtime efficiency  and is reactive in the sense that actions 
are taken according to observations. 
　a model-based approach  dvorak and kuipcrs  1  has been introduced to deal with this problem  first computing off-line faulty models of the artefact that are used on-line as associative models matched with incoming observations. next steps are predicted from the hypothetical current situation  and incoming observations are checked to confirm the hypothesis. but time is only implicit  through successive states in a dynamic decision tree  which is not expressive enough in realistic domains. some approaches  nejdl and gamper  1  manage to attach a temporal qualification to behavioral modes  using a rich set of temporal relations. more appealing are temporal chronicle recognition systems  dousson et a/.  
1 	planning and scheduling thierry vidal 
lgp  enit 
1 avenue d'azereix 
f-1 tarbes cedex  france 
1   where temporal constraint networks are used as associative models. 
　but all these approaches deal with systems in which an action is the consequence of a complete and fully recognized sequence of observed events. this paper deals with an explicit temporal framework in highly reactive domains  in which actions and observations continuously respond to each other. we extend the temporal chronicle formalism to mix events representing observations from the artefact and the agent actions  and equip them with goodness values expressing preferences among them. we also use a decision tree such as the one designed in  coradeschi et a/.  1   but here only as a preprocessing step before decision making  to merely branch the current static context onto a subset of relevant chronicles  therefore restricting the number of candidate patterns. 
　the agent has now a larger choice of decisions to take at any time. the goal is not to recognize a bad situation and put the system back into a good one  but to continuously take decisions that anticipate bad situations and push the system into better ones  which is basically a game playing process. for that we rely on the timed game automaton model  asarin et a/.  1  that is best suited for continuously changing systems. next transitions are computed from the set of candidate patterns  inheriting corresponding goodness values. then our new algorithm play-automat chooses a transition to select and a correct time to do so  using a least-commitment and opportunistic strategy  and following a formal decision policy based on a definition of the utility of each possible decision: we prove that our process is optimal in that it always takes the decision with highest utility. section 1 recalls the basic models and global architecture described in  coradeschi and vidal  1 . our approach is illustrated through a specific example in the area of one-to-one aircraft combat in section 1. then section 1 focuses on the very contribution of this paper  namely the new algorithm for on-line decision making. 
1 basic models and global process 
1 the basic decision-tree approach 
in  coradeschi et o/.  1   the agent is equipped with a context  i.e. a set of propositions describing the current 

state of the world   arid a decision tree. to each leaf of the tree is attached a decision  atomic action or sequence of actions . at each step of the simulation process the context is updated with new events received and interpreted. then the decision tree is visited down  propositions in the context being matched with the conditions appearing at each node  until a leaf is reached. each action has a priority value that changes dynamically  so that in the cases where multiple leaves are applicable and actions are mutually exclusive  the best one is selected. it is then sent to the simulator  which will update the context  and so forth. 
　this mechanism is highly reactive and efficient  and agent behaviours are easy to specify and test. it is however difficult to code in it reactions to sequences of temporally related events. 
1 possible evolutions as chronicles 
in the temporal system ixtet  dousson et a/.  1  used for dynamic situation assessment  temporal evolutions are taken into account in the shape of chronicles  that are temporal constraint networks  schwalb 
and dechter  1  on which classical constraint propagation techniques can be run: time-points represent instantaneous changes or begin/end points of intervals of time over which a fluent is true  and constraints between them are precedences labelled by arithmetic intervals of possible values  allowing to express dates of events and durations of fluents. then chronicles are matched with incoming events  dynamically maintaining the set of candidates. as soon as a chronicle is fully recognized  an action written in the chronicle description is triggered. 
　this model is well-suited for dynamic applications with temporally expressive behaviours like nuclear plant or gas turbine monitoring  milne et a/.  1   where supervision is the key word. as an associative model  it provides high on-line efficiency; anyway  two shortcomings compelled us to improve it somehow. 
  reactivity can be considered as being weak in ix-tet  since an action only follows an ordered set of observations. therefore we chose to extend the initial formalism to mix events representing both observations from the artefact and the agent actions  which in turn enforces a corresponding distinction between two types of constraints between events  as in  vidal and fargier  1 . a numerical constraint between e1 and e1  with e1 before e1  will be said to be controllable iff e-1 is an action  and contingent iff e-1 is an observation. 
  the second restriction is in the classical strict dis-tinction between normal and abnormal behaviours: the agent here has a large choice of actions at any time that can push the system into various  more or less good  situations. therefore we chose to extend the ixtet approach by adding to each chronicle a goodness value in the range  -1   -1 meaning the worst possible case  e.g. breakdown  at the chronicle completion  1 a behaviour that fully entails the system specifications  1 a situation that keeps balanced between eventual failure or success. 
1 timed game automata 
the model we present here is inspired by recent advances 
 asarin et a/.  1  on timed automata models  alur and dill  1   used for describing the dynamic behaviour of a system. it consists in equipping a finite-state automaton with time  allowing to consider cases in which a system can remain in a state during some time t before taking the next transition. this is made by adding continuous variables called clocks that are reset when some transitions are taken  then grow uniformly until they are checked on a later transition through some condition  guard  that must be true for enabling it. 
　such tools are well-suited for continuous real-time games: for each player  transitions are either activated or received  and some states are designated as wimiing ones. that extends the discrete game approach  pearl  1   with the following pros:  1  there are no  turns  and the adversary need not wait for the player's next move  and  1  each player not only chooses a transition  but also the delay to wait before taking it. 
　this is especially relevant for controlling reactive systems in which one has to  play against nature   the goal being to synthesize a  safe  controller  i.e. add conditions to compel the automaton to reach winning states for the agent. for our purpose we only need a restricted model: we do not need to build a complete automaton but we just compute the next states  transferring the goodness values from corresponding chronicles so as to compare them and choose the  best  one. that leads to the following restricted game automata definition. 

1 	global architecture 
we have already presented  coradeschi and vidal  1  our associative model mixing a decision tree and chronicles  simply replacing actions in leaves by sets of chronicles  as a simple way to add context handling to the chronicle model. then our global process will be to  1  determine the set of active candidate chronicles visiting down the decision tree   1  build the corresponding automaton  extracting from the chronicles the possible 
	coradeschi and vidal 	1 

next states and transitions to them  and  1  select the best decision to make now by  playing  the automaton. this is illustrated through figure 1. 

figure 1: the new proposed architecture 
　the global algorithm and its two first steps will not be reported here  see  coradeschi and vidal  1    but the following example should give the reader an insight into it. then we will focus on the decision-making step  1   namely the algorithm pl ay-automat. 
1 	an example in the air-combat domain 
the air-combat domain is a highly reactive domain where decisions are made under real-time constraints. an automated pilot should be able to predict his opponent's next moves and select the action that minimizes possible threats and maximizes chances of success. predicting other agents behaviours is a hard task but some typical patterns have been developed by the military to help in identifying manoeuvres of the opponents. 
　we have built a simplified example of a one-to-one beyond visual range combat situation  the aircrafts can see each other just with board instrumentation . figure 1 shows the chronicles corresponding to most plausible typical patterns  where events labelled with a are the automated pilot own actions and the ones with b are the opponent observed actions. we start with a and b flying towards each other  and a sees b moving right. two possible guesses for a arc that b will continue escaping or turn back for an intercept. then a can move left  evolution q1   making b moving left as well. the resulting situation gives no special advantage for any of the pilots. otherwise a may accelerate  then both pilots turn left for an intercept  q1   which would put a into a better situation as he can more easily attack on the side  or he may observe b escaping by moving right  q1   which is even better. q1 is also a good situation as a does not do anything and b escapes. but not doing anything might as well be bad  if b moves left to 
1 	planning and scheduling 
intercept  q1 . precise delays  here in seconds  are also 
added to the constraints. 

figure 1: the temporal chronicles of the example 
　then figure 1 shows the result of step  1   the algorithm build-automat. one should notice the correspondence between durations of controllable/contingent constraints in chronicles and guards on activated/received transitions in the automaton. receiving the event bmr puts the pilot into a state in which there are four posible next events  given by the five chronicle candidates of figure 1. for each one a new state is added together with the transition to this state  labeled with  computing the guard from the corresponding chronicle. goodness values are transferred as well  with here the case where an event  belongs to two different chronicles  so one keeps the min of the goodness values. 

figure 1: the game automaton of the example 
1 on-line decision making 
1 decision policy: basic definitions 
before presenting the algorithm that computes an accurate decision from the built automaton  wes first define 

our decision policy  in the spirit of studies carried on in qualitative decision theory  see e.g.  boutilier  1  . goodness values provide preferences over consequences of the decisions  but a decision might raise distinct consequences. in this uncertain framework the utility of a decision must be defined as a function over the utilities 
 the goodness values  of all the possible consequences. for that we rely on a choice criterion similar to the classical pessimistic/cautious wald criterion  that suits well our illustrative application area. of course in other domains less strict criteria balancing between the risk and the expected outcome may be preferred. 
definition 1 the utility of a decision  is the lowest utility of all its possible consequences  i.e. the lowest goodness value of all the possible states in which the system might get after the decision  is taken. 
　the decision strategy amounts to choose the decision with highest utility  which is consistent with min-max game strategies  pearl  1 . we will now give our algorithm and prove that it obeys the given criterion  but before that we need some additional definitions. 
definition 1 a selected transition r is said to be ensured iff the system cannot take another one. on the contrary  r is said to be threatened by r'  the threat   if the system might take r' instead of r  obviously t ' can only be a received transition . we use the same terms for the corresponding decision s of selecting the transition t. 
1 	the game-based decision procedure 
our procedure has been inspired by classical controller synthesis algorithms  asarin et a/.  1   but highly simplified in our case of  one step ahead  automata with goodness values. one needs first sorting the next transitions starting from the one with highest goodness value  and then consider them one by one. 
　1  if the current best transition r is a received one  the decision is to wait for its labelling event to occur. we watch out if there is a worse received transition r' possibly threatening r by occurring meanwhile  test 
. ensuring r amounts 
to deciding to wait only while t' cannot occur: r can hence be ensured iff lower   .if not  this selection is forgotten and one tries next item in 
ranked  goto 1 . otherwise the waiting delay is set to 
  and a new transition is added  labelled by the special event wait  that will be taken for sure just before the delay has elapsed  the guard therefore exactly equals delay   which means the atomic duration immediately lower than delay . then there are two possibilities; in the worst case  nothing happens  the transition wait is taken after delay  has elapsed  and a new automaton accounting for this elapsed time is recomputed in the next stage of the global algorithm; or in the lucky case where the event labelling r occurs before the delay has elapsed  then r is taken. 
　the algorithm has been slightly simplified for the sake of clarity: it should take into account cases with several threats: the delay to wait is actually the min of the corresponding delays. 
　last  when there is no threat r' the decision is just to wait for r to be taken  but actually no more than maxdelay  which corresponds to the time after which a better activated transition that had been put aside may become releasable  which is explained herebelow. 
　1  in the case where r is an activated transition  things get simpler  but r still needs to be ensured. if the action can be released now  lower g r   = 1   then it is released  is the action label of r . if not  the test about a possible threat is now on the value lower  g r   instead of upper   r being an activated transition it can be released as soon as lower g r   has elapsed . here again  if r cannot be ensured  then this selection is put aside and the next one tried. but in the case where the decision ends up being wait  as soon as the action a r  becomes releasable  it will not be threatened anymore  lower  g r   equalling 1   and it should be reconsidered as a possible decision. for that we use maxdelay that keeps track of the min of the times after which an action becomes releasable  so as to restart the algorithm at that time. 

　this algorithm is illustrated through the example of figure 1. in the first iteration  q1 is the best transition  maximal goodness value 1   but it is threatened by the transition to . ensuring the selected transition amounts to wait no longer than 1. the new wait transition added to the automaton appears in figure 1. we show there a possible improvement of the algorithm  not developped here for the sake of clarity  where one not only adds this transition  but at the same stage computes the next states from  which should be the same as 
	coradeschi and vidal 	1 

in q1 recomputed guards according to the delay. the two activated transitions from qo are in dashed lines because they will not be taken at this stage  trying to opportunistically let occur first. next  falling into qwait} the choice would be forgotten since it can no longer be ensured  and the next best transition aacc would be selected since it can be released at once. this improvement  which amounts to consider two successive decision steps in one stage  would end up with an expressive decision sounding like wait no longer than one 
second  then accelerate  which of course could be cancelled by the early fortunate occurrence of 

figure 1: the wait policy 
1 	decision making algorithm properties 
our algorithm is a least commitment strategy: one always prefers waiting if this may lead to a better situation and is not threatened by a. worse one. it is as well an opportunistic process: actions that were put aside be-
cause they could not be ensured are considered again when they become releasable  and waiting instead of releasing a  rather good  action may allow to receive a better event  such as in our example. last  our algorithm is definite in the sense that a decision will always be taken  provided the automaton is not empty : even a wait decision ensures the system evolution either when a received event occurs  or when maxdelay has elapsed and an action can then be released. 
　about algorithmic complexity  it gets low first thanks to our associative models  see  coradeschi et ft/.  1  and  dousson et al.  1    but also in the algorithm play-automat: the automaton is bounded in depth  one never computes a complete automaton but only needs to consider the next step thanks to the goodness values   and also in breadth  the number of alternative transitions to consider is lower than the maximum number of chronicles in a given context  i.e. in a leaf of the decision tree . in fact the global complexity only depends upon the size of the decision tree and the maximum number of chronicles in each leaf  i.e. the size of the expertise knowledge base  which is constant and known. 
1 	planning and scheduling 
　the key point is to prove the optimality  i.e. that one always gets the best decision according to the given policy  of course the decision taken will only be the best according to the given goodness values  hence it highly depends on how accurate and realistic the associative model built off-line is . for that purpose  we check that in each case the decision with highest utility is taken. 
1. if the transition r with highest goodness value  is an activated one  the decision s will be an action : 
 a  if s is not threatened: the action will necessarily be released and has the highest utility. 
 b  if s is threatened by leading to a state with goodness value : then 
i. if there exists   not threatened leading to a state with goodness value the algorithm will take the decision with 

ii. if there exists s threatened: the reasoning process can be recursively applied as in l. b . 
iii. if no other satisfactory decision is found: the algorithm only waits until some action becomes releasable  thanks to maxdelay   hence all the other possibly better decisions remain available at the next stage. 
1. if the transition r with highest goodness value  is a received one  the decision s will be to wait : 
 a  if  is not threatened: it is ensured hence the corresponding state will be reached  and  has the highest utility. 
 b  if  is threatened by an event that may occur after a certain delay: the system will wait for this delay  in an opportunistic way  which might hopefully lead the system into the state reached by r  which means highest utility   or put it into the following situation  c . 
 c  if  is threatened by an event that may occur immediately: s has the same utility as the threat  and hence is not better and might be forgotten  as the algorithm does. 
1 	discussion and conclusion 
a main strength of our approach is to use different models that are best suited for each of the requirements of highly reactive monitoring: a decision-tree for the context  chronicles for the temporal evolutions  and a game automaton for the predictive decision making process. it is straightforward to integrate those models and make them work together in a smooth way. 
　our approach can be compared to  tambe and rosenbloom  1   where opponent actions are considered while making dynamic decisions. there a single interpretation of actions and observations is used. our approach is more general since actions and observations may be of any kind  making it fit the more general area of operator/artefact reactive loop. in the tiger gas turbine monitoring project  milne et a/.  1   three models are 

mixed in a similar way as ours: decision trees are replaced by compiled rule bases  and a classical chronicle recognition mechanism feeds a model-based diagnosis system. but we do more than mere supervision  getting a highly reactive decision making system processing hypothetical reasoning on the future instead of comparative reasoning on the past. this compelled us to improve the chronicle structure to incorporate actions in it. in a  related game-based military application  katz and butler  1   decision is also dynamically made  but thanks to a discrete game model. besides  classical heuristic techniques are used  exploring down the tree in a lookahead simulation  which we avoid by directly inheriting goodness values from our associative model. 
　one shortcoming of our approach is the classical expertise acquisition problem: uncompleteness of such an expertise is always to be feared. anyway  our architecture can be made robust to unexpected events as well  thanks to some additional features that have not been presented here  see  coradeschi and vidal  1  . 
　as a matter of conclusion  our approach is relevant in highly reactive real-time monitoring applications with complex temporal constraints. it brings out expressive and accurate dynamic decision making. we strongly believe that mixing symbolic models from the artificial intelligence community and control models built by theoretical computer scientists  as we did  might help making substantial advances in that field. 
acknowledgments 
thierry vidal has been supported by the excellence center for computer science and systems engineering  ecsel  in linkoping. silvia coradeschi has been supported by the wallenberg foundation project  information technology for autonomous aircraft . the authors are also grateful to dan stromberg  swedish national defense research center  linkoping  and goran petterson  saab military aircraft  linkoping  for useful comments and discussions that inpired the simplified application example used in the paper. 
