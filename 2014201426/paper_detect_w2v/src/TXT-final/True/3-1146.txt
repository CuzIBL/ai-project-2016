 
in this paper we present a novel methodology for textual case-based reasoning. this technique is unique in that it automatically discovers case and similarity knowledge  is language independent  is scaleable and facilitates semantic similarity between cases to be carried out inherently without the need for domain knowledge. in addition it provides an insight into the thematical content of the casebase as a whole  which enables users to better structure queries. we present an analysis of the competency of the system by assessing the quality of the similarity knowledge discovered and show how it is ideally suited to case-based retrieval  querying by example . 
 
1 	introduction 
textual case-based reasoning  cbr  is very different in many respects from more conventional cbr applications where case knowledge is usually more easily acquired  structured and adequately  and often simplistically  represented as simple vectors or objects. in textual cbr  tcbr  the knowledge to be embodied within a case is much more intricate in that it contains complex linguistic terms and concepts on various topics which are often encompassed within the same case. acquiring and representing this knowledge adequately  without loosing its meaning and with a low knowledge engineering overhead to users  remains a challenging prospect. equally challenging is the prospect of discovering  implementing and maintaining useful similarity knowledge within such systems where it is often vital to identify similar cases that perhaps do not necessarily contain the same words but semantically similar themes or concepts. it may initially seem that the problem of tcbr could be adequately addressed by standard information retrieval  ir  techniques.  however there are limitations with these approaches. perhaps the best known ir approach  commonly known as the bag of words  transforms documents into a vector space model  sebastiani  1   whereby the case knowledge representation formalism is a vector of weights representing the individual words present in each document and similarity is based on a simplistic comparison of overlap among case vectors. however  as is clear  word order and negation information is lost in the transformation  which may be vital for reasoning in certain domains of interest. additionally  word sense disambiguation is lost in the process e.g. the word  java  can have different meanings depending on its context of usage. most importantly of all  there is a loss in transparency to users  which is a major drawback during case retrieval. nlp techniques can determine word order but they can be brittle and computationally expensive. some researchers in the cbr community have followed this approach and developed novel but often domain specific approaches to the task of tcbr. examples of such systems include work by br¨¹ninghaus & ashley    cunningham et al.    kunze & hubner  and lenz . these systems are very domain-dependent  require considerable knowledge engineering effort and are usually designed for the situation where all documents have a similarly structured content. 
¡¡other researchers have instead focused on advancing ir techniques using technologies such as latent semantic indexing  deerwester et al.  1  and probabilistic latent semantic indexing  hoffman  1 . these go beyond the bag-of-words model providing a more advanced view of the document space that may require less knowledge engineering activity within a tcbr context  zelikovitz and hirsh  1 . unfortunately these have crucial drawbacks also. for example they use a transformation to project documents into cases and as such they become non transparent to a user. transparency is an important component of cbr systems to enable the user gain insight into the reasoning and explanation processes of the system. as such they provide little insight into the knowledge contained within a case or how similarity is determined during retrieval. a further limitation is that these approaches are essentially dimensionality reduction techniques and as such important information may be lost. additionally word order and negation knowledge is also lost. wiratunga et al.    combine ir and machine learning techniques in their approach. a strength is its ability to facilitate automated semantic similarity determination but unfortunately their approach is completely supervised and as such the class of each document within the collection must be known apriori to enable the discovery of similarity knowledge. this limits the application of the technology. in addition the effectiveness of the technique to multi class domains has not been addressed and remains an open question. cunningham et al.    have also recently proposed a very different approach to tcbr based on graph theory  that maintains the original document structure and word order. a disadvantage of their approach is that they require an expert to identify domain dependant indexes  which they rely on to assess case similarity using graph distance techniques. therefore there is a significant knowledge engineering burden in terms of similarity knowledge. in addition it cannot determine semantic similarity between cases or cope with ambiguities that may occur. one final open question with this technique is its efficiency with medium to large case-bases as assessing sub graph similarity is a complex matter. 
¡¡in this research  we present a new approach for tcbr called sophia cbr  based upon a scaleable contextual document clustering approach  dobrynin et al.  1   which facilitates an advanced and rich knowledge discovery framework for case-based retrieval. sophia's case representation formalism is similar to a classical vector space model except  rather than using word frequencies  it is based on the conditional probability distributions of terms within documents. it then intelligently discovers important contexts within the case-base and organizes cases into one of a large number of clusters which have these contexts as attractors. this produces groups of semantically related cases  i.e. cases which are on the same or similar subjects but use different terminology  can be recognized as similar  for a given cluster context. this process of forming clusters  allows both a very efficient and competent case retrieval process. it is this unique feature that provides much of the power behind the technology.  
¡¡sophia cbr is advantageous in that  it is domain independent  and has low knowledge engineering overheads as it does not require any user intervention to acquire domain knowledge. as such  all knowledge can be discovered automatically  although if background knowledge is already present it can be utilized . additionally it uses a transparent case knowledge representation  automatically discovers and provides users with additional knowledge about the domain that they can use to refine queries  is language independent  is scaleable and can differentiate between the different contexts of potentially ambiguous terms. the novel technology presented in sophia cbr is useful for both classification tasks and for retrieval  browsing and searching by example. sophia does not have a mechanism to identify word order or negation  features which undoubtedly are important for document collections where each document has a similar internal structure. however in terms of its application to document collections such as presented in this paper  where each document does not necessarily have a similar internal content structure   we show that sophia is capable of providing a competent tcbr system. firstly we present the sophia technology  then we carry out an initial experiment to demonstrate the quality of the discovered case and similarity knowledge. this is followed by an additional experiment which investigates the potential of the system to casebased retrieval  query by example . finally we discuss the results and present future work. 
 
1 	sophia cbr methodology  
knowledge is automatically discovered at various stages within the sophia framework. in step 1 we describe how case knowledge is discovered and represented. in step 1 we show how numerous specific narrow context words are automatically identified. these narrow contexts act as attractors for clustering cases and this step can be regarded as global similarity knowledge discovery. in step 1 cluster level similarity knowledge is discovered and used to determine which narrow context each case should be assigned to. step 1  is strictly not part of the clustering algorithm itself but is an additional processing step that provides extra knowledge about the internal case structure of clusters and provide a means for visualizing them. this can be regarded as localized similarity knowledge discovery. as will become apparent  not only does this empower the user with extra domain knowledge about the problem area but it improves both the case index and the ability of the system to identify similar cases.  
 
step 1 case knowledge discovery. here we describe how case knowledge is automatically extracted from a document corpus. in the following definitions  a term refers to a word in the document corpus. let ¦® denote the set of all documents in the document corpus and ¦· denote the set of all terms present in ¦® . for every document in the corpus a case is automatically extracted and represented by a probability distribution over all terms occurring in that document. 
 p y x 	|   = tf x y     	  
¡Ætf x t     
t¡Ê¦·
where tf x y      is the term frequency of the term y in  document x and t is a term from the set of all terms present in the document collection. although by itself this does not provide a richer case representation than using conventional ir approaches  it does facilitate the process of grouping  indexing and retrieving semantically similar cases  which forms the centerpiece of the power of this technology.  
 
step 1 global similarity knowledge discovery. given a term z¡Ê¦·   we define its context as the probability distribution of a set of words which co-occur with the given term. more specifically the context of the term z is represented in the form of a conditional probability distribution p y z|    where the random variable y takes values from ¦· and p y z|   is equal to the probability of randomly selecting the term y in a randomly selected case within which the term z co-occurs. we can approximate this distribution as:
 	 	 	 	 	 	 
 	  
 
where tf x y      is the term frequency of the term y in  case x and ¦®   z is the set of all cases from the corpus which 
contain the term z . it is obvious that in most cases the context of the term z is too general in scope to present useful information about the corpus. so we are interested only in identifying narrow context terms z . the narrowness of the term z is estimated by the entropy of the probability distribution p y |  z : 
 	 
 h y z|	  = ¡Æp y z 	|	 log  p y z 	|	    
y
 let  ¦·   z denote the set of all different terms from cases in ¦®   z . when there is a uniform distribution of terms from  ¦·   z the entropy h y |  z is equal to log | ¦· z  |. according to heaps law log | ¦·   z |= o log | ¦® z |    baeza-yates & ribeiro-neto  1  there is a   relationship between the case frequency df z    = ¦®|    z | of the term z and the entropy of its context. to allow for this dependency  we divide the whole set of words into r disjoint subsets:  
 
i
i
¡¡¦· =i { :z z¡Ê¦· dfi ¡Ü df z      dfi+1}  i =1..r
¡¡here the threshold dfi satisfies the condition dfi+1 =¦Ádfi where ¦Á 1is a constant. choosing narrow word contexts are based on the assumption that in total there are n narrow word contexts. for every i =1 .. r a set ¦¦  ¦·i i   is selected such that 
 	 	| ¦¦ =i | ¡Æn.||¦·¦·ii||   	 	 
j=1 ..r
and z1 ¡Ê¦¦i   z1 ¡Ê¦·  ¦¦ ¡úi	i	h y 	| z1  ¡Ü h y 	| z1  . 	then 
¦¦ =¡Èi ¦¦i   where ¦¦ is the set of selected narrow contexts. these contexts form the seeds for clustering semantically related cases  where cluster membership  similarity  is measured using the jensen-shannon  js  divergence  lin  1 . in this respect contexts are regarded as global similarity knowledge. 
 
step 1 cluster level similarity knowledge discovery. narrow contexts {  p y |  }z z¡Ê¦¦   discovered in step 1  are considered as cluster attractors. within this study all cases are grouped into at most one cluster based on the case similarity to the attractor  i.e. this is a hard clustering approach but equally a softer approach could be applied. in this way cases are associated with the contexts they most closely match to form clusters of cases that are on similar  closely related  subjects or themes. the similarity between a case x and the context for the term z is estimated by the js divergence between the probability distributions p1 and p1 representing the case and the context respectively: 
 
 	js{1 1} p1  p1   = h   p  1h p  1  1h p  1    
 
where h   p denotes the entropy of the probability distribution p and p denote the average probability distribution 
= 1p1 + 1p1 . a case x is therefore assigned to a cluster with attractor z if  
 
 	z = argmin js{1 1}   p y |   x	p y t 	|   
t¡Ê¦¦
in other words a case is assigned to the cluster whose attractor it has the highest semantic similarity to. 
¡¡once these three stages are completed  we have discovered all case knowledge  all narrow contexts within the case-base and assigned cases to the context they are most semantically similar to. in an equivalent fashion  the similarity between cases within a cluster can be discovered using the js divergence. as such the lower the js divergence  the higher the similarity and as will be seen in step 1  it is this similarity knowledge that forms the key to discovering semantically related cases.  
 
step 1 localized similarity knowledge discovery. up to this point all knowledge discovered has been at the global/cluster level. in this step we discover localized similarity knowledge that defines the inner case structure of each cluster. we represent the relationships  similarities  between cases by a graph where each vertex in the graph represents a case. any two vertices are connected by an undirected edge  whose weight denotes the distance between corresponding cases. this weight is determined as before using the js divergence. the standard kruskal's algorithm is used to find the minimum spanning tree  mst  which spans all graph vertices and has the minimum total weight for its edges. the knowledge within the mst can then be presented to a user as a complete description of the internal structure of a cluster relating to a narrow context. useful knowledge includes  cases nearer to the top of the tree are most similar to the narrow context  while those further away are less similar. cases in close proximity within the mst are more similar than cases that are far apart. cases in one branch of the tree are more similar than those in separate branches. this localized similarity knowledge can either be used to interactively browse the relevant cluster structure looking for useful cases or as a means of accurately classifying a new case.  
1 experiments 
in this work we demonstrate the efficacy of the sophia cbr system to textual case retrieval. in the first experiment we investigate the quality of the case and similarity knowledge by demonstrating the high degree of  semantic  similarity between cases within clusters. in the second  we investigate the quality of the retrieval process itself and show how it is ideally suited to case-based retrieval. 
 
1 case base description 
 in our experiments we use the well known modified apte   modapte   split of the reuters-1 collection  davidlewis.com/resources/testcollections/reuters-1  containing 1 training documents and 1 test documents. not all documents are actually assigned a category. therefore we only use those that have at least one topic  1 from the training set and 1 from the test set  in our experiments. all other documents were considered as background information. we use these documents to accumulate information about the document corpus. all training and background documents were used for case knowledge and similarity knowledge discovery  clustering & mst formation . test documents were used as queries in the retrieval experiments only. it should be noted at this point that the majority of research carried out with this corpus in the past  mostly within the information retrieval community  has tended to focus on evaluations using the 1 most popular categories only  sebastiani  1 . as such our task is much more challenging in that we attempt to use all 1 categories to evaluate our technique. the experiments described in this section could equally be applied to other document collections as sophia cbr is domain and language independent. 
¡¡initially documents were parsed  transformed to cases  by converting all words from the title and body of a document into lower case  deleting stop-words using a standard stopwords list  smart  1 words  and using the porter algorithm for stemming. we consider a word as any maximal sequence of symbols which start with a symbol in the range a-z  and ends with any symbol between a and z or any symbol between 1 and 1 and in between contains symbols from the set {a-z1 -/}. it should be noted that no additional information about the document set is used  apart from the title and body of the documents themselves. in particular  expert assigned knowledge such as category labels are not used in the clustering/retrieval process and as such the approach is totally unsupervised in nature and therefore has as a consequence  no manual knowledge engineering overheads. in total  1 different terms remained after parsing. 
1 experiment on similarity knowledge quality in the first experiment we use the topic categories of cases as a means of independently assessing the quality of our similarity knowledge and hence our clustering process. note that this topic categorization domain knowledge is not utilized as part of the process of case similarity determination  which is based solely on the textual content of cases  but as a means for assessing the effectiveness of the discovered similarity knowledge only. this experiment provides us with insight into the quality and meaningfulness of the sophia tcbr clustering process. our hypothesis is that if the system identifies cases as semantically similar then there should be a high probability that they share many of the same categories. to facilitate this we determine the degree of overlap between adjacent cases in the mst. we therefore evaluate the actual similarity of 1 cases a and b based on the similarity of the topics assigned to them by experts. let t x  be the set of topics assigned to case x by an expert. to evaluate the similarity between t a  and t b  we use the jaccard coefficient  jc .  
 	jc = ||t at a      ¡É¡Èt bt b       ||
a coefficient of 1 indicates that both cases have identical topics assigned to them and have maximum similarity  while a value of 1 signifies no overlap between topics  indicating minimum similarity. figure 1 shows the results of this experiment. the most striking observation from this graph is that the vast majority of cases  1 pairs -1%  have an extremely high jc  1-1 . this provides extremely strong evidence for the high quality of our similarity knowledge and confirms that semantically similar cases  i.e. those linked in the mst  have a large degree of overlap in their categories and therefore must of necessity be genuinely very similar. this also confirms that forming clusters based on narrow contexts combined with a mst based on js divergence is a powerful approach for determining textual case similarity which also provides real meaning and transparency to users. that is users can easily see the context of all cases within a cluster and also which cases are most similar to others within the cluster. at the opposite end of the graph  where we can observe case pairs with poor category overlap  it can be seen that there are 1 pairs of cases  1%  with very poor jc and almost no overlap between their categories. an interesting phenomenon can be observed at a jc value of 1-1. here we see a slight rise in the number of case pairs. the reason for this is that many cases have only 1 categories and this peak represents the situation whereby they agree on one category and disagree on the other  e.g. one case could have 1 categories and another only one  which they share . 

intervals for jaccard coefficient
figure 1 assessment of similarity for mst edges using jc 
 
¡¡for all other jc values there are practically no case pairs. it is reasonable to conclude that clusters are not entirely homogeneous in terms of their topics. that is  each cluster may contain cases on different topics. therefore it is to be expected that  within the mst when traversing from one case to another  topic shifts should be encountered. whenever this occurs adjacent cases in the mst will differ on their topics. these shifts may be gradual  as evidenced by the rise in the graph around a jc of 1  ie adjacent cases overlap on 1% of their topics  or radical  as evidenced by the rise at a jc of 1  adjacent cases overlap on none of their topics . these results provide compelling evidence for quality of the case and similarity knowledge discovered and utilized by sophia cbr.  
 
1 experiment on case retrieval 
in this section  we propose a case retrieval approach designed to provide flexible querying plus a high retrieval accuracy and a good explanatory facility. sophia enables queries to be generated using the traditional key word approach or by using query by example  where entire documents  or parts thereof  can be used as the query. sophia then supplies the user with quality knowledge about  all  possibly relevant cases within the case base. it is important that this knowledge should be presented to the user in the context of the whole case collection. in other words  the user should have the facility to estimate: 
  which region of the whole case base contains the most relevant cases   
  how large this region is   
  which semantically similar documents are relevant      
 
¡¡we propose that the mst is ideally suited to presenting this knowledge to the user. through a process of system supported browsing  the user can evaluate the relevancy of different parts of the tree  estimate its size and estimate the topics of neighboring cases. this process is demonstrated in the subsequent experiment where we consider the following indexing and browsing scenario. the user has a target case d  test case/query example  and the system presents the mst of the most relevant cluster  based on the closest matching context attractor  for browsing. this tree is the mst of a cluster c z d   whose context z d  is the smallest distance from the target case  as before we use js divergence to evaluate distances . the nearest neighbor  nn z d  d   to case d from within cluster c z d    is selected from the mst. the user then starts browsing from this case. we will consider three notions of relevance in this experiment. let t x  be the set of all topics assigned by the expert to case x. 
then 
predicate r= x y    means that t x    = t y    predicate r  x y    means that t x      t y    
predicate r¡É x y    means that 	   	   	 
in other words if r= x y    then cases x and y are considered to be very relevant as they have exactly the same set of topics assigned by experts. if r  x y    then the relevance of case x to case y is slightly weaker than in the previous definition but the user can be sure that all topics of case x are also contained in case y. if r¡É x y   then we have the weakest notion of relevance in that both cases share at least one common topic. we say that the target case d is successfully matched with an existing case if  in the minimum spanning tree mst c z d    of the cluster c z d   there exists at least one relevant training case y within a distance of k edge links from the nearest neighbor case nn z d . in this evaluation we include the nearest neighbor  nn  case as part of the evaluation and only consider edges which connect to training cases  edges connecting to background cases are ignored . let predicate s1 d k   indicate that case d is successfully matched with an existing case within distance k  where relevance is determined by predicate r= x y    . similar predicates s1 d k  and s1 d k  are used when relevance is determined by predicates r  x y    and r¡É x y    respectively. let p1 k    p1 k   p1 k   be the probability that for a randomly selected test case d   the predicate  s1 d  k    s1 d  k    s1 d  k   is true.  figure 1 shows how values of p1 k   p1 k  and p1 k   depend on k. these results show that the knowledge discovered and utilized by sophia cbr  when exploited by the process of interactive browsing  provides an ideal medium for locating and retrieving relevant cases. using the most stringent definition of relevance  where all cases must have exactly he same topics  p1   it can be seen that 1% of the nn cases themselves are relevant  distance 1 . it can be seen that this rises to almost 1% when cases within a vicinity of 1 links are considered. considering larger vicinities  add little more to the relevancy. defining relevancy as in p1  a similar picture is observed. this time the nn cases are relevant 1% of the time  rising to 1% when k=1. again little benefit is added by increasing k further. 
 proportion of test cases that match a training case within distance k 

figure 1 showing how retrieval relevancy depends on k 
finally the least stringent definition of relevancy provides the best results. 1% of nn cases are relevant  rising to 1% when k=1. there is little improvement by increasing k further. these results are particularly encouraging  especially when considering that there are 1 possible categories a case can take and many cases have more than 1 category. it should be noted that the least stringent definition of similarity is the truest reflection of similarity within a tcbr system as the goal is to retrieve cases which are semantically similar  ie have some similar topics . this type of retrieval can be regarded as querying by example. a user can cut a piece of text from any other document  sourced from the web for example  and paste it into the sophia system and through the process described retrieve documents that are semantically similar. the authors are unaware of any other scaleable tcbr system  or document clustering algorithm  that can accomplish this. 
 
1 	conclusions  
in this paper we present a novel approach for discovering case and similarity knowledge within a tcbr system. we describe how the cases are automatically grouped into semantically related clusters focused around discovered central contexts or themes. we show initially how the case and similarity knowledge discovered is of a very high quality and go on to show how the natural organization of the cases within clusters into a mst  provides a very natural environment to enable case based retrieval  query by example . important advantages of this technique include the fact it is completely automated  requires no domain knowledge  and therefore no manual knowledge acquisition   it is language independent  can be used for case classification  dobrynin et al.  1   facilitates semantic similarity determination and very importantly  unlike all other clustering based approaches for document collections  it is scaleable for very large case bases. this is due to the fact that similarity between cases is only calculated at a local cluster level as opposed to the global level. future work will include investigating the formation of sub clusters to aid retrieval and case based classification. 
