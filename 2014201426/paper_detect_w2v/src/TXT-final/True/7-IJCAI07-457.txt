 
in this paper we present a novel method that fuses the ensemble meta-techniques of stacking and dynamic integration  di  for regression problems  without adding any major computational overhead. the intention of the technique is to benefit from the varying performance of stacking and di for different data sets  in order to provide a more robust technique. we detail an empirical analysis of the technique referred to as weighted meta - combiner  wmetacomb  and compare its performance to stacking and the di technique of dynamic weighting with selection. the empirical analysis consisted of four sets of experiments where each experiment recorded the cross-fold evaluation of each technique for a large number of diverse data sets  where each base model is created using random feature selection and the same base learning algorithm. each experiment differed in terms of the latter base learning algorithm used. we demonstrate that for each evaluation  wmetacomb was able to outperform di and stacking for each experiment and as such fuses the two underlying mechanisms successfully. 
 
1 introduction 
the objective of ensemble learning is to integrate a number of base learning models in an ensemble so that the generalization performance of the ensemble is better than any of the individual base learning models. if the base learning models are created using the same learning algorithm  the ensemble learning schema is homogeneous otherwise it is heterogeneous. clearly in the former case  it is important that the base learning models are not identical  and theoretically it has been shown that for such an ensemble to be effective  it is important that the base learning models are sufficiently accurate and diverse in their predictions  krogh & vedelsby  1   where diversity in terms of regression is usually measured by the ambiguity or variance in their predictions. to achieve this in terms of homogeneous learning  methods exist that either manipulate  for each model  the training data through instance or feature sampling methods or manipulate the learning parameters of the learning algorithm. brown et al. provide a recent survey of such ensemble generation methods. 
having generated an appropriate set of diverse and sufficiently accurate models  an ensemble integration method is required to create a successful ensemble. one well-known approach to ensemble integration is to combine the models using a learning model itself. this process  referred to as stacking  wolpert  1   forms a meta-model created from a meta-data set. the meta-data set is formed from the predictions of the base models and the output variable of each 
training instance. as a consequence  for each training instance i =  x y    belonging to training set d   metainstance mi =  f x 1    ..f n    x y    is formed  where f  i    x is the prediction of base model i i  =1..n where n is the size of the ensemble. to ensure that the predictions are unbiased  the training data is divided by a j -fold cross-validation process into j subsets  so that for each iteration of a crossvalidation process  one of the subsets is removed from the training data and the models are re-trained on the remaining data. predictions are then made by each base model on the held out sub-set.  the meta-model is trained on the accumulated meta-data after the completion of the cross validation process and each of the base models is trained on the whole training data. breiman 1b  showed the successful application of this method for regression problems using a linear regression method for the meta-model where the coefficients of the linear regression model were constrained to be non-negative. however there is no specific requirement to use linear regression as the meta-model  hastie et al.  1  and other linear algorithms have been used particularly in the area of classification  seewald  1  d eroski  &  enko  1 . 
a technique which appears at first very similar in scope to stacking  and can be seen as a variant thereof  is that proposed by tsymbal et al. referred to as dynamic integration. this technique also runs a j-fold cross validation process for each base model  however the process by which it creates and uses meta-data is different. each metainstance consists of the following format 
 x err x1    .. err x yn        where err xi     is the prediction error made by each base model for each training instance  x  y  . rather than forming a meta-model based on the derived meta-data  dynamic integration uses a knearest neighbour  k-nn  lazy model to find the nearest neighbours based on the original instance feature space  which is a subspace in the meta-data  for a given test instance. it then determines the total training error of the base models recorded for this meta-nearest neighbour set. the level of error is used to allow the dynamic selection of one of the base models or an appropriate weighted combination of all or a select number of base models to make a prediction for the given test instance. tsymbal et al.  describes three dynamic integration techniques for classification which have been adapted for regression problems  rooney et al.  1 . in this paper we consider one variant of these techniques referred to as dynamic weighting with selection  dws  which applies a weighting combination to the base models based on their localized error  to make a prediction  but excludes first those base models that are deemed too inaccurate to be added to the weighted combination. 
a method of injecting diversity into a homogeneous ensemble is to use random subspacing  ho  1a  1b . random subspacing is based on the process whereby each base model is built with data with randomly subspaced features. tsymbal et al.  revised this mechanism so that a variable length of features are randomly subspaced. they shown this method to be effective in creating ensembles that were integrated using di methods for classification. it has been shown empirically that stacking and dynamic integration based on integrating models  generated using random subspacing  are sufficiently different from each other in that they can give varying generalization performance on the same data sets  rooney et al.  1 . the main computational requirement in both stacking and dynamic integration  is the use of cross-validation of the base models to form meta-data. it requires very little additional computational overhead to propose an ensemble meta-learner that does both in one cross validation of the ensemble base models and as a consequence  is able to create both a stacking meta-model and a dynamic integration meta-model  with the required knowledge for the ensemble meta-learner to use both models to form predictions for test instances. the focus of this paper is on the formulation and evaluation of such an ensemble meta-learner  referred to as weighted metacombiner  wmetacomb . we investigated whether wmetacomb will on average outperform both stacking for regression  sr  and dws for a range of data sets and a range of base learning algorithms used to generate homogeneous randomly subspaced models. 
1 methodology 
in this section  we describe in detail the process of wmetacomb. the training phase of wmetacomb is shown in figure 1..  
algorithm : wmetacomb: training phase
input: d
output: f  sr   f  di   f i i  =1..n
 * step 1 *  
partition d into j-fold partitions di=1..j
initialise meta-data set mdsr =  initialise meta-data set mddi = 
for i = 1..j-1 
	dtrain = d d 	i
　　　　dtest = di  build n learning models f i i  =1..n using dtrain
for each instance  x  y   in dtest
 	 	form meta-instance mi sr :   f 1   x  ..  f n    x   y  
 	 	form meta-instance mi di :  x err1   x  .. errn    x   y  
 	 	add mi sr to mdsr
 	 	add mi di to mddi
 	endfor endfor
build sr meta-model f  sr using the meta-data set mdsr
build  di meta-model f  di using the meta-data set mddi
 * step 1 *  i=j dtest = di
determine totalerrsr and totalerrdi using dtest
 * step 1 *  i = j-1 
dtrain = d d  i dtest = di  build n learning models f i i  =1..n using dtrain
for each instance  x  y   in dtest
 	form meta-instance mi sr :   f 1   x  ..  f n    x   y  
 	form meta-instance mi di :  x err1   x  .. errn    x   y  
 	add mi sr to mdsr
 	add mi di to mddi
endfor
retrain the f i i  =1..n on d
build sr meta-model f  sr using the meta-data set mdsr
build di meta-model f  di using the meta-data set mddi
figure 1. training phase of wmetacomb 
the first step in the training phase of wmetacomb consists of building sr and di meta-models on j-1 folds of the training data  step 1 . the second step involves testing the meta-models using the remaining training fold  as an indicator of their potential generalization performance.  it is important to do this as even though models are not created using the complete meta-data  they are tested using data that they are not trained on  in order to give a reliable estimate of their test performance. the performance of the meta-models is recorded by their total absolute error  totalerrorsr and totalerrordi within step 1. the meta-models are then rebuilt using the entire meta-data completed by step 1. the computational overhead of wmetacomb in comparison to sr by itself centres only on the cost of training the additional meta-model. however as the additional meta-model is based on a lazy nearest neighbour learner  this cost is trivial. the prediction made by wmetacomb is made by a simple weighted combination of its meta-models based on their total error performance determined in step 1. denote totalerr = totalerrorsr  totalerrordi  . the individual weight for each meta-model is determined by finding the normalized error for each meta-model: 
 
normerrori =totalerrori /  totalerrori  
	 	i=1..1
a normalized weighting  for each meta-model is given by: 
 
	mwi = e	t
 
   1  norm   mwi =mwi
 
mwi 1   normerrori
i
 
equation 1 is influenced by a weighting mechanism that wolpert & macready  proposed for combining base models generated by stochastic process such as used in bagging   breiman 1a . 
the weighting parameter t determines how much relative influence to give to each meta-model based on their normalized error e.g. suppose normerror1 = 1 and normerror1 = 1   then table 1 gives the normalized weight values for different values of t . this indicates that a low value of t will give large imbalance to the weighting  whereas a high value of t almost equally weights the metalearners regardless of their difference in normalized errors. 
 
t norm mw 	1 norm mw 	1 1 1 1 1 1 1 1 1 1  
table 1. the effect of the weighting parameter 
 
based on these considerations  we applied the following heuristic function for the setting of t   which increases the imbalance in the weighting dependent on how great the normalized errors differ. 
	t = | normerror1  normerror1 | 	 1  

1
 
wmetacomb forms a prediction for a test instance x by the following combination of the predictions made by the metamodels: 
 
 
norm   mw1 * f  sr    x + norm  mw f1.   di    x
 
1 experimental evaluation and analysis 
in these experiments  we investigated the performance of wmetacomb in comparison to sr and dws. the work was carried out based on appropriate extensions to the weka environment  such as the implementation of dws. the ensemble sets consisted of 1 base homogeneous models where the data for each base model was randomly subspaced  tsymbal et al.  1 . note that random subspacing was a pseudo-random process so that a feature sub-set generated for each model i in an ensemble set is the same for all ensemble sets created using different base learning algorithms and the same data set. the meta-learner for sr and the stacking component within wmetacomb was based on the model tree m1  quinlan  1   as this has a larger hypothesis space than linear regression. the number of nearest neighbours for dws k-nn meta-learner was set to 1. the performance of dws can be affected by the size of k for particular data sets  but for this particular choice of data sets  previous empirical analysis had shown that the value of 1  gave overall a relatively strong performance. the value of j during the training phase of each metatechnique was set to 1. 
in order to evaluate the performance of the different ensemble techniques  we chose 1 data sets available from the weka environment  witten & frank  1 . these data sets represent a number of synthetic and real-world problems from a variety of different domains  have a varying range in their attribute sizes and many contain a mixture of both discrete and continuous attributes. data sets which had more than 1 instances  were sampled without replacement to reduce the size of the data set to a maximum of 1 instances  in order to make the evaluation computationally tractable. any missing values in the data set were replaced using a mean or modal technique. the characteristics of data sets are shown in table 1. 
we carried out a set of four experiments  where each experiment differed purely in the base learning algorithm deployed to generate base models in the ensemble. each experiment calculated the relative root mean squared error  rrmse   setiono et al.  1  of each ensemble technique for each data set based on a 1 fold cross validation measure. 
 
data set original  data set size data set size in  experiments number of  input attributes con-
tinuous discrete total abalone 1  1 1 1 1 autohorse 1 1 1 1 1 autompg 1 1 1 1 1 autoprice 1 1 1 1 1 auto1 1 1 1 1 1 bank1fm 1 1 1 1 1 bank1nh 1 1 1 1 1 bodyfat 1 1 1 1 1 breasttumor 1 1 1 1 1 cal housing 1 1 1 1 1 cloud 1 1 1 1 1 cpu 1 1 1 1 1 cpu act 1 1 1 1 1 cpu small 1  1 1 1 1 echomonths 1 1 1 1 1 elevators 1 1 1 1 1 fishcatch 1 1 1 1 1 friedman1 1  1 1 1 1 housing 1 1 1 1 1 house 1l 1 1 1 1 1 house 1h 1  1 1 1 1 lowbwt 1 1 1 1 1 machine cpu 1 1 1 1 1 pollution 1 1 1 1 1 pyrim 1 1 1 1 1 sensory 1  1 1 1 1 servo 1 1 1 1 1 sleep 1 1 1 1 1 strike 1 1 1 1 1 veteran 1 1 1 1 1 table 1. the data sets' characteristics 
the rrmse is a percentage measure. by way of comparison of the ensemble approaches in general  we also determined the rrmse for a single model built on the whole unsampled data  using the same learning algorithm as used in the ensemble base models. we repeated this evaluation four times  each time using a different learning algorithm to generate base models. the choice of learning algorithms was based on a choice of simple and efficient methods in order to make the study computationally tractable  but also to have representative range of methods with different learning hypothesis spaces in order to determine whether wmetacomb was a robust method independent of the base learning algorithm deployed.  the base learning algorithms used for the four experiments were 1 nearest neighbours  1nn   linear regression  lr   locally weighted regression  atkeson et al.  1  and the model tree learner m1. 
the results of the evaluation were assessed by performing a 1 tailed paired t test  p=1  on the cross fold rrmse for each technique in comparison to each other for each data set. the results of the comparison between one technique a and another b were then summarised in the form wins:losses:ties  asd   where wins is a count of the number of data sets where technique a outperformed technique b significantly  losses is a count of the number of data sets where technique b outperformed technique a significantly and ties a count where there was no significant difference. the significance gain is the difference in the number of wins and the number of losses. if this zero or less  no gain was shown. if the gain is less than 1 this is indicative of only a slight improvement. 
we determined the average significance difference  asd  between the rrmse of the two techniques  averaged over wins +losses data sets only where a significant difference was shown. if technique a outperformed b as shown by the significance ratio  the asd gave us a percentage measure of the degree by which a outperformed b  if technique a was outperformed by b  then the asd value is negative . the asd by itself of course has no direct indication of the performance of the techniques and is only a supplementary measure. a large asd does not indicate that technique a performed considerably better than b if the difference in the number of wins to losses was small. conversely even if the number of wins was much larger than the number of losses but the asd was small  this reduces the impact of the result considerably. this section is divided into a discussion of the summary of results for each individual base learning algorithm then a discussion of the overall results.  
1 base learner: k-nn 
table 1 shows a summary of the results for the evaluation when the base learning algorithm was 1-nn. clearly all the ensemble techniques strongly outperformed single 1-nn whereby in terms of significance alone  wmetacomb shown the largest improvement with a gain of 1. although sr showed fewer data sets with significant improvement  it had a greater asd than wmetacomb. wmetacomb outperformed dws with a gain of 1  and showed a small improvement over sr  with a gain of 1. sr showed a significance gain of only 1 over dws. 
technique 1-nn sr dws wmetacomb 1-nn  1:1   -1  1:1  
 -1   1:1 
  -1  sr1:1 
  1   1:1 
  1  1:1  
 -1  dws 1:1 
  1  1:1   -1   1:1  
 -1  wmetacomb 1:1  
 1  1:1  
 1  1:1 
  1  table 1. comparison of methods when base learning algorithm was 1-nn 
1 base learner: lr
table 1 shows that all ensembles techniques outperformed single lr  with wmetacomb showing the largest improvement with a gain of 1 and an asd of 1. wmetacomb outperformed sr and dws with a larger improvement shown in comparison to dws than to sr both in terms of significance and asd. sr outperformed dws with a significance gain of 1 and asd of 1. 
technique lr sr dws wmetacomb lr  1:1 
 -1  1:1  
 -1  1:1 
  -1  sr1:1  
 1   1:1  
 1  1:1  
 -1  dws 1:1  
 1  1:1  
 -1   1:1  
 -1  wmetacomb 1:1  
 1  1:1  
 1  1:1  
 1  table 1. comparison of methods when base learning algorithm was lr 
1 base learner: lwr 
table 1 shows that all ensembles techniques outperformed single lwr  with wmetacomb showing the largest improvement with a significance gain of 1. wmetacomb outperformed sr and dws with a larger improvement shown over dws than sr in terms of significance and asd.  sr outperformed dws with a significance gain of 1 and  an asd of 1. 
technique lwr sr dws wmetacomb lwr  1:1 
 -1  1:1 
 -1  1:1 
 -1  sr1:1 
  1   1:1   1  1:1 
 -1  dws 1:1 
  1  1:1  
 -1   1:1   -1  wmetacomb 1:1 
  1  1:1  
 1  1:1 
  1  table 1. comparison of methods when base learning algorithm was lwr 
table 1 shows that all ensembles techniques outperformed single lwr  with wmetacomb showing the largest improvement with a significance gain of 1. wmetacomb outperformed sr and dws with a larger improvement shown over dws than sr in terms of significance and asd.  sr outperformed dws with a significance gain of 1 and asd of 1. 
1 base learner: m1 
technique m1 sr dws wmetacomb m1  1:1 
  1  1:1  
 -1  1:1  
 -1  sr1:1 
  -1   1:1   -1  1:1 
  -1  dws 1:1 
  1  1:1 
  1   1:1  
 -1  wmetacomb 1:1 
  1  1:1 
  1  1:1 
  1  table 1. comparison of methods when base learning algorithm was m1 
table 1 shows a much reduced performance for the ensembles in comparison to the single model  when the base learning algorithm was m1. in fact  sr showed no improvement over m1  and wmetacomb and dws although they improved on m1 in terms of significance did not show an asd as large as for the previous learning algorithms. the reduced performance with m1 is indicative that in the case of certain learning algorithms  random subspacing by itself is not able to generate sufficiently accurate base models  and needs to be enhanced using search approaches that improve the quality of ensemble members such as considered in  tsymbal et al.  1 .  wmetacomb showed improvements over sr and to a lesser degree both in terms of significance and asd values. 
overall  it can be seen that wmetacomb gave the strongest performance of the ensembles for  1 out of 1 base learning algorithms  1-nn  lr  m1  in comparison to the single model  in terms of its significance gain  and tied in comparison to dws for lwr. for 1 out of the 1 learning algorithms  lr  lwr  m1   wmetacomb also had the highest asd compared to the single model. wmetacomb outperformed sr for all learning algorithms. the largest significant improvement over sr was recorded with m1 with a significance gain of 1  and the largest asd in comparison to sr was recorded with lwr of 1. the lowest significant improvement over sr was recorded with 1-nn  lr  and lwr with a significant gain of 1  and lowest asd of 1 with m1. 
wmetacomb outperformed dws for all four base learning algorithms. the degree of improvement was larger in comparison to dws than sr. the largest significant improvement over dws was shown with lr with a significance gain of 1 and the largest asd of 1 with lwr. the lowest improvement was shown with m1 both in terms of gain and asd which were 1 and 1 respectively. in effect  wmetacomb was able to fuse effectively the overlapping expertise of the two meta-techniques. this was seen to be case independent of the learning algorithm employed. in addition  wmetacomb provided benefit whether overall for a given experiment  sr was stronger than dws or not  as was the case with lr and lwr  but not m1   indicating the general robustness of the technique. 
1 conclusions 
we have presented a technique called wmetacomb that merges the technique of stacking for regression and dynamic integration for regression  and showed that it was successfully able to benefit from the individual metatechniques' often complementary expertise for different data sets. this was demonstrated based on 1 sets of experiments each using different base learning algorithms to generate homogeneous ensemble sets. it was shown that for each experiment  wmetacomb outperformed stacking and the dynamic integration technique of dynamic weighting with selection. in future  we intend to generalize the technique so as to allow the combination of more than two metaensemble techniques.  in this  we may consider combinations of stacking using different learning algorithms or other dynamic integration combination methods.
