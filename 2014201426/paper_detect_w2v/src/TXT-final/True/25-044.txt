 
this paper deals with learning in reactive multi-agent systems. the central problem addressed is how several agents can collectively learn to coordinate their actions such that they solve a given environmental task together. in approaching this problem  two important constraints have to be taken into consideration: the incompatibility constraint  that is  the fact that different actions may be mutually exclusive; and the local information constraint  that is  the fact that each agent typically knows only a fraction of its environment. 
the contents of the paper is as follows. first  the topic of learning in multi-agent systems is motivated  section 1 . then  two algorithms called ace and age  standing for  action estimation  and  action group estimation   respectively  for the reinforcement learning of appropriate sequences of action sets in multi agent systems are described  section 1 . next  experimental results illustrating the learning abilities of these algorithms are presented  section 1 . finally  the algorithms are discussed and an outlook on future research is provided  section 1 . 
1 	introduction 
m u l t i - a g e n t systems. in computer science and artificial intelligence the concept of multi-agent systems has influenced the initial developments in areas like cognitive modelling  selfridge  1; minsky  1   blackboard systems  erman and lesser  1   object-oriented programming languages  hewitt  1   and formal models of concurrency  petri  1; brauer et ai  1 . nowadays multi-agent systems establish a major research sub-
ject in distributed artificial intelligence; see  bond and gasser  1; brauer and hernandez  1; gasser and 
huhns  1; huhns  1 . the interest in multi-agent systems is largely founded on the insight that many realworld problems are best modelled using a set of agents instead of a single agent. in particular  multi-agent modelling makes it possible  i  to cope with natural constraints like the limitation of the processing power of a single agent or the physical distribution of the data to be processed and  it  to profit from inherent properties of distributed systems like robustness  fault tolerance  parallelism and scalability. 
¡¡generally  a multi-agent system is composed of a number of agents that are able to interact with each other and the environment and that differ from each other in their skills and their knowledge about the environment.  usually an individual agent is assumed to consist of sensor component  a motor component  a knowledge base  and a learning component.  there is a great variety in the multi-agent systems studied in distributed artificial intelligence  huhns  1  foreword . this paper deals with reactive multi-agent systems  where 'reactive  means that the behavior and the environment of the system are strongly coupled  there is a continuous interaction between the system and its environment . 
   learning. there is a common agreement that there are two important reasons for studying learning in multi agent systems: to be able to endow artificial multiagent systems  e.g.  systems of interacting autonomous robots  with the ability to automatically improve their behavior; and to get a better understanding of the learning processes in natural multi-agent systems  e.g.  human groups or societies . in a multi-agent system two forms of learning can be distinguished  shaw and whinston  1 . first  centralized or isolated learning  i.e. learning that is done by a single agent on its own  e.g. by creating new knowledge structures or by practicing motor activities . and second  distributed or collective learning  i.e. learning that is done by the agents as a group  e.g. by exchanging knowledge or by observing other agents . this paper focusses on collective learning  and the central question addressed is:  how can eacn agent learn which action it shall perform under which circumstances   in answering this question  two important constraints have to be taken into consideration  weib  1a  1b . first  the incompatibility constraint  i.e. the fact that different actions may be 
incompatible in the sense that the execution of one action leads to environmental changes that impair or even prevent the execution of the others. and second  the local information constraint  i.e. the fact that an agent typically has only local information about the actualenvironmental state  and this information may differ from the one another agent has; this situation is illustrated by figure 1. 
	two algorithms called the 	ace algorithm 	and the 
age algorithm for reinforcement learning in reactive multi-agent systems are described  ace and age are acronyms for  action estimation  and  action group estimation   respectively . these algorithms base on the action-oriented version  weib  1  of the bucket brigade learning model for classifier systems  holland  1 . according to both algorithms the agents collectively learn to estimate the goal relevance of their actions and  based on their estimates  to coordinate their actions 
	weiss 	1 


1 	distributed al 

	weiss 	1 

figure 1: a blocks world task. 
   as it is described in section 1  learning proceeds by the repeated execution of the basic working cycle. a trial is defined as any sequence of at most four cycles that transforms the start into the goal configuration  successful trial   as well as any sequence of exactly four cycles that transforms the start into a non-goal configuration. at the end of each trial the start configuration is restored  and it is again presented to the agents. additionally  at the end of each successful trial a non-zero external reward rext is provided. 
   task analysis. as a consequence of the local information constraint  an agent may be unable to distinguish between environmental states in which its actions are useful and relevant to goal attainment and environmental states in which its actions are useless.  this situation is sometimes called the sussman anomaly.  consider the environmental states t  u and v shown in figure 1. based on the usual blocks world notation  these three states are completely described by 

as it is easy to see  the action put a 1 b  of the agent a  is useful in state t but not useful in state v. however  because a1's local information t  and v1 about the states t and v; respectively  are identical  the agent a  is unable to distinguish between these two states.  of course  an agent does not always fail to distinguish between  useful and useless states ; see e.g. the states 1 and u. a1's local information is given by 
1 	distributed al 

¡¡an analysis of the search space of the task depicted in figure 1 shows that there are only 1 successful trials of length 1  and 1 successful trials of length 1. the probability that a randomly generated sequence of applicable sets of compatible actions transforms the start into the goal configuration is 1 percent  if the sequence has the length 1  and 1 percent  if the sequence has the length 1. with that  the probability that a random trial solves the task to be learnt is less than 1 percent.  an example of a successful trial of length 1 is given by 
guential  one-action-per-cycle  approach would require five cycles in order to implement this sequence.  
¡¡experimental results. a series of experiments was performed to test the ace and the age algorithm. figure 1 shows the performance profiles of the ace algorithm  the age algorithm  and a random walk algorithm  i.e. an algorithm which randomly chooses an applicable set of compatible actions in each cycle . the parameter setting underlying these performance profiles was as follows: 
 randomly chosen   and  it has to be mentioned that the learning effects reported below can be observed for a broad range of parameters and are not limited to this setting.  each data point in figure 1 reflects the average external reward per episode obtained during the previous 1 episodes. there are several important observations. first  the ace and the a g e algorithm performed significantly better than the random walk algorithm  and they reached their maximum performance level after about 1 trials. after that  the performance levels remained almost constant; this shows that the a c e / a g e algorithms were able to learn stable sequences of action sets. second  the perfor-

figure 1: blocks world states. 
figure 1: performance profiles. 
mance level of the age algorithm is clearly above the performance of the ace algorithm. this illustrates the importance of estimating the goal relevance of an action  as it is done by the age algorithm  in dependence on other  concurrent  actions. the reason for that is that an action may be useful in one activity context but useless in another. however  the improved performance is achieved at the cost of higher space and computation time: whereas the costs of the ace algorithm are proportional to the number of possible actions that can be carried out by the agents  the costs of the age algorithm are proportional to the number of possible action sets. and third  despite their learning abilities both algorithms remain below the possible maximal reward level  which is 1 . the reason for that is the local information constraint and  with that  the inability of the agents to distinguish between all different environmental states; as a consequence  the same estimates are used for different environmental states and necessarily remain inaccurate on some scale. 
1 	concluding remarks 
this paper took the first steps towards learning to coordinate actions in multi-agent systems. two algorithms called the ace algorithm and the ace algorithm for the delayed reinforcement learning of sequences of action sets were introduced and experimental results illustrating the learning abilities of these two algorithms were presented. both algorithms are  elementary  in a twofold sense. on the one side  they make only weak demands on the cognitive abilities of the individual agents. for instance  they do not require that the agents are able to reason about the other agents' knowledge or intentions and they do not require that the agents possess complex decision making strategies. as a consequence  the algorithms are 
	weiss 	1 

even applicable to systems that are composed of rather simple agents. on the other side  both algorithms are very flexible learning schemes that can be extended in a number of ways. for instance  they allow to incorporate high-level problem solving and planning mechanisms known from the field of single-agent systems  as well as a number of refinements that have been proposed for the bucket brigade learning model  e.g.  tax payment  support and look-ahead mechanisms . 
   our future research will concentrate on these possible extensions of the a c e / a g e algorithms. a major topic is the development of algorithms that implement multi agent learning of sequences of compatible actions like the a c e / a g e algorithms do  but that better cope with the local information constraint. 
   another goal of future research is the development of learning algorithms for more complex structured  e.g. hierarchically organized  multi-agent systems  fox  1 . up to now this topic has been not addressed in the field of distributed artificial intelligence. however  there are various related works from other disciplines like psychology  e.g.   guzzo  1; laughlin  1   and economics  e.g.   argyris and schon  1; galbraith  1; hedberg  1; sikora and shaw  1j  that are likely to be very stimulating and useful for achieving this challanging goal. 
r e f e r e n c e s 
 argyris and schon  1  c. argyris and d.a. schon. organizational  earning. addison wesley. 1. 
 bond and gasser  1  a h . bond and l. gasser  editors. readings in distributed artificial intelligence. morgan kaufmann. 1. 
 brauer 	and 	hernandez  	1  	w. 	brauer 	and 
d. hernandez  editors. verteilte kunstliche intelitgenz und kooperatives arbeiten. springer. 1. 
 brauer et a/.  1  w. brauer  w. reisig  and r. rozenberg  editors. petri nets. lecture notes in computer science  vol. 1  part i  and vol. 1  part ii . springer. 1. 
 erman and lesser  1  l.d. erman and v.e. lesser. a multi-level organization for problem-solving using many  diverse  cooperating sources of knowledge. in proceedings of the 1 international joint conference on artificial intelligence  pp. 1 . 1. 
 fox  1  m.s. fox. an organizational view of distributed systems. in i e e e transactions on systems  man  and cybernetics  vol. scm-1  no. 1  
pp. 1 . 1. 
 galbraith  1  j r . galbraith. 	designing complex organizations. addison wesley. 1. 
 gasser and huhns  1  l. gasser and m.n. huhns  editors. distributed artificial intelligence  vol. 1 . pitman. 1. 
 guzzo  1  r.a. guzzo  editor. improving group decision making in organizations - approaches from theory and research. academic press. 1. 
 hedberg  1  b. hedberg. how organizations learn and unlearn. in p. c. nystrom & w. h. starbuck  eds   handbook of organizational design  vol. 1  pp. 1 . oxford university press. 1. 
1 	distributed al 
 hewitt  1  c.e. hewitt. viewing control structures as patterns of passing messages. artificial intelligence  1   1. 1. 
 holland  1  j.h. holland. properties of the bucket brigade. in j. j. grefenstette  editor  proceedings of the first international conference on genetic algorithms and their applications  pp. 1 . 1. 
 holland  1  j.h. holland. escaping brittleness: the possibilities of general-purpose learning algorithms to parallel rule-based systems. in r. s. michalski  j. g. carbonell  & t. m. mitchell  eds.   machine learning: an artificial intelligence approach  vol. 1  pp. 1 . morgan kaufmann. 1. 
 huhns  1  m.n. huhns  editor. distributed artificial intelligence. pitman. 1. 
 laughlin  1  p.r. laughlin. collective induction: group performance  social combination processes  and mutual majority and minority influence. journal of personality and social psychology  1   1. 1. 
 minsky  1  m. minsky. the society theory of thinking. in artificial intelligence: an m i t perspective  pp. 1 . m i t press. 1. 
 petri  1  c.a. petri. kommumkation mit automates. schriften des instituts fur instrumentelle mathematik  universitat bonn  germany. 1. 
 selfridge  1  o.g. selfridge. pandemonium: a paradigm for learning. in proceedings of the symposium on mechanisation of thought processes 
 pp. 1 . her majesty's stationery office  london. 1. 
 shaw and whinston  1 m.j. shaw and a.b. whinston. learning and adaptation in distributed artificial intelligence. in  gasser & huhns  1  pp. 1 . 
 sian  1  s.s. sian. the role of cooperation in multiagent learning. in proceedings of the first international conference on cooperating knowledge based systems. 1. 
 sian  1  s.s. sian. extending learning to multiple agents: issues and a model for multi-agent machine learning   m a ml . in y. kodratoff  ed.   machine learning - e w s l 1  pp. 1 . springer. 
1. 
 sikora and shaw  1  r. sikora and m. shaw. a double-layered learning approach to acquiring rules 
for financial classification. faculty working paper no. 1  college of commerce and business administration  university of illinois at urbanachampaign. 1. 
 weifi  1  g. weifi. learning the goal relevance of actions in classifier systems. in b. neumann  ed.   proceedings of the 1th european conference on artificial intelligence  pp. 1 . wiley. 1. 
 weifi  1a  g. weifi. action selection and learning in multi-agent environments. appears in proceedings of the second international conference on simulation of adaptive behavior. 1. 
 weifi  1b  g. weifi. collective learning of action sequences. appears in proceedings of the 1th international conference on distributed computing systems. 1. 
