 
covering and divide-and-conquer are two wellestablished search techniques for top-down induction of propositional theories however  for top-down induction of logic programs  only covering has been formalized and used extensively in this work  the divide-and-conquer technique is formalized as well and compared to the covering technique in a logic programming framework covering works by repeatedly specializing an overly general hypothesis  on each iteration focusing on finding a clause with a high coverage of positive examples divide-and-conquer works by specializing an overly general hypothesis once  focusing on discriminating positive from negative examples experimental results are presented demonstrating that there are cases when more accurate hypotheses can be found by divideand-conquer than by covering moreover  since covering considers the same alternatives repeatedly it tends to be less efficient than divideand-conquer  which never considers the same alternative twice on the other hand  covering searches a larger hypothesis space  which may result in that more compact hypotheses are found by this technique than by dmde-andconquer furthermore  divide-and-conquer is  in contrast to covering  not applicable to learning recursive definitions  
1 	introduction 
the search for an inductive hypothesis can be performed either bottom-up  i e from an overly specific hypothesis to a more general  or top-down  i e from an overly general hypothesis to a more specific  in prepositional learning  two search techniques for top-down induction have been proposed covering and divide-and-conquer covering  which has been used in e g the aq family 
 michalski 1   constructs a hypothesis by repeatedly specializing an overly general hypothesis  on each iteration selecting a disjunct that satisfies a subset of the positive examples and no negative examples until all positive examples are satisfied by the selected disjuncts divide-and-conquer  which has been used m e g id1 
 quinlan 1   constructs a hypothesis by dividing an overly general hypothesis into a set of hypotheses  which correspond to disjoint subsets of the examples it then continues recursively with those hypotheses for which thr corresponding subsets contain both positive and negative examples the resulting hypothesis consists of all specialized hypotheses for which the corresponding set'; contain positive examples only 
　one of the mam long term goals of inductive logic pro gramming is to upgrade the techniques of the proposi tional learning paradigm to a logic programming frame work this in order to allow for the use of a more expressive formalism and to allow for the use of substantial background knowledge in the learning process however  of the two search techniques used in proposi tional learning  only covering has been formalized and used extensively for top-down induction of logic programs {e g in mis  shapiro 1   foil  quinlan 1  
ana-ebl  cohen 1   focl  pazzani et a/ 1l   grendel  cohen 1  and focl-frontier  pazzani and 
brunk 1   this work contributes to the above long term goal by giving a formalization of dmde-andconquer in a logic programming framework this formal ization is in fact a reformulation of the algorithm spectre  bostrom and idestain-almquist 1   and can alsu be viewed as a generalization of the technique u1ed in ml-smart  bergadano and giordana 1  
　in the next section  the two search techniques are formalized and analysed in a logic programming framework in section three  an empirical evaluation of the two tech niques ls presented  m which the techniques are com pared with respect to efficiency and to the accuracy of the resulting hypotheses finally  concluding remarks are given in section four in the following  we assume the reader to be familiar with the standard terminology in logic programming  lloyd  1  
1 	top-down induction of logic programs 
in this section  we first define the top-down induction problem in a logic programming framework we then present two common specialization operators and show how covering and divide-and-conquer can be formal ized using one of these finally  we analyse the tech-

niques with respect to the hypothesis spaces that are explored  the ability to produce recursive hypotheses and the amount of redundancy in the produced hypotheses 
1 	t h e t o p - d o w n i n d u c t i o n p r o b l e m 
the top-down induction problem can be formulated as follows 
given a definite program o  overly general hypothesis   a definite program b  background predicates  and 
two sets of ground atoms   positive and neg-
ative examples  
find1 	a definite program h  called a valid hypoth and 
in this work we assume tlial all positive and negative examples are ground instances of the same atom t  referred to as the target predicate  and that all clauses in o  and only those  define t furthermore  we assume the target predicate to be non-recursive  1 e no instance of the target predicate is allowed in the body of a clause  it should be noted that this assumption does not prevent other recursive predicates from being used in the definition of the target predicate the reason for this assumption is discussed in section 1 
1 	specialization operators 
literal addition is a specialization operator that has been used in several approaches for top-down indue tion of logic programs  e g  shapiro 1  quintan  1  pazzani et a/ 1   by this operator  a clause is specialized by adding a literal to the body  where the literal usually is restricted to be an instance of a background predicate various restrictions are normally also put on the variables in the literals  e g at least one of the variables should appear elsewhere in the clause  quinlan 1   goal reduction is another specialization operator that has been used in several approaches  eg  cohen 1  pazzani el a/ 1l   by this operator  a clause is specialized by resolving upon a literal in the bod  using one of the background clauses it should be noted that any specialization obtained by goal reduction can also be obtained by literal addition  not necessarily in one step  on the other hand  it is also possible to define predicates that may introduce any literal  cf  cohen 1    which makes any clause obtainable by literal addition also obtainable by goal reduction since goal reduction allows 
for more explicit control of what specializations are allowed than literal addition  we have chosen the former operator when studying search techniques for top-down induction 
1 	c o v e r i n g 
the covering principle can be applied in a logic programming framework in the following way one of the clauses in the overly general hypothesis is selected and 
1
mp denote* the least herbrand model of p 
specialized until the selected clause does not cover1 any negative examples this process is iterated until all positive examples are covered by the selected clauses this technique is formalized in figure 1  using goal reduction as a specialization operator 

figure   the covering algorithm 
example 
assume that we are given the overlv general hypothesis rewaxdcs.r  - suit s  rank r  
and the background predicates in figure 1  together with the following sets of positive and negative examples 
e+ = { rewardcopadeb 1  reward cltibs 1 } 
e~ - { reward hearts 1  rewardfclube j   } 
buit s  - red s  suit s  - black s  rank r  - num r  rank r  - face r  red hearts  red diamonds  black spades  black clubb  
	num l  	num lo  
	face j  	face q  	face k  
figure 1 background predicates 
　since the clause in the overly general hypothesis covers negative examples  it is specialized choosing the first literal to resolve upon using the second clause defining suit s  results m the following clause reward s.r  - black s  rank r  
this clause still covers the second negative example; and is thus specialized choosing the second literal to resolve upon using the first clause defining rank r  results in the following clause 

	bostrfjm 	1 

this clause does not cover any negative examples and is thus added to the resulting hypothesis since the hypothesis now covers all positive examples  the algorithm terminates ＆ 
　the algorithm produces a valid hypothesis in a finite number of steps under the assumptions that there are a finite number of sld-derivations of the positive and negative examples w r t the overly general hypothesis and background predicates and that no positive and negative example have the same sequence of input clauses in their sld-refutations it should be noted that this property is not dependent on how the nondeterministic choices in the algorithm are made however  these choices are crucial for the result normally  a few number of clauses of high generality are preferred to a large number of specific clauses  and making the wrong choices may result in a non-preferred  although valid  hypothesis since it is computationally expensive to find the optimal choices  these are often approximated in several approaches this has been done by selecting the refinement that maximizes the information gain  quinlan 1  pazzani et at 1  cohen 1  
1 	d i v i d e - a n d - c o n q u e r 
the divide-and-conquer principle can be applied in a logic programming framework in the following way each clause in the overly general hypothesis covers a subset of the positive and negative examples if a clause covers positive examples only  then it should be included in the resulting hypothesis  and if it covers negative examples only then it should be excluded if a clause rovers both negative and positive examples  then it corresponds to a part of the hypothesis that needs to be further divided into sub-hypo theses when taken together  these hypotheses should be equivalent to the divided hypothesis this means that a clause that covers both positive and negative examples should be split into a number of clauses  that taken together are equivalent to the clause thai is split this can be achieved by applying the transformation rule unfolding*  tamaki and sato 1  this 
technique is formalized in figure 1 
example 
consider again the overly general hypothesis  background predicates and examples in the previous example calling the divide-and-conquer algorithm with the clause in the overly general hypothesis together with the background predicates and all covered examples results in the following 
　since the clause covers both positive and negative examples  unfolding is applied unfolding upon euit s  replaces  the clause with the following two clauses r haid s r  - r d s   ranjc r  reward s.r  - black s   rank r  
　the first clause coverb one negative example only  while the second clause covers  two positive examples and 
when unfolding upon a literal l m the body of a clause 
c in a definite program p  c is replaced with the resolvents of c and each clause in p whose head unifies with l 

figure 1 the divide-and-conquer algorithm 
one negative example the algorithm is then called once with each of these clauses the empty hypothesis is re turned by the first rail since the first clause does not rover any positive examples the clause used in the sec ond call is unfolded since it covers both positive and negative examples unfolding upon rank r  replaces the clause with the following two clauses reward s r  - b l a c k   s     num r  reward  s.r  - black s   face r  
the first of these clauses covers two positive and no negative examples and is therefore included in the resulting hypothesis  while the second covers one negative example only  and is therefore not included hence  the resulting hypothesis is rewardcs.r  - b l a c k   s     num r  
d 
　divide-and-conquer produces a valid hypothesis in a finite number of steps under the same assumptions as for covering  1 e that there are a finite number of slddenvations of the positive and negative examples and that no positive and negative example have the same sequence of input clauses in their sld-refutations as for covering  it should be noted that the non-deterministic choices  in this case of which literals to unfold upon  are crucial for the result again  the optimal choices can be approximated by selecting the specialization that maxi mizes the information gain  as is done in  boetrom and idestam-almquist 1  cf 1  quinlan 1   
1 	t h e hypothesis spaces 
let o be an overly general hypothesis and b be background predicates the hypothesis space for covering is . 	and 
	  	  	  where 	the hypothesis space 
for dmde-and-conquer is where h' is obtained from by applying unfolding upon clauses that are not in b} 
　note that hdac  which follows from that each set of clauses obtained by unfolding can be obtained by 
 denotes a. resolvent of c and d upon a literal un 
the body of c 

goal reduction and that there are programs that can be produced by covering that can not be produced by divide-and conquer for example  consider the overly general hypothesis p   i   - q x   r x  
and the background predicates 
q x  - b x  r x  - t x  
then the following hypothesis is in hcov   but not m hdac p x  - s   i     r x  p x  - q x   t x  
1 	recursive hypotheses 
as was stated in section 1  the target predicate is assumed to be non-recursive the reason for this is that divide-and-conquer is not applicable when specializing clauses that define recursive predicates  since decisions regarding one subset of examples may then affect other subsets of examples for example  consider the overly general hypothesis 

together with the positive and negative examples 

then the first clause covers negative examples only and should therefore not be included in the resulting hypothesis however  this can not  be achieved without obtaining an o erlj specific hypothesis since then the positive examples would no longer bt covered evtendmg the definition of cover to include all examples that use a clause  and not only use it as the first clause in their refutations does not solve the problem then in the example above all examples would be covered by the first clause  and all except one by the second clause however  since the first clause can not be removed or specialized  it means this sub-part of the problem can not be treated separately according to the divide-and conquer principle one solution to this problem is to transform the overly general hypothesis into an equivalent non-recursive hypothesis  as proposed in  bostrom and ldestam-almquist 1  however  although this transformation allows divide-and-conqucr to be applied il prevents recursive hypotheses from being found it should be noted that although divide-and conquer is not applicable to recursive predicates it does not mean that recursive definitions can not be found by applying unfolding and clause removal on the contrary  a technique for achieving this is presented in  bostrom 1  
　covering  on the other hand  can be easily extended to deal with recursive predicates instead of searching for a clause that together with background predicates covers some positive examples and no negative examples  a clause can be searched for that together with the clauses found so far and the background predicates can cover some not yet covered positive examples without covering any negative examples  and that allows for the remaining positive examples to be covered without covering any negative examples 
1 	redundancy 
when using covering the number of sld-refutations of the positive examples is not necessarily the same for the resulting hypothesis as for the overly general hypothesis  i e the amount of redundancy may increase or decrease on the other hand.  when using divide-and-conquer  the number of sld-refutations of the positive examples is the same for both the overly general and the resulting hypothesis this follows from the fact that the number of sld-refutations does not increase when unfolding is applied  proven in  kanamon and kawamura 1   in order to reduce the amount of redundancy when using divide-and-conquer  only a minor change to the algorithm is needed instead of placing a positive example in all subsets that correspond to clauses that cover the example  the example can be placed in one such subset 
1 	empirical evaluation 
in this section we empirically evaluate the performance of covering and divide-and-ronquer we first present three domains that are used in the experiments and then present the experimental results 
1 	domains 
two domains are taken from the uci repository of machine learning databases and domain theories king+rook versus king+pawn on a1 and tic tac-toe the third domam considers natural language parsing using a definite clause grammar and is taken from 
 bratko 1  
　the example sets in the uci repository are represented by attribute vectors  and have to be transformed into atoms in order to be used together with the algorithms the number of examples is 1 in the first domain  of which 1% art positive  and 1 in the second domain  of which  1% are positive  
　since the algorithms also require overly general hypotheses as input such are constructed for the two first domains in the following way  cf  cohen 1   a new larger predicate is defined with as manv arguments as the number of attributes  and for each attribute a new background predicate is defined to determine the possible values of the attribute this technique is illustrated by the following overly general hypothesis and background predicate for determining win for x in the 
tic-tac-toe domain win ior x si s1 s1 s1 s1 s1 s1 s1 s1  square s1   square s1   square s1   square s1   square sb   square s1   square s1   squara s1   square s1  
square z  square o  square  b  
an alternative formulation of the tic-tac-loe domain is used as well  where a new intermediate background predicate is introduced in the alternative formulation  the definition of the predicate square s  is changed into the following 
	bostrom 	1 


the hypothesis is that the new intermediate predicate will reduce the number of clauses in the resulting definition  and hence increase the accuracy the reason for this is that it does not matter in the correct definition of the target predicate whether a square has the value o or b 
　the set of positive examples in the third domain consists of all sentences of up to seven words that can be generated by the grammar in  bratko 1  p 1   1 e 1 sentences the set of negative examples is generated by randomly selecting one word in each correct sentence and replacing it by a randomly selected word that leads to an incorrect sentence thus the number of negative examples is also 1 two versions of an overly general hypothesis are used for this domain the first version is shown below 

where the definition of the predicate s  x  y  is the same as for sentenced y   but with s substituted for sentence and with one extra clause s x x  by referring to the background predicate a x y  instead of sentenced  y   the problem with recursive overly general hypotheses is avoided  as discussed in section 1 
　the second version introduces intermediate predicates in the definitions of aentencecx.y  and s x y   that group words into classes in the following way 

the hypothesis is  like for the tic-tac-toe domain  that the intermediate predicates will improve the accuracy of the resulting hypotheses 
1 	e x p e r i m e n t a l results 
the performance of covering and dmde-and-conquer in the three domains is evaluated using the information gain heuristics that were mentioned in section 1 an experiment is performed with each domain  in which the entire example set is randomly split into two halves  where one half is used for training and the other for testing the number of examples in the training set1 that are given as input to the algorithms are varied  representing 1%  1%  1%  1% and 1% of the entire example set  where the last subset corresponds to the entire set of training examples and a greater subset always includes a smaller the same training and test sets are used for both algorithms each experiment is iterated 1 times and the mean accuracy on the test examples is presented below in addition  the amount of work performed by the two algorithms is presented measured as the number of times it is checked whether a clause covers an example or not1 
　in figure 1 and figure 1  the results from the kingrook versus king-pawn domain are presented it can be seen that dmde-and-conquer produces more accurate hypotheses than covering for all sizes of the training set furthermore  covering checks more examples than divide-and-conquer for all sizes of the training sets when the size of the training set is 1%  the number of checks made by covering is about 1 times as many as the number of checks made by divide-and-conquer the mean cpu time for divide-and-conquer at that point is 1 s  and for covering 1 s 

size of training sel  %  
figure 1 accuracy for the kr vs kp domain 

size of training sel  %  
figure 1 no of checks for the kr vs kp domain 
     the reason for using this measure of efficiency and nol e g cpu seconds  ie that this measure is not implementation dependent nevertheless  for some cases we also present the learning time measured in cpu seconds the algorithms were implemented in sicstus prolog 1 and executed on a sun 
sparcstation 1 

　in figure 1 and figure 1  the results from the dcg domain are presented the two upper curves in figure 1 denote the accuracy of the hypotheses produced by divide-and-conquer and covering using intermediate predicates interestingly  when no intermediate predicates are used  covering and divide-and-conquer produce identical hypotheses  which is shown by the lowest curve this experiment also illustrates that the amount of background knowledge can be far more important than what search technique is used 
　in figure 1  it can be seen that covering checks more examples than divide-and-conquer for all sizes of the training sets and for both overly general hypotheses when the size of the training set is 1%  the number of checks made by covering without intermediate predicates is about 1 times as many as the number of checks made by divide-and-conquer the mean cpu time for divide-and-conquer at that point is 1 s  and for covering 1 s 
in figure 1 and figure 1  the results from the tic-tac-
toe domain are presented the curves labeled covcrtng i  and divide-and-conquer  i  in figure 1 represent the accuracy of the hypotheses produced by divide-andconquer and covering with intermediate predicates the two other curves represent the accuracy of the programs produced by divide-and-conquer and covering without intermediate predicates it can be seen that covering performs better than divide-and-conquer both with and without intermediate predicates 
　the amount of work performed by covering is more than what is performed by divide-and-conquer for all sizes of the training sets and for both overly general hypotheses  as shown in figure 1 when the size of the training set is 1%  the number of checks made by cov-
ering without intermediate predicates is about 1 times as many as the number of checks made by dvide-andconquer the mean cpu time for divide-and-conquer at that point is 1 s and for covering 1 s 
	bostrom 	1 

1 	concluding remarks 
we have formalized the covering and divide-and-conquer techniques for top-down induction of logic programs the main difference between the techniques is that covering specializes an overly general hypothesis repeatedly  while dmde-and-conquer only specializes it once this has the consequence that a large amount of the work performed at each iteration in covering might be repeated  while this is not the case for divide-and-conquer this was demonstrated by the experiments  in which the amount of work performed by covering was up to 1 times the amount of work performed by divide-aud conquer the experiments also demonstrated that the accuracy of the resulting hypothesis can be higher when focusing on discriminating positive from negative examples  which was done by divide-and-conquer  rather than focusing on a high coverage of positive examples  which was done by covering on the other hand  the hypothesis space is larger for covering and thus more compact hypotheses may be found by this technique than by divide-and-conquer moreover  a major draw-back of divide-and-couquer  in contrast to covering  is that it is not applicable to learning recursive definitions 
　the termination conditions for covering and divideand-conquer could be relaxed b  slightly altering the algorithms instead of requiring that no positive and negative examples have the same sequence of input clauses in their sld-refutations  it is enough to require that for 
each positive example there is one sld-refutalion with a unique sequence of input clauses this alterations would lead to that some valid hypotheses can be found that are not found by the algorithms in their current formulations 
　instead of using goal reduction as a specialization operator  literal addition could have been used in the formalizations and the experiments in the covering algorithm  a clause would then be specialized by adding a literal rather than resolving upon a literal in the body in the divide-and-conquer algorithm  a clause would then be replaced by all clauses obtainable by adding a literal  rather than all resolvents upon a literal however  as was pointed out earlier  by using goal reduction instead of literal addition  explicit control of the possible specializations is obtained  where the overly general hypothesis is used as a declarative bias that not only limits what predicate symbols are used  but also how they are invoked 
a ckn owl edgement s 
this work has been supported by the european community esprit bra 1 ilp  inductive logic programming  this work benefitted a lot from discussions with peter idestam-almquist  who originally made the observation that the algorithm spectre resembles id1 and suggested the use of an impurity measure 
r e f e r e n c e s 
 bergadano and giordana  1  bergadano f and giordana a    a knowledge intensive approach to 
concept induction   proceedings of the fifth inter 
learning 
national conference on machine learning  morgan kaufmann  ca  1  1 
 bostrom  1  bostrom h    specialization of recursive predicates   proceedings of the eighth european conference on machine learning  spriuger-vorlag 
 1  
 bostrom and idestam-almquist 1  bostrom h and idestam-almquist p    specialization of logic programs by pruning sld-trees   proceedings of the 1th international workshop on inductive logic program ming  volume 1 of gmd studien  gesellschafi fur mathematik und datenverarbeitung mbh  1  1-
1 
 bratko 1  bratko i   prolog programming for ar tificial intelligence   1nd edition   addison-wesley 
 1  
 cohen 1l  cohen w w    the generality of overgenerality   machine learning proceedings of the eighth international workshop  morgan kaufmann 
 1  1 
 cohen 1  cohen w w   'compiling prior knowledge into an explicit bias   machine learning pro 
cttdings of the ninth international workshop  morgan kaufmann  1  1 
 kanamon and kawamura 1  kanamon t and kawamura t  preservation of stronger equivalence in unfold/fold logic program transformation  ti '   icot technical report tr-1  japan  1  
 lloyd 1  lloyd j w foundations of logic pro gramming  1nd edition   springer-verlag  1  
 michalski 1  michalski r s    pattern recognition as rule-guided inductive inference   ieee transactions on pattern analysis and machine intelligence 
1  1  1 
 pazzani and brunk 1j  pazzani m 	and brunk c 
 finding accurate frontiers a knowledge-intensive approach to relational learning  proceedings of the eleventh national conference on artificial intelli gence  morgan kaufmann  1  1 
 pazzani et al 1l  pazzani m   brunk c and silverstein g  a knowledge-intensive approach to learn ing relational concepts   machine learning pro ceedings of the eighth international workshop  mor-
gan kaufmann  1  1 
 quinlan.1  quinlan j r    induction of decision trees'   machine learning 1  1  1 
 quinlan 1  qmnlan j r    learning logical definitions from relations  machine learning 1  1  
1 
 shapiro 1  shapiro e y   algorithmic program de bugging  mit press  1  
 tamaki and sato  1  tamaki h and sato t    unfold/fold transformations of logic programs   pro ceedings of the second international logic programming conference  uppsala university  uppsala  swe den  1  1 
1 	learning 

1 	learning 

1 	learning 







1 	learning 

1 	learning 

1 	learning 







1 	learning 

1 	learning 

1 	learning 







1 

1 

1 

