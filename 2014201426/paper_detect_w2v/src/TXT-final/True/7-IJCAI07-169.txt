
a key challenge in applying kernel-based methods for discriminative learning is to identify a suitable kernel given a problem domain. many methods instead transform the input data into a set of vectors in a feature space and classify the transformed data using a generic kernel. however  finding an effective transformation scheme for sequence  e.g. time series  data is a difficult task. in this paper  we introduce a scheme for directly designing kernels for the classification of sequence data such as that in handwrittencharacterrecognitionand object recognition from sensor readings. ordering information is represented by values of a parameter associated with each input data element. a similarity metric based on the parametric distance between corresponding elements is combined with their problemspecific similarity metric to produce a mercer kernel suitable for use in methods such as support vector machine  svm . this scheme directly embeds extraction of features from sequences of varying cardinalities into the kernel without needing to transform all input data into a common feature space before classification. we apply our method to object and handwritten character recognition tasks and compare against current approaches. the results show that we can obtain at least comparable accuracy to state of the art problem-specific methods using a systematic approach to kernel design. our contribution is the introduction of a general technique for designing svm kernels tailored for the classification of sequence data.
1	introduction
a common technique in mechanical classification and regression is to define a feature space for the domain of interest  transform the input data into vectors in that space  and then apply an appropriate classification or regression technique in the feature space. however  some data are more naturallyrepresented in a much more irregular form than feature vectors.
　in this paper  we propose a new approach based on direct matching using parametric kernels  where inputs are variable length sequences of elements. each element is associated with a point in a parameter space  and similarity metrics are defined independently for the elements and their parameters. the parameter space is decomposed into possibly overlapping ranges  and the overall similarity of the sequences is computed by taking the summation kernel over the elements and their associated parameters for each range. the intuition behind the use of parameters is to encode information about the ordering of input data in the kernel in a flexible way. such information is frequently lost in feature-space based approaches. since we can choose to apply a wide range of order-preserving transformations on the distances between elements in our parameterizations  we obtain additional control over the similarity metrics we can define as well. note that our use of the term  parametric  here does not imply that we assume any sort of a priori parametric model of the data  as for example in the case of generative approaches  jaakkola and haussler  1 . note also that sequences of any fixed dimensional feature vectors can be used instead of input data elements in our method as needed since all we assume about elements is that they are fixed-dimensional.
　consider an input x represented as a sequence of n elements xi （ x  i.e. x =  x1 ，，，  xn . we could choose to associate each element xi with parameter τ xi  in parameter space t as follows
 1 
.
　the associated parametersform a non-decreasingsequence of non-negativereal numbers  starting from zero. for any two such sequences  a parametric kernel at each iteration picks one element from each of the sequences and computes the similarities between the elements and their associated parameters separately. these are multiplied to return an overallsimilarity between the elements. this product of the similarities of the elements and their parameters implies that two similar elements may contribute significantly to the overall sequence similarity only when their associated parameters are similar as well. this step is repeated for all pairs of elements from the two sequences  and the results are summed to return the overall sequence similarity.
　na： ve application of this approach has the potential to be swamped by the computational expense of performing many comparisons between elements with widely divergent param-
	x1	z1 z1
	x	z
	x	z
x1	x1	x1	x1 z1	z1	z1 z1	z1
 t
	1	Δ	1Δ	1Δ
figure 1: mapping sequence to parameter space
eters which contribute little or nothing to the final result. intuitively  we could handle this by limiting the comparisons only to subsets of elements that are close in parameter space. this closeness in parameter space is easily specified by the decomposition of parameter space into ranges  so that close elements are defined to be those whose parameters fall into the same range. for instance  we could decompose t into non-overlapping intervals of equal length Δ  and elements from the two sequences are close if their associated parameters fall into the same interval. see figure 1  where elements in x and z are grouped into three ranges based on the aforementioned decomposition scheme. x1  for instance  will be compared against only z1 and z1  both in x and t. any sequence data types fall into this category  e.g. handwritten characters  laser sensor readings  digital signals  and so on.
　ideally  we wish to find a method that meets all of the following requirements : a  computationally efficient to handle largeinputs  b  no need to assume any probabilisticmodels  c  kernels are positive semi-definite  d  no requirement of fixed form inputs  and e  the ability to flexibly embed structural information from the input. while previous approaches fail to satisfy some or all of these  all of the requirements are satisfied by our approach. with an appropriate decomposition scheme  parametrickernels may be computed in time linear to the numberof elements in inputs. parametric kernels are positive semi-definite  thus they are admissible by kernel methods such as svms that require mercer kernels for optimal solution.
　we have applied our method to handwritten character recognition and object recognition from sensor readings. our results compare favorably to the best previously reported schemes using only very simple similarity metrics.
1	related work
a great deal of work has been done on the application of kernel-based methods to data represented as vectors in a feature space  shawe-taylor and christianini  1; scho：lkopf and smola  1 . discriminative models can find complex and flexible decision boundaries and their performance is often superior to that of alternative methods  but they often rely on a heuristic preprocessing step in which the input data is transformed into a set of uniform size feature vectors. the similarity of two input data is then assessed by evaluating generic kernel functions on the feature vectors. to handle sequence data in this traditional kernel framework  features based on distance metrics are often extracted  e.g. the mean coordinates  second order statistics such as median  variance  minimum and maximum distances  area  etc. however  such features do not generalize well because they often impose restrictions that input patterns must be of equal lengths or sampled at equal rates. alternatively  histograms can be constructed from locations  speed  size  aspect ratio  etc. histograms are an effective scheme to map varying length sequences into uniform dimensional feature vectors  porikli  1; grauman and darrell  1 . unfortunately  structural information between elements in input patterns is inevitably lost in histogramming.
　methods based on direct matching do not suffer from such drawbacks as they retain the structural information. they map input sequences into sequences of equal dimensional local features  which are further compared using well known direct sequence matching techniques. for instance  a number of methods based on dynamic time warping  dtw  have recently been proposed for classifying sequence data such as speech or handwritten characters  bahlmann et al.  1; shimodaira et al.  1 . indeed  an extensive survey has shown that dtw methods are among the most effective techniques for classifying sequence data  lei and govindaraju  1; keogh and kasetty  1 . in  bahlmann et al.  1   normalized coordinates and the tangent slope angle is computed at each of the points in a stroke sequence to form a feature vector. dtw computes the optimal distance between two sequences along a viterbi path  which is then used as the exponent of a radial basis function  rbf . in  tapia and rojas  1   a fixed size feature vector is computed from strokes and an svm classifier is used. they achieved a high accuracy over 1% but since they allowed only fixed feature vectors for each stroke  the method's application is quite limited. in  lei and govindaraju  1   extended r-squared  er1  is proposed as the similarity measure for sequences. it uses coordinates of points directly as features  but can only operate on fixed-length windows of such points.
　dtw methods define kernels which can be shown to be symmetric and satisfy the cauchy-schwartz inequality  shimodaira et al.  1 . svm classifiers may employ such kernels to classify sequential data. however  kernels based on dtw are not metrics. the triangle inequality is violated in many cases and the resulting kernel matrices are not positive semi-definite. therefore  they are not admissible for kernel methods such as svm in that they cannot guarantee the existence of a correspondingfeature space and any notion of optimality with respect to such a space. in contrast  our parametric kernels are positive and semi-definite and are thus wellsuited for use in kernel-based classifiers.
1	approach
kernel-based discriminative learning algorithms can find complex non-linear decision boundaries by mapping input examplesinto a featurespace f with an innerproductthat can be evaluated by a kernel function κ : x 〜 x ★ r on examples in the input space x. a linear decision function in f is found  which corresponds to a non-linear function in x. this  kernel trick  lets us find the decision boundary without explicit evaluation of the feature mapping function φ : x ★ f and taking the inner product  which is computationally very expensive and may even be intractable. for this to guarantee a unique optimal solution  the kernel functions must satisfy mercer's condition  i.e. the gram matrix obtained from the kernel functions must be positive and semi-definite.
1	parameters
the underlying intuition in our work is to associate a parameter with each of the elements so that enforcing parametric similarity is equivalent to the similarity in the structure of elements in the input patterns. our work assumes that input patterns are varying length sequences of elements from some common input or feature space. for example  handwritten characters are sequences of 1d points  while images are 1d grids of fixed dimensional color values. the structure of a sequence is a 1d manifold of 1d vectors and that of an image is a 1d manifold of 1d vectors  assuming colors are represented as rgb for instance. a parameter of an element is then a point in this manifold that corresponds to the element. therefore  if two parameters of any two elements are close  then they are structurally close. this parametric association must be defined as part of making the choice of kernel functions.
1	parametric kernel
our input pattern x is a sequence of |x| elements  where each element xi is a d-dimensional vector  i.e. x =  x1 ，，，  x|x|  and xi （ rd. the number of elements may vary across sequences. we associate each element with a parameter in parameter space t via a function τ : rd ★ t. consider a decomposition of t into n non-overlapping ranges
	.	 1 
　for instance  recall the earlier sequence example  where t was the set of non-negative real numbers and τ is defined as  1 . t was decomposed as shown in figure 1.

	1	Δ	1Δ	1Δ	1Δ	1Δ
figure 1: parameter space is decomposed into nonoverlapping ranges of length Δ.
　for the derivation of parameter kernel functions  we first define decomposed element set for tt as it x  = {xi|τ xi  （ tt}  which is the set of elements of x whose associated parameters are in tt. in our previous example shown in figure 1  for instance  i1 x  = {x1 x1} and i1 z  = {z1 z1 z1 z1}. we then compute a similarity for each range by taking a weighted sum of the similarities of every pair of elements of the sets being compared whose parameters fall within the range. for t1  we will compare x1 with z1  x1 with z1 ，，，   and x1 with z1. the similarity for a given pair of elements is obtained by taking the product of the similarity κτ : t 〜 t ★ r between those elements' parameters and a similarity κx : rd 〜 rd ★ r defined directly on the elements themselves  each of which is a mercer kernel function. then  the feature extraction function φ of a parametric kernel is defined as
         φ x  =  φ1 x  φ1 x  ，，，φn 1 x   	 1  where
	 	 1 
and wxi is a non-negative weighting factor. given two sequences x =  x1 ，，，  x|x|  and z =  z1 ，，，  z|z|   the parametric kernel function before normalization is then defined as the sum of similarity for all ranges :

where

　note that since the product of κx and κτ is taken in  1   both must score high to have significance in  1 . also note that no comparison is made between elements that are not from a common range. if  instead  we have to compare all elements from one input with all from the other input  we will face a number of undesirable consequences. for instance  in figure 1  we will compare x1 with {z1 ，，，  z1}  rather than {z1 z1}. this will result in a higher similarity value  which may be helpful for certain cases. but  at the same time  we are more likely to be confused by inputs where there are too many far elements  in which case we are swamped by bad comparisons. of course  we may need dramatically more time to compute  1  as the number of kernel evaluations is significantly increased.
　parameter space decomposition solves such problems. however  such a decomposition scheme can introducequantization errors. to overcome this problem  we allow the ranges to overlap. for instance  ranges in figure 1 may overlap by Δ/1  as shown in figure 1. to suppress over-contribution of elements that fall into the intersections of ranges  we introduce weighting factors in  1 .
　the simplest weighting scheme is to take the average of the similarity at the overlapped regions. in this scheme  the default value for wxi is 1/|txi|  where txi = {tt|τ xi  （ tt}. when no ranges overlap  we have |txi| = 1 and therefore  wxi = 1. otherwise  overlapped ranges may yield
	t1	t1	t1
t
	1	Δ/1Δ/1Δ	1Δ/1
figure 1: ranges overlap by Δ/1.
wxi   1. for instance  with the decomposition scheme in figure 1  at intersection  Δ/1 Δ   we will set wxi = 1  since |txi| = 1. note that this scheme will not result in |txi| = 1  i.e. wxi ★ ±  since  1  will be evaluated only when  then φt x  《 1 and the term is just ignored. further discussion of different decomposition schemes is given in 1.
　finally  to avoid favoring large inputs  we normalize  1  by dividing it with the product of their norms 
	.	 1 
　this is equivalent to computing the cosine of the angle between two feature vectors in a vector space model.
1	parameter space decomposition scheme
in this section  our decompositionscheme is discussed in general in terms of pros and cons with respect to additional cost of computationand changes in classification performancedue to range overlapping and similarity weighting. then  a number of different decomposition schemes are presented.
　as mentioned above  decomposition lets us avoid swamping by bad comparisons and dramatically reduces the computational cost of kernel evaluation but introduces quantization error. this is alleviated by allowing for range overlapping and similarity weighting. however  overlapping must be allowed with care. increasing the size of range overlaps will require additional computation since it is likely to involve more kernel evaluations as more elements are found in each intersection. the gain of decreasing quantization error may provide little improvement in classification performance if we are swamped by bad comparisons. thus there is a tradeoff between quantization error and classification performance.
　one issue left is the time to compute the decomposed element sets. the more complicated a decomposition scheme gets  the more difficult the implementation becomes and the more time it takes to run. fortunately  our experimentation has revealed that classification performance is relatively insensitive to minor changes in the decomposition scheme. therefore  we can often favor simpler decomposition schemes for ease of implementation and efficient kernel evaluation with the expectation of only minimal losses in performance.
　we now present a number of example parameter space decomposition schemes.
regular decomposition	parameter ranges all have a common length Δ as in figure 1 or 1. implementation is simple  and it shows good classification performance in general. but we have limited freedom to fit the data.
irregular decomposition parameter ranges are of varying lengths. implementation is complicated and often decomposed element sets take longer to compute. but we can freely decompose the parameter space to better fit the data.
multi-scale decomposition parameter ranges form a hierarchical structure at different resolutions. for instance  we may consider a decomposition where ranges form a pyramid as shown in figure 1. elements from non-adjacent ranges could be compared at coarser resolutions. along with a proper weighting scheme  this may improve the performance. but implementation is more complex and kernel evaluation may take longer.

figure 1: pyramidal parameter space decomposition
1	mercer condition
according to mercer's theorem  a kernel function corresponds to an inner product in some feature space if and only if it is positive and semi-definite. a unique optimal solution is guaranteed by kernel methods only when kernels are positive and semi-definite. to see that our proposed method produces positive and semi-definite kernels  we first note that  1  is positive and semi-definite. in  haussler  1   such kernels are called summation kernels and proven to be positive and semi-definite. it is not difficult to see that  1  is just a sum of summation kernels. since we can synthesize a new mercer kernel by adding mercer kernels   1  is a mercer kernel.
1	efficiency
the time complexity to compute parametric kernels depends greatly on the particular decomposition scheme used. here we provide a brief analysis only for regular decomposition schemes.
　assume that constant time is needed to evaluate κx and κτ and that we are using a regular decomposition scheme as in figure 1 or 1. then the time complexity of evaluating  1  for sequences x and z composed of |x| and |z| elements  respectively  is  on average  o |x||z|Δ/l   where l = max τ x|x|  τ z|z|  . in the worst case without decomposition  Δ = l  so the complexity is o |x||z| . in general  we expect decomposition to produce since we would like to have only a reasonably small subset of elements in each range.
　the storage complexity is o 1  since we only keep the sum of kernel evaluations in memory. the time complexity to decompose the sequences into their respective element sets is o |x| + |z|   if constant time is taken for each element.
1	results
for our handwritten character and object recognition experiments  an implementation based on matlab svm toolbox in  gunn  1  was used. for handwritten character recognition  we limited the scope of our testing to the recognition of isolated characters. we built our own training and test sets  similar to those in unipen data1.
1	handwritten character recognition
handwritten characters are represented as sequences of 1d  x y  coordinates of pixels  points  on the screen. our objective is to learn from a training set a function that correctly classifies an unseen handwritten character. we normalize by scaling inputs to fit a 1〜1pixel bounding box and aligning them at the center. the input data points in each sequence are chosen as the elements. our training set is composed of 1 labeled examples created by two writers  each of whom wrote numeric characters from '1' to '1' ten times. our test data is composed of 1 labeled examples created by other authors  1 for each character. figure 1 shows some training examples for characters '1' and '1'  with the number of points in each of them shown below.

	1 points	1 points	1 points	1 points
figure 1: sample raw inputs for handwritten numbers
　we trained one-versus-all multi-class support vector novelty detectors  svnds  with the parametric kernel. see  shawe-taylor and christianini  1; scho：lkopf and smola  1  for an introduction to svnd. parameter space was regularly decomposed with overlapping allowed as shown in figure 1. window and hop sizes were set to 1 and 1  respectively. we chose rbf for κx and κτ with σ = 1 and ν = 1  where ν is the lower bound on the ratio of support vectors and at the same time the upper bound on the ratio of outliers. snvd predicts how novel a test sample is in terms of novelty. each of the test examples is classified as the class that has the minimum novelty value  and the result is compared against its label. the result is shown in figure 1. the classification error was measured as the percentage of misclassified examples in the test set. we obtained an error rate less than 1% in most cases.
　this represents a classification accuracy that is comparable to the recognizer in  tapia and rojas  1  and superior to those in  lei and govindaraju  1; keogh and kasetty 
class# svserrorclass# svserror'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %'1'1.1 %figure 1: handwritten number recognition
1; bahlmann et al.  1 . however  unlike those approaches  we achieved this without any sophisticated feature extraction or restrictions on the input structures.
1	object recognition from sensor data
we also analyzed sensor data captured at 1 second intervals by a hokuyo urg-1lx laser range finder1 mounted on the front side of a segway rmp1. the segway rmp navigates in its environment under the control of an attached tablet pc communicating with it via usb commands. our tablet pc program reads the sensor data and detects nearby objects and obstacles during the navigation. the specific objective here is to locate a subregion in the sensor data that corresponds to a soccer ball. each frame of input data is represented as an array of 1 regular directional samples in a range of  1  to 1 . each of these is the distance to the nearest object in millimeters  ranging from 1 to 1 with maximum 1% error. see figure 1  a  for a snapshot.
	1  1  1	  1
	1	1	1	  1
	1
	 a  raw input	 b  segmented	 c  detected
figure 1: raw sensor input is shown in  a  and thick curves in  b  are the blobs after segmentation. in  c   the thick round blob at angle about 1  and distance 1 is detected as the soccer ball.
　after normalizing the sensor data by dividing it by the maximum distance  it is segmented into several subregions called blobs. a blob is a subregion of a frame where all distance values are in  θ 1  and every consecutive values are at most δ apart. in this experiment  we used θ = 1 and δ = 1. blobs are further normalized by scaling and translating so that the min and max distance values in each blob are 1 and 1.
　blobs are sequences of scalar values. ball blobs resemble semi-circles distorted by some small sensor noise because the laser range finder scans from only one side of the ball. however  since this is also the case for all round objects such as human legs or beacons  they will confuse our classifier. for demonstration purposes  it is assumed that no such confusing objects exist in the scene. some of the ball and non-ball examples are shown in figure 1.
　our data is composed of examples for 1 ball blobs and 1 non-ball blobs. ball and non-ball blobs are labeled as +1 and -1  respectively. we trained a soft-margin support vector classifier  svc  with a quadratic loss function and the parametric kernel. see  shawe-taylorand christianini  1; scho：lkopf and smola  1  for an introduction to svcs. we used a similar parameter space decomposition scheme as in the handwritten character recognition task. window and hop sizes were set to 1 and 1  respectively. we chose rbf for κx and κτ with σ = 1 and c = 1  where c is a positive number that defines the relative trade-off between norm and loss in regularized risk minimization. we measured the classification error from cross-validation. we randomly sampled 1 positive and 1 negative examples from our data for training  and the classification error was measured by the percentage of examples misclassified by the trained classifier among the rest of the data. on average  we obtained error rates of 1% and 1% for positive and negative test data  respectively  and 1% of the training data were support vectors. the error rate for non-ballblobswas a bit higherbecause of over-fitting when we treated all non-ball blobs as negative examples. each scene on average contained about 1 to 1 blobs  among which only one corresponds to a soccer ball. our c++ implementation running on a tablet pc with intel pentium m 1 ghz processor on average required less than 1 seconds to classify blobs in each scene.
 1.1
1
	1	1	1
figure 1: solid  dashed  curves are ball  non ball  examples  where vertical and horizontal axis correspond to the normalized distance and the number of points in blobs. clearly  ball blobs are semi-circular  while others are irregular.
1	conclusions and future work
our parametric kernel provides an efficient and very flexible framework for extending discriminative learning to sequential input data. our approach allows for direct use of input data without computing sophisticated feature vectors  which makes the problem simpler to solve and easier and more efficient to implement. thus we believe that this scheme often provides a more systematic  flexible  and intuitive way to build effective kernel functions for discriminative classifiers than previous methods while providing similar or superior performance.
　while we have designed our techniques to apply to sequential input data  we believe that the advantages of a systematic approach that is free of the need for uniform input data can be applied to other types of data with natural input orderings such as grids  graphs  sets  etc. we believe that this is due to the intuitive binding of an application-dependent feature comparison scheme with kernel functions based on the natural ordering of the input data.
