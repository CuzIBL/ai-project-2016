 
a principal problem of auditory scene analysis is stream segregation: decomposing an input acoustic signal into signals of individual sound sources included in the input. while existing signal processing algorithms cannot properly solve this inverse problem  a multiagent-based architecture has been considered to be a promising methodology in its modularity and scalability. however  most attempts made so far depend on subjectively defined rules to deal with variability of sounds. here we propose a quantitatively principled architecture in agent interaction by formulating the problem as least-squares optimization. in this architecture  adaptation of the agents is the essential idea. we have developed two kinds of processing to realize adaptivity: template filtering and phase tracking. these mechanisms enable each agent to optimally  in the least-squares sense  track the individual sound. as an example application of the proposed architecture  we have built a music recognition system that recognizes instrument names and pitches of the notes included in ensemble music performances. experimental results show that these adaptive mechanisms significantly improve the recognition accuracy. 
1 	introduction 
in recent years scene analysis based on acoustic information  termed auditory scene analysis  has received a renewal of interest. recognizing external events based on acoustic information is an essential function for systems that work in the real world. 
　a principal problem toward auditory scene analysis is stream segregation  bregman  1 . this segregation means decomposing an input signal into signals of individual sound sources included in the input. however  once multiple acoustic signals are mixed up  their segregation is  so far  considered very difficult because it is an ill-posed inverse problem. 
1 	neural networks 
　nevertheless  technical and applicational importance has attracted researchers to this field of study. specifically  works intended to model integration of bottom-up and top-down processing includes  lesser et a/.  1    nakatani  et a/.  1   and  ellis  1 . these works are characterized by their architectures based on processing modules with simplified functions and communications between these modules  which we call a multi-agent architecture. while the architecture intrinsically enjoys modularity and scalability  quantitative background for behavior of agents is not yet established. practically  the multi-agent based systems mentioned above require sub-
jectively defined rules to control interaction schemes or to adjust parameters for modules in order to deal with variations of sounds. 
　here we propose a quantitatively principled architecture  called ipanema  designed to solve the stream segregation problem for sound mixtures. the essential idea is adaptation of agents to cope with variation of a sound. we have developed two kinds of processing in order to realize adaptivity: template filtering and phase tracking. these mechanisms enable each agent to optimally  in the least-squares sense  track the individual sound. as an example application of the proposed architecture  a music recognition system has been built. the evaluation tests show that the adaptive mechanisms have significantly improve the recognition accuracy in comparison to a conventional signal-detection/separation method based on the matched filtering. 
　in the following part of this paper  section 1 focuses the discussion on the adaptive processing  which is an essential part of the architecture. section 1 then describes general configuration of the ipanema architecture. after section 1 introduces evaluation results of the recognition accuracy  section 1 discusses implications of the present work in the context of existing related approaches. finally section 1 concludes the paper with the expected future work. 
1 	adaptation of templates 
1 	template filtering 
we consider representing an input acoustic signal z{k  with a sum of template waveforms yn k   where n is 


kashino & murase 


1 	neural networks 

in the frame. then it outputs the fundamental frequencies to p-bus  promotion bus   which is observed by all the agents. 
agents 
in our architecture  each agent in the  agent network  is a processing module that corresponds to a single sound source  flute  for example . each agent has a bank of raw-template waveforms  each of which corresponds to a specific pitch and expression. 
　each agent examines the p-bus information to check the possibility that the sound source corresponding to the agent is included in the input or not. if the agent infers that there is a possibility of being included  the agent suggests waveform ri  applying the adaptive phase tracking method described in section 1 to one of the rawtemplate waveforms. if the agent finds little possibility of presence of the corresponding sound  it just keeps silence. 
　waveforms rt are written to the common place called m-bus  mediation bus  and passed to the mediator. then the agents wait for the mediator to feed back answers. the answers from the mediator are sets of filter coefficients that optimally modify ri each agent reads the answer from the mediator via m-bus and then calculates an fir filter with the answered coefficients to obtain a waveform y{. the final output of the agent is waveform yi  its average power e y1i  as activity level  and an information label that the agent is given; for example   flute c1 . 
　in the current implementation  the agents in the agent network only communicates with the mediator. however  we expect that the extension of processing scheme would enable communication among the agents themselves in the agent network through m-bus. 
mediator 
here mediation of agents is formulated and reduced to the problem of matrix calculation  as discussed in section 1. thus the mediator first receives the r  via m-bus from agents. then it calculates the optimal filter coefficients for each agent  using equation  1 . finally the mediator sends the coefficients back to each agent using m-bus. 
information integrator 
the information integrator is a post processing module that revises the symbolic output of the system. it receives an information label  e.g. flute c1  and activity level from each agent  and basically  the label with the highest activity level for each note composes a symbolic version of output of the system. it is inevitable  however  that noises occasionally appear without higher-level information such as temporal or simultaneous relations between sounds. thus the bayesian networks are employed here in order to integrate multiple sources of information. 
　in the current system  note transition information has been introduced. the information integrator first constructs the bayesian networks where nodes encode probabilities for the information labels and links represent temporal relation between the nodes. the integrator then updates the probabilities for the labels based on figure 1: test patterns used in note-recognition benchmark tests. note that each chord includes a perfect fifth interval  i.e. 1 fundamental frequencies   making the recognition more difficult than a completely-random interval pattern. 
the probability propagation scheme  pearl  1   integrating note transition statistics stored in advance. this paper  however  focuses on the adaptive mechanisms and the details on the information integrator will be reported in a separate article  kashino and murase  1 . 
1 	evaluations 
we performed two kinds of tests to evaluate the system: a benchmark test for musical note recognition and a sample song recognition test. in both tests  the information integrator was turned off  i.e. the note transition information was not integrated  in order to evaluate basic performances of the adaptive mechanisms. 
1 	b e n c h m a r k test 
to evaluate the advantages of adaptive processing described in section 1  we tried to conduct the benchmark test of note recognition used in  kashino  et a/.  1a; 1b . 
　the test signal was a three-simultaneous-notes pattern  as shown in figure 1. the pattern was composed and created by a computer using digitized acoustic signals  1bit  1khz  of natural musical instruments  flute  piano  and violin . we first recorded the single notes of those instruments at a recording studio and stored the waveforms on a computer. we then mixed the stored waveforms  selecting a designated number of notes. the selection of the notes were programmed to produce the class-1 note pattern  which is the term of  kashino  et a/.  1b   where the interval between at least two simultaneous notes is a perfect fifth. 
we defined the recognition rate  r  as 

where right is the number of correctly identified and correctly source-separated notes  wrong is the number of spuriously recognized  surplus  notes and incorrectly identified notes  and total is the number of notes in the input; this is the same definition as in the above mentioned papers. from preliminary experiments  number of taps of the fir filter was chosen to be 1 for the template-adaptation-on condition. the templateadaptation-off means that the number of taps of the fir filter was fixed to 1. 
　in this test  if one use the same waveforms as the templates as the ones used for test signals  these waveforms will completely match and results will become in-
kashino & murase 

table 1: results of a benchmark test indicating that both kinds of adaptive processing discussed in section 1 have improved the note recognition accuracy. 

table 1: results of a music recognition test. 

appropriate. therefore we used different manufacturers' instruments  for example  boesendolfer's piano and yamaha's piano  in making test signals and templates  respectively. 
　the results are listed in table 1  which clearly show advantages of the processing scheme developed in section 1. the condition where both template adaptation and phase tracking are turned off is equivalent to the matched filtering  which is a conventional signal processing method for signal identification. 
1 	music recognition tests 
we have evaluated the system using music sound signals. table 1 lists the note recognition rates for a sample music: a live recording of a chamber ensemble  auld lang syne   arranged in three parts and performed by violin  flute  and piano. a part of output of the agents network and a part of score-like data produced by the system are shown in figures 1 and 1. 
1 	related work and discussions 
for the acoustic signal separation task  much work has been done since as early as 1's. the approach using microphone arrays has been one of major research streams  mitchell et a/.  1l   bell et a/.  1   and the harmonic selection is another major method  parsons  1   nehorai and porat  1 . these approaches have been principally based on a single cue  localization of sources or harmonicity . on the other hand  works trying to integrate multiple cues for stream segregation are 
1 	neural networks 

figure 1: a part of output from agents. ordinate: pitch and abscissa:time. each square stands for a recognized note. bars in each square denote the average powers of the activated agents. 

figure 1: a part of results yielded by the implemented system. a note value recognition  e.g. quarter notes  half notes  ...  is not considered here and all the notes are displayed as quarter notes with a real-time scale. 
recently emerging. the most closely related works to the presented architecture are the ipus project  lesser et a/.  1   nakatani et al.   kashino et al.  1a  
and ellis . 
　the ipus is an acoustic signal understanding project based on the blackboard architecture lesser et a/.  1   seeking adaptive processing according to the input to the system. ipus realized its adaptivity basically in a rulebased strategy while the ipanema architecture does not employ symbolic rules for adaptation. 
　nakatani et al. invented a speech segregation system that consists of multiple processing modules called a generator and tracers  nakatani  et a/.  1 . the function of the tracer is to trace harmonic structure  which is similar to the function that the promoter in our architecture performs. however  their system does not have an explicit mechanism to identify sound sources; it only 

segregates harmonic or localized sound into signals. our main point is the adaptive processing to trace the specific sound sources  investigating what the sources are. 
   ellis proposed a prediction-driven architecture for an auditory scene analysis system  ellis  1   where context sensitivity in scene interpretation is realized. in our current implementation of ipanema  musical context does not affect the template adaptation scheme; the context is utilized in the information integrator. 
   the studies toward music recognition includes  montreynaud  1    chafe et a/.  1  and  brown and 
cooke  1 . however  automatic music transcription systems or music stream segregation systems which can deal with given ensemble music played by multiple musical instruments with a reasonable accuracy have not yet been realized. a quantitative architecture was proposed by kashino et al.  1a   in which a bayesian probability scheme was applied. however  their architecture does not yet include adaptive processing and has been applied only to artificial performances synthesized by a sampler  kashino  et ai  1a . the system presented here  on the other hand  was designed for  and tested by  real performances rather than sampler performances. 
1 	conclusion 
we have presented a new system architecture designed for auditory stream segregation. to cope with a variety of sounds that appear in the real world  two mechanisms  template filtering and phase tracking  have been devised. in implementation  we have taken advantage of modularity and scalability of the multi-agents approach. the adaptivities realized in this paper enable each agent to optimally  in the least-squares sense  track the individual sound included in input sound signals. 
   as an example application of the proposed architecture  we have built a music recognition system that recognizes instrument names and pitches of the notes included in ensemble music performances. experimental results show that the adaptive mechanisms significantly improve the recognition accuracy in comparison to the matched-filter-based processing  which is a conventional signal detection/separation method. 
   this paper has focused on the adaptive mechanisms and left the information integrator  the post-processing module  almost untouched. however  information integration is an important issue to be addressed: our preliminary tests have shown that integration of the statistical information of note transitions improves the note recognition rate up to approximately 1 % in the same music recognition test as used here  kashino and murase  1 . to obtain further accuracy  we anticipate that sound source models that explicitly model variations of the sources would be necessary. 
acknowledgments 
the authors would like to thank hiroshi g. okuno  takeshi kawabata  and tomohiro nakatani for contributing valuable discussions with the authors. the authors also wish to thank ken'ichiro ishii for his support as the executive manager of their research laboratory. 
