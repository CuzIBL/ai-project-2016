 
by automatically reformulating the problem domain  constructive induction ideally overcomes the defects of the initial description. the reformulation presented here uses the version space primitives d e  f   defined for any pair of examples e and f  as the set of hypotheses covering e and discriminating f  from these primitives we derive a polynomial number of m-of-n concept. experimentally  many of these concepts turn out to be significant and consistent. a simple learning strategy thus consists of exhaustively exploring these concepts  and retaining those with sufficient quality. tunable complexity is achieved in the monkei algorithm  by considering a usersupplied number of primitives d ei  f i    where ei and fi are stochastically sampled in the training set. monkei demonstrates good performances on some benchmark problems  and obtains outstanding results on the predictive toxicology evaluation challenge. 
1 	introduction 
the goal of machine learning  ml  is to find a set of hypotheses accurately describing the target concept at hand  and to do so with an acceptable complexity this is made possible only if the learner  the description of the problem domain and the distribution of the training examples fit well together. 
　when learning small disjuncts  holte  1  for instance  the difficulty might come from the distribution of the examples  and the existence of rare cases  weiss and hirsh  1 . it might also be due to the lack of relevant primitives in the problem description  perez and rendell  1  - and indeed new primitives might allow to generalize/cluster rare cases in such a way that they are not  rare  any more. last  the existence of rare cases might be caused by the learning strategy  e.g. based on set-covering  michalski  1 . 
　constructive induction traditionally focuses on refining  rewriting  the problem description  michalski  
1 	machine learning 
1 . the quality of a reformulation is measured by the improvement of some base learner accuracy. indeed  expert-driven reformulations of the problem domain can significantly improve the learning performances  craven and shavlik  1 . 
　constructive induction is the process of automatically finding a good quality reformulation. a first possibility is to derive the candidate reformulations from rules learned in a previous learning step. wnek and michalski  look for new attributes allowing one to compact the previous rules. gama  uses the prediction of previously learned classifiers as new attributes. another possibility is to syntactically define the space of candidate reformulations. for instance  mrp explores a set of relational patterns  defined as boolean functions of the initial attributes of the problem domain  perez and rendell  1 . in first order logic  sp searches a set of boolean functions  used to rewrite first-order examples in propositional form  kramer et al.  1 . 
　these approaches strongly depend on the quality of the knowledge provided to the system  through rules  classifiers or syntactic definitions of the candidate reformulations   which must be relevant and make the search tractable. 
　to alleviate this limitation  we present a three-step approach interleaving induction and constructive induction: first  some initial hypotheses are constructed from the examples; second  these hypotheses incur a simple reformulation; last  some simple concepts of the reformulated problem are considered  and those satisfying the validation criteria  minimal number of covered examples  maximal number of allowed exceptions  are retained. this way  the learning workload might be balanced between constructive induction  reformulation  and induction  making it possible to relax their respective requirements: the quality of the initial hypotheses might be low  as these will be reformulated; the complexity of the reformulation might be low  as it is based on hypotheses instead of examples; the last induction step might be rough  as simple worthy concepts are emerged by reformulation. only attribute-value languages will be considered in the paper. 
　this approach is rooted in the version space  vs  framework  which canonically characterizes the hypothe-

ses solutions of a learning problem  mitchell  1 . in order to give a polynomial characterization of the vs  we have introduced the primitives d e  f  of the vs  defined as the set of hypotheses covering any given example e and discriminating any example f  sebag  1   section 1 . indeed  d eyf  can be viewed as the logical analog of the set of hyper-planes separating e from f. 
　the point here is that d e f  naturally gives rise to an integer attribute noted he f  section 1 . simple concepts built on this attribute  e.g.  correspond to m-of-n concepts  of the initial domain language. reformulating the problem domain according to these attributes thus gives access to a polynomial-sized subset of the exponential set of all m-of-iv concepts. experimentally  it turns out that many of these concepts are worthy  i.e. they cover a significant number of examples  and are  almost  consistent. it is then sufficient to evaluate all candidate concepts  and retain those with acceptable significance and consistency. 
　however  if all primitives d e f  were considered  the complexity of the approach would be cubic in the number of examples  making it unrealistic to handle medium-to-large datasets. this paper thus presents an algorithm called monkei  for m-of-n-based konstructive induction   using stochastic heuristics to achieve resource bounded induction  along the lines of bounded resource reasoning  zilberstein  1 : the number of considered primitives d ei fi  is set by the user  and examples ei and fi are randomly selected in the training set  section 1 . 
our approach is situated with respect to related work 
 section 1   and monkei is experimentally validated  section 1 . the advantage of this approach is successfully demonstrated on some problems in the irvine repository  c. blake and merz  1   and a real-world problem proposed as an ijcai challenge  srinivasan  1   known as predictive toxicology evaluation ii. the main limitation of monkei is that it provides a dnf theory  less intelligible than standard cnf theories. how to address this limitation  and other perspectives of research  are discussed in the last section. 
1 	the primitives of version space 
we assume the reader's familiarity with the version 
space framework  mitchell  1  and its limitations due to an exponential complexity  haussler  1 . as a general remark  the complexity of a concept is commanded by its representation: a concept in dnf form  expressed as a conjunction of disjunctions  corresponds to an exponential concept in cnf form  expressed as a disjunction of conjunctions . one way of having an affordable level of complexity  for both inductive and deductive reasoning  might thus be the use of a dnf formalism instead of a cnf one  khardon and roth  1 . 
　along these lines  the disjunctive version space proposes a polynomial dnf characterization of the version space  sebag  1 . this characterization is built from elementary hypotheses d{e f   called version space primitives: d e  f  is defined as the set of all hypotheses covering e and rejecting f  where e and f are two distinct training examples. 
　we restrict ourselves to attribute-value logic  where hypotheses are conjunctions of selectors  att  interval  and  att = value  respectively built on numerical-and nominal attributes att. in this language  the upperbound of d e f  is the disjunction  the set  of all maximally general selectors covering e and rejecting f  termed maximally discriminant selectors  michalski  
1 . for instance in table 1   is the maximally discriminant selector built on attribute attz. by abuse of notations  d e  f  is equated to its upper bound  hence characterized with linear complexity in the number of attributes. 
att1 att1 att1 att1 att1 att1 e f yes no 1 1 1 1 
1 red 
  blue 
　let h be a conjunction of selectors  and assume that h belongs to the version space; by definition h is complete  covers all positive training examples  and consistent  rejects all negative examples . it follows that h belongs to  is subsumed by at least one selector in  d e  f  for e ranging over the set of positive examples and f ranging over the set of negative examples: 

　inversely  let g denote the above conjunction of d e  f . one can show with no difficulty that all maximally general  conjunctive  hypotheses in g are complete and consistent. the version space and g thus have same upper bound  and g is expressed in dnf form with quadratic complexity in the number of examples and linear complexity in the number of attributes. 
　other limitations of version spaces due to noisy and sparse data  or disjunctive target concepts  are dealt with by using parameterized combinations of the d e  f   sebag  1 . 
1 	learning m-of-iv concepts 
this section describes new attributes derived from the 
version space primitives and uses them to reformulate the problem domain. an overview of the monkei algorithm is then presented. 
1 	separating concepts 
let denote the attribute-value logic description of the problem domain  hypothesis and example language   defined by numerical and/or nominal attributes. 
　let e and f be two training examples  and let d e  f  be constructed as in section 1  as the set  disjunction  of n maximally discriminant selectors seli-
	sebag 	1 

from d e  f  we derive a mapping he f from the problem domain c onto the set of integers: for each example or hypothesis u  simply counts the number of selectors seli that are satisfied by  covers  u.  or ft when no ambiguity can arise  maps c onto  1  n : it defines a new computable attribute of the domain. 
att1 att1 att1 att1 att1 att1 he f e f yes no 1 1 1 1 
1 red 
    
blue 1 
1 !   u   1 1 1   blue 1 i table 1: attribute he f maps  onto  1  
　consider the selectors built on attribute ft   e.g.  ft = af    termed concepts. by definition  concept  ft = m  corresponds to the m-of-n concept of selectors  an example u satisfies  ft = m   equivalently  h u  = m  iff u satisfies exactly m selectors among the seli. 
　by construction  concepts  h = 1  ...   ft = n  are disjoint and define a partition of the examples. 
　attribute he f can conveniently be viewed as a new discrete  dimension  of the problem domain. this dimension separates e from f  as these examples belong to opposite regions along this dimension: e belongs to since e satisfies all and f belongs to 
since f satisfies none of the seli. 
1 	properties 
concepts  ft = m  constitute a very flexible hypothesis language  ranging from conjunctive hypotheses  e.g.  ft = n  is conjunctive   to xor patterns  e.g.  ft = 1  corresponds to the xor of selectors seli . 
　note that concepts  h = m  can cover examples that are syntactically very different  inducing thereby unusual clusters of examples; this might hopefully decrease the number of  rare  cases. 
　further  the distribution of the examples along dimension ft  i.e. the number of examples covered by  ft = m   for m  1  n   shows an interesting characteristic. if the initial attributes were independent  with probability pi for any example to satisfy a given selector seli  the 
centra! limit theorem shows that the distribution of examples along ft tends toward a gaussian law of mean  when n goes to infinity  the approximation being considered accurate for n   1  pitman  1  . the concepts  ft = m  in the tails of the distribution  m close to 1 or n  would then cover few or no examples. 
　as could have been expected  experiments show that the initial attributes are not independent: concepts in the tails of the distribution happen to cover a significant number of examples. further  these concepts happen to be consistent  i.e. all or most covered examples belong to the same class. as concepts   h e   f = m  are  sufficiently often  significant and consistent  a simple learning strategy is to exhaustively explore these concepts  and retain all those that are sufficiently good. further instances u are then classified by a majority vote of the concepts covering u. 
　the number of such new attributes he f is quadratic in the number p of examples; evaluating each he f on 
1 machine 	learning 
the training set is linear in the number of attributes and in the number of examples. hence  the complexity of the exhaustive strategy is cubic in the number of examples  making it unrealistic to handle medium-to-large datasets. 
1 	o v e r v i e w of monkei 
tunable complexity is achieved in monkei by considering a user-supplied number d of primitives d ei  fi   where ei and fi are iteratively selected in the training set. the pairs of examples  ei fi   called seed examples  are sampled by a stochastic boosting mechanism  based on the notion of margin. within a majority votebased classifier  the margin of a training example e is the number of votes for the right class  minus the number of votes for the other class  or the second best class  in case of a multi-class discrimination problem   freund and shapire  1 : e is misclassified iff its margin is negative. 
　in monkei  one of the seed examples  say ei  is selected with uniform probability among low-margin examples  i.e.  whose margin according to the current theory th of the system is less than 1 through the following . the other seed example is selected with uniform probability among the training examples that do not belong to the same class as the first seed example. 
　after a seed pair  ei  fi  has been selected  the corresponding attribute ft is computed for all training examples  and its domain  1  n  is discretized in k intervals i1 ......... ik  concepts  h = m  and  ft = m+1  are merged if they cover examples in the same class . each concept  is evaluated; it is added to the current theory if it covers more than a prescribed number a of examples and admits less than a prescribed rate e of exceptions1; ft is then termed dimension of the domain. 
algorithm m o n k e i 
dimension = 1; th = {}; idlesteps = 1 
while  dimension   d  
draw e among the low margin examples 
if no such e is found  return th 
	draw f s.t. 	class f  	dass e  
construct attribute he f 
discretize its domain into intervals i1 ..ik 
for each ik 
if 
to th 
	if at least one 	is selected  
idlesteps = 1 
	increment 	dimension 
else increment idlesteps 
   if idlesteps   t return th end while return th 
it might happen that many pairs of seed examples 
 ei  fi  are considered and no concept is retained; these 
　　1  we further require the concept to cover at least one so far misclassified or unclassified example. 

steps are named  idle steps . the number of idle steps increases as learning proceeds  as there are less and less low-margin examples  and less chances to cover previously misclassified examples. 
　monkei repeatedly selects pairs of examples until the desired number of dimension d is reached  or the number of consecutive idle steps reaches a threshold t  set to 1 through the following . 
　complexity. the worst case learning complexity is 1{d x t x n x p   where d stands for the user-supplied number of dimensions  t is the maximal number of consecutive idle steps allowed  p is the number of training examples  and n is the number of initial attributes. at most d x n concepts are learned. as classifying u requires to compute hei  fi u  only once  with complexity o n    the classification complexity finally is o d x n . 
1 	discussion 
monkei explores a set of concepts that express some relations of the initial attributes  e.g. by counting the number of particular features that are simultaneously satisfied. compared to the relational patterns used in mrp  perez and rendell  1   the difference is that the concepts explored here are automatically and polynomiauy derived from the examples  and no preliminary discretization of numerical domains is required. 
the constructed m-of-iv concepts are simply evaluated; as opposed to mrp  perez and rendell  1  or id1-of-1  murphy and pazzani  1   monkei does not consider their combination  conjunction . 
　monkei must also be compared to support vector machines  svms   scholkopf et oi.  1   which reformulate the problem domain using kernel functions derived from the examples. in the new description space  svms look for a separating surface optimizing the minimal margin over the training set; the optimization proceeds by pruning all examples but those with a minimal margin  called support vectors. 
one major difference between svm and monkei is that every kernel function used in svms depends on a single example; every  kernel function  h e   f used in monkei depends on a pair of examples. another difference is that svms start with a large set of kernel functions which is gradually pruned along the optimization of the minimal margin. in opposition  monkey gradually grows the set of kernel functions until a satisfactory margin has been found for all examples - or the computational resources have been exhausted. 
like boosting algorithms  preund and shapire  1   
monkei pays more attention to misclassified examples than to others; if naively done  this strategy might lead to rewarding noisy examples. this drawback is limited in monkei as all constructed concepts are independently validated: only sufficiently good concepts can be added to the current theory. 
　one main limitation of monkei is that it is unlikely to deal with irrelevant attributes. for any example u  let h u  be decomposed as where respectively denote the number 
of discriminant selectors based on relevant and irrelevant attributes that are satisfied by can be viewed as some kind of noise which blurs the information contained in making it unlikely to discover the worthwhile concepts additional heuristics need be designed to overcome this problem. 
　another limitation  discussed in section 1  regards the intelligibility of the theory produced by monkei. 
1 	experimental validation 
since a universal learner does not exist  wolpert and macready  1   experimental validation should make clear when a new learner is worth using. 
1 	benchmark problems 
we first consider six problems  artificial waveform 
 breiman et al.  1   glass  balance  tic-tac-toe  monks1  vehicle  from the irvine repository  c. blake and merz  
1   illustrating various types of learning difficulties  noisy data  ill-distributed classes  many classes  many disjuncts . 

table 1: datasets and reference results. 
　table 1 recalls the characteristics of the datasets and the previous best results  obtained  as far as we know  by local cascade generalization  gama  1   g-net  anglano et al.  1  and hci  wnek and michakki  1  using a 1-fold cross-validation. 
　the experiment goal here is to check whether competitive results can be obtained for a reasonable number of dimensions. the number d of considered dimensions is chosen from the interval  1 . other parameters of monkei are frozen to their default value: the minimal number a of examples covered by a concept is 1  the maximal percentage of exceptions e is 1%. 
　on the balance  glass  tic-tac-toe and vehicle problems  monkei accuracy is evaluated by 1-fold crossvalidation  averaged over 1 independent splits of the dataset as recommended by  dietterich  1 . 1 independent runs are executed for each split of the dataset  as recommended when evaluating a stochastic algorithm. on the monks1 problem  the accuracy is evaluated on the 1-example test set  averaged on 1 independent runs . on the waveform problem  the predictive accuracy is evaluated on a 1-examples test set  averaged over ten 1-examples training sets  1 independent runs are executed for each training set . 
cpu times are given in seconds on a pentium 1. table 1 shows the lowest number of dimensions d allowing monkei to match or outperform the state-of-theart results on the balance  glass  vehicle and monks-1 
	sebag 	1 

problems. #c denotes the number of af-of-iv learned concepts. 

for the lowest d matching reference results 
　table 1 illustrates how the performance depends upon the number of dimensions  on the waveform and tic-tactoe problems. monkei matches the optimal theoretical accuracy on the waveform problem  1%   but falls behind g-net on the tic-tac-toe problem. some care must be exercised when comparing the results  since the reference results were obtained according to a 1-fold cv against a 1-fold cv for ours  meaning that monkei is evaluated with a more pessimistic estimate . 

table 1: monkei results  dependence upon d  
generally  good results can already be achieved using comparatively small values of d  d   1 for the waveform problem  d   1 for the vehicle problem  d   1 otherwise . 
1 	a real-world p r o b l e m 
the real-world predictive toxicology evaluation  pte1  problem is nicely motivated and detailed in  srinivasan  1 . this problem both is very inspiring and appears difficult for experts  learners  and even learners cooperating with experts  srinivasan  1 . 
　this dataset includes 1 examples. the  test  set includes 1 examples which were unknown at the beginning of the pte challenge; 1 have been since diagnosed and their class was available by november 1. foil  quinlan  1  and progol  muggleton  1  consider a first-order description of the problem  other learners consider a 1 attributes description. a comprehensive presentation of the descriptions and the reference results is found in  srinivasan  1 . 
| representation  | algorithm accuracy relational foil 
progol 1% 1 % propositional c1 prune 
c1 rules 
c1 rules 1- hand 1% 
1% 
1% table 1: reference results on pte1 
given 	the 	practical 	importance of the 	problem  
monkei was run with unbounded resources  meaning that all primitives d e  f   for e and f respectively ranging over the positive and negative examples  were considered. the cpu time is 1 minutes on a pentium 
ii-1. 
1 	machine learning 
　the selection of concepts thus only depends on the minimal number of covered examples a  chosen from the interval  1  and the maximal rate of exceptions e chosen from the interval  1% . 
　the results  table 1  demonstrate the good performances of monkei  and its stability with respect to parameters a and . #c denotes the number of m-ofn learned concepts. 
♀ a  test  training # c 1 
1 1 1% 
1% 1% 
1% 1 1 
1 1 1% 
1% 1% 
1% 1 
1 1 
1 1 
1 1% 
1% 1% 1% 1 table 1: unbounded monkei on ptes 1 	discussion 
in the field of scientific discovery  a major drawback of monkei is that it fails to produce an intelligible theory: this failure is basically due to the fact that it handles dnf concepts  which are generally considered to be less intelligible than standard rulesets  i.e. cnf concepts. still  an intelligible interface can be constructed on a dnf-based system  khardon and roth  1 . 
　in monkei  as in svms   the system can answer the user's questions about the typicality of examples. the typicality of an example  interpreted with regard to its margin  can be computed  instead of  explained  from the theory. the experiment design might take advantage of this information  to preferably run physical experiments corresponding to borderline  untypical  examples. 
　the logic-based formalism of monkei can facilitate the detection of attribute dependencies  as it allows one to focus on the subsets of examples that are covered by the concepts  and the selectors that are simultaneously satisfied by the examples. 
　one might also focus on the selectors that are never simultaneously satisfied in these conditions  and search for xor subconcepts. the favorable case is when all m-of-iv concepts can be expressed as conjunctions of simple xor subconcepts; the theory would then directly be intelligible. 
1 	conclusion and perspectives 
the approach investigated in this paper considers dnf concepts as hypothesis language; the advantage of the language is its high expressiveness  both conjunctive and xor expressions are dnf concepts   and a low computational complexity. 
　within the version space framework  we define a particular set of dnf concepts  polynomially characterized and evaluated from the examples. this set is explored by stochastic sampling  in order to let the user control the learning cost  any-time algorithm . all explored concepts that are  sufficiently good  are retained. this strategy demonstrates its efficiency on several irvine 

problems  as it matches or outpasses the previous best results for quite a limited amount of resources. on the real-world pte1 problem  monkei achieves outstanding results with unlimited resources  still in reasonable time. 
   this work opens up several perspectives. one already mentioned is to develop an  intelligible  interface for monkei. another perspective is to upgrade monkei to first order logic  using the polynomial approximations of the version space relational primitives developed in  sebag and rouveirol  1 . 
last  we shall examine in more detail the relationship between monkei and support vector machines. using d pairs of seeds  ei   f    monkei maps the initial training set onto n d ; this makes it possible to apply support vector machines  and determine the support vectors examples vj. an interesting question is how  if any  the support vectors vj are related to the seeds ei and fi. 
a c k n o w l e d g m e n t s 
many thanks to yves kodratoff and fabien torre  l r i   and marc schoenauer  ecole polytechnique  for their support and many discussions. thanks also to lise fontaine and the anonymous referees  for their help in making the paper readable. 
