
the association of perception and action is key to learning by observation in general  and to programlevel task imitation in particular. the question is how to structure this information such that learning is tractable for resource-bounded agents. by introducing a combination of symbolic representation with bayesian reasoning  we demonstrate both theoretical and empirical improvements to a general-purpose imitation system originally based on a model of infant social learning. we also show how prior task knowledge and selective attention can be rigorously incorporated via loss matrices and automatic relevance determination respectively.
1 introduction
program-level imitation  byrne and russon  1   or the acquisition of behavioural structure from observation  is an under-researched field in ai. in robot ai  much effort has rightly been directed toward action-level imitation; the reproduction of movements involving a fine degree of motor control. indeed  this highly complex and difficult task needs to be solved by a robot before it is able to acquire structural data. however  by using 'intelligent' virtual environments which implicitly deal with low-level actions  we can gain access to a rich class of higher-level problems. unreal tournament  digital extremes  1  is an example of such an environment  and it is popular with those few looking at this problem  thurau et al.  1; le hy et al.  1 . it is also the domain of choice for the system that underpins this paper: coil  wood and bryson  1 .
　in this paper we demonstrate how a formal bayesian frameworkcan be incorporatedinto a complex modularlearning system. through this we widen its potential applicability in theory  significantly improve its learning performance in practise  and add natural extensibility to the system via tried and tested bayesian methodology. our experiments also enable us to examine the broader issue of the combinatorial complexity of social learning. social learning is an important mechanism for acquiring intelligent behaviour - arguably it accounts for the massive accumulation of culture and artifacts seen in humans compared to our nearest biological relatives  the other apes. understanding the complexity characteristics of this task is key to both understanding human intelligence and harnessing the approach for ai.
1 system overview
coil is an adaption to generic imitation learning of roy's language learning system  cell. cell is one of the most convincing examples to date of real-time interactive social learning. it enables a robot to learn the names for various toys using the same sorts of input as an infant. both cell and coil are detailed elsewhere  roy  1; wood and bryson  1 ; here we give a skeletal overview of coil  to clarify those parts most relevant to the extentions described later.

figure 1: the inputs and outputs of each stage of coil  wood and bryson  1 .
　the learning part of coil has five constituent stages of processing  see figure 1 . it is designed to function embedded in an imitator agent observing a conspecific expert agent executing a given task in a shared environment. the input to the first stage consists of the incoming raw data from the imitator's sensors. during feature extraction  these data are directed into separate channels which are one of two types: action channels receive data pertaining to the actions taken by the expert  and perception channels receive data pertaining  but not identical  to the perceptual state of the expert. once in channels the data are segmented into a- and p-events respectively  and then further into a- and p-subevents according to a set of pre-programmed triggers; this is event segmentation. the co-occurrencefilter then simply binds cooccurring a- and p-events together as ap-events and shunts them to a queue called stm  short term memory . whenever a new ap-event arrives in stm  it is compared with each of those already there in turn. the recurrence filter scans for co-occurring a- and p-subevent pairs and  if any are found  binds them as a motivation-expectation or m-e candidate and stores them in mtm  mid term memory . finally  the mutual information filter calculates a mutual information score for each m-e candidate using a somewhat complex algorithm not described here  see  wood and bryson  1  for details . those candidates whose scores exceed a pre-determined threshold are saved in ltm  long term memory as m-e items.
　the m-e items themselves represent observed perceptionaction pairs  though note the original agent may not have had this model  bryson and wood  1    and can consequently be used to create a reactive imitative behaviour. the imitator's sensors define a perception space which it will move through as the task surroundings change. m-e items can be used to create maps from regions of this space onto the imitator's action repertoire. in practise  the imitator searches the perception chunks stored in ltm for those that match its current perceptual state. if matches are found  the highest priority match is selected  ranked by mutual information score   and then the associated action chunk is returned and executed.
1 system shortcomings
although coil has had some previously-reported success in performing imitation tasks  we have discovered a number of flaws which prevent it from scaling to more difficult problems.
representations
coil's primary weakness results from trying to represent general action and perception with the same system cell uses for speech and vision. cell receives continuous microphone data  later converted into discrete phonemes during event segmentation. coil receives continuous action data which is parsed into discrete action segments. the crucial difference is between the spaces in which these discrete objects lie. we omit the details of roy's metric here  see  roy  1  p. 1    but intuitively both phonemes and shapes can have infinite variation and can be mapped into a continuous space using histograms where the notion of 'nearness' is relatively well-defined. in contrast  the limited set of executable actions1 required for an imitation task lie in a discrete space where 'nearness' is not only hard to define but not a particularly useful concept. for example  how far is jump from turnright  and what use would that information be anyway  perceptual channels are similarly unrelated since we are not simply learning to respond to different shapes  but are interested in different proprioceptive / physical positioning as well as more thoroughly categorised 'visual' stimuli  e.g. identifying an object vs. an agent . these comparisons point toward using a more thoroughly discrete representation for both actions and perceptions throughout coil.
　an obvious solution might appear to be to reduce the level of abstraction at which the learning and action selection occurs. for example  by descending into finer-grained muscle and motor position spaces we would reclaim continuity. there are two problems here: firstly  both of these spaces have unusual discontinuities which lead to the problems of inverse kinematics which ensnare traditional robotics. secondly  the complexity inherent in the remaining imitation  and subsequent action selection  process demands a reduced space of more generic operators. thus rather than learning 1 different turn-right commands  each varying by only one degreeof turnarc  we simply learnto initiate turnright  and then check for stopping criteria. note that this solution is more robust to uncertain motion and sensing  assuming the stopping or homing criteria are perceivable within some range of degrees during the turn.
algorithms
the performance of coil is further affected by certain characteristics inherited from the cell algorithms. firstly  the recurrence filter is designed to search for events recurring only within a short history of observations. this is appropriate for aspects of infant learning that assume frequent repetition  e.g. of word/object pairs  but task learning in general may have arbitrarily long gaps between recurring perception/action pairs. the problem with simply increasing the size of the recurrence filter window is that every arriving ap-event is compared to all those already present  which clearly results in combinatorial problems as the size of the queue increases. the mutual information filter has a similar scaling problem. it has cubic complexity in the number of elements in mtm  which in general could grow without bound  and exponential in the number of monitored channels  which grows with the complexity of the task domain . additionally  the probabilities used in the calculation of the mutual information are frequentist  as opposed to bayesian  approximations  and are consequently very sensitive to noise caused by small frequency counts  ie. rarely observed events . roy tackles this by interpolating these probabilities with priors  but the choice of prior mass parameter required by this technique can have significant effects on the resulting probabilities  particularly if many of the events in question are infrequent. this may well be the case for our applications  so we desire a more robust method.
1 model
we therefore wish to implement a learning algorithm which can operate within the coil framework and minimise the potential problems outlined above. specifically  it should be:
scalable - both in terms of memory requirements and learning complexity.
incremental - so that all observations are used and knowledge is consolidated in a natural way.
rigorous - having output that is interpretable and justifiable through established mathematical argument.
robust - not prone to failure when processing unusual  unforseen or extreme events.
　it would also be preferable for this algorithm to be sufficiently general-purpose to be applied to other similar learning problems in this field. the bayesian framework would seem a good place to begin  as it allows each observed event to update prior belief in an efficient and well-defined manner  bishop  1  p. 1 . however  there are many algorithms which make use of it  so the question becomes which one to use in order to obtain the desired posterior beliefs. we chose a multi-layer perceptron  mlp  architecture which  given a certain set of constraints  provides bayesian posterior probabilities as its outputs. we describe this specific configuration in the next section.
1 network architecture
the parts of coil most prone to the kinds of problems described above are the recurrence and mutual information filters. to replace these  we therefore require that the new algorithm receive perception-action data from the co-occurrence filter and output a behavioural map. in mlp terms  this map looks like a classifier network  which receives perceptual data and assigns it to the appropriate action class. to allow an mlp to learn such a classification  we must translate the observed perception-action pairs into an appropriate set of training examples. each example should consist of a set of input variables and a set of target variables. in this case  the input variables should correspond to the observed perceptual state of the expert  and the target variables should correspond to the observed action. the question is  what encoding to use for these variable sets 
　as explained above  we are assuming for now that perceptual categories  such as itemleft and noitemvisible  have no implicit relationship to each other. they do not lie in a metric space and so cannot be represented by ordinal variables. we therefore use a purely categorical 1-of-c encoding for the input  bishop  1  p. 1 . suppose a given perception channel has n possible symbolic states. then symbol i can be represented by a vector of n bits where only the ith bit is set  1  = i  = n . if there are m perception channels then the concatenation of m such vectors produces the complete required binary input vector of length n1 + ... + nm. if there are k observable actions  then this is equivalent to a classification problem with k target classes  and we can create one output node for each class. we have already stated that it is desirable for these outputs to have a bayesian interpretation as posterior probabilities of action class membership for a given perceptual state. this is achievable using a softmax activation function for the network output units  bridle  1  and minimising a cross-entropy error function for the network weights  bishop  1  p. 1 . after some empirical testing  we chose to include three hidden units in the network  although the results were not particularly sensitive to this choice. we are currently looking into bayesian model selection techniques for selecting the number of hidden units  see section 1 . given the above node structure  the network we used was fully connected with a simple feedforward structure  as shown in figure 1.
outputscorrespondtoactionclassprobabilities

binaryinputsgroupedbyperceptionchannel
figure 1: diagram of mlp architecture. the binary input vector is a concatenation of 1-of-c encoded symbols for each perception channel. there are three hidden units with softmax activation to the outputs  which consequentlycorrespond to posterior probabilities of action class membership. arrow shows direction of forward propagation; biases are shown explicitly for clarity.
　the network training scheme uses bayesian regularisation with separate hyperparameters for each of four weight groups: first-layer weights  first-layer biases  second-layer weights and second-layer biases  bishop  1  p. 1 . training was by back-propagation for up to 1 cycles of scaled conjugate gradient search  fewer if convergence occurred beforehand   followed by re-estimation of the hyperparameters using the evidence approximation technique  mackay  1b . this cycle of re-estimation and re-training was carried out 1 times. the test data for the network consisted of querying all possible combinations of perceptual state  to evaluate the most probable action assigned to that state by the classifier. finally  these posterior probabilities were marginalised according to the observed data  mackay  1a . although this last step does not affect the most probable action class  it can have significant effects if loss matrices are added  see section 1 below .
　empirical evidence showing the increased learning performance of the new algorithm can be found in the next section. before we examine this however  we review the theoretical criteria set out at the beginning of this section and ask if they are satisfied.
scalable - as far as learning complexity is concerned  network training time increases only linearly with the number of observed events  as compared to the combinatorics of the original algorithms  see section 1 . also  the mlp is a function which requires storage equal to the number of network weights as opposed to a potentially boundless number of stored m-e items.
incremental - due to this increase in efficiency and the belief accumulation property of the bayesian framework  every observation can be taken into account and consolidated with prior knowledge.
rigorous - the fact that we can interpret the mlp outputs as posterior probabilities is a well-proven property of this type of network and totally independent of the domain in which we're working.
robust - the parametric re-estimation carried out after each network training cycle serves to minimise any problems caused by local minima relating to  say  weight initialisation.
1 experiments
to evaluate our new model we tested it against the same data collected for the original coil experiments. these data were gathered in unreal tournament  ut   a commercially released  multi-player 'first person shooter'  game   digital extremes  1 . as the term suggests  the user has an agent's-eye view of the game and direct  real-time control of an avatar's actions. ut also supports remote control of agents by sending commands to the game server over a network. this provides a framework for allowing external programs to direct an agent's actions. as such  ut provides a viable platform for testing strong ai  since humans and ai can compete and interact in a real-time  spatially situated domain. the game server sends two categories of sensor data back to the client. the first is synchronous: at regular intervals the client is informed of the agent's status  e.g. health  ammo  current weapon  etc . the second is asynchronous: for example whenever a wall is bumped  a footstep is heard or damage is taken.
　these data when viewed as a whole are a highly dynamic  high-dimensional mixture of categorical and continuous variables  akin to sensory data acquired in the real world. other similarities include physics simulation  such as collisions  gravity  and sound and light transmission. ai agents'  bots'  sensor data are incomplete in the sense that only a reduced subset of the game variables are observable; the bots have limited virtual sensors. for example  the imitator cannot know the health state of the expert  although this may well affect the expert's choices. the environment contains many independent features  each of which could be represented in a number ways. thus the problem of assimilating the behaviour of another agent via observation is far from simple. the first three stages of coil each serve to reduce the complexity of this problem  see section 1  so that it arrives at the inputs of our new algorithm in a tractable state.
　in short  ut agents deal with real-world temporallybounded cognitive constraints  not least the combinatorial complexity of learning  which makes them ideal test subjects for our research.
1 task 1
the first task involved collecting health vials  one of many so-called 'pickups' available in ut  from various locations within a game map. the data was originally received and processed by a coil system embedded in an ai-controlled bot  programmed to observe from within the environment a human-controlled bot carrying out the task. we used three different 'tactics' during our demonstrations:
cw tend to turn clockwise if no vials are visible.
acw tend to turn anticlockwise.
mix no fixed tendency.
ten trials for each tactic were carried out  for a total of 1 trials each lasting approximately 1 seconds. during the original experimental runs  the data arriving in channels  ie. post feature extraction  were saved prior to further processing. this allowed us to compare the new learning algorithms on the same data sets. the mlp replaces only the recurrence and mutual information filtering stages of coil  with the first three stages remaining unchanged. for further performance comparison  we also fed this data into a decision tree algorithm  c1  quinlan  1 .
　our performance metric derives from our representation of behaviour as a mapping from discrete regions of perceptual space to discrete actions. we defined the behaviour on which we based our demonstrations as the 'ideal' map from perception to action for this task. the proportion of the learned behaviour which matches the ideal allowed us to assign a 'percentage correct behaviour' score to each trial. the results comparing the three techniques are shown in figure 1 a . as can be seen from the figure  the mlp  grey bars  generated universally perfect behaviour for this task  correcting all errors made by coil's native algorithms  black bars . interestingly though  c1  white bars  also performs a perfect classification
1 task 1

	 a  task 1	 b  task 1	 c  ard
figure 1: comparative performance of the different learning algorithms over a variety of tasks. the black bars correspond to the original coil algorithms  the white bars correspond to c1 and the grey bars correspond to the new mlp classifications. in the second study  the right-hand grey bar corresponds to the system moderated by a loss matrix  see text for details . the third study shows the automatically determined relative importance of two perception channel input sets. error bars show thethe second task required the expert to locate and destroy enemy bots in an environment which also contained an equal number of friendly bots. each trial lasted as long as it took for this task to be completed; typically around 1 seconds. tactics cw  acw and mix were used analogously to task 1  again with ten trials each for a total of 1. all algorithms and training methods remained the same for this task as for the previous one. results are summarised in figure 1 b . the mlp  left-hand grey bar in each group  provides a small but not signifcant  t-test  p = 1  increase in performance from both coil  black bars  and c1  white bar   which for this task performs no better than coil. upon inspection of the data it is clear that for a majority of the trials  the associations that would be necessary to form a fully correct behaviour are never observed. specifically  most of the misclassifications are made for turning toward an enemy; in the absence of such associations the imitator tended to adopt the dominant turning direction observed and consequently err in either the enemyleft or enemyright state. the other common mistake was to fire before the enemy was centred in sights; both were made less by the network than the other standard error of the means.
algorithms. performance is further improved  this time significantly  t-test  p = 1   by introducing a loss matrix to represent prior task knowledge  right-hand grey bars in each group ; one of the extentions we go on to talk about in the next section.
1 bayesian extentions
as discussed in section 1  the probabilistic interpretation of results possible from the network model is highly desirable. this also allows other bayesian techniques to be applied to the network and its outputs. we now discuss two such techniques and show preliminary results.
loss matrices
in general decision theoretic terms  a loss matrix describes the relative penalties associated with misclassifying a given input  bishop  1  p.1 . in this case we can describe the matrix as having elements lkj representing the loss resulting from assigning an action aj to a given perceptual state when in fact ak should be assigned. decisions are then made by minimising the risk  or equivalently by using the following discriminant to classify a given perceptual state x:

where p ak|x  can be obtained from the  marginalised  network output probabilities. to demonstrate this we applied a simple loss matrix to the networks generated during task 1:
		 1 
where a1 is the fire action  a1 is turnleft and a1 is turnright. this matrix specifies that 'accidentally' firing instead of correctly turning should incur five times greater a penalty than any other misclassification1. informally  one would expect this to be equivalent to giving the imitator the instruction  only shoot if you're sure   prior to acting. the results of applying this matrix to the task 1 network outputs are shown in figure 1 b   right-hand grey bars in each group . the improvement  as expected  is due to fewer cases of firing before the enemy is in position. although this is a relatively simple example of the application of this technique  it does demonstrate the ease at which prior knowledge can be formally incorporated into the model  and how it could be systematically altered to test the effect on output behaviour.
selective attention
it is likely that for a given task  only a small subset of the full available perceptual state will be required for good performance. so far in this paper  this subset has been chosen by hand  but the mlp model can enable us to make this selection automatically  within a sound bayesian framework. this automatic relevance determination  neal  1  p. 1  is achieved by using a different hyperprior. instead of grouping the weights such that there are four independent regularisation hyperparameters  the weights associated with each input have their own hyperparameter. these coefficients vary in proportion to the inverse posterior variance of the weight distribution for that input. thus if a given input has a high coefficient value  the weight distribution for that input has a low variance and the input has little bearing on the ultimate classification: the input has low relevance. using a similar training and re-estimation scheme as described earlier  these hyperparameters can be used to determine the relative relevance of the different network inputs  which in this case correspond to different aspects of the environment. thus we have a method for automatic attention selection within a broader set of channels.
　to test this theory  we added a perception channel to task 1 which identified the absolute direction the imitator was facing  represented by one of four 'compass' symbols. one would expect this set of inputs to have lower relevance than the existing channel relating to the relative bearing of the vials. we carried out a further ten trials under much the same conditions as task 1. the results are summarised in figure 1 c . as expected  the coefficient values for the inputs associated with the new channel are significantly higher on average than the inputs associated with the original channel. the exception to this is the fourth bearing input which  bearing in mind the 1-of-c encoding  was fully determined by the state of the first three inputs. given these inputs converged to high relevance  the fourth was correctly deemed of very low relevance. in principle  this technique could be used to prune perception space down to make local task learning more accurate and efficient. to allow this  some kind of 'relevance threshold' would have to be chosen  which may well vary from task to task. the method for making such a choice remains an open question.
1 conclusion
we have presented a new and significantly improved version of the coil system for program-level imitation. in doing this  we have shown that taking advantage of bayesian reasoning  along with the large body of systems-independent research that comes with it  can allow for improvement both in the theoretical bounds of a system and its empirical performance. this may require some representational transformation  but in return unlocks an arsenal of well-established techniques that can be brought to bear on practical problems in novel ways. also  the techniques themselves may suggest new avenues of research  and therefore provide the system with natural extensibility.
　the fundamental issue of interest to us  motivating and examined in both this paper and its parent  is the combinatorial complexity of social learning. in this case study  we have compared coil's perception-action association storage method with that of learning a function from perception to action. the former is both more memory-intensive and more search-intensive due to coil's native representations and algorithms  but is in fact an arbitrarily flexible specification for behaviour. in practise for simple tasks  and in theory for more complex tasks  the functional approach is more efficient. however  it is likely that there will come a point where the task domain becomes too dynamic and hierarchical for behaviour to be adequately specified by a single mlp  or indeed by any 'stock' function.
　our latest work suggests two possible alternative solutions: firstly a hierarchical system of functions  monitoring local task domains at the lower level and arbitrating between task domains at the higher. this would still require an external process to exchange information between functions. secondly  we could apply the hierarchical model to an improved association storage method. by using perceptual prioritisation and action concatenation we can carefully control the trade-off between performance and complexity while at the same time capitalising on the flexibility of this approach.
acknowledgements
the research presented is partially funded by an epsrc dta studentship  administered by the university of bath department of computer science.
