
recent scaling up of pomdp solvers towards realistic applications is largely due to point-based methods which quickly convergeto an approximate solution for medium-sizedproblems. of this family hsvi  which uses trial-based asynchronous value iteration  can handle the largest domains. in this paper we suggest a new algorithm  fsvi  that uses the underlying mdp to traverse the belief space towards rewards  finding sequences of useful backups  and show how it scales up better than hsvi on larger benchmarks.
1 introduction
many interesting reinforcement learning  rl  problems can be modeled as partially observablemarkovdecision problems  pomdps   yet pomdps are frequently avoided due to the difficulty of computing an optimal policy. research has focused on approximate methods for computing a policy  see e.g.  pineau et al.  1  . a standard way to define a policy is through a value function that assigns a value to each belief state  thereby also defining a policy over the same belief space. smallwood and sondik  show that a value function can be represented by a set of vectors and is therefore piecewise linear and convex.
﹛a promising approach for computing value functions is the point-based method. a value function is computed over a finite set of reachable belief points  using a backup operation over single belief points - point-based backups  generating 汐-vectors. the assumption is that the value function would generalize well to the entire belief space.
﹛two of the key issues in point-based methods are the choice of the set of belief points and the order of backups. a successful approach for addressing both of these issues uses trial-based value iteration - simulating trajectories and executing backups over the encountered belief-states in reversed order  see e.g.  bonet and gefner  1  . such methods use some type of heuristic to find useful trajectories.
﹛heuristic search value iteration  hsvi -  smith and simmons  1   is currently the most successful point-based algorithm for larger domains. hsvi maintains both a lower bound  v  and an upper bound  v‘  over the optimal value function. it traverses the belief space using a heuristic based‘ on both bounds and performs backups over the observed belief points in reversed order. unfortunately  using v‘ slows down the algorithm considerably as updating v‘ and computing upper bound projections  v‘ b   are computationally intensive. however  this heuristic pays off  as the overall performance of hsvi is better than other methods.
﹛in this paper we suggest a new method for belief point selection  and for orderingbackups which does not use an upper bound. rather  our algorithm uses a search heuristic based on the underlyingmdp. using mdp informationis a well known technique  especially as an initial guess for the value function  starting with the qmdp method of  littman et al.  1    but did not lead so far to very successful algorithms. our novel algorithm - forward search value iteration  fsvi  - traverses together both the belief space and the underlying mdp space. actions are selected based on the optimal policy for the mdp  and are applied to the belief space as well  thus directing the simulation towards the rewards of the domain. in this sense  our approach strongly utilizes the underlying mdp policy.
﹛we tested our algorithm on all the domains on which hsvi was tested  and it both converges faster than hsvi and scales up better  allowing us to solve certain problems for which hsvi fails to converge within reasonable time.
1 background and related work
we review the pomdp model and the associated notation  and provide a short overview of relevant methods.
1 mdps  pomdps and the belief-space mdp
 a markov decision process  mdp  is a tuple where s is a set of world states  a is a set of actions   is the probability of transitioning from state s to state s using action a  and r s a  defines the reward for executing action a in state s.
	a	partially	observable	markov	decision	process
 pomdp  is a tuple where s a tr r are the same as in an mdp  次 is a set of observations and o a s o  is the probability of observing o after executing a and reaching state s. the agent maintains a belief-state - a vector b of probabilities such that b s  is the probability that the agent is currently at state s. b1 defines the initial belief state - the agent belief over its initial state.
﹛the transition from belief state b to belief state b using action a is deterministic given an observation o and defines the 而 transition function. we denote where:

﹛while the agent is unable to observe the true state of the world  the world itself still behaves as an mdp  which we call the underlying mdp.
﹛in many mdp and pomdp examples the agent should either reach some state  called the goal state  where it receives a reward  or collect rewards that can be found in a very small subset of the state space. other problems provide varying rewards in each state.
1 value functions for pomdps
it is well known that the value function v for the beliefspace mdp can be represented as a finite collection of |s|dimensional vectors known as 汐 vectors. thus  v is both piecewise linear and convex  smallwood and sondik  1 . a policy over the belief space is defined by associating an action a to each vector 汐  so that represents the value of taking a in belief state b and following the policy afterwards. it is therefore standard practice to compute a value function - a set v of 汐 vectors. the policy 羽v is immediately derivable using: 羽v  b  =argmaxa:汐a﹋v 汐a ﹞b.
﹛v can be iteratively computed using point-based backup steps  efficiently implemented  pineau et al.  1  as:
backup b =argmaxgab:a﹋a b ﹞ gab	 1  argmax	 1 

﹛the vector representation is suitable only for lower bounds. an upper bound can be represented as a direct mapping between belief states and values. the h operator  known as the bellman update  updates such a value function:

	hv  b  = maxa qv  b a 	 1 
1 trial-based value iteration
synchronous value iteration assumes that the entire state space is updated over each iteration. asynchronous value iteration allows some states to be updated more than others  based on the assumption that value function accuracy is more crucial in these states. a well known form of asynchronous value iteration is trial-based updates  where simulation trials are executed  creating trajectories of states  for mdps  or belief states  for pomdps . only the states in the trajectory are algorithm 1 rtdp

1: initialize q s a  ↘ 1
1: while true do
1:	s ↘ s1
1:	while s is not a goal state do
1:	for each a ﹋ a do
1:	
1:	a ↘
1:	s ↘ sample from tr s a   

updated and then a new trial is executed. rtdp  barto et al.  1  is a trial-based algorithm for mdps  algorithm 1 .
﹛rtdp-bel - an adaptation of rtdp to pomdps was suggested by bonet and geffner  bonet and gefner  1 . rtdp-bel discretizes belief states and maintains q-values for the discretized belief states only. trials are executed over the pomdp mapping the real belief state into the closest discretized belief state which is then updated.
﹛an important aspect for the convergence speed of trialbased algorithms is a good heuristic that leads towards important states and updates. unfocused heuristics may cause the algorithm to spend much time updating useless states.
1 heuristic search value iteration  hsvi 
computing an optimal value function over the entire belief space does not seem to be a feasible approach. a possible approximation is to compute an optimal value function over a subset of the belief space. an optimal value function for a subset of the belief space is no more than an approximation of a full solution. we hope  however  that the computed value function will generalize well for unobserved belief states.
﹛point-based algorithms  pineau et al.  1; spaan and vlassis  1; smith and simmons  1  compute a value function only over a reachable subset of the belief space using point-based backups  equations 1- 1 .
﹛of this family of algorithms  hsvi  algorithm 1  has shown the best performance over large scale problems. hsvi  smith and simmons  1  maintains both an upper bound  v‘  and lower bound  v  over the value function. it traverses the belief space following the upper bound heuristic  greedily‘ selecting successor belief points where v‘ b    v b  is maximal  until some stopping criteria have been reached. it then‘ executes backup and h operator updates over the observed belief points on the explored path in reversed order.
﹛hsvi can be viewed as a trial-based value iteration for pomdps  even though the next belief state is not sampled but selected using the upper and lower bounds.
﹛even though hsvi is able to handle large problems  the h operator update and the interpolations used to find the value v‘ b  are computationally intensive  becoming more time consuming with each explored belief state. as such  hsvi spends much time maintaining the upper bound that is used only as a heuristic directing the belief space traversal.
﹛we note  however  that smith and simmons also present theoretical results for the convergence of hsvi  relying on the existence of the upper bound. as we suggest to remove the upper bound  these theoretical results no longer apply.
algorithm 1 hsvi
function hsvi
1: initialize v and v‘
1: whiledo
1:	explore b1 v‘ v‘ 
‘
function explore b v v‘ 
1: if stopping criteria have not been reached‘	then
1:	a  ↘ argmax  see equation 1 
1:	o  ↘ argmaxo v‘ 而 b a o     v 而 b a o  
1:	explore 而 b a  o   v v‘ 	‘
1:	add v backup b v  ‘ 1:	v‘ ↘‘hv  b 	‘

1 using the underlying mdp
using the underlying mdp optimal policy for the pomdp is a very well known idea and has been explored from many aspects in the past. littman et al.  littman et al.  1  suggest to use the optimal q-values of the underlying mdp to create the qmdp value function for a pomdp:
	qmdp b  = maxq s a b s 	 1 
a
many grid-based techniques  e.g.  zhou and hansen  1   initialize the upper bound over the value function using the underlying mdp. rtdp-bel initializes a q function for the pomdp using the optimal q function for the mdp.
﹛an important drawback of using the underlying mdp is the inability of the mdp to assess actions that gather information. agents in a partially observable environment occasionally need to execute actions that do not move them towards a reward  but only improve their state of information about the current world state  such as activating a sensor. in an mdp the world state is always known and therefore information gathering actions produce no additional value. agents relying on the qmdp heuristic  for example  will therefore never execute any such actions.
1 forward search value iteration
we propose a new algorithm  forward search value iteration  fsvi   using trial-based updates over the belief space of the pomdp. fsvi maintains a value function using 汐-vectors and updates it using point-based backups.
﹛the heuristic fsvi uses to traverse the belief space is based on the optimal solution to the underlying mdp. we assume that such a solution is given as input to fsvi in the form of a q-function over mdp states. this assumption is reasonable  as a solution to the underlying mdp is always simpler to compute than a solution to the pomdp.
﹛fsvi  algorithm 1  simulates an interaction of the agent with the environment  maintaining both the pomdp belief state b and the underlying mdp state s - the true state of the environment the agent is at within the simulation. while at policy execution time the agent is unaware of s  in simulation we may use s to guide exploration through the environment.
﹛fsvi uses the mdp state to decide which action to apply next based on the optimal value function for the underlying mdp  thus providing a path in belief space from the initial algorithm 1 fsvi
function fsvi
1: initialize v
1: while v has not converged do
1:	sample s1 from the b1 distribution
1:	mdpexplore b1 s1 
function mdpexplore b s 
1: if s is not a goal state then
1:	a  ↘ argmaxa q s a 
1:	sample s from
1:	sample o from
1:	mdpexplore 而 	 o  s  
1: add v backup b v  

belief state b1 to the goal  or towards rewards . as we assume that the value function of the underlying mdp is optimal  this heuristic will lead the agent towards states where rewards can be obtained.
﹛the trial is ended when the state of the underlying mdp is a goal state. when the mdp does not have a goal state we can use other criteria such as reaching a predefined sum of rewards or number of actions. if the goal is unreachable from some states we may also add a maximal number of steps after which the trial ends.
﹛fsvi  algorithm 1  is apparently very simple. its simplicity translates into increased speed and efficiency in generating belief points and updating the values associated with belief points. fsvi's method for selecting the next action is very fast  requiring to check only o |a|  values  mdp q-values  as opposed to any action selection method based on the current belief state. for example  rtdp-bel takes o |s|  operations for discretizing the belief state before it can select the next action and hsvi needs o |a||o||s||v‘|  operations  where |v‘| is the number of points in the upper bound  to compute the values of all the successors of the current belief state. as fsvi generates trajectories using forward projection  it is easy to determine good sequences of backups  simply going in reversed order. this ability is shared by hsvi  but not by other point-based algorithms such as perseus  spaan and vlassis  1  and pbvi  pineau et al.  1 .
﹛other algorithms  such as hsvi and rtdp-bel  use a heuristic that is initialized based on the mdp q-function and use some form of interpolation over these q-values. these algorithms also improve the heuristic by updating the q-values to fit the pomdp values. such algorithms therefore work initially much like the qmdp heuristic which is known to preform badly for many pomdp problems and in many cases gets stuck in local optima. these algorithms can potentially need many updates to improvethe heuristic to be able to reach rewards or goal states.
﹛as noted earlier  a major drawback of mdp based approaches is their inability to perform in information gathering tasks. fsvi is slightly different in that respect. fsvi uses point-based backups in which information gathering actions are also evaluated and considered. it is therefore able to perform single step information gathering actions such as the activation of a sensor. for example  in the rocksample domain  smith and simmons  1   the robot should activate a sensor to discover whether a rock is worthwhile  and indeed fsvi performs very well in the rocksample domain  executing such actions. however  when the information gathering requires a lengthy sequence of operations  such as in the heaven-hell problem  bonet and gefner  1  where the agent is required to pass a corridor to read a map and then return to take a reward  fsvi's heuristic will fail to lead it through the corridor  and therefore it cannot learn of the existence of the map. fsvi can learn to read the map  using an information gathering action  if it is on the path from an initial state to the goal.
﹛this limitation can be removed by adding an exploration factor causing fsvi to occasionally choose a non-optimal action. in practice  however  it is unlikely that random exploration will rapidly lead towards meaningful trajectories.
1 empirical evaluations
1 evaluation metrics
we evaluate the following criteria:
value function evaluation - average discounted reward
p#i=1trials p#j=1steps 污jrj
 adr : #trials . adr is filtered using a first order filter to reduce the noise in adr.
﹛execution time - we report the cpu time but as this is an implementation specific measurement  we also report the amount of basic operations such as backup  而 function  dot product  汐 ﹞ b  and equation 1  computations .
﹛memory - we show the size of the computed value function and the number of maintained belief points.
1 experimental setup
as hsvi is known to perform better than other point-based algorithms such as perseus  spaan and vlassis  1  and pbvi  pineau et al.  1   we compare fsvi only to hsvi. comparison is done on a number of benchmarks from the point-based literature: hallway  hallway1  littman et al.  1   tagavoid  pineau et al.  1   rocksample  smith and simmons  1  and network ring  poupart and boutilier  1 . these include all the scalability domains on which hsvi was tested in  smith and simmons  1   except for rock sample 1 which could not be loaded on our system due to insufficient memory. table 1 contains the problem measurements for the benchmarks including the size of the state space  action space and observation space  the number of trials used to evaluate the solutions  and the error in measuring the adr for each problem.
﹛the rock sample domain provides an opportunity for testing the scalability of different algorithms. however  these problems are somewhat limited: they assume deterministic state transitions as well as full observability forthe robotlocation  making the problems easier to solve. to overcome these limitations we added a new domain - noisy rock sample  in which agent movementsare stochastic and it receives noisy observations as to its current location.
﹛we implemented in java a standard framework that incorporated all the basic operators used by all algorithms such as vector dot products  backup operations  而 function and so

figure 1: convergence on the noisy rock sample 1 problem  a   the rock sample 1 problem  b  and the rock sample 1 problem  c . the x axis shows cpu time and the y axis shows adr.

figure 1: normalized comparison of cpu time for the rock sample problems.
forth. all experiments were executed on identical machines: x1-bit machines  dual-proc  1ghz cpu  1gb memory  1mb cache  running linux and jre 1.
﹛we focus our attention on convergence speed of the value function to previously reported adr. we executed hsvi and fsvi  interrupting them from time to time to compute the efficiency of the current value function using adr. once the filtered adr has reached the same level as reported in past publications  execution was stopped. the reported adr was then measured over additional trials  number of trials and error in measurement is reported in table 1 .

hallway1ms1米s1米s1ns1米s1mshsvi111.1.1.1.1.1.1fsvi111.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1hallway11ms1米s1米s1ns1米s1mshsvi111.1.1.1.1.1.1fsvi111.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1tag avoid1ms1米s1米s1ns1米s1mshsvi-111.1.1.1.1.1.1fsvi-111.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1rock sample 11ms1米s1米s1米s1米s1mshsvi111.1.1.1.1.1.1fsvi111.111㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1rock sample 11ms1米s1米s1米s1米s1mshsvi111.1.1.1.1.1.1fsvi11111111㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1noisy rock sample 11ms1米s1米s1米s1米s1mshsvi111.1.1.1.1.1.1fsvi111.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1rock sample 11ms1米s1米s1米s1米s1mshsvi111.1.1.1.1.1.1fsvi11111.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1rock sample 11ms1米s1ms1米s1米s1sechsvi111.1.1.1.1.1.1fsvi111.1.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1rock sample 11ms1米s1ms1米s1米s1sechsvi111.1.1.1.1.1.1fsvi111.1.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1networkring 11ms1米s1ms1ns1米s1mshsvi111.1.1.1.1.1.1fsvi1111.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1networkring 11ms1米s1ms1ns1米s1mshsvi111.1.1.1.1.1.1fsvi111.1.1.1.1.1㊣1㊣1㊣1㊣1㊣1㊣1㊣1㊣1table 1: performance measurements. model rows show rough estimates of basic operations execution time.
problem	|s|	|a|	|o|	#trials	adr error	1. number of computed belief states.
hallway11 1㊣1hallway111㊣1tag avoid11 1㊣1rock sample 111 1㊣1rock sample 111 1㊣1noisy rs 111 1㊣1rock sample 1111㊣1rock sample 1111㊣1rock sample 1111㊣1network ring 111㊣1network ring 111㊣1 1. 而 function computations count.
 1. dot product operations count.
 1. number of points in the upper bound  |v‘| .
 1. upper bound projection computations count  v‘ b  .
methodadr|v |time# backups#|b|#而# 汐 ﹞ b|v‘|v‘ b #hv‘ secs x 1x 1x 1x 11 1. upper bound value updates count  hv‘ b  . the last 1 items refer only to hsvi as fsvi does not maintain an upper bound. the reported numbers do not include the repeated expensive computation of the adr  or the initialization time  identical for both algorithms .
table 1: benchmark problem parameters to illustrate the convergence rate of each algorithm  we also plotted the adr as a function of cpu time in figure 1. these graphs contain data collected over separate executions
1	results	with fewer trials  so table 1 is more accurate.
table 1 presents our experimental results. for each problem
and method we report:	1	discussion
1. resulting adr.	our results clearly indicate that fsvi converges faster than
1. size of the final value function  |v | .	hsvi and scales up better. the convergence rate shown in
1. cpu time until convergence.	figure 1 is always faster  the time required to reach optimal
1. backups count.	adr is also always faster  and this difference become more
1. ga o汐	operations count.	pronounced as problem size increases. figure 1 presents a

more focused comparison of the relative ability of the two solvers to scale up. overall  it appears that hsvi is slowed down considerably by its upper bound computation and its updates. while it is possible that hsvi's heuristic leads to a more informed selection of belief points  fsvi is able to handle more belief points  and faster. in fact  in the rock sample domains  we can see that fsvi is able to converge with fewer belief points and even slightly improved adr. thus  on these domains  the heuristic guidance in point selection offered by the mdp policy is superior to the bounds-based choice of hsvi. indeed  we can also see that the number of backups fsvi executes is in most cases less than hsvi  hinting that the chosen trajectories  and hence backups  are also better than those selected by hsvi.
﹛observe that the upper boundof hsvi allows the definition of additional properties such as a stopping criterion based on the gap between bounds over the initial belief point b1  and an upper bound over the number of trials required to close that gap. in practice  however  hsvi never manages to reduce this gap considerably  as also noted in  smith and simmons  1   and reaches maximal performancewhen the gap is still very large. this is to be expected  as the the convergence guarantees for hsvi relies on the eventual exploration of the entire and-or search graph for the domain until an appropriate depth that grows as 污 approaches 1.
﹛as noted above  fsvi is not able to deal adequately with longer information-gathering sequences  such as those required in the heaven-hell domain. hsvi performs better than fsvi on these domains  although it  too  does not do too well. we tested both algorithms on a deterministic version of heaven-hell. hsvi took 1ms to converge and required 1 backups  while fsvi needed 1ms and 1 backups using an exploration factor of 1. when only the activation of single step information gathering actions is needed  such as all the rock sample and network domains  fsvi performs well.
﹛given that different approximation algorithms for pomdps are likely to perform well on different classes of domains  it is desirable to be able to determine prior to execution of an algorithm whether it will work well on a given problem. fsvi appears to be the best current candidate for domains in which long information seeking plans are not required. thus  it is interesting to ask whether such domains are easy to recognize. we believe that the following initial schema we are currently exploring might work well. roughly  we start by defining informative states as states where observations are available that result in radically reduced belief state entropy. if all such states are visited with reasonably high probability by following the underlying mdp optimal policy  then fsvi should perform well. in comparison to the cost of solving the pomdp  this test is relatively cheap to perform.
1 conclusions
this paper presents a new point-based algorithm - forward search value iteration  fsvi  - that executes asynchronous value iteration. fsvi simulates trajectories through the belief space choosing actions based on the underlying mdp optimal action. once the trajectory has reached a goal state  fsvi performs backups on the belief states in reversed order.
﹛fsvi is a simple algorithm that is easy to implement and leads to good quality solutions rapidly. this was demonstrated by comparing fsvi to a state-of-the-art algorithm  hsvi  a closely related point-based algorithm that uses a different heuristic for belief space traversals. our experiments show that fsvi is much faster and scales up better on almost all benchmark domains from the literature.
﹛the underlying mdp provides a heuristic that is both fast to compute and use and also always directs the agent towards rewards. this heuristic  however  limits the ability of the agent to recognize traversals that will improve its state information. namely  information gathering tasks that require lengthy traversals to gather information cannot be solved by fsvi. this deficiency was demonstrated on the heaven-hell problem which requires a detour in order to collect important information. thus  it is apparent that future work should consider how to revise fsvi so that its generated trajectories will visit such states. this can perhaps be done by executing simulations that try to reduce the entropy of the current belief space  using information visible to the underlying mdp  or by examining some augmented state space. given fsvi's performance and simplicity  we believe that such useful modifications are possible.
