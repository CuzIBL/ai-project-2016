 	1 	introduction 

dynamic probabilistic networks  dpns  are a useful tool for modeling complex stochastic processes. the simplest inference task in dpns is monitoring - that is  computing a posterior distribution for the state variables at each time step given all observations up to that time. recursive  constant-space algorithms are well-known for monitoring in dpns and other models. this paper is concerned with hindsight - that is  computing a posterior distribution given both past and future observations. hindsight is an essential subtask of learning 
dpn models from data. existing algorithms for hindsight in dpns use o sn  space and time  where n is the total length of the observation sequence and s is the state space size for each time step. they are therefore impractical for hindsight in complex models with long observation sequences. this paper presents an 1 s log n  space  1 sn log n  time hindsight algorithm. we demonstrates the effectiveness of the algorithm in two real-world dpn learning problems. we also discuss the possibility of an o s -space  1 siv -time algorithm. 
   'this research was funded by the national science foundation under grant no. fd1  and by aro under the muri program  integrated approach to intelligent systems   grant number daah1-1. 
1 	probabilistic: reasoning 
dynamic probabilistic networks are variants of probabilistic  bayesian  networks designed to represent stochastic temporal processes. they were first used extensively by dean and kanazawa   and have since become a standard tool in ai. as figure 1 shows  a dpn consists of infinitely repeated slices; the variables in the tth slice represent the state of the process at time t. 
we assume for the sake of expository simplicity that the variables within a slice can be divided into state variables x t   which are always hidden  and evidence variables et  which are always observable. we also assume that only adjacent time slices are connected  and that the topological structure and conditional distributions are identical for all time slices. that is  the dpn is specified by defining the structure and interconnectivity of the first two slices  and this is then  unrolled  n times  to provide storage space to hold the xt and et values for t €  1  n . 
¡¡if there is only one hidden variable and one observable variable per time slice  a dpn is equivalent to a hidden markov model  hmm . the main advantage of dpns over hmms arises when the state can be decomposed into several state variables  as in figure 1.  thus  ghahramani and jordan  refer to dpns as  factorial hmms.   if each state variable is directly influenced by at most a constant number of other variables  then the number of parameters required to specify the dpn will be linear in the number of state variables  whereas for standard hmms it is exponential. this reduction makes both inference and learning potentially much more efficient. 
¡¡this paper begins with a discussion of the dpn monitoring task-that is  computing a posterior distribution 
for the state variables at each time step given all observations up to that time. recursive  constant-space algorithms are well-known for monitoring in dpns and other models. we then discuss the problem of hindsight-that is  computing a posterior distribution given both past and future observations. hindsight is an essential subtask of learning dpn models from data. existing algorithms for hindsight - which simply unroll the dpn and treat it like a static network - use o sn  space and time  where n is the total length of the observation sequence and s is the state space size for each time step. 

process model 
/ 

figure 1: the generic structure of a dynamic probabilistic network  showing the state variables x and the evidence 

figure 1: a fragment of an actual dpn  showing the internal structure of each time slice. this was used to model the behavior of cars driving on the freeway  forbes et ai  1 . 

they are therefore impractical for hindsight in complex models with long observation sequences. as an example of the severity of the space problem  we note that the network in figure 1 has and  during model learning  may require therefore  we present a simple divide-and-conquer algorithm that reduces the space requirements from o sn  to 1   1 log n   thus making such applications feasible in practice. we present experimentals results to validate our claims. we also briefly discuss a possible way of reducing the space requirements even more dramatically  namely to use only o s  space. this would be particularly useful for online learning algorithms. 
1 	monitoring in dpns 
the most common use of dpns is for monitoring  i.e.  computing. where we use the notation as shorthand for the variables . in this capacity  a dpn allows one to keep track of the state of a process given partial  noisy observations. for example  one could estimate the progress of a disease in a patient from clinical observations over time  or estimate the position of a missile from a sequence of radar observations. in control theory  this task is called filtering. the well-known kalman filter  kalman  i1  can be viewed as a special case of a continuous-variable dpn in which the sensor model is restricted to be gaussian and the process model is restricted to be linear with gaussian noise  alag and agogino  1 . we note that dpns can handle a much larger variety of processes than kalman filters. 
¡¡standard arguments  see e.g.   russell and norvig  1  p.1   show that monitoring corresponds to the following recursive update equation 
 1  
where is a normalizing constant. for future reference  we represent this equation more abstractly as 
		 1  
where we have introduced the forward probabilities 
 we assume the initial  boundary condition  fo is known. 
¡¡the process of computing the forward probabilities can be implemented in a variety of ways. kjaeruhlff  describes dhugin  an extension of the hugin package that handles dpns via direct manipulation of the join tree. russell and norvig  describe the roll-up method  which operates directly on the dpn. finally  one can implement equation 1 simply by enumerating over the possible states xt. 
¡¡the space and time complexity of monitoring can be expressed in terms of n  the total number of time steps; |e|  the number of evidence variables per time step; |x|  the number of state variables per time step; and 1  the space required to store the probability distribution over 
	binder  murphy  & russell 	1 

x*. in the worst case  which corresponds to a completely connected network  even if the original dpn was only sparsely connected. this is because conditioning on past evidence causes variables in the current slice to become directly dependent.1 storing all the input  et for all i  and output  ft for all t  takes 1v|e|  and o ns  space respectively. however  it is reasonable to assume that the input is available as a stream from the environment or is read in sequentially from secondary storage  and similarly for the output. that is  we assume that there are  producer  and  consumer  processes for the input and output; we just focus on the amount of temporary working space required by the montoring algorithm itself  namely 1 . the time required is of course 1{sn . 
¡¡the related task of prediction  computing for t   n  can be performed by running a monitoring algorithm with no evidence beyond n  and hence also requires o s  working space. 
1 	hindsight in dpns 
in some settings  we may wish to take  future  evidence  as well as past and present evidence  into account e.g.  where the t index does not refer to time  but is simply an index into a static sequence. the observations after t may help to eliminate uncertainty about the state at t. for example  one can deduce who was in the house at the time of the murder by subsequent observation to see who leaves the house  even if one neglected to observe the house prior to the murder. the process of computing  is called smoothing in 
control theory  where it is often used to reduce the apparent wiggliness in a trajectory computed by filtering. in ai  perhaps the most important use of hindsight is in learning. in order to learn a dpn model from observation sequences  it is necessary to compute likelihoods for the hidden variables given all available data  lauritzen  1; russell et al.  1   hence hindsight is an integral part of dpn learning. 
¡¡the most obvious algorithm for hindsight is to perform monitoring in the forwards and backwards directions  and to combine the information at each time step  as follows: 
 1  
 1  
     1  technically  the joint distribution has only the independences present in the stationary distribution of the markov chain represented by the dpn. 
1 	probabilistic reasoning 

again  we assume the boundary condition bn is known. we can think of this method as propagating the forwards and backwards  messages  ft and bt from both ends towards slice t  which takes o sn  time and o s  working space   combining them  and then repeating for each t. hindsight for the complete sequence therefore requires 1 n1s  time and o s  working space. 
¡¡by storing ft and bt at each time step  i.e.  caching intermediate results   we get an algorithm that is essentially identical to pearl's  message-passing algorithm for chains  to the join-tree algorithm operating on an explicitly represented dpn with n slices  or to the forward-backward hmm algorithm  see also  smyth et al.  1  . all these approaches require o sn  time  but unfortunately require o sn  space  which is impractical. we now go on to discuss a simple divide-andconquer algorithm that requires 1 sn log n  time and o s log n  space. 
1 	the space-efficient algorithm 
the essential idea of our space-efficient method is to store the ft and bt messages at k - 1 intermediate  checkpoints  or  islands   thereby dividing the original sequence into k segments; we then recursively apply the hindsight algorithm to each smaller segment  making use of the boundary conditions stored at each checkpoint. for example  if n = 1 and k = 1  we compute f1  f 1  ...  f1 and b 1   b 1   ¡ö      b1 by iteratively applying the forward and backward operators to the boundary conditions fo and b 1 respectively  but we only store ft and bt for t = 1. we compute 
 = combine  pass it to the 
consumer1  and then recursively apply the hindsight algorithm to the sub-sequences for t = 1 . . . 1  using fo and b1 as boundary conditions  and for t = 1.. .1  using f1 and b 1 as boundary conditions. 
¡¡the total working space required by this approach is determined by two parameters: k  the number of checkpoints we store at each level  and d  the number of levels of recursion before we invoke the base case algorithm  which uses linear time and linear space . if we treat k as a fixed parameter  and recurse  all the way to the bottom   i.e.  apply the base case only to segments of length 
1   we have d = logk n. 	the working space is then 
1 ks  ogk n   since we store k checkpoints  each of size 1  at each level of recursion  and there are d = logk n levels of recursion. the time required by this approach  t n   is the time taken to propogate the messages across k segements of length roughly n/k each  plus the time to solve the k subproblems: 
so t n  = n logk n. we can decrease the running time  and increase the space requirements  either by increasing k from 1 to n  or by decreasing d from logk  n to i. 
1
¡¡¡¡the consumer might store on secondary storage  or it might use it in a learning algorithm to perform an in-place update of the model parameters  before discarding it. 
an interesting compromise is  which takes only twice as long as the standard algorithm and needs only  space. 
1 	experimental results 
we have implemented the abstract operators fwdop  
backop  and combineop in terms of a modified version of the jensen join tree algorithm  jensen et a/.  1 . the details will be presented in another paper  but the basic idea is to modify the triangulation heuristic to ensure that the resulting join tree has a repeating structure. this repeating block may span two slices in the original network  i.e.  it might contain cliques which have nodes from adjacent slices   so that ft and bt now refer to the forwards and backwards messages associated with the j'th repetition of this block  rather than the t'th time slice. 
¡¡in figure 1  we show how much space is required to do inference on the simple network shown in figure 1 as a function of the length of the sequence. it is clear that by increasing the depth of recursion  d  we can reduce the space requirements dramatically. the time taken for d = 1 with n = 1 was 1 seconds  for d = 1 it was 1 seconds  and for d = 1 it was 1 seconds.1 in figure 1  we plot a similar curve for the more complicated network shown in figure 1. 
1 	further work: constant space  
we have shown how to avoid the otherwise crippling space requirements for hindsight in dpns  giving an algorithm whose space complexity grows logarithmically with the length of the sequence. this has allowed us to address far more complex dpn learning problems than previously possible. 
¡¡an analogous technique could be applied to cope with cases where s  and not just n  is very large. the idea would be to store check points for only some of the nodes within a slice. for example  in the join tree algorithm  instead of storing messages at every node in the repeating tree structure  we could store messages at only some of them  recomputing the others on demand.1 this could of course be combined with the current approach to yield an 1  ognlog s-space algorithm. 
¡¡a more ambitious goal would be to find a constantspace algorithm  that is  one having space requirements which are independent of n. this would be particularly useful in the context of online learning  in which a continuous stream of evidence is arriving  and hence n grows indefinitely . we currently only know how to do this in restricted circumstances. the idea is as follows. according to equation 1  we can compute the posterior at time step t given ft and bt. we can filter forward 
1
¡¡¡¡experiments were run on a pentium pro 1mhz pc with 1 megabytes of ram  using linux and gcc. 
1
¡¡¡¡ the key requirement is that the set of nodes at which we choose to store checkpoint information d-separate the evidence. 

figure 1: space complexity of the island algorithm on the dna network. the curve labelled  statically unrolled network  refers to the results produced by applying the standard linear time  linear space algorithm to the network which has been unrolled a fixed number of times. on compiling the dna network  the size of the repeating clique set  s  is 1 bytes  1 real numbers  and the size of the f and b messages are each 1 bytes  1 real numbers . 
all the way to the end to obtain fn  and bn is given. by backward monitoring  we can now compute  if we can also compute  from fn  we are in business. this can be done by inverting the process model and by undoing the effects of en. we then continue this process all the way back to the beginning of the sequence  computing posteriors as we go. 
¡¡the details of the inversion process are best understood in terms of matrix operations  where we consider ft as a vector of probabilities over all possible states xt of the variables in xt. let ot be the diagonal matrix whose entries are the probabilities in   and let m  the transition matrix  contain the probabilities 
  note that ac-
cording to our assumptions  m is independent of time.  equation 1 now becomes the vector equation 

from this we can obtain the inverse operation: 

thus  in the generic case where m and o are invertible  we have a constant-space hindsight algorithm. this algorithm will fail  however  if either matrix is singular. ot will be singular only if some entries in  are zero  that is  if the observations rule out some of the possible states. m will be singular if  for example  two columns are identical-which could easily occur if transitions are independent of one of the state variables. thus  
	binder  murphy  & russell 	1 


figure 1: a simple network for learning intron/exon coding behavior in dna. figure 1: space complexity of the island algorithm on the car network. s is 1 bytes  1 real numbers  and the size of the f and b messages are each 1 bytes  1 real numbers . 
invertibility will fail in many realistic situations where information is lost as the process proceeds  and in many others the inversion step will be highly ill-conditioned. it remains to be seen whether it is possible to somehow store a small amount  independent of n  of extra information to make the transformation invertible. 
acknowledgments 
thanks to paul horton  geoff zweig  nir friedman  and the reviewers for useful comments. 
