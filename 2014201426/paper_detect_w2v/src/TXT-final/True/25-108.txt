 
vision is a key function not only for robotics but also for ai more generally. today realtime visual processing is becoming possible; this means that vision based behavior can become more dynamic  opening fertile areas for applications. one aspect of this is real-time visual tracking. we have built a real-time tracking vision system and incorporated it in an integrated robot programming environment. using this  we have performed experiments in vision based robot behavior and human-robot interaction. in particular  we have developed a robotic system capable of  learning by seeing . in general  it is important for the ai community not to lose sight of the problems and progress of robotics. after all  an ai system which acts in real-time in the real-world is no less  and no more  than an intelligent robot. 
1 	introduction 
a robot is a versatile intelligent machine which can carry out a variety of tasks in real-time. the interaction with the outside world is the essential aspect which distinguishes robotics from ordinary ai. in order to make this interaction more intelligent  a robot needs functions such as: the ability to understand the environment by visual recognition  the ability to perform dexterous manipulation using force  tactile  and visual feedback  the ability to plan task procedures  the ability to naturally communicate with humans  the ablity to learn how to perform tasks  the ability to recover from errors  and so on. all of these are required for robot intelligence to be realized. 
　from the earliest days of ai research  aspects of robotrelated intelligence have been tackled; these include the principles for problem solving  planning  scene understanding  and learning. whereas ai research generally takes the quest for the basic principles of intelligence as its goal  in robotics  the results of task planning or scene understanding are not the ultimate goal  but are rather the means for acting and reacting properly in the real world. 
　visual information plays a very important role for robot-environment interaction. if provided with visual sensing  the potential repertoire of robotic behavior becomes very rich. to actually experiment with such behaviors  we need very fast visual information processing. section 1 sketches our efforts towards high speed robot vision. this system is implemented as a multi-processor configuration  greatly enhancing the performance. 
　we have combined the real-time tracking vision system with a radio control system for wireless servo units  giving us a robot development system. in this approach  the robot body consists of mobility  manipulator and vision. the robot does not carry its own computer; rather it is connected with the powerful vision system and computer by radio link. thus  this approach enables very compact robot bodies which actually behave in the real world. section 1 describes this remote-brained approach and several environments for robot behavior research. 
　high speed visual tracking capability opens up another important way to make human-robot interaction smarter:  learning by seeing . section 1 explains our preliminary experiments on this. despite the limited performance of the vision system  the system as a whole can observe and understand pick-and-place sequences which a human acts out for the benefit of robot vision. 
　section 1  discusses some future directions for real world ai research and speculates on the possiblity of developing humanoid robots. 
1 	a real-time visual tracking system 
vision is an essential sense for robots. in particular  robot vision requires real-time processing of visual information about motion and depth. motion information should include recognition of moving objects  tracking  and ego-motion understanding. depth information is always necessary for a robot to act in the threedimensional real world. flexible interaction between visual data and motion control is also important for attaining vision based intelligent robot behavior. 
1 	using correlations between local image regions 
the fundamental operation on which our system is based is the calculation of correlation between local image regions. it computes the correlation value between a region r in image f  and subregions s within a search area s in image f1  where f1 and f1 are either part of the same 
	inoue 	1 

figure 1: visual tracking system 
image  two time-consecutive image frames or left/right images at the same sampling time  and finds the best matching subregion  namely  that which minimizes the 
value 
　the correlation between r and s is given by the equation 

although correlation is generally defined as a sum of products  we employ this simpler equation  the mean absolute error criterion  to decrease the computation time. 
1 	hardware organization 
　figure 1 diagrams the organization of the hardware  inoue 1 . the system is implemented as a transputer-based vision system augmented with a high speed correlation processor. the transputer vision board is equipped with three image-frame memories  each of which can be used simultaneously for image input  image processing  and image display. thus  the system can devote all its computation power to image processing without waiting for image input or display. the vision board also incorporates an off-the-shelf chip  mep: motion estimation processor  sgs 1     designed for image compression  but used here as a correlation processor. using this chip  we have developed a very fast correlation based robot vision system. this system can also be used in a multi-processor configuration  greatly 
increasing performance. the transputer controls the image data stream for  this data is transferred to the correlation chip  and the results are returned to the transputer. 
1 	visual tracking based on local correlation 
real-time visual tracking is an important requirement for robot vision. in the usual approach  various feature 
1 	invited speakers 
parameters of objects  such as region center or edge information  are computed for the input image data and the objects represented by these parameters are tracked. such approaches are simple and fast enough  however they sometimes have the drawback of over-sensitivity to noise  lighting conditions  and background image characteristics. our method is simpler: local correlation is used to search for the corresponding location between the last and current image or between the reference image and the current input image. until now  this method has been considered much too computation-intensive  but by using the powerful correlation chip this computation can be performed in real-time if the reference frame is of moderate size. 
　the tracking process repeats the following two step procedure:  1  search the reference image in the local neighborhood around the current attention point  and determine the location of the highest correspondence.  1  move the point of attention to this location. 
　we performed a simple experiment using the vision hardware described in the previous section. the target region r was 1 x 1  the search area 1 was 1 x 1  and the sen-ch region 
we found at tracking for a 1 x 1 reference region is performed in 1 msec  significantly faster than possible with the transputer alone. further  the hardware configuration using the mep chip is very simple  compact  and inexpensive. 
　using this system we can track more than 1 regions at video rate; which is more than sufficient for many realtime tracking applications. if it is necessary to track more regions a multi-processor system can be used; the number of tractable regions increases linearly with the number of processors. 
1 	real-time optical flow computation 
although optical flow provides a very attractive way to detect motion by vision  its computation also has been extremely time consuming. using the correlation processor  we managed to speed-up the calculation of optical flow. 
　the input image is divided into a set of small patch regions  each of which is correlated with the image taken at the time dt later  and the flow vector is determined as the vector from the patch region in the previous image to the best corresponding region on the subsequent image. 
　by using a single mep chip the optical flow vectors for 1 x 1 points were computed in 1 msec. the processing time for local correlation was less than 1 msec; the rest of the time was consumed by the transputer for data dispatch from image frame memories to the mep chip. if the data dispatch were done by a dedicated circuit the computation time would be much faster. 
1 	stereo and depth map generation 
we next attempted depth map generation based on binocular stereo matching. in the experiments  the depth map at 1 x 1 points was generated. the 1 x 1 measurement points were fixed on the left view image. the reference window to be located at each measurement point was defined as a 1 x 1 pixel local image. 


figure 1: robot world in remote brained approach 

the reference window on the left view was matched to sub-regions within a search window of 1 x 1 pixels on the right view. 
1 applying visual tracking to robot behavior control 
when the speed of visual processing reaches real-time  the nature of sensor interaction can be made dynamic instead of static. in particular  the performance of our tracking vision system enables us to perform new experiments in real-time intelligent robot behavior  such as game playing between a computer-controlled robot and a human-controlled robot. 
1 	experimental setup: the remote-brained approach 
in order to advance the study of vision based robot behavior  we built a system to serve as a base for experiments. figure 1 shows how this system is constructed using a transputer-based multi-processor organization. it is intended to provide a high performance  flexible system for implementing vision based behavior experiments. each workstation is interfaced with the vision unit and the motion control unit. the transputer/mep based vision system in multi-processor configuration provides powerful sensing means for behavior control. for the controller interface  we use radio control servo units  which are available as parts for radio controlled model kits. in our system there are 1 wireless channels for servo units. the video signal is transmitted by uhf radio from onboard cameras to the vision processor. we can say that  rather than lugging its brain around  the robot leaves it at a remote computer and talks with it by radio  inaba 1 . 
in order to build an experimental setup for robot behavior study  we need to work on mechanisms  on the control interface  and on software. until everything has been integrated  we cannot do any experiments. this is one of the things that makes robotics research timeconsuming. however  the remote-brained approach can help; it partitions the work on mechanism  on interface  and on software. this approach provides a cooperative environment where each expert can concentrate on his own role. for the software engineer  the definition of the control interface can be treated as the specification of just another output device. for the mechanical engineer designing the robot hardware  the wireless servo unit can be considered as just another mechano-electrical component. we believe this approach makes it easier for ai people to face up to the real world intelligence problem. figure 1 shows a remote-brained experimental environment consisting of seven radio-linked mobile robots. 
1 	coordination of hand-eye robots 
using the basic hardware described in the previous section  we have built an integrated experimental environment   c1sm1s-1 . cosmos-1 enhances the realtime capacity of vision system and provides an easy interface for developing experimental robot mechanisms. we have used it in several experiments in multiple robot coordination. for instance  we made two small hand-eye robots tie a knot in a rope using visual guidance. videotapes of several other experiments will be shown at the conference. 
1 	computer-human sumo wrestling 
figure 1 shows the system overview. two robots face off other in the  dohyo  ring  1 cm in diameter. one robot is controlled by a human operator via wireless controller. the control signal of the other robot is transmitted from a computer through the radio link. each  sumo  robot is 
	inoue 	1 
1 cm in length and width  and its weight is under 1 kg. the two driving wheels are powered by dc motors  each of which is controlled independently through a radio link. the maximum speed of the robot is 1 cm/sec. the two robots have the same mechanical performance to make things fair. 

figure 1: robot  sumo  system 
　the key to the success of the experiment is the realtime visual tracking of the two battling robots. a tv camera is placed above the ring looking down at the whole environment. as the robots move in the ring  changing their position and orientation  they are observed by the vision system; their position and direction are tracked in real-time. based on the real-time tracking of two robots's behavior  the fighting strategy and motion planning is computed. 
　for this application the performance of the vision system is adequate; using just one vision board the motions of both robots can be tracked completely in real-time. experiments show that the computer controlled robot tends to beat the human controlled one. this is because the computer is quite fast in observation and control processing  and makes fewer errors in control operation than the human operator. 
1 	towards a vision-guided autonomous vehicle 
the behavior of autonomous vehicles in natural environments is another interesting goal for research on real world ai. natural environments include not only lanes for vehicles  but also pedestrians and obstacles  both stationary and moving. we wish to develop an intelligent vehicle which behaves like an animal such as a horse. when we ride a horse  its behavior is controlled only through high-level  multi-modal communications from the human. if we let the horse free  it walks as it pleases  choosing a trail  avoiding obstacles  keeping itself safe  and interacting with other horses and moving objects. by training or teaching  a human and a horse can interact with each other for successful riding. 
　figure 1 shows the design of our vehicle. our purpose is to develop an semi-autonomous vehicle with horselevel abilities. we adapted a compact electric scooter originally designed for senior citizens. it is battery pow-
1 	invited speakers 

figure 1: hyper scooter 
ered  carries a single driver  and has a maximum speed of 1 km/h. we modified it for computer control. the steering is powered by a servo-mechanism. a video camera is mounted at the front. we put a trackball and a monitor tv on the steering bar to give instructions and to communicate. at the back  we installed a high speed robot vision system and control computer. we have built this experimental prototype and have just begun preliminary experiments. our long-term challenge is built an autonomous vehicle which can behave like a mechanical animal in being teachable/trainable. 
1 seeing  understanding and repeating human tasks 
as a step towards an integrated robot intelligence  we have built a prototype system that observes human action sequences  understands them  generates robot program for the actions  and executes them. this novel method for robot programming we call  teaching by showing  or  learning by seeing   kuniyoshi 1 . it includes various aspects of intelligent robot behavior. 
1 	experimental setup 
figure 1 shows the hardware setup of the system. the system is implemented on cosmos-1  a network based robot programming system. 
　 1  camera configuration: task presentation is monitored by three monochrome video cameras   two for stereo and one for zoom-in  connected to the networkbased robot vision server. 
　 1  vision server: special vision hardware is connected to a host workstation. the host runs a server which accepts commands  controls the vision hardware and transmits the extracted data through a socket connection over the ethernet. the vision hardware consists of a high speed line segment finder  lsf  and a multi window vision system  mwvs . the lsf extracts lists of connected line segments from a gray scale image  1  within 1 msec  moribe 1 . the mwvs is a multi processor hardware component that 


figure 1: system for teaching by showing 
extracts various image features at video rate from within rectangular  windows'1 of specified size  sampling rate and location  inoue 1b . it can handle up to 1 windows in parallel for continuous tracking and detection of features. 
　 1  high-level processing servers: two workstations are dedicated for action recognition and plan instantiation. the action recognizer consists of an action model  an environment model and an attention stack. it extracts visual features by actively controlling the vision server and generates a symbolic description of the action sequence. plan instantiation involves matching this  action plan  against the environment model  which is updated by visual recognition of the execution environment. from this plan  motion commands for the manipulator are generated and sent to the motion server. the programs are written in euslisp  an object-oriented lisp environment with geometric modeling facilities. 
　 1  motion server: a cartesian type arm with a 1 dof wrist mechanism supporting a parallel-jaw gripper is used for task execution. the host workstation interprets robot commands from the ethernet and sends primitive instructions to the manipulator controller. 
1 	required functions 
seeing  understanding and doing must be integrated. our approach is to connect these at the symbolic level. 
as shown in figure 1  the system consists of three parts  divided by dotted lines in the figure   for seeing  understanding and doing. the following functions are performed by each of these parts: 
　seeing :  1  recognizing the initial state and constructing the environment model.  1  finding and tracking the hand.  1  visually searching for the target of the operation.  1  detecting meaningful changes around the target and describing them qualitatively. 
　understanding :  1  segmentation of the continuous performance into meaningful unit operations.  1  classification of operations based on motion types  target objects  and effects on the targets.  1  dependency analysis of observed task procedures to infer subprocedures consisting of temporally dependent operations. 
　 1  bottom-up plan inference to generate abstract .operators for each subprocedure and to gather target ob-
jects and state changes descriptions from the lower-level operators. 
　doing :  1  instantiating the task plan. recognizing the given initial state. matching the result with the stored task plan to produce goal positions for each operation.  1  path planning and generation of motion commands.  1  using sensor feedback for guiding motions.  1  error detection by vision and performance of recovery actions for the error. 
1 	example: recognizing a pick and place sequence 
the detailed technical content will not be be described here  however  to give the flavor of teaching by showing  the process of recognition of a  place  operation is sketched in figure 1. the top arrow is the time axis annotated with scene descriptions. ''attention  lines represent continuous vision processing executing in parallel. marks on the  events  line show when the various events are flagged. intervals on motion  lines denoted segmented assembly motions. tw types of  snapshots  at segmentation points and their  changes  are also shown:   sil.   snapshots are gray-scale silhouettes and   junct.   snapshots are connectivity configurations of local edges around the target face of an object. 
　 1  recognition of transfer: first a motion-detector is invoked. when a large movement is detected  an event  found-moving  is raised  signaling the start of a  transfer  motion. at the same time  a hand-tracker is invoked to track and extract motion features. for explanatory purpose  we assume that a pick operation was completed and a transfer motion was started during the break marked by wavy lines. 
　 1  initial point of localmotion: when the hand starts to move slowly downward  a  moving-down  event is raised. this event invokes a visual search procedure. when the target object is found  a  near  event is raised. 
this signals the end of the  transfer  motion and the start of a  localmotion . the environment model remembers that the hand is holding an object  a fact recorded when the system recognized the previous motion as a pick. this information gives rise to an anticipation that the held object is going down to be placed on the target object just found. a change-detector is invoked to extract and store a snapshot around the expected place position. 
　 1  final point of localmotion: the hand starts to move again. when it gets far enough away from the target object  a  far  event is detected. this signals the end of the  localmotion  and the start of the next  transfer . the change-detector takes another snapshot and finds that the area of the silhouette of the target has significantly increased. this results in identification of the operation as a  place-on-block   if there were no change in silhouette area  it would be identified as a 
 no-op   and if there were a decrease  as a  pick .  
　 1  updating the environment model: the environ-
	inoue 	1 
ment model is updated  based on the operation identified  to reflect the current state of the environment. to be specific  the  holding  relation between the hand and the placed object is deleted and an  on  relation between the placed object and the target object is added. the target position of the operation is initially estimated by measuring the center of area found by differentiating the stereo images. then  the vertical position of the placed object is recalculated  based on knowledge of the type of operation  from the action model  and the dimensions of the objects  from the environment model   and this information is stored. copies of environment model nodes corresponding to the hand and the object are made and stored in the  final-state  slot of the current node of the action-model. 
　 1  recognition of finemotion: a finer level of recognition proceeds in parallel with that of the  localmotion . the relative positions of the held object and the target object are continuously monitored by vision. when they touch each other  a  join  event is established; this signals the start of  finemotion . a coplanar-detector is invoked and gives the result  noncoplanar   because the faces of the objects are not alligned at this point. 
　when the fingers release the placed object  an event  split  is detected  signaling the end of  finemotion . this time the coplanar-detector detects the  coplanar  state. comparing the initial and final states  the  finemotion  is identified as an  align  operation. the coplanar relation defines the relative orientation of the objects  which is stored in the environment model. 
1 	invited speakers 
1 concluding remarks : robot behavior and real world computing 
at an invited talk at ijcai-1 i presented a system intended to help bridge the gap between ai and robotics  inoue 1a . that system  called cosmos  is a lispbased programming environment which integrated a 1d vision system  a geometric modelling system  and a manipulator control system. the early cosmos was built in a mini-computer based centralized configuration. its successor  cosmos-1  is implemented in a networkbased configuration consisting of several robot-function servers. using cosmos-1  we built the intelligent robot system  mentioned above  which can observe a humanperformed task sequence  understand the task procedure  generate a robot program for that task  and execute it even in a task space different from the one in which it was taught. as described in section 1  we recently succeeded in devleloping a very fast robot vision system  and cosmos-1 is the extension of this to a multi-transputer configuration  greatly enhancing its real-time capacity. 
　this paper has focused on our current efforts towards intelligent robots as real-world ai. the remainder of this paper presents some of our hopes and plans for the robots of the future. 
　real world environments are full of uncertainty and change. however  a human brain can recognize and understand a situation  make a decision  predict  plan  and behave. the information to be processed is enormous in quantity and multi-modal. an real world intelligent system must perform logical knowledge processing  pattern information processing  and integration of the two. the japanese ministry of international trade and industry  miti  recently initiated  the real world computing project   which aims to investigate the foundations of human-like flexible information processing  to develop a massively parallel computer  and to realize novel functions for a wide range of applications to real world information processing. as dr. otsu will present this project in an invited talk  otsu 1   i will merely make a few comments from the viewpoint of intelligent robotics. 
　a robot can be viewed as an ai system which behaves in the real world in real-time. in a robot system  various autonomous agents such as sensing  recognition  planning  control  and their coordinator must cooperate in recognizing the environment  solving problems  planning a behavior  and executing it. research on intelligent robots thus covers most of what is involved in any real world agent. a robot can therefore be considered an adequate testbed for integrating various aspects of real world information processing. 
　as a concrete image for such a robot  i propose a humanoid-type intelligent robot  to serve as a base for the integration of real world ai research. i imagine a 
　body designed to sit on a wheeled chair to move about  as legged walking is not an essential purpose for intelligent humanoids . i imagine a head equipped with binocular vision to see  a microphone to listen  and a speech synthesizer to talk. i imagine two arms  in a humanlike configuration  with five-fingered hands. i imagine a brain capable of learning by seeing. further  i intend to give this robot the ability to communicate naturally with humans. 
　to build such a robot we will have to deal with many issues. to mention a few:  1  visual observation and understanding of complex hand motions for object manipulation   1  representation and control of coordinated motion of five-fingered robot hands   1  sensor based manipulation skill   1  direct visual feedback and forecast for dynamic motion  such as juggling   1  handing flexible materials like ropes or clothes   1  error recovery and reactive problem solving   1  control of visual attention   1  learning by seeing  and  1  recognition of and fusion of information from facial expression  gesture  and speech  allowing natural human-computer communication  among others. the tasks such a robot can perform will demonstrate its degree of dexterity and degree of intelligence. our short-term goal is to build a robot that can play games with our children. 
