
in this paper  nogood recording is investigated within the randomization and restart framework. our goal is to avoid the same situations to occur from one run to the next one. more precisely  nogoods are recorded when the current cutoff value is reached  i.e. before restarting the search algorithm. such a set of nogoods is extracted from the last branch of the current search tree. interestingly  the number of nogoods recorded before each new run is bounded by the length of the last branch of the search tree. as a consequence  the total number of recorded nogoods is polynomial in the number of restarts. experiments over a wide range of csp instances demonstrate the effectiveness of our approach.
1 introduction
nogood recording  or learning  has been suggested as a technique to enhance csp  constraint satisfaction problem  solving in  dechter  1 . the principle is to record a nogood whenever a conflict occurs during a backtracking search. such nogoods can then be exploited later to prevent the exploration of useless parts of the search tree. the first experimental results obtained with learning were given in the early 1's  dechter  1; frost and dechter  1; schiex and verfaillie  1 .
¡¡contrary to csp  the recent impressive progress in sat  boolean satisfiability problem  has been achieved using nogood recording  clause learning  under a randomization and restart policy enhanced with a very efficient lazy data structure  moskewicz et al.  1 . indeed  the interest of clause learning has arisen with the availability of large instances  encoding practical applications  which contain some structure and exhibit heavy-tailed phenomenon. learning in sat is a typical successful technique obtained from the cross fertilization between csp and sat: nogood recording  dechter  1  and conflict directed backjumping  prosser  1  have been introduced for csp and later imported into sat solvers  bayardo and shrag  1; marques-silva and sakallah  1 .
¡¡recently  a generalization of nogoods  as well as an elegant learning method  have been proposed in  katsirelos and bacchus  1; 1  for csp. while standard nogoods correspond to variable assignments  generalized nogoods also involve value refutations. these generalized nogoods benefit from nice features. for example  they can compactly capture large sets of standard nogoods and are proved to be more powerful than standard ones to prune the search space.
¡¡as the set of nogoods that can be recorded might be of exponential size  one needs to achievesome restrictions. for example  in sat  learned nogoods are not minimal and are limited in number using the first unique implication point  first uip  concept. different variantshave been proposed e.g. relevance bounded learning  bayardo and shrag  1    all of them attempt to find the best trade-off between the overhead of learning and performance improvements. consequently  the recorded nogoods cannot lead to a complete elimination of redundancy in search trees.
¡¡in this paper  nogood recording is investigated within the randomization and restart framework. the principle of our approach is to learn nogoods from the last branch of the search tree before a restart  discarding already explored parts of the search tree in subsequent runs. roughly speaking  we manage nogoods by introducing a global constraint with a dedicated filtering algorithm which exploits watched literals  moskewicz et al.  1 . the worst-case time complexity of this propagation algorithm is o n1¦Ã  where n is the number of variables and ¦Ã the number of recorded nogoods. besides  we know that ¦Ã is at most nd¦Ñ where d is the greatest domain size and ¦Ñ is the number of restarts already performed. remark that a related approach has been proposed in  baptista et al.  1  for sat in order to obtain a complete restart strategy while reducing the number of recorded nogoods.
1 technical background
a constraint network  cn  p is a pair  x  c  where x is a set of n variables and c a set of e constraints. each variable x ¡Ê x has an associated domain  denoted dom x   which contains the set of values allowed for x. each constraint c ¡Ê c involves a subset of variables of x   denoted vars c   and has an associated relation  denoted rel c   which contains the set of tuples allowed for vars c .
¡¡a solutionto a cn is an assignmentof values to all the variables such that all the constraints are satisfied. a cn is said to be satisfiable iff it admits at least one solution. the constraint satisfaction problem  csp  is the np-complete task of determining whether a given cn is satisfiable. a csp instance is then defined by a cn  and solving it involves either finding one  or more  solution or determining its unsatisfiability. to solve a csp instance  one can modify the cn by using inference or search methods  dechter  1 .
¡¡the backtracking algorithm  bt  is a central algorithm for solving csp instances. it performs a depth-first search in order to instantiate variables and a backtrack mechanism when dead-ends occur. many works have been devoted to improve its forward and backward phases by introducing look-ahead and look-back schemes  dechter  1 . today  mac  sabin and freuder  1  is the  look-ahead  algorithm considered as the most efficient generic approach to solve csp instances. it maintains a property called arc consistency  ac  during search. when mentioning mac  it is important to indicate which branching scheme is employed. indeed  it is possible to consider binary  1-way  branching or non binary  d-way  branching. these two schemes are not equivalent as it has been shown that binary branching is more powerful  to refute unsatisfiable instances  than non-binary branching  hwang and mitchell  1 . with binary branching  at each step of search  a pair  x v  is selected where x is an unassigned variable and v a value in dom x   and two cases are considered: the assignment x = v and the refutation. the mac algorithm  using binary branching  can then be seen as building a binary tree. classically  mac always starts by assigning variables before refuting values. generalized arc consistency  gac   e.g.  bessiere et al.  1   extends ac to non binary constraints  and mgac is the search algorithm that maintains gac.
¡¡although sophisticated look-back algorithms such as cbj  conflict directed backjumping   prosser  1  and dbt  dynamic backtracking   ginsberg  1  exist  it has been shown  bessiere and re¡ägin  1; boussemart et al.  1; lecoutre et al.  1  that mac combined with a good variable ordering heuristic often outperforms such techniques.
1 reduced nld-nogoods
from now on  we will consider a search tree built by a backtracking search algorithm based on the 1-way branching scheme  e.g. mac   positive decisions being performed first. each branch of the search tree can then be seen as a sequence of positive and negative decisions  defined as follows:
definition 1 let p =  x  c  be a cn and  x v  be a pair such that x ¡Ê x and v ¡Ê dom x . the assignment x = v is called a positive decision whereas the refutation is called a negative decision.   x = v  denotes and  denotes x = v.
definition 1 let  be a sequence of decisions where ¦Äi is a negative decision. the sequence  is called a nld-subsequence  negative last decision subsequence  of ¦². the set of positive and negative decisions of ¦² are denoted by pos ¦²  and neg ¦²   respectively.
definition 1 let p be a cn and ¦¤ be a set of decisions. p|¦¤ is the cn obtained from p s.t.  for any positive decision  x = v  ¡Ê ¦¤  dom x  is restricted to {v}  and  for any negative decision  is removed from dom x .
definition 1 let p be a cn and ¦¤ be a set of decisions. ¦¤ is a nogood of p iff p|¦¤ is unsatisfiable.
¡¡from any branch of the search tree  a nogood can be extracted from each negative decision. this is stated by the following proposition:
proposition 1 let p be a cn and ¦² be the sequence of decisions taken along a branch of the search tree. for any nldsubsequence  the set ¦¤ = {¦Ä1 ...  ¦Äi} is a nogood of p  called nld-nogood 1.
proof. as positive decisions are taken first  when the negative decision ¦Äi is encountered  the subtree corresponding to the opposite decision  ¦Äi has been refuted. 
¡¡these nogoods contain both positive and negative decisions and then correspond to the definition of generalized nogoods  focacci and milano  1; katsirelos and bacchus  1 . in the following  we show that nld-nogoods can be reduced in size by considering positive decisions only. hence  we benefit from both an improvement in space complexity and a more powerful pruning capability.
¡¡by construction  csp nogoods do not contain two opposite decisions i.e. both. propositional resolution allows to deduce the clause r =  ¦Á¡Å¦Â  from the clauses x¡Å¦Á and  x ¡Å ¦Â. nogoods can be represented as propositional clauses  disjunction of literals   where literals correspond to positive and negative decisions. consequently  we can extend resolution to deal directly with csp nogoods  e.g.  mitchell  1    called constraint resolution  c-res for short . it can be defined as follows:
definition 1 let p be a cn  and ¦¤1 = ¦£ ¡È {xi = vi} and be two nogoods of p. we define constraint resolution as c-res ¦¤1 ¦¤1  = ¦£ ¡È ¦«.
it is immediate that c-res ¦¤1 ¦¤1  is a nogood of p.
proposition 1 let p be a cn and ¦² be the sequence of decisions taken along a branch of the search tree. for any nld-
subsequence   the set 
{ ¦Äi} is a nogood of p  called reduced nld-nogood .
proof. we suppose that ¦² contains k ¡Ý 1 negative decisions  denoted by ¦Äg1 ... ¦Ägk  in the order that they appear in ¦². the nld-subsequence of ¦² with k negative
decisions is sponding nld-nogood is ¦¤1 = {¦Ä1 ... ¦Äg1 ...  ¦Ägk 1  ...   ¦Ägk}  ¦Ägk 1 being now the last negative decision.
the nld-subsequence of ¦² with k   1 negative decisions is
. its correspondingnld-nogood
is ¦¤1 = {¦Ä1 ... ¦Äg1 ...  ¦Ägk 1}. we now apply c-res between ¦¤1 and ¦¤1 and we obtain  c-res ¦¤1 ¦¤1  =
{¦Ä1 ... ¦Äg1 ... ¦Ägk 1 ... ¦Ägk 1 ¦Ägk 1  ...  ¦Ägk}. the last negative decision is now ¦Ägk 1  which will be eliminated with the nld-nogood containing k   1 negative decisions. all the remaining negative decisions are then eliminated by applying the same process. 
¡¡one interesting aspect is that the space required to store all nogoods corresponding to any branch of the search tree is

figure 1: area of nld-nogoods in a partial search tree
polynomial with respect to the number of variables and the greatest domain size.
proposition 1 let p be a cn and ¦² be the sequence of decisions taken along a branch of the search tree. the space complexity to record all nld-nogoods of ¦² is o n1  while the space complexity to record all reduced nld-nogoods of ¦² is o n1d .
proof. first  the number of negative decisions in any branch is o nd . for each negative decision  we can extract a  reduced  nld-nogood. as the size of any  resp. reduced  nldnogood is o nd   resp. o n  since it only contains positive decisions   we obtain an overall space complexity of o n1   resp. o n1d  . 
1 nogood recording from restarts
in  gomes et al.  1   it has been shown that the runtime distribution produced by a randomized search algorithm is sometimes characterized by an extremely long tail with some infinite moment. for some instances  this heavy-tailed phenomenon can be avoided by using random restarts  i.e. by restarting search several times while randomizing the employed search heuristic. for constraint satisfaction  restarts have been shown productive. however  when learning is not exploited  as it is currently the case for most of the academic and commercial solvers   the average performance of the solver is damaged  cf. section 1 .
¡¡nogood recording has not yet been shown to be quite convincing for csp  one noticeable exception is  katsirelos and bacchus  1   and  further  it is a technique that leads  when uncontrolled  to an exponential space complexity. we propose to address this issue by combining nogood recording and restarts in the following way: reduced nld-nogoods are recorded from the last  and current  branch of the search tree between each run. our aim is to benefit from both restarts and learning capabilities without sacrificing solver performance and space complexity.
¡¡figure 1 depicts the partial search tree explored when the solver is about to restart. positive decisions being taken first  a ¦Äi  resp.  ¦Äi  corresponds to a positive  resp. negative  decision. search has been stopped after refuting ¦Ä1 and taking the decision  ¦Ä1. the nld-nogoods of p are the following: ¦¤1 = {¦Ä1  ¦Ä1  ¦Ä1 ¦Ä1  ¦Ä1 ¦Ä1}  ¦¤1 = {¦Ä1  ¦Ä1  ¦Ä1 ¦Ä1 ¦Ä1}  ¦¤1 = {¦Ä1  ¦Ä1 ¦Ä1}  ¦¤1 = {¦Ä1 ¦Ä1}. the first reduced nld-nogood is obtained as follows:
 c-res c-res c-res ¦¤1 ¦¤1  ¦¤1  ¦¤1 
= c-res c-res {¦Ä1  ¦Ä1  ¦Ä1 ¦Ä1 ¦Ä1} ¦¤1  ¦¤1 
=
	=	1 1
¡¡applying the same process to the other nld-nogoods  we obtain:
 c-res c-res ¦¤1 ¦¤1  ¦¤1  = {¦Ä1 ¦Ä1 ¦Ä1}.
 ¦¤1 ¦¤1  = {¦Ä1 ¦Ä1}.
	¦¤1 =¦¤1 =	1 .
¡¡in order to avoid exploring the same parts of the search space during subsequent runs  recorded nogoods can be exploited. indeed  it suffices to control that the decisions of the current branch do not contain all decisions of one nogood. moreover  the negation of the last unperformed decision of any nogood can be inferred as described in the next section. for example  whenever the decision ¦Ä1 is taken  we can infer  ¦Ä1 from nogood  and  ¦Ä1 from nogood .
¡¡finally  we want to emphasize that reduced nld-nogoods extracted from the last branch subsume all reduced nldnogoods that could be extracted from any branch previously explored.
1 managing nogoods
in this section  we now show how to efficiently exploit reduced nld-nogoods by combining watched literals with propagation. we then obtain an efficient propagation algorithm enforcing gac on all learned nogoods that can be collectively considered as a global constraint.
1 recording nogoods
nogoods derived from the current branch of the search tree  i.e. reduced nld-nogoods  when the current run is stopped can be recorded by calling the storenogoods function  see algorithm 1 . the parameter of this function is the sequence of literals labelling the current branch. as observed in section 1  a reduced nld-nogood can be recorded from each negative decision occurring in this sequence. from the root to the leaf of the current branch  when a positive decision is encountered  it is recorded in the set ¦¤  line 1   and when a negative decision is encountered  we record a nogood from the negation of this decision and all recorded positive ones  line 1 . if the nogood is of size 1  i.e. ¦¤ =     it can be directly exploited by reducing the domain of the involved variable  see line 1 . otherwise  it is recorded  by calling the addnogood function  not described here   in a structure exploiting watched literals  moskewicz et al.  1 .
¡¡we can show that the worst-case time complexity of storenogoods is o ¦Ëp¦Ën  where ¦Ëp and ¦Ën are the number of positive and negative decisions on the current branch  respectively.
algorithm 1 storenogoods 

1: ¦¤ ¡û  
1: for i ¡Ê  1 m  do 1: if ¦Äi is a positive decision then
1:	¦¤ ¡û ¦¤ ¡È {¦Äi}
1:	else
1:	if ¦¤ =   then
1:	with ¦Äi =  x v   remove v from dom x 
1:	else
1:	addnogood ¦¤ ¡È { ¦Äi} 
1:	end if
1:	end if
1: end for

1 exploiting nogoods
inferences can be performed using nogoods while establishing  maintaining  generalized arc consistency. we show it with a coarse-grained gac algorithm based on a variableoriented propagation scheme  mcgregor  1 . the algorithm 1 can be applied to any cn  involvingconstraints of any arity  in orderto establish gac. at preprocessing propagate must be called with the set s of variables of the network whereas during search  s only contains the variable involved in the last positive or negative decision. at any time  the principle is to have in q all variables whose domains have been reduced by propagation.
¡¡initially  q contains all variables of the given set s  line 1 . then  iteratively  each variable x of q is selected  line 1 . if dom x  corresponds to a singleton {v}  lines 1 to 1   we can exploit recorded nogoods by checking the consistency of the nogood base. this is performed by the function checkwatches  not described here  by iterating all nogoods involving x = v as watched literal. for each such nogood  either another literal not yet watched can be found  or an inference is performed  and the set q is updated .
¡¡the rest of the algorithm  lines 1 to 1  corresponds to the body of a classical generic coarse-grained gac algorithm. for each constraint c binding x  we perform the revision of all arcs  c y   with. a revision is performed by a call to the function revise  specific to the chosen coarsegrained arc consistency algorithm  and entails removing values that became inconsistent with respect to c. when the revision of an arc  c y   involves the removal of some values in dom y    revise returns true and the variable y is added to q. the algorithm loops until a fixed point is reached.
¡¡the worst-case time complexity of checkwatches is o n¦Ã  where ¦Ã is the number of reduced nld-nogoods stored in the base and n is the number of variables1. indeed  in the worst case  each nogood is watched by the literal given in parameter  and the time complexity of dealing with a reduced nld-nogood in order to find another watched literal or make an inference is o n . then  the worst-case time complexity of propagate is o er1dr + n1¦Ã  where r is the greatest constraint arity. more precisely  the cost of establishing gac  using a generic approach  is o er1dr  when an algorithm such as gac1  bessiere et al.  1  is used and the cost algorithm 1 propagate s : set of variables  : boolean
1:	¡û s
1:	do
1:pick and delete x from q1:if | dom x  | = 1 then1:let v be the unique value in dom x 1:if checkwatches x = v  = false then return false1:end if1:for each c | x ¡Ê vars c  do1:	for  do
1:if revise c y   then1:if dom y   =   then return false1:else q ¡û q ¡È {y }1: end while 1: return true

of exploiting nogoods to enforce gac is o n1¦Ã . indeed  checkwatches is o n¦Ã  and it can be called only once per variable.
¡¡the space complexity of the structures introduced to manage reduced nld-nogoods in a backtracking search algorithm is o n d+¦Ã  . indeed  we need to store ¦Ã nogoods of size at most n and we need to store watched literals which is o nd .
1 experiments
in order to show the practical interest of the approach described in this paper  we have conducted an extensive experimentation on a pc pentium iv 1ghz 1mounderlinux . we have used the abscon solver to run m g ac1  denoted by mac  and studied the impact of exploiting restarts  denoted by mac+rst  and nogood recording from restarts  denoted by mac+rst+ng . concerning the restart policy  the initial number of allowed backtracks for the first run has been set to 1 and the increasing factor to 1  i.e.  at each new run  the number of allowed backtracks increases by a 1 factor . we used three different variable ordering heuristics: the classical brelaz  brelaz  1  and dom/ddeg  bessiere and re¡ägin  1  as well as the adaptive dom/wdeg that has been recently shown to be the most efficient generic heuristic  boussemart et al.  1; lecoutre et al.  1; hulubei and o'sullivan  1; van dongen  1 . importantly when restarts are performed  randomization is introduced in brelaz and dom/ddeg to break ties. for dom/wdeg  the weight of constraints are preserved from each run to the next one  which makes randomization useless  weights are sufficiently discriminant .
¡¡in our first experimentation  we have tested the three algorithms on the full set of 1 instances used as benchmarks for the first competition of csp solvers  van dongen  1 . the time limit to solve an instance was fixed to 1 minutes. table 1 provides an overview of the results in terms of the number of instances unsolved within the time limit  #timeouts  and the average cpu time in seconds  avg time  computed from instances solved by all three methods. figures 1 and 1 represent scatter plots displaying pairwise comparisons for dom/ddeg and dom/wdeg. finally  table 1 focuses on the most difficult real-worldinstances of the radio link frequencyassignment problem  rlfap . performance is measured in terms of the cpu time in seconds  no timeout 
mac+ rst+ rst+ ngdom/ddeg#timeouts avg time1
11
11
1brelaz#timeouts avg time1
11
11
1dom/wdeg#timeouts avg time1
11
11
1table 1: number of unsolved instances and average cpu time on the 1 csp competition benchmarks  given 1 minutes cpu.
and the number of visited nodes. an analysis of all these results reveals three main points.
restarts  without learning  yields mitigated results. first  we observe an increased average cpu time for all heuristics and fewer solved instances for classical ones. however  a close look at the different series reveals that mac+rst combined with brelaz  resp. dom/ddeg  solved 1  resp. 1  less instances than mac on the series ehi. these instances correspond to random instances embedding a small unsatisfiable kernel. as classical heuristics do not guide search towards this kernel  restarting search tends to be nothing but an expense. without these series  mac+rst would have solved more instances than mac  but  still  with worse performance . also  remark that dom/wdeg renders mac+rst more robust than mac  even on the ehi series .
nogood recording from restarts improves mac performance. indeed  both the number of unsolved instances and the average cpu time are reduced. this is due to the fact that the solver never explores several times the same portion of the search space while benefiting from restarts.
nogood recording from restarts applied to real-world instances pays off. when focusing to the hardest instances  van dongen  1  built from the real-world rlfap instance scen-1  we can observe in table 1 that using a restart policy allows to be more efficient by almost one order of magnitude. when we further exploit nogood recording  the gain is about 1%.
¡¡finally  we noticed that the number and the size of the reduced nld-nogoods recorded during search were always very limited. as an illustration  let us consider the hardest rlfap instance scen1   f1 which involves 1 variables and a greatest domain size of 1 values. mac+rst+ng solved this instance in 1 runs while only 1 nogoods of average size 1 and maximum size 1 were recorded.
1 conclusion
in this paper  we have studied the interest of recording nogoods in conjunction with a restart strategy. the benefit of restarting search is that the heavy-tailed phenomenon observed on some instances can be avoided. the drawback is that we can explore several times the same parts of the search tree. we have shown that it is quite easy to eliminate this drawback by recording a set of nogoods at the end of each run. for efficiency reasons  nogoods are recorded in a base  and so do not correspond to new constraints  and propagation is performed using the 1-literal watching technique in-
mac+ rst+ rst+ ngscen1-f1cpu nodes1 1.11 1scen1-f1cpu nodes1 1.11 1scen1-f1cpu nodes1
1.1
11
1scen1-f1cpu nodes¡¡1k1
11
1scen1-f1cpu nodes¡¡1k1
11
1scen1-f1cpu nodes¡¡1k¡¡1k1
1scen1-f1cpu nodes¡¡1k¡¡1k¡¡1kscen1-f1cpu nodes1m¡¡1k¡¡1kscen1-f1cpu nodes1k 1m¡¡1k1kscen1-f1cpu nodes1k 1m1m1mtable 1: performance on hard rlfap instances using the dom/wdeg heuristic  no timeout 
troduced for sat. one can consider the base of nogoods as a unique global constraint with an efficient associated propagation algorithm.
¡¡our experimental results show the effectiveness of our approach since the state-of-the-art generic algorithm macdom/wdeg is improved. our approach not only allows to solve more instances than the classical approach within a given timeout  but also is  on the average  faster on instances solved by both approaches.
acknowledgments
this paper has been supported by the cnrs and the anr  planevo  project nojc1.
