 
the utility problem in explanation-based learning concerns the ability of learned rules or plans to actually improve the performance of a problem solving system. previous research on this problem has focused on the amount  content  or form of learned information. this paper examines the effect of the use of learned information on performance. experiments and informal analysis show that unconstrained use of learned rules eventually leads to degraded performance. however  constraining the use of learned rules helps avoid the negative effect of learning and lead to overall performance improvement. search strategy is also shown to have a substantial effect on the contribution of learning to performance by affecting the manner in which learned rules arc used. these effects help explain why previous experiments have obtained a variety of different results concerning the impact of explanation-based learning on performance. 
1. introduction 
　　　the utility problem in explanation-based learning concerns the ability of learned rules or plans to actually improve the performance of a problem solving system  minton1j. the concern that too much learned information might eventually degrade problem solving performance was initially expressed in the work on learning macro-operators in strips  fikes1 . since then  several studies have found that the unrestricted learning and use of rules or macro-operators can degrade rather than improve overall performance  markovitch1  minton1  minton1a . this decrease in performance is due to time wasted trying to apply learned rules or macro-operators in situations in which they are incapable of efficiently solving the problem at hand. however  other experiments have revealed only overall performance improvement from explanation-based learning even though no attempt was made to limit the number or form of learned rules  1'rorke1  shavlik1|. 
　　　* this research was supported by the department of computer sciences and university research institute of the university of texas at austin. 　　　this paper presents experimental results and informal analysis that may help explain this apparent contradiction in the existing empirical data. the proposed explanation relies on the fact that how learned rules are actually used in problem solving can greatly affect their tendency to improve or degrade overall performance. this focus on the use of acquired knowledge differs from previous attempts at addressing the utility problem  which have focused on forgetting or selectively retaining learned knowledge or on translating it into a more efficient form  markovitch1  minton1  minton1b . 
　　　the paper is organized as follows. section 1 describes the performance system under investigation  a horn-clause theorem prover that learns macro-rules  compositions of existing rules . section 1 presents the basic experimental methodology used to examine the effect of learning on performance and describes the problem sets used in the experiments. section 1 compares two approaches to using learned rules. it shows that unrestricted use of macro-rules can degrade overall performance; however  limiting chaining on learned rules can help alleviate the negative impact of learning and lead to overall performance improvement. section 1 shows how different search techniques such as depth-first and breadth-first can greatly affect the impact of learning on performance by changing how learned rules are used. section 1 concludes and presents some problems for future research. 
1. the performance system 
　　　unfortunately  comparing different explanation-based learning systems is difficult because they generally employ different underlying performance elements and knowledge representation schemes. however  a performance element that has been used in a number of systems is a backwardchaining  depth-first  horn-clause theorem prover  like prolog   hirsh1  kedar-cabelli1  mooney1  pricditis1   a general  well-understood  and popular performance system in ai. 
　　　the system used in the current experiments is a version of eggs  mooney1  mooney1  that includes a horn-clause theorem prover as a performance component. 
when eggs finds a proof for a query by chaining together several existing rules  the proof is generalized using standard explanation-based techniques  dejong1  mitchell1  and compiled into a macro-rule. in the system's list of available rules for each predicate  learned rules are placed before the original rules in the domain theory so that using 
	mooney 	1 
learned rules is preferred to solving a problem from scratch. however  new rules are placed at the end of the set of learned rules so that rules learned from problems encountered earlier in system's experience are tried first  since they probably represent more typical problems . this approach has been empirically found to work better than adding new rules to the beginning of the list of learned rules  shavlik1 . 
     the horn clauses initially given to eggs as a domain theory are simply assumed to be a set of declarative facts rather than a well-ordered prolog program which is guaranteed to terminate  as in prolearn  prieditis1  and prolog-ebg  kedar-cabelli1  . in order to prevent depth-first search from getting lost  possibly in an infinite recursion   the theorem prover is given a depth bound that limits the number of rules it is allowed to chain together. another feature of the system is the ability to heuristically order the antecedents of learned rules in an attempt to make them more efficient. antecedents that have a greater percentage of variables that are likely to be already bound  by occurring in the consequent or a previous antecedent  are positioned earlier in the list. 
1. experimental methodology 
     the effect of learning on performance is empirically examined by having both learning and non-learning systems solve a sequence of problems in a particular domain and comparing their performance. the learning system learns rules from its solutions which are available for solving subsequent problems in the sequence. since search is limited by a depth-bound  not all problems will necessarily be solved. consequently  performance is compared by measuring both the number of problems solved and the total effort expended on the complete problem set  in terms of both cpu time and search nodes generated . time spent on the generalization and learning process itself is generally minimal  less than 1% of the overall time  and is not reported. this simple methodology seems cleaner and fairer than a complicated methodology involving stages in which learning is turned on and off on different selected sets of problems  e.g.  minton1a  . all experiments were run on a texas instruments explorer ii with 1 mb of main memory. 
     two problem domains are used to test eggs and compare its performance with learning to its performance without learning. one problem domain is proving theorems in propositional logic. the problem set used in this domain consists of 1 problems from principia mathematica 
 whiteheadl1  that were first used in experiments with the logic theorist  lt   newell1 . experiments on this problem set with an explanation-based learning version of lt  ebl-lt   1'rorke1  showed overall performance improvement due to learning even though no attempt was made to limit the number of learned rules. for use in eggs  all implies in the theorems were rewritten in terms of not  -i  and or  v   and the system was given the following domain theory for constructing proofs. 

     the other problem domain used to test the system is blocks-world planning. standard blocks-world problems can be solved with a horn-clause theorem prover by stating them as theorems in situation calculus. the blocks-world theory used for the current experiments is a simplified version of the rules used to test the bagger system  shavlik1 . since blocks-world problems are quite difficult to solve using a theorem prover and axioms in situation calculus  the problem sets are restricted to building stacks of one to three blocks given a randomly generated scene with five blocks. the procedure described in  shavlik1  is used to randomly generate problems and initial states. complete problem sets are produced by randomly generating a sequence of 1 stacking problems. 
     although a horn-clause theorem prover is a weak problem solver and can perform quite poorly in certain domains  e.g. planning domains   this does not affect its suitability as a performance element for a learning system. the goal of learning is to improve performance and a weak problem solver simply has more room for improvement. the important factor is not absolute performance but the relative performance of learning and non-learning systems. 
1. full versus limited use of learned rules 
     one approach to using learned rules is to treat them exactly like rules in the original domain theory. as a result  learned rules can be combined arbitrarily with domain rules and with other learned rules. this approach may seem promising since it allows for the greatest possible use of learned information. unfortunately  it can quickly lead to a great deal of effort being spent trying to use a learned rule before eventually giving up and trying another path in the search tree. minton has referred to the fact that  in the general case  matching the antecedents of a learned rule to a set of facts is np-complete  minton1bj. using arbitrary theorem proving to prove them is  of course  even worse  i.e. undecidable . 
     consider the following case in which unrestricted use of learned rules is applied to blocks-world problems. first  given the situation shown in figure la  the system is asked to build a three block tower on table-1. it solves the problem  transfer a table-1   transfer b a   transfer cb   and learns a rule for building a tower of three blocks on a table by inverting an existing tower of three blocks on another table. next  it is given the same goal with the initial state shown in figure lb. since the goal matches the consequent of the rule that was just learned  the system backwardchains on this rule and tries to achieve its antecedents. this requires constructing a tower of three blocks that can then be inverted. the system will waste a great deal of effort trying to achieve this subgoal before eventually finding it is 
　　　
impossible and attempting to solve the problem from 
machine learning 
　　　
scratch. the result is a significant decrease in performance due to learning. since the antecedents of a learned rule frequently involve more constraints than the original goal  using arbitrary problem solving to achieve them is generally much more trouble than it is worth. 
     in logic theorem proving  since learned rules are simply generalized theorems  facts  rather than rules with antecedents  one might suspect that such problems do not arise. however  matching a subgoal to a learned fact may require that variables in the subgoal be bound to particular formulae. these bindings may cause problems in subsequent subgoals and cause the system to backtrack on the decision to use the learned fact  after possibly having wasted a lot of time trying to use it . for example  imagine the system has learned the fact: theorem -ix v  x v y  . given the problem theorem pv-tp   the system might use rule 1 above to generate the subgoals: theorem  x v  p v  p   and theorem x . if the first subgoal is matched to the learned theorem  then the second subgoal becomes: theorem p . the system may waste a great deal of time trying to prove this impossible subgoal which was generated due to the attempt to use the learned fact. consequently  even the use of learned facts that do not have any antecedents may cause considerable wasted effort. 
     in order to prevent the possibility of spending too much time trying to use learned rules  limiting the use of what is learned is perhaps a better alternative. an extreme approach is to only allow a learned rule to be used if it completely solves the problem at hand without being combined with other rules. in other words  the problem solver is not allowed to backchain on the antecedents of learned rules nor is it allowed to use a learned rule  or fact  to solve a subgoal  since both of these actions may result in a great deal of wasted effort if the learned information is not actually relevant. this is a very strict schema-based or scriptbased approach; a learned plan either solves a problem completely or it is simply not used. 
     table 1 presents empirical data  cpu time and search nodes generated  on various versions of the eggs system proving 1 logic theorems from the principia with depth bounds of 1 and 1. the conditions shown are: no learning  learning with unrestricted use of learned facts  full-use   and learning with no chaining with learned facts  limiteduse . full-use learning takes more time than the nonlearning version; however  it solves more problems. this is an ambiguous result with respect to the effectiveness of this method. some clear advantage of full-use learning is shown by the fact that the full-use version with a depth-bound of three expends less than a tenth of the effort of the nonlearning version with a depth-bound of four and yet solves three more problems. however  the results are hard to interpret since both search effort and correctness vary. on the other hand  limited-use learning is incapable of solving more problems than no-learn since  unlike full-use learning  it does not use learned rules to search even deeper into the search tree defined by the original domain theory. however  learned rules can be used to more quickly handle problems solvable by the original theory within the depth bound. the data in table 1 shows that limited-use learning does increase speed relative to no learning without changing correctness and that its relative advantage increases with the size of the search space  i.e. the depth bound . 
     correctness can be completely controlled by using only the problems that can be solved by the original domain theory within the given depth-bound. in this case  only the time to solve the problems varies and can be used as the basis of a fair comparison. table 1 presents data in which correctness has been controlled in this manner. these results clearly show the negative effects of full-use learning and the advantage of limited-use learning. for a depth bound of three  full-use is 1 times as slow as no-learn while limited-use is 1 times as fast. for a depth bound of four  full-use is 1 times as slow as no-learn while limiteduse is 1 times as fast. the fact that the speedup actually increases with the size of the search space for limited-use while it decreases for full-use underlines the conclusion that limited-use learning is best. 

	mooney 	1 
　　　
　　　an issue that has been raised regarding the lt problems is that  as a sequence of problems from the principia  they are explicitly ordered so that later problems build on earlier ones  minton1b . in order to address this issue and present a more detailed picture of the performance of the various methods  figure 1 shows cumulative time curves for random orderings of the problems. cumulative time is the the total time spent solving all problems up to that point. the curves are for the 1 problems solvable by all versions with a depth bound of four. each curve is the average of five random orderings of the problems. not surprisingly  the full-use learning system performs even poorer on randomly ordered problem sets. for the full set  full-use takes on average 1 times as long as no-learn while limited-use takes 1 as long. one would not expect problem ordering to have an effect on limited-use learning since if the rule learned from problem a is capable of completely solving problem b  then the same rule can be learned from problem b and applied to problem a if the order of the two problems is reversed. 
　　　figure 1 shows cumulative time curves for the blocks-world domain. these curves are the average of five randomly generated sets of 1 stacking problems. all problems can theoretically be solved within the depth-bound of 1 supplied to all systems. however  due to time constraints  the full-use learning version was stopped after it had spent about 1 hours on a problem set. in this time  it never got past the third or fourth problem in the set. backward chaining on the antecedents of a learned rule consistently caused 
machine learning 
the system to get lost in a huge search space. in order to increase the applicability of learned rules in the limited-use case  a minimal amount of chaining is allowed with learned rules. specifically  a few efficient rules for concluding various static properties of blocks can be used to prove the antecedents of learned rules. unlike the abysmal performance of full-use learning  on average the limited-use version solves all 1 problems in one third the time of the no learning version.1 
　　　therefore  avoiding or severely limiting chaining with learned rules is clearly preferable to the unrestricted use of learned rules. the explanation-based learning experiments on blocks-worlds problems presented in |shavlik1  also allowed only very limited chaining on learned rules. the current experiments indicate that the limited use of learned rules was crucial to the overall beneficial effects of learning obtained in these experiments. 
1. the effect of search methods 
　　　although the previous section presents data that unrestricted acquisition and use of learned rules degrades performance on the principia problems  experiments with ebl-lt on this problem set showed only positive effects of 
　　　　1  the average number of search nodes explored to solve all 1 problems is 1 1 for no-learn and 1 for limited-use learning. 
　　　
learning  1'rorke1 . limiting the use of learned rules in the manner discussed in the previous section does not explain this apparent contradiction since ebl-lt treated learned theorems just like the initial axioms in the domain theory. 
　　　there are a number of differences between ebl-lt and eggs that could account for the inconsistent results. for example  unlike eggs  ebl-lt is only capable of constructing linear proof trees and it is allowed to learn from unproven theorems. however  a difference between the two systems that greatly affects the use of learned rules is the fact that ebl-lt uses breadth-first search while eggs  like prolog  uses depth-first. when a learned rule is tried with a depth-first theorem prover  the system exhaustively tries to prove its antecedents and may waste a great deal of time if they cannot be proven. a breadth-first prover  on the other hand  pursues all paths  in parallel'' and does not spend an inordinate amount of time first trying to exhaust the use of learned rules before eventually resorting to the initial domain theory  like depth-first does . like the approach described in the previous section  breadth-first search limits the use of learned rules compared to depth-first search. 
　　　a breadth-first horn-clause theorem prover is used to test the hypothesis that switching from depth-first to breadth-first search can change the effect of learning on performance. table 1 presents data on the performance of a breadth-first prover given the lt domain theory and problem set used in the previous experiments. in order to limit run-time  each system is only allowed to generate a certain number of search nodes for each problem. if this limit is exceeded  the system gives up and goes on to the next problem. results are presented for limits of 1  1  and 1 nodes per problem. unlike the results reported for depth-first search  the learning system shows significant improvement in performance compared to the non-learning version. the learning system solves about twice as many problems in 1 to 1 the time and its relative performance improves as the search space gets larger. comparing the data in tables 1 and 1 shows that breadth-first search with learning can solve more problems with less search than depth-first with limited-use learning. however  breadth-first 

takes more time per search node because of the extra memory operations required. it is also important to notice that without learning  the performance of breadth-tirst search in terms of both run-time and search is significantly worse than depth-first since it requires more memory and cannot take advantage of rule-ordering. the overall poor performance of breadth-first search on problems requiring relatively deep proofs prevented the possibility of running it on blocks-world problems. 
1. conclusions and future research 
　　　the data and informal analysis presented in this paper demonstrate that the use of learned information can have a large impact on the utility of explanation-based learning. previous research on the utility problem has focused on the amount or form of learned information rather than on its use in problem solving. unconstrained use of learned rules can clearly lead to degraded performance. however  appropriately limiting the use of learned information can help avoid the negative effects of learning and help insure overall performance improvement. various properties of the performance element can have a significant impact on how learned information is used and consequently on the effect of learning on performance. for example  allowing chaining on learned rules in a depth-first system leads to degraded performance while allowing chaining on learned rules in a breadth-first system does not. therefore  it is unwise to generalize utility results on one performance system to different performance systems. 
　　　one important area for future research is finding a workable compromise between the extremes of full use and no-chaining on learned rules. a good compromise will allow greater use of learned knowledge while avoiding the problems with unrestricted use. allowing a limited amount of chaining with learned rules  as in the experiments with the blocks-world  is one promising approach. in domains like logic theorem proving where learned information takes the form of facts or theorems instead of rules with antecedents  methods are needed for being able to effectively use these facts as lemmas. sometimes a subgoal can be satisfied by a learned theorem without the risk of creating other difficult subgoals. a problem only arises if matching the subgoal to a learned theorem creates bindings for variables in the subgoal that may not satisfy the remaining antecedents  see section 1 . therefore  a good approach may be to allow a subgoal to be proven by matching a learned fact only if the match does not bind any variables occurring in the subgoal. this would allow the use of lemmas without the risk of having to eventually backtrack on this decision. initial experiments with this approach on the lt problems have given promising results. 
　　　as shown by the above experiments  both depth-first and breadth-first search strategies have advantages and disadvantages with respect to being a successful search strategy for a system which learns macros. depth-first iterative-deepening  korf1  exhibits some of the advantages of both depth-first and breadth-first and may be a good strategy for a learning system. some additional specific issues which need further study are the effect of the 
	mooney 	1 
　　　
 minton1a  s. minton   quantitative results concerning the 
utility 	of 	explanation-based 	learning   
proceedings of the national conference on artificial intelligence  st. paul  m n   august 1  pp. 1.  minton1b  s. minton   learning effective search control 
knowledge: an explanation-based approach   
ph.d. thesis  department of computer science  
carnegie-mellon 	university  	pittsburgh  	pa  1.  mitchell1  t. m. mitchell  r. keller and s. kedar-cabelli  
 explanation-based generalization: a unifying view   machine learning i  1  1   pp. 1.  mooney1  r. j. mooney and s. w. bennett   a domain 
independent 	explanation-based 	generalizer   
proceedings 	of 	the 	national 	conference 	on 
artificial intelligence  philadelphia  pa  august 1  pp. 1.  a longer updated version appears as technical report uilu-eng-1  coordinated science laboratory  
university of illinois at urbana-champaign   mooney1  r. j. mooney   a general explanation-based 
learning mechanism and its application to 
narrative 	understanding   	ph.d. 	thesis  
department of computer science  university of illinois  urbana  il  january 1.  also appears as technical report uilu-eng-1  coordinated science laboratory  
university of illinois at urbana-champaign.   newell1  a. newell  j. c. shaw and h. a. simon  
 empirical explorations with the logic theory 
machine: a case study in heuristics   in computers and thought  e. feigenbaum and j. 
feldman  ed.   mcgraw-hill  new york  ny  1.  1'rorke1  p. v. o'rorke   lt revisited: experimental results of applying explanation-based learning to the logic of principia mathematica   proceedings of the 1 international machine learning workshop  irvine  ca  june 1  pp. 
1.  prieditis1  a. e. prieditis and j. mostow   prolearn: 
towards 	a 	prolog interpreter that learns   
proceedings 	of 	the 	national 	conference 	on 
artificial intelligence  seattle  wa  july 1  pp. 1.  shavlik1  j. w. shavlik   generalizing the structure of explanations in explanation-based learning    
ph.d. thesis  department of computer science  university of illinois  urbana  il  january 1.  also appears as technical report uilu-eng1  coordinated science laboratory  
university of illinois at urbana-champaign.   whiteheadl1  a. n. whitehead and b. russell  principia mathematica  cambridge university press  cambridge  england  1.  dejong1  
 fikes1  
 hirsh1 g. f. dejong and r. j. mooney   explanationbased learning: an alternative view   machine learning 1  1  1   pp. 1. 
r. e. fikes  p. e. hart and n. j. nilsson  
 learning and executing generalized robot plans   artificial intelligence 1  1  1   pp. 1. 
h. hirsh   explanation-based generalization in a logic-programming environment    korf 	1  r. e. korf   depth-first iterative deepening: an optimal admissible tree search   artificial intelligence 1  1  1   pp. 1.  markovitch1  s. markovitch and p. d. scott  'the role of 
forgetting in learning   proceedings of the fifth 
international conference on machine learning  ann arbor m i   june 1  pp. 1.  minton1  s. n. minton   selectively generalizing plans for problem-solving   proceedings of the ninth 
international joint conference on artificial intelligence  los angeles  ca  august 1  pp. 
1.  minton1  s. minton  j. g. carbonell  o. etzioni  c. a. 
knoblock and d. r. kuokka   acquiring effective search control rules: explanationbased learning in the prodigy system   ordering of learned rules1 and their antecedents and the effect of learning rules for subgoals and subsequences of solutions. another important research area concerns the effect different domains and problem solving architectures have on the relative performance of various approaches such as learning heuristic control rules versus learning macro-operators  minton1a . further investigations will hopefully result in even better ways for insuring that explanation-based learning improves overall system performance. 
acknowledgements 
     i would like to thank jude shavlik and paul o'rorke for helpful discussions on the utility of explanation-based learning and for contributing to the code and/or data used in the experiments. 
