 
reinforcement learning is a technique to learn suitable action policies that maximize utility  via the clue of reinforcement signals: reward or punishment. q-learning  a widely used reinforcement learning method  has been analyzed in much research on autonomous agents. however  as the size of the problem space increases  agents need more computational resources and require more time to learn appropriate policies. whitehead proposed an architecture called modular q-learning  that decomposes the whole problem space into smaller subproblem spaces  and distributes them among multiple modules. thus  each module takes charge of part of the whole problem. in modular q-learning  however  human designers have to decompose the problem space  and create a suitable set of modules manually. agents with such a fixed module architecture cannot adapt themselves to dynamic environments. here  we propose a new architecture for reinforcement learning called amql  automatic modular q-learning   that enables agents to obtain a suitable set of modules by themselves using a selection method. 
through experiments  we show that agents can automatically obtain suitable modules to gain a reward. furthermore  we show that agents can adapt themselves to dynamic environments efficiently  through reconstructing modules. 
1 	introduction 
reinforcement learning is one of the techniques by which agents can learn suitable action policies that maximize 
   *also with  research for the future  project  faculty of science and technology  keio university  shin-kawasakimitsui building west 1f  1 kashimada  saiwai-ku  
kawasaki  1  japan 
   +also with sony computer science laboratory inc. 1 higashi-gotanda  shinagawa-ku  tokyo  1  japan 
1 	learning 
their utilities  via reinforcement signals of reward or punishment. there have been various investigations of autonomous agents using this learning technique. however  reinforcement learning with monolithic methods lacks scalability  since agents require much more time to learn suitable action policies when dealing with more complex and larger problems. additionally  agents use more computational resources to memorize the utilities of all states and actions. 
　to tackle this problem  some methods that decompose the problem space have been suggested. whitehead proposed an architecture called modular q-learning  that decomposes the whole problem space into smaller subproblem spaces and distributes them among multiple modules. the goals of multiple goal problems are decomposed into subgoals  which are then distributed as subgoals among multiple modules. since each module learns only to accomplish its own goal  the number of states that each module can take decreases in comparison with monolithic q-learning. as a result  whitehead showed that modular q-learning improves on learning time and requires less computational resources. 
　modular q-learning and other methods that decompose the problem into modules need a human designer to decompose the whole problem and design an appropriate set of modules. this is because the learning performance depends on the design of the module set. however  agents with fixed modules might not be able to adapt to dynamic environments flexibly. 
　in this paper  we propose a new architecture that enables agents to obtain a suitable set of modules through interaction with the environment  so that agents can flexibly adapt to dynamic environments. also  we experimentally evaluate this architecture. as an example of a learning problem that is computationally intractable for agents  we take the pursuit gam e problem. we investigate the suitability of this architecture and its adaptability under dynamic environments. 
1 	learning with the modular approach 
in this section  we describe the basics of q-learning and modular q-learning. also  we show an example that uses modular q-learning. 

1 	reinforcement learning  q-learning  
autonomous creatures generally receive reinforcement signals from the environment for their actions or action sequences. reinforcement learning is a technique for an agent to learn suitable action policies that maximize a utility from the clues of reinforcement signals. utility is often represented by the discounted cumulative reinforcement in the future  as follows: 

where vt is the discounted cumulative reinforcement from time t through the future  rt is the reward received at time f  and  is the temporal discount factor. 
　q-learning  watkins  1  is a widely used method of reinforcement learning  where learning corresponds to building a precise q-function. 

the q-function gives the estimated utilities that agents receive when they take an action in a state  and the agents refer to this function to decide on their actions. for deterministic domains  the utility of an action a in response to a state x is equal to the immediate payoff r plus the best utility that can be obtained from the next state y. however  during the course of learning  the qfunction may not be true. therefore  the value of q x  a  is updated in the following way: 

where  	s the learning rate  maxq  / 1  i s 
b 
the maximum q-value at state y  and 1 is the discount rate. 
　the simplest way to express the q-function is to make a q-table that contains each q-value for every pair of state and action. however  as the size of the problem space increases  this takes more computational resources and requires much more time for learning. 
1 	modular q-learning 
some research shows that methods which decompose a problem space improve learning performance in reinforcement learning  dayan and hinton  1   singh  1 . one investigation of the modular approach shows the improvement of learning performance  whitehead et a/.  1 . 
　a modular q-learning architecture contains a number of modules  each of which dedicates itself to the corresponding subproblem. this architecture makes the qtable smaller and improves learning time for problems with multiple goals. 
　in modular q-learning  each module has its own subgoal and decides its action policy according to its subgoal. an arbiter is used to mediate global action with a certain strategy  decision by majority  etc. . each module includes a q-table for the pairs of partial states and actions. modules learn to achieve their respective 

figure 1: pursuit game 
goals through updating their q-values. since individual modules cannot consider other modules' goals  learning agents may yield suboptimal performance. what is worse  even if human designers succeed in designing an appropriate set of modules for a situation  such a predetermined method reduces agents' ability to adapt to dynamic environments. 
　ono  applied modular q-learning to the learning of cooperative behavior of multi-agents in a pursuit game 
 figure 1 a  . the pursuit game that was presented by benda  is a basic problem of distributed artificial intelligence. the purpose of this problem is to capture a prey agent by making four hunter agents surround the prey  figure 1 b  . in ono's research  hunter agents learned the optimal action policy to capture the prey. ono's learning agents consist of three modules. each module takes the relative position of the prey and of one of the other hunters as its state. 
　considering the state space  for example  in the case of each hunter agent having a limited visual field of depth  1 x 1   as shown in figure 1 a   the possible number of states that agents may encounter reaches  1 + 1 = 
1. this means a learning agent with a monolithic q-table needs enormous memory resources. however  when only two agents organize separate q-tables  the possible number of states decreases to  1 + l 1 1. through experimenting with architectures in the pursuit game  ono showed that agents with modular q-learning 
	kohri  matsubayashi  & tokoro 	1 

not only solve state space issues but also learn cooperative behavior. however  the method which ono took was to decompose the state space and allocate suitable modules of modular q-learaing in advance. 

figure 1: amql architecture:  a  element selection phase   b  fitness allocation phase  and  c  module selection phase. 
1 	a m q l architecture 
in modular q-learning  human designers have made modules suitable for the environment. the resulting agents with fixed modules cannot adapt to dynamically changing environments. here  we propose a new architecture  called amql  automatic modular q-learning   that enables agents to obtain a suitable set of modules by themselves. also  we experimentally show the availability of this architecture. 
1 	learning 
　the amql architecture has three phases when it obtains the set of modules. firstly  modules in agents select elements of the environment  and make a q-table  figure 1 a  . while agents learn  the amql mechanism provides fitness estimates for modules that contribute a reward acquisition action  figure 1 b  . at selection time  modules are evaluated and selected according to fitness  figure 1 c  . after repeating these three phases  agents obtain a suitable set of modules. in short  amql executes module selection like a genetic algorithm. the specific mechanisms are as follows. 
1 	element selection phase 
in the amql architecture  agents have a fixed number of modules. each agent recognizes the environment as a state of n elements e1  e1         en . in other words  the input of an agent is as follows. 

where si is a state of element ei  elements correspond to the sensors of robot agents. each module randomly selects 1 - m elements  1   rn   n  from the set of elements e  and makes a q-table for the states of selected elements. through this process  the state space is allocated to multiple modules. 
1 	fitness allocation phase 
when agents receive a reward by an action  the modules that contribute to this action are allocated a fitness. this fitness indicates the degree of contribution of a module. 
　agents decide their action by using the greatest mass  gm  strategy  proposed by whitehead . 

the expression  where f1m s  is an action policy of an agent  represents that the q-values of / modules are summed up for all possible actions  and the action that has the largest sum is selected. 
　when agents receive a reward at state s and action a  the q-value at state s is compared for each module. modules that have the q-value of action a have the largest sums. in other words  all modules that desire action a increase their fitness. the fitness is cleared at every stage of module selection  and is added whenever the module contributes to the acquisition of a reward. 
　modules do not refer to other modules' q-tables and each updates its own q-table individually. the goal of the module is to learn appropriate action policies for inputs of elements to which the module should pay attention. modules that pay attention to the appropriate subset of elements can learn appropriate action policies. therefore  appropriate modules achieve greater fitness. 
1 	module selection phase 
at every period of module selection t  agents evaluate their own modules. in evaluating modules  selection mechanisms assess the fitness of each module. modules 

whose fitness is over the threshold remain  but if fitness is under the threshold  modules select elements of the environment again  and make a new q-table. threshold at module selection is represented as follows: k =  number of reward acquisition  x p 
in the case that p 1   p   1  is a constant that determines how good modules must be to be retained  the closer p is to 1  the higher the quality of the module selected. 
1 	algorithm 
the amql architecture consists of the above three phases. the actual algorithm is as follows: 
1. each module selects 1 - m elements from n elements that organize the environment. modules then make a q-table and set an initial q-value. 
1. set the fitness of all modules to 1. 
1. set the execution counter t of agents to 1. 
1. set agents to initial states. 
1. agents receive the current state x. 
1. if state x is the goal state  then t = t + 1 and go to 
1. 
1. agents select their own action by the gm strategy and execute it. 
1. update q-tables of each modules. 
1. if agents receive a reward with action a  then set the fitness of the module  that q-value of action a is the biggest in every action  to fitness - fitness + 1. 
1. go to 1. 
1. if t   t  the period of select   then go to 1. 
1. calculate a fitness threshold and evaluate fitness of all modules. modules where fitness is under the threshold select 1 - m elements again. these modules then make a q-table and set an initial q-value. go to 1. 
1 	implementation and evaluation 
in this paper  we use the pursuit game as a problem that is computationally intractable. specifically  we investigate the following. firstly  we compare the performance of the amql architecture with previous architectures. also  we investigate the adaptability of amql architecture in dynamic environments compared to fixed module q-learning. 
1 	simulation environment 
we assume that the fundamental simulation environment of the pursuit game is as follows: 
  the environment consists of a 1 x 1 grid. edges are connected  torus . 
  initial positions of each agent are determined ran-domly. 

figure 1: time steps till caption at each trial 
  at each time step  agents choose an action from any of 1 actions  move north  east  south  west  or stay at their current position . more than one hunter agent can share the same grid. however  hunter agents cannot share with the prey. therefore  hunter agents that try to move to the grid already occupied by the prey  cannot move and must stay at their current positions. the prey agent selects its own action randomly. . 
  hunter agents have a 1 x 1 sight  as shown in fig-ure 1 a . each agent is assigned an identifier  agent 1 . a hunter agent can recognize the relative position and identifier of any other agents in its sight. 
  a trial ends when the goal is accomplished  the prey agent is captured  or at 1 time steps. 
parameters for the amql architecture are as follows. 
a hunter agent has four modules. the period of module selection t is 1 trials. the constant p that determines the threshold of the module is 1. when modules are initialized or there are some modules that are regarded as not adaptable at the module selection phase  these modules select any 1 - 1 other agents randomly and make a q-table. 
　parameters for q-learning are as follows. the learning rate is  = 1 . 1   the discount factor is  = 1  a reward is r = 1  and the initial value of the q-value is 1. 
1 comparison among a m q l and previous architectures 
we have compared the amql architecture with previous architectures: monolithic q-learning and a fixed module architecture. hunter agents with the monolithic architecture have one q-table for all possible states. in 
	kohri  matsubayashi  & tokoro 	1 

the case of fixed modules  hunter agents have three modules that pay attention to each different hunter agent and the prey agent. this set of modules is the same as ono's . one execution is performed in up to 1 trials. figure 1 shows the time steps taken for capture at each trial. each experiment is repeated ten times  and the averages of these are plotted. 
   the results of the monolithic architecture show that it takes a long time to learn suitable behaviours. this is because the number of states that the monolithic qtable has to take increases to 1 = 1  so that it takes a long time to update q-values suitably. however  using amql or the fixed module architecture  agents can capture the prey agent with less time steps. in almost every experiment  the amql architecture selected the module that takes states of two or three agents as an input. therefore  the number of states that amql modules take becomes 1 = 1 or 1 = 1  and agents with the amql architecture can learn to capture the prey faster. comparing learning speed and quality of solution for this simple case  the fixed module architecture previously designed by human designers is better than the amql architecture. after learning  agents with amql took about 1 time steps more on average than the well formed modules. however  in every experiment  agents with amql succeeded in obtaining a suitable set of modules. 
   the reason why convergence of amql is slower is that the amql architecture takes time to find a suitable set of modules. in other words  in the case of fixed module architecture  human designers are needed to design a suitable set of modules. the amql architecture cannot only learn faster  but can also obtain suitable modules automatically. these two features are important for autonomous agents. this result shows that the amql architecture has the capability to obtain suitable sets of modules with improvement of learning speed due to the modular architecture. 
1 	adaptation to changing environments 
　in the next experiment  we evaluate the adaptability to problem change. to change the problem dynamically  we set the following additional conditions. 
  there are four hunter agents and two prey agents in the grid field. each prey agent has an identifier  preyl  prey1 . 
  from the beginning to trial 1  hunter agents re-ceive a reward only when the prey agent 1 is captured  but no reward when the prey agent 1 is caught. however  from trial 1  the environment changes so that hunter agents receive a reward only when prey 1 is captured. 
　in this experiment  we compare the adaptability to dynamic environments  between the amql architecture and the fixed module architecture that is designed by human designers. the fixed module architecture in this experiment is the same as that in the previous experiment: hunter agents have three modules that pay at ten-
1 	learning 

figure 1: time steps till caption at each trial. comparison between amql and fixed module architecture in a changing environment. 
table 1: elements of modules of hunter1 in a changing environment 
module before change after change modutel hunter1 
hunter1 
preyl hunter1 
hunter1 
prey1 module1 hunter1 
hunter1 
hunter1 hunter1 
hunter1 
prey1 module1 hunter1 
hunter1 
preyl hunter1 
preyl 
prey1 module1 hunter1 
hunter1 hunter1 
prey1 tion to each different hunter agent and prey agent 1. no modules pay attention to prey agent 1. 
　figure 1 shows the time steps for capture at each trial. each experiment is also repeated ten times  and the averages of these are plotted. before the problem is changed  the convergence of learning with the fixed module architecture is faster than with the amql architecture. however  the fixed module architecture cannot capture well in the environment after the problem changes. this is due to the set of modules being designed for the environment before the change. however  agents with the amql architecture can capture well even after changing the problem  because the architecture changes the modules when the target is changed. 
　table 1 shows the changes of elements to which the modules of hunter agent1 pay attention. this result shows that agents have some modules that pay attention to preyl before changing  and that these modules pay attention to prey1 after changing  because agents always have to pay attention to elements that are related to reward. moreover  when agents pay attention to elements that are not related to reward  agents waste learning time and computational resources. 
　human designers cannot design a suitable set of modules in dynamically changing environments  since they cannot predict the future environment. however  agents with amql architecture obtain suitable  though still not optimal  sets of modules that consider their own reward and learning performance in a current environment. 
　figure 1 shows that convergence of amql  immediately after changing environment  takes longer to converge than at the beginning. this is because q-functions immediately after changing have learned to catch prey1. this obstructs the module in learning the new problem. however  this feature can be an advantage  if what modules have learned can be used even after the environment has changed. 
　these experimental results show that an agent with the amql architecture is able to obtain a suitable set of modules and to adapt according to changing problems. 
1 	discussion 
there has been much research dealing with modular architectures for reinforcement learning. thrun and schwartz offered the skills algorithm which obtains the structure of problem space that can be used among multiple tasks and called skills  and considered both performance loss and description length . however  to obtain useful skills takes more time than to find optimal policies with a monolithic q-function. amql makes the description length smaller and learning convergence faster than monolithic architecture  though not optimal. we consider that the idea of considering performance and description length is useful for amql. 
　at the present time  modules in amql have lookup tables  because these are easy to treat and to analyse. also  general function approximators such as neural networks have been applied to reinforcement learning. sabes and jordan discussed the association between reinforcement learning and expert networks . we consider that this model also can be applied to amql. in this case  each module obtains an appropriate part of input using amql. such an architecture can be expected to have faster learning convergence and more adaptability than a monolithic architecture. 
　it is important to know how autonomous agents should design their adaptive mechanism for the environment. the real world is an open system  and changes dynamically. thus  it is difficult to predict the behavior of future environments perfectly and to design optimal structures. in that respect  amql seems a promising architecture  because it can dynamically and automatically obtain suitable module structures through interactions with the environment  and learn faster than monolithic q-learning architectures. we consider the amql architecture to be applicable to various autonomous agents in order to improve both the adaptability and the learning time. 
1 	conclusions and future work 
in this paper  we proposed an amql architecture that obtains a suitable set of modules through the interaction with the environment. we showed the availability of the architecture and its adaptability to dynamic environments  through experiments on pursuit game problems. simulation results showed that agents with the amql architecture can not only learn faster but also can obtain suitable sets of modules automatically. moreover  through experiments where the problem changed dynamically  we showed that agents with the amql architecture can adapt themselves to dynamically changing environments  in a way that was impossible for agents with the previous fixed method. these features enable autonomous agents to adapt more flexibly and efficiently. 
　when the environment changes drastically  the current amql architecture may abandon modules which have learned strategies that are no longer relevant. it seems useful to keep such modules  in case a similar situation arises in the future. we plan to investigate this architecture. 
