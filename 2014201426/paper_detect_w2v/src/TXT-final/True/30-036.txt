 
a central problem in case based reasoning 
 cbr  is how to store and retrieve cases. one approach to this problem is to use exemplar based models  where only the prototypical cases are stored. however  the development of an exemplar based model  ebm  requires the solution of several problems:  i  how can a ebm be represented   ii  given a new case  how can a suitable exemplar be retrieved   iii  what makes a good exemplar   iv  how can an ebm be learned incrementally  this paper develops a new model  called a probabilistic exemplar based model  that addresses these questions. the model utilizes bayesian networks to develop a suitable representation and uses probabilistic propagation for assessing and retrieving exemplars when a new case is presented. the model learns incrementally by revising the exemplars retained and by updating the conditional probabilities required by the bayesian network. the paper also presents the results of evaluating the model on three datasets. 
1 	introduction 
case based reasoning  cbr  is an approach that utilises past situations in an attempt to solve new problems. the basic cbr cycle involves retrieving cases that are similar to the current problem and utilising them to solve the current problem. this makes memory organisation and indexing a fundamental part of cbr systems. one approach is to store a flat database of cases and scan all the cases to identify the most similar cases. for applications where many more are involved  this simple organisation is considered to be slow  kolodner  1 . 
　a more sophisticated method is to partition the cases into clusters and organise them hierarchically. the hierarchy can then be searched more efficiently by following a path depending on the features of the new case. different types of hierarchies have been proposed leading to different approaches. one approach is to use decision trees so that the leaf nodes contain the cases and where the internal nodes contain questions that can be 
1 	case-based reasoning 
used to partition the cases. so for example  systems like remind  althoff et o/.  1  provide a tree induction algorithm that can be used to avoid examining all the cases. this kind of approach is particularly useful when large databases of cases are already available. however  when cases are not available in advance  and the domain is not well defined this approach is more difficult to apply. 
　another approach is to use an abstraction hierarchy where each internal node is an abstraction of the cases represented by its children. these hierarchies are known as discrimination networks or redundant discrimination networks when the nodes represent overlapping regions of cases. a number of research systems  such as mediator and casey  see  kolodner  1  for a description and references  have used this approach and their outcomes have shown its utility. however  these systems require much more memory to store the network and the procedures for adding new cases arc very expensive since the abstraction hierarchy may need to be restructured  kolodner  1 . 
　thus  current approaches to cbr work well in some situations  but also have problems in other situations. in particular  for domains  sometimes called weak dojnains  porter et a/.  1   where:  i  the categories or concepts are difficult to define by necessary and sufficient features   ii  the categories can be non-disjoint   iii  the data are not structured   iv  all the data do not exist in advance  and  v  there is uncertainty in how the categories are represented by cases  these approaches have limitations since they often require all the features and examples in advance  and do not handle uncertainty explicitly  they use a weighted sum of the differences . 
　an alternative approach  that is perhaps more applicable to weak domains  is to store only prototypical cases. this approach  known as the exemplar  based model has its basis in cognitive theories  which postulate that concepts can be represented by exemplars  smith and medin  1 . exemplar based models do not necessarily require all the features or all the cases in advance. 
　this paper focuses on developing an exemplar based model. the next section presents the main problems of developing exemplar based models. the paper is organised as follows: section 1 presents the model in terms 

of the knowledge representation  the classification  and learning processes; section 1 presents an empirical evaluations on three datasets; and section 1 presents the conclusions. 
1 	the problem 
to understand the problem  consider the diagram shown in fig. 1 that shows a weak domain in which there are two categories a and b  solid lines . the category a has nine cases  the points  and and the category b has five cases and 
 note that the cases that occur in both categories. the main problem is to 

figure 1: example of a weak domain. 
proceed from a view like the one shown in fig. 1 to an exemplar based view like the one shown in fig. 1 where the exemplars e1 e1 e1  and e1 represent sets of similar cases  dashed lines . that is  instead of storing all the 

figure 1: exemplar based view in weak domain. 
cases  only the prototypical cases are stored. although conceptually  this is an elegant idea  attempting to develop it raises the following difficult questions: 
1. what is a good representation of the model  
1. how can a new case be classified  
1. what notion of similarity can be adopted  
1. what makes a good exemplar  
1. how can the model be learned incrementally  
　the next section of this paper develops the model by addressing these questions. 
1 	the model 
1 	the knowledge representation 
one way of representing the information in fig. 1 is to use a network in which nodes can be used to denote exemplars  features  and categories. thus  fig. 1 shows the network representing the exemplar based model shown in fig. 1. in this representation  the dashed lines show the relationship between categories and exemplars  and the solid lines show the relationship between exemplars and their features. so for example  category a has the exemplars e1  e1  and e1 and exemplar e1 has the features f1 f1  and f1. notice that exemplars can be shared by categories  and features can be shared by exemplars. 
　as it stands  fig. 1 is not an adequate representation of an exemplar based model since it does not contain any information about the degree of dependency between a category and its exemplars and an exemplar and its features. so for example  a car can have features such as colour  engine  and make. but  which of them is more relevant in the representation of a car  the above representation would not differentiate between the strong dependency: an object being a car and having an engine  and the weak dependency: an object being a car and its colour. 

figure 1: a basic exemplar based representation. 
   hence  to include the strengths of such dependencies  the relationships between exemplars and features are represented as probabilistic dependencies. that is  each feature   that is a leaf node in the network  is labelled with the conditional probability   where  are the exemplars that share the feature fj. sim-
ilarly  the importance of an exemplar in the category is represented by probabilistic dependencies. each exemplar c   which is an intermediate node in the network  is labelled with the conditional probability where jc is the joint category formed by the parents of  this probability is the prior probability of the exemplar when no evidence is available. with this additional information  the network of fig. 1 becomes a hybrid representation. 
1 	t h e classification process 
given the above representation  how can the following questions  raised earlier  be addressed: 
  how can a new case be classified  
  what notion of similarity can be adopted  
	rodriguez and vadera 	1 

　the majority of current cbr systems address these questions by adopting a similarity metric  which is a 
　weighted sum of the differences between a new case and a stored case  kolodner  1 . the main problem with this approach is that the weights of the similarity metric need to be estimated and obtaining reliable weights is not easy for non-triviai problems  see  wettschereck et al  1  for a survey . 
　in this paper  the notion of similarity adopted is that two cases are similar if they are represented by the same exemplar. but how can one determine if a new case is represented by a particular exemplar  since in the above representation  the lower network that relates exemplars and features is a bayesian network  the degree to which a new case with features finc-  fqnc is represented by an exemplar e can be computed by: 

this computation can be carried out by using propagation methods developed by pearl . 
　given this capability of calculating the extent to which an exemplar represents a new case  all the exemplars could be investigated  in theory at least. however  probabilistic propagations methods can be computationally expensive  it is known to be np-hard in general  and investigating all the exemplars is therefore not practical. 
　hence  first it is necessary to rank the categories in order of the likelihood of them containing a suitable exemplar. this ranking has to be performed in a way that avoids missing suitable exemplars but is computationally efficient. this ranking can be obtained by utilizing an observation by smith and medin  who point out that: 
 the features that represent a concept are salient ones that have a substantial probability of occurring in instances of the concept . 
　thus  the important features will have high values of occurrence given an exemplar  i.e.  high values of p fj | e . hence  a reasonable way of ranking the categories is to obtain the contribution of the features of the exemplar that are present in the new case  averaged over the number of features in the exemplar ei 

where 

in this equation  nc is a new case and n/e i is the number of features in the exemplar ei. 
　then  the categories can be ranked in order of the rank of their exemplars. once the ranking is obtained  a suitable investigation strategy can be adopted. for example  the list of categories can be investigated in order of rank until a good exemplar is found. within each category  propagation methods can be used to assess the merits of an exemplar e by computing  and stopping if this is above a threshold that is normally dependent on the application. 
1 	case-based 	reasoning 
1 	the learning process 
learning an exemplar based model incrementally involves two aspects:  i  learning the the model and  ii  estimating its parameters  both of which are described in this subsection. 
learning the model 
the learning process of an exemplar based model needs to answer the following questions  that were raised earlier: 
1. what makes a good exemplar  
1. how can the model be learned incrementally  
to answer these questions  consider a situation where there is a category c that is represented by three exemplars e1 e 1 arrives- suppose a new training case withcategory c arrive can arise:  i  the new case is not classified by the exemplars in c  and  ii  the new case is correctly classified by an exemplar in c. 
　in the first case  clearly the new case should be retained as a new exemplar since it must be different from the other exemplars. in the second case  criteria need to be developed for deciding which of the two  the new case or exemplar  will be the best representative of all cases in the region. 
　for exemplar based models these criteria have to be based on the notion of prototypicality. before describing the measure of prototypicality used in this work  it is necessary to first describe the idea of a summary representation. earlier  an exemplar was represented as a bayesian network with dependencies from the exemplar to its features. in general  an exemplar may not have the same features as all the similar cases that it represents. for example  an exemplar e1 may have the features f1  f1  and f1 while the union of all the features of the cases it represents may be f1 f1 f1 f1  and f1. a summary representation is a bayesian network where all the features of the similar cases are included. 
　returning to the notion of prototypicality  the problem is to develop a measure of prototypicality so that a good prototype can be selected. rosch and mervis  argued that a case is an ideal prototype if : 
  it has the highest family resemblance with other members in the same category   this is known as 
focality  biberman  1   and 
  it has the least family resemblance with members of other categories  this is known as peripherality  biberman  1  . 
in the context of the model being developed here  family resemblance is viewed as the collection of similar cases and which have a summary representation. in terms of regions  a case that maximizes the probability of covering a region can be considered to have the highest family resemblance. since the summary representation denotes regions  and takes the form of a bayesian network  a suitable measure of focality of an exemplar ei is the probability of covering a region: 


where sr ei  denotes the summary representation of the region that contains ei. 
likewise  a suitable measure of peripherally is obtained by working out the average probability of an exemplar representing regions in other categories: 

these two measures can be used to define a measure of prototypicality as follows. since a good prototype is one that has the greatest focality and the least peripherally  the measure of prototypicality adopted here is: 

this measure of prototypicality can now be used to decide which case makes the better exemplar in a region. these considerations lead to a learning algorithm that can be summarised as follows. given a new training case  nc  first use the classification process described above. if the case is not classified successfully  then the training case becomes an exemplar. if it is correctly classified by an exemplar e then use the above prototypicality measure to determine which of the two best represents the region of cases and retain the more representative one. 
estimating the probabilities 
to use the above classification and learning processes  one needs the probabilities that define the bayesian network which represents the exemplars. since the model is incremental  and the cases are not retained  estimating the probabilities in a manner that enables a good exemplar based model to be learned is a non-trivial problem. 
　the bayesian exemplar based model requires the estimation of two parameters that need to evolve as new cases are seen: 
1. prior probabilities of the exemplars in the joint category .  and 
1. the conditional probability 
the first of these is obtained in a standard way by utilising the beta distribution which leads to the following equation  see  lindgren  1  for details  that can be used to compute and update the prior probabilities: 

	estimating 	the 	conditional 	probabilities 
p f parents f   is much more difficult. in general  1 n + l probabilities need to be estimated for n parents. in particular  there may not be enough cases in the intersection of the parent events  even if there are enough cases in the regions represented by the parents. this means that estimates of probabilities such as  could only be based on a small number of cases and would therefore be inaccurate even when many cases have been seen. 
to overcome this problem  the noisy or model  pearl  
1  is considered. if this model can be adopted  then instead of requiring  is needed  for each parent ei of f. to see if the noisy or model can be used  consider the assumptions that it makes  pearl  1 : 
accountability an event is false  = 1  if all conditions listed as causes of are false. 
exception independence if an event mj is a consequence of two conditions d1 and d1  then the inhibition of the occurrence of  under is independent of the mechanisms of inhibition of under d1. 
　in the context of this model  the exception independence assumption can be interpreted as requiring that the absence of the feature given one exemplar is independent of the absence of the feature given another exemplar. the extent to which this assumption holds depends on the way the exemplars are selected. in section 1  the selection scheme uses a measure of prototypicality that aims to reduce the possibility of selecting exemplars that represents similar regions. that is  the selection scheme used minimizes the possibility of the exception independence assumption being broken. 
　the accountability assumption requires that if a case is not represented by the parent exemplars of a feature  then that feature does not occur in the case. although this may hold when an accurate exemplar based model has been learned  it clearly does not hold while it is still learning  e.g. consider a new case that should be a new exemplar . to overcome this problem  an additional virtual exemplar is added in the representation of each category. this additional exemplar can be viewed as representing all the cases that have not yet been seen and therefore ensures that the accountability condition holds. with this additional exemplar  the revised model is illustrated in fig. 1. as the figure shows  this introduces dependencies between the virtual exemplar and the features. but how can the strengths of the dependencies be estimated  since the virtual exemplar represents unseen cases  estimating the strengths of these dependencies is 

figure 1: virtual exemplar. 
a task that requires predicting the behaviour of the dependencies as more cases are observed. this behaviour can be expected to have the following characteristics: 
  the strengths of the dependencies should be the highest initially when no cases have been seen and ignorance is greatest. 
  as more cases are observed  the strengths of the dependencies can be expected to decay since the 
	rodriguez and vadera 	1 

virtual exemplar will represent fewer unseen cases. 
  there is always a small chance that a new case will be in the region represented by the virtual exemplar even after many cases have been observed. 
　there may be several functions that satisfy these characteristics. however  this work utilises the exponential function  which is often used to represent decay  e.g.  in modelling radioactivity  and takes the form: 

where n is the number of cases in a category and the parameters a and a determines the rate of decay. the lower bound of 1 in this function reflects the possibility that a new case will be in the region represented by the virtual exemplar even after many cases have been seen. 
　this completes the description of how the probabilities can be learned incrementally  thereby allowing the use of the classification and the learning procedures. 
1 	empirical evaluation 
the model described has been implemented and evaluated on three datasets  votes  zoo  and audiology  available from the university of california repository of datasets. due to the lack of space  this section presents only a very brief summary of the results which are presented more fully in  rodriguez  1 . table 1 summarises the characteristics of each dataset. each exper-

iment randomly partitioned the data into a 1% training set and a 1% testing set  and was repeated 1 times to obtain an average accuracy and a compression ratio  defined as the proportion of cases not retained  for each class. all the experiments were carried out with the parameters a and a set to 1 v d 1 and a threshold of 1. although no attempt  has been made to find optimal values for these parameters  the model works well with these values. the problem of recommending their optimal values given the characteristics of the domain is a subject for future work. tables 1 and 1 give the results for the votes and the zoo datasets. as the results show  the model performs well both in terms of accuracy and the number of exemplars retained. the overall compression ratio for the votes dataset is 1% with an accuracy of 1%. the overall compression ratio for the zoo dataset is 1% with an accuracy of 1%. in table 1  an interesting difference in accuracy occurs between class-1  which has a low accuracy of 1%  and thc category of all the unclassified cases is omitted. 
1 	case-based reasoning 


class-1 which has an accuracy of 1% and both classes have about 1 training cases on average. a close look at the classes reveals the reason for this behaviour. class1 consists of five relatively different animals: pitviper  seasnake  slowworrn  tortoise  and tuatara  while class-1 consists of fairly similar animals: frog  poisonous frog  newt  and toad. since  class-1 is very polymorphic and only a few cases have been observed  the exemplars representing that category are weak and hence the accuracy of class-1 is low. however  although there are only a few cases in class-1  they are similar and the exemplars are therefore more representative of the category. hence  the accuracy for class-1 is significantly better. 
　figure 1 present  the accuracy obtained  and the number of training cases per category for the audiology dataset. the accuracies obtained are good for some of the categories and poor for some categories where there are few training cases. this behaviour is to be expected since the model is not expected to learn exemplars from a few cases. the motivation for applying pebm to the audiology dataset was to enable some comparison with a closely related system  protos  bareiss  1  that utilised that data. unfortunately  due partly to the nature of protos  and partly because of the lack of availability of the information utilised in the protos experiments  it is not possible to repeat the experiments reported in  bareiss  1 . although it would be incorrect to draw comparative conclusions from the results of the single trial presented in  bareiss  1   it is encouraging that the results obtained in terms of accuracy and compression ratio  are similar to those obtained when protos was trained with the aid of an audiology expert  see  rodriguez  1  for details . 
1 	related work and conclusion 
the model presented in this paper is related to work on 
cbr  bayesian networks  and inductive learning. there are numerous systems in these categories and a comparison with these systems is too lengthy for the space avail-


categories 
figure 1: accuracy for the audiology dataset. 
able in this paper. it is  however  important to merit ion the main differences with two of the most related models: protos and tirri et al.'s  model. the representation used by protos is similar to the one used by pebm in that exemplars are used to define categories. the notion of exemplar is  however  very different in that cases denote exemplars  whereas in pebm  exemplars are represented by bayesian networks. the classification process used by protos is dependent on the use of indices called remindings  censors  and difference links. in contrast  classification in pebm is achieved by probabilistic propagation. the learning mechanisms are also very different since protos relies heavily on heuristics that learn from user provided explanations  while pebm learns from data. the most significant difference  however  is that pebm has foundations in probabilistic reasoning  whereas protos appears to be based primarily on heuristics. 
　the bayesian network representation used in tirri et al.'s  work is very similar to the one adopted for pebm but with the exception that their upper level nodes are random variables that represent cases and not prototypes. given the potentially large number of cases  standard propagation methods would not be practical. hence  they assume that the cases are mutually exclusive in order to simplify the network to a tree. the extent to which this assumption holds or the effects of violating the assumption are unclear since a new case can be expected to be similar to a number of previous cases. in contrast  pebm does not make this assumption and uses exemplars which aim to represent regions of similar cases. this difference is also reflected in the requirements for learning  since their model only estimates the probabilities from all the cases  while pebm identifies prototypes incrementally. 
　to conclude  this paper has presented an exemplar based model with foundations in bayesian networks. the model learns exemplars by using a measure of prototypicality and utilises probabilistic propagation to determine whether a new case is similar to an exemplar. the model has been evaluated on 1 datasets and shows promising results in terms of accuracy and in terms of the number of exemplars retained. 
acknowledgements 
the authors are grateful to enrique sucar for his useful comments and discussions on the model presented in this paper. 
