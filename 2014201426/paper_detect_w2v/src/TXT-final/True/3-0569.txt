
hybrid approximate linear programming  halp  has recently emerged as a promising framework for solving large factored markov decision processes  mdps  with discrete and continuous state and action variables. our work addresses its major computational bottleneck - constraint satisfaction in large structured domains of discrete and continuous variables. we analyze this problem and propose a novel markov chain monte carlo  mcmc  method for finding the most violated constraint of a relaxed halp. this method does not require the discretization of continuous variables  searches the space of constraints intelligently based on the structure of factored mdps  and its space complexity is linear in the number of variables. we test the method on a
set of large control problems and demonstrate improvements over alternative approaches.
1 introduction
markov decision processes  mdps   bellman  1; puterman  1  offer an elegant mathematical framework for solving sequential decision problems in the presence of uncertainty. however  traditional techniques for solving mdps are computationally infeasible in real-world domains  which are factored and contain both discrete and continuous state and action variables. recently  approximate linear programming  alp   schweitzer and seidmann  1  has emerged as a promising approach to solving large factored mdps  de farias and roy  1; guestrin et al.  1 . this work focuses on hybrid alp  halp   guestrin et al.  1  and addresses its major computational bottleneck - constraint satisfaction in the domains of discrete and continuous variables.
¡¡if the state and action variables are discrete  halp involves an exponential number of constraints  and if any of the variables are continuous  the number of constraints is infinite. to approximate the constraint space in halp  two techniques have been proposed: ¦Å-halp  guestrin et al.  1  and monte carlo constraint sampling  de farias and roy  1; hauskrecht and kveton  1 . the ¦Å-halp formulation relaxes the continuous portion of the constraint space to an ¦Ågrid  which can be compactly satisfied by the methods for discrete-state alp  guestrin et al.  1; schuurmans and patrascu  1 . however  these methods are exponential in the treewidth of the discretized constraint space  which limits their application to real-world problems. in addition  the ¦Ågrid discretization is done blindly and impacts the quality of the approximation. monte carlo methods  de farias and roy  1; hauskrecht and kveton  1  offer an alternative to the ¦Å-grid discretization and approximate the constraint space in halp by its finite sample. unfortunately  the efficiency of monte carlo methods is heavily dependent on an appropriate choice of sampling distributions. the ones that yield good approximations and polynomial sample size bounds are closely related to the optimal solutions and rarely known a priori  de farias and roy  1 .
¡¡to overcome the limitations of the discussed constraint satisfaction techniques  we propose a novel markov chain monte carlo  mcmc  method for finding the most violated constraint of a relaxed halp. the method directly operates in the domains of continuous variables  takes into account the structure of factored mdps  and its space complexity is proportional to the number of variables. such a separation oracle can be easily embedded into the ellipsoid or cutting plane method for solving linear programs  and therefore constitutes a key step towards solving halp efficiently.
¡¡the paper is structured as follows. first  we introduce hybrid mdps and halp  guestrin et al.  1   which are our frameworks for modeling and solving large-scale stochastic decision problems. second  we review existing approaches to solving halp and discuss their limitations. third  we compactly represent the constraint space in halp and formulate an optimization problem for finding the most violated constraint of a relaxed halp. fourth  we design a markov chain to solve this optimization problem and embed it into the cutting plane method. finally  we test our halp solver on a set of large control problems and compare its performance to alternative approaches.
1 hybrid factored mdps
factored mdps  boutilier et al.  1  allow a compact representation of large stochastic planning problems by exploiting their structure. in this section  we review hybrid factored mdps  guestrin et al.  1   which extend this formalism to the domains of discrete and continuous variables.
¡¡a hybrid factored mdp with distributed actions  hmdp   guestrin et al.  1  is a 1-tuple m =  x a p r   where x = {x1 ... xn} is a state space represented by a set of state variables  a = {a1 ... am} is an action space represented by a set of action variables  p x¡ä | x a  is a stochastic transition model of state dynamics conditioned on the preceding state and action choice  and r is a reward model assigning immediate payoffs to state-action configurations1. state variables: state variables are either discrete or continuous. every discrete variable xi takes on values from a finite domain dom xi . following hauskrecht and kveton 1  we assume that every continuous variable is bounded to the  1  subspace. the state is represented by a vector of value assignments x =  xd xc  which partitions along its discrete and continuous components xd and xc.
action variables: the action space is distributed and represented by action variables a. the composite action is defined by a vector of individual action choices a =  ad ac  which partitions along its discrete and continuous components ad and ac.
transition model: the transition model is given by the conditional probability distribution p x¡ä | x a   where x and x¡ä denote the state variables at two successive time steps. we assume that the model factors along x¡ä as p x¡ä | x a  =  and can be compactly represented by a dynamic bayesian network  dbn   dean and kanazawa  1 . usually  the parent set par xi¡ä    x ¡È a is a small subset of state and action variables which allows for a local parameterization of the model.
parameterization of transition model: one-step dynamics of every state variable is described by its conditional probability distribution p xi¡ä | par xi¡ä  . if xi¡ä is a continuous variable  its transition function is represented by a mixture of beta distributions  hauskrecht and kveton  1 :

where ¦Ðij is the weight assigned to the j-th component of the mixture  and ¦Áj = ¦Õ¦Áij par xi¡ä   and ¦Âj = ¦Õ¦Âij par xi¡ä   are arbitrary positive functions of the parent set. the mixture of beta distributions provides a very general class of transition functions and yet allows closed-form solutions to the integrals in halp. if xi¡ä is a discrete variable  its transition model is parameterized by |dom xi¡ä | nonnegative discriminant functions ¦Èj = ¦Õ¦Èij par xi¡ä    guestrin et al.  1 : ¦È
	.	 1 
j=1
reward model: the reward function is an additive function on the subsets of state and action variablesp	xj and aj. r x a  = j rj xj aj  of local reward functions defined
optimal value function and policy: the quality of a policy is measured by the infinite horizon discounted reward
e   where ¦Ã ¡Ê  1  is a discount factor and rt is the reward obtained at the time step t. this optimality criterion guarantees that there always exists an optimal policy ¦Ð  which is stationary and deterministic  puterman  1 . the policy is greedy with respect to the optimal value function v    which is a fixed point of the bellman equation  bellman  1; bertsekas and tsitsiklis  1 :
x a v   x¡ä dx¡äc .  1 
	¡äd	c	 
1 hybrid alp
value iteration  policy iteration  and linear programming are the most fundamental dynamic programming  dp  methods for solving mdps  puterman  1; bertsekas and tsitsiklis  1 . however  their computational complexity grows exponentially in the number of used variables  which makes them unsuitable for factored mdps. moreover  they are built on the assumption of finite support for the optimal value function and policy  which may not exist if continuous variables are present. recently  feng et al. 1 showed how to solve general state-space mdps by performing dp backups of piecewise constant and piecewise linear value functions. this approximate method has a lower scale-up potential than halp  but does not require the design of basis functions.
linear value function: value function approximation is a standard approach to solving large factored mdps. due to its favorable computational properties  linear value function approximation  bellman et al.  1; roy  1 :
v w x  = xwifi x 
i
has become extremely popular in recent research  guestrin et al.  1; schuurmans and patrascu  1; de farias and roy  1; hauskrecht and kveton  1; guestrin et al.  1 . this approximation restricts the form of the value function v w to the linear combination of |w| basis functions fi x   where w is a vector of tunable weights. every basis function can be defined over the complete state space x  but often is restricted to a subset of state variables xi.
1 halp formulation
various methods for fitting of the linear value function approximation have been proposed and analyzed  bertsekas and tsitsiklis  1 . we adopt a variation on approximate linear programming  alp   schweitzer and seidmann  1   hybrid alp  halp   guestrin et al.  1   which extends this framework to the domains of discrete and continuous variables. the halp formulation is given by:
minimizew xwi¦Ái
i
	subject to: xwifi x a    r x a  ¡Ý 1	  x a;
                     i where ¦Ái denotes basis function relevance weight:
	¦Ái = x	 	 1 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡x ¦× x  is a state relevance density function weighting the quality of the approximation  and fi x a  = fi x    ¦Ãgi x a  is the difference between the basis function fi x  and its discounted backprojection:
	.	 1 
x¡äx¡äc d
we say that the halp is relaxed if only a subset of the constraints is satisfied.
¡¡the halp formulation reduces to the discrete-state alp  schweitzer and seidmann  1; schuurmans and patrascu  1; de farias and roy  1; guestrin et al.  1  if the state and action variables are discrete  and to the continuousstate alp  hauskrecht and kveton  1  if the state variables are continuous. the quality of this approximation was studied by guestrin et al. 1 and bounded with respect to minw kv     v wk¡Þ 1/l  where k¡¤k¡Þ 1/l is a maxnorm weighted by the reciprocal of the lyapunov function l x  = i wilfi x . the integrals in the objective function  equation 1  and constraints  equation 1  have closed-formp solutions if the basis functions and state relevance densities are chosen appropriately  hauskrecht and kveton  1 . for example  the mixture of beta transition model  equation 1  yields a closed-form solution to equation 1 if the basis function fi x¡ä  is a polynomial. finally  we need an efficient constraint satisfaction procedure to solve halp.
1 constraint satisfaction in halp
if the state and action variables are discrete  halp involves an exponential number of constraints  and if any of the variables are continuous  the number of constraints is infinite. to approximate such a constraint space  two techniques have been proposed recently: ¦Å-halp  guestrin et al.  1  and monte carlo constraint sampling  de farias and roy  1; hauskrecht and kveton  1 . ¦Å-halp: the ¦Å-halp formulation relaxes the continuous portion of the constraint space to an ¦Å-grid by the discretization of continuous variables xc and ac. the new constraint space spans discrete variables only and can be compactly satisfied by the methods for discrete-state alp  guestrin et al.  1; schuurmans and patrascu  1 . for example  schuurmans and patrascu 1 search for the most violated constraint with respect to the solution w t  of a relaxed alp:
argmin 1 
x a
and add it to the linear program. if no violated constraint is found  w t  is an optimal solution to the alp.
¡¡the space complexity of both constraint satisfaction methods  guestrin et al.  1; schuurmans and patrascu  1  is exponential in the treewidth of the constraint space. this is a serious limitation because the cardinality of discretized variables grows with the resolution of the ¦Å-grid. roughly  if the discretized variables are replaced by binary  the treewidth increases by a multiplicative factor of log1/¦Å + 1   where  1/¦Å + 1  is the number of discretization points in a single dimension. therefore  even problems with a relatively small treewidth are intractable for small values of ¦Å. in addition  the ¦Å-grid discretization is done blindly and impacts the quality of the approximation.
monte carlo constraint sampling: monte carlo methods approximate the constraint space in halp by its finite sample. de farias and van roy 1 analyzed constraint sampling for discrete-state alp and bounded the sample size for achieving good approximations by a polynomial in the number of basis functions and state variables. hauskrecht and kveton 1 applied random constraint sampling to solve continuous-state factored mdps and later refined their sampler by heuristics  kveton and hauskrecht  1 .
¡¡monte carlo constraint sampling is easily applied in continuous domains and can be implemented in a space proportional to the number of variables. however  proposing an efficient sampling procedure that guarantees a polynomial bound on the sample size is as hard as knowing the optimal policy itself  de farias and roy  1 . to lower a high number of sampled constraints in a relaxed halp  monte carlo samplers can be embedded into the cutting plane method. a technique similar to kveton and hauskrecht 1 yields significant speedup with no drop in the quality of the approximation.
1 mcmc constraint sampling
to address the deficiencies of the discussed constraint satisfaction techniques  we propose a novel markov chain monte carlo  mcmc  method for finding the most violated constraint of a relaxed halp. before we proceed  we compactly represent the constraint space in halp and formulate an optimization problem for finding the most violated constraint in this representation.
1 compact representation of constraints
guestrin et al. 1 and schuurmans and patrascu 1 showed that the compact representation of constraints is essential in solving alp efficiently. following their ideas  we define violation magnitude ¦Ów x a :
¦Ów x a  =  xwi fi x    ¦Ãgi x a   + r x a 	 1 
i
to be the amount by which the solution w violates the constraints of a relaxed halp. we represent ¦Ów x a  compactly by an influence diagram  id   where x and a are decision nodes and x¡ä are random variables. the id representation is built on the transition model p x¡ä | x a   which is already factored and contains dependencies among the variables x  x¡ä  and a. we extend the diagram by three types of reward nodes  one for each term in equation 1: rj = rj xj aj  for every local reward function  hi =  wifi x  for every basis function  and gi = ¦Ãwifi x¡ä  for every backprojection. the construction is completed by adding arcs that represent the dependencies of the reward nodes on the variables. finally  we verify that ¦Ów x a  = ep x¡ä|x a   i hi+gi + j rj .
therefore  the decision that maximizes the expected utility inp p the id corresponds to the most violated constraint.
¡¡we conclude that any algorithm for solving ids can be used to find the most violated constraint. moreover  special properties of the id representation allow its further simplification. if the basis functions are chosen conjugate to the transition model  section 1   we obtain a closed-form solution to ep x¡ä|x a  gi   equation 1   and thus the random variables x¡ä can be marginalized out of the diagram. this new representation contains no random variables and is know as a cost network  guestrin et al.  1 .
1 separation oracle
to find the most violated constraint in the cost network  we use the metropolis-hastings  mh  algorithm  metropolis et al.  1; hastings  1  and construct a markov chain whose invariant distribution converges to the vicinity of argmaxz ¦Ów z   where z =  x a  and z = x ¡È a is a joint set of state and action variables. the metropolishastings algorithm accepts the transition from a state z to a proposed state z  with the acceptance probability a z z   =
  where q z  | z  is a proposal distribu-
tion and p z  is a target density. under mild restrictions on p z  and q z  | z   the chain always converges to the target density p z   andrieu et al.  1 . in the rest of this section  we discuss the choice of p z  and q z  | z  to solve our optimization problem.
target density: the violation magnitude ¦Ów z  is turned into a density by the transformation p z  = exp ¦Ów z  . due to its monotonic character  p z  retains the same set of global maxima as ¦Ów z   and thus the search for argmaxz ¦Ów z  can be performed on p z . to prove that p z  is a density  we
where zd and zc are the discrete and continuous componentspzd rzc show that it has a normalizing constant p z dzc  of the vector z =  zd zc . as the integrand zc is restricted to the finite space  1 |zc|  the integral is proper as long as p z  is bounded  and therefore it is riemann integrable and finite. to prove that p z  is bounded  we bound ¦Ów z . let rmax denote the maximum one-step reward in the hmdp. if the basis functions are of unit magnitude  wi can be typically bounded by |wi| ¡Ü ¦Ã 1 ¦Ã  1rmax  and consequently |¦Ów z | ¡Ü  |w|¦Ã 1   ¦Ã  1 + 1 rmax. therefore  p z  is bounded and can be treated as a density function.
¡¡to find the mode of p z   we adopt the simulating annealing approach  kirkpatrick et al.  1  and simulate a non-homogeneous markov chain whose invariant distribution equals to p1/tt z   where tt is a decreasing cooling schedule with limt¡ú¡Þ tt = 1. under weak regularity assumptions on p z   p¡Þ z  is a probability density that concentrates on the set of global maxima of p z   andrieu et al.  1 . if the cooling schedule decreases such that tt ¡Ý c/log1 t + 1   where c is a problem-specific constant independent of t  the chain converges to the vicinity of argmaxz ¦Ów z  with the probability converging to 1  geman and geman  1 . however  this schedule can be too slow in practice  especially for a high initial temperature c. following the suggestion of geman and geman 1  we overcome this limitation by selecting a smaller value of c than is required by the convergence criterion. as a result  convergence to the global optimum argmaxz ¦Ów z  is no longer guaranteed. proposal distribution: we take advantage of the factored character of z and adopt the following proposal distribution  geman and geman  1 :
	q z  | z  = 1 p zi  | z i 	if z  
	1	otherwise
where z i and z are the assignments to all variables but zi in the original and proposed states. if zi is a discrete variable 
                                     its conditional can be derived in a closed form. if zi is a continuous variable  a closed form of its cumulative density function is not likely to exist. to allow sampling from its conditional  we embed another mh step within the original chain. in the experimental section  we use the metropolis algorithm with the acceptance probability  where zi and zi  correspond to the original and proposed values of zi. note that sampling from both conditionals can be performed in the space of ¦Ów z  and locally.
¡¡finally  we get a non-homogenous markov chain with the acceptance probability that converges to the vicinity of the most violated constraint. a similar chain was derived by yuan et al. 1 and applied to find the maximum a posteriori  map  configuration of random variables in bayesian networks.
1 constraint satisfaction
if the mcmc oracle converges to a violated constraint  not necessarily the most violated  in a polynomial time  it guarantees that the ellipsoid method can solve halp in a polynomial time  bertsimas and tsitsiklis  1 . however  convergence of our chain within an arbitrary precision requires an exponential number of steps  geman and geman  1 . even if this bound is too weak to be of practical interest  it suggests that the time complexity of finding a violated constraint dominates the time complexity of solving halp. therefore  the search for violated constraints should be performed efficiently. convergence speedups that directly apply to our work include hybrid monte carlo  hmc   duane et al.  1   slice sampling  higdon  1   and raoblackwellization  casella and robert  1 .
¡¡to evaluate the mcmc oracle  we embed it into the cutting plane method for solving linear programs  which results in a novel approach to solving halp. as the oracle is not guaranteed to converge to the most violated constraint  we run the cutting plane method for a fixed number of iterations rather than having the same stopping criterion as schuurmans and patrascu 1  section 1 . our mcmc solver differs from the monte carlo solver in that it samples constraints based on their potential to improve the existing solution  which substitutes for an unknown problem-specific sampling distribution. comparing to the ¦Å-halp method  the mcmc oracle directly operates in the domains of continuous variables and its space complexity is linear in the number of variables.
1 experiments
n =1n =1n =1ovrewardtimeovrewardtimeovrewardtime111¡À11.1.1¡À11.1.1¡À11¦Å-halp¦Å = 111¡À11.1.1¡À11.1.1¡À11111¡À11.1.1¡À11.1.1¡À111.1.1¡À11.1.1¡À11.1.1¡À11mcmcn = 1.1.1¡À11.1.1¡À11.1.1¡À111.1.1¡À11.1.1¡À11.1.1¡À11mc11¡À11.1.1¡À11.1.1¡À11ring topology
n =1n =1n =1ovrewardtimeovrewardtimeovrewardtime111¡À11.1.1¡À11.1.1¡À11¦Å-halp¦Å = 111¡À11.1.1¡À11.1.1¡À11111¡À11.1.1¡À11.1.1¡À111.1.1¡À11.1.1¡À11.1.1¡À11mcmcn = 1.1.1¡À11.1.1¡À11.1.1¡À111.1.1¡À11.1.1¡À11.1.1¡À11mc11¡À11.1.1¡À11.1.1¡À11ring-of-rings topology
figure 1: comparison of halp solvers on two irrigation-network topologies of varying sizes  n . the solvers are compared by the objective value of a relaxed halp  ov   the expected discounted reward of a corresponding policy  and computation time  in seconds . the expected discounted reward is estimated by the monte carlo simulation of 1 trajectories. the ¦Å-halp and mcmc solvers are parameterized by the resolution of ¦Å-grid  ¦Å  and the number of iterations  n .the performance of the mcmc solver is evaluated against two alternative constraint satisfaction techniques: ¦Å-halp and uniform monte carlo sampling. due to space limitations  our comparison focuses on two irrigation-network problems  guestrin et al.  1   but our conclusions are likely to generalize across a variety of control optimization tasks. the irrigation-network problems are challenging for state-of-theart mdp solvers due to the factored state and action spaces  figure 1 . the goal of an irrigation network operator is to select discrete water-routing actions ad to optimize continuous water levels xc in multiple interconnected irrigation channels. the transition model is parameterized by beta distributions and represents water flows conditioned on the operation modes of regulation devices. the reward function is additive and given by a mixture of two normal distributions for each channel. the optimal value function is approximated by a linear combination of four univariate piecewise linear basis functions for each channel  guestrin et al.  1 . the state relevance density function ¦× x  is uniform. a comprehensive description of the irrigation-network problems can be found in guestrin et al. 1.
¡¡the ¦Å-halp solver is implemented with the method of schuurmans and patrascu 1 as described in section 1. the monte carlo solver uniformly generates one million constraints and establishes a baseline for the comparison to an uninformatively behaving sampling method. the chain of the mcmc oracle is simulated for 1 steps from the initial temperature c = 1  which yields a decreasing cooling schedule from t1 = 1 to t1 ¡Ö 1. these parameters were chosen empirically to demonstrate the characteristics of our approach rather than to maximize the performance of the mcmc oracle. all experiments were performed on a dell
precision 1 workstation with 1ghz pentium 1 cpu and 1gb ram. all linear programs were solved by the simplex method in the lpsolve package. the results of the experiments are reported in figure 1.
¡¡based on our results  we draw the following conclusions. first  the mcmc solver  n = 1  achieves the highest objective values on all problems. higher objective values can be interpreted as closer approximations to the constraint space in halp since the solvers operate on relaxed versions of halp. second  the quality of the mcmc policies  n = 1  surpasses the monte carlo ones while both solvers consume approximately the same computation time. this result is due to the informative search for violated constraints in the mcmc solver. third  the quality of the mcmc policies  n = 1  is close to the ¦Å-halp ones  ¦Å = 1   but does not surpass them significantly. in the irrigation-network problems  the ¦Åhalp policies  ¦Å = 1  are already close to optimal  and therefore hard to improve. even if the mcmc solver reaches higher objective values than the ¦Å-halp solver  the policies may not improve due to the suggestive relationship between our true objective minw kv     v wk¡Þ 1/l and the objective of halp minw kv     v wk1 ¦×  guestrin et al.  1 .
¡¡finally  the computation time of the ¦Å-halp solver is seriously affected by the topologies of tested networks  which can be explained as follows. for a small ¦Å and large n  the time complexity of formulating cost networks grows approximately by the rates of  1/¦Å + 1 and  1/¦Å + 1 for the ring and ring-of-rings topologies  respectively. the ¦Å-halp solver spends a significant amount of time by formulating cost networks  which makes its decent time complexity on the ring topology  quadratic in 1/¦Å + 1  deteriorate on the ring-of-rings topology  cubic in 1/¦Å + 1 . a similar crosstopology comparison of the mcmc solver shows that its computation times differ only by a multiplicative factor of 1. this difference is due to the increased complexity of sampling p zi  | z i   which is caused by more complex local dependencies  and not the treewidth of the ring-of-rings topology.
1 conclusions
development of scalable algorithms for solving large factored mdps is a challenging task. the mcmc approach presented in this paper is a small but important step in this direction. in particular  our method overcomes the limitations of existing approaches to solving halp and works directly with continuous variables  generates constraints based on their potential to improve the existing solution  and its space complexity is linear in the number of variables. moreover  the mcmc solver seems to be less affected by larger treewidth than the ¦Å-halp method while delivering substantially better results than uniform monte carlo sampling. empirical results on two large control problems confirm the expected benefits of the approach and its potential to tackle complex real-world optimization problems. the objective of our future research is to eliminate the assumptions placed on the transition model and basis functions  which would make the framework applicable to a broader class of problems.
acknowledgment
this work was supported in part by national science foundation grants cms-1 and ani-1. the first author was supported by an andrew mellon predoctoral fellowship for the academic year 1. we thank anonymous reviewers for providing insightful comments that led to the improvement of the paper.
