 
we present an approximate policy iteration algorithm that uses rollouts to estimate the value of each action under a given policy in a subset of states and a classifier to generalize and learn the improved policy over the entire state space. using a multiclass support vector machine as the classifier  we obtained successful results on the inverted pendulum and the bicycle balancing and riding domains. 
1 introduction 
reinforcement learning provides an intuitively appealing framework for addressing a wide variety of planning and control problems. there has been significant success in tackling large-scale problems through the use of value function and/or policy approximation. the success of these methods  however  is contingent upon extensive and careful feature engineering  a common problem in most machine learning methods. 
　modern classification methods have mitigated the feature engineering problems through the use of kernel methods  but very little has been done to exploit these recent advances for the purposes of reinforcement learning. of course  we are not the first to note the potential benefits of modern classification methods to reinforcement learning. for example  yoon et al.  use inductive learning techniques  including boosting  to generalize across similar problems. dietterich and wang  also use a kernel-based approximation method to generalize across similar problems. the novelty in our approach is the application of modern learning methods within a single  noisy problem at the inner loop of a policy iteration algorithm.1 by using rollouts  we avoid the sometimes problematic step of value function approximation. thus  our technique aims to address the critiques of value function methods raised by the proponents of direct policy search  while avoiding the confines of a parameterized policy space. 
1 definitions and assumptions 
a markov decision process  mdp  is defined as a 1-tuple 
 1  a  p  r  r  d  where: s is the state space of the process; 
!
fcrn  yoon  and givan are also pursuing a similar approach. 
a is a finite set of actions; p is a markovian transition model  where  is the probability of making a transition to state sf when taking action a in state s; r is a reward  or cost  function  such that r s  a  is the expected reward for taking action a in state s:  1  is the discount factor for future rewards; and  d is the initial state distribution from which states are drawn when the process is initialized. 
　in reinforcement learning  it is assumed that the learner can observe the state of the process and the immediate reward at every step  however p and r are completely unknown. in this paper  we also make the assumption that our learning algorithm has access to a generative model of the process which is a black box that takes a state s and an action a as inputs and outputs a next state s' drawn from p and a reward r. note that this is not the same as having the model  p and r  itself. 
　a policy for an mdp is a mapping : s a from states to actions  where is the action the agent takes at state s. the value of a state s under a policy n is the expected  total  discounted reward when the process begins in state s and all decisions at all steps are made according to policy 1r: 

the goal of the decision maker is to compute or learn an optimal policy  that maximizes the expected total discounted reward from the initial state distribution: 

it is known that for every mdp  there exists at least one optimal deterministic policy. 
1  approximate  policy iteration 
policy iteration  pi  is a method of discovering such a policy by iterating through a sequence of monotonically improving policies. improvement at each iteration i is typically achieved by computing or learning the state-action value function of the current policy 1rt  defined as 

and then the improved policy as 


1 	poster papers 

in practice  policy iteration terminates in a surprisingly small number of steps  however  it relies on exact representation of value functions and policies. 
　approximate methods are frequently used when the state space of the underlying mdp is extremely large and exact  solution or learning  methods fail. a general framework of using approximation within policy iteration is known as ap-
proximate policy iteration  api . in its most general form  api assumes approximate representations of value functions  q  and policies  as a result  api cannot guarantee monotonic improvement and convergence to the optimal policy. a performance bound on the outcome of api can be constructed in terms of  bounds on the approximation errors  bertsekas and tsitsiklis  1 . in practice  api often finds very good policies in a few iterations  since it normally makes big steps in the space of possible policies. this is in contrast to policy gradient methods which  despite acceleration methods  are often forced to take very small steps. 
1 practical api without value functions 
the dependence of typical api algorithms on approximate value functions places continuous function approximation methods at their inner loop. these methods typically minimize l1 error  which is a poor match with the  bounds for api. this problem is not just theoretical. efforts to improve performance  such as adding new features  can sometimes lead to surprisingly worse performance  making feature engineering a somewhat tedious and counterintuitive task. 
　an important observation  also noted by fern  yoon and givan 1  is that a monte-carlo technique  called rollouts  can be used within api to avoid the problematic value function approximation step entirely. rollouts estimate qnt  s a  from a generative model by executing action a in state s and following policy 1r  thereafter  while recording the total discounted reward obtained during the entire trajectory. this simulation is repeated several times and the results are averaged over a large number of trajectories to obtain an accurate estimate  rollouts were first used by tesauro and galperin  for online improvement of a backgammon player. for our purposes  we can choose a representative set of states sp  with distribution  over the state space  and perform enough rollouts to determine which action maximizes  for the current policy. rather than fitting a function approximator to the values obtained by the rollouts  we instead train a classifier on spy where each state is labelled with the maximizing action for the state. the algorithm is summarized in figure 1. 
　learn is a supervised learning algorithm that trains a classifier given a set of labeled training data. the termination condition is left somewhat open ended. it can be when the performance of the current policy does not exceed that of the previous one  when two subsequent policies are similar  the notion of similarity will depend upon the learner used   or when a cycle of policies is detected  also learner dependent . if we assume a fortuitous choice of sp and a sufficiently powerful learner that can correctly generalize from sp to the entire state space  the algorithm at each iteration learns the improved policy over the previous one  effectively implement-
poster papers 

figure 1: api with rollouts and classification 
ing a full policy iteration algorithm  and terminating with the optimal policy. however  for large-scale problems  choosing sf  and dealing with imperfect classifiers poses challenges. 
　we consider a number of alternative choices for the distribution p. a uniform distribution over the state space is a good choice for a low-dimensional state space  but it will result in poor coverage in high-dimensional spaces. a better choice would be a distribution that favors certain important parts of the state space over others. in classification  it is widely assumed that the classifier is trained on examples that are drawn from the same distribution it will be tested on. therefore  representative states for learning policy in iteration i should be drawn from the future state distribution  jaakkola et al  1 of the yet unknown policy 1ti+1. even though this policy is yet unknown  it is possible to closely approximate this distribution1. starting in some state .s1 drawn from z   a trajectory under  can be simulated by using rollouts repeatedly in each visited state to determine the actions of if the trajectory terminates with probability then states visited along such trajectories can be viewed as samples from the desired distribution. 
   the main contribution of this paper is a particular embodiment of the algorithm described in the previous section. for rollouts  we used basic statistical hypothesis testing  twosample f.-test  to determine statistically significant differences between the rollout estimates of  for different actions. if action a is determined to be a clear winner for state s  then we treat  s  a  as a positive training example for state s  and all  s  a'    as negative examples for s. even if there is no clear winner action in some state s  negative examples can be still extracted for the clearly bad actions in s  allowing the classifier to choose freely any of the remaining  good  actions as the winner. the only case where no examples are generated is when all actions seem to be equally good. 
   the most significant contribution of effort is the use of support vector machines  svms  for learn. in our implementation we used the svmtorch package  collobert and bengio  1  as the multi-class classifier. with the kernel trick  svms are able to implicitly and automatically consider classifiers with very complex feature spaces. 
special thanks to alan fern for sharing this observation. 
1 


figure 1: positive +   negativc x  examples; support vcctors o . 
1 experimental results 
in the inverted pendulum problem the goal is to balance a pendulum by applying forces to the left  lf   or to the right  rf   or no force  nf  at all. all actions are noisy. the state space is continuous and consists of the vertical angle and the angular velocity of the pendulum. transitions are governed by the nonlinear dynamics of the system and the time step is 1 seconds. the reward is -1 when the angle exceeds  in absolute value  end of the episode  and 1 otherwise. the discount factor is 1. using about 1 states from p to perform rollouts  the algorithm consistently learns balancing policies in one or two iterations  starting from the random policy. the choice of p or the kernel  polynomial/gaussian  did not affect the results significantly. figure 1 shows the training data for the lf action with p being the uniform distribution. the number of support vectors was normally smaller than the number of rollout states. the constant c  the trade-off between training error and margin  was set to 1. 
   in the bicycle balancing and riding problem  randlov and alstrom  1  the goal is to learn to balance and ride a bicycle to a target position located 1 km away from the starting location. the state description is a six-dimensional vector where is the angle of the handlebar   is the vertical angle of the bicycle  and  is the angle of the bicycle to the goal. the actions are the torque r applied to the handlebar { - 1   1   +1} and the displacement of the rider v {-1 1-1}. actions are restricted so that either r = 1 or v = 1 giving a total of 1 actions. the noise is a uniformly distributed term in  -1  + 1  added to the displacement. the dynamics of the bicycle are based on the model of randlov and alstrom  and the time step is set to 1 seconds. as is typical with this problem  we used a shaping reward  ng et al.  1 . the reward rt given at each time step was rt -  where dt is the distance of the back wheel of the bicycle to the goal position at time t and 1 is the discount factor which is set to 1. 
   using a polynomial kernel of degree 1  we were able to solve the problem with uniform sampling of about 1 rollout states. sampling from the distribution of the target policy  the problem is solved with as few as 1 rollout states  as shown in figure 1 which shows ten sample trajectories of the bicycle in the two-dimensional plane from the initial position  1   left side  to the goal position 

figure 1: successful trajectories of the bicycle. 
 1   right side . the input to the svm was simply the six-dimensional state description and the value of c was 1. performance was sensitive to the svm parameters. we are currently doing further experimentation with larger numbers of rollout states and different kernel parameters. 
this research was supported in part by nsf grant 1. 
