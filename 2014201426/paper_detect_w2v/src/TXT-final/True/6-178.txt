 
we investigate the calculation of a* bounds for sequence and tree models which are the explicit intersection of a set of simpler models or can be bounded by such an intersection. we provide a natural viewpoint which unifies various instances of factored a* models for trees and sequences  some previously known and others novel  including multiple sequence alignment  weighted finitestate transducer composition  and lexicalized statistical parsing. the specific case of parsing with a product of syntactic  pcfg  and semantic  lexical dependency  components is then considered in detail. we show that this factorization gives a modular lexicalized parser which is simpler than comparably accurate non-factored models  and which allows efficient exact inference with large treebank grammars. 
1 introduction 
the primary challenge when using a* search is to find heuristic functions that simultaneously are admissible  close to actual completion costs  and efficient to calculate. in this paper  we describe a family of tree and sequence models in which path costs are either defined as or bounded by a combination of simpler component models  each of which scores some projection of the full structure. in such models  we can exploit the decoupled behavior over each projection to give sharp heuristics for the combined space. while we focus on models of trees and sequences within nlp applications  the approach can be applied more generally  and already has been  in the case of biological sequence models . all the concrete cases we consider here involve search over spaces which are equivalent to dynamic programming lattices  though this detail  too  is somewhat peripheral to the basic ideas. 
1 projection models for graphs 
the core idea of factored a* search can apply to any graph search. assume that g =  n  a  is a very large graph  with a single source node s and a single goal node g  and that we wish to use a* search to efficiently find a best path from s to g. for concreteness  assume also that the score of a path is the 
1 
christopher d. manning department of computer science stanford university stanford  ca 1 manning cs.stanford.edu 
sum of the scores of the arcs along the path  and that lower scores are better.1 the particular assumption in this paper is that the arc scoring function has a special factored form. specifically  there exists a set of projections  tt   .. .1ik  of nodes  and therefore also arcs and graphs  such that for any arc  jc  y   its score a is given by: 

whenever the scoring function factors in this way  we have an immediate recipe for a factored a * bound  which we denote by h. specifically  we can bound the shortest path in g from a node n to the goal g by the sum of the shortest paths inside each projection  g . formally  if ag n  g  is the length of a shortest path from n to g in a graph g  then: 

this follows immediately from the optimality of the projected paths and the structure of the scoring function. these pro-
jections need not be mutually compatible  and therefore the bound may not be tight. broadly speaking  the greater the degree to which each projection prefers similar paths  the better the bound will be  and the more efficient our search will be. 
1 projection models for sequences 
for intuition  we first consider applications to sequence models before extending to the more complex case of tree models. 
1 	example: multiple sequence alignment 
a situation which fits this framework perfectly is the alignment of multiple genome sequences in bioinformatics  where such multiple sequence alignments  msas  are standardly evaluated by sum-of-pairs scoring  durbin et al.  1 . msa is a generalization of the longest-common-subsequence problem  in which one is given sequences like those in figure la  and asked to produce pointwise alignments. alignments of d sequences  consist of t vertical timeslices which specify  for each sequence  either a successive element of the sequence or a gap  -   and are such that if the gaps are removed  the rows contain the original sequences. the score of a timeslice is the sum of the scores of each of the pairs 
　　1  we will talk about minimum sums  but other semirings work as well  specifically maximum products. 
search 


figure 1: an example of a multiple sequence alignment   a  the original sequences   b  a multiple alignment with one timeslice distinguished   c  the sum-of-pairs scoring function for that timeslice. 
in that slice  where each pair of symbols is assigned some experimental goodness value . 
　the well-known dynamic program for calculating optimal multiple alignments involves a lattice of position nodes  which specify an index along each sequence  durbin et ai  1 . when each node is visited  each of its  successors   where each position is either incremented or not  are relaxed with the best score through p combined with the score of the timeslice that the change from p to p' represents. these nodes form a lattice of size where n is the maximum length of the sequences. this becomes extremely inefficient as k grows. 
　the specific following idea has been used before   ikeda and lmai  1  is tht earliest reference we could find  and it has been worked on recently   yoshizumi et al  1  inter alia   though perhaps it has not received the attention it deserves in the bioinformatics literature. we present it here because  1  it is a good example that can be cast into our framework and  1  it gives a good starting intuition for the novel cases below. since the score of an arc  timeslice  is a sum of pairwise goodness scores  we can define a set of projections  one onto each pair of indices under a node  will project to its a and b indices   
it is easy to see that the optimal path in this projection is just the optimal 1-way alignment of the portions of sequences a 
and b which are to the right of the indices i and respectively. we can therefore bound the total completion cost of the a-way alignment from n onward with the sum of the pairwise completion costs of the 1-way alignments inside each projection. 
　figure 1 shows some experimental speed-ups given by this method  compared to exhaustive search. we took several protein sequence groups from  mcclure et ai  1   and  for each set  we aligned as large a subset of each group as was possible using uniform-cost search with 1gb of memory. the left four runs show the cost  in nodes visited  of aligning these subsets with both uniform-cost search and a* search. in the right four runs  we added another sequence to the subsets and solved the multiple alignment using only the a* search. the a* savings are substantial  usually providing several orders of magnitude over uniform-cost search  and many orders of magnitude over the exhaustive dynamic programming approach. this verifies previous findings  and 
　　1 note that the dp was never run - it would not have fit in memory - but it is easy to calculate the size of the lattice. there are also subtleties in running the uniform-cost search since the score of a timeslice can be negative  we add in a worst possible negative score . 

figure 1: effectiveness of the factored a* bound for the multiple alignment of several sequence sets. numbers in the data set names give the number of sequences being aligned. note the log scale: for example  on glob-1 the a* search is over a trillion times more 
efficient than exhaustive search. 
shows that factored a* bounds can be highly effective. 
　one potential worry with a* search is that the cost of computing heuristics can be substantial. in this case  the 1 k1  pairwise alignments could be calculated very efficiently; in our experiments this pre-search phase took up much less than 1% of the total time. 
1 	example: finite state transducers 
we briefly describe another potential application of factored a* search for sequence models: the intersection of weighted finite-state transducers  wfsts .1 wfsts are probabilistic mappings between sequences in one alphabet to sequences in another  for example a transducer might map an input of written text to an output of that text's pronunciation as a phoneme sequence. intersections of wfsts have been applied to various tasks in speech and nlp  mohri  1   such as text-to-speech  and  most famously in the nlp lit-
 erature  modeling morphophonology  kaplan and kay  1; albro  1 . in these cases  each transducer constrains some small portion of the overall output sequence. the case of finding the most likely intersected output of a set of wfsts  a/ } for an input sequence  involves the following: 
1. for each a/   create the projection  of the full output space o onto mt's output space  note that this can be the identity projection .1 
1. 
1.  
　while transduction intersection fits cleanly into the factored framework  the primary utility of transducers lies in their composition  not their intersection  mohri  1 . in this case  transducers are chained together  with the output of one serving as the input to the next. in this case  it is worth switching from talk of summed distances to talk of multiplied probabilities. say we have two transducers  which gives a distribution from sequences to sequences x  and which gives from x to o. 
　　1  wfsts are equivalent to hmms which have emission weights assigned to their transitions  not states  and which may have epsilon transitions. 1 for simplicity  we assume all history relevant to any transducer is encoded into the state space o. 

search 	1 

figure 1: two representations of a parse:  a  a tree   b  a path in the edge hypergraph. 
we then wish to answer questions about their composed behavior. for example  we might want to find the output o which maximizes  according to this model. the common viterbi approximation is to settle for the o from the pair  1  x  which maximizes  this problem would fit neatly into the factored framework if the  usually false  conditional independence where 
true - in fact it would then be wfst intersection. however  something close to this does trivially hold:  
given this  we can define another model 
r is not a proper probabilistic 
model - it might well assign probability one to every transduction - but its intersection with  does upper-bound the actual composed model. hence these two projections provide a factored bound for a non-factored model  with the practical utility of this bound depending on how tightly typically bounds  
1 projection models for trees 
search in the case of trees is not over standard directed graphs  but over a certain kind of directed hypergraph in which arcs can have multiple sources  but have a single target . this is because multiple sub-trees are needed to form a larger tree.1 figure 1 shows a fragment of such a hypergraph for a small parse tree  note that all the lines going to one arrowhead represent a single hyperarc . we don't give the full definitions of hypergraph search here  see  gallo et al.  1  for details   but the basic idea is that one cannot traverse an arc until all its source nodes have been visited. in the parse case  for example  we cannot build a sentence node until we build both a noun phrase node and an  adjacent  verb phrase node. the nodes in this graph arc identified by a grammar 
　　1 these directed b-hypergraphs model what has been explored as and/or trees in ai. 

next  we present a concrete projection model for scoring lexicalized trees  and construct an a* parser using the associated factored a* bound. 
　generative models for parsing natural language typically model one of the kinds of structures shown in figure 1. while word-free syntactic configurations like those embodied by phrase structure trees  figure 1a  are good at capturing the broad linear syntax of a language  charniak  1   wordto-word affinities like those embodied by lexical dependency trees  figure 1b  have been shown to be important in resolving difficult ambiguities  hindle and rooth  1 . since both kinds of information are relevant to parsing  the trend has been to model lexicalized phrase structure trees like figure 1c. 
　in our current framework  it is natural to think of a lexicalizcd tree as a pair /  =  t  d  of a phrase structure tree t and a dependency tree d. in this view  generative models over lexicalized trees  of the sort standard in lexicalized pcfg parsing  collins  1; charniak  1   can be regarded as assigning mass p t  d  to such pairs. in the standard approach  one builds a joint model over p t  d   and  for a given word sequence own  one searches for the maximum posterior parse: 
since for the maximizer of p t  d  w . 
the naive way to do this is an  dynamic program 
 often called a tabular parser or chart parser  that works as follows. the core declarative object is an edge  such as which encapsulates all parses of the span 
which are labeled with grammar symbol x and are headed by word  edges correspond to the nodes in the 

1 	search 

1. extract the pcfg projection and set up the pcfg parser. 
1. use the pcfg parser to find projection scores  for each edge. 
1. extract the dependency projection and set up the dependency parser. 
1. use the dependency parser to find projection scores  for each edge. 
1. combine pcfg and dependency projections into the full model. 
1. form the factored a* estimate  
1. use the combined parser  with h s  e  g  as an a* estimate of  
figure 1: the top-level algorithm  left  and an illustration of how paths decompose in the parsing hypergraph  right . 

parsing hypergraph. two edges  can be combined whenever they are contiguous  the right one starts where the left one ends  and the grammar permits the combination. for example  if there were a rewrite  those two edges would combine to 
form  z  z  i  a   and that combination would be scored by some joint model over the word and symbol configuration: 
 these weighted combinations are the arcs 
in the hypergraph. 
   a natural projection of a lexicalized tree l is onto its components t and d  though  to our knowledge  this projection has not been exploited previously . in this case  the score for the combination above would be  

this kind of projected model offers two primary benefits. 
first  since we are building component models over much simpler projections  they can be designed  engineered  and tested modularly  and easily. to underscore this point  we built three pcfg models of p t  and two lexical dependency models of p t . in section 1  we discuss the accuracy of these models  both alone and in combination. 
   second  our a* heuristic will be loose only to the degree that the two models prefer different structures. therefore  the combined search only needs to figure out how to optimally reconcile these differences  not explore the entire space of legal structures. figure 1 shows the amount of work done in the uniform-cost case versus the a* case. clearly  the uniform-cost version of the parser is dramatically less efficient; by sentence length 1 it extracts over 1k edges  while even at length 1 the a* heuristics are so effective that only around 1k edges are extracted. at length 1  the average number is less than 1  and the fraction of edges not suppressed is better than 1 ok  and it improves as sentence 
1
　　most models  including ours  will also mention distance; wc ignore this for now. 
   1 as a probabilistic model  this formulation is mass deficient  assigning mass to pairs which are incompatible  either because they do not generate the same terminal string or do not embody compatible bracketings. therefore  the total mass assigned to valid structures will be less than one. we could imagine fixing this by renormalizing. in particular  this situation fits into the product-of-experts framework  hinton  1   with one semantic expert and one syntactic expert that must agree on a single structure. however  since we are presently only interested in finding most-likely parses  no global renormalization constants need to be calculated. in any case  the question of mass deficiency impacts only parameter estimation  not inference  which is our focus here. 
length increases .1 the a* estimates were so effective that even with our object-heavy java implementation of the combined parser  total parse time was dominated by the initial  array-based pcfg phase  see figure 1b .1 
1 	specific projection models for parsing 
to test our factored parser  we built several component models  which were intended to show the modularity of the approach. we merely sketch the individual models here; more details can be found in  klein and manning  1 . for p{ 1   we built successively more accurate pcfgs. the simplest  pcfg-basic  used the raw treebank grammar  with nonterminals and rewrites taken directly from the training trees  charniak  1 . in this model  nodes rewrite atomically  in a top-down manner  in only the ways observed in the training data. for improved models of p t   tree nodes' labels were annotated with various contextual markers. in pcfg-pa  each node was marked with its parent's label as in ljohnson  1 . it is now well known that such annotation improves the accuracy of pcfg parsing by weakening the pcfg independence assumptions. for example  the np in figure 1a would actually have been labeled np*s. since the counts were not fragmented by head word or head tag  we were able to directly use the mle parameters  without smoothing.1 the best pcfg model  pcfg-ling  involved selective parent splitting  order-1 rule markovization  similar to  collins  1; charniak  1    and linguistically-derived feature splits. 
   1 note that the uniform-cost parser does enough work to exploit the shared structure of the dynamic program  and therefore edge counts appear to grow polynomially. however  the a* parser does so little work that there is minimal structure-sharing. its edge counts therefore appear to grow exponentially over these sentence lengths  just like a non-dynamic-programming parser's would. with much longer sentences  or a less efficient estimate  the polynomial behavior would reappear. 
　1 thcre are other ways of speeding up lexicalized parsing without sacrificing search optimally. eisner and satta  eisner and satta  1  propose a clever  modification which separates this process into two steps by introducing an intermediate object. however  even the  formulation is impractical for exhaustive parsing with broad-coverage  lexicalized treebank grammars. the essential reason is that the non-terminal set is just too large. wc did implement a version of this parser using their formulation  but  because of the effectiveness of the estimate  it was only marginally faster; as figure 1b shows  the combined search time is very small. 
   1 this is not to say that smoothing would not improve performance  but to underscore how the factored model encounters less sparsity problems than a joint model. 

search 	1 



　models of p d  were lexical dependency models  which deal with part-of-speech tagged words: pairs  first the head of a constituent is generated  then successive right dependents until a stop token is gen-
erated  then successive left dependents until is generated again. for example  in figure 1  first wc choose fell-vbd as the head of the sentence. then  we generate in-in to the right  which then generates september-un to the right  which generates  on both sides. we then return to i/i-in  generate  to the right  and so on. the dependency models required smoothing  as the word-word dependency data is very sparse. in our basic model  dbp-basic  we generate a dependent conditioned on the head and direction  requiring a model of  this was estimated using a back-off model which interpolated the sparse bilexical counts with the denser but less specific counts given by ignoring the head word or by first generating the dependent tag and then generating the dependent word given only the dependent tag. the interpolation parameters were estimated on held-out data. the resulting model can thus capture classical bilexical selection  such as the affinity between payrolls and fell  as well as monolexical preferences  such as the tendency for of to modify nouns. in the enhanced dependency model  depval  we condition not only on direction  but also on distance and valence. note that this is  intentionally  very similar to the generative model of  collins  1  in broad structure  but substantially less complex. 
1 	parsing performance 
in this section  we describe our various projection models and test their empirical performance. there are two ways to measure the accuracy of the parses produced by our system. first  the phrase structure of the pcfg and the phrase structure pro-
jection of the combination parsers can be compared to the treebank parses. the parsing measures standardly used for this task are labeled precision and recall.1 we also report fi  the harmonic mean of these two quantities. second  for the dependency and combination parsers  we can score the dependency structures. a dependency structure d is viewed as a set of head-dependent pairs  h  d   with an extra dependency {root  x  where root is a special symbol and x is the head of the sentence. although the dependency model gencrates part-of-speech tags as well  these are ignored for dependency accuracy. punctuation is not scored. since all dependency structures over n non-punctuation terminals contain n dependencies  n - 1 plus the root dependency   we report only accuracy  which is identical to both precision and recall. it should be stressed that the  correct  dependency structures  though generally correct  are generated from the pcfg structures by linguistically motivated  but automatic  and only heuristic rules. 
　figure 1 shows the relevant scores for the various pcfg and dependency parsers alone. the valence model increases the dependency model's accuracy from 1% to 1%  and each successive enhancement improves the fj of the pcfg models  from 1% to 1% to 1%. the combination parser's performance is given in figure 1. as each individual model is improved  the combination f  is also improved  from 1% with the pair of basic models to 1% with the pair of top models. the dependency accuracy also goes up: from 1% to 1%. note  however  that even the pair of basic models has a combined dependency accuracy higher than the enhanced dependency model alone  and the top three have combined f  better than the best pcfg model alone. for the top pair  figure 1c illustrates the relative f1 of the combination parser to the pcfg component alone  showing the unsurprising trend that the addition of the dependency model helps 

1 	search 

more for longer sentences  which  on average  contain more attachment ambiguity. the top f1 of 1% is greater than that of the lexicalized parsers presented in  magerman  1; collins  1   but less than that of the newer  more complex  parsers presented in  charniak  1; collins  1   which reach as high as 1% f1. however  it is worth pointing out that these higher-accuracy parsers incorporate many finely wrought enhancements which could presumably be applied to benefit our individual models.1 
1 	factored bounds for non-projection models 
arbitrary tree models will not be factored projection models. 
for example  while our parsing model was expressly designed so that  to our knowledge no other model over lexicalized trees with this decomposition has been proposed. nonetheless  non-factored models can still have factored bounds. given any model p a  b   we can imagine 
bounds r a  and r b  that obey: 
trivially  		do. 	to get a non-trivial bound  consider a joint  local  model  of lexicalized tree rewrites. 	early lexicalized parsing work  charniak  1  used models of exactly this form. 	we can use the chain rule to write 
this technique allows one to use 
factored a* search for non-factored models  though one might reasonably expect such bounds to be much less sharp for non-factored models than for factored models. a particular application of this method for future work would be the exact parsing of the models in  charniak  1; collins  1; charniak  1   as the details of their estimation suggest that their word dependency and phrase structure aspects would be approximately factorizable. 
1 conclusion 
not all models will factor  nor will all models which factor necessarily have tight factored bounds  for example msa with many sequences or parsing if the component models do not prefer similar structures . however  when we can design factored models or find good factored bounds  the method of factored a* search has proven very effective. for the msa problem  a* methods allow exact alignment of up to 1 protein sequences  though 1 is more typical  of length 1  when even three-way exhaustive alignment can easily exhaust memory. for the parsing problem  we have presented here the first optimal lexicalized parser which can exactly parse sentences of reasonable length using large real-world penn treebank grammars. the projected models can be designed and improved modularly  with improvements to each model raising the combined accuracy. finally  we hope that this framework can be profitably used on the other sequence 
1 for example  the dependency distance function of  collins  
1  registers punctuation and verb counts  and both smooth the pcfg production probabilities. 
models we outlined  and on any large space which can naturally be viewed as a composition of  possibly overlapping  projections. 
acknowledgements 
we would like to thank lillian lee  fernando pereira  and dan melamed for advice and discussion about this work. 
