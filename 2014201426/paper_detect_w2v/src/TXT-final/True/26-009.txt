 
this paper investigates alternative estimators of the accuracy of concepts learned from examples. in particular  the cross-validation and 1 bootstrap estimators are studied  using synthetic training data and the foil learning algorithm. our experimental results contradict previous papers in statistics  which advocate the 1 bootstrap method as superior to crossvalidation. nevertheless  our results also suggest that conclusions based on cross-validation in previous machine learning papers are unreliable. specifically  our observations are that  i  the true error of the concept learned by foil from independently drawn sets of examples of the same concept varies widely   ii  the estimate of true error provided by cross-validation has high variability but is approximately unbiased  and  iii  the 1 bootstrap estimator has lower variability than cross-validation  but is systematically biased. 
1 introduction 
the problem of concept induction  also known as the classification problem  kononenko and bratko  1  and known as the prediction problem in the statistical literature  efron  1   is perhaps the most intensively studied topic in machine learning. given a training set of classified examples drawn from a certain domain  and a language for stating concept definitions  the problem is to invent a concept definition inspired by the training set that will correctly classify new examples drawn from the domain. 
　various methods have been proposed to evaluate the accuracy of learned concept definitions. most researchers have used methods whose objective is  explicitly or implicitly  to approximate what we call true error: the expected error of the learned concept on new examples. cross-validation is the method most often used  
    this work wis supported in part by the national science foundation under award no. iri-1. timothy bailey is supported by an nih human genome project predoctoral training grant. 
as for example in  towell et a/.  1  and  weinstein et a/.  1 . however several monte carlo studies of cross-validation and alternative methods have been done 
 fitzmaurice et a/.  1; sanchez and cepeda  1; efron  1   which tend to indicate that cross-validation is inferior to the other methods. 
　compared to previous studies of methods for evaluating the performance of learning algorithms  this paper has several novelties. first  in addition to investigating the variance and bias of estimators of true error  we also examine how randomness in the choice of training examples leads to variance in true error itself. to do this we need to be able to evaluate true error exactly  which we achieve with synthetic data sets of examples of known concepts. knowledge of exact true error rates also underlies the second novelty of our work  which is that we investigate the correlation between estimators of error and true error  distinguishing between downward  optimistic  and upward  pessimistic  bias. the third novelty here is the learning algorithm used  foll quinlan  1 . logical rule learning  sometimes called inductive logic programming  is an active research area currently  and foil and its variants are the most widely used rule-learning algorithms. from a more general point of view  the novelty of foil is that it is almost completely insensitive to the presence of duplicate examples in a training set  unlike the learning algorithms used in previous studies. 
　the rest of this paper is laid out as follows. the  true error  metric for evaluating learned concepts and methods of estimating this metric are described in section 1. the foil learning algorithm and the synthetic data sets used in our experiments are the topic of section 1. our experimental results are presented in section 1  and finally  our conclusions appear in section 1. 
1 measuring learned concept accuracy 
in the concept induction problem  the learning algorithm is given as input a training set x consisting of a set of examples x1  x1        xn where each example consists of two parts xi =  ti yi   where ti is a vector of attributes and yi  is the class. the algorithm constructs a concept definition that uses the attributes to predict the class. 
we assume that the training set is selected randomly acbailey and elkan 1 
cording to some  unknown  probability distribution f.1 
　for the classification problem  the most commonly used criterion for the goodness of a concept definition is its true error rate: the probability that the concept definition will incorrectly classify an example drawn randomly from the same distribution f as the training set. this is sometimes referred to as the generalization error of the concept definition since it measures how well the concept definition generalizes to examples the learning algorithm did not see. 
　the definition of true error. the following definitions are essentially the same as those in  efron  1 . suppose a learning algorithm constructs prediction rule r  t x  from training set x. let ni  = n t i  x  be the prediction of this rule on example xi  and let q yi  yi   be the error of the learned rule on that example. formally 

the true error rate err is then the probability of incorrectly classifying a randomly selected example xq =  to lo   which is the expectation 

readers familiar with pac learning theory will notice that this definition of true error rate subsumes the definition of the error of a hypothesis with respect to a target concept given in  haussler  1  and often used by pac theorists. the difference is that the pac framework usually assumes that the examples are either  in  or  out  of a hypothesis: that the hypothesis and target concept are deterministic. we do not make that assumption about either the learned concept or the target concept  i.e. the source of examples . it is quite possible that two examples may have the same attribute values and yet have different classes in our framework. 
　the issue of estimating true error. we must usually estimate true error rate err since the distribution f is usually unknown. the most conceptually straightforward way to do this is to randomly draw a test set of examples  independent of the training set  and take the mean error to be the test error 

where m is the size of the test set. terr approaches err as m -  1. unfortunately  this test set method of estimating err is often infeasible because collecting examples is expensive. also  if the number of examples available is limited  the user of the learning algorithm will want to use them all in constructing the classification rule. of course  if data is cheap and err is very low  the test set approach may be used. 
　several methods for estimating err exist for use when the test set method is unattractive. these methods use 
　　1  notice that the distribution of the examples  f  is over both the class and the attributes of the examples. in much work in machine learning the distribution is just over the attributes  and the class is assumed to be deterministtcally dependent on the attributes. our definition subsumes the usual definition as a special case. 
the training set x itself as the basis for estimating err.  efron  1  shows that the three methods known as bootstrap  jackknife and cross-validation are mathematically related. bootstrap is a nonparametric maximum likelihood estimator 1 jackknife is a quadratic approximation to bootstrap  and cross-validation is similar in form and value to jackknife. later work  efron  1  argues empirically and analytically that a modified bootstrap method known as 1 bootstrap is superior. therefore  we focused on cross-validation and 1 bootstrap estimators in our work. 
　the cross-validation method. cross-validation estimates err by reserving part of the training set for testing the learned theory. in general  v-fold cross-validation  randomly  splits the training set into v equal-sized subsets  trains on v-1 subsets and tests on one subset. each subset is left out of the training set and used as the test set once. a common choice for v is the size n of the original training set. since each subset is then a singleton  this is called  leave-one-out  or n-fold cross-validation. 
　for a given amount of training data  leave-one-out cross-validation allows learning from the largest possible number of examples  while still basing the estimation of accuracy on unseen data. intuitively  the true error of concepts learned on n - 1 examples during leave-one-out cross-validation should be close to what we are trying to estimate  which is the true error of the concept learned from all n examples. other methods which use smaller subsets for training  in particular v-fold cross-validation where v   n  should intuitively be poorer estimates of err when the number of training examples available is small. 
　the 1 bootstrap method. the 1 bootstrap technique  efron  1  for estimating err creates a  resample  from a training set by choosing n samples with replacement from the training set. resamples are typically multisets. the 1 bootstrap estimator is defined as e1 = 1er + 1eb where eb is the proportion of the examples not chosen in the resample that are misclassified by the rule learned on the resample  and er is the proportion of training set examples which are misclassified by the rule learned on the whole training set. 
in practice  eb is averaged over many resamples. 
　previous comparisons of methods. the 1 bootstrap method is reported in  efron  1  to estimate true error better in five experiments than several other methods including the original bootstrap method and crossvalidation. more recent and comprehensive experiments using linear discriminant classifiers confirm the good performance of the 1 bootstrap method  fitzmaurice et a/.  1; sanchez and cepeda  1 . the main crite-
rion for evaluating an estimator err in these papers is mean squared error  defined as mse = e err - err 1. the mse of an estimator is a combination of its bias and variance  and is insensitive to whether bias is upward or downward. here we examine variance  bias  and the direction of bias separately. 
　　1  the bootstrap estimate of err replaces the true distribution f with its nonparametric maximum likelihood estimate f where f is the empirical probability distribution putting equal mass 1/n on each observed sample z . 

　in experiments using nearest neighbor classifiers  both cross-validation and the 1 bootstrap method have been reported to perform poorly  and a composite method has been suggested  weiss  1; weiss and kulikowski  1 . linear classifiers and nearest neighbor methods are very different from symbolic concept induction methods such as foil or decision tree algorithms. in almost all symbolic learning work cross-validation has been used to estimate the accuracy of learned concepts. one exception is work using the cart decision tree learning algorithm  for which the original and 1 bootstrap methods are compared with cross-validation in  crawford  1   with the conclusion that 1 bootstrap is best. 
1 experimental framework 
this section describes the learning algorithm and the data sets that we used to study the performance of crossvalidation and the 1 bootstrap method as estimators of the true error of learned concepts. 
　the foil algorithm. this algorithm  quinlan  1  produces concept definitions which are sets of functionfree datalog-with-negation clauses. training sets given to foil are encoded as relation extensions  i.e. as lists of ground tuples. one or more of the relations is designated as the target relation  and foil attempts to learn an intensional definition for it in terms of the other relations. 
　for example  foil might be given the relations linkedto x  y  and can-get-to x  y  defined extensionally and asked to find an intensional definition of can-get-to x  y . if the extensions of the two relations are such that canget-to x  y  is the transitive closure of linked-to x  y   foil may succeed in finding the intensional definition  
can-get-to x y  -  linked-to x  y  can-get-to x y   --- linked-to x z   inked-to z y . 
there are many possible encodings for a given classification problem. the particular one chosen can greatly affect the efficiency of foil and whether or not a concept definition is found at all. one advantage of foil over other learning algorithms is that background knowledge can be provided in the form of additional relations that can be used in forming the concept definition. 
　foil uses a greedy algorithm that builds concept definitions one clause at a time. each clause is built one literal at a time  trying all possible variabilizations of each possible relation  and adding the one with the highest  information gain  to the clause. there is limited within-clause backtracking: if no literal can be found with positive information gain  the algorithm removes the last literal added to the clause and replaces it with another candidate with positive  but lower  gain. 
　synthetic data sets. we constructed synthetic data sets in order to be able to evaluate true error exactly. these data sets were designed to be similar to real molecular biology data sets that we extracted from the epd eukaryotic promoter genetic sequence database for other work. each synthetic data set contained positive and negative examples of a single  short  disjunctive normal form concept  dnf . each example was defined by 1 binary attributes. the dnf concepts chosen were all short  i.e.  they contained few clauses and the clauses were short   so the majority of the 1 attributes are uncorre-ted with the concept: they are  noise  or  irrelevant  attributes. the actual concepts used are as follows. 

the distribution of training examples for each concept was the same. we randomly selected examples from a mixture distribution with positive and negative examples having equal probability. in other words  to generate an example we randomly chose to generate either a positive or negative example  with equal probability  and then randomly generated binary strings of length 1 until an example of the correct class was found. this example was then put in the training set and the process was repeated until the desired number of examples had been generated. 
1 experimental results 
we ran a number of monte carlo simulations using synthetic data sets and foil as just described. in each experiment we measured true error rate err  its crossvalidation estimate  and its 1 bootstrap estimate. in brief  we discovered that 1 bootstrap performs very poorly with this learning algorithm and these target concepts. 1 bootstrap had lower variance than crossvalidation  but it had very poor correlation with err and was strongly biased. 
　experimental design. we performed a number of experiments using data generated by the target concepts dnfl  dnf1  dnf1 and dnf1. the basic procedure in each experiment for each distribution was  using foil as the learning algorithm  as follows. except in figure 1  because of space limitations results are plotted below for dnfl only. qualitatively similar results were always obtained for each concept. 

computing the value of err was accomplished by exhaustively comparing the value of the learned concept definition and the true concept definition for every possible combination of values of the relevant attributes. the relevant attributes were those mentioned in the true concept definition or in the learned concept definition. in general  the number of relevant attributes was only about 1  making it possible to exhaustively check all 1 possible combinations. it would have been prohibitive to check all 1 possible combinations of attributes. 
　scatter plot experiments. in these experiments  the basic procedure was repeated 1 times for each distribution. the size of the training set in each experiment 
	bailey and elkan 	1 



true error  err   %  
figure 1: scatter plot and least-squares line fit for crossvalidation and 1 bootstrap estimates versus err. 
was 1 samples. all the experiments gave very similar results so we combined them into two scatter plots. the results are shown in figure 1. we can see that both cross-validation and 1 bootstrap have high variance as estimators of err. it is also apparent that err  the quantity which we are trying to estimate  has high variance as well. foil often learns the dnf concepts perfectly from some training sets of a given size  but learns a concept with very high err on other training sets of the same size. this fact makes the correlation of the estimators with the quantity being estimated of significant interest. the correlation of 1 bootstrap with err is close to zero  while the correlation of cross-validation with err is much better  around 1. note that leastsquares regression of each estimator on err gives a line with positive intercept: this indicates that the expected estimate of err is nonzero even when the target concept is learned perfectly. 
　the following table shows the mean and  sample  standard deviation of err and its cross-validation and 1 bootstrap estimates for the 1 runs in the scatter plot experiments. 

clearly 1 bootstrap is biased upward for all four concepts. the bias of cross-validation as an estimator of err  on the other hand  is very small. the variance of 1 bootstrap is generally much less than that of 
1 number of training examples 
figure 1: learning curve for foil on dnfl. 
cross-validation. this has always been a primary reason for considering bootstrap methods over cross-validation  efron  1 . 
　the scatter plot experiments show the basic pitfalls of cross-validation as a method of estimating err. the outliers in the scatter plots of cross-validation estimate of err versus err itself show that cross-validation estimates based on a single training set can greatly overor under-estimate true error. it is dangerous to conclude that one learned concept definition is better than another solely on the basis of cross-validation. however  the low bias and relatively high correlation of cross-validation with err indicates that  on average  cross-validation is a good estimator of err. 
　learning curve experiments. to understand better the reasons behind the poor performance of the 1 bootstrap method  and to see how cross-validation performed with different training set sizes  we conducted experiments to construct learning curves for foil on the four target concepts. 
　the learning curve experiments consisted of repeating the basic procedure 1 times for a given training set size and a given distribution to get mean values for err  cross-validation and 1 bootstrap. this was repeated for various sizes of the training set and plotted. the results are shown in figure 1 for dnfl. error bars show plus and minus the sample standard deviations of the measured quantities. for visual clarity  only the error bars for err are shown. 
　it can be seen that cross-validation does well over a large range of training set sizes at estimating the mean value of err for that training set size. its bias is quite low. on the other hand  the bias of 1 bootstrap is downward for small training sets and upward for large training sets. 
　the 1 bootstrap estimator has been reported to have lower variance than cross-validation. figure 1 confirms this: the sample standard deviation  ssd  of 1 bootstrap is lower than that of both cross-validation and err in almost all cases. for dnfl  the ssd of 1 bootstrap tends to be flat over a wide range of training set sizes. for other target concepts  the ssd of 1 bootstrap 


figure 1: sample standard deviation of error measures versus training set size for foil learning dnf1. 

figure 1: learning curve for foil on dnfl with bootstrap plotted against 1 number of samples . 

becomes larger than that of err and cross-validation when the size of the training set becomes large and consequently  the value of err becomes small. the 1 bootstrap method can thus lose its main advantage over cross-validation when the value of err is small. 
　explaining the failure of 1 bootstrap. the poor performance of 1 bootstrap is surprising in view of earlier  positive reports in the statistical literature  fitzmaurice et a/.  1; efron  1 . however  that work measured the accuracy of 1 bootstrap for training data generated from two multivariate normal classes. the best rule for classifying such data will have nonzero true error since the attributes of an example are not sufficient to determine which class it belongs to. by contrast  our data always came from classes which could be perfectly discriminated. 
　the 1 bootstrap estimate of error  as mentioned previously  is a weighted average of erroreb on the samples left out during resampling and resubstitution error er. the foil algorithm tends to learn concepts that  cover  all the training examples. thus  resubstitution error tends to be close to zero  so the 1 bootstrap estimate of error is essentially equal to 1eb. it is noticeable that eb is a good estimate of err on 1n samples. this can be seen in figure 1  which plots the values of eb measured on n samples at 1n on the x axis. 
　bootstrap resampling results in training sets which are multisets. the expected number of distinct points in a resample is about 1 times the size of the original dataset.1 the effect shown in figure 1 can be explained if foil learns essentially the same concept on the bootstrap resampled multiset as it would on the set obtained by removing duplicates from the resample. figure 1 shows learning curves for foil applied to resamples with and without duplicates removed. the results confirm our suspicion. the poor performance of the 1 bootstrap method used with foil appears to be due to 
1
　　the probability of any given example in the original dataset being chosen during resampling is 1 -  1 - l/n n  which is approximately 1 - e-1 = 1. 
the fact that foil does not benefit from duplicates in a multiset of training examples. the concepts learned by foil are  however  different with and without duplicates  as can be seen by the fact that the curves for eb are different. 
　it is worth noting that the one-nearest-neighbor classification method  with which the 1 bootstrap method is reported to perform poorly  weiss  1   is a learning algorithm for which by definition duplicates in the training set have no influence. other learning algorithms with which the 1 bootstrap method has been reported to work well  notably fisher's linear discriminant method used in  fitzmaurice et a/.  1  and  efron  1  and the cart decision tree algorithm used in  crawford  1   are strongly influenced by duplicates. 
1 discussion 
we studied the performance of cross-validation and 1 bootstrap as estimators of the accuracy of concept definitions learned from synthetic data by foil. we observed the following. 
 i  the true accuracy err of learned concepts has high variance. that is  the error of the concept learned by foil from independently-drawn sets of examples of the same concept varies widely. 
 ii  the 1 bootstrap estimator has lower variance than cross-validation  but it is biased. in our experiments  this bias was upward when the value of err was below 1%  and downward when err was above 1%. 
 iii  the estimate of err provided by cross-validation has high variance but is approximately unbiased. 
each of these observations carries implications for future experimental work comparing learning algorithms. the first observation  if it also applies to algorithms other than foil  implies that standard statistical tests are invalid for deciding whether one algorithm is significantly better than another  when the available data concerns 
	bailey and elkan 	1 


figure 1: foil learns poorly on multisets. average err for 1 training sets  multiset resamples and resamples with duplicates removed. 
both algorithms learning from single data sets. in  weinstein et ai  1   for example  backpropagation in a neural network with 1 hidden units is compared with fisher's linear discriminant method using a single common data set of 1 examples. the following leave-oneout cross-validation data are given: 
correct incorrect  nn with 1 hidden units   linear discriminant 1 
1 1 
1 the authors apply a statistical test similar to the x1 test to conclude that backpropagation in a neural network with 1 hidden units performs significantly better than fisher's linear discriminant method. the null hypothesis in this test is that the concept learned by each algorithm has the same accuracy  and that the observed correct/incorrect numbers are therefore the result of two sets of 1 binomial trials with the same probability of success. however  an extra level of randomness is involved here. the training set used to measure the performance of both algorithms may by chance be one on which backpropagation performs well  whereas the linear discriminant method performs poorly. despite the significant difference in performance of the two algorithms on this data set  the algorithms may well be indistinguishable on independent data sets identically distributed to this data set. 
　the second observation above is disappointing because previous work has concluded that 1 bootstrap performs better than cross-validation. the poor performance of 1 bootstrap here appears to be caused by the fact that the foil algorithm learns almost the same concept on a multiset as on its projection. future experimental work on learning algorithms should not use the 1 bootstrap method unless all learning algorithms being tested are sensitive to repeated training examples. despite the caution required given the first observation listed above  the third observation leads us to recommend continuing to use cross-validation for evaluating the performance of learning algorithms. we recommend cross-validation in particular because of its strong  but not perfect  correlation with err. however  experimenters must keep in mind that unfortunately  which estimator of learned concept accuracy is best depends on which learning algorithm is used. 
