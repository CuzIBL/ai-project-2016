 
　we propose a mechanism for the promotion of high-standards in commercial artificial intelligence products  namely an association of companies which would regulate their own membership using a code of practice and the precedents set by previous cases. membership would provide some assurance of quality. we argue the benefits of such a mechanism  and discuss some of the details including the proposal of a code of practice. this paper is intended as a vehicle for discussion rather than as the presentation of a definitive solution. 
acknowledgements 
　we are grateful to members of the edinburgh computing and social responsibility group for feedback on an earlier draft of the code  and to maggie boden for comments on the paper itself. 
1. the need for high standards in ai products 
credibility has always been a precious asset for 
ai  but never more so than now. the current commercial interest in ai is giving us the chance to prove ourselves. if the range of ai products now coming onto the market are shown to provide genuine solutions to hard problems then we have a rosy future. a few such useful products have been produced  but our future could still be jeopardised by a few  well publicised  failures. 
　genuine failures - where there was determined  but ultimately unsuccessful  effort to solve a problem - are regrettable  but not fatal. every technology has its limitations. what we have to worry about are charlatans and incompetents taking advantage of the current fashion and selling products which are overrated or useless. ai might then be stigmatised as a giant con-trick  and the current tide of enthusiasm would ebb as fast as it flowed.  remember machine translation - it could still happen.  both companies selling ai products and academic ai research groups would suffer in the resulting crash. 
　ai companies are very dependent on the good-will of their customers. the current life-span of typical ai products is about 1 years. the customers of ai products are likely to stay in the market for several times this period; typically they are themselves companies or academic groups engaged in ai research or interested in the long term application of ai techniques. to stay in business the ai company must sell successive upgrades of its products to the same group of customers and  therefore  must build up and maintain a good reputation. if ai business is to expand then new customers must be brought into this existing group. this will only happen if the overall range of ai products is of high-quality and the reputation of this particular company is good. thus it is in the interests of each company to raise both the general and its particular standard. it must also convince customers that its products are of high standard. when the market was small a company with highquality products could win new customers by word of mouth. now the market is growing they must use advertisements  and it becomes harder for a company to convince potential customers that its products are of high quality. 
　apart from improving the public image of ai and increasing the market for ai products  producing more high-quality products would raise morale and standards in ai itself  leading to a virtuous circle of standards being raised  better work being done  good people being attracted to the field  and even more high quality products emerging. poor-quality products will produce a vicious circle going in the opposite direction. 
　but these internal reasons for wanting high standards  while important to insiders  are perhaps less important than external reasons. ai products look destined to play a major role in society. that society deserves  and has the right to expect  protection from exploitation by ai companies and from being harmed by at products. 
　an extreme  potential example of such harm is described in  thompson et al 1   which argues that it is not possible to build an automatic or semiautomatic launch-on-warning system for nuclear weapons with anything like an acceptable failure rate. anybody who claimed to have done so  or who claimed to be able to do so  would be guilty of misleading the public in a way that could have disastrous consequences. if such a claimant were an ai company then the whistle might be blown on it by the mechanism described below. 
　however  the main purpose of this proposal is to catch less apocalyptic  but more common-place  mis-

1 a. bundy and r. clutterbuck 
leading claims  whether or not they might give rise to a legal remedy. examples might be an expert systems shell whose advertised range far exceeds the problems it is really suitable for  or a natural language front end which is presented as being able to deal with a much wider input than it  in fact  can. 
1. a professional association 
　the academic field guards itself against charlatans and incompetents by the peer review of research papers  grants  phds  etc. there is no equivalent safeguard in the commercial ai field. faced with this problem other fields set up professional associations and codes of practice. ai needs a similar set-up. we propose that the responsible ai companies should get together now to found such an association. continued membership should depend on a constant high-standard of ai products and in-house expertise. members would be able to advertise their membership  and customers would have some assurance of quality. charlatans and incompetents would be excluded or ejected  so that the failure of their products would not be seen to reflect on the field as a whole nor on the companies in the association. 
　since the trade in ai products is international  with multi-national companies involved both as vendors and customers  the association would also need to be international. otherwise  there would be difficulty over membership of multi-national companies and about dealing with complaints resulting from international sales. it is particularly important to make membership attractive to multi-nationals because  with their existing reputation  they have less to gain from the cachet of membership and are more able to avoid the full impact of national registration. 
　the association would be self-regulating. if its decisions were seen to be too arbitrary then the value of association membership would be devalued in the eyes of the public  the customers and the vendors  and the importance of its decisions would decrease in proportion; customers would take no account of association membership when deciding to buy  so vendors would not bother to join. for such self-regulation to work it is necessary for the association to be publically visible. both vendors and potential customers must be aware of the association and must see its decisions as fair and its sanctions as effective. customers will then use association membership as a major determinant when deciding whether and what product to buy. vendors will regard association membership as a valuable asset to their company  and will aim for high quality in their products in order to retain membership. they will want to use association membership in their advertising  and this will  in turn  improve the visibility of the association. it will be necessary for the association to maintain a high profile of both its existence and its actions  to be 
1 e.g. the vehicle builders and repairers association and the 
national association of estate agents. note that  unless otherwise stated  all examples are of uk institutions or laws. open about its decisions  and to employ effective sanctions. 
　the association would need a panel to consider applications for membership and to hear complaints against members. its main sanction would be refusing membership or expelling existing members  backed up by lesser sanctions like a public admonition  payment of compensation  etc. the rules of the association might include a contractually binding committment by members to fulfil any compensation order made by the panel. the association might also insist that contracts issued by members contained various standard clauses  e.g. giving customers the right of reimbursement if returning products within a certain period  guaranteeing compensation under certain circumstances  insisting that precise  testable statements be made about the product  etc.  the panel need only take a passive role in determing disputes; it would publicise its address and its willingness to hear complaints. the burden of making a case complaint would fall on the complainant. the panel might then need to employ a small team of experts to investigate discrepancies between the evidence brought by the complainant and that by the company complained of. this investigative team could be recruited on an ad hoc basis  e.g. academic researchers as consultants. the complainant would usually be the dissatisfied purchaser of one of the companies products  but could be any member of the public with a 
　legitimate interest in the product. there might be a multi-stage process  so that cases were only heard by the full panel when a prima-facie case had been established. this is to filter out malicious complaints and those that are outwith the remit of the panel. it would not be necessary actively to investigate ai products or companies before any complaint had been received  and would probably be prohibitively expensive to do so. 
　the panel needs a code of practice to which members would agree to adhere and which would serve as a basis for applying sanctions. what form should such a code take  i.e. what counts as malpractice in ai  we suspect malpractice may be a lot harder to define in ai than in insurance  architecture or travel agency. 
- due to the state of the art  ai products cannot be perfect. no-one expects 1 accurate diagnosis of all known diseases. on the other hand a program which only works for slight variations of the standard demo is clearly a con. where is the threshold to be drawn and how can it be defined  
- it is unlikely that any current ai product could fulfil a claim to: understand any natural language input  or to make programming redundant  or to allow the user to volunteer any information what-so-ever. however  the claimant could defend the 
　1 c.f. the vehicle builders and repairers association which has standard forms for estimates and recommends clauses in repair contracts. 
	claim 	by 	debating 	the 	meaning 	of 
	'understand'. 	'programming'  	or 
'information'  and this would muddy the water. what constitutes an exaggerated claim  
- given the ambiguity of such terms it would be difficult to decide whether an exaggerated claim was intended to deceive or was due to a difference in terms or was just a genuine oversight on the part of the vendor. how is a vendor's claim to be assessed  should one try to assess the vendor's intention  or should one ignore this and only assess how a reasonable customer might interprete the claim  
- because of the ambiguity of such terms the full description of an ai product must describe its limitations  e.g. what sentences it cannot understand  as well as its abilities. it is not enough to refrain from false claims. 
- a vendor may claim that it cannot accurately describe the limitations of its product without revealing confidential information about the technique and/or software it is based on. the problem is particularly acute for software products because of the lack of protection afforded by patent and copyright law. where do we draw the line between a complete and accurate description of the capabilities of the product and the protection of trade secrets  
　the difficulty is to give a precise definition of what constitutes reasonable behaviour on the part of a vendor. it seems impossible to cover all the possible situations  in advance  with a list of precise standards to be attained  but it is often possible retrospectively to detect unreasonable practice in particular cases. the usual legal solution to such problems is to use a high-level code in combination with judgements about individual cases in order to build up gradually a picture of the reasonable vendor  i.e. to establish case law'  and to use this to evaluate complaints rather than to pre-vet products. note  however  that it takes a long time to build up an extensive range of cases - most of the early judgements must be made solely on the basis of the high-level code. 
　the panel would evaluate a complaint against this code. they would be able to take account of the state of the art and compare the product with the claims made for it. a high-level code and the injunction to judge 'reasonableness' would enable the panel to assess whether the spirit of the code had 
   law is in scare quotas bacause wa ara dafining an axtra-lagal mechanism. 
   compare  for instances  the unfair contract terms act 1. which uaaa caaa law to dafina raaaonablanaaa in ralation to exclusion clauaaa. 
a. bundy and r. clutterbuck 1 
been broken  rather than the letter of some spuriously precise  low-level code. the accumulation of case 'law' would ensure some uniformity of treatment  and prevent favouritism or victimisation. openness about the grounds for decisions would also help ensure uniformity. there is a current trend in other areas towards the giving of reasons - a procedural safeguard which promotes the the quality of decision making.1 difficult decisions  like those outlined above  would be decided in particular cases. rather than in general. the general answers would emerge over time with the accumulation of judgements. previous ajudications would guide future ones without pre-empting them. the case 'law' would provide a guide to vendors as to how to practice reasonably. 
1. a proposed code of practice 
　as a basis for discussion  we propose below such a 
　code of practice for ai vendors. before we give this we must define our terms. in what follows below  the term: 
- at product  means any piece of software or hardware or any service or any combination of these which is based on ai techniques and which is offered for sale; 
- vendor  means a company selling an ai product  either to a customer direct or to a middleman  whether that company made the product or not; 
- customer  means a person or group who buys or attempts to buy an ai product from a vendor; 
- user  means the person who uses the ai product  in particular the person who interacts with the product if it is interactive. 
the proposed code  $: 
　the vendor of an ai product should describe to the customer  and where appropriate the general public  the abilities and limitations of the product as accurately as possible  taking account of the likely expectations of the intended customer. in particular  the vendor should accurately describe  in so far as the state of the art and its own knowledge enables this to be done: 
1. what the product does  including an account of its scope  limitations and 
reliability; 
1. known bugs in the product; 
1. the consequences of failure of the product; 
saa a.g. tha criminal justice act 1. 

1 a. bundy and r. clutterbuck 
1. the amount and type of user interaction required and how and at what cost it is to be obtained; 
1. the skill and knowledge required of the user; 
1. the computational requirements of the product  e.g. hardware and software environment. space and time requirements in different en-
vironments; 
1. the amount and type of maintenance required and the cost of this; 
1. any social  economic or legal implications of the use of the product  
where these can be assessed. 
it is the responsibility of the vendor to see that this code is observed by any agent acting on its behalf  e.g. a salesman. the vendor is also responsible for ensuring that any middlemen which sell its products are fully acquainted with the necessary information to enable them to comply with 
the code. 
an informed customer will  in any case  ask about 
1 above  and a reasonable vendor should supply the information unasked. thus this part of the code merely makes good practice explicit  as it was intended to do. in conjunction with the case 'law'  it should help protect the uninformed customer and define the standards to be met to become a reasonable vendor. 
　point 1 is rather different from the others. it was inserted to try to protect the wider interests of society as well as those of the customers. it might be criticised as being impractical to realise or as not appropriate in this context. however  we feel that something like it is required somewhere  and we would welcome suggestions as to how best to meet this requirement. maybe it needs to be dealt with by separate machinery. 
　it is not our intention that vendors be required to state political or ethical opinions  nor that the association be asked to judge such opinions; it would be beyond their competence to do so. in point 1 we wanted only to encourage vendors to make statements which were within their technical  legal  etc. competence so as to enable others to form accurate political and ethical opinions about the impact of the product. for instance  all the photocopiers at sussex university have a prominent notice above them detailing the law relating to copyright. vendors of ai products should  similarly  draw the attention of users to illegal uses of their product. 
　it is not intended that complainants actually intend to be customers of the vendors they complain of  but they should have a legitimate interest in the product over and above commercial competition. for instance  suppose an expert in the field believes that a vendor is misleading customers about a product or that the vendor is producing a product that will be harmful to society; we would like that expert to be able to bring a case to the association. on the other hand the association would have to be alive to attempts by vendors to undermine their rivals by bringing malicious complaints - and should filter out such complaints at an early stage. 
1. related codes and laws 
　the above proposal is complementary to the existing system of codes and laws applying to ai. 
　for instance  in the uk  the british computer society and  in the usa  the association for computing machinery both provide codes of conduct for their members   bcs 1  acm 1  . we imagine that most other national computing societies have similar codes. neither of these codes apply to ai products or vendors  as such. there is a small area of overlap in that a salesman who is a member of the bcs or acm is required to behave honestly and competently in promoting a product. however  any sanctions for breaking the bcs or acm codes would fall on the individual rather than the company. our code is intended to apply to vendors  which would usually be companies rather than individuals. 
　in the uk  the trade descriptions act uses criminal sanctions to protect customers from false claims  and the sale g＼ goods acts and the unfair contract terms act 1 permit civil remedies for defective goods. other countries have similar laws. however  as illustrated above  there is a large grey area between illegal behaviour and the behaviour of a reasonable vendor. it is the purpose of the above proposal to deal with this grey area  where the vendor has clearly behaved unreasonably  but not in such a way as to constitute a criminal offence or grounds for a civil action. for this reason it 
would not be appropriate for the proposed association to impose the kind sanctions that would be imposed by a court of law. an unreasonable vendor should still be allowed to trade  but should not be allowed to use the cachet of membership of an association of reasonable vendors  with whatever assurance of high-quality that that was generally felt to imply. 
　the british standards institute defines standards for many products. vendors whose products meet these standards are able to advertise that fact with a 'kite mark'. in some cases the standards 
1 
set have been adopted by the law. there is currently an attempt to define a bsi standard for prolog. unfortunately  few ai products lend themselves to such definitions of standards. it is not worthwhile to try to define one unless very similar products are being produced by a number of vendors and there is a wide agreement on a de facto standard. major and stabilised programming languages seem possible candidates  but customised expert systems  and even expert systems shells  do not. 
　e.g. motorcycle crash helmets must be worn and must comply with a minimum bsi standard. 

1. a discussion of problems 
　there is a danger of a few companies annexing the association to themselves and excluding worthy competition. but this is not a major danger. firstly  in the current state of the ai market  ai companies have a lot to gain by encouraging highquality in other ai companies. every success increases the market for everyone  whereas failure decreases it. until the size of the market has been established and the capacity of the companies has risen to meet it  ai companies have more to gain than to lose by mutual support. secondly  excluded companies can always set up a rival association. there is room for more than one association  and they could compete by trying to set the highest standard for membership. however  too many associations would be confusing to consumers and would diminish their influence and effectiveness. 
　there is also a danger of the association developing into a trade protection society  i.e. of maintaining low standards by protecting its members from disgruntled customer by offering weak excuses for faulty products and by not employing effective sanctions. if this happened then customers would lose confidence in the association and membership would cease to carry any assurance of high-quality. membership would still be attractive to vendors who wanted to use the association's excuses to fob off disgruntled customers. however  provided the association did not have a monopoly  there would be nothing to stop a more principled group of vendors forming a rival association as above. 
　the association would always be at risk of legal action against it from disgruntled vendors who might sue for libel. it would need to take care that its pronouncements were fair comment' and to take legal advice on them. it should also try to create the conditions under which vendors would have more to lose from the bad publicity accruing from the court case than they would gam from any damages awarded. but to guard against such actions  the association would have to maintain a legal defence fund contributed by the members. 
1. passive vs active role 
　we have proposed that the association take a passive role  reacting to complaints  rather than an active role  pre-vetting products and/or instigating investigations of products with its own team of investigators. our reasons are practical rather than principled. an active role would require money and people. most ai companies are small and newly set-up; they might be loath to provide the large sums of money required to set up the investigative machinery  but might be prepared to fund a  much cheaper  passive association. ai experts are currently rare and expensive. most such experts want either to conduct their own research or set-up their own companies. it would be hard to recruit good full-time investigators. one might find academics prepared to work part-time  but most candidates would have some existing consultancy arrangements that might disqualify them. we have also argued that  while the field is still immature  it is more difficult to set standards that a prevetted product must meet than to evaluate the 
a. bundy and r. clutterbuck 1 
complaints of a customer against the code of practice and previous cases. 
   for the same reasons we have not proposed the direct registration of ai products. to issue a certificate of good quality to an individual product would require a prior investigation of that product; it would no longer be possible to employ the default assumption that the product of a member of the association was assumed good unless proved otherwise. that is  direct registration of products would require an active association with all the associated expense  employment of rare expertise and setting of prior standards. the passive mechanism proposed above indirectly ensures good quality products by encouraging the vendors to produce products that will not attract complaints. the burden of criticising the product and proving that criticism falls on the complainant. the panel need only investigate differences in the evidence presented to them by the complainant and vendor about the product complained of; this investigation would be relatively cheap compared with that required to register every product. 
　unfortunately  this means that the burden of proof falls on the customer. but this is not as bad as it seems; currently  most ai customers are themselves ai practioners  to some degree. for instance  the customers for expert systems shells and knowledge representation systems are often researchers from the ai laboratories of other companies. hence  they are in a position to investigate the product  and bring a complaint. they only lack a body to bring it to. 
　all this might change: ai companies might get richer and more able to fund an active association. ai experts might get thicker on the ground  standards might get better defined as the field matures  the average ai expertise of customers might decline as the customer base expands. in this case  there is nothing to stop the association moving to a more active role. 
1. relationship to the law 
　we have proposed a extra-legal regulatory mechanism  i.e. one without legally enforceable sanctions. one reason is that we are aiming to regulate in the grey area of legal but unreasonable practice. another reason is that we would like to see an international association covering countries with different legal systems  so no one legal framework can be assumed. a third reason is that we want to make it as easy as possible for the association to get started; an extra-legal mechanism involves the minimum of bureaucratic hassle. 
　this proposal  however  creates problems. the boundary between legal and illegal practice is fuzzy; infringement of some aspects of the code of practice  e.g. points 1  may sometimes give rise to legal remedies. even when legal remedies were possible there may still be situations in which both complainant and vendor would prefer to refer the matter to the association because it provided a cheaper and quicker mechanism for settling the dis1 a. bundy and r. clutterbuck 
pute. however  if cases were referred for legal remedv then the workings of the association might be undermined; such cases would become sub-judice and the association would no longer be able to comment on them until they were decided  which might take a long time . 
　however  as it develops the association might seek statutory authority in the countries in which it operated. this authority might include  for instance: 
- the right to grant licences enabling companies to become vendors of ai products; 
- protection from libel action in its judgements; 
- that infringement of the code of practice would raise a presumption of fault against an infringing vendor in any legal action. 
note that such legal powers increase the dangers of the association becoming a trade protection society. in particular  the right to grant licences gives a monopoly that might be used to exclude competition. therefore  it would need to be offset with rights to the customer and the vendor to prevent abuse  e.g the vendor might have the right of legal action if unfairly excluded from membership  the association might be required to pre-vet and then underwrite the products of its members  so that dissatisfied customers could sue the association. 
1. conclusion 
　in this paper we have proposed a mechanism for peer-policing of standards in ai products. the mechanism consists of an association of vendors of ai products who would use a code of practice to sanction vendors who are guilty of unreasonable practice. it is similar to associations used by other groups of companies and professionals offering services or selling products. it is necessary if ai is not to harm itself and society by the products it produces. some such mechanism is vital to the existing  responsible ai companies if they are to protect their investment in ai; they must try to prevent a few irresponsible companies from exploiting their customers with overrated or useless products and putting those customers off of ai products in general. 
　most of the discussion above is relevant to computing generally. we have limited ourself to ai because of the nature of this conference and in order to help focus our ideas and to identify an area small enough to be tackled. ai is growing fast  
　see  e.g. the statutory disciplinary powers of the law society over solicitors. 
　see. e.g. that an infringement of the highway code raises a presumption  albeit rebuttable  that the driver of a vehicle involved in an accident is at fault. 
but there are not yet many major ai companies  and it seems more likely that they would be prepared to get together than that ibm  icl  honeywell  etc. would be. however  since many ai products will also contain non-ai techniques  it may eventually be necessary to widen the remit of any ai association. 
　the above mechanism is only able to regulate commercial companies. we have argued above that academics already have a self-regulatory mechanism of peer review. however  there is major gap in that neither mechanism covers government organisations  e.g. the military. that is beyond the scope of this paper  but not beyond our desires. 
　this paper is intended as a vehicle for discussion of the problems of maintaining high standards in ai products. it does this by presenting a proposal  but this proposal is not intended as definitive. we would welcome feedback - especially on the code of practice itself. we hope that this paper will inspire those companies that care about high standards to get together  and we hope that they will see that it is in their direct interest to do so. 
