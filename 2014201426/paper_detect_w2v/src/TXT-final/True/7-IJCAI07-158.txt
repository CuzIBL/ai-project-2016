
in this paper  we consider the problem of producing balanced clusterings with respect to a submodular objective function. submodular objective functions occur frequently in many applications  and hence this problem is broadly applicable. we show that the results of patkar and narayanan  can be applied to cases when the submodular function is derived from a bipartite object-feature graph  and moreover  in this case we have an efficient flow based algorithm for finding local improvements. we show the effectiveness of this approach by applying it to the clustering of words in language models.
1	introduction
the clustering of objects/data is a very important problem found in many machine learning applications  often in other guises such as unsupervised learning  vector quantization  dimensionality reduction  image segmentation  etc. the clustering problem can be formalized as follows. given a finite set s  and a criterion function jk defined on all partitions of s into k parts  find a partition of s into k parts {s1 s1 ... sk} so that jk  {s1 s1 ... sk}  is maximized. the number of k-clusters for a size n   k data set is roughly kn/k!  so exhaustive search is not an efficient solution. in   it was shown that a broad class of criteria are submodular  defined below   which allows the application of recently discovered polynomial time algorithms for submodular function minimization to find the optimal clusters. submodularity  a formalization of the notion of diminishing returns  is a powerful way of modeling quality of clusterings that is rich enough to model many important criteria  including graph cuts  mdl  single linkage  etc. traditionally  clustering algorithms have relied on computing a distance function between pairs of objects  and hence are not directly capable of incorporating complicated measures of global quality of clusterings where the quality is not just a decomposable function of individual

모모  part of this work was done while this author was at the university of washington and was supported in part by a microsoft research fellowship.  
모모this work was supported in part by nsf grant iis-1 and an intel corporation grant.distances. submodularity allows us to model these decomposable criteria  but also allows to model more complex criteria. however one problem with all of these criteria is that they can be quite sensitive to outliers. therefore algorithmswhich only optimize these criteria often produce imbalanced partitions in which some parts of the clustering are much smaller than others. we often wish to impose balance constraints  which attempt to tradeoffoptimizing jk with the balance constraints. in this paper we show that the results that patkar and narayanan  derived for graph cuts are broadly applicable to any submodular function  and can lead to efficient implementations for a broad class of functions that are based of bipartite adjacency. we apply this for clustering words in language models.
1	preliminaries and prior work
let v be a ground set. a function 붞 : 1v 뫸 r  defined on all subsets of v is said to be increasing if 붞 a  뫞 붞 b  for all a   b. it is said to be submodular if 붞 a  + 붞 b  뫟 붞 a 뫋 b  + 붞 a 뫌 b   symmetric if 붞 a  = 붞 v   a   and normalized if 붞 뷋  = 1. for any normalized increasing submodular function 붞 : 1v 뫸 r+  the function 붞c : 1v 뫸 r+ defined by 붞c x  = 붞 x +붞 v  x  붞 v   is a symmetric submodular function. this function is called the connectivity function of 붞  and is normalized  symmetric and submodular. we can think of 붞c x  = 붞c v   x  = 붞c x v   x  as the cost of  separating  x from v   x. because 붞c is submodular  there are polynomial time algorithms for finding the non-trivial partition  x v   x  that minimizes 붞c. such normalized symmetric submodular functions arise naturally in many applications. one example is the widely used graph cut criterion.
모here  the set v to be partitioned is the set of vertices of a graph g =  v e . the edges have weights we : e 뫸 r+ which is proportional to the degree of similarity between the ends of the edge. the graph cut criterion seeks to partition the vertices into two parts so as to minimize the sum of the weights of the edges broken by the partition. for any x   v   let 붺 x  = set of edges having at least one endpoint in x
붻 x  = set of edges having exactly one endpoint in x
for example  if x = {1 1}  the red/darkshaded set in figure 1-left   then 붺 x  = { 1   1   1   1   1   1 }  the set of edges

  which can be shown to be a normalized in-figure 1: left: the undirected graph cut criterion: 붞c x  is the sum of weights of edges between x and v   x. right: the bipartite adjacency criterion: 붞c x  is the number of elements of f  features  adjacent to both x and v   x.
which are either dashed/red or solid/black  and 붻 x  is the set of solid/black edges { 1   1   1   1 }. it is easy to verify that for any  positive  weights that we assign to the edges we : e 뫸 r+   the function
 is a normalized
increasing submodular function  and hence the function 붞c x  = we 붻 x   = we 붺 x  +we 붺 v  x   we e 
is a normalized symmetric submodular function1. we will refer to this as the undirected graph cut criterion.
모in this paper  we will be particularly interested in a slightly different function  which also falls into this framework. v will be the left part of a bipartite graph  and f the right part of the graph. we think of v as objects  and f a set of features that the objects may posses. for example  v might be a vocabulary of words  and f could be features of those words  including possibly the context in which the words occur . other examples include diseases and their symptoms  species and subsequences of nucleotides that occur in their genome  and people and their preferences.
모we construct a bipartite graph b =  v f e  with an edge between an object v 뫍 v and a feature f 뫍 f if the object o has the feature f. we assign positive weights wv : v 뫸 r+ and wf : f 뫸 r+. the weight wf f  measures the  importance  of the feature f  while the weight wv  v  is used to determine how balanced the clusterings are. in some applications  such as ours  there is a natural way of assigning weights to v and f  probability of occurrence for example . in this case  for x   v   we can set

where ne 몫  is the graph neighbor function. in other words  if x bi-partitions v into sets of two types  x and v   x  of objects  then 붺 x  is the set of features with neighbors of  type x   and 붻 x  is the set of features with neighbors of both types of object. we let 붞 x  = wf 붺 x   =

creasing submodular function for any positive weight function wf : v 뫸 r+  and 붞c x  = 붞 x +붞 v  x  붞 v   measures the weight of the common features. for the example shown in figure 1-right  if we take x = {1 1}  the red/dark-shaded set   then 붺 x  = {a b d e}  and 붻 x  = {b d e}. we will refer to this as the bipartite adjacency cut criterion.
모because 붞c is symmetric and submodular  we can use queyranne's algorithm  to find the optimal partition in time o |v |1 . there are two problems with this approach. first  since the algorithm scales as |v |1  it becomes impractical when |v | becomes very large. a second problem is that the criterion is quite sensitive to outliers  and therefore tends to produce imbalanced partitions in which one of the parts is substantially smaller than the others. for example  if we have a graph in which one vertex is very weakly connected to the rest of the graph  then the graph cut criterion might produce a partitioning with just this vertex in a partition by itself. for many applications it is quite desirable to produce clusters that are somewhat balanced. there is some inherent tension between the desire for balanced clusters  and the desire to minimize the connectivity between the clusters 붞c x : we would like to minimize the connectivity 붞c x   while making sure that the clustering is balanced. there are two similar criteria that capture this optimization goal
ratiocut normcut
모the two criteria are clearly closely related. unfortunately  minimizing either criterion is np-complete   and so we need to settle for solutions which cannot necessarily be shown to be optimal. the normalized cut is also closely related to spectral clustering methods   and so spectral clustering has been used to approximate normalized cut. in this paper  we present a local search approach to approximating normalized cut. the advantage of local search techniques is that they allow us to utilize partial solutions  such as a preexisting clustering of the objects  which is useful in dynamic situations. further  since local search techniques produce a sequence of solutions  each one better than the last  they serve as anytime algorithms. that is  in time-constrained situations  we can run them only for as much time as available. the local search strategy we employ will allow us to make very strong guarantees about the final solution produced. such a local search technique was originally proposed by patkar and narayanan  for producing balanced partitions for the graph cut criterion. in this paper  we show that the results in his paper are equally applicable for any submodular criterion. it should be noted that the applicability of these general techniques to any submodular function does not necessarily make it practical. one of the contributions of  was to show that it could be done efficiently for the graph cut criterion via a reduction to a flow problem. in this problem  we show that the bipartite adjacency criteria can also be solved in a similar efficient fashion by reducing to a  different  flow problem.
1	local search and the principal partition
in a local search strategy  we generatea sequenceof solutions  each solution obtained from the previousone by a  sometimes small  perturbation. for the case of clustering or partitioning  this amounts to starting with a  bi partition v = v1 뫋 v1  and changing this partition by picking one of a set of moves. this set of moves that we will consider is going from the bipartition {v1 v1} to the bipartition {u1 u1}  where the new partition is obtained by moving some elements from one partition to the other. for example  we could go from {v1 v1} to {v1   x v1 뫋 x}  where x   v1. this amounts to moving the elements in x from v1 to v1. the key to a local search strategy is have a good way of generating the next move  or to pick a x   v1 so that moving x to the other side will improve the objective function . in this section  we show that when 붞c is a submodular function  then the principal partition of the submodular function  to be defined below  can be used to compute the best local move in polynomial time. moreover  for the specific application we discuss in this paper  we can actually compute this fast even for very large data sets.
for any bipartition v = v1 뫋 v1  and x   v1  let ggain x  = 붞c  v1    붞c  v1   x 
averagegain
	wv  	 
 averagegain  x 
so  for figure 1-left  if we assign a weight of 1 to all edges and all vertices   so we 뫖 1 and wv 뫖 1   and let v1 be the set of blue/light nodes and v1 be the set of red/shaded nodes.
then
ggain {1}  = 붞c  v1    붞c  v1   {1}  = 1   1 =  1
in figure 1-right  the bipartite graph   we get
   ggain {1}  = 붞c  v1    붞c  v1   {1}  = 1   1 = 1 ggain x  measures the amount of change in the partition cost 붞c  v1   ignoring the balance constraints . now  we are really interested in the change in normcut v1 v1   which incorporates the balance constraint. 뷃 g v1  can be seen to be related to the ratio/normalized cut  and so we use solutions to 뷃 g v1  to find the set of moves for the local search algorithm. in this section  we present some results which relate changes in normcut v1 v1  to the principal partition of the submodular function 붞c  and for this  뷃 g v1  will play a central role. the principal partition of a submodular function 붞c consists of solutions to minx v1  붞c  x    뷂 몫 wv  x   for all possible values of 뷂 뫟 1. it can be shown  1;
1  that the solutions for every possible value of 뷂 can be computed in polynomial time  in much the same way as the entire regularization path of a svm can be computed  . we will give specifics of the computation procedure in section 1. in this section  we will present results which will relate the solutions of minx v1  붞c  x    뷂 몫 wv  x   with solutions to  averagegain x .
모the following proposition was proven by  narayanan 1  for the graph cut criterion  but generalizes immediately for an arbitrary increasing submodular function 붞c. in particular  this is applicable to the problem we are interested in which the submodularfunction is derived from the bipartite graph.
proposition 1  narayanan 1  proposition 1 . let  v1 v1  is a bipartition of v  so v = v1 뫋v1  and v1 뫌v1 = 뷋   and let be a proper subset of v1 satisfying

then ratiocut v1   u v1 뫋 u    ratiocut v1 v1 
proof. by assumption 

in particular  for x = v1 we have

since 붞c v1   v1  = 1  we have

observe that if   then ab   ad 뫞 ab   bc and so
. therefore  we get
		 1 
dividing both sides by wv  v1   we get
붞c  v1 
ratiocut v1 v1  = wv  v1 wv  v1 
	뫟	붞c  v1   u 

wv  v1   u wv  v1 
  by equation 1 

  because wv  v1 뫋 u    wv  v1  
= ratiocut v1   u v1 뫋 u 

we have a similar  but not strict  result for normalized cuts.
corollary 1. under the same assumptions of the previous proposition  we have normcut v1   u v1 뫋 u  뫞 normcut v1 v1 
proof. since wv  v1    wv  v1  u   and from equation 1  it follows that 붞c  v1    붞c  v1   u . we consider two cases. first  assume that wv  v1  뫞 wv  v1 . in this case  from the normcut definition and equation 1  normcut
hence
붞c  v  	붞c  v  
  because |v1| 뫞 |v1| 
	wv  v1 	  because 붞c  v1   u    붞c  v1  
		  because
hence normcut
= normcut v1   u v1 뫋 u 
for the remaining case  assume that wv  v1  뫟 wv  v1 .
again using equation 1  we have

it follows that normcut
= normcut v1   u v1 뫋 u 

모the two previous results show that if we can find a nontrivial solution to  averagegain x   then we can find a local move that will improvethe normalized cut and the ratio cut. ideally  we want to show that if it is possible to improve the normalized cut  or the ratio cut   we can in fact find a local move that will improve the current solution. unfortunately we do not have such a result  but we have one that is slightly weaker which serves as a partial converse.
proposition 1. suppose that  satisfies 붸1 몫 normcut v1 v1  뫟 normcut v1   u v1 뫋 u  where 붸 =
 wv  v1  wv  v1뫋u . then	.
proof. see appendix.	
모now  the previous result shows the existence of a set which can be moved to the other side which will let us improve the current value of the normalized cut. however  an existence result is not enough. we need to be able to compute this set. the following theorem gives a connection between this set and the principal partition of the bipartite adjacency function. since we can compute the principal partition  we can explicitly compute a local move which will improve the normalized cut.
proposition 1  narayanan  1  proposition 1 . 뷂 = 뷃 g v1  iff there is a proper subset z   v1 such that
min  붞c  x    뷂 몫 wv  x   =  붞c  v1    뷂 몫 wv  v1  
x v1
=  붞c  z    뷂 몫 wv  z  
proof. suppose that 뷂 = 뷃 g v1   then there is a v1 so that

with equality holding for x = w. it follows that
because wv is always positive. hence
붞c  v1    뷂wv  v1  뫞 붞c  v1   x    뷂 몫 wv  v1   x 
because the left hand side is a constant  it follows that
붞c  v1    뷂wv  v1  뫞 min  붞c  v1   x    뷂wv  v1   x  
x v1
note that equality holds for x = w. in particular  for z =
v1   w  we get
 붞c  v1    뷂 몫 wv  v1   = min  붞c  x    뷂 몫 wv  x  
x v1
=  붞c  z    뷂 몫 wv  z  
therefore  by taking z = v1   w  the forward direction follows. for the reverse direction  suppose that for some 뷂   1  we have min  붞c  x    뷂 몫 wv  x   =  붞c  v1    뷂 몫 wv  v1  
x v1
then by taking w = v1   x  we get
 붞c  v1   w    뷂wv  v1   w   뫟  붞c  v1    뷂 몫 wv  v1  
therefore 
뷂 몫 wv  v1    뷂 몫 wv  v1   w  = 뷂 몫 wv  w 
뫟 붞c  v1    붞c  v1   w 
because  we have


모therefore  if we can compute solutions to minx v1  붞c  x    뷂 몫 wv  x    then we can find a local move that will let us improve the normalized cut. in the next section  we show how we can compute these solutions efficiently for our application.
1	computing the principal partition
proposition 1 tells us if we have a partition  v1 v1   then if we can find a set satisfying 뷃 g v1  =
붞c v1  붞c v1 u 
wv  u    then we can improve the current partition by moving u from v1 to the other part of the partition. proposition 1 tells us that we can find such a subset by finding 뷂 so that
min  붞c  x    뷂 몫 wv  x   =  붞c  v1    뷂 몫 wv  v1  
x v1
=  붞c  u    뷂 몫 wv  u  
while this can be done in polynomial time for any submodular function  1; 1   in this section  we show that it can be done especially efficiently in our case by reducing it to a parametric flow problem. for parametric flow problems  we can use the results of   to solve the flow problem for all values of the parameter in the same time required to solve a single flow problem. now  for a fixed parameter 뷂  we can compute minx v1  붞c  x    뷂 몫 wv  x   by solving a max flow problem on the network which is created as follows. add a

figure 1: a flow network to compute the principal partition of the bipartite adjacency cut
source node s  and connect s to all the nodes in v 뫍 v with edge capacity 뷂 몫 wv  v . add a sink node t  and connect all the nodes in f 뫍 f to t  with capacity wf f  as shown in figure 1. the remaining edges  from the original graph  have infinite capacity. by the max-flow/min-cut theorem  every flow corresponds to a cut  and so we just examine the cuts in the network. it is clear that the min cut must be finite  since there is at least one finite cut   and hence the only edges that are part of the cut are the newly added edges  which are adjacent to either s or t . hence if a vertex v 뫍 v is on one side of the cut  all its neighbors must be as well. therefore  every cut value is of the form wv v   x  + wf 붺 x   = 뷂몫wv  v   뷂몫wv  x +wf x . minimizing this function is equivalent to minimizing wf 붺 x   뷂몫wv  x . we can computethis for every value of 뷂 by using the parametricflow algorithm of . in   it is also shown that there are distinct solutions corresponding to at most |v | values of 뷂  and further  the complexity of finding the solutions for all values of 뷂 is the same as the complexity of finding the solution for a single value of 뷂  namely that of a flow computation in this network . this algorithm returns the values of 뷂 corresponding to the distinct solutions along with the solutions. since there are at most |v | distinct solutions  each one of them can be examined to find the one which results in the maximum improvement of the current partition  i.e.  the local move that improves the normalized cut value by the most . since the complexity of the flow computation is o |v |1 |e|   the final search through all the distinct solutions does not add to the complexity  and hence the total time required for computing a local improvement is o |v |1 |e| .
1	word clustering in language models
statistical language models are used in many applications  including speech recognition and machine translation  and are often based on estimating the probabilities of n-grams of words:	
 the prob-
lem is that the number of n-grams grows as |w|n  where w is the set of words in the vocabulary. as this grows exponentially with n  we cannot obtain high-confidence statistical estimates using naive methods  so alternatives are needed in order to learn reliable estimates with only finite size training corpora. brown et al.  suggested clustering words  and then constructing predictive models based only on word classes: if c w  is the class of word w  then we approximate the probability of the word sequence
w	by pr w k	뫘	n 1 pr wi c wi	w
pr wi c wi 1   wi n+1  . in this case  the number of probabilities needing to be estimated grows only as |c|n 1 몫 |w|. factored language models  generalize this further  where we use pr wi|wi 1 c wi 1  ... wi n+1 c wi n+1   - note that conditioning on both wi 1 and c wi 1  is not redundant  as backoff-based smoothing methods are such that if  say  an instance of wi wi 1 was not encountered in training data  an instance of wi c wi 1  might have been encountered via some other word such that  was encountered  and with . often  we can construct such models in a data-dependent way.
모the quality of these models depends crucially on the quality of the clustering. in this section  we construct a bipartite adjacency graph  and use the algorithm described above for generating the clusters. while the algorithm described in this paper only generates a partition with two clusters  we can apply it recursively  in the form of a binary tree  to the generated clusters to generate more clusters  stopping only when the number of elements in a cluster goes below a prescribed value  or if the height of the tree exceeds a pre-specified limit . the bipartite graph we use is constructed as follows: v and f are copies of the words in the language model. we connect a node v 뫍 v to a node f 뫍 f if the word f follows the word v in some sentence. ideally  we want to put words which have the same set of neighbors into one cluster. the model as described ignores the number of occurrences of a bigram pair. however  we can easily account for numbers by replicating each word f 뫍 f to form f1 f1 ... fk  where a word v 뫍 v is connected to f1 f1 ... fr if the bigram vf occurs r times in the text. it is very simple to modify the network-flow algorithm to solve networks of this type in the same complexity as the original network. the goal is to partition the words into clusters so that words from different clusters share as few neighbors as possible  and words from the same cluster share as many neighbors as possible . observe that this criterion does not require us to compute  distances  between words as is done in the clustering method proposed by brown et al. . the advantage of our scheme over a distance based approach is that it more naturally captures transitive relationships.
manually generatedbipartite adjacencybrown et al.   bigram  minimum 111bigram  average 111bigram  maximum 111trigram  minimum 111trigram  average 111trigram  maximum 111table 1: comparing the perplexity of bigram and trigram models for various clustering schemes. the first column  manually generated  uses manually labeled part-of-speech tags  and is used as an idealized baseline only.모to test this procedure  we generated a clustering with 1 clusters on wall street journal  wsj  data from the penn treebank 1 tagged  1  wsj collection. word and  human generated  part-of-speech  pos  tag information was extracted from the treebank. the sentence order was randomized to produce 1-fold cross validation results using  1 / 1  training/testing sizes. we compared our submodular clustering with both the manual pos clusters  and also the clustering procedure described in brown et al.   as shown in table 1. we note that this particular bipartite model is designed specifically for bigram n = 1 models  and not surprisingly  we get a significant improvementin perplexity for such models. we find a non-significant improvement in the trigram case  but the non-significance is expected as it shows the importance of a correct model - it would be straight-forward  however  when clustering wt 1 to use a different bipartite graph  where f contains not only wt but also wt 1  to cluster wt 1 as a predictor for wt relative to the context in which it will be used in the trigram. in this fashion  a separate clustering could also be done for wt 1. this shows the generality of our technique.
