 
a macro-operator is an integrated operator consisting of plural primitive operators and enables a problem solver to solve more efficiently. however  if a learning system generates and saves all macro-operators extracted from worked examples  they will increase explosively and eventually its problem solving will be less efficient than even a non-learning system. thus  it is very important for macro-operator learning to select only the effective macro-operators. to cope with this 
problem  we propose a new method to select macrooperators by perfect causality  a new heuristic  and generalization of them with ebg. both in classical robot planning and solving algebraic equations  we made the experiments using a selective macro-learning system with perfect causality  a non-selectively macrolearning system and a non-learning system. the experimental results verify much higher efficiency of the selective learning system than the other two systems over a lot of various problems. finally  we discuss perfect causality as an operationality criterion in ebl perspective. 
1. introduction 
     in general  a problem solver cannot solve problems efficiently  and various methods for solving more efficiently have been proposed. one of the significant methods is a macro-operator learning. a macro-operator is an integrated operator consisting of plural primitive operators. since the macro-operators can reduce a search space  a problem-solver can solve more efficiently with them. macrops in strips  fikes et al.  1  are the first learned macro-operators and the experiments were made in a classical robot planning. however  strips saves all of many macro-operators generated from worked examples  the processes of solving in past. the saved macro-operators hence explosively increase and the 

cost of searching for applicable ones will eventually make the problem solver less efficient than a non-learning one  minton  1 . since most candidates for macro-operators are actually useless  it is very important for the macro-operator learning to select only effective macro-operators from them. some methods to select macro-operators have been proposed  minton  
1  iba  1 . we propose a new method with perfect causality ; a new heuristic to select only useful macrooperators. we built the frame work system; pil1 which selectively extracts macro-operators with perfect causality and generalizes them with ebg method  mitchell et al.  1 . both in a classical robot planning and a solving various equations  we made the experiments using the pil1  a non-selectively macrolearning system and a non-learning system. as a result  we found pil1 could keep the more efficiency than other two systems over a lot of various problems both in two domains. 
     in this paper  we first explain pil1 system. next  we show the definition of perfect causality  the algorithm for extracting macro-operators and the generalization of them with ebg. 
finally  the experimental results are shown. 
1. pil1 ; a frame work for the selectively macro-operator learning 

	yamada and tsuji 	1 

therefore  if macro-operators are generated from all subsequences  their amount will increases explosively and the cost for searching applicable macro-operators makes the problem solving inefficient. s.minton has reported that strips which learns macrops non-selectively becomes less efficient than 

1 	machine learning 

non-learning strips even in a small number of problems  minton  1 . since the sub-operator sequences include many useless ones  selecting only the effective ones enables a learning system to keep efficiency. we propose a method to select only useful macro-operators with perfect causality  a 
new heuristic. we assume that the macro-operators are generated from only the sub-sequences which satisfy perfect causality. the definition of perfect causality is as follows. 
     let a worked example be ops= op1...1pn  and an initial problem state be is. if an arbitrary operator:opm  m¡Ùi  in pcops= opi...opj    l   i   j   n   satisfies the following two preconditions  then pcops satisfies perfect causality. 
1  opm is not applicable to is. 
1  after  opi...opm-l  were applied to is  opm is applicable to the problem state. 
these preconditions mean that the applications of  opi 
...opm-1  guarantee the application of opm which cannot be applied to an initial problem state. from every sub-operator sequence: opk...opn  l k n   the longest operator sequences which includes opk and satisfies perfect causality are extracted for macro-operators. the algorithm for extracting macro-operators with perfect causality is shown in fig.1. 
     we explain how the algorithm concretely extracts the operator sequences from the worked example in fig.1  which is cited from fikes' paper  fikes et al.  1 . the applied basic operators in fig.1 are shown in fig.1. let ops be  opl op1  op1p1 . first   opl op1  is substituted for iop. because all wffs in the cond-lists of opi and op1 are satisfied in is. next  opi is applied to is without checking its cond-list. then f l 1 is added and result= fo~f1   mop= opl  are determined. since next op1 is not included in iop and its cond-list  
 fl f1 fll fl1 fl1   is satisfied in result  op1 is applied and result is updated. then mop is also updated to  opl op1 . op1 is included in iop and op1 is not applicable to result. 
thus  this cycle with i=l is finished and mop =lopl op1   ¡Ù  opi   is extracted for a macro-operator. 
     then the next cycle with i=1 begins. mop= op1  is determined and op1 is applied to is without checking its 

cond-list. since f l 1 in the delete-list is not included in is  f l 1 is not removed. the fl1 and fl1 are added to is and result is updated to tfl~fl1 fl1 fl1 . next  op1 included in iop is skipped and the applicability of op1 is investigated. the f1 in the cond-list of op1 is not in the problem state because of non-application of op1. thus  op1 is not applied and this cycle finishes as mop= op1 . this mop= op1  cannot satisfy the precondition: mop¡Ù opi   thus this cycle dose not yield any macro-operator. 
finally  the output: mop=  opl:gotob op1:pushb   op1 
:gotod op1:gothrudr   is obtained. as seeing from this result  perfect causality can extract the operator sequences which are executive independently. fig.1 shows all suboperator sequences from the worked example in fig.1. the m1 m1 are not executive in any problem state and m1 m1 are nonsense. therefore  most these candidates are useless and only useful 

	yamada and tsuji 	1 

m1 m1 are extracted with perfect causality. futhermore  this algorithm can extract even discontinuous operator sequences for macro-operators and is available to the worked examples including macro-operators. 
1. generalizing macro-operators with ebg and integrating them 
     the extracted operator sequences are instances  therefore pil1 generalizes them with ebg method  mitchell et al.  1 . 
the extracted operator sequence corresponds to an explanation tree in ebg. fig.1 a  shows the explanation tree constructed with the operator sequence   gotod  gothrudr  obtained in a last section. in fig.1 a   the black circles stand for the basic operators and the nodes over  right and under them indicate wffs in a add-list  a delete-list and a cond-list  respectively. 
     in general  ebg method needs four inputs: goal concept  training example  domain theory and operationality criterion. the underlined leaf nodes in fig.1 a  arc training examples for learning the precondition of the macro-operator and the two operators correspond to the domain theory. a goal concept is the precondition of the macro-operator. however  what corresponds to an operationality criterion  this problem is discussed in section 1. the generalized explanation tree with ebg method |mitchell et al.  1  is shown in fig.1 b   where the upper-case letters indicate variables. 
     next  we explain how to integrate the explanation tree into a macro-operator. fig.1 shows an operator sequence consisting of two operators  opl op1. the cn dn an and men indicate a cond-list  a delete-list  a add-list and a maineffect-list  respectively. every list is a set of wffs. therefore  a macro-operator is generated by the recursive applications of the set operations in the following. 
  
     the mc md ma and mme indicate a cons-list  a deletelist  a add-list and a main-effect-list of the generated macrooperator  respectively. fig.1 shows the macro-operator generated from fig.1 b . in pil1  all operators including macrooperators are represented in the same structure. 
1. the experiment in a classical robot planning 
     in a domain of a classical robot planning  we made the experiment using the three systems ;  a  a non-learning system  strips  b a non-selective macro-learning system  m-strips   c  a selective macro-learning system  pil1. the strips is the problem solver of pil1 and the m-strips generates macrooperators from all sub-operator sequences of worked examples. note that these three systems use the same problem solver and the only difference between m-strips and pil1 is in the 


1 	machine learning 

numbers of the macro-operators saved. 
     we gave each of them 1 basic operators and 1 problems as input. the basic operators include ones in fig.1 and are the same to ones in  fikes et al.  1 . we did not select the operators and the problems for pil1's good performance. fig.1 shows samples of given problems and the number of steps for solving the most difficult problem is 1. the experimental results for five problems sampled from a series of 1 are shown in tablel. the cpu time does not include the time taken to generate macro-operators  only the time necessary to find a solution. typically  the learning time is considerably less than the search time. as seeing from this table  m-str1ps has the result only for p1  because a stack-overflow occurred when m-strips was generating the macro-operators after pi1 was solved and we could not continue the experiment. m-str1ps generated 1 macro-operators on p1-p1 and it took 1sec cpu time to search all of them. therefore  if m-strips could continue to solve the problems after pi1  the cpu time would be more than 1sec and this time is much longer than pil1's one. 
for p1  the efficiency of m-strips is already worst. 
though the branches evaluated of m-strips are less than 
strips's ones  m-strips spends longer cpu time than strips. because the time for searching the relevant macrooperators is  in general  longer than that of basic operators. note that m-strips has already generated 1 macro-operators. comparing with other two systems  pil1's branches evaluated  generated macro-operators and cpu time are very small. therefore  pil1 is most efficient. 
     for p1-p1  tablel shows the results for only strips and pil1. note that pil1 has only five macro-operators even for p1. these five operators are shown in fig. 1. if pil1 learned non-selectively  the number of macro-operators would be more than 1. we can find that perfect causality extremely prevented pil1 from generating a lot of redundant 

macro-operators. for p1-p1  pil1 is constantly more efficient than strips. 
     table1 shows the averages of cpu time. as is evident from it  pil1 could solve far more efficiently than strips and m-strips both over pi~p1 and p1-p1. this means that 
pil1 can learn only useful macro-operators over many various problems in a classical robot planning. 
1. the experiment in solving various equations 
     furthermore  we made the experiment in another domain  solving algebraic equations. in this experiment  we used pil system instead of pil1. the difference between pil and pil1 is only in their problem solvers. the pil's problem solver uses the forward breadth-first search without any heuristic and its problem states are represented in list structures. in pil  as well as pil1  the macro-operators are selectively learned with perfect causality and generalized by an ebg method. 
     fig. 1 shows a part of basic operators given to pil. in this figure  a b c indicate arbitrary formulas. rn  a l   na and nrn stand for an arbitrary real number  a variable  an arbitrary formula but zero and an arbitrary real number but zero  respectively. the rlooo is a operator for checking the solution state  1*al=r. when this operator is applied  a problem solving finishes. we gave a set of training problems consisting of 1 equations of the first degree  1 equations of the second degree  1 fractional equations  1 logarithmic equations and 1 exponential equations. only when pil could not solve them by itself  we gave the worked examples and pil learned macro-operators from them. 
     as a result  1 macro-operators were generated for the equations of the first degree  1 ones for the equations of the second degree  1 ones for the fractional equations  1 ones for the logarithmic equations and 1 ones for the exponential equations. all the macro-operators for the equations of the first degree are shown in fig. 1. the number in the bracket indicates the number of basic operators in each macrooperator. the macro-operators marked with  ds  can directly solve the problems. although pil could not solve any given problem before learning  it was able to solve all of them after 

	yamada and tsuji 	1 

learning. solving the most difficult problem needs more than 1 applications of basic operators. if pil generated all macrooperators  they would increase explosively. therefore  it is evident that even in solving various equations  our selective macro-operator learner can solve many problems more efficiently than a non-selective learning system and a non-learning system. 
1. related works 
strips  fikes et a!.  1| saves all the sub-sequences from 
worked examples as macrops. however  our macrolearning method can selectively generate only the useful macro-operators from many candidates. our system can thereby solve more efficiently than strips. this is evident from the experimental results in a classical robot planning. 
     both minton's  minton  1 and iba's methods  iba   1  for selecting macro-operators depend on the heuristic evaluation function for the problem solving. however  our method can select only useful macro-operators independently from the evaluation function. furthermore  the generating minton's s-macro |minton  1   common sequences in worked examples  needs a lot of worked examples. our method learns macro-operator from only a single worked example. 
     k o r f s method is powerful to generate macro-operators in the domain that exhibits operator decomposability  korf  1 . however  our method's cost for generating the macrooperators is considered less than korf's one. furthermore  korf's definition; a macro-operator achieves one of the subgoals of the problem without disturbing any subgoals that have been previously achieved  is considered more restricted than ours. 
     the soar's generalization method of macro-operators is implicit and may leads the over/under generalization  laird  
1   our macro-operators are explicitly generalized by an ebg method and the under/over generalization never occurs. 
1. perfect causality as an operationality criterion in ebl 
     we discuss perfect causality in the ebl perspective. our macro-learning method is considered one of ebl frame works. therefore  we think our method has inputs corresponding to the four inputs of ebl. as already mentioned in section 1  our method has ebl's three inputs but an operationality criterion. what is an operationality criterion in macro-operator learning   
     the definition of operationality commonly cited in describing ebl system is the following; a concept description is operational if it can be used efficiently to recognize instances of the concept it denotes  keller  1 . the cond-lists of macro-operators  which are the concept descriptions in macro-learning  consist of wffs in the cond-list of basicoperators. since a problem solver can easily recognize the wffs in basic operators  instances of the concept descriptions can be recognize efficiently. thus  according to the definition of operationality mentioned before  all concept descriptions in macro-learning are operational. is there no operationality criterion in macro-learning   
     keller's research for an operationality gives the answer to this problem. he redefined an operationality more precisely  keller  1 . his definition is the following; the concept 
1 1 	machine learning 
description is considered operational if it satisfies the following two requirements: 1. usability: the description must be usable by the performance system  1. utility: when the description is used by the performance system  the system's performance must improve in accordance with the specified objectives. the concept descriptions in macro-learning satisfy the first requirement and not the second one. because most macro-operators can not actually make a performance system more efficient. perfect causality can select the effective macro-operators  whose cond-lists are the concept descriptions satisfying the second requirement. thus  we consider perfect causality is an operationality criterion in macro-learning. 
1. conclusion 
     we proposed the method to selectively learn only useful macro-operators with perfect causality  a new heurictic  and to generalize them with an ebg method. the capability of our method was tested both in a classical robot planning and solving equations. 
     we verified the utility of perfect causality in two different domains. however  we do not know the utility in other domains and the analytical evaluation for the limitation of our method is necessary. 
acknowledgements 
     many thanks to dr. norihiro abe for his useful discussions about our research and to tomohiro ishikawa for providing very valuable comments on an earlier draft of this paper. thanks also to all researchers of fai group in tusji lab for helpful comments. finally  i would like to thank the reviewers of aaai-1 for inspiring us to this research. 
