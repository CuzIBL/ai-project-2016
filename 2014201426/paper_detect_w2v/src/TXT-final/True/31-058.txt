 
research in machine learning has typically addressed the problem of how and when to learn  and ignored the problem of formulating learning tasks in the first place. this paper addresses this issue in the context of the 
castle system 1 that dynamically formulates learning tasks for a given situation. our approach utilizes an explicit model of the decision-making process to pinpoint which system component should be improved. castle can then focus the learning process on the issues involved in improving the performance of the particular component. 
1. determining what to learn 
a theory of learning must ultimately address three issues: when to learn  what to learn  and how to learn. the overwhelming majority of research in machine learning has been concerned exclusively with the last of these questions  how to learn. this ranges from work in purely inductive category formation to more knowledge-based approaches. the aim of this work has generally been to develop and explore algorithms for generalizing or specializing category definitions. the nature of the categories being defined-i.e.  what is being learned-is rarely a consideration in the development of these algorithms. for purely inductive approaches  this is entirely a matter of the empirical data that serves as input to the learner. in explanation-based approaches  ebl   it is a matter of the user-defined  goal concept -in other words  input of another sort. in neither case is the formulation of the learning task itself taken to be within the purview of the model under development. 
　some work-in particular that in which learning has been addressed within the context of performing a task-has addressed the first question above  namely  when to learn. a common approach to this issue  known as failure-driven learning  is based on the idea that a system should learn in response to performance failures. the direct connection this 
　the research presented here was carried out at the institute for the learning sciences at northwestern university  and is discussed in detail in the first author's ph.d. thesis  krulwich  1 . 'castle stands for concocting abstract strategies through learning from expectation-failures. 
larry birnbaum 	gregg collins 
the institute for the learning sciences 
northwestern university 
1 maple ave  evanston  il 
{birnbaum  collins} ils.nwu.edu 
establishes between learning and task performance has made this approach among the most widespread in learning to plan 
 e.g.  sussman  1; schank  1; minton  1; hammond  1 . for the most part  however  even these models do not address the second question above  what to learn. in many cases  this is because the models are only capable of learning one type of lesson. what to learn is thus predetermined. 
　for example  many systems that learn exclusively from planner success always learn the same thing  namely a generalized form of the plan that was created  e.g.   mitchell  1  . similarly  many systems that learn from plan outcomes always learn the same type of planning knowledge-e.g.  when it is feasible to make certain simplifying assumptions or to defer planning-from each situation  e.g.   chien  1; dejong el al  1  . even systems that can learn more than one thing generally do so in a predetermined and inflexible fashion  e.g.   hammond  1  . 
　while this type of solution can often be effective for any individual application setting  it fails to provide an account of how a learning system could determine for itself what to learn  and do so in a manner that is flexible enough to take account of the internal and external context of learning. for a system that is capable of learning a wide variety of types of concepts  in a wide variety of settings  the number of hardwired mapping rules required to do this would be very large  and the rules themselves would get very difficult to manage or reason about  and may even be impossible to formulate. 
　more importantly  however  is the fact that hard-wired rules of this type do not provide a theory of determining what to learn. just as a set of rules for actions can result in intelligent behavior without providing a foundation for the actions  hard-wired rules for determining what to learn can be effective but nonetheless do not necessarily provide a theory underlying these decisions. the point is that just as complex decisions about actions are very difficult to formulate using hard-wired rules  and thus require inference  so too complex decisions about what to learn require inference. 
1. an everyday example 
consider the case of a person cooking rice pilaf for the first time. the last step in the directions says to  cover the pot and cook for 1 minutes.  suppose the person starts the rice cooking and then goes off to do something else-say  
	krulwich  birnbaum  and collins 	1 
clean up the house. in the interim  the pot boils over. when the person returns to the kitchen a half-hour later  the rice pilaf is ruined. 
what should be learned from this sequence of events  
intuitively we can imagine a number of lessons that might be learned: 
  when a covered pot containing liquid is on the stove  keep an ear peeled for the sound of the lid bouncing or the sound of the water bubbling. 
  do not put a covered pot with liquid in it over a high flame  because it will boil over. the flame should be turned down. 
  when cooking over a high flame  leave the pot uncovered or the lid ajar. 
don't do loud things while cooking on the stove. 
  when cooking liquid in a covered pot  stay in the kitchen. 
  don't cook over high flame when busy. 
while all of these lessons are sensible  they are very disparate  in that they address very different issues. the lessons concern different aspects of behavior  refer to different portions of the agent's plan  and are expressed in different vocabularies. it is difficult to imagine how any learning process that did not distinguish among these alternatives in some way would be capable of such diverse behavior. rather  it seems more likely that before the agent can undertake the task of learning from the mistake  he must select a lesson  or set of lessons  to learn. in other words  given that the agent has decided to learn from the mistake  and given that he is capable of carrying out the learning task  he still has to first determine what to learn. 
　we see  then  that the agent could learn several things in response to the rice pilaf boiling over. which of the lessons the agent should learn  whether changes to the cooking methods  the idea of staying in the kitchen  or of tuning his perceptual apparatus  depend on the agent's perceptual and planning abilities  and on his knowledge of the domain. the key point is that many different lessons are possible. any approach to determining what to learn must be flexible enough to account for this diversity. 
1. modeling cognitive tasks 
what would an appropriate theory of determining what to learn look like  imagine the thought processes going on in the agent's head  consciously or subconsciously  in viewing the situation and considering what lesson to learn: 
question 1: why was the rice ruined  answer: the rice boiled over. 
question 1: without doing this  could i have prevented it from boiling over  
answer: yes  if i had heard it. 
question 1: could i have heard it boiling over  
answer: maybe i could have  if i'd paid more attention. 
question 1: why couldn't i hear it boiling over  
answer: 1 was using the vacuum in the living room. question 1: could i have planned things differently to enable me to hear  
answer: yes  i could have delayed vacuuming or stopped every few minutes to check the rice. 
question 1: why didn't 1  
answer: i didn't think about the inability to hear from the other room. 
the focus of this dialogue is on the decisions and actions of the agent that led to the rice burning  and particularly on what the agent could have considered or done to prevent the problem from arising. a self-dialogue of this sort is a means of analyzing the situation to explain what happened  and thereby focus learning from the experience  chi et. al  1; ram  1; oehlmann et. al  1 . in this case the agent is performing self-diagnosis  trying to determine what mistake s  he made that led to the rice burning. put another way  the agent is considering possible lessons  and trying to determine which of them to learn. it is important to notice at this point that the dialogue is not specifically aimed at diagnosing the actions that the agent took to determine which action is to blame  although that may be involved as well . rather  the dialogue is diagnosing the decisions that the agent made  birnbaum et. al  1; collins et al  1 . 
　the key insight that will allow us to model this learning process is that each of the lessons that our agent can learn  and each of the questions in the hypothetical dialogue above  relates to a particular cognitive task that the agent was carrying out in the example: 
  listen harder for bubbling: perceptual tuning 
  adjust the flame more carefully: plan step elaboration 
  leave the lid ajar: plan step elaboration 
  don't do loud things while cooking: scheduling interleaving 
  stay in the kitchen while cooking: scheduling/ interleaving 
  don't cook over high flame when busy: scheduling/ interleaving 
question 1: could i have done something differently at the the flame on the stove  and perceptual tuning  e.g.  deciding time i started the rice cooking to prevent the problem   what to listen for  in this case the sounds of the rice bubbling answer: yes  i could have lowered the flame or uncovered over . these tasks are themselves general cognitive abilities the pot. that are used frequently in goal-based behavior. 
1 	cognitive modelling by  cognitive tasks  we mean here the classes of decisions that the agent makes in the course of decision-making. in our example  these cognitive tasks include such things as plan step elaboration  e.g.  such as deciding how high to adjust given this insight  we can reformulate the learning problem posed above. determining lessons to learn from a problem means first determining which cognitive tasks are relevant  and then determining what can be learned from the experience about how those particular tasks can be better carried out. in other words  we have transformed the problem of determining what to learn to two subproblems: determining what task to repair  and determining what aspect of the situation relates to that task  krulwich  1  1 . 
1. modeling planner structure 
how can a computer system reason about its own decisionmaking and the cognitive tasks involved in that decisionmaking  the approach we will take is to design the system to facilitate this reasoning  by structuring its architecture in terms of components that carry out specific cognitive tasks  collins et. al  1. in other words  we partition the system into chunks  each of which is responsible for a particular cognitive task  and treat each chunk as a 
component of the architecture. the behavior of the system  and the opportunities to improve it  can then be analyzed in terms of the behavior of components and the interactions between them.1 
　the next step  which answers the question of what to learn  is to associate with each component information about its ideal  desired  behavior. the system can use this information to determine what can be learned from the situation about better carrying out the component's task. the perceptual tuning component  for example  will have an associated description of the task of adjusting the agent's perceptual apparatus in response to its goals and environment. in the rice pilaf example the system could realize  based on this information  that it can learn a lesson about attending to the rice while cleaning the living room. if this kind of information is provided for each component  the system can determine what to learn by retrieving the relevant information for each component potentially in need of repair. 
　given this component-based approach to modeling the planning process  how can a computer system diagnose which component is responsible for a failure  the process of self-diagnosis  as discussed above  is aimed not at diagnosing the agent's actions  at least not directly   but rather at diagnosing the decision-making constructs that gave rise to these actions. in other words  the over-arching question is not  what action of mine led to the problem   but is rather  what deficiency in the way i make decisions led to the problem   in contrast  previous research in learning to plan or solve problems in response to failures has generally been aimed at the first question  and has thus employed knowledge of the causal relations between the steps in the plan and the desired outcomes of those steps in diagnosis. when a plan 
1 this approach is often taken in reasoning about physical devices  davis  1  sec. 1 . 
failure occurs  this information is used to see what step in the plan resulted in the failure. 
　to diagnose failures in terms of faulty planner components  however  an agent requires analogous information concerning the causal relations between the decision-making processes used in planning  the actions taken  and the expected results. in other words  diagnosing the failure of a plan  or  more generally  of an expectation about the plan's performance  in terms of decision-making constructs requires information about the causal relations between the two. more concretely  the agent must reason explicitly about his justification for his actions in terms of his own reasoning mechanisms  birnbaum et. al  1 . we consider our agent to know  or to be able to reconstruct  the reasons that he thought his decision-making was sound  and how his decision-making mechanisms led to the failure. introspective dialogues such as the one we saw above correspond to the agent's examination of this justification  and his consideration of where the faults lie. 
1. flexible learning in the castle system 
we have seen that an intelligent agent must be able to dynamically determine what to learn  and that this process can require a significant amount of inference. by viewing the planning process as being composed of a variety of cognitive tasks  we transformed the problem of determining what to learn into two sub-problems: determining what cognitive task is at fault  and determining what could be learned to improve that task. what we still need to specify is how this process is initiated in the first place. our approach is for the system to maintain and monitor explicit expectations that reflect the assumptions made during planning  schank  1; doyle et. al  1; ortony and partridge  1 . these expectations carry with them justification structures that relate them to the decision-making processes and otherwise-implicit assumptions that underly their being expected in the first place  birnbaum et. al  1 . the failure of an expectation thus can directly lead to diagnosis of the relevant portions of the system's planning architecture. 
this leads us to a fairly straightforward learning process. 
the planner considers the current situation and the active goals  and outputs a plan  along with a set of expectations to monitor. the failure of one of these expectations leads to diagnosis  which uses associated justification structures that represent part of the system's explicit self-model. the diagnosis process concludes which component is at fault  and how it should in fact have behaved. this information is passed to the repair module  which uses a model of the component's cognitive task to construct a repair. 
　we have implemented this approach in a system called castle  krulwich  1  1   castle operates in the domain of chess  and learns new rules for a variety of cognitive tasks. 
　consider the example shown in figure 1. in board  a  the opponent  playing white  chooses to move the queen to the 
	krulwich  birnbaum  and collins 	1 

right  to a square from which it can subsequently be moved to pin the computer's rook against the king. the computer  playing black  doesn't detect any strategies  offensive or option-limiting  that the opponent can execute  so it goes ahead with its own plan to capture the opponent's pawn. in board  c  the opponent moves its queen to pin the computer's rook  and the computer finds itself in board  d  with its rook pinned and a pawn  or the queen itself  able to make the capture in two moves. 
　the expectation that failed in this example relates to an assumption that is implicitly made by all intentional systems  that in general there will exist reasonable options-plans- that can be carried out for any goal that arises. this assumption underlies castle's belief that it will be able to achieve its goals over the course of the game. one instantiation of this assumption  which castle can directly monitor  is that the natural prerequisites to carrying out the plan  namely the ability to move pieces  will in general be met. in other words  the system's pieces will have mobility. since the opponent presumably would like to limit the system's options  castle uses its option-limiting planning rules during the plan recognition phase to check whether the opponent has the ability to limit castle's options. if so  castle will try to counterplan. if not  castle will assume that its pieces will remain mobile until the subsequent turn. this process serves two purposes. first  any plans of the opponent's to limit castle's options will hopefully be anticipated and countered. second  if castle fails to detect an opportunity for the opponent to limit the system's options  and the opponent takes advantage of the opportunity  castle could learn a new option limiting planning rule in response to the failure. this is exactly what happens in the pin example. 
　this expectation failure invokes castle's diagnosis and repair mechanisms. the diagnosis engine traverses the justification for the expectation  shown in figure 1  and determines that there must have been a plan executed by the opponent to limit the computer's options that was not generated by a method in the option-limit planning component. this fault description is then passed to castle's repair module. 
　once castle has isolated the fault as a lack of an optionlimiting planning rule  the repair mechanism takes over and first generates an explanation of why the opponent's move constituted an option-limited plan. to do this  the system retrieves a component specification that represents the purpose of the component being repaired. an explicit model of planning and execution is used to construct an explanation of how the component should have behaved in the example. 
　the component specification for the option-limiting planning component says roughly that an option-limiting plan is a single move that disables more than two opponent moves by the same piece for the same reason. this specification is used to generate the explanation shown in figure 1. the explanatory model specifies that a precondition for a move is that there is no opponent piece with a possible king threat through the square being vacated. the effect of the queen move  that the queen is now at its new location  conflicts with this precondition  because the queen in its new location does in fact have a possible king threat through the rook's location. this conflict explains why the queen's move disables six previously possible rook moves. 
　the learned rule for the pin as an option-limiting plan is shown in figure 1. the rule says roughly: to compute a plan to limit the opponent's options  determine a piece to move  and a location to move to  and an opponent piece  such that the moved piece can attack the king with a move blocked only by the opponent piece. this rule correctly predicts the opponent's ability to pin the computer's rook in the situation in figure 1 a   and can also be used offensively by castle's planner to devise plans to limit the opponent's options. 


1 	cognitive modelling 

1. related work 
how do other programs determine what to learn  in most cases there is only one type of concept to learn  so the determination is made in a fixed way by the program's control structure. cbg  minton  1   for instance  learns a forced-move sequence schema whenever it loses a game. theo-agent  mitchell  1  similarly learns a stimulusresponse rule whenever its planner produces a new plan. 
　chef  hammond  1  uses a fixed scheme of this sort to learn more than one type of construct. whenever plan transformation is complete  chef stores the resulting plan back into its case library. whenever a bug is found during simulation that was not anticipated earlier  the system learns a bug anticipator rule. finally  whenever a bug is repaired  the system learns a generalized bug repair rule. the first thing to note is that the three types of things that chef can learn correspond to three components of a case-based planner  namely case retrieval  indexing  and adaptation  respectively . chef's relatively simple approach to the task of determining which of these three things to learn is highly effective for a number of reasons. first  there are only three types of things to learn  and they are highly distinctive. there is not much chance of confusing which category is applicable  as there might be if more subtle distinctions  e.g.  between different types of indices relevant to different aspects of the planning process  were considered. second  chef's plan simulator returns a complete causal analysis of the bugs that arise  so there is no need for complex inference to determine which of its components needs to be repaired  as there might be if chef were to learn in response to problems that arose later in time during plan execution. if either of these two conditions did not hold  a more complex approach would probably be needed. 
　soar  e.g.  newell  1   uses a similar scheme to learn chunks  production rules  after resolving impasses. in soar's case  however  the chunks fit a variety of purposes because they are learned in response to impasses in different stages of decision-making. soar's impasses thus serve the same purpose as castle's expectation failures. in the absence of explicit reasoning about what to learn  soar relies on its knowledge representations to direct the learning of new rules. in other words  decisions about when and what 
	krulwich  b1rnbaum  and collins 	1 

to learn are tied directly to the representation of problems and sub-problems  and thus to the expressiveness of the system's vocabulary. the lack of explicit control over learning often leads to the creation of undesirable chunks  and the only solution within the soar philosophy has been to limit the expressiveness of the system's vocabulary  tambe and rosenbloom  1 . in contrast  castle's approach is to reason explicitly about what rules should be learned  in a sense taking the opposite approach of extending the system's vocabulary. it may be possible to extend soar to carry out explicit reasoning of this sort as another heuristic search problem  but this has not been investigated.1 
　prodigy  minton  1  is probably the closest in spirit to castle  in that it performs dynamic inference to determine what to learn in a given situation. the system has a set of example recognizers that look for conditions indicating that a node in a problem-solving trace is an instance of a particular type of concept. for example  if a node refers to a choice of an operator to apply  and the choice ended up failing  an example recognizer will signal the node as an example of an operator that should not have been chosen. this node  along with the concept it is believed to be an instance of  is passed to an ebl mechanism which constructs a new search control rule. 
　this process appears to satisfy the criteria discussed above for analytical approaches to determining what to learn. moreover  this  pattern-matching  approach to determining what to learn seems quite appealing  because it appears to require fairly little inference  certainly less than castle's diagnostic approach   and because it works using clear declarative knowledge about each of its types of rules. 
　in practice  however  assessment of prodigy's approach is more complicated  in part because the system additionally requires the use of procedurally encoded example selection heuristics to  eliminate uninteresting examples   minton  1  sec. 1 . while the rationale to these heuristics sounds innocuous  they in fact embody quite sophisticated reasoning. some of these rules select interesting examples for particular rule types  one per rule type   others are used to search for specific conditions that make learning a particular type of rule beneficial  and others are used for determining  interestingness of a number of rule types. all of these are separate from the initial example recognition process. additionally  these functions maintain a history of the process of searching for examples to learn that is used in subsequent example selection. 
　all in all  it seems clear that prodigy's determination of what to learn is far more complex than the simple recognition of patterns in the problem-solving trace. the process 
1 this appraoch would seem consistent with soar's methodology of expressing all aspects of problem-solving in terms of searchbut it would raise the question of the source of soar's leverage  its architecture or its representational models. for this reason the approach might be said to undercut soar's goal of achieving intelligent behavior through a uniform architecture. 
1 	cognitive modelling 
employs a great deal of heuristic information about what will constitute a good search control rule  and this information enables prodigy to learn effectively. were it not for these heuristics  the learning process would spend an inordinate amount of time learning pointless search control rules.1 
　the point  however  is not that prodigy is in any way wrong to carry out complex reasoning of this sort to determine what to learn-indeed  the fundamental claim of this paper is that such inference is necessary. rather  the point is that while prodigy relegates the complexities of example selection to procedurally-encoded heuristics  the research presented in this paper attempts to make such reasoning and information explicit. 
　a number of recent research endeavors have taken an approach similar to castle's in carrying out explicit inference to determine how to apply learning routines to a given situation. one significant initiative is in the area of multi-strategy learning  michalski  1   in which systems have the ability to apply a number of learning algorithms to a 
　problem  and use dynamic inference to determine which is best. this work is certainly similar in spirit to castle  but it is important to note that it does not inherently address the issue of determining what to learn  rather it represents an inferential method of determining how to learn. 
　a number of other research projects have addressed more specifically the issue of learning goals  hunter  1; cox and ram  1; leake and ram  1; michalski  1  the use of learning goals per se does not imply inferential determiniation of what to learn  because such goals are often treated simply as inputs to the system. some of these projects  however  decide dynamically which learning goals to pursue  and as such directly address the issues we have been discussing in this paper. several of these projects take approaches that are strikingly similar to castle's  notably in the areas of story understanding  cox and ram  1  and case-based planning  oehlmann et. al  1; fox and leake  1 . 
1. conclusions 
castle's approach to modeling planning in terms of semantically meaningful components gives it the ability to reason dynamically about what to learn. castle is currently able to learn twelve strategies  including forking  pinning  and boxing in  that relate to a variety of cognitive tasks  krulwich  1 . this is possible because of the system's ability to reason explicitly about its tasks and subtasks. previous research has determined what to learn in a fixed way  either by wiring the determination into the control structure of the program  or by providing complex ad hoc methods for making the determination. 
　the guiding theme throughout this research has been the use of self-knowledge in learning to plan  collins et. al  
1 thanks to steve minton for personal communication clarifying the issues involved in prodigy's selection heuristics. 

1 . several forms of self-knowledge have been delineated  including component specifications  justifications  and explanatory models  and learning algorithms have been developed and adapted to use this knowledge properly. the reification of this knowledge and methods for using it effectively form the bulk of the contributions made by the research. 
　the castle system  while demonstrating the viability of the approach  is only a first step in implementing a system that learns using self-knowledge  and there are many areas of open research in extending the application of these ideas. one such area is to apply the approach to more complex decision-making methods  such as non-linear planning  casebased reasoning  or hierarchical planning  in which the approach consists of repeated application of a number of subprocesses  each of which would be explicitly modeled. another area is in more complex domains of application  such as robotic planning  complex route planning  or scheduling  which would again require extending the system's models. 
　in a broader sense  the research suggests an agenda exploring the use of self-models in learning  planning  and understanding. the formulations of self-knowledge that are useful in learning should also give leverage into planning  execution  knowledge acquisition  communication  understanding  and design. 
acknowledgements 
davis  r.  1. representations of commonsense knowledge. morgan kauffman  san mateo  ca. 
dejong  g.  gervasio  m.  and bennett  s.  1. on integrating machine learning with planning. in foundations of knowledge acquisition: machine learning  pages 1. kluwer press. 
doyle  r.  atkinson  d.  and doshi  r.  1. generating perception requests and expectations to verify the execution of plans. in proceedings of the 1 national conference on artificial intelligence  pages 1. 
fox  s. and leake  d.  1. using introspective reasoning to guide index refinement in case-based reasoning. in proceedings of the 1 conference of the cognitive science society. 
hammond  k.  1. case-based planning: viewing planning as a memory task. academic press  san diego  ca. also yale technical report #1. 
hunter  l.  1. knowledge-acquisition planning: gaining expertise through experience. ph.d. thesis  yale university. 
krulwich  b.  1. determining what to learn in a multicomponent planning system. in proceedings of the 1 cognitive science conference  pages 1. 
krulwich  b.  1. flexible learning in a multi-component planning system. ph.d. thesis  the institute for the learning sciences  northwestern university. technical report #1. 
leake  d.  and ram  a.  1. goal-driven learning: fundamental issues and symposium report. tech. report 1  indiana university. 
michalski  r.  1. machine learning  special issue on multistrategy learning  1/1 . 
minton  s.  1. constraint-based generalization: learning game-

the research described in this paper was done in collaboration with playing plans from single examples. in proceedings of the 1 mike freed. the institute for the learning sciences was established national conference on artificial intelligence. in 1 with the support of andersen consulting. the institute 
                                               minton  s.  1. learning effective search-control knowledge: receives additional support from ameritcch and north west water  
an explanation-based approach. ph.d. thesis. school of computer 
corporate sponsors. 	science  carnegie mellon university. technical report cmu-cs-
1. also published by kluwer academic publishers. 
