 
latent semantic analysis  lsa  is a statistical  corpus-based text comparison mechanism that was originally developed for the task of information retrieval  but in recent years has produced remarkably human-like abilities in a variety of language tasks. lsa has taken the test of english as a foreign language and performed as well as non-native english speakers who were successful college applicants. it has shown an ability to learn words at a rate similar to humans. it has even graded papers as reliably as human graders. we have used lsa as a mechanism for evaluating the quality of student responses in an intelligent tutoring system  and its performance equals that of human raters with intermediate domain knowledge. it has been claimed that lsa's text-comparison abilities stem primarily from its use of a statistical technique called singular value decomposition  svd  which compresses a large amount of term and document co-occurrence information into a smaller space. this compression is said to capture the semantic information that is latent in the corpus itself. we test this claim by comparing lsa to a version of lsa without svd  as well as a simple keyword matching model. 
1 	introduction 
although classical natural language processing techniques have begun to produce acceptable performance on real world texts as shown in the message understanding conferences  darpa  1   they still require huge amounts of painstaking knowledge engineering and are fairly brittle in the face of unexpected input. recently  corpus-based statistical techniques have been developed in the areas of word-tagging and syntactic grammar inference. but these techniques are not aimed at providing a semantic analysis of texts. 
   *this work was supported by grant number sbr 1 from the national science foundation's learning and intelligent systems unit. 
1 	natural language processing 
　in the late 1's  a group at bellcore doing research on information retrieval techniques developed a statistical  corpus-based method for retrieving texts. unlike the simple techniques which rely on weighted matches of keywords in the texts and queries  their method  called latent semantic analysis  lsa   created a high-dimensional  spatial representation of a corpus and allowed texts to be compared geometrically. in the last few years  several researchers have applied this technique to a variety of tasks including the synonym section of the test of english as a foreign language  landauer et al.  1   general lexical acquisition from text  landauer and dumais  1   selecting texts for students to read  wolfe et al.  1   judging the coherence of student essays  foltz et a/.  1   and the evaluation of student contributions in an intelligent tutoring environment  wiemer-hastings et al.  1; 1 . in all of these tasks  the reliability of lsa's judgments is remarkably similar to that of humans. 
　the specific source of lsa's discriminative power is not exactly clear. a significant part of its processing is a type of principle components analysis called singular value decomposition  svd  which compresses a large amount of co-occurrence information into a much smaller space. this compression step is somewhat similar to the common feature of neural network systems where a laxge number of inputs is connected to a fairly small number of hidden layer nodes. if there are too many nodes  a network will  memorize  the training set  miss the generalities in the data  and consequently perform poorly on a test set. the input for lsa is a large amount of text  on the order of magnitude of a book . the corpus is turned into a co-occurrence matrix of terms by  documents   where for our purposes  a document is a paragraph. svd computes an approximation of this data structure of an arbitrary rank k. common values of k are between 1 and 1  and are thus considerably smaller than the usual number of terms or documents in a corpus  which are on the order of 1. it has been claimed that this compression step captures regularities in the patterns of co-occurrence across terms and across documents  and furthermore  that these regularities are related to the semantic structure of the terms and documents. 

　in this paper  we examine this claim by comparing several approaches which assess the quality of student contributions in an intelligent tutoring situation. we use human judgments of quality as a baseline  and compare them to three different models: the full lsa model  a version of lsa without svd  and a simple keywordmatching mechanism. the paper starts with a description of the quality judgment task  and describes how lsa was used to rate the contributions. in section 1  we describe the implementation of lsa without svd  and compare it to the svd results. in section 1  we compare these to a basic keyword matching algorithm which used both a weighted and an unweighted matching technique. we close with a discussion of these results. 
1 evaluating student contribution quality with lsa 
to provide a baseline description against which the alternative methods can be judged  this section describes the rating task for both the humans and lsa  gives some technical details of the lsa implementation  and describes how it performed in relation to the human raters. 
1 	t h e quality evaluation task 
as the litmus test for the various evaluation techniques  we have chosen the domain of an intelligent tutoring system called autotutor that was developed with the goal of simulating natural human-human dialogue  wiemerhastings et al  1 . the tutoring domain for this project was computer literacy. the main knowledge structure for autotutor was a curriculum script  putnam  1  that contained 1 questions in each of three different topics: computer hardware  operating systems  and the internet. for each question in the curriculum script  there was a variety of information about expected student answers and possible follow-up dialogue moves. the questions were designed to be deep reasoning questions which for which a complete answer would cover several aspects. autotutor's curriculum script contained an expected good answer for each of the aspects of a question  as well as a prompt  hint  and elaboration that could potentially elicit that answer. the use of these dialogue moves was based on studies of human tutors  graesser et a/.  1 . dialogue move rules decided which move to use based on the student's ability and on which expected good answers were already covered. lsa was the primary mechanism for determining that coverage based on comparisons between the student responses and the expected good answers. when a particular contribution achieved a cosine match above an empirically determined threshold  that aspect of the question was considered as covered for the purposes of the tutoring task. this approach led to the definition of the basic evaluation measure: 
compatibility = matches / propositions  where propositions is the number of speech acts in the student contribution  and matches is the number of propositions that achieved an abovethreshold lsa cosine with one of the expected good answers for this question. 
loosely speaking  this is the percentage of the student's contribution that sufficiently matched the expected answer/ 
　the test set for this task was based on eight questions from each of the three tutoring topics. students in several sections of a university-level computer literacy course were given extra credit for typing in answers to the questions in a word processing document. they were encouraged to write complete  thorough answers to the questions. eight substantive  i.e. not  i don't know   answers were randomly selected for each of the 1 questions  constituting a test set of 1 items. 
1 	h u m a n ratings 
to assess the depth of knowledge that lsa uses  human raters of different levels of experience with the subject matter were used. two raters  a graduate student and a research fellow  were computer scientists with high levels of knowledge of the computer literacy domain. two additional raters  a graduate student and professor in psychology  had intermediate-level knowledge. they were familiar with all of the text materials from the computer literacy domain that were used in the project. 
　the human raters were asked to break the student responses into propositions  i.e. parts that could stand alone in a sentence. then they were asked to judge on a six-point scale the percentage of each student's propositions that  matched  part of the ideal answer. they were not instructed as to what should constitute a match. the correlation between the two expert raters was r=1. between the intermediate knowledge raters  the correlation was r=1. the correlation between the average expert rating and the average intermediate rating was r=1. all of the correlations were significant at the 1 level. 
1 	l s a implementation 
we briefly describe the lsa mechanism here in order to demonstrate the difference between it and the other approaches. further technical details about lsa can be found in  deerwester et a/.  1; landauer and dumais  1  and several of the articles in the 1 special issue of discourse processes on quantitative approaches to semantic knowledge representations. 
　as mentioned above  the basic input to lsa is a large corpus of text. the computer literacy corpus consisted of two complete computer literacy textbooks  ten articles on each of the tutoring topics  and the entire curriculum script  including the expected good answers . each curriculum script item counted as a separate document  and the rest of the corpus was separated by paragraphs because they tend to describe a single complex concept. the entire corpus was approximately 1 mb of text. lsa defines a term as a word  separated by whitespace or punctuation  that occurs in at least two documents. 
	wiemer-hastings 	s1 


figure 1: the correlation between lsa quality judgments and those of human raters. 

there is also a list of about 1 very frequent words 
  the    and   and  for   for example  that are not used as terms. as previously mentioned  lsa creates from this corpus a large co-occurrence matrix of documents by terms  in which each cell is the number of times that that term occurred in that document. each cell is then multiplied by a log entropy weight which essentially reduces the effect of words which occur across a wide variety of contexts  more about this later . svd then creates a kdimensional approximation of this matrix consisting of three matrices: a d by k documents matrix  a k by t terms matrix  and a k by k singular values  or eigenvalues  matrix  d is the number of documents  and t is the number of terms . multiplying these matrices together results in an approximation to the original matrix. each column of the terms matrix can be viewed as a k-long vector representing the  meaning  of that term. each row of the documents matrix can be seen as a k-long vector representing the meaning of that document. furthermore  each document vector equals the sum of the vectors of the terms in that document. 
　the lsa mechanism for autotutor works by calculating the vectors for the student contributions and comparing them to the document vectors for the expected good answers using the cosine metric. empirical analyses of the corpus size  the number of dimensions  and the thresholds showed that the lsa mechanism performed best with the entire corpus described above  and with 1 dimensions in the lsa space. figure 1 shows the correlations between the lsa ratings and the average of the human ratings over a variety of cosine match thresholds. the correlation between lsa and the humans approaches that between the human raters. although a slightly higher correlation was achieved with a 1dimension lsa space  this increased performance was limited to a single threshold level. this was interpreted as a potential outlier  and the 1 dimension space  with its relatively flat performance across thresholds  was preferred. 
1 natural 	language 	processing 
1 	lsa without svd 
as previously mentioned  lsa has several attributes that may be responsible for its ability to make effective similarity judgments on texts. in addition to the compression/generalization provided by the svd calculation  lsa might get its benefits from its initial representation of word  meaning  as a vector of the documents that it occurs in. before the svd processing  this representation is modified by an information theoretic weighting of the elements  which gives higher weights to terms that appear distinctively in a smaller number of texts  and lower weights to terms that occur frequently across texts. the comparison of texts using the cosine measure on such vectors might also be responsible for such good performance. to test how much discriminative power lsa gains from svd  we implemented a version of lsa without svd. this section describes the implementation and evaluation of this mechanism  and relates it to the evaluation of the standard lsa approach. 
1 	creating the term vectors 
to create this model  we started with the documents by terms co-occurrence matrix after the information theoretic weighting and before the svd processing. we took the columns of this matrix as a representation of the meaning of each term. because there were over 1 documents in the corpus and most terms occur in a small number of documents  this is a very sparse representation. still  it is possible to compare these vectors using the cosine metric. two terms which occur in exactly the same set of documents would have a cosine of 1. terms which occur in disjoint sets of documents have a cosine of 1. it is also possible with this representation to compute a document vector by adding the vectors of the terms in the document. however  it is not possible to construe the rows in the co-occurrence matrix as the vectors representing document meaning because they have a different rank  the number of terms in the corpus  and 


figure 1: the correlation between lsa without svd and human raters. 

because there is no reason to equate a pattern of term occurrence  the terms are alphabetized in the representation  with a pattern of document occurrence. thus  we had to calculate vectors not just for the student contributions but for the expected good answer documents as well. 
1 	evaluation 
after these vectors were computed  the evaluation was done in exactly the same way as the evaluation of the full lsa model. figure 1 shows the correlations between the average of the humans' ratings and the nonsvd model. it is clear that the combination of the distributed  weighted vectors and the geometrical comparisons were sufficient to produce judgments approaching those of the full lsa model. the maximum performance here is r = 1. as a reminder  the maximum performance of the full lsa model was r = 1. the maximum performance in this case  however  occurs at just one threshold. for the 1-dimension lsa model  there was fairly stable performance across several thresholds. 
1 	keyword matching 
because the performance of the non-svd algorithm was so close to that of the full lsa implementation  we decided to evaluate a simple keyword-based approach for this task. this section describes the implementation and testing of that approach. 
1 	the matching algorithm 
to compare texts with a keyword-matching approach  we used the same segmentation of the student contribution  the same set of expected good answers for each question  and the same set of terms  as keywords  as in the other approaches. we used the same compatibility measure  matches / propositions  that we used for lsa. to determine the extent to which a student contribution speech act s matched an expected good answer ey we defined the keyword match  km  as follows: 

the variable wt is the weight for a particular term. we tested this keyword approach using both a 1 weight for all terms  and also using the information theoretic weight calculated by lsa. the keyword match is essentially the sum of the weights for each keyword that occurs in both the student contribution and the expected good answer  divided by the maximum number of keywords in these two texts. as in the other evaluations  we correlated the performance of the metric at a range of different threshold levels as described in the next section. 
1 	evaluation 
in our first evaluation of the keyword model  we used the same set of thresholds as in the non-svd evaluation  namely from 1 down to 1 in 1 increments. this resulted in somewhat of a floor effect in the testing however. the lsa weights for terms varied from about 1 to 1  but the highest values were only for very rare terms. thus  most km values for the weighted approach were relatively low  reaching a maximum of around .1  so we also ran the analysis on a set of thresholds from 1 down to 1 in 1 increments. 
　figure 1 shows the correlations with the human ratings for the unweighted keyword model  and both threshold sets for the the weighted model. note that the threshold labels do not correspond to the actual thresholds for the 1 to 1 threshold set. the actual thresholds  however  are not important. the general shape of the curve is a fairly clear indicator of the behavior of these models. 
　the most striking feature of this experiment is the peak correlation of r=1 shown by the weighted model at the 1 threshold level. this is almost equivalent to the maximum performance of the full lsa model. similar to the 1-dimension lsa model and the no-svd model described earlier  however  this point appears to 
	wiemer-hastings 	1 


figure 1: performance of the keyword matching technique. 

be an outlier that would be unlikely to apply across another test set  because it is significantly higher than the neighboring thresholds  which display a fairly flat curve. 
　we are comfortable in claiming that the simple keyword model can achieve a reliable correlation of r = 1 with the human raters  with the weighted model showing a relatively flat contour across a range of thresholds. this level of performance is quite close to that shown by the lsa without svd model  and within about 1% of the performance of the full lsa model. given the large difference in computational resources required to calculate the keyword approach  the terms and their weights are simply accessed in a hash table   such an approach to text comparison could be beneficial when computational resources are more important than getting the most reliable judgments. 
　although the computation of the keyword match was fairly simple  it must be noted that the information theoretic approach used in the weighted keyword model came from the two-textbook corpus that was used for lsa. collecting this amount of text was a daunting task  but alternative term weights could be calculated from a smaller corpus or from an online lexicographic tool like wordnet  fellbaum  1 . 
1 	discussion 
in this paper we addressed the question of the contribution of the compression step of svd to lsa  and we compared lsa to a simple keyword-based mechanism in evaluating the quality of student responses in a tutoring task. we showed that although the performance of the fall lsa model was superior to the reduced models  these alternatives approached the discriminative power 
1 	natural language processing 
of lsa.1 
　lsa gets its power from a variety of sources: the corpus-based representation of words  the information theoretic weighting  the use of the cosine to calculate distances between texts  and also svd. svd should make lsa more robustly able to derive text meaning when synonyms or other similar words are used. this may be reflected by the wider range of thresholds over which lsa performance remains relatively high. 
　even though lsa without svd seems to perform fairly well  it must be noted that the use of svd results in a very large space and processing time advantage by drastically reducing the size of the representation space. if we took lsa without svd as the original basis for comparison  and then discovered the advantages of svd with its ability to  do more with less   it would clearly be judged superior to the non-svd lsa model. 
　it should also be noted that this task is rather difficult for lsa. it has been previously shown that lsa does better when it has more text to work with  rehder et a/.  1   with relatively low discriminative abilities 
in the 1 - 1 word range  and steadily climbing performance for more than 1 words. in fact  other researchers have reported that in short-answer type situations  lsa acts rather like a keyword matching mechanism. it is only with longer texts that lsa really distinguishes itself  walter kintsch  personal communication  january  1 . because the student texts in this study are relatively short  average length = 1 words   lsa had less information on which to base its judgments  and therefore  its abilities to discriminate were reduced. it is possible that with longer texts there would be more of a 
　　1  similar results of a relatively small effect of svd on a different corpus were reported by guy denhiere  personal communication  july 1. 

difference between the performance of lsa and the alternative methods presented here. on the other hand  we must also point out that this lack of text seems to have hurt the human raters' abilities to discriminate as well  resulting in fairly low inter-rater reliability scores. 
　the results presented here do not mitigate the promise of such corpus-based  statistical mechanisms as lsa. they suggest  however  that more research is needed to further tease apart the strengths of the various aspects of such an approach. in future research  we will remove the information theoretic weighting from the non-svd model to determine how well the system can perform by treating all words as equals. 
　in conclusion  if you want a text evaluation mechanism based on comparisons  and if you have a good set of texts as a basis of comparison  you have several options. a simple keyword match performs surprisingly well  and is relatively inexpensive computationally. a mechanism like the no-svd model presented here does not produce better maximum performance than the keyword model on these relatively short texts  but it does produce good performance across a range of thresholds  indicating a robustness to be able to handle a variety of inputs. the full lsa model exceeds both the performance and the robustness of both of these models  achieving results comparable to those of humans with intermediate domain knowledge. because the initial goal of the autotutor project is to simulate a normal human tutor that has no special training but nevertheless produces significant learning gains  we are happy with this level of performance. in future research  we will address the possibility of combining structural analysis of the student texts with lsa's semantic capabilities. this may hold the key to approaching the performance of expert human raters in this task. 
acknowledgments 
this work was completed with the help of katja wiemerhastings  art graesser  roger kreuz  lee mccaulley  biaiiea klettke  tim brogdon  melissa ring  ashraf anwar  myles bogner  fergus nolan  and the other members of the tutoring research group at the university of memphis: patrick chipman  scotty craig  rachel dipaolo  stan franklin  max garzon  barry gholson  doug hacker  xiangen hu  derek harter  jim hoeffner  jeff 
janover  kristen link  johanna marineau  bill marks  
michael muellenmeister  brent olde  natalie person  victoria pomeroy  holly yetman  and zhaohua zhang. we also wish to acknowledge very helpful comments on a previous draft by three anonymous reviewers. 
