 
the past several years have seen much progress in the area of propositional reasoning and satisfiability testing. there is a growing consensus by researchers on the key technical challenges that need to be addressed in order to maintain this momentum. this paper outlines concrete technical challenges in the core areas of systematic search  stochastic search  problem encodings  and criteria for evaluating progress in this area. 
1 introduction 
propositional reasoning is a core problem in many areas of artificial intelligence. in recent years this area has seen growing interest and activity  due to advances in our ability to solve large problem instances  including ones that encode real-world problems such as planning and diagnosis. contributions to the area of propositional deduction and satisfiability testing have come from research communities in artificial intelligence  operations research  and theoretical computer science. a set of key technical challenges have begun to emerge from interactions within and among these research groups. we will describe some of these challenges and discuss why they are important for progress in the field. 
　for many years the problem of propositional reasoning received relatively little attention. it appeared that nothing could be done to improve upon the performance of the davis-putnam procedure  1   which held its position as the most efficient satisfiability testing algorithm for three decades. furthermore  most researchers in artificial intelligence felt that the representational power of propositional logic was too limited  and turned their attention to first-order logic or even more powerful formalisms  such as various modal and non-monotonic logics. several factors contributed to a renewed interest in propositional reasoning. first  new algorithms were discovered  including ones based on stochastic local 
1 	ai challenges 
search as well as systematic search  that have better scaling properties than the basic davis-putnam algorithm. second  improvements in machine speed  memory size  and implementations extended the range of the algorithms. third  researchers began to develop and solve propositional encodings of interesting  real-world problems such as planning and diagnosis  with others on the horizon  such as natural language processing and machine learning. such encodings were not even being considered a few years ago because they were thought to be far too large to be handled by any method. between 1 and 1 the size of hard satisfiability problems that could be feasibly solved grew from ones involving less than 1 variables to ones involving over 1 variables. 
　progress has been spurred by the interaction of researchers in ai  operations research  and theory  in making available benchmark problems  dimacs  see trick and johnson 1   holding joint workshops  ginsberg 1   and sharing algorithms and code. there is a growing consensus  however  that certain key technical challenges must be addressed in order to continue to increase the range of problems that can be practically solved. work on propositional reasoning can be grouped in three core areas: systematic search; stochastic search; and problem encodings. for each area  we will discuss concrete challenges that have arisen. we stress that these challenges are not particularly our own invention  but rather summarize the issues that frequently arise in informal discussions among researchers. progress on any of these challenges would directly extend the usefulness of the propositional reasoning approach to problems in ai. 
　because of the large number of references involved  we have not attempted to cite all related literature. however  a web page for these challenge problems has been constructed with a more complete bibliography  further details on the challenge problems  and pointers to existing satisfiability testing procedures. see http://www.research.att.com/~selman/challenge. 
　
　with each challenge  we give our best estimate of a time frame within which we think the challenge can be met. for the harder challenges  even partial solutions will constitute a significant contribution to the field. 
1 evaluation criteria 
the main evaluation criteria that has been adopted in the satisfiability testing  sat  community is empirical performance on shared benchmark problems. we believe that this remains the best way to evaluate responses to any of the algorithmic challenges we will present. the alternative  evaluation by theoretical analysis  is problematic. worst-case complexity results are all usually exponential. furthermore  theoretical average case results are difficult to obtain  and can even be misleading because of the difficulty of formally characterizing problem distributions. even when a sound theoretical analysis is devised  it may only characterize the asymptotic performance of the algorithm  and that asymptote may lie too far out  e.g.  to problems containing trillions of variables  to be of practical consequence. of course  theoretical analysis can be a very useful complement to empirical evaluation  and can help us gain insights into why an algorithm works well or poorly on a given problem distribution.  in fact  one of our non-algorithmic challenges  as we will see  is to develop a theoretical analysis that explains why local search works well on certain problem distributions.  
　reasonably good benchmark collections of satisfiability problems  mainly in cnf form  are publically available from a number of sources. these include the dimacs collection  which was used as a testbed for a large number of algorithms as reported in  trick and johnson 1 ; a collection of circuit diagnosis problems encoded as sat  larrabee 1 ; the planning problems kautz and selman  1  devised for their satplan system; hardware and software verification problems from the model checking community; and others. all these collections have the property that published papers exist specifying the best known algorithms and running times for solving each of their instances. 
　progress on any of our challenges  when applicable  will be measured by showing that the proposed method outperforms all other known methods on at least one set of benchmark problems. some of our challenges in the area of problem encodings involve finding better ways to represent a real-world problem as sat. in these cases  the empirical evaluation should involve comparing performance of the best sat algorithms on the proposed encoding against the best published results for any algorithm that can solve the original  i.e.  unencoded  problem instances. in such a comparison it is usually not possible to outperform the specialized algorithms  although it is sometimes possible  see for example  kautz and selman 1    but one should provide evidence that sofving the encoded problem with general purpose boolean satisfiability procedures is at least competitive. 
　whenever possible  comparisons should also be made to the difficulty of solving the problems when alternative encodings are used  such as may be found in some of the benchmark sets. in cases where alternative encodings are available  one should show that the new encoding yields problems that are easier to solve by at least one state-of-the-art sat procedure.  for example  it would be counted as progress if one developed a new encoding of planning problems that was good for stochastic search procedures  even if the encoding did not help systematic search procedures.  
　in order to keep the criteria for comparison as objective as possible  it is important that results include total running time. this may be adjusted for machine speed  but it is not sufficient to only report certain characteristics of the execution  such as  number of nodes expanded   johnson 1 . there are many ways to shift the computational effort in search algorithms - for example  to visit fewer nodes by doing more work at each node - and an objective evaluation must consider the entire picture. comparisons should cite the best results from the operations research and computer science theory literature  as well as the ai literature. 
　tests should be done on a variety of problem sizes  up to the hardest available instances that the method can solve. it is important to show the limits of the proposed method  that is  where it becomes highly exponential or otherwise fails. often methods that look good on small instances break down on larger ones  johnson 1 . 
1 challenging sat problems 
we shall begin the list of challenges by citing two specific open sat problems. the first is to develop a way to prove that unsatisfiable 1 variable 1-cnf formulas  randomly generated in the  hard  region where the ratio of variables to clauses is 1  are in fact unsatisfiable  mitchell et al. 1; crawford and auton 1 . randomly-generated satisfiable formulas of this size are regularly solved by stochastic search algorithms such as gsat  but they are unable to prove unsatisfiability; and no systematic algorithm has been able to solve hard random formulas of this size. a generator for these hard random problem instances can be found at the dimacs benchmark archive  trick and johnson 1 . 
　challenge 1:  1 yrs  prove that a hard 1 variable random 1-sat formula is unsatisfiable. 
　the second challenge problem is satisfiable. it is an encoding of a 1-bit parity problem  that appears in the dimacs benchmark set. it appears to be too large for 
	selman  kautz  & mcallester 	1 
　
current systematic algorithms. it also defeats the hillclimbing techniques used by current local search algorithms. 
　challenge 1:  1 yrs  develop an algorithm that finds a model for the dim a cs 1-bit parity problem. 
　for the second challenge  of course  the algorithm should not be told in advance the known solution! given the amount of effort that has been spent on these two instances  any algorithm solving one or both will have to do something significantly different from current methods. 
1 challenges for systematic search 
in the previous section we described two challenging sat problems. in this section  and the next one  we describe promising ideas whose utility has not yet been demonstrated. in each case the challenge is to demonstrate the utility of a currently known  but as yet unproven  approach to solving sat problems. in keeping with the discussion in section 1  to demonstrate the utility of an idea one must construct a procedure which uses that idea in an essential way in solving at least one class of problems more effectively than any other known approach. 
　our next challenge is using proof systems stronger than resolution. all of the best systematic methods for propositional reasoning are based on creating a resolution proof tree. this includes depth-first search algorithms such as the davis-putnam procedure  where the proof tree can be recovered from the trace of the algorithm's execution  but is not explicitly represented in a data structure  the algorithm only maintains a single branch of the proof tree in memory at any one time . most work on systematic search concentrates on heuristics for variable-ordering and value selection  all in order to the reduce size of the tree. 
　however  there are known fundamental limitations on the size of the shortest resolution proofs for certain problems  haken 1; chvatal and szemeredi 1 . for example   pigeon hole  problems  showing that n pigeons cannot fit in n - 1 holes  are intuitively easy  but shortest resolution refutation proofs are of exponential length. shorter proofs do exist in more powerful proof systems. examples of proof systems more powerful than resolution include extended resolution  which allows one to introduce new defined variables  and resolution with symmetry-detection  which uses symmetries to eliminate parts of the tree without search. assuming np 』 co - np  even the most powerful propositional proof systems would require exponential long proofs worst case - nonetheless  such systems provably dominate resolution in terms of minimum proof size. 
　however  at this point in time  attempts to mechanize these more powerful proof systems usually yield no 
1 	ai challenges 
computational savings  because it is harder to find the small proof tree in the new system  than to simply crank out a large resolution proof. in essence  the overhead in dealing with the more powerful rules of inference consumes all the potential savings. there is promising work in this area  crawford et al. 1; de la tour and demri 1   but not yet convincing empirical results on a variety of benchmark problems; such evidence would meet our third challenge: 
　challenge 1:  1 yrs  demonstrate that a propositional proof system more powerful than resolution can be made practical for satisfiability testing. 
　our next challenge is the use of integer programming techniques in solving sat problems. it is straightforward to translate sat problems into 1 integer programming problems  hooker 1   and thus it has been argued that integer programming techniques should be useful for propositional reasoning. in fact  however  this has not been shown to be the case. for example  one of the main techniques in integer programming is to compute the linear relaxation of the problem  and then to use the  easily-found  solution of the relaxed problem to guide the selection of values in solving the integer problem. however  in most formulations the solution to the linear relaxation of any sat problem simply sets all the variables to the value  1   modulo unit propagation   thus yielding no guidance at all. therefore we offer a challenge to show that the well-developed body of tools and techniques from operations research does in fact have something new to offer for propositional reasoning. 
　challenge 1:  1 yrs  demonstrate that integer programming can be made practical for satisfiability testing. 
1 challenges for stochastic search 
stochastic local search has been shown to be a powerful alternative to systematic search for finding models of satisfiable cnf formulas. stochastic algorithms are inherently incomplete  because if they fail to find a model for a formula one cannot be certain that the formula is unsatisfiable. this has led to an asymmetry in our ability to solve satisfiable and unsatisfiabie instances drawn from the same problem distribution. stochastic algorithms can solve hard random satisfiable formulas containing thousands of variables  but we cannot solve unsatisfiabie instances of the same size. 
　can local search be made to work for proving unsatisfiability  this apparently would require searching in space of refutation proofs  rather than in space of truth assignments. each state would be an incomplete proof tree  e.g.  a proof tree that rules out some fraction of the truth assignments. the neighborhood of a state would be similar proof trees. the step in the local search would 
　
try to transform the proof into one that rules out a larger fraction of assignments. 
challenge 1:  1 yrs  design a practical 
stochastic local search procedure for proving unsatisfiability. 
　our next challenge is that of distinguishing  dependent  from  independent  variables. sat encodings of structured problems such as planning and diagnosis often contain large numbers of variables whose values are constrained to be a simple boolean function of other variables. we call these dependent variables. variables whose values can not be easily determined to be a simple function of other variables are called independent. for a given sat problem there may be many different ways to classify the variables as dependent and independent. but for most sat encodings of real-world problems there is a natural division between dependent and independent variables. since an assignment to the independent variables determines a truth value for each dependent variable  the number of assignments that need be considered in a systematic search is at most 1n where n is the number of independent variables. 
　we belief that it fairly easy to empirically demonstrate that identification of dependent variables improves the performance of systematic search. it appears to be more difficult to establish that identification of dependent variables improves stochastic search. the challenge  therefore  is to improve local search for problems with dependent variables: the search should concentrate only  or mainly  on the independent variables  and some fastmechanism should then set the dependent variables. 
challenge 1:  1 yrs  improve stochastic lo-
cal search on structured problems by efficiently handling variable dependencies. 
　as we have noted  systematic and local search procedures outperform each other on different problem classes. even putting the issue of incompleteness aside  there exist classes of satisfiable problems for which one or the other approach is clearly the winner. this leads to the general question: can we develop a procedure that leverages the strengths of each  the obvious way  of course  is to simply run good implementations of each approach in parallel. but is there a more powerful way of combining the two  recently there has been some intriguing work on using local search to implement the variable ordering heuristic for systematic search  boufkhad 1; mazure et al. 1 . these methods and other ways of combining systematic and stochastic search need to be further developed and compared with previous approaches on a range of benchmark problems. 
challenge 1:  l-1yrs  demonstrate the suc-
cesful combination of stochastic search and systematic search techniques  by the creation of a new algorithm that outperforms the best previous examples of both approaches. 
1 challenges for problem encodings 
the value of research on propositional reasoning ultimately depends on our ability to find suitable sat encodings of real-world problems. as discussed in the introduction  there has been significant recent progress along this front. examples include classical constraintbased planning  blum and furst 1; kautz and selman 1   problems in finite algebra  pujita et al. 1   verification of hardware and software  scheduling  crawford and baker 1   circuit synthesis and diagnosis  larrabee 1   and many other domains. experience has shown that different encodings of the same problem can have vastly different computational properties. for example  in planning   causal  encodings appear to be harder to solve than  state-based  encodings. 
　a challenge  therefore  is to develop a general characterization of encodings that can be efficiently solved. this characterization may involve  for example  understanding the relationship between the encoding and the shape of its search space. note that the characterization cannot be as simple as stating that the search space has no local minima  because realistic problems will almost certainly have local minima. for example  the sat encodings of blocks-world planning problems do have deep local minima  yet can be solved by local search. we need to understand why search can escape from local minima in some encodings but not in others. perhaps one can find easily measurable statistical properties of the encoding that predict the behavior of various search algorithms on a given instance. 
　challenge 1:  1 yrs  characterize the computational properties of different encodings of a realworld problem domain  and/or give general principles that hold over a range of domains. 
　note that evaluation of progress on this last challenge cannot be purely empirical. there will be some sub-
jective judgement necessary in determining whether the characterization is useful and enlightening. predictions of the theory can be put to an empirical test: in the best case  the general principles would suggest new encodings which can be solved more easily than previous ones. 
　one problem with all of the propositional encodings for real-world problems that have been suggested in the literature is that they are extremely  brittle . if we look at formulations where models that satisfy the encoding correspond to solutions to the original problem instance  we see little relationship between models that  almost  satisfy the encoded problem and candidate solutions that  almost  satisfy the original problem instance. for example  kautz and selman  1  noted that it was easy to find truth-assignments that satisfied all but a single 
	selman  kautz  & mcallester 	1 
　
clause of sat encodings of blocks-world planning problems. these  near models  corresponded to making a series of random motions  and then  in the last state the blocks  magically   violating physical constraints  arrange themselves in the correct position. 
　the robustness of encodings  and in particular  the robustness of local search algorithms applied to those encodings  could be improved by finding ways to more closely align the semantics of the sat instance with the semantics of the source domain: 
　c h a l l e n g e 1:  1 yrs  find encodings of realworld domains which are robust in the sense that unear models  are actually  near solutions . 
　as we discussed earlier  benchmarks have been an important driving force behind work on propositional reasoning algorithms. ideally one would have access to a very large number of real-world problem encodings  so that one could develop solid statistical evidence of the performance of various techniques. in practice the number of real problems that can be obtained is limited. it is extremely time-consuming and knowledge-intensive work to manually create such instances  and the largest and potentially most useful examples are often part of some proprietary project  and thus cannot be shared. hard randomly-generated problems have proven to be a good alternative for testing - so far  the algorithms that are the best on randomly generated instances are also best on structured problems. however  there is the concern that we may be reaching a point where this is no longer the case  and that of the simple random distributions now used for testing may be driving us in the wrong direction in our research  johnson 1 . therefore we present a final challenge  which if answered would provide vital tools for ensuring progress in this field. 
　c h a l l e n g e 1:  1 yrs  develop a generator for problem instances that have computational properties that are more similar to real-world instances. 
　it is necessary to provide concrete evidence  empirical and/or theoretical  that the problem distribution so generated closely matches some set of real-world domains. 
1 	conclusions 
we have presented a series of technical challenge problems in the area of propositional reasoning and search. we believe that progress towards solving these problems will directly extend the usefulness of the propositional reasoning approach to problems in artificial intelligence  and computer science in general. 
