 	two drawbacks to a* explain its high complexity. 

   the algorithm a*  nilsson  1  presents two significant drawbacks. first  in seeking strict optimal solution paths it necessarily has high order of complexity. second  the algorithm does not explicitly descriminate between the cost of a solution path and the cost of finding the solution path. to confront these problems we propose the algorithm ae  a generalization of a*. instead of seeking an optimal solution  it seeks one which is within a factor  1+e  of optimum  e   1 . the first  a* tends to do much backtracking due to the invariable choice of n' as the node to be expanded next. a* tends to expand many nodes not in the final solution path since h  being a heuristic  fluctuates in quality and hence various near optimal paths take random turns appearing to be optimal. such paths effectively  race  with one another to reach their goals. so we can trace the cause of backtracking to a*'s desire to fine tune an optimal cost solution. 

basic idea is to avoid doing any search at all on most near optimal partial solutions by sticking to a small number of most fruitful paths. various strategies for searching for near optimal partial solutions are discussed. experimental results are presented indicating that a e has average complexity of lower order than a* and compares favorably to the related algorithm af*  pearl and kim  1 . 
i introduction - admissible search is expensive 
   this paper emphasizes the main ideas behind ae and serves to introduce new empirical results concerning its behavior. a more extensive presentation of ae and its properties appears in  ghallab  1  and in  ghallab and allard  1 . 
we assume familiarity with a* and notation of 
 nilsson  1 . we denote the set of successors of node n by x n . the cost of arc  n x  is k n x  1. we denote a path between n and m by  n...m . for start node s  the true costs of minimal cost paths  s...n    n...t   and 
 s...n...t   over all goal nodes t  are denoted by g* n   h* n   and f* n  respectively. the estimators for these functions are g  h  and f respectively. open is assumed ordered by increasing f  and decreasing g in the case of ties . at all times  n' denotes the first  least f  element of open. 
   the complexity of a* has been studied in some detail. for perfect h = h*  a* is linear. but in general it has worst case complexity which is exponential as a function of the number of nodes in the found solution path  pohl  1 . for a large class of problems a* remains exponential in the average case unless h is very precise and remains unrealistically close to h*  pearl  1 . 
¡¡the second drawback is that a* makes no explicit attempt to minimize search cost  i.e. the number of nodes expanded. given two equally promising paths with respect to path cost  it would seem wise to develop that path which is the fewest arcs from completion. but for a*  it does not matter too much which promising path is developed next since eventually all promising paths must be. thus a* does not have much need for an explicit heuristic to reduce search cost. simply put  admissible search is inherently expensive. 
   our basic premise is that a bounded loss of optimality can always be favorably traded for a gain in computational efficiency. the very notion of  cost  and  optimal cost  are imprecise in most applications involving modelling of the real world. moreover  in heuristic search one often requires only a near optimal or even just decent solution. for example  in robotics  there is a tradeoff between the cost of generating a plan and the optimality of the plan when the plan is to be executed only once. 
ii a1 and near optimal path search stratigies 
   ae drops the strict optimality criteria and seeks instead a solution within a factor  l+f  of optimum for a user specified €  1  this at least makes possible avoiding a*'s defect of having to investigate all optimal looking paths. the problem 
now becomes - which of the many near optimal paths merit attention  
the second author is participating in an exchange program between laas du cnrs f n  is bounded by   i +€ max f  n' } over all n' and the electronics research laboratory  university of california  berkeley. which have appeared as the first element  least f     a€ attempts to answer this question by generalizing a* in two primary ways. first  a€ performs a depth oriented search  preferring to stay on a single path as long as a successor to the frontier node  the last one expanded  of that path is  acceptable . an open node n is acceptable iff 1 m. ghallab and d. allard 
in open. to choose an acceptable successor to expand next or  in the case that no such successor exists  to choose which node in open to backtrack to  a€ can make use of a second heuristic hc. hc n  provides an explicit guess as to the computational cost involved in reaching a goal node by estimating the minimum number of arcs between n and a goal node. 
the second difference between a€ and a* is that 
a€ possesses an inner loop invoked when the frontier node has no acceptable successor. the inner loop attempts to render some successors acceptable by expanding a certain number of times the first node of open  n'. doing so often  if h is monotone  always  increases f n'  and thus may turn unacceptable nodes into acceptable ones. we refer to this idea as the perserverant strategy of a€. a more precise specification of a€ is as follows. 

   notice output €' which gives the actual relative cost of the found solution with respect to max {f n' ' }. it is always true that €' :€ since the output solution  if any  satisfies acceptable. 
   selectax selects which acceptable successor of n is to become the new n. a general approach is to minimize a weighted sum af x  + bhc x . since the main purpose here is to stay on a path previously seen to be near optimal  we believe that a should be larger than b. the extreme approach of taking a = 1 and b - 1 leads to a depth oriented best first search and has the advantage of simplifying the main loop -- statement 1 can be replaced by 
if acceptable xmin  then 
n: = xmin 
else 
n:= selectopen fi 
where xmin is the open successor of n having least f. and statement 1 can be removed entirely. 
   select open has the more d i f f i c u l t task of deciding which acceptable n in open to backtrack to. again we can minimize some weighted function cf n  + dhc n  over all n in open. minimizing fn  gives us precisely n'. this would lead after expansion to raising fthreshold as much as possible hence increasing the likelihood that the developed path will remain acceptable. minimizing hc n  moves us closer to a solution but increases the risk that the inner loop may have to abandon the selected path soon thus provoking backtracking. nevertheless  we argue that selectopen should place importance on hc since the inner loop will be capable of raising €threshold. in general  experimentation should help determine weights for f and hc in any specific application. 
   the perserverant strategy of a1 is embodied in the predicate perservere  the heuristic element of the inner loop. perservere should return true as long as it seems wise to try to render at least one of the successors of n acceptable  i.e. to perservere on the current path. we l i s t below factors which would indicate that doing so is worthwhile. 
- n has a successor which is  almost  acceptable. 
- n has a successor close to a goal  hc small . 
- the second or third best f n  over n in open is significantly greater than f n' . 
- the inner loop has iterated few times. 
- a node already in solved is almost acceptable  n' should be expanded to try rendering a found goal acceptable . 
-  l+f f n'  = fthreshold. if h is monotone this will always be true so this condition would not be useful in that case. 
notice that a* halts when it runs into a goal node as f i r s t element of open whereas a€ continually surveys all generated goal nodes and explicitly attempts to render one or more of them acceptable. 

m. ghallab and d. allard 1 
   to obtain completeness with respect to pearl's results we also tested a  on  difficult problems  in which intercity distances are confined to the 

iii experimental results and conclusion 
   we have tested the hehavior of a€ on tsp  the travelling salesman problem  under the same test conditions  n = 1 cities randomly distributed uniformly in the unit square  and using the same heuristics h and hc as  pearl and kim  i1 . tests were made on sixty different distributions of cities. for each test a  was run with € set successively to 1  1i  1  1  1  1  and 1   € = 1 yields the optimum solution; € = oo yields a solution in the minimum number of steps -exactly n-1 nodes are expanded . 
¡¡table i indicates the computational effort exerted by a . three complexity indicators were employed: e  the number of nodes expanded  g the number of nodes generated  and b  the number of times a  had to backtrack  switch paths . table 1 gives the actual cost of the solutions obtained and f'  the guarantee returned by a€ of the degree of optimality of the solutions. all figures 
 € excepted  are given relative to their values for f = 1  corresponding to what a* would do . the numbers in parentheses give the standard deviation   for the corresponding mesures. 
   notice that for all indicators  table i   computational cost decreases rapidly withincreasing €  from exponential complexity for t = 1 down to linear complexity for   = 1   . the obtained results compare favorably with those of  pearl and kim  1  fig. 1  shown as e1 in table i below. interval  1 1 . in 1 test cases  as soon as 
  exceeded .1  no backtracking was ever performed. our results for number of nodes developed are similar to those of pearl  c.f. his fig. 1 ; e = 1 and 1 for   = .1 and .1 respectively. 
these results compare a€ favorably to a* and to 
a€ *. we are currently conducting further tests on 
tsp and other search problems in order to experiment with the search strategies discussed in section i i . 
