 
the ability to generalize remains one of the central issues of concept learning. a general generalization algorithm -the candidate elimination algorithmexists but practical applications of this algorithm are still limited  due to its low convergence. the issue has shifted to the design of a useful  bias  limiting the size of the version space. this paper proposes a new kind of bias  called empirical bias  and a new general algorithm  ice  for generalization in presence of bias. this proposition is founded on the concept of focus set  which provides a very flexible way to express expectations or constraints on the space of generalizations. 
1. introduction 
we assume the reader to be familiar with the main concepts elaborated by mitchell  in his work on generalization as a search in a version space  vs . we just want to emphasize in this section the main limitation of the approach and the need for a more constrained algorithm. 
1 the issue : the bias 
the candidate elimination algorithm  cea  is a double constrained search in a space lying between and represented with two sets of generalizations: a boundary set s of most specific generalizations and a boundary set g of most general generalizations. the main limitation we want to address in this work is the constraint on acceptable generalization languages due to the quadratic dependency of the complexity of the algorithm to the size of the boundary sets. due to the cost of an elementary treatment for a single instance  the size of s and g becomes too large for applications where the version space contains large  slices   that is large sets of   uncomparable elements 1  sec ′ 1 . the issue is to 
note that it only weakly depends on the branching of the 
te1  1  1 1 
impose further restrictions on the shape of the version space. 
　several means have been explored in order to express constraints on the class of admissible and inadmissible patterns for generalizations. the main ideas have been to take into account a theory of the domain of learning  michalski 1   to introduce a bias on the language of generalization  and/or. on the generalization operators  and then refine it by necessity  utgoff  1   and to specify a more constrained preference criterion  blumer  1; nicolas  1 . 
1 what is needed 
all these attempts share the common characteristic to introduce formalized knowledge in the generalization mechanism. though it is the neatest way to focus the search mechanism  we claim that this represents only one part of the control knowledge. we need a more empirical ability to express constraints  according to the empirical approach chosen in generalization from examples. concept learning is dedicated to a performance element. this performance element may have some ideas about the shape of the concept definitions it needs. however  it is not coherent to ask for this knowledge only in an abstract way  since it is assumed that the main work of the generalization algorithm is abstraction on the data of the performance element 
we illustrate our purpose on several cases. 
　 assume the performance element to be a human specialist even if he does not know the precise definition  nor formalize enough knowledge to help the search of the concept he is looking for  he seldom has no idea of it. he can generally give an approximate definition of this concept this generalization is not consistent in most cases but provides useful hints on the type of generalizations that the specialist is looking for. this may be considered as a form of learning by instruction. 
　 when the performance element is automated  it may also be the case that approximate generalizations are available. 
partial ordering  . 
	nicolas 	1 
our ideas originate in fact from such a situation. we are tackling a  real world  problem  the conceptual clustering of phlebotomine sandflies of french guiana  lebbe et al.  1 . on the one hand  we need a rich representation language which precludes the simple application of the cea. on the other hand  we use a statistical module which is able not only to build some clusters  but also furnish some clues concerning the form of each cluster characterization. it provides information about attributes involved in the emergence of a cluster which can be combined to build some approximate generalizations. 
　 finally  another way to generate approximate descriptions is to use a generalization algorithm with a strong bias on a training set of instances. the bias leads on one side to a search in a high level abstracted space and ensure a quick convergence of this search. on the other side the learned generalizations are rough  they are likely to become inconsistent with the treatment of more instances  and represent only an indication of the desired ones. note that if we know some means to exploit these approximations  this process may be iterated. for example  we may first restrict the generalization langage to kcnf descriptions  with k fixed  and progressively increase k by a constant step p. 
1. the version space revisited 
we 	assume 	now 	that 	approximate 	descriptions 
 generalizations  are available as an input for the c.e. algorithm. the issue then becomes : how to extend it in order to make profit of these supplementary data without loosing the good properties of the original algorithm  the two subproblems are: 1  how to formalize an approximation and 1  how to modify the original algorithm. the development of the first point has led to the concepts of focus set and seed set  which are presented in this section. the second point is the subject of section 1. 
the focus set is a set of consistent descriptions 
 approximations . the elements of the seed set may be not consistent. 
1 definition of a focus set 
　we introduce in this paragraph the basic concept of our extension. the version space approach requires the initialization of two sets of descriptions  namely the s-set and the g-set. we require the consideration of a new set of descriptions for our algorithm  called the focus set  f-set . this set is intended to provide a structural bias on the version space. indeed  a version space may be viewed as a collection of paths between some element of s and some element of g. the focus set restricts the allowed paths by giving intermediate nodes along these paths. 
learning and knowledge acquisition 

these definitions may be naturally extended for sets. 
   the focus set is simply a set of descriptions which have to belong to the initial vs  we say vs definable   such that every concept description c  a target generalization  is  attainable  by at least one of these  say fbest . we now  make precise this concept of attainability in a vs. the casual reader may skip to the proposition without inconvenience. to be attainable means at least to be comparable with. however  we need a supplementary property  governing the consistency of the whole set. this property will guarantee  during the search for generalizations  that no interaction between two or more elements of the focus set precludes the paths leading from fbest to c to be eventually followed by the search algorithm. for this purpose  we introduce the notion of representativeness. a set of descriptions f is representative of a set of concepts if  when there exists a path from a element of f to a concept  there exists a no-deviation path between these two elements. that is  a set of descriptions f is representative of a set of concepts if  for every path starting at the same element of f  one going through c and the other  the deviation  through another element f of f not comparable with c  there exists a third distinct element of f which is comparable with c and not comparable with f. 
formally  it leads to the following definition. 
　definition let c be the set of target  consistent with any proposable instance and vsdefinable  concept descriptions. 

1 interpretation of a focus set 
we now want to show that  if the focus set is an extension of the boundary set concept  the size of the version space itself  with a natural interpretation of this focus set  may only be reduced with the input of such a set. this is because it leads to an extension of the constraints on the set of consistent generalizations. we define for this purpose the interpretation of the focus set in terms of a search inside a version space. 
we first recall the interpretation of the boundary sets in the 
candidate elimination algorithm of mitchell  with the point of view of focus sets. consider the union of boundary sets  characterizing a version space. it is a focus set  propositions 1 . now  the goal of the cea. may be defined in terms of the maintenance of this focus set with the input of new instances. the updating method is a kind of bidirectionnal search  with the following meaning: in order to maintain one of the boundary sets b  consider all the paths from each inconsistent element b of b to the consistent elements of comparable with b. 
　we keep this same point of view to describe the search when considering an additionnal focus set f. then  the set of 
generalizations to be maintained becomes  the updating method is a bidirectionnal search  with the following meaning: in order to maintain one of the boundary sets b  consider all the paths from each inconsistent element b of b to the elements of comparable with b and 
 nearest  from b  that is most specific if b=s or most general if b=g . otherwise stated  we do not explore the paths between two comparable points from the boundary sets but we explore the paths between two comparable points from the focus set. since the focus set is representative  we ensure that eventually  a path leading to a concept definition will be explored. 
　we end this section with an illustration of the decrease of complexity of the search on a simple but enlightening example. assume the language of generalizations to be representable with a boolean lattice of boolean functions with n variables  atoms . the initial version space is characterized with s = {false} and g={true} and contains exactly  consistent generalizations  these can be built from the variables and the connectives . assume the focus set f to be composed of elements involving only m variables  then it is easy to show that the search for consistent generalizations will be focussed such that none of the elements of the inner version space  which is the boolean lattice generated from the n-m other variables  will be explored. that is  the size of the search space is reduced by at least . a related problem consists of determining how far we can go in the reduction  given a suitable focus set. in our example  the answer is quite simple. the maximum of reduction is reached when there is only one path in the version space to be considered. in the boolean lattice  all paths are 1n long. so  one can ultimately reduce the size of the search to 1 n   that is  the elements of the focus set itself. more generally  if the focus set contains two adjacent nodes of the lattice  the space is reduced by 1n-1 elements. 
1 definition of a seed set 
　one of the basic points common to all generalization methods consists of effectively producing the set of concept definitions  starling from a well-chosen set of descriptions and then progressively refining them by application of generalization and specialization operators  in order to achieve the selection criteria. the focus set provides such a set of descriptions. 
　however  providing a set  whose elements belong to the version space  are representative of the concept descriptions  and differ from s or g  may be too strong a constraint in practice. this motivates the introduction of the seed set. this definition is much less constraining than the previous one. it is intended to formalize the intuitive notion of  generalization pattern . the aim of the seed set is to capture the minimal assumptions to be made about such models or patterns  in order to guarantee that a solution can be reached. the next section provides a definition of the projection of the seed set on the version space  leading to the production of an associated focus set 
definition let d be a set of descriptions 
d is a seed set iff d and  initial version space  are comparable. 
note that this condition is not really restrictive  since d 
may be completed with as many elements of  as necessary to obtain the comparability. the definition no longer requires that the elements of the set be consistent or representative of the concepts. however  even if it does not appear in the definition  recall the basic underlying assumption that elements of the seed set share strong links with the target generalizations. 
　the issue then is to map the seed set on to a focus set  taking advantage of this assumption. the idea is to keep the same notion of proximity between descriptions as in the generalization algorithm itself. that is  a description d j is close to a description d if they are comparable and a description d1 is closer to a description d than a description d1 if d1 is between d1 and d. 
	nicolas 	1 
1 seed set and focus set 
　these last definitions rely on the focus set and the seed set concepts in such a way that the seeds are minimally generalized or specialized  closest to their original description . the notion of projection on a version space reflects these minimal refinements. f1 is proposed first. if it is not sufficient to find the concept c  then the new projection fj is produced and the algorithm restart from these new approximations  etc...the focus set is defined as the convergence of a sequence of trials f1  f1 f1... see figure  progressively loosening the strength of the links between the seed set d and its projection  in the direction of s or g   until the boundary s or g is reached. 


　the next proposition reflects the desired properties of this definition. 
　proposition for every seed set d  there exists a set e s.t the closest focus set f from d w.r.t. e is a focus set 
　proof: it is sufficient to choose for e the set of all descriptions. in that case  since d and are 
comparable  the projection reduces to focus set 
　however  this choice for e represents a worst case  where the search occurs in the whole initial version space. the seed set is not pertinent  i.e.  it conveys no additional information with respect to  the idea of the algorithm is to start from a closest focus set f where   nearest  from d  and in case of failure  to restart with e:=f. more generally  each time a failure occurs  e is replaced with the union of the previous e and the closest focus set w.r.t this previous e. 
note that this search for the closest focus set can not be 
learning and knowledge acquisition 
stated as a search in a version space  as there arc no instances involved here. 
　1 the informed candidate elimination algorithm 
　we designed a generalized version of the ce a  able to take into account the seed set as a new source of knowledge  this is not concurrent with the inclusion of other types of control knowledge . the cea has two interesting properties that we need to preserve. first  its strategy is a bidirectionnal search. and second  if a concept is sogo-definable  i.e.  in the original version space   then it will be found eventually. since we have no idea of how accurate are the elements of the focus set for the concept at hand  the algorithm has to include a  repair  strategy  loosening the bias until the production of consistent generalizations for the whole set of instances. in fact  if the focus set is of no help for the definition of the concept  the behaviour of the algorithm converges towards that of the cea. 
　the general schema of the algorithm remains unchanged with respect to mitchell's work. however  taking into account the duality of s and g and in order to emphasize the parameters and components of the generalization process  we present a parameterized form of this algorithm. a more detained algorithm may be found in  nicolas  1 . 
 proc informed candidatc elimination  s1 g1 f1   reset-instances; 
until single solution or no solution or no more instances 

do 
i:= readjnstances; case type i  in 
positive -  remove in boundary  incomplete  g  update boundary incomplete  s  remove  equivalent or  notj iefeited  s  
negative -  	remove in boundary weakly  inconsistent  	s  update boundary weakly jnconsistent g  remove equivalent or not jjrefemed  g  
end case end untill 
exit no generalization: if  
%unsucessfull search in the whole version space so go then print no generalization else fl:=repair focus set fo so go ; 
　　　　　informed candidate elimination so gojfl  exit single generalization: print learning completed exit no more instances: print current version space 
end proc 
　the cea requires the specification of two types of transformations on generalizations : most specific generalizations and most general specializations. note that our algorithm requires the definition of two new simplified types of transformations on descriptions during the repair strategy. the specification of a projection is indeed a search for most specific generalizations and most general specializations  but it does not involve any instance. an illustration of these new operators is given in the next section. 
　1. example: a feature value intervals space 
　this problem derives from the one described in  mitchell  1 . it must be clear that it remains didactic. 
　the goal is to define a concept  whose characteristics are attributes taking their values in the domain of integerss. an instance is a vector  conjunction  of attributes with particular values. assume here that the descriptions are represented with only two attributes. following the single representation trick  instances arc expressed in the same langage as generalizations. assume the chosen representation to be: 
for example  the instance  1  is represented as  
　　　　the matching predicate and the order of specificity between concepts is simply the inclusion relation between couples of intervals. for example  the concept  
　　　　is more specific than due to space restrictions  we can not give the diagrams illustrating the behaviour of the algorithm. intermediary and final results of the algorithm are presented with the previous syntax  but we strongly encourage the reader to draw a graphical representation of the concepts. 
assume the following scenario for the initialization part. 
the algorithm reads a first positive instance  1 . 
g is the singleton 
s is initialized to the singleton 
　furthermore  we dispose of two approximative models  leading to the seed set 

　then  a procedure computes the closest focus set from d and leads to the following result  notation: each element of the focus set has the form identification number  description  list of immediate predecessors  list of immediate successors  list of comparable elements of s  list of comparable elements of g  : 

　elements 1 and 1 of f are just the elements of s and g. the other elements are the least definable generalizations f of each element of d  that is  such that  
　we describe now the behaviour of the algorithm with the input of instances in the following order  + for positive instances  - for negative instances : 

the computation gives for s et g  after step  instance  1: 

s has  absorbed  the focus points and contains fairly general concepts. after step  instance  1 : 

　since the next positive instance is not included in any element of g  the algorithm find no generalization at step 1. the procedure repairjbcus.set then computes a new focus set. in this example  assume the generalization operator of the projection is replacing one of the bounds of the elements in the focus set with the corresponding bound of a comparable element of the boundary sets. we are sure that the process converges in at most 1 calls for the repair procedure  one for each bound in a couple . each time  instances are considered from the beginning. 
the new focus set contains the following concepts: 
　with this new focus set  learning may be completed after step 1 and leads to the following boundary sets: 

　the convergence of the algorithm is remarkable in this case  where we only need 1 instances. without d  the s set would be unchanged  but the g set would be 

1. related work 
many studies involving the bias problem have been developed since mitchell emphasized the issue  mitchell  1   the common goal shared by all these studies is to hasten the convergence of the boundary sets. if one more deeply investigates the existing methods  one can divide them further in three main directions: 
　1  definitions of constraints or properties on the set of preferred generalizations. bias is included in the selection criteria. michalski handles for this purpose a lexicographic evaluation function concept  michalski  1   to be defined at a user level. at a theoretical level  we proposed some general logical criteria  nicolas  1  for the selection of generalizations when the representation language includes negations and disjunctions. 
　1  multiplication of the effect of an instance. bias is included in a theory. hirsch's approach  consists of using a theory able to determine for some instances their type 
	nicolas 	1 
 positive or negative . this theory allows the generalization of the explained instances with an ebg technique and to generate in such a way multiple instances of the same type and same explanation of their type. a related technique is the perturbation  kibler and porter  1   wich automatically explores the  nearest  instances from the observed instance. 
　1  change of the representation language. bias is included in the language. utgoff's algorithm  utgoff  1  initially searches for generalizations in a restricted space defined with a restricted language. in case where no generalization is found  the language is augmented with new terms which are linked to the previous one so as to produce a generalization. russel and grosof  russel and grosof  1  use a determination knowledge base  expressing dependencies between concepts  to build a pertinent version space for the concept to be learned. 
　our approach may be viewed as a fourth direction  striving towards the inclusion of bias in the search control sets which are the boundary sets. one of the basic assumptions of learning from examples is that the knowledge is easily accessible at a specific   instanced  level  but hard to produce at an abstracted   theoretical  level. our proposition is based on the same assumption. we require the bias at an empirical level. 
　rendell's classification of biases  rendell  1  considers three kinds of biases. fixed bias is defined once for all  independently of any application. parameterized bias is adjusted for some application by the user. variable bias may evolve during learning  according to its results on some particular problems. the kind of bias we propose is basically a parameterized one. however  the focus set may be not only user defined  but also automatically generated from the 
results of another module  like a data analysis tool . this initial bias may be then refined  if no satisfying generalization is found. consequently  the bias possesses also a variable characteristics. 
1. conclusion 
we have proposed a general framework enabling the development of further studies in the field of constrained generalization algorithms. it shifts the limitations on  blind  inductive learning  dietterich  1  to the issue of building interesting approximations of the concept to be learned. 
　the worst case complexity remains unchanged in terms of s and g but it has to be defined in terms of the structure of the focus set or  even better  in terms of the structure of the seed set a basic interesting property is that the growth of the focus set is solely dependent on the growth of boundary sets: the other elements can just be discarded by the algorithm. the other main issue that remains to be explored is the design of an algorithm optimizing the adaptation of the bias  that is  choosing the new elements in the focus set  when the ice 
learning and knowledge acquisition 
algorithm fails to produce a generalization. a related question occurs with the design of the seed set. actually  it seems reasonnable not to produce this set once for all  but iteratively  with progressive refinements towards more and more detained approximations. 
