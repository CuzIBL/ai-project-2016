
often an agent that has to solve a problem must choose which heuristic or strategy will help it the most in achieving its objectives. sometimes the agent wishes to obtain additional units of information on the possible heuristics and strategies in order to choose between them  but it may be costly. as a result  the agent's goal is to acquire enough units of information in order to make a decision while incurring minimal cost. we focus on situations where the agent must decide in advance how many units it would like to obtain. we present an algorithm for choosing between two options  and then formulate three methods for the general case where there are k   1 options to choose from. we investigate the 1-option algorithm and the general k-option methods effectiveness in two domains: the 1-sat domain  and the ct computer game. in both domains we present the experimental performance of our models. results will show that applying the 1-option algorithm is beneficial and provides the agent a substantial gain. in addition  applying the k-option method in the domains investigated results in a moderate gain.
1	introduction
when making decisions  automated agents need to decide which heuristic function or strategy would best assist them in achieving their objectives. in particular  when an agent possesses several alternatives to tackle a problem  and the best alternative is unknown in advance  its success depends on choosing the most beneficial alternative. in  azoulayschwartz and kraus  1  a formal model for solving a problem of choosing between alternatives in e-commerce have been presented. they formulated the problem in terms of units of information and agent's utility  and assumed that the agent should decide in advance on how many information units to obtain about each alternative. moreover  the agent could not change its decision during the information obtaining process. then they suggested an algorithm for acquiring

 
¡¡¡¡this work was supported in part by nsf grant no. iis-1 and by isf grant no. 1. sarit kraus is also affiliated with umiacs.an optimal amount of information units  through which the best alternative could be determined.
¡¡in this paper  we generalize their model to suit domains that involve choosing between heuristics or strategies. azoulayschwartz and kraus' assumptions hold in this research as well: the agent chooses the best alternative in advance and is able to hold onto its decision for a long period of time. this approach is valid in many situations  especially when acquiring information is costly. in these cases  the agent would like to execute its decision-making process once and apply the process's output thereafter. moreover  once the agent realizes the decision made does not produce satisfactory results at any given time  it may re-execute the decision-making process  and proceed with its new output. nevertheless  we are more concerned with the applicative rather than the theoretical aspect of the model. indeed  the model suggests many implementation challenges. we present two examples  from the heuristics domain and from the computer games domain. in both domains we compare the utility of the automated agent with and without using our algorithm. results will show that an agent that applies our model is more likely to choose the best option among the group of possible options. furthermore  it does not waste too many resources during the decision process  and thus  its overall utility increases.
¡¡the paper first briefly presents the model constructed by azoulay-schwartz and kraus. it then introduces the algorithm for choosing between two options  which was formulated by azoulay-schwartz and kraus  and thereafter the three generalized methods for cases where there are more than two options. the first of these three methods is a theoretical one which was formulated by azoulay-schwartz and kraus. we generalized their ideas to form the other two methods  which are much more feasible in real-time environments. we proceed by describing the experimental design and analysis. in section 1 we describe the first domain we considered - the 1-sat domain  and in the following section we describe the other domain  the ct game domain. in both sections we present the experiments we conducted and the outcomes of applying the 1-option algorithm and the k-option generalized
models. in section 1 we compare our work with previous researches. the concluding section discusses the experimental results and suggests several avenues for future research.
1	model construction
a risk neutral agent has to choose a heuristic from k heuristics to solve m problems. after choosing alternative i  the agent will obtain a value of xi which is unknown in advance. we assume that xi is normally distributed  that is  xi ¡« n ¦Ìi ¦Òi1 . in addition  the agent does not know ¦Ìi  but it has some prior beliefs about its distribution. we further assume that ¦Ìi also follows the normal distribution - ¦Ìi ¡« n ¦Æi ¦Ói1 . we assume that ¦Òi  ¦Æi and ¦Ói are known and reflect the agent's believes. otherwise  if ¦Òi is unknown  the agent can use the student distribution. in addition  if ¦Æi and ¦Ói are unknown  we assume that the agent can estimate their values based on its beliefs as to the possible values of ¦Ìi. furthermore  in future work we will investigate the effect of eliminating normal distribution assumptions.
¡¡the agent has ni units of information about each alternative i  with an average value of xi. for instance  in the e-commerce example presented by azoulay-schwartz and kraus  a customer would like to buy an item available from two suppliers. the customer collected n1 and n1 customerimpressions from friends or from the web about these suppliers  in order to be able to decide between them. the average impression was calculated to form x1 and x1.
¡¡the agent is able to obtain a combination of comb =  m1 ... mk  additional units about the different alternatives. however  this operation is costly  either in time or in direct costs. given 1   ¦Ä ¡Ü 1  the discount time factor  the cost model  the list of alternatives and the parameters for each alternative  the agent decides whether to proceed with the information it has so far  or to accumulate additional information units. one could simply use a greedy algorithm in order to find the optimal allocation of additional experiments. in each step the option which proved to be better so far is chosen  executed and its result is added to the data accumulated. the greedy algorithm stops when there is no additional sample that increases the expected utility. however   azoulayschwartz and kraus  1  showed that the greedy algorithm is not optimal. the reason being that there may be situations where obtaining one unit of information about a particular alternative is not worthwhile since it will not lead to a change in the decision  but obtaining two and more may be worthwhile. in conclusion  first we will present an algorithm for k=1 heuristics  or strategies  and then generalize it for k   1.
algorithm 1 the hsc algorithm
for ma from 1 to ¡Þ do for mb from 1 to ¡Þ do calculate fchange. calculate utility.
if utility ma  1 mb  1    utility ma mb  and
utility ma  1 mb  1    utility ma  1 mb  1  then return  ma  1  mb  1 .
add mi experiments to alternative i.
choose the best alternative according to the new results
is fchange ¦Ìa ¦Ìb ¦Òa ¦Òb xa xb na nb ma mb  = pr z   z¦Á   where z is a random variable  having the standard normal distribution and z¦Á represents the first value where b outperforms a  see  azoulay-schwartz and kraus  1  for more information . then  the expected benefits from obtaining additional ma and mb is a function of fchange and is denoted by benefitsr r  ma mb . the agent's utility is utility ma mb  = ¦Ìa ¦Ìb benefits ma mb  ¡¤ ¦Äma+mb cost ma mb d¦Ìbd¦Ìa. the specifics of the benefits function depend on the cost model. later we will show the specifications for three different cost models. the agent's goal is to find the pair ma and mb  that yields the highest utility ma mb  value. by considering all possible combinations of information units about a and b  the optimal combination is achieved. algorithm 1 presents the heuristicsstrategies-choosing  hsc  algorithm for the two heuristics  or strategies  case.
proposition 1. the hsc algorithm stops and returns ma and mb that maximize the utility function.
proof. the intuition for the correctness of the algorithm lies in the form of the utility function. since it consists of the multiplication of similar normal distribution functions  its form is gaussian as well  with only one maximum point. as soon as the algorithm finds that point it stops and returns ma and mb.	
1 choosing between multiple heuristics there are k   1 alternatives  and the agent can obtain up to m-1 units of information about each of them. we suggest three models:
1. the statistical model in which we considered the agent utility function given a combination of information units as suggested by  azoulay-schwartz and kraus  1 . nevertheless  in order to find a combination of additional units for each alternative  one must solve a quadruple integral. as a result  the model proved to be inapplicable for our purposes;
1. the binary tree model where the alternatives construct the leaves' level of the tree. we apply the hsc algorithm for each pair. the winning alternative goes up a
level in the tree. we repeat the procedure until the best alternative reaches the root. this model deviates from our initial assumption that all experiments are conducted prior to decision-making. the binary tree model is an intermediate approach between the greedy algorithm and the hsc algorithm. however  since the comparisons are done in pairs  a large number of experiments may be executed for alternatives that would not have been considered at all when regarding all the alternatives together;
1. the fixed number of experiments model - in which we
distribute a fixed number of experiments denoted by  n  between the different alternatives using the information we have so far. for example  suppose we have five alternatives to choose from. alternative 1 produces the best results and alternative 1 produces the worst ones. thus  it is worthwhile to execute more experiments of alternative 1 than of alternative 1  as less time will be wasted on fewer alternatives while accumulating information on all possible alternatives. following preliminary experiments  we set n to be four times the number of alternatives. thus  it is large enough to accommodate experiments of all alternatives and nonetheless  economical in the number of additional experiments. consequently  for the example of five alternatives n=1  and we suggest that the best alternative be assigned one third of n. the rest will be assigned two thirds of the remaining experiments: ;
.
this ad-hoc approach will be refined in future work.
1	experimental design and analysis
our investigation using the hsc algorithm was conducted in two domains. the first  a classic np-complete problem  i.e. the 1-sat problem. to demonstrate the hsc algorithm's vast usage possibilities  it was employed in two different cost models:
1. minimal time  mt  scenario - the agent had to solve m formulas as quickly as possible;
1. maximal formulas  mf  scenario - the agent had t units of time to solve as many formulas as possible.
¡¡we chose three best-known search algorithms as the agent's possible heuristics: greedy-sat  gsat   simulated annealing  sa  and gsat with random walk. the agent had to perform its task by using one of these search algorithms. the second domain was a computer game  the ct game  for game specifications see  grosz et al.  1  . here  the agent's task was to maximize its game score. to this end  our agent had seven different strategies to employ against its opponent agent in the game  and it had to choose the best strategy against the opponent. the experiments compared the agent's achievements without using the hsc algorithm and its achievements after executing this algorithm. we also compared these results with methods where a large set of additional experiments had been conducted.   selman et al.  1  for instance conducted an unlimited number of experiments in order to choose the best heuristic . these methods found the best heuristic more frequently than our algorithm. nevertheless  though the decisions made were slightly improved  given our cost model  the approach applied by selman yielded a huge loss in the number of experiments executed. for space reasons  these results are not detailed here.
¡¡we first executed preliminary runs to prepare an offline database. the database comprised vital information  namely  which option is the best  and moreover  the mean quality of each option. our hypothesis was that the execution of the hsc algorithm will indeed benefit the agent. moreover  by executing a small number of additional experiments the agent's utility will increase.
1	the 1-sat domain
we assumed that each truth-assignment flip takes one unit of time. thus  the mt scenario in this domain is a minimum problem: the agent must solve m formulas within a minimal number of flips. without loss of generality  when comparing two possible heuristics  we assumed that the better heuristic after na experiments is heuristic a. as a result  the agent would have chosen heuristic a for the m formulas. in that case  it would have taken the agent the average number of flips multiplied by the number of formulas  m ¡¤ ¦Ìa  to solve m formulas. after the hsc algorithm is executed  the total number of flips will be assembled from 
1. the number of flips in the ma + mb extra experiments yielded by the hsc algorithm;
1. the number of flips to solve the remaining m   ma   mb formulas in case heuristic b prevails;
1. the number of flips to solve the remaining formulas in case heuristic a still leads.
¦Ä in this case is 1  since the agent solves the required formulas in the additional experiments. accordingly  the utility functionr r in this domain is utility ma mb  = ¦Ìa ¦Ìb benefitsr r ma mb  ¡¤ ¦Äma+mb   cost ma mb d¦Ìbd¦Ìa = ¦Ìa ¦Ìb m ¡¤ ¦Ìa   fchange m  ma mb ¡¤¦Ìb   1 fchanger r   m  ma  mb ¡¤¦Ìa  ma¦Ìa+mb¦Ìb d¦Ìbd¦Ìa = ¦Ìa ¦Ìb m ma  mb ¡¤fchange¡¤ ¦Ìa  ¦Ìb  mb ¡¤ ¦Ìb  ¦Ìa d¦Ìbd¦Ìa. the agent searches for ma and mb that maximize this equation.
¡¡the mf scenario in the domain is a maximum problem: the agent must solve as many formulas as possible within t flips. considering heuristic a is the better heuristic after na experiments  the agent would have solved t/¦Ìa 1-sat formulas. however  if it would have used the hsc algorithm  the total number of 1-sat formulas would have been the summation of:
1. the number of formulas solved during the extra experiments yielded by the hsc algorithm  ma + mb;
1. the number of formulas to be solved using heuristic b  if it prevails  fchange ¡¤  t/¦Ìb ; 1. the number of formulas to be solved if heuristic a is not changed   1   fchange  ¡¤  t/¦Ìa .
here  ¦Ä is 1 as well  and cost ma mb  consists of the number of flips lost in each case  namely cost ma mb  = fchange ¡¤   ma¦Ìa + mb¦Ìb /¦Ìb  +  1   fchange  ¡¤
   ma¦Ìa + mb¦Ìb /¦Ìa . lastly  the hsc algorithm will yield ma and mb that will maximize utility ma mb  = t

1	implementation issues
we constructed 1 different 1-sat formulas  each consisting of 1 different variables and 1 clauses. the formulas were tested in advance for the existence of a valid truth assignment. the gsat algorithm restarted with a new random truth assignment after 1 flips  and the total number of restarts was set at 1. the temperature of the sa algorithm was set at 1%  and the experiment was stopped after 1 unsuccessful flips. the random walk with gsat was implemented using three different probabilities of walk: 1% 
heuristicgsatsimulatedrandomrandomrandomannealing1%1%1%flips #11111table 1: offline results of number of flips  in thousands 

figure 1: 1-sat % of choosing the best heuristic without the hsc algorithm  with the algorithm in the mt and with the algorithm in the mf scenarios
1% and 1%  the three different heuristics of this algorithm will be denoted by random 1%  1% and 1% . in each experiment the maximal number of flips was set at 1 and the number of restarts was set at five. we established all the algorithms' parameters such that  on the one hand  they will have a reasonable potential to solve any formula  but on the other  their execution time will remain low.
¡¡the preliminary experiments executed each of the five heuristics on the 1-sat formulas. table 1 presents the average number of flips each heuristics obtained. the parameters in the equations of section 1 were estimated using the data accumulated during the preliminary experiments stage  since this data comprises our whole population. thus  the apriori parameters ¦Æa and ¦Æb were estimated with the mean of all the offline results of all heuristics  1   and ¦Óa and ¦Ób were estimated by their standard deviation  1 . in addition  ¦Òa and ¦Òb were estimated by heuristic a and heuristic b standard deviations  respectively. a sensitivity analysis of these parameters determined that the results were not sensitive to changes in the parameters. in the analyses we varied the values of ¦Òa and ¦Òb  of ¦Æa and ¦Óa and of ¦Æb and ¦Ób and re-executed the experiments. except for several extreme situations we obtained similar results for the specific values described above.
¡¡furthermore  in each experiment na and nb were set to five  m to 1  and t to 1. that is  in each experiment of both scenarios  five formulas chosen randomly from the 1 formulas mentioned above  were solved first  and were used as the agent's preliminary units of information. then  it had to decide with which heuristic to proceed solving the remaining 1 formulas. we allowed the agent only five units of prior information since this is often the case in the real world - agents need to base their decision on only few observations due to uncertainty in their environment or due to a cost associated with the information.

figure 1: 1-sat mt scenario influence of m
1	1-heuristic experimental results
according to the results presented in table 1 we compared  1 
random 1% to random 1%;  1  sa to gsat;  1  random 1% to sa. other pairs  although feasible  would not effectively demonstrate the hsc algorithm's performance: the difference between their means is too large for the algorithm to improve. from empirical results we know that the algorithm will not advise on further experiments in those cases  and simply yield the better heuristic according to the information the agent has. thus  it will not endure any loss for these other pairs. the offline results revealed that the best heuristic in each pair was random 1%  sa and random 1%  respectively. the total number of experiments in each scenario was 1  1 experiments per pair.
¡¡in the mt scenario  when the agent did not use the hsc algorithm  it always chose the heuristic with the better 1-game average. in contrast  when the agent applied the hsc algorithm  it mostly executed more experiments for both heuristics  and then either changed its mind or continued with the better 1-game heuristic. on average  only 1 additional experiments were executed for each heuristic. figure 1 summarizes the percentage of experiments in which the agent chose the best heuristic  with and without the hsc algorithm across the three heuristic pairs. as we expected  the algorithm improved the agent's decision-making  and directed it to the best heuristic in 1% of the experiments. moreover  without the hsc algorithm  the agent would have chosen the best heuristics only in 1% of the experiments. this improvement resulted in a gain in the number of flips: the average number of flips for 1-sat formulas without the algorithm  1 million  was significantly higher than with the hsc algorithm  1 million  wilcoxon pv=1 .
¡¡to show the influence of m on the average gain  we executed 1 additional experiments  with 1 different values for m. figure 1 summarizes the average gain per formula solved for m varying between 1 and 1. as expected  the average gain is in linear relation to the size of m  since the hsc algorithm poses a greater potential benefit as the number of formulas increases.
¡¡the agent's task in the mf scenario was to solve as many formulas as possible within 1 flips. here  on average nine additional experiments were executed for each heuristic. figure 1 presents the percentage of experiments in which the agent chose the best heuristic  with and without using the hsc algorithm  note that the percent of without the hsc algorithm is the same as in the mt scenario . in this scenario

figure 1: 1-sat mf scenario influence of t
as well  the agent succeeded in choosing the best heuristic more frequently by using the algorithm  1% vs 1% . consequently  it managed to solve six more formulas on average: 1 formulas with the hsc algorithm in contrast to 1 without it. to demonstrate the influence of t  we executed an additional 1 experiments for seven different values of t varying from 1 to 1. as the average gain is dependent on t  the larger the t the larger the gain   we normalized the gain by calculating the average gain per 1 flips. thus  when t was set to 1 the average gain was divided by 1  and when is was set to 1 the average gain was divided by 1. figure 1 summarizes the average gain per 1 flips. it supports our hypothesis that the gain of using the hsc algorithms increases as t increases. it seems that the increase is very steep at the beginning  but when t   1 it becomes moderate.
1 k-heuristic experimental results
we tested the generalized k-heuristic model using the binary tree algorithm and the fixed number of experiments algorithm. the binary tree was built according to the pairs we described in section 1. that is  the binary tree's leaves were random 1% and random 1%  then gsat and sa and finally random 1%. the fixed number of experiments procedure was executed according to the description presented in 1. we repeated the procedures 1 times for both algorithms.
¡¡table 1 summarizes the number of times  of the forty experiments  the agent chose each heuristic with and without each algorithm for the mt scenario. as expected  both algorithms improved the agent's decision-making  in the binary tree algorithm by 1% and in the fixed number of experiments algorithm by 1%. nevertheless  the gain in the agent's utility was not very impressive in the binary tree case: on average 1 flips were lost per experiment due to the execution of heuristics that although seemed promising in their pair  were in fact time consuming in the overall aspect. on the other hand  in the fixed number of experiments case 1 flips were gained per experiment  the average total number of flips per experiment was 1 . the number of additional experiments were 1 for each heuristic on average. due to the disappointing results of the binary tree method  for the mf scenario we executed only the fixed number of experiments method. here  on average 1 formulas more were solved per experiment when applying the algorithm.
greedysarandomrandomrandom1%1%1%without111with111greedysarandomrandomrandom1%1%1%without111with111table 1: mt scenario chosen heuristic distribution  with and without applying the hsc algorithm:  top  the binary tree results;  bottom  the fixed number of experiments results
1	the ct domain
we investigated the hsc algorithm in a two-player negotiation game that uses the ct game  grosz et al.  1 . in this game each player has a goal placed on the game board and certain resources to help it reach the goal. the players can exchange resources  and at the end of the game are assigned a score that corresponds to their performance throughout the game. during the game the agents may negotiate on resources  commit to resource exchanges and execute exchanges. however  the commitments made by an agent are not enforceable  and it might decide to back down on a commitment or even deceive an opponent agent by committing to an exchange it does not intend to keep.
¡¡in  talman et al.  1   an automated agent able to play repeated ct games was developed. the agent characterizes itself and its opponent in terms of cooperation and reliability. the cooperation trait measures the willingness of an agent to share its resources with others  whereas the reliability trait measures the agent's willingness to keep its commitments in the game. accordingly  the agent is capable of employing seven strategies  differing in the level of cooperation and reliability the strategy dictates. each strategy is suitable to a different type of an opponent  but the optimal matching scheme is unknown. for example  it may prove beneficial to play a low-reliability strategy against a highlycooperative opponent by deceiving it. on the other hand  perhaps a more logical strategy against such an opponent will be a high-cooperation strategy which promotes reciprocity in the game  and may benefit the agent in the long run. thus  in order to maximize the agent's score in the game  it must determine which strategy best-suits each opponent type. each strategy trait can be low  l   medium  m  or high  h   and a strategy will be referred to by its cooperation-reliability level  such as a low-cooperation medium-reliability strategy  or lm strategy . therefore  the seven possible strategies in the game are ll  lm  lh  mm  mh  hm and hh. the remaining two strategies  ml and hl are not applicable as an agent applying low reliability strategy has by definition a low cooperation level - when an agent almost never keeps it commitments  it is not willing to share its resources.
¡¡we expect that our suggested model will assist the agent in its decision-making  which will result in higher score in the game. following ni games of each strategy the agent executes the hsc algorithm and determines which strategy best-
strategylllmlhmmmhhmhhavg. score111111.1table 1: average score of each strategy against an lh opponent in the ct domain
suits each opponent type. the model adjusted to the ct game is similar to the 1-sat model except for the introduction of the time discount factor  ¦Ä . each game the agent plays with a non-optimal strategy results in a lower score. hence  each experiment added by the hsc algorithm brings about a time cost of ¦Ä. in contrast  cost ma mb  = 1 in this domain: the execution of ma + mb additional games does not incur an additional cost associated with ma and mb. in conclusion  for two strategies a and b  with a as the current winning strategy  having a mean score of ¦Ìa and ¦Ìb  respectively  the agent searches forr r ma and mb that maximize the utility function: ¦Ìa ¦Ìb fchange ¦Ìb   ¦Ìa  ¡¤ ¦Äma+mbd¦Ìbd¦Ìa.
in addition  we define the agent's gain from applying the hsc algorithm as follows: let scorea be the score of the strategy the agent chose before the algorithm and scoreb be the score of the strategy it chose after using the algorithm. then  its total gain from the algorithm is obtained by  scoreb   scorea  ¡¤ ¦Äma+mb.
1	implementation issues
for our study  we constructed 1 game scenarios  which varied the game boards  the resources each player was assigned and the dependency relationship between the two players. for evaluation purposes the hsc algorithm was executed to establish which strategy best suits an lh type opponent  best matches for the other opponent types are carried out in a similar manner . to this end  we first executed all 1 games  in which the agent played all seven strategies against an lh type opponent. the results served as the ct domain offline database as shown in table 1. interestingly  the lm strategy produced the best results. in addition  we set ¦Ä at 1  to allow a certain amount of exploration. we later varied ¦Ä and investigated its influence on the algorithm's performance.
1	1-heuristic experimental results
in light of the results in table 1  we compared strategies:  1  ll and lh;  1  lm and mh;  1  mm and hm. other pairs can be compared in a similar manner as long as the difference between them is not too diverse for the hsc algorithm to introduce an improvement nor a loss. each comparison was executed 1 times  totaling 1 experiments. in each experiment  our agent played five 1-player ct games against an lh opponent applying each strategy. then  the agent executed the hsc algorithm  which yielded the additional number of games of each strategy the agent needs to play. on average only three games of each strategy were additionally executed. figure 1 summarizes the percentage of experiments in which the agent chose the best strategy  with and without using the hsc algorithm across the pairs of the three strategies. by using the hsc algorithm  the agent was able to apply the better strategy in 1% of the games  whereas without the hsc algorithm it would have applied the better strategy in only 1% of the games. nevertheless  as we anticipated  when the

figure1: ct%ofchoosingthebeststrategywithandwithout the hsc algorithm

figure 1: ct % of correct re-evaluations for different ¦Äs
difference between the two alternatives was negligible as in the mm vs hm case  the algorithm did not improve the performance. this is because the two strategies are almost identical. thus  applying the algorithm resulted in no gain since it rarely decided to perform additional experiments. however  no loss was incurred as well  the average number of additional experiments was 1 for each alternative . accordingly  the agent's gain was on average 1. this gain is somewhat low because it is multiplied by ¦Äma+mb.
¡¡naturally  the time discount factor has a great influence on the hsc algorithm's performance. we envisioned that if we increase ¦Ä the hsc algorithm will shift more frequently towards the better strategy. to test our hypothesis  we randomly selected ten experiments from the experiments described above  across the three pairs  in which the worse strategy would have been chosen by the agent according to the preliminary experiments. we then re-evaluated the hsc algorithm's performance in those games with ¦Ä varying between 1 and 1. figure 1 presents the percentage of experiments the hsc algorithm pointed out the better strategy  for each possible ¦Ä.
1 k-heuristic experimental results
we tested the generalized k-heuristic model using the binary tree algorithm and the fixed number of experiments algorithm. in the binary tree  the pairs were established according to the settings described in 1: ll vs lh  followed by mm vs hm  then lm vs mh  and finally we added the hh strategy. then  we repeated the hsc algorithm bottom up. the fixed number of experiments procedure was executed according to the description in 1. we repeated the procedures 1 times for both algorithms.
¡¡the results in this domain were similar to those of the 1sat domain: both algorithms improved the agent's decisionmaking while keeping the number of additional experiments to a minimum  on average  1 additional games per strategy  - the binary tree algorithm by 1% and the fixed number of experiments algorithm by 1%. the agent's gain was 1 in both cases. again  this figure seems low  but any positive figure indicates a gain in performance due to the algorithm.
1	related work
our problem is different from the k-armed bandit problem  tins  1  since once the agent decides which alternative to use it cannot change its decision. in the k-armed bandit problem  an item is chosen repeatedly and the decision of which item to select may change over time. another approach is the statistical one. in  pizarro et al.  1  the statistical anova was employed for choosing the best model for a neural network. the procedure enabled them to isolate a subset of models whose mean error was the smallest  and subsequently the simplest model was chosen  occams razor criteria . the main difference between that work and ours is that they were not interested in minimizing the number of additional experiments since no cost was involved. the same approach can be found in heuristics-related works.  selman et al.  1   for instance  conducted a large number of experiments in order to identify the best heuristic. we suggest a method that will minimize the number of experiments to be executed  while providing the best option in a high percentage of the cases. in another work  tseng and gmytrasiewicz  1  developed an information-gathering system that uses the information value to guide the process. however  they assume that the number of possible answers for a query in the gathering process is finite while we consider a continuous set of possible answers. moreover  they consider a myopic sequential procedure for the information gathering process. thus  their solution is not optimal: they only consider the nearest step of information gathering  assuming that in each step the agent can decide about the next information to be obtained.  grass and zilberstein  1  developed a decision theoretic approach that uses an explicit representation of the user's decision model in order to plan and execute information-gathering actions. however  their system is based on information sources that return perfect information about the asked query.
¡¡in the world of strategies  researchers also have been more concerned with finding the most beneficial strategy in a given domain rather than formulating a general model. for example  in  carlsson and johansson  1  three types of strategies - generous  even-matched and greedy - were investigated as concepts for analyzing games. they granted the participating agents a strategy and examined their performances in two types of games. their aim was to determine which kind of strategy was preferable and in which environments.
1	conclusions and future work
in this paper  we generalized a model developed in  azoulayschwartz and kraus  1   which considered the problem of choosing between alternatives in e-commerce. we have presented an adjusted model which was designed to improve automated agents' decision-making. we evaluated this model in two different domains - an np-complete problem and a computer-game framework. in each domain we executed a
large number of experiments for several aspects of the domain. we have shown that agents that apply the hsc algorithm have improved their decision making by at least 1%  while executing a minimal number of additional experiments. moreover  we have demonstrated how to adjust the hsc algorithm to suit these different domains and to perform well when solving both minimum and maximum problems.
¡¡our future goal is to generalize the fixed number of experiments method. in this work we established its parameters empirically  and although it produced satisfactory results  we are interested in investigating its feasibility in additional domains. we also intend to examine the normal distribution assumption sensitivity. we assume that when the information units' distribution and the a-priori distribution are symmetric with one maximum  our assumptions hold. nevertheless further analysis is required to ascertain this assumption.
