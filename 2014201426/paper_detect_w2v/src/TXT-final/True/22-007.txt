 
parsss* is a parallel formulation of sss* that is suitable for shared-memory multiprocessor systems. it is based on the distributed tree search paradigm of ferguson and korf. the main difficulty in parallelizing sss* lies in achieving proper coordination between processes running on different subtrees of the game tree. this has been resolved in parsss* by the use of a shared array which maintains summary information on all processes that are currently in execution. problem-independent speed-up values for parsss* have been obtained experimentally. it is shown that an earlier algorithm of the authors  called itersss*  which allows sss* to run in restricted memory  can also be parallelized using the above scheme. 
1 introduction 
shared memory multiprocessor systems  local area networks  and other types of parallel and distributed computer systems have become increasingly available in recent years. this has caused a spurt of activity in the development of parallel versions of existing sequential algorithms. the general objective has been to achieve an effective reduction in net processing time through simultaneous execution of code by more than one processor. in the area of ai search methods  parallel versions of a*  kumar et a/.  1  and ida*  rao et al  1  have been announced. alpha-beta and other game tree search algorithms have been similarly parallelized   finkel and fishburn  1  leifkar and kanal  1  . 
　　an efficient parallelization of alpha-beta has recently been achieved by ferguson and korf  using their notion of distributed tree search. but a corresponding parallelization of sss* has not yet been reported. indeed  there appears to be no completely satisfactory parallelization of sss* in existence. the main hurdle lies in coming up with a suitable implementation for a global open list from which 
tools 
the highest-valued node must be selected at each iteration  and from which some non-promising nodes must be purged at intervals. a convenient distributed realization would make it imperative to maintain many local open lists. coordination between processes would then become a problem  and would have to be accomplished either through shared memory or by messagepassing. 
　　in this paper we propose a new parallel implementation of sss* based on distributed tree search. the basic idea is to let a number of processes execute in parallel  each searching a different subtree of the given game tree  and each running sss* on a local open list. the game tree can be irregular in shape with a nonuniform branching factor and depth. the version of our algorithm presented here  which we call parsss*  achieves coordination through the use of shared memory. only minor changes are required to get an alternative version where coordination is achieved through messagepassing. the maximum number n of processes that can execute at the same time  which can be thought of as being equal to the actual number of physical processors  can be fed as a parameter at runtime. our experimental results show the approximate variation of speed-up with the number n of processors. 
　　there are two primary reasons for the preference shown by users for alpha-beta over sss* : the apparent simplicity of the alpha-beta algorithm  and the high memory requirement of sss*. it is known  however  that sss* never examines more terminals than alpha-beta and frequently examines less  pearl  1 . in  bhattacharya and bagchi  1   a method was proposed for running sss* in restricted memory. sss* was modified slightly to yield itersss*  which could be fed at runtime the memory m available for use by the open list. for successful operation  m had to lie above a small threshold value mo. the number of terminals examined by itersss* was a function of m  but never exceeded the number of terminals examined by alphabeta. in parsss* each process maintains its own local open list  but it is still possible for the size of a local 
open list to become unwieldy. to solve the problem we can wed itersss* and parsss*. in the resulting algorithm paritersss*  each process would run itersss* instead of sss* on its local open  thereby cutting down significantly on the memory requirement. 
　　in section 1 we explain how sss* can be modified to run in parallel. the detailed formulation of parsss*  together with an example of its operation  is given in section 1. the next section summarizes our experimental results. paritersss*  the parallel version of itersss*  is described in section 1  and the concluding section contains suggestions for further work. 
1 running sss* in parallel 
when searching a game tree t  sss* sends out simultaneous probes across the entire breadth of t  in contrast with alpha-beta which searches t in an 
essentially left-to-right order. at all instants  sss* strives to keep in open a representative node from each of the constituent solution trees of t. however  nodes representing sub-optimal solution trees can get purged from open. sss* scans from left to right the most promising solution tree currently known; if this is an optimal solution tree then the root s gets solved  otherwise a more promising solution tree is found and the search continues. 
　　in sss*  when a live max node x is expanded  all its live min immediate successors enter open  since they represent different solution trees in t. we do not know in advance which one of these min nodes will ultimately cause x to get solved. thus  when parallelizing sss*  it would appear advisable to run independent processes on each of these min nodes. this is what we do in parsss*. each process mns on a subtree rooted at a min node of t  and all the processes are identical. initially  the algorithm creates a process for the root node s; this is the only process that runs on a subtree rooted at a max node. during execution of this root process  the max node s splits up into its min successors. the root process continues the search under the leftmost of these min nodes  and creates new processes for as many as possible of the other min successors of s. each of the newly created processes maintains its own local open list  and can in its own turn spawn processes at lower level min successors. at no instant of time  however  can the total number of processes exceed the given bound n. 
how is coordination between processes achieved   
in sss*  when a solved min node x.j is selected from open  its father max node x can be labelled solved  and all successors of x can be purged from 
open. but this cannot be done in parsss*. suppose that a process p is running on the subtree of the game tree rooted at a min node y. when p selects a solved min node x.j from its local open  other processes may be running on other descendants of x  so it may not be correct to label x solved. to resolve this difficulty  we keep a global array minproc in shared memory; for each process  there is an entry in minproc specifying the current h-value of the process. the entry for process p stores the associated root min node y  and the current h-value of the subtree below y. this value is the maximum of the h-values of nodes in the local open of p and the current h-values of the subprocesses spawned by p. when a process p* is spawned by the process p at a live min node z  the h-value of p' is initialized to the current h-value of z in p. as the search below z progresses  p* updates its own h-value to the value of the last node selected by it from its local open  assuming no other processes are running below z . thus when the process p selects a solved min node x.j from its local open  the current h-values of the processes spawned by p at nodes below the max node x are all available in minproc. among these  those that are no greater than the h-value of x.j are no longer required  and the corresponding processes can be killed. if all satisfy the condition  then x can be labelled solved. otherwise we must wait until the h-values drop or some other min successor of x solves x. since each process confines its attention to its own local open  the total number of terminals examined by parsss* can in general exceed the number examined by sss*. but parsss* takes less time than sss* since the processes run in parallel. 
1 algorithm parsss* 
for a description of sss*  we refer the reader to  stockman  1    pearl  1  pp. 1   and  bhattacharya and bagchi  1 . algorithm parsss* consists of a short root procedure parsssroot  and a process parsss*. many copies of process parsss* are simultaneously in execution  as has been explained above. before presenting the algorithm  we clarify some of its features below. 
 a  each copy of process parsss* has an entry in the global list minproc which is located in shared memory. the entry has three components : 
 i  the root node x of the subtree on which the process is running  except for s  all such nodes are min nodes ;  ii  the current h-value of the subtree;  iii  the current status  live or solved  of x. 
 b  the local open list of each process has been split up for convenience into two sublists  open and livemins. the list open keeps track of solved nodes and live max nodes  while livemins contains the live min nodes on which other processes can be initiated in future. the list livemins is kept sorted on the depth of the min nodes. 
 c  the procedure first returns the highest valued node from open u livemins. 
 d  n is an upper bound on the maximum number of concurrent parsss* processes that can be spawned by the algorithm. the value of n is fed as a parameter to parsssroot  and is stored in shared memory. 
	bhattacharya and bagchi 	1 

u 
1 
tools 

is indicated in terminalcounts. process 1 runs on the root and its left subtree; it spawns process 1 which runs on the subtree rooted at node 1. at each terminalcount we show the values of a terminal node when it is live  i.e.  prior to being examined  and when it is solved  i.e.  after being examined . 

table 1 
　　parsss* executes for 1 terminalcounts. after 1 terminalcounts  the first process solves node 1  but the root node s cannot be solved yet  since the h-value in minproc corresponding to process 1 is still too high. 
only at the end of the 1th terminalcount can s be solved. note that h-values in minproc are being updated right after first selects the highest-valued node from open. 
1 	experimental results 
we conducted some experiments on a vax 1 to find out the order of speed-up obtained by parsss* over sss* when run on uniform game trees  with branching factor b and depth d  with randomly generated terminal values. to get problem-independent results  running time was determined in terminalcounts. the computer system had only one physical processor  but multiprocessing was simulated by setting n to a value greater than one and then running n processes concurrently under the vms operating system. programs were written in c. terminalcount was initialized to zero for the process corresponding to the root node s  and thereafter whenever a new process got spawned  its terminalcount was initialized to the current terminalcount of its father. since in this environment the speed-up obtained becomes dependent on the order in which processes get scheduled by the operating system from the ready queue  it is necessary to run the same problem instance a number of times. for the purposes of this experiment we randomly selected only one problem instance for each set of  b  d  values. we ran each problem instance 1 times and took the minimum of the 1 running times  as shown in table 1. the corresponding values of the total number of terminals examined by all the processes together are also shown in the table. in order to restrict the number of processes that get created within a reasonable bound  we did not allow processes to be spawned on nodes at heights   1. 
owing to the use of local open and livemin lists  the speed-ups are less than linear. in a true parallel processing environment it is likely that higher speed-ups would be realised for the following reason. before declaring a max node solved  parsss* updates the terminalcount of the current process with the maximum of its terminalcount and the terminalcounts of all processes spawned from the current process. when there is a single processor  time slices get allocated to processes in a sequential manner  and no true parallel processing takes place; this puts some restrictions on when processes can get killed. when processes execute simultaneously  it is possible for a process p to be killed by an ancestor process p' in the middle of a timeslice as soon as the h-value of p' is updated by a descendant of p  this also explains why it is more reasonable to take the minimum rather than the average of the running times for a specific instance. the execution of a true parallel processing system cannot be simulated exactly on a single processor system because the user has little direct control on the length of the time slice or on the way processes get scheduled from the ready queue. the smaller the time slice  and the closer it is in duration to the time taken to examine a terminal  the closer the analogy. 

1 running itersss* in parallel 
a scheme almost identical to the one described above enables us to parallelize itersss*  bhattacharya and bagchi  1 . the total memory requirements of parsss* and sss* are of the same order. if so much storage is not available  the memory that is available can be distributed among a number of processes; of course  each process must be given the minimum storage space required for running itersss* on the subtree assigned to this process. 
	in 	itersss*  	every 	node 	in 	open 	has 	an 
	bhattacharya and bagchi 	1 


tools 

table 1 
ated problem instance. results are given in table 1. speed-up computations were made relative to sss*; since paritersss* allocates memory m to each of the processes that get generated  it would not be meaningful to compute speed-up with respect to itersss* running with memory m. note that when m = 1  itersss* is identical to sss*. as in the case of parsss*  we did not allow processes to be spawned on nodes at heights   1. 
1 	conclusion 
the objective of this paper has been to suggest ways to parallelize sss* and itersss*. our stress has been on the formulation of the parallel algorithms because of our belief that no other completely satisfactory formulation exists in the literature. we have run the algorithms and obtained problem-independent speed-up estimates empirically. these results are only indicative. the algorithms should now be run on true multiprocessor systems and on real game trees from different games  and speed-ups should be determined for each problem on the basis of total running time. it also appears possible to reformulate parsss* and paritersss* for distributed systems  though the details of the algorithm would depend on the specific features of the distributed system under consideration. another interesting area where further work is possible concerns the theoretical derivation of upper and lower bounds on the speed-ups obtainable by parsss* and paritersss* under different sets of assumptions. 
