
a matrix formulation for an adaptive genetic algorithm is developed using mutation matrix and crossover matrix. selection  mutation  and crossover are all parameter-free in the sense that the problem at a particular stage of evolution will choose the parameters automatically. this time dependent selection process was first developed in moga  mutation only genetic algorithm   szeto and zhang  1  and now is extended to include crossover. the remaining parameters needed are population size and chromosome length. the adaptive behavior is based on locus statistics and fitness ranking of chromosomes. in crossover  two methods are introduced: long hamming distance crossover  lhdc  and short hamming distance crossover  shdc . lhdc emphasizes exploration of solution space. shdc emphasizes exploitation of local search process. the one-dimensional random coupling ising spin glass problem  which is similar to a knapsack problem  is used as a benchmark test for the comparison of various realizations of the adaptivegenetic algorithms. our results show that lhdc is better than shdc  but both are superior to moga  which has been shown to be better than many traditional methods.
1 introduction
the notion of mutation matrix is used in the development of adaptive genetic algorithm so that the selection process does not require any external input parameter. numerical examples on knapsack problems indicate that this new adaptive genetic algorithm  called moga  mutation only genetic algorithm  is superior to many traditional methods  ma and szeto  1; szeto and zhang  1; zhang and szeto  1 . the idea behind moga is that mutation probability is a function of time  fitness ranking of the chromosome  and locus. the dependence is automatically determined by the statistics of the locus and the fitness ranking of the chromosome at the given time. the traditional genetic algorithm is a special case of moga in which all the mutation probabilities are fixed. in this paper  we extend the basic idea behind moga to include the crossover process. section 1 reviews the moga formalism without crossover. section 1 focuses on the implementation of the crossover matrix. a brief explanation of the test problem - ising spin glass is given in section 1  and the test results on ising spin glass are discussed in section 1.
1 mutation matrix
traditional genetic algorithm can be considered as a special case of the mutation only genetic algorithm  moga . for a population of np chromosomes  each encoded by l locus  can be arranged into a population matrix a with np rows and l columns. each element of the matrix a has a probability p  1   p   1  to undergo mutation. these mutation probabilities mij i = 1 ... np;j = 1 ... l  form a mutation matrix m: mij is the mutation probability of aij. in matrix a  each row is one chromosome. the rows are listed in decreasing order of the fitness: fi ≡ fj if i   j .
﹛in traditional genetic algorithm  the best n1 = c1 ℅ np chromosomessurvive  1   c1   1 . these survivors produce n1 = c1 ℅ np children to replace the less fit ones through crossover and/or mutation  1   c1   1 c1 . the remaining n1 =  1 c1 c1 ℅np unfit chromosomes are replaced by the same number of randomly generated candidates in order to ensure population diversity. thus the traditional genetic algorithm can be described by a mutation matrix which has mij = 1 for the first n1 rows; mij = m 1   m   1  for the next n1 rows; mij = 1 for all remaining rows. in traditional genetic algorithm  we must input three time independent parameters: c1  c1 and m.
﹛moga makes use of the information of the fitness ranking and loci statistics to determine mij dynamically. it outperforms traditional genetic algorithm in terms of both speed and quality of solution. in this formalism  the mutation matrix is nearly the same as the population matrix a  but with the loci arranged according to the standard deviation 考 j  for the allele distribution. if there are v allele values possible for the locus   in ordinary binary string representation of genetic algorithm  the possible value of an allele is either 1 or 1  so v = 1   then we define the standard deviation 考 j  for the allele distribution as:
		 1 
where  e.g. 汐 np  = 1 for
p = 1.	.	 1 
here  c f i   is the fitness cumulative probability of chromosome i  which  as well as fitness  can be considered as an informative measure of chromosome i. high fitness cumulative probability indicates high informative measure. the allele standard deviation 考 j  for each loci can also be regarded as an informative measure. a locus which has a smaller allele standard deviation contains more useful information than the other loci.
﹛at the beginning of the algorithm  chromosomes are randomly generated. alleles are randomly distributed. thus the allele standard deviation is not so informative. after generations of evolution  chromosomes acquire higher fitness  good chromosome structures emerge. then more chromosomes evolve to acquire the good structures. therefore  a locus with high structural information has a small allele standard deviation.
﹛the presence of the factor of  in the allele standard deviation is to take account of the informative usefulness of each chromosome. loci in chromosomeswith high fitness generally are more informative than those in chromosomes with low fitness. with this factor  the allele standard deviation more appropriately reflect the amount of information contained in a particular locus. our cumulative probability c f i   is defined by
	.	 1 
n g  is the numberof chromosomeswith fitness valueg. note that c f  is a non-decreasing function of f where c fmax  =
1 and .
﹛therefore  the mutation matrix can be viewed as an arrangement of genes based on the informative measures: the fitness cumulative probability c f i   and the standard deviation 考 j  for the allele distribution. the higher the row is  the more useful information the loci in this row contains. similarly  the closer to the right side the locus is  the more information it contains. our mutation is to make use of these two informative measures through the mutation matrix.
﹛in each generation  rows  i.e. chromosomes  in the mutation matrix were first selected for mutation based on the fitness cumulative probability. the probability 汐 i  to choose the ith row for mutation is defined as 
	汐 i  = 1   c f i  .	 1 
therefore  a chromosome with higher fitness has a lower probability to participate in mutation  or in other words  it has higher ability to survive. as the chromosomes are kept in a decreasing order of the fitness: fi ≡ fj if i   j  in the matrix a; 汐 i  is a non-decreasing function of the row index  chromosome index  i. thus  汐 i  is higher for less fit rows  chromosome . the fittest chromosome  i.e. the first row of the population matrix a  汐 i  = 1. it is  hence  never selected for mutation. the worst chromosome  i.e. the last row of a  will mutate with a probability close to unity for large next  we employ the fitness cumulative probability again to determine the number nmg of loci to undergo mutation for the selected chromosomes  each with l loci. stating the formula for nmg 
	nmg = 汐 i  ℅ l.	 1 
it is obvious that nmg is also a function of the row index i  or in ga language  the fitness. if a selected chromosome has a high fitness  it has fewer loci to undergo mutation. if a selected chromosome has a low fitness  it has more loci to mutate. finally  we mutate the nmg leftmost loci located in the selected rows in the mutation matrix  i.e. the nmg least informative loci of each chromosome. in this way  most of the informative loci remain and guide the evolution process to the next generation  while most of the less informative loci mutate until they contain more information.
1 crossover based on hamming distance
with the implementation of the mutation matrix  the crossover is operated implicitly on the population. the implicit crossover deploys the chromosome fitness ranking and the loci statistics in search of solution. besides these two kinds of information  the hamming distance can also be used to search the solution. through the introduction of a crossover matrix  two different explicit crossover methods which employ the information of the hamming distance are implemented and compared.
﹛the distance matrix h is a square np ℅ np matrix with matrix element defined by a distance measure between
             ↙  the i-th chromosome  represented by ri which is the i-th row vector of the population matrix a  and the i'-th chromosome . in this paper  we have two differ-
ent measures for h. the first one is called long hamming distance crossover  lhdc  in which  where  is equal to the number of different alleles in these two rows. the second measure is called short hamming distance crossover  shdc   with. we see that in lhdc  the smaller the distance between two chromosomes i and i' is  the more similar they are. while in shdc  it is just the opposite: smaller distance indicates low similarity. note that the matrix h is a symmetric matrix. the diagonal entries of h are zero in lhdc  but equal l in shdc.
﹛after defining the matrix h  we have to select two chromosomes for crossover. the probabilities to choose the two chromosomes are different  but they are both related to h. one probability is just the fitness cumulative probability  and we call it the first crossover probability pci i .
	pci i  = c f i  .	 1 
a chromosome  which we call the first chromosome  is first chosen with this first crossover probability. as the row index in h is also the row index in a  chromosome represented by smaller h row index has higher probability to be selected for crossover. then  the second chromosome is selected with another probability called the second crossover probability. the second crossover probability depends on the chromosome already chosen. if chromosome i  i.e. chromosome with h row index i   is the first chromosome  then the second crossover probability to choose chromosome i' as the second chromosome is:
	.	 1 
﹛if the first and the second chromosome are the same  the second one is chosen again until they are both different. both lhdc and shdc are single-point crossovers with the crossing points chosen randomly once in a generation. in each generation  a total of np chromosomes are selected to participate in the crossover. there is no limitation on the number of times a chromosome can take part in crossover in one generation. later  the fittest np chromosomes are selected to form a new generation from the pool of np original chromosomes and the np children chromosomes produced by crossover.
﹛in each generation  we first carry out crossover and then mutation. at the end of crossover  i.e. when the fittest np chromosomes are selected  as well as at the end of the mutation  we update the fitness and cumulative fitness probability for all the chromosomes in the population.
1 application: one-dimensional ising spin glass
the algorithm is applied to find the minimum energy of a 1d periodicising model consisting of nloci spins  nloci = 1  1  1  1  1 and 1 . each spin can be either up or down  represented by '1'  or '-1' respectively. the interaction energy between a pair of neighbouring ising spin is randomly generated in the range  -1 . the problem is to find the minimum of the energy
		 1 
for a given set of interaction energy . for each value of nloci  two different sets of interaction energy were generated. for each set of interaction energy  the exact minimum energy emin was first computed by exhaustive search over 1nloci spin configurations. the program is then executed 1 times with a fixed population size of np = 1 chromosomes. each executionis regarded as one sample. an executionstops when an acceptable solution is found or the maximum allowed generationsnma = 1is reached. an acceptable solution is a candidate solution which is within 1% tolerance of the exact minimum energy emin. when no solution can be found within nma generations in an execution  the execution is considered as a failure sample.
1 results
we have compared the efficiency of three adaptive genetic algorithms  moga  lhdc and shdc  on the spin glass problem. table 1 summarizes the average generation to reach solution and the number of failure. both gas with crossover  lhdc and shdc  outperform moga in terms of average
nlocisamplemogalhdcshdc111 1  1  1 11 1  1  1 111 1  1  1 11 1  1  1 111 1  1  1 11 1  1  1 111 1  1  1 11 1  1  1 111 1  1  1 11 1  1  1 table 1: average generation and number of failure in 1 samples to reach solution using the moga  lhdc and shdc. the numberof failure is written in the round brackets.
generation and number of successful searches. moreover lhdc gives the best performance.
﹛the two gas with crossover is better due to the information exchange between chromosomes. in lhdc  fit chromosome is selected and crossover with a very different chromosome to generate two new chromosomes. this expands the exploration space and minimizes the chance of premature convergence. in shdc  fit chromosome is selected and crossover with a similar one. this exploits the local search process and speeds up convergence. the results suggest that a second crossover probability emphasizing the exploration of search space is better for the ising-spin-glass-like problem  i.e. knapsack-type problem  kellerer et al.  1 .
1 conclusion
three different ways to implement the adaptive genetic algorithm are tested and compared using the one-dimensional ising model for spin glass as the testing problem. the results show that lhdc is best and that all three methods are parameter-free. since moga outperforms other algorithms  including dynamic programming  kellerer et al.  1; simoes and costa  1; 1   for solving the knapsack problem  our test shows the superiority of lhdc in this kind of search problem. this work also provides a matrix formulation of parameter free adaptive genetic algorithm with both mutation and crossover.
acknowledgments
n.l.law and k.y. szeto acknowledge interesting discussion from c.w. ma. k.y. szeto acknowledges the support of rgc grant 1.
