 
many large markov decision processes  mdps  can be represented compactly using a structured representation such as a dynamic bayesian network. unfortunately  the compact representation does not help standard mdp algorithms  because the value function for the mdp does not retain the structure of the process description. we argue that in many such mdps  structure is approximately retained. that is  the value functions are nearly additive: closely approximated by a linear function over factors associated with small subsets of problem features. based on this idea  we present a convergent  approximate value determination algorithm for structured mdps. the algorithm maintains an additive value function  alternating dynamic programming steps with steps that project the result back into the restricted space of additive functions. we show that both the dynamic programming and the projection steps can be computed efficiently  despite the fact that the number of states is exponential in the number of state variables. 
1 introduction 
over the past few years  there has been a growing interest in the problem of planning under uncertainty. markov decision 
processes  mdps  have received much attention as a basic semantics for this problem. an mdp represents the domain via a set of states  with actions inducing stochastic transitions from one state to another. the key problem with this type of representation is that  in virtually any real-life domain  the state space is quite large. however  many large mdps have significant internal structure  and can be modeled very compactly if that structure is exploited by the representation. in 
factored mdps  a state is described implicitly as an assignment of values to some set of state variables. a dynamic bayesian network can then allow a compact representation of the transition model by exploiting the fact that the transition of a variable usually depends only on a small number of other variables. in a simple robotics example  the location of the robot at time t + 1 may depend on its position  velocity  and orientation at time t  but not on what it is carrying. the momentary rewards can often also be decomposed: as a sum 
ronald parr 
computer science department 
 stanford university parr cs.stanford.edu 
of rewards related to individual variables or small clusters of variables. in our robot example  our reward might be a sum of two subrewards: one associated with location  for getting too close to a wall  and one associated with the printer status  for letting paper run out . 
　while these representations allow very large  complex mdps to be represented compactly  they do not help address the planning problem. standard algorithms for solving mdps require the representation and manipulation of value functions - functions from states to values. since a state is a full instantiation of all state variables  the representation of the full value function is exponential in the number of state variables. unfortunately  structure in a factored mdp does not  in general  guarantee any type of structure in the value function. even some very simple mdps with very compact transition models and fully decomposed rewards lead to value functions that have no usable structure whatsoever  see section 1 . thus  algorithms that represent the value function 
exactly will be impractical for dealing with many large  structured domains. 
　one approach to controlling the size of the value function representation is to use a truncated value function that ignores some state variables. several methods exist for detecting irrelevant state variables. a closely related approach uses approximate or abstract model that eliminates some state variables.  see boutilier et ai  for a survey of these approaches.  these methods have the common limitation that they can force different states to have the same value  producing a coarse-grained approximation to the value function with large plateaus of indistinguishable states. 
　our work is based on the intuition that  while value functions might not be structured  there are many domains where they are  close  to having exploitable additive structure. consider  for example  a stochastic version of a traditional planning task  where several subgoals can contribute to the overall success of the plan. here  it is quite plausible that the value of a state is approximately linear in the set of subgoals achieved at that state  with more important subgoals having  perhaps  a higher weight . clearly  this is only an approximation  as subgoals might interact in various ways; however  it may be a fairiy good approximation. furthermore  if some subgoals interact strongly  we can have value function components that depend on the status of several subgoals. such value functions have a long history in multi-attribute utility 

1 	uncertainty and probabilistic reasoning 

theory  keeney and raiffa; 1 ; they also play a central rote in influence diagrams  hiyard and matheson; 1 . 
　based on this intuition  we propose a new approach to computing approximate value functions for structured mdps. we restrict attention to value functions that are a linear combination of local basis functions  each of which depends only on a small subset of the state variables. as discussed above  we believe that  in many cases  such a value function can be a good approximation to the correct value function. in particular  unlike the coarse-grained approximations discussed above  this approximation can assign a distinct value to every state. however  it can still be represented compactly using a small number of parameters  the coefficients of the basis functions . we propose to use algorithms whose entire computation is restricted to value functions that are compactly represented in this form. 
　our focus in this paper will be on the value determination task: computing the value function associated with a particular policy. this task can be solved using a simple algorithm that iterates through a dynamic programming step  gradually converging to the correct value function. our algorithm augments this basic iteration with an additional projection step  where the value function resulting from each dynamic programming step is projected back down into the  allowable  space of value functions. 
there are several key issues that arise in such an algorithm. 
first  we must show how the operations required by such an algorithm - dynamic programming and projection - can be executed efficiently. one of our main contributions is that we provide  for both of these steps  an efficient algorithm that depends upon the size and structure of the model and basis function representations  not upon the exponentially sized state space. we note that we can accomplish these operations efficiently despite the fact that each of the exponentially many states may have a distinct value. second  we must show that the algorithm behaves reasonably: that it converges to a stable fixed point  hopefully one which is close to the true value function. by a careful choice of distance metric  we can show conditions under which the algorithm will converge to a solution which is not too far away from the best value function describable in the restricted space. finally  we must show how to apply this algorithm to the task of evaluating policies in a factored mdp. this application is far from trivial  as policies in a large mdp may be very complex. we show that our algorithm allows effective policy evaluation for policies represented as a small set of conjunctive decision rules  e.g.  decision trees ; this class is arguably the most widely used class of compactly represented policies  boutilier and dearden  1; dean et al.  1 . 
1 preliminaries 
we begin by introducing some of the basic concepts that we will be using. our primary focus is on the value determination problem. thus  for the time being  we will restrict attention to processes without a decision component. a markov process  mp  is defined as a triple  1  r  p  where: is a set of states;  is a reward function such that r s  represents the reward obtained by the agent in state p is a transition model  so that represents the probability of going from state to state  
   a markov process is associated with a value function  where  is the total cumulative value that the agent gets if it starts at state  we will be assuming that the mp has an infinite horizon and that future rewards are discounted exponentially with a discount factor thus  is defined using the fixed point equation 
a simple iterative pro-
cess can be used to perform value determination. we choose 
　　　　　we then repeatedly execute a dynamic programming 	step: 
1  
we use to denote the operator that takes a value function and returns  we call the backprojection of repeated applications of t are known to converge to the true value function  
　in a factored mp  the set of states is described via a set of random variables where each  
takes on values in some finite domain a state x defines a value for each variable the transition model is described as a dynamic bayesian network  dbn . let  denote the variable at the current time and the variable at the next step. the transition graph is a two-layer directed acyclic graph whose nodes are  in this paper  we will assume that all edges in this graph are directed from nodes in 
  this assumption 
can be relaxed  but the resulting algorithm becomes somewhat more complex. we defer details to a longer paper.  we denote the parents of  in the graph by  each node is associated with a conditional probability table  cpt  
                         the transition probability  is then defined to be where ui is the value in x of the variables in  
　　we also need to provide a compact representation of the reward function. as discussed in the introduction  we assume that the reward function is factored additively into a set of localized reward functions  each of which only depends on a small set of variables. in general  we say that a function / is restricted to a set of variables c if let  be a set of functions  where each is restricted to a cluster of variables such that : val  the reward function associated with the state x is then defined to be we note that  here and in the future  we use the following shorthand: if is a function over some set of variables y  and  we will use  to denote the value that takes over the part of the vector z that corresponds to variables in y. 
　one might be led to believe that factored transition dynamics and rewards would result in a structured value function. unfortunately  this usually is not the case. as we execute dp steps  the value function typically becomes more complex. 
1
　　　we begin with by convention. any choice for will converge to the same fixed point. 
	koller and parr 	1 

example 1: assume that we have only a single-reward function r that depends only on  so it also 
pends on and on the entire parent set of x1. similarly  depends on the union of all of their parents  etc.  
in general  the value function will eventually depend on all of the variables that have any influence whatsoever  direct or indirect  on a reward. in practice  this set will typically be all of the state variables  as it is somewhat superfluous to introduce into the process description variables whose value never matters to the agent's utility. 
1 constrained value determination 
as we described in the introduction  the key idea behind our approach is the restriction of our algorithms to the use of value functions in a limited class. this idea is best known under the name value function approximation  which is used frequently in the context or reinforcement learning  tadepalli and ok  1; van roy  1 . we use this idea in the context of maintaining full value functions and propagating them through the dp equation  1   gordon  1; tsitsiklis and van roy  1 . however  unlike other methods  which deal with large state spaces by considering only a restricted set of  representative  states  our method efficiently finds a least squares approximation for the entire state space. 
　more precisely  let be a restricted set of value functions. we will define via a set of basis functions 
　　　　　　that is  we have that h is a basis for if every function can be written as for some weights  
　　our algorithm repeatedly executes the following steps. it begins with a value function it then backprojects it via  1   resulting in a value function . in general  the dp step does not maintain the property of being in a restricted class therefore  we typically have that  we 
then project into  i.e.  find the  closest  value function in the result is our new value function  
　　we must decide what it means for a value function to be  close  to some other value function by choosing a suitable distance measure. several constraints combine to make this choice difficult. on the one hand  we need a contraction property: exact value determination converges because  1  is a contraction - each iteration decreases the error of by a constant factor. in order to get convergence for approximate value determination  we need a similar contraction property. we also need an effective algorithm for pro-
jecting into relative to this distance. finally  we need this projection operation to be a non-expansion under the same distance  otherwise we are not guaranteed the desired convergence property. tsitsiklis and van roy  underscore the importance of this point by demonstrating a two-state mdp 
for which this type of approximate value determination diverges if we use standard least-squares approximation. unfortunately  it turns out to be nontrivial to find a distance metric that satisfies all criteria. for example   1  is not a contraction in euclidean distance. it is a contraction in loo and  norms1  but we have no efficient projection algorithm for 
these distances. 
　one choice that turns out to satisfy both desiderata is the weighted euclidean distance. let  be the occupancy 
probability ofthe state 1 in the stationary  steady-state  distribution we define the weighted euclidean distance 
we say that  is the 
least 	projection of 	into 	is the function in  
that is closest to in terms of intuitively  unweighted euclidean distance places equal emphasis on getting each as close as possible to whereas the weighted distance places more emphasis on getting correct values for states that are visited more often. 
　fortunately  there exists a fairly straightforward algorithm for doing projection relative to weighted euclidean distance. the key operation is the weighted dot product. for two func-
tions 	we define  we define the length 	to project a function 	into 	we first compute each coefficient 
     this intermediate weight vector has to be transformed  in order to ac-
commodate for any  linear  dependencies between our basis vectors. we define and define a to be the matrix if we now define a weight vector  we know that is the least projection of  
into 	 strang  1 . we define vp to be the operator that projects into  
　the contraction of the  operator in  distance is established in nelson  and is combined with projection in van roy   yielding: 
theorem 1:  a  the operator is a contraction in dp distance with rate hence  has a unique fixed point 
	then 	is a contraction in dp dis-
tance with rate hence has a unique fixed point v*. furthermore  satisfies: 
in other words  the alternating dp and projection algorithm converges to a fixed point; the value function at that point is 
     1 the norm contraction requires stronger assumptions than we have made here. for example  positive rewards and a pessimistic v＜ will suffice. 1  we assume  for simplicity  that the process is mixing. 

1 	uncertainty and probabilistic reasoning 

at most a constant factor worse than the optimal value function within i.e.  the one closest to the true optimal value function  thus  if we assume that our true value function is well-approximate by some function within  we have strong bounds on the performance of the algorithm. 
1 factored value functions 
the ideas described in the previous section are well-known. 
they allow a compact representation of very complex value functions: we only need to maintain the coefficients  the 's  for the limited number of basis functions that we choose to use. why have these methods not already been used to address the problem of value determination in factored markov processes  unfortunately  a compact representation of the value function is not enough. the representation has to support efficient execution of our two main computational steps: the dp step   and the dot product step required for  
　　we thus turn our attention to the specific properties of factored processes. as we discussed in the introduction  we propose restricting our algorithm to factored value functions  ones that are linear in functions over small clusters of variables. more precisely  we define a cluster c of variables to be a subset of x. the key idea behind our result is that we can efficiently perform approximate value determination with any basis whose functions are restricted to small clusters. in decision-analytic terminology  we say that a function is generalized additive  bacchus and grove  1  over a set of clusters if it can be written as a where each is restricted to some the function is said to be additive 
if the clusters in are disjoint. now  let and assume that each is restricted to some small cluster 
       clearly  the functions in  are generalized additive in in fact  it is easy to construct a basis that allows us to have represent exactly the set of all generalized additive functions over a particular set of clusters. in the remainder of the analysis  we assume that we have restricted attention to a specific basis and use to denote  
　　as we now show  restricted basis functions allow an efficient implementation of the dp and projection steps. assume that we are restricting to value functions in  so that  as a result of the algorithm so far   is in this space. thus  as we argued  can be represented as  for some set of functions each restricted to some small cluster  specifically  we have let us examine the result of the dp step: 
  1  
 consider one of the terms in  1  that corresponds to some function  as we saw in example 1  if the domain of a 
value function is restricted to a single variable its backprojection through  1  depends on all of parents. the same principle holds here. thus  let be the variables in be the instantiation of these variables in the state x. it is easy to show that 

that depends only on the value of the variables in in the preceding state x. we denote the function in  1  by   to compute it  we must  at worst  generate the entire conditional probability this can take  
           operations. the locality of influence that is characteristic of dbns typically implies that  is small  allowing this computation to be done efficiently. 
to compute the unweighted projection of  we 
simply need to compute its dot product with each basis function  as the dot product is a linear operation  it can be decomposed into a separate dot product of with each of the terms in  1 : and for it is straightforward to verify that computing the dot product of with a function restricted to a set y requires time which is 
linear in  
　unfortunately  the unweighted projection is not the operation we need; rather  we need to compute the weighted projection in order to get a least  projection. there are two major obstacles preventing us from using a weighted dot product in our context. first  the stationary distribution of our current policy is not known. more importantly  the stationary distribution has no structure that can be exploited to allow a compact representation  boyen and roller  1 . in other words  we can represent this distribution only explicitly  as an exponentially-sized probability function. this problem makes it impractical to compute the stationary distribution; it also prevents us from doing the projection step efficiently  as discussed above. 
　we solve both problems at once by building on the work of boyen and koller   bk from here on . they show how a process very similar to our iterated dp & project algorithm can be used to maintain and propagate a compactly represented approximate belief state - a distribution over the states at a given point in time. most simply  they partition x into a set of disjoint clusters  the distribu-
tion over the states is represented implicitly as a product of distributions over these clusters. the step of propagating to the next time step results in a distribution that is not of the right form; the algorithm generates by computing the marginal distributions over each 
　　bk show that  if the marginalization introduces an error of then  for all away  in relative entropy  from the distribution that would have been maintained by a similar process without the approximation. here  is the mixing rate of the process  which can be shown to depend on the sizes of the cpts in the dbn  and not on the size of the process. thus  we have that  
hence  the number of iterations required to get close to the stationary distribution is very small - logarithmic in the benefits of this approximate computation are twofold:  1  the complexity of the algorithm is not prohibitive; its complexity depends on the sizes of the clusters and 
	koller and parr 	1 

on the interconnectivity of the dbn.  1  the result is a factored distribution  which can be used in an efficient weighted projection algorithm. 
　let us see how a factored distribution can be used in the context of a weighted projection. the key procedure used in the algorithm is that of computing 
for some function restricted to a set y. it is easily verified that 	we note that we can 
easily compute let yk denote the part of y that overlaps with variables in the cluster we compute by a simple marginalization process over the pk component of p  and then multiply the results for all  1= 1   . . .   note that this operation requires time which is linear in 
and in for the relevant clusters qk. we can use this subroutine for doing the weighted projection  simply by noting that and that if is restricted 
to y and to z  then   is restricted to the full algorithm is as follows: 
1. compute a factored approximation to the stationary 
the number of operations required by an iteration of this algorithm grows linearly with the number of basis functions  and exponentially with the sizes of the clusters and their backprojections. it also depends  in step 1b  on the extent to which backprojected clusters  overlap with other clusters thus  the complexity is determined by the interconnectivity of the dbn  by the amount of interaction in our restricted set of value functions  and by the extent to which the structure of the value functions matches the structure of the process. 
　the main remaining question involves the performance of this algorithm. ideally  we would like to state a theorem along the lines of theorem 1. the problem is that our projection step no longer uses the exact stationary distribution as weights. however  one would hope that if the approximation is a good one  as suggested by the results of bk   the error introduced by using projection relative to the approximate weights would not prevent the process from converging. indeed  we can show that such a result holds  albeit only for relative error approximation  a slightly stronger notion than the one guaranteed by bk : 
theorem 1: assume that is an relative approximation to the true stationary distribution i.e.  for all x  be the contraction then our approx-
imate value determination algorithm 	is a contraction in 

figure 1: a factory with six machines. a reward of 1 is given for each time step in which machine is functioning. 
added basis functions unweighted projection 
weighted projection 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 table 1: the change in the sum of squared errors  sse  weighted by the uniform distribution  and by the true sta-
we performed some simple experiments to demonstrate the convergent behavior of our algorithm and to verify our intuitions about the additive structure of mdp value functions. 
figure 1 shows an abstracted factory with six machines  
in this problem  a machine can be in two states  working and not-working. if a machine and all of its predecessors were working in the previous time step  then the machine will work in the current time step with probability 1. there is a stochastic startup lag  which reduces this probability to 1 if the machine was not working in the previous time step. no machine can operate if its predecessors were not both working in the previous time step. note that a failure in any of will have a cascade effect  ultimately caus-
ing m1 to fail. it will take several time steps for the system to recover. a reward of 1 is received whenever machine is working. 
　we evaluated this 1-state system using our constrained value determination algorithm with a discount factor of  = 1 and a series of six different bases. the basis functions are binary so that  for example  has value 1 when both and  are working  and value 1 otherwise. we performed both unweighted projection  which is not guaranteed to converge  and our weighted projection algorithm. for this problem  we achieved convergence in all cases. the second and third columns of table 1 show the reduction in error as new basis functions are added. the first three additions allow the value function to depend on the status of more individual machines. the last three allow it to depend in a correlated way on the status of pairs of machines. in each case  we ap-

1 	uncertainty and probabilistic reasoning 

added basis functions unweighted projection 
weighted projection 
1 1 1 1   1 1 1 1 1 1 1 1 1 1 1 1 t  1 1 1 1 1 1 1 1 table 1: results with modified to work if either of m1 or m1 are working. 
proximated the stationary distribution using a product of distributions over the individual machine states  notice that the weighted projection outperforms unweighted projection in all cases  even in the unweighted norm. also  the  weighted error for the weighted projection is monotonically decreasing  while the other errors fluctuate. the final value function for the weighted projection with all ten basis functions corresponds to an unweighted rms error of less than 1. the true value function for this problem assigns values between 1 and 1  so that 1 is a very reasonable error bound. hence  our final value function is a good 1 parameter approximation to the true  1 parameter value function. 
   to further verify our hypothesis about additive structure of value functions  we changed machine  so that it would work if either of or were functioning. in this case  we would expect the value function to be less sensitive to correlations between the status of and the results  shown in the table 1  support this hypothesis. we see that introducing the basis function  provides no sig-
nificant reduction in the error.  the overall increase in error compared to the original problem is probably due to higher overall values in this model - between 1 and 1.  we observe a similar pattern in the comparison of weighted and unweighted projection: weighted projection provides more reliable approximations. 
　finally  we demonstrate how the value functions generated by our algorithm can be useful in decision making. suppose that there are two types of workers  reliable and resilient. when a reliable worker is operating a working machine  it will continue to work properly with probability 1  but if the machine fails  the worker will restart the machine successfully with probability 1. a resilient worker will keep a working machine functioning with a probability of only 1  but recovers from failures with probability 1. if the manager of our 1-machine factory has three of each type of worker  he may wish to compare different strategies for allocating workers to machines. one possibility would be a reliable-
first strategy that places the reliable workers on machines  to avoid cascade failures. a resilient-first strategy would place the resilient workers on machines  to recover quickly when failures occur. we consider the original machine configuration  which requires both and to be working for machine  to work properly. for this problem the reliable-first strategy completely dominates the the resiliant-first strategy. 
　figure 1 shows a graph of the true value function for the resilient-first strategy versus the approximation with all 1 basis functions. the states are numbered 1... 1. the status 

figure 1: the resilient-first strategy. 

figure 1: the reliable-first strategy. 
of the machines is used to identify the states  e.g.  when machine m1 is working  the highest order bit of the state number has value 1; when machine is working the lowest order bit of the state number has value 1. figure 1 shows both value functions for the reliable-first strategy. the qualitative closeness of the these approximations is apparent from inspection - the shape of the approximations are quite close to the shape of the exact value functions. moreover the approximate value functions maintain the dominance of the reliablefirst strategy over the resiliant-first strategy. thus  we see that our approximation closely matches the qualitative structure of the exact value functions for these domains. 
1 policy evaluation 
we now address the issue of using our value determination algorithm to evaluate a policy in an mdp  where we extend the definition of our process to incorporate actions. most importantly  the transition probability p and the reward r now also depend on a. in the context of factored mdps  this extension is usually done by defining a separate transition graph and associated set of cpts for each action the resulting transition probability  is thereby factored  perhaps in a different way  for every action  the reward function can also vary over actions in a similar way. 
　　our goal now is to compute an approximate value function for a particular policy intuitively  there seems to be no problem. a policy in an mdp specifies a transition model  and a reward function  we might simply apply our algorithm to these. unfortunately  the problem is 
	k1ller and parr 	1 

somewhat more subtle. the most obvious difficulty is* that the naive representation of a policy in an mdp with exponentially many states is also exponentially large. therefore  we might not even be able to construct a compact transition model and reward function for  clearly  we must restrict the set of policies that we consider to ones that can be efficiently manipulated by our algorithm. 
　to understand this issue  assume that we have constructed a value function  and are now trying to perform our dp and projection steps. we do not have a single transition model to use for the next step of backprojection  because the action depends on the state. however  we can use our algorithm to compute a factored approximation to the  function we simply back-
project through the transition model and then project into we now have to combine the different functions to get a complete value function. for each action let be the subset of states in which a is taken  and let be the indicator function which takes value 1 at x when and value 1 otherwise. then  
we see that  depending upon the policy  can be composed of an arbitrary combination of pieces of the functions. in particular  if the indicator functions isa depend on variables in many different clusters  the combination will no longer be in even if each of the thus  we need to project back into  
　as before  we need to compute to perform the projection. this reduces to the problem of com-
 1  
thus  
in order to perform this weighted dot product operation  we must be able to compute the total probability  according to the approximate stationary distribution  of  in many cases  this computation is far from trivial  even if the set  has a simple and compact description  e.g.  as a linear separator defined over the variables  
　however  for what is arguably the most often-used class of compactly represented policies  this computation can be done easily. a region-based policy is one represented as a 
　set  where each  is an assignment of values to some subset of the variables in x.  different can refer to different sets of variables.  each  thereby specifies a 

　　1 for other classes of policies  monte carlo sampling may succeed in providing good estimates for  we defer discussion to a longer paper. 
region in the state space  such that action is taken in this region. the assumption is that the regions corresponding to the different  are mutually exclusive and exhaustive. note that a decision tree representation of the policy {boutilier and dearden  1   falls into this category  as do the policies for the aggregated state spaces  dean et al.  1 . given the factored representation of can be computed simply and efficiently  in the obvious way. 
　　it remains only to extend the algorithm of bk to the task of computing the approximate stationary distribution  for a region-based policy  recall that their algorithm was based on repeatedly computing a factored belief state from a 
　factored belief state where 
here  we have a potentially different transition model for each  therefore  we must do a separate computation for each of them: 

　fortunately  the algorithm of bk can easily be adapted to this task. we simply instantiate  as evidence into the bayesian network that they use for doing the one step propagation  and then compute the internal summation. the probabilities  can be computed easily  as described above. this algorithm provides a factored representation of the stationary distribution  which can be used in equation 1. thus  we can efficiently compute an approximate value function for any region-based policy. 
1 	discussion and conclusions 
we have shown that we can efficiently compute factored approximations to value functions. we have also demonstrated  both theoretically and empirically  that the result of our approximate value determination algorithm can often be quite close to the true value function. finally  we showed how to use our algorithm for evaluating policies in a restricted but interesting class. 
　it is useful to compare our algorithm to other algorithms for factored mdps  boutilier et al. 1 . some algorithms for factored mdps try to construct compact value functions by exploiting structured  usually tree-based  cpts. when the value function representation becomes too large  the standard approach is to perform a type of state aggregation  boutilier and dearden  1; dean et al. 1 . thus  their approximate value function is one that is piecewise constant over the state space. in other words  there are substantial blocks of states all of which take the same value. a main advantage of our approach is that it uses a richer class of value function approximations. it seems clear that  at least in some domains  a linear approximation is much more accurate. while the algorithm presented in this paper does not exploit structured cpts  this feature can be added easily to make both the dynamic programming steps and projection steps more efficient. 

1 	uncertainty and probabilistic reasoning 

　other approaches  meuleau et al.  1; boutilier et al.  1; singh and cohn  1  decompose the process into subprocesses  and compute a separate value function for each one. they then attempt to combine the different value functions into a global one using some heuristic approximation. generally speaking  the decoupling of the process is a much harsher approximation than the factorization of the value function. the latter still allows influence to flow between the different subprocesses  and thereby is likely to get more accurate results. 
　of course  it is important to remember that these other approaches solve the planning problem whereas  at least for now  our algorithm is restricted to the value determination problem. the obvious question is how our value determination algorithm can be used for planning. the most obvious approach is to use our algorithm as a subroutine in an algorithm such as policy iteration  where we compute the value function for some policy  and use it as guidance for locally improving the policy. unfortunately  there are barriers to this application of our algorithm. the difficulty lies in the nature of our approximation. we measure our distance from the true value function in a norm that is weighted by the probability that the different states will be visited in the stationary distribution. hence  states that are visited infrequently can have very poor value estimates. changes to the policy based upon these estimates will not be guaranteed to improve the overall policy as in exact policy iteration  and could actually make the policy much worse. the issue of using our value functions as guidance in policy search is an important direction for future work. 
　there are  however  several other important uses for a value determination algorithm. plan evaluation is an important problem in and of itself; our algorithm allows us to take policies generated by a user or some other tool  and produce a fairly good value function for them. furthermore  it is known that simple policies can be improved significantly by the addition of a small lookahead search that uses the value function of the policy as a heuristic value at the leaves of the search. indeed  the recent work of sutton   1  shows that even very simple policies can be quite powerful if a meta-level reasoner is allowed to choose which one to apply in different contexts. our algorithm can readily be used to compute the value functions necessary for making such a choice. 
　factored mdps provide us with a compact and natural representation for the type of complex problems that arise in realworld settings. unfortunately  the structure is not reflected in the value function for these processes. however  we believe that the value functions that arise are often approximately factored  i.e.  approximated well by the generalized additive functions that we have discussed. the fact that such functions are often used by human decision makers supports this intuition. we therefore believe that algorithms that exploit this structure can be very successful. this paper takes a first step towards this goal. 
acknowledgement 
we are grateful to craig boutilier  andrew ng  satinder 
singh  and benjamin van roy for helpful discussions. this work supported by aro grant daah1-1 under the muri program  integrated approach to intelligent systems   by onr contract n1-c-1 under darpa's hpkb program  and by the generosity of the the powell foundation and the sloan foundation. 
references 
 bacchus and grove  1  f. bacchus and a. grove. graphical models for preference and utility. in proc. uai  1. 
 boutilier and dearden  1  c. boutilier and r. dearden. approximating value trees in structured dynamic programming. in proc. icml  pages 1  1. 
 boutilier et al  1  c. boutilier  r.i. brafman  and c. geib. prioritized goal decomposition of markov decision processes: towards a synthesis of classical and decision theoretic planning. in proc. uai  pages 1  1. 
 boutilier et al.  1  c. boutilier  t. dean  and s. hanks. decision theoretic planning: structural assumptions and computational leverage. journal of artificial intelligence research  1. 
 boyen and koller  1  x. boyen and d. koller. tractable inference for complex stochastic processes. in proc. uai  1. 
 dean et al.  1  t. dean  r. givan  and s. leach. model reduction techniques for computing approximately optimal solutions for markov decision processes. in proc. uai  1. 
 gordon  1  g j. gordon. stable function approximation in dynamic programming. in proc. icml  pages 1  1. 
 howard and matheson  1  r.a. howard and j.e  matheson. influence diagrams. in readings on the principles and applications of decision analysis  pages 1. strategic decisions group  1. 
 keeney and raiffa  1  r.l. keeney and h. raiffa. decisions with multiple objectives: p