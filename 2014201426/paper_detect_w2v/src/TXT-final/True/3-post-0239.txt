
weighted a*  wa*  is a popular search technique that scales up a* while sacrificing solution quality. recently  researchers have proposed two variants of wa*: kwa* adds diversity to wa*  and msc-wa* adds commitment to wa*. in this paper  we demonstrate that there is benefit in combining them. the resulting msc-kwa* scales up to larger domains than wa*  kwa* and msc-wa*  which is rather surprising since diversity and commitment at first glance seem to be opposing concepts.
1 introduction
weighted a*  wa*  is a popular search technique that scales up a* while sacrificing solution quality  by weighing the heuristic values more strongly than a*. in this paper  we study how to scale up wa* to even larger domains  building on two variants of wa* that have been proposed recently: first  wa* expands only one state per iteration. kwa* is a variant of wa* that expands several states per iteration  which adds diversity to wa* to give it a stronger breadthfirst component. second  wa* considers all states in the open list as candidates for expansion. multi-state commitment wa*  msc-wa*  is a variant of wa* that considers only a small number of states in the open list as candidates for expansion  which adds commitment to wa* to give it a stronger depth-first component. in this paper  we demonstrate that there is benefit in combining them. our experimental results  for example  show that the resulting msc-kwa* with the manhattan heuristic scales up to the 1-puzzle  whereas wa*  kwa* and msc-wa* do not.
1 wa*  kwa* and msc-wa*
wa*: wa* is a variant of a* that scales it up by making its search more greedy and thus more focused  pohl  1 . while a* calculates the f-value of a state s as f s  = g s + h s   wa* calculates it as f s  =  1 w g swa*. +wkwa*:¡Áh s   where 1 ¡Ý w ¡Ý 1 is the only parameter of
due to its strong focus  wa* is likely to be led into goal-free regions of the search space by misleadingly small heuristic

¡¡¡¡ we thank richard korf for providing us with wa* code and acknowledge funding from nsf under contracts iis-1 and iis-1. the views and conclusions contained in this document are those of the authors and not of nsf.values. kwa* tackles this problem by adding a breadthfirst component to wa*  diversity  to make its search less focused. it expands the k ¡Ý1 states with the smallest f-values in the open list  rather than just one  in parallel during each iteration  felner et al.  1 . thus  kwa* has two parameters  namely w and k. if k = 1  then kwa* reduces to wa*. larger values of k increase the probability that wa* selects a state on the shortest path from the start state to the goal state during the next iteration at the risk of generating more states than necessary during each iteration. mscwa*: wa* focuses the search more than a*. however  the state that wa* expands next can be any state in the open list  depending on the f-values . multi-state commitment wa*  msc-wa*  increases the focus of wa* even more by focusing the search on a subset of the states in the open list  commitment   kitamura et al.  1 . msc-wa* splits the open list into a commit list of at most c states and a reserve list. thus  msc-wa* has two parameters  namely w and c. the commit list plays the role of a smaller open list  whereas the reserve list stores the remaining states of the original open list. msc-wa* expands the state with the smallest f-value in the commit list. msc-wa* never re-expands a state  even if it finds a shorter path from the start state to the state in question. the number of states in the commit list can thus get smaller than c. in this case  msc-wa* uses the states with the smallest f-values in the reserve list to refill the commit list. if c = ¡Þ  then the reserve list remains empty and msc-wa* reduces to a variant of wa* that never re-expands a state. smaller values of c make msc-wa* focus the search on a smaller subset of the states in the original open list.
1 comparison
table 1 reports the results of our experiments with wa* 
table 1: comparison of wa*  kwa*  msc-wa*  and msc-kwa* for the n-puzzle
nperformancewa*kwa*msc-wa*msc-kwa*measurevaluewbestvaluewkbestvaluewcbestvaluewk=cbest1solution cost11111x111111xstored states11111111111xgenerated states11111111111xruntime  seconds 11111111111x1solution cost11111x111111stored states11111111111xgenerated states11111111111xruntime  seconds 11111111111x1solution cost111x111111stored states111111111xgenerated states111111111xruntime  seconds 111111111x1solution cost1.1.1xstored states111xgenerated states111xruntime  seconds 111xkwa* and msc-wa* on the n-puzzle with n =1 1 and 1  using the manhattan distance as the heuristic function. the goal state has the blank in the upper left corner. for each combination of parameters  we average over 1 experiments  1 experiments for n =1  with a memory capacity of six million states.  for w  for example  we use the values 1  1  1  1  1  1  1  1  1 and 1.  each row of the table then reports the parameter combinations that optimize the average performance for each one of four performance measures  namely the solution cost  the number of stored states  the number of generated states  and the actual runtime. a check mark indicates that the algorithm is within one percent of the optimal performancereported in the row. an empty cell indicates that the algorithm does not solve all of our random instances. the table shows the following trends: first  all runtimes are small because the algorithms quickly either solve a search problem or run out of memory. second  kwa* has a smaller solution cost than wa* and msc-wa*. the solution cost of msc-wa* remains roughly the same as the one of wa*  whereas the one of kwa* improves with n compared to the one of wa*  which can be attributed to the breadth-first component  diversity  of kwa*. third  msc-wa* has a smaller number of generated states than wa* and kwa*  which can be attributed to the depthfirst component  commitment  of msc-wa*. fourth  both kwa* and msc-wa* store a smaller number of states than wa* although neither kwa* nor msc-wa* dominates the other one in this respect.  kwa* stores a smaller number of states than wa* only for large values of n.  the small memory consumption of kwa* and msc-wa* allows them to solve all of our random instances of the 1-puzzle  while wa* solves only 1 percent of our random instances  even for the best possible value of w. for kwa*  this result is partly due to our improved implementation of kwa* that stores each state at most once on the open and closed list. however  it turns out that the low memory consumption of our implementation of kwa* is not a systematic effect but due to noise since it requires one to hit the correct value of k precisely  which is not possible in practice.
1 msc-kwa*
although the concepts of diversity and commitment seem to be opposing at first glance  we now combine them: mscwa* splits the open list into a commit list of c states and a reserve list. this makes msc-wa* even more focused than wa*  and thus makes it more likely that mscwa* is led into goal-free regions of the search space by misleadingly small heuristic values. we alleviate this problem by adding to msc-wa* the mechanism used by kwa*. msckwa*  the resulting algorithm  expands in parallel during theeachcommititeration alllistk rather¡Ü c thanstatesjustwithone .the smallestthus  msc-kwa*f-values in has three parameters  namely w  c and k. we eliminate one of them by setting k = c to simplify the experimental conditions  which is reasonable since decreasing k and c both increase the focus of msc-kwa*. table 1 shows that msc-kwa* has a smaller number of stored states  a smaller number of generated states  and a smaller actual runtime than wa*  kwa* and msc-wa*. the savings are significant and increase with n. for example  the number of stored states of msc-kwa* is at least a factor of seven smaller for the 1puzzle than the ones of wa*  kwa* and msc-wa*. the small memory consumptionof msc-kwa* allows it to solve all of our random instances of the 1-puzzle  which wa*  kwa* and msc-kwa* cannot.  kwa* solves only 1 percent and msc-wa* solves only 1 percent of our random instances  even if their parameter values are optimized.  furthermore  the solution cost of msc-kwa* is always within about three percent of the solution cost of kwa*  the algorithm with the smallest solution cost. additional experiments are reported in  furcy  1 . for example  msc-kwa*
 with k 1=hanoi c  solves all of our random instances of the 1-pegwhich wa*  kwa* and msc-wa* can-
towers of
not. note that beam search is a special case of msc-kwa*  namely  with k = c and no reserve list  and thus can be understood as a variant of best-first search with commitment and diversity  not just commitment  the current view . it is future work to determine the best parameters values of msc-kwa* automatically.
