
formal analysis of decentralized decision making has become a thriving research area in recent years  producing a number of multi-agent extensions of markov decision processes. while much of the work has focused on optimizing discounted cumulative reward  optimizing average reward is sometimes a more suitable criterion. we formalize a class of such problems and analyze its characteristics  showing that it is np complete and that optimal policies are deterministic. our analysis lays the foundation for designing two optimal algorithms. experimental results with a standard problem from the literature illustrate the applicability of these solution techniques.
1 introduction
decentralized decision making under uncertainty is a growing area of artificial intelligence addressing the interaction of multiple decision makers with different goals  capabilities  or information sets. markov decision processes  mdps  have been proved useful for centralized decision making in stochastic environments  leading to the development of many effective planning and learning algorithms. more recently  extensions of mdps to decentralized settings have been developed. examples include stochastic games  sgs  or competitive markov decision processes  filar and vrieze  1   and decentralized partially observable markov decision processes  dec-pomdps   bernstein et al.  1 . sgs concentrate mainly on observable domains  capturing the competitiveness among the players. dec-pomdps generalize partially observable mdps to multi-agent settings  modeling cooperative players who may have different partial knowledge of the overall situation.
¡¡this paper addresses the problem of average-reward decentralized mdps. we analyze a class of dec-pomdp problems  where the dependency and interaction among the agents is defined by a common reward function. the agents are otherwise independent and they each have full observation of their local  mutually-independent states. a similar model has been previously studied by  becker et al.  1  with the objective of optimizing cumulative reward over a finite-horizon. in contrast  we analyze the infinite-horizon average reward problem  which has not been previously studied in a decentralized settings.
¡¡the need to optimize average reward has been demonstrated in many applications  including ones in reinforcement learning  mahadevan  1   decentralized robot control  tangamchit et al.  1   decentralized monitoring  sensor networks  and computer networks  rosberg  1; hajek  1 . in these problems  the system operates over an extended period of time and the main objective is to perform consistently well over the long run. the more common discounted reward criterion usually leads to poor long-term performance in such domains.
¡¡one motivation for using discounted reward models-even when the average reward criterion seems more natural-is that the problem is easier to analyze and solve  puterman  1 . but it is not clear if the same argument prevails in decentralized settings. in fact  in some problems the average reward analysis may be simpler because the system may exhibit complex behavior initially  but quickly settle into simple one. one such example is the automaton used to prove that an optimal policy for a pomdp may not be regular  madani  1 . the optimal discounted policy in this simple example is infinite  while the optimal average reward policy is very simple and contains just a single action.
¡¡in the case of pomdps  the analysis of the average reward case is known to be much more involved than the discounted reward criterion. a thorough account of these issues and solutions may be found in  arapostathis et al.  1; puterman  1 . since a dec-pomdp is a generalization of a pomdp  one would expect to face the same difficulties. however  we demonstrate a way to circumvent these difficulties by focusing on a subclass of the problem-the case of observation and transition independent decentralized mdps  becker et al.  1 . the results we obtain for this class of problems are encouraging and they lay the foundation for studying more complex models and competitive settings.
¡¡the main contribution of this paper is the formulation and analysis of the dec1-mdp problem with the average reward criterion. section 1 defines the problem and provides a motivating example. in section 1  we show that calculating the gain of a fixed policy is easy  but finding an optimal policy is np complete. in section 1  we propose two practical algorithms to solve the problem. in addition  these algorithms show a connection with centralized average reward mdps and thus help to analyze the properties of the optimal policies. both algorithms are based on mathematical programming formulations. finally  in section 1 we demonstrate the algorithms on a standard problem  in which the average-reward objective is the most natural one. some of the results we provide are easily extensible to the competitive case. in the interest of clarity  we only mention these extensions in this paper.
1 average reward dec-mdps
this section provides a formal definition of the problem and shows that under certain conditions  the average reward may be expressed compactly  leading to a compact formulation of the problem. the model we propose is similar to the one proposed in  becker et al.  1 . we introduce a different definition  however  because the original is not easily extensible to infinite-horizon problems. to simplify the discussion  the problem is defined for two agents. however  the definition as well as some of the results can be easily extended to an arbitrary number of decision makers.
definition 1. a dec1-mdp is an n-tuple  s a p r  such that:
  a = a1¡Á a1 represents the actions of the players;
  s = s1¡Á s1 is the set of states;
  p =  p1 p1  is the transition probability. each pi is a function: si ¡Á si ¡Á ai ¡ú r. the transition probabilities  s1 s1  ¡ú  s1 s1  given a joint action  a1 a1  is: p s1 s1   s1 s1   a1 a1  = p1 s1  s1 a1 p1 s1  s1 a1 
  r s1 a1 s1 a1  ¡Êr is the joint reward function; and
   s1 s1  is a given initial state for both agents.
¡¡the process  as we defined it  is both transition and observation independent as defined in  becker et al.  1 . transition independence means that the transition to the next state of one agent is independent of the states of all other agents. observation independence means that the observations that an agent receives are independent of the current states and observations of other agents.
¡¡while transition independence is a plausible assumption in many domains  observation independence is somewhat restrictive. it prevents any kind of communication among the agents. this assumption is suitable when communication is prohibitively expensive  risky  or when it does not add much value to the process. when communication is useful  the problem with no observations provides as an easilycomputable lower bound that could help to decide when to communicate.
¡¡we refer to  s1 a1 p1  and  s1 a1 p1  as the local processes. in fact  they represent mdps with no reward. to simplify the analysis  we assume that the state set s and action set a are finite  and that the markov chains for both agents  induced by any policy  are aperiodic. a markov chain is aperiodic when it converges to its stationary distribution in the limit  puterman  1 . we discuss how to relax these assumptions in section 1.
¡¡a policy ¦Ð is a function that maps the history of the local states into an action for each agent. to prevent the need for introducing measure-theoretic concepts  we assume that the policy may only depend on a finitely-bounded number of previous states. if the action selection depends only on the last state of both agents  the policy is stationary  or markov.
¡¡we extend the standard terminology for the structure of average reward mdps to the decentralized case. a dec1-mdp is said to be recurrent or unichain if the local decision processes of all the agents are recurrent or unichain respectively. otherwise  it is said to be multichain  puterman  1 .
¡¡given a fixed policy  the history of the process can be represented by a sequence of random variables. the random variables xt and yt represent the state-action pairs of the two agents at stage t. the corresponding sequence of rewards is defined as. we seek to maximize the expected average reward  or gain  defined as follows. definition 1. the gain of a markov policy is
 .
the expectation subscript denotes the initial value of the random variables  thus x1 =  s1 ¦Ð s1   and y1 =  s1 ¦Ð s1  . furthermore  the gain matrix g is defined as g s1 s1  = g s1 s1 . the actual gain depends on the agents' initial distributions ¦Á1 ¦Á1 and may be calculated as.
¡¡one problem that fits the average reward criterion is the multiple access broadcast channel  mabc   rosberg  1; ooi and wornell  1 . in this problem  which has been used widely in recent studies of decentralized control  two communication devices share a single channel  and they need to periodically transmit some data. however  the channel can transmit only a single message at the same time. thus  when several agents send messages at the same time interval  this leads to a collision  and the transmission fails. the memory of the devices is limited  thus they need to send the messages sooner rather than later. we adapt the model from  rosberg  1   which is particularly suitable because it assumes no sharing of local information among the devices.
¡¡the mabc problem may be represented as a dec1-mdp where each device is represented by an agent. the state space of each agent represents the number of messages in the buffer. there are two possible actions  send and do not send. the arriving data is modeled by random transitions among the buffer states. we provide more details on the formulation and the optimal policy in section 1.
1 basic properties
in this section  we derive some basic properties of dec1mdps  showing how to efficiently evaluate fixed policies and establishing the complexity of finding optimal policies. the analysis is restricted to stationary policies. an extension to non-stationary policies is mentioned in section 1.
¡¡for average reward mdps  the gain of a policy can be calculated directly by solving a set of linear equations  which resemble value function equations in the discounted case  puterman  1 . these value optimality functions cannot be used in dec1-mdps  but as the following theorem states  the gain can be expressed using the invariant or limiting distribution. the limiting distribution p represents the average probability of being in each state over the whole infinite execution.
it is defined as:
.
the limiting distribution can be calculated from a the initial distribution using a limiting matrix. limiting matrix of transition matrix p is defined as
.
then  we have that p = ¦Áp   where ¦Á is the initial distribution.
theorem 1. suppose p1 and p1 are state-action transition matrices of the two agents for fixed policies  and r is the reward matrix. then  the gain matrix can be calculated as follows:
.
¡¡the theorem follows immediately from the fact that both processes are aperiodic and from the definition of gain.
¡¡calculating p  is not practical  because it typically requires calculating all eigenvectors of the matrix. however  the same idea used for calculating the gain in the single agent case can be used here as follows.
theorem 1. for any initial distribution ¦Á and transition matrix p  the limiting distribution p fulfills:
 1 
 1  moreover  if p fulfills  1  and  1  then:
	p =  pt  ¦Á.	 1 
¡¡note that i denotes the identity matrix of the appropriate size and that  1  also implies that . due to space constraints  we do not include the proof  which is a similar derivation to the one in  puterman  1 .
¡¡next  we examine the complexity of finding an optimal policy in this model. the complexity of finite-horizon decmdps has been shown to be nexp complete  while infinitehorizon problems are undecidable  bernstein et al.  1 . while the model we study here is not tractable  the following theorem shows that it is easier than the general problem. however  we focus only on stationary policies  while the truly optimal policy may depend on a longer history. the complexity further justifies the use of the algorithms we present later  since a polynomial algorithm is unlikely to exist.
theorem 1. the problem of finding an optimal markov policy for the aperiodic dec1-mdp is np-complete.
proof. to show that finding an optimal policy is in np  we note that theorem 1 states the optimal markov policies are deterministic. since calculating the gain  given fixed policies is possible in polynomial time  and the policy size is polynomial  all policies can be checked in nondeterministic polynomial time.
¡¡the np hardness can be shown by reduction from satisfiability  sat . the main idea is that the first agent may assign

figure 1: a sketch of the dec1-mdp created from a 1-sat problem with 1 clauses and 1 variables.
values to variable instances to satisfy the problem  and the second one can assign values to variables. the agents are penalized for assigning a different value to the actual variable and to the variable instance.
¡¡let s =  v c  be a 1-sat problem in conjunctive normal form  where v is the set of variables and c is the set of clauses. if the literal l is an instantiation of variable v  then we write l ¡« v. we construct a corresponding dec1-mdp in the following way. the state and action sets for the agents are:
s1
s1= ={s f}¡È{lij 
vi ¡Ê c j = 1 1}a1
a1= ={t f}
{t f}each state lij ¡Ê s1 represents the j-th literal in the i-th clause  which is an instantiation of a single variable. states s1 represent the variables of the problem. the actions represent whether the variable value is true or false. the main idea is to make the first agent assign arbitrary values to variable instances to satisfy the formula. the second agent then determines whether the value assigned to multiple variable instances is the same.
¡¡for lack of space  we omit the complete definition of transition matrices. a sketch of the reduction is shown in figure 1. the first agent follows the transitions depicted when the action is chosen such that the literal is not true. otherwise  the agent transitions to state s. for both agents  for each state  there is a small probability    1  of remaining the in same state. for the second agent  the transition probabilities are independent of the actions taken. notice that this process is aperiodic.
¡¡the rewards are defined such that a negative reward is received when the value assigned by the first agent to an instance of a variable is different from the value assigned to the variable by the second agent. in addition  a negative reward is received when the first agent transitions to state f. notice that this happens only when the assigned variable instance values do not satisfy at least one clause.

in all other cases the reward is zero.
¡¡it is easy to show that the gain of the above dec1-mdp is 1 if and only if the sat problem is satisfiable.	
1 optimal dec1-mdp algorithms
in this section we present two methods for finding optimal policies for dec1-mdps. both methods are based on formulating the problem as a mathematical program. we also discuss additional approaches to solving the problem.
¡¡in previous work  a similar problem was solved using an iterative method that fixes the policy of one agent  while computing the optimal policy of the other agent  nair et al.  1 . such methods are unlikely to produce good results in the average-reward case  because of the high number of local minima. in contrast  the following milp formulation facilitates an efficient search through the set of all local minima.
1 quadratic programming solution
in this section  we give a mathematical program that defines the optimal solutions. the reward definition is based on theorem 1. we also discuss how to obtain the optimal policy from the solution of the quadratic program.
maximize	subject to	p1 ¡Ý 1 p1 ¡Ý 1
 j ¡Ê s1 p1 j a    1 p1 s a p1  j  s a  = 1
	a¡Êa	s¡Ês a¡Êa
 j ¡Ê s1 p1 j a  + 1 q1 j a  
	a¡Êa	a¡Êa
1  q1 s a p1  j  s a  = ¦Á1 j  s¡Ês a¡Êa1
 j ¡Ê s1 p1 j a    1 p1 s a p1  j  s a  = 1
	a¡Êa	s¡Ês a¡Êa
 j ¡Ê s1 p1 j a  + 1 q1 j a  
	a¡Êa	a¡Êa
1  q1 s a p1  j  s a  = ¦Á1 j   1  s¡Ês a¡Êa1
¡¡the benefit of this formulation is not so much the computational efficiency of the solution technique  but that it helps identify some key properties of optimal policies. later  we use these properties to develop a more efficient formulation  which is also shown to be optimal.
theorem 1. an optimal solution of  1  has the optimal gain.
¡¡we omit the proof due to lack of space. the correctness of the constraints follows from the dual formulation of optimal average reward  shown for example in  puterman  1   and from theorem 1. the choice of the objective function follows from theorem 1.
¡¡the solution of  1  may also be used to determine an optimal policy  given an optimal value of the program's variables. the procedure is the same as determining an optimal policy from a solution to the dual of an mdp. that is  the optimal policy is randomized such that for each recurrent state:
p.
this is well-defined because the recurrent states are those for which. for transient states the policy can be determined as
p.
proposition 1. the policy constructed according to the above description is optimal.
¡¡the proof is only a small variation of the proof of optimal policy construction for average reward mdps. see  for example   puterman  1 .
   it has also been shown that for any average-reward mdp there exists an optimal deterministic policy. notice  that when p1 is fixed  the formulation for p1 is equivalent to that of the dual of average-reward linear program. thus  for any optimal   we can construct a deterministic policy as follows. fixing  we can find p 1 with the same gain  by solving the problem as an mdp. following the same procedure  we can obtain also a deterministic p 1. hence the following theorem.
theorem 1. there is always a deterministic markov policy for dec1-mdp with optimal gain.
1 mixed integer linear program
this formulation offers a method that can be used to calculate the optimal policies quite efficiently. the approach generalizes the approach of  sandholm et al.  1   which is based on game-theoretic principles and works only for matrix games. our approach is based on lagrange multipliers analysis of the problem and thus may be extended to include any additional linear constraints.
¡¡for clarity  we derive the algorithm only for the unichain dec1-mdp. the algorithm for the multi-chain case is similar and is derived in the same way. to streamline the presentation  we reformulate the problem in terms of matrices as follows:
	maximize	
	subject to	t1 = 1	t1 = 1
 1 
et1 p1 = 1 et1 p1 = 1 p1 ¡Ý 1 p1 ¡Ý 1
 where e1 and e1 are vectors of ones of length n1 and n1 respectively  and t1 and t1 are matrix representations of the 1rd and the 1th constraint in  1 .
¡¡the mixed integer linear program  milp  formulation is based on application of the karush-kuhn-tucker  kkt  theorem  bertsekas  1 . thus  the lagrangian for the formulation  1  is
l p1 p1 ¦Ë ¦Ñ ¦Ì  =
=	pt1 rp1 + ¦Ë1 et1 p1  1  + ¦Ë1 et1 p1  1  + +	¦Ñt1 t1 + ¦Ñt1 t1 + ¦Ìt1 p1 + ¦Ìt1 p1.
¡¡let a1 and a1 denote the sets of active constraints for inequalities p1 ¡Ý 1 and p1 ¡Ý 1 respectively. following from the kkt theorem  every local extremum must fulfill the following constraints:
 rp1 =  ¦Ë1  t1t¦Ñ1  ¦Ì1 rtp1 =  ¦Ë1  t1t¦Ñ1  ¦Ì1 p1 i  = 1  i ¡Ê a1	p1 i  = 1  i ¡Ê a1 ¦Ì1 i  = 1  i /¡Ê a1	¦Ì1 i  = 1  i /¡Ê a1 ¦Ì1 i  ¡Ý 1  i ¡Ê a1	¦Ì1 i  ¡Ý 1  i ¡Ê a1
the gain in a local extremum simplifies to:
	 	 	 	   
because	i	    = 1 ¦Ìi = 1.
though the above constraints are linear  the problem cannot be directly solved since a1 and a1 are not known. they can however be represented by binary variables b1 i  b1 i  ¡Ê {1}  which determine for each constraint whether it is active or not. therefore  we can get the following milp: maximize  ¦Ë1
subject to t1 = 1 t1 = 1 et1 p1 = 1	et1 p1 = 1
rp1 =  ¦Ë1  t1t¦Ñ1  ¦Ì1
rtp1 =  ¦Ë1  t1t¦Ñ1  ¦Ì1
¦Ì1 ¡Ý 1¦Ì1 ¡Ý 1¦Ì1 ¡Ü m1 p1 ¡Ý 1¦Ì1 ¡Ü m1 p1 ¡Ý 1p1 ¡Ü e1  b1
b1 i  ¡Ê{1.1  1}¡¡in this program  m1 and m1 are sufficiently large constants. it can be show that they may be bounded by the maximal bias  puterman  1  of any two policies. however  we are not aware of a simple method to calculate these bounds. in practice  it is sufficient to choose a number that is close the largest number representable by the computer. this only eliminates solutions that are not representable using the limited precision of the computer.
¡¡the optimal policy can be in this case obtained in the same way as in subsection 1. notice that ¦Ë1 = ¦Ë1  so unlike a competitive situation  one of them may be eliminated. the preceding analysis leads to the following proposition.
proposition 1. the gain and optimal policy obtained from  1   and  1  are equivalent.
¡¡though we present this formulation only for cooperative cases  it may be easily extended to the competitive variant of the problem.
1 application
this section presents an application of our average-reward decentralized mdp framework. it is a simple instance of the
s1s1s1s1s1s1s1s1a1nnnsssssa1nnsssfigure 1: sample policy for agents a1 and a1. in each state  action s represents sending and n  not sending. the states are s1 - s1.
practical application introduced in section 1. the experimental component of the paper is designed to illustrate the scope of problems that can be solved quickly using our new algorithm.
¡¡as mentioned earlier  the problem we use is an adaptation of the multiple access broadcast channel problem solved in  rosberg  1 . in order to make the problem more interesting  we assume that the following events may occur: a single or two messages arrive  a single message in the buffer expires  or nothing happens. these events occur independently with some fixed probabilities:  ¦Á ¦Â ¦Ã ¦Ä .
¡¡the communication devices may have buffers of fixed but arbitrary length  unlike the original problem. in addition  the possible actions are send and not send. for the action not send  the probabilities are updated according to the list stated above. for the action send  the buffer level is decreased by 1  and then the transition probabilities are the same as for the action not send. though  rosberg  1  derives a simple analytical solution of the problem  it is not applicable to our extended problem.
¡¡the reward associated with overflowing either of the buffers of the agents are r1 and r1 respectively. if both agents decide to broadcast the message at the same time  a collision occurs and the corresponding reward is r. the above rewards are negative to model the cost of losing messages. otherwise  the reward is zero.
¡¡we ran the experiments with the following message arrival probabilities  1 1 1 1 . an optimal policy for the problem with rewards r1 =  1 r1 =  1 r =  1 and buffer sizes of 1 and 1 is depicted in figure 1. while the optimal policy has a simple structure in this case  the algorithm does not rely on that. note that choosing different thresholds for sending messages leads to significantly lower gains.
¡¡to asses the computational complexity of the approach and its practicality  we ran experiments using a basic commercially available milp solver running on a pc. the solver was a standard branch-and-bound algorithm  using linear programming bounds. the linear programs to calculate the bound were solved by the simplex algorithm. for all our runs  we fixed the transition probabilities and experimented with two sets of rewards. the first was r1 =  1 r1 =  1 r =  1 and the second was r1 =  1 r1 =  1 r =  1.
¡¡the results are summarized in figure 1. they show that the time required to calculate an optimal solution is below 1 seconds for problems of this size. it is clear from the results that the different reward sets have significant impact on how quickly the problem is solved. this is quite common among branch-and-bound algorithms. for larger problems  it may be beneficial to use one of the branch-and-cut algorithms  which often offer good performance even on large problems.
n1n1rsgt  sec. 11-1111-1111-1111-1111-1111-1111-1e-1.111.1figure 1: performance results: n1 and n1 represent the number of states of each agent  rs is the reward set  g is the gain  and t is runtime in seconds.
1 conclusion
we introduced an average-reward model for observation and transition-independent decentralized mdps. analysis of the model shows that finding optimal joint policies is easier compared with the general dec-pomdp framework  but it remains intractable. using standard mathematical programming  we designed algorithms that can solve problem instances of substantial size for decentralized problems. while we expect performance to vary depending on the actual structure of the problem instance  the efficiency of the techniques is encouraging.
¡¡the definitions and policy analysis that we presented generalize in a straightforward manner to problems with an arbitrary number of decision makers. the quadratic program formulation may be generalized as well  but the objective function becomes a high-degree polynomial. this  unfortunately  makes the analysis that leads to the milp formulation inapplicable.
¡¡one issue that we have not addressed here is the use of nonmarkov policies. the algorithms we have presented can be extended to find the optimal policies of problems with dependency on a longer history than a single state with increased complexity. however  the question whether the optimal policy for any problem is finite or may be infinitely long remains open.
¡¡in future work  we plan to relax some of the limiting assumptions  most importantly  allowing observations in the model. this extension is not trivial  since such observations break the independence among the state-action visitation probabilities.
¡¡this work lays the foundation for expanding models of decentralized decision making to address the practical averagereward objective. moreover  the milp formulation we introduce could be adapted to dec-mdps with other optimality criteria. this opens up several new research avenues that may help tackle more complex domains and competitive settings.
acknowledgments
this work was supported in part by the air force office of scientific research under award no. fa1-1 and by the national science foundation under grants no. iis1 and iis-1.
