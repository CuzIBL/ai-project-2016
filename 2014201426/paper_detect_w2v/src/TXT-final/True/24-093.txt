 
this work uses an alignment approach for classifying objects according to their shape similarity. previous alignment methods were mostly limited to the recognition of specific rigid ob-
jects  allowing only for rigid transformations between the model and the viewed object. the current work extends previous alignment schemes in two main directions: extending the set of allowed transformations between the model and the viewed object  and using structural aspects of the internal models  namely  their part decomposition. 
the compensating transformation is divided into two parts. the first  rough alignment  compensates  approximately  for changes in viewpoint and is derived by matching tangen tial points on the silhouette of the model and the viewed object. the second  the adjustment transformation  is derived by matching local features - discontinuities of the contour orientation and curvature. 
principal aspects of the scheme suggested here are also relevant for the recognition of flexible objects. 
1 	introduction 
object recognition is a process of establishing a correspondence between a viewed object and an internal representation of a previously known one. the recognition process can rely on different cues  and its results can be at different levels of specificity. the same object can be recognized  for instance  as a furniture  a chair  or the particular chair in my office. an adequate definition of the problem addressed by a recognition method must therefore include the cues the method relies on  and the specificity level at which the answer is expected. recent reviews of many of the methods proposed for object recognition can be found in  besl and jain 1; chin and dyer 1; ullman 1 . 
   *the research described therein was preformed in the weizraann institute of science  rehovot  israel. 
+s.u. was supported in part by nsf grant iri 1. 
shimon ullman+ 
dept. of brain and cognitive science 
massachusetts institute of technology 
cambridge  ma 1  usa; 
dept. of applied mathematics 
weizmann inst  of science  rehovot  israel e-mail: 	shim ai.mit.edu 
　in this paper we address several aspects of classifying objects at their so-called  basic object  level  relying on similarities of their visual shape. basic object classification is the name we normally use spontaneously to describe an object. a category at this level usually has a one word name with no one word subclasses  such as a  chair  as opposed to  furniture  and a  kitchen chair . categorization at this level has been shown by several researchers  rosch ct.al 1; newport and bellugi 1  to be achieved faster than at other classification levels. furthermore  rosch and her collaborators  rosch et.al. 
1  have also shown that shape similarity among class members is more significant  at. the basic object level than at other possible levels  rosch et.al 1  pp. 1   indicating that this level is a natural domain for classification methods based on shape similarity. these findings also suggest that the procedures used for classification should be relatively fast and simple compared to those used for a more detailed level of recognition. 
　in the method proposed in this paper object  classes are represented by single prototypes  rosch ct.al 1; bajcsy and solina 1 . the differences between class members are considered as deformations of the shape of the stored prototype. therefore  the proposed classification method applies in many respects also to objects that can actually undergo non-rigid deformations  e.g.  animals  plants  etc. . 
　the classification method belongs to the family of alignment methods  as presented recently by a number of researchers  e.g.  fischler and holies 1; lowe 1; faugeras and hebert 1; ullman 1 . some of these methods employ pictorial models as the internal representation of the objects that are to be recognized. the models can be  for instance  the edge map of an image of these objects  augmented with some depth information  basri and ullman 1 . the alignment method proceeds by compensating for the transformation separating the viewed object and the stored model. a small number of special features can be extracted from the image and used to derive the transformation required for aligning the model with the image. since the identity of the viewed object is not known at this stage  this alignment transformation is deduced for  and applied to  all the relevant stored models. after this alignment stage  the correct model is expected to be significantly more similar to the image object than any of the competing 
	shapira and ullman 	1 

internal models. a simple comparison procedure in the subsequent stage  such as a simple correlation  will then be sufficient to indicate the correct model. 
　the pictorial nature of the representation used in the alignment approach  as opposed to more abstract attributes  and the exact alignment transformations that are derived  make the approach suitable for recognizing specific rigid objects. alignment methods have been applied to recognize both flat and solid objects  yielding encouraging results  e.g.  fischler and bolles 1; lowe 1; faugeras and hebert 1; basri and ullman 1 . 
1 	an extended sat of compensating transformations 
alignment methods for object recognition have been applied in the past to rigid or almost rigid objects. to cope with shape changes associated with different members of a given class  two major extensions have been introduced to the basic alignment method. the first is using a broader set of transformations than the rigid motion and uniform scaling used for rigid object recognition. such a set should compensate for two types of differences between the viewed object and its stored model: changes in viewpoint  and possible shape differences between the stored prototype and another member of its class. 
　the components of the compensating transformation associated with these two sources of variations are treated separately. a simplified procedure  called rough alignment  is used for compensating for viewpoint differences between the internal models and the object in the image. the accuracy of this alignment is considerably lower than the alignment used in recognizing rigid objects. this version is particularly adequate  however  when the alignment is followed by a second stage of compensating transformation. such a simple process of preliminary alignment may also be useful for a variety of recognition schemes  other than the one presented below. 
　the role of the second stage of the compensating transformation is to account for actual shape differences between the viewed object and the  prototype  model. this part is denoted as adjustment. in principle  it might have been useful to associate with each of the internal prototypes a minimal set of allowed transformations. such a minimal set would account for all possible members of the modeled class and at the same time exclude all non-member objects. however  it is difficult to predefine such a minimal set of transformations. although the set of transformations relating one position of  e.g.  scissors to another is feasible  what would be the transformations that relate all possible positions of a shirt  or different types of chairs  this suggests a different approach: allowing liberal and flexible general deformations  and then assessing the distortion that was required in order to bring each of the models and the image into correspondence. 
　the general scheme for aligning a stored prototype with a viewed object is therefore the following. first  a rough alignment procedure is used to compensate approximately for changes in viewing position. in a second stage  more flexible and general distortions are used to 
1 	vision 
bring each candidate model into correspondence with the image. the amount of distortion required at this stage will be used to asses the quality of match. 
1 	the internal representation of object classes 
alignment schemes for rigid object recognition usually employ pictorial  unarticulated internal models. the current classification scheme has been extended  to specify explicitly the principal parts of the model. the purpose of adding the object's part decomposition to the pictorial representation is to obtain a convenient and natural control of the deformations that are allowed for these models. many of the allowed deformations are easier to specify in terms of relatively simple changes that may be applied to parts of the objects. the model used for the  car  class of objects  as well as its part decomposition  are shown in figure 1. 

figure i: the  car  class model   a  the model   b  the different parts composing the model. 
   relying on the part decomposition of objects for their recognition is usually associated with methods that employ a more structural approach to recognition  eg.   marr and nishihara 1  biederrnan 1; bajesy and solina 1  . there are two major differences between these methods and the one presented here. the first is that parts are defined here only in the internal model and are not extracted from the image. this is advantageous since bottom-up processes for extracting object parts are often difficult and unreliable. the second difference con cerns the expressive power of the internal representation being used. we only use parts as means of controlling and restricting the applied deformations. the eventual comparison between the models and the viewed object relies on complete pictorial descriptions that are not restricted by the use of any predetermined set of generic shapes or geometrical relations. 
　two dimensional models are used in our scheme to represent the object classes. using a finite number of two dimensional models is not sufficient for representing all views of a general solid object  even when using an alignment transformation . part of the inaccuracies resulting from the  flatness  of the models are expected to be corrected by the second part of the compensating transformation  namely  the adjustment. 
　finally  we point out that a parametric line representation is used in the implemented scheme for describing both the viewed images and the internal models. the representation  consisting of straight line segments and circular arcs  is not obtained in an all-automatic procedure; it involves at present some user interaction. 

　the subsequent sections are organized as follows. section 1 describes the process of rough alignment. in sections 1 and 1 we describe a method used for establishing correspondence between points of orientation discontinuity in the model and the image  and using this correspondence for extracting the appropriate adjustment transformation. finally  section 1 discusses various possibilities for evaluating the overall degree of match between the models and the viewed object and examines one of them in detail. 
1 	rough alignment 
1 	approximating 1-d by 1-d transformations 
the role of the rough alignment part of the compensating transformation is to account  approximately  for the viewpoint differences. due to its simplicity and robustness to partial occlusions  such a procedure is also appropriate as a preliminary transformation stage for other recognition methods. 
　rough alignment is different from what would be a perfect alignment of the internal representation with the viewed object in two ways. the first difference is that the procedure can only be applied for a restricted range of in-depth rotations  i.e.  about an axis in the image plane  of the particular internal model. typically  this range is ＼1＜. limiting the value of the in-depth rtation avoids the need to eliminate hidden lines. as discussed bellow  it also allows to approximate the actual  1-d  rigid object transformation by a simpler  1-d one. such viewer centered models have also been used for rigid object recognition  e.g.  koenderingk and van doom 1; basri and ullman 1  and are supported by psychological evidence  e.g.  palmer et.al. 1 . 
　the second difference is conceptual in nature. for rigid objects viewed from different positions  the notion of correct alignment is well defined. for objects that undergo shape changes as well as changes in viewpoint  the  correct  alignment transformation is more difficult to define uniquely. in this case  the best 1-d alignment can be defined as the one that minimizes some global difference measure between all matched point pairs in the image and the model. such an alignment transformation is difficult to compute and  due to possible shape changes  it also leaves many model points far apart from their image counterparts  preventing the immediate application of a simple comparison between the model and the image. 
　a different approach  using a simplified procedure for extracting and applying an approximate alignment to 1d object models  is therefore proposed. the main goal of this simplified procedure is to facilitate the second part of the compensating transformation  namely  the adjustment. 
　to account for different types of shape changes  the adjustment transformation must be determined by many parameters. extracting these parameters requires that many features be matched in the model and the image. this matching can be facilitated if  following the rough alignment  the viewed object and the candidate models  as well as their corresponding parts  would substantially overlap in the image plane  obviously  the requirement for overlap of corresponding parts may be meaningless for wrong models . when such an overlap is accomplished  points in the transformed model are likely to be close to their counterpart in the image object. 

figure 1: applying rough alignment to a box-shaped object   a  the image used as the model   b  an overlay of the model  in thicker lines  and the  viewed image  in an arbitrary positioning. both are images of the same object in different positionings. the in-depth orientation is different by 1＜ about both the height and length of the box.  c  the results of applying the enhanced similarity transformation to the model. 
　a two-dimensional  enhanced  similarity transformation is used for rough alignment. the transformation is determined by five parameters: two for translation  one for rotation  and two for scaling along two perpendicular axes -- all within the image plane. the results of roughly aligning a box-shaped object using the enhanced 1-d similarity transformation are shown in figure 1. qualitatively  the situation following rough alignment seem to enable the use of a distance measure in matching model and image anchor features. 
1 	anchor features for rough alignment 
the extraction of the rough alignment transformation can rely on partial information in the image using a small number of image features  called  anchors  for the transformation . matching the center of mass and the 
two inertia moments of the model and the viewed object  for instance  is sufficient for determining the five parameter of the enhanced similarity transformation. alternative anchors for this purpose are any combination of two matched points and a direction. we use local feature points labeled by global properties as the anchor features. such a combination provides robustness against partial occlusions  unlike the case of relying just on global moments of the shape  for instance  without imposing an extensive search on matching the anchor features  as matching local features often does . 
　the global properties used to label the local anchor features are the bounding contour of the viewed object  also called the silhouette   and its prominent orientation. a simple algorithm is used to extract a rather primitive version of the silhouette. this version consists in associating a  silhouette  binary label to each image line which has no neighbors on one of its sides  along some portion of its length. the existence of neighbors is verified by examining possible intersections of rays extending from each image line  perpendicular to it  with 
	shapiro and ullman 	1 

other contours in the image. the silhouette found for an image of a car is shown in figure 1 a . 
　there are a number of ways to define the prominent orientation in an image of an object. one possibility is to use the global inertia moments of the object's area. similarly to other moments of the shape  however  such a definition is sensitive to partial occlusions. other possible definitions may rely on common direction of texture elements  the direction of symmetry axes  or the common direction of long lines in the image. 
   here we define the prominent orientation of the viewed object to be the most frequent orientation of the segments in its silhouette. such a definition is both local and  for many objects  it also agrees with the general elongation direction. the prominent orientation extracted for a car image is shown in figure 1 a . this direction corresponds to the highest peak in an histogram of the orientations of the segments in the object's silhouette  where each contribution is weighted by the length of the segment . 
　having a prominent orientation in the model and image can be exploited by using orientation  relative to the prominent one  to label local features  such as end-points of long straight lines  orientation discontinuities  etc.  and to facilitate their matching. we use the prominent orientation as reference for labeling tangential points on the silhouette according to their tangent orientation. to reduce the number of points that need be extracted in the image  they are marked on all of the internal models for a  standard  set of tangent orientations. 
　the tangential points extracted for four tangent orientations  1＜  1＜  1＜  and 1＜  are marked in figure 1 a  by small 't'-s. they are extracted by sorting extremum contour points in the direction perpendicular to the set of tangent orientations. as demonstrated below  using the tangential points for extracting the rough alignment parameters can be made fairly robust against partial occlusions. the implications of other possible sources of sensitivity  namely  in-depth rotations and shape changes are discussed elsewhere  shapira 1 . 
1 	extracting the rough alignment parameters 
the difference between the prominent orientations of the viewed object and the internal model is used to determine the rotation parameter  within the image plane . then  a one-to-one correspondence is established between tangential points of the same orientation. the scaling factors along the prominent orientation of the model and along the direction perpendicular to it are determined by comparing the respective components of the distance between pairs of  matching  tangential points. having applied the appropriate rotation and scaling  the translation is determined by comparing the location of corresponding tangential points. 
   to avoid the effect of lateral displacement of the tangential points  which may result from in-depth rotations  partial occlusions  and shape changes  both scaling and translation are determined only according to the perpendicular components of the tangential points. pairs of tangential points contribute to a scaling factor only 
1 	vision 
if the line joining them is roughly perpendicular to that of the tangent at both points. similarly  each tangential point contributes only a translation component perpendicular to its tangent orientation. the total translation vector is computed as the one that agrees best with all the perpendicular components that are contributed by the individual points. 

figure 1: rough alignment for real images   a  the silhouette of the viewed object  in thicker lines  with the prominent orientation  a short arrow inside the object  and the tangential points  thick 't'-s on the silhouette  marked in it.  b  the model  an image of the same object rotated by 1＜ and 1＜ about the image x and y axes    c  an arbitrary configuration of the model  in thicker lines  and the viewed object   d  the result of applying rough alignment to the model   e  overlaying the  roughly  aligned model and the image   f  
rough alignment with partial occlusion a  naive  rough alignment relying on global shape moments   g  the result of applying the proposed procedure in the occluded case. 
　the result of applying the rough alignment algorithm to the different views of the same car is shown in figure 1. one of the images 1 b  is taken as the internal model  while another image 1 a  is considered to be the viewed object. in the initial configuration the overlap between the image and model is small  1c . following the rough alignment it increases significantly  1e   facilitating the subsequent adjustment stage. figures 1 f g  demonstrate the insensitivity to partial occlusions. a demonstration of what may be denoted as a  naive rough alignment  for an occluded version of the object in figure 1 a   is shown in 1 f . it relies on global moments such as the center of mass and the area. comparing the latter figure with figure 1 g  demonstrates that the results obtained by the rough alignment algorithm are significantly more robust to occlusions. 
　the procedure proposed here for rough alignment is thus simple and reliable. applying it to the internal model yields an approximate  rather than a perfect alignment with the viewed object. this facilitates  however  the subsequent stage. in the next section we discuss the subsequent adjustment  assuming that the models are already roughly aligned with the viewed object. 

1 anchor features for the adjustment transformation 
1 	extracting corners 
the adjustment transformation  described in section 1  is based on the extraction and matching of many features  the anchors . in this section we describe briefly the extraction of one type of anchors  called  corners   which are points of discontinuity in orientation or curvature. the significance of such points in image perception has been supported by a number of studies  e.g.   attneave 1; biederman 1; link and zucker 1  . other types of anchor points  such as inflections and small blobs are also possible  but will not be discussed here. 
　orientation and curvature discontinuities can be found by locating intersecting pairs of straight segments and circular arcs  by solving the appropriate equation system . as lines in the parametric line representation often do not actually intersect  configurations in which lines  almost intersect  need also be considered. these include locations at which two co-terminating lines form a point of orientation discontinuity  as well as other configurations such as points at which lines co terminate  almost  linearly  t-like junctions of contours  and configurations in which a straight line is  almost  tangential to a circular arc. 
　the extent by which a given line segment may be extended to the intersection point is determined by a threshold that measures the quality of the resulting corner. this measure  v  was taken to be: 

where li is the length of the i-th line segment  and ci is the length by which it had to be continued to the intersection point  i = 1 . for  almost tangential  intersections  a third term  measuring the distance by which the straight line missed the circular arc  is added: 

where r is the radius of the arc  and d the distance to the straight segment. intersection points are accepted only for v values smaller than 1. using this threshold for 'virtuality'  about 1 corners were extracted from a typical object image  such as figures 1 a  and 1 b  . each corner is represented internally using its location  the angle it formed  its bisector  pointers to the image lines that generate it  and a silhouette label  reflecting the silhouette labels of its generating lines. 
1 	matching corners 
the next stage in the adjustment process involves the matching of model and image corners. a subset of the model corners used in the adjustment phase is shown in figure 1a. the corners matching proceeds in two stages. in the first  all the image corners compatible with each model corner are marked. the compatibility is determined by local parameters: distance between the corners  direction of bisectors  angle size  and silhouette labels. 
　in the second stage  the best matching image corner is selected from the set of compatible ones. this is obtained by examining the degree of match between the generating lines of the model and each of the compatible image corners. for each image corner  the model corner is displaced and rotated in the image plane so that its location and bisector orientation would be identical to those of the image corner. then  the mismatch between the generating lines of the model and image corner is evaluated  based on the fraction of matched portions of the lines and their residual misalignment  for the exact formula  see  shapira 1  . 

figure 1: corresponding corners   a  the corners selected as anchors for the adjustment in the car class model  marked by small dots    b  the corrsponditig silhouette corners matched in the image. 
　this procedure yields good matches for model and image corners that lie on the silhouette of the object. the image counterparts found for the silhouette model corners in figure 1 a  are shown in figure 1 b . the results obtained for internal  non-silhouette  model corners are considerably inferior. the difference in performance for silhouette and internal corners is due to the nature of the silhouette label that cuts down significantly the number of compatible matches for silhouette  but not for internal  contours. 
1 	the adjustment transformation 
following the corner matching  an adjustment transformation is applied independently to each of the model parts. each part is allowed to undergo an affine transformation that will make it as similar as possible to a corresponding image part. the affine transformation is determined by six parameters  determining a displacement and a linear transformation: 

where  x' y'  are the coordinates of the transformed model point  originally located at  x y   and  dx dy  is the translation vector. this transformation may be interpreted as the image plane projections of a planar object undergoing a general rigid motion in 1-d space. in general  the number of matched anchor points of each model part will be large enough to over-constrain the affine transformation parameters. in such a case  the affine transformation can be determined by computing the average displacements of the anchor points  and usshapira and ullman 1 

ing pseudo inverse to determine the linear transformation. 
　the affine transformation is somewhat limited  and may not be sufficient to account well for the observed distortion. we have used instead a more flexible transformation  defined as follows. the part points that are close enough to a matched anchor point are translated in the image by the same displacement vector as the neighboring anchor. the remaining part points are transformed according to the affine transformation determined by all the matched anchors of that part. the resulting transformation is more flexible  but its extent is still easy to evaluate by examining the parameters of the affine transformation involved. 
　the result of applying this transformation to the model parts that lie on the silhouette is shown in figures 1 a-c . it should be noted that using the line representation avoids the need to transform all the points that comprise the contours of a given model part  since it is straightforward to compute the effect of the affine transformation on straight segments and circular arcs. 

	 a  	 b  

	 c  	 d  
figure 1: the adjustment transformation   a  the silhouette of the model   b  the result of applying the combined transformation to model parts that he on the silhouette   c  overlay of the transformed model silhouette  in thicker lines  over the viewed object   d  overlay of the entire adjusted model and the object's image. the internal parts were transformed relying on the transformations of the silhouette parts  in b  and the inter-part relations. 
　the above procedure applies only to points along the silhouette. the transformation must then be extended to internal contours as well. a possible means to this aim is to impose certain relations between the parts defined in the model. since deformation modes are usually not exclusive to specific objects or classes  a finite set of prototypical relations between parts may be relevant for many objects and classes. we used four generic types of such relations and associated with them respective ways for extending the transformation  known for the silhouette  onto the internal parts. the relevant details are discussed elsewhere  shapira 1 . 
　the result of transforming the internal parts of the car class models according to the known transformation of the silhouette parts is shown in figure 1 d . similar results were also obtained for different car objects. the results of the entire adjustment phase for both silhouette and internal parts appear to be promissing. the 
1 	vision 
transformed silhouette parts  however  appear to match the corresponding parts of the viewed object better than the internal ones. one possibility of improving the results for the entire model is to use a more efficient global label than the silhouette. a particular proposal as to such a global label  as well as a working matching algorithm  are discussed elsewhere  shapira 1 . 
1 	evaluating the distortions 
the alignment and adjustment bring the model into close agreement with the viewed object. this process can be applied with considerable success  i.e.  obtaining good match between the model and the image contours   also in the case of matching the wrong model for the viewed object. in such a case  however  the distortion required in the process is expected to be large and unnatural. this is illustrated in figure 1 where the class model for cars has been matched to an image of a telephone. the final stage evaluates the amount of distortion that was required in the process. the final classification is obtained by selecting the prototype that requires the smallest  most natural  distortion. there are two possible strategies for evaluating the applied deformations: using general criteria that apply to many object classes  or using model-specific criteria for distinguishing between allowed and unreasonable distortions. here we examine only a very simple scheme of the general type. the method is probably insufficient  but it does suggest that the model-invariant strategy merits further consideration. 

figure 1: adjustment in the  wrong case    a  the image of the telephone  with its receiver displaced . the corners that have been matched to the model anchor corners are marked.  b  the adjusted model  in thick lines  overlaid over the telephone image  see text for discussion . 
　to evaluate the induced distortions  we compare the parameters of the adjustment transformations applied to the different model parts. the distortion is considered larger as the parameters of the transformation applied to its parts become more different  
　a simple statistical analysis was applied to the parameters of the affine transformation extracted for the parts of the car class model. the scalar mean and the standard deviation of each of five parameters are computed for 1 parts  the linear coefficients of the affine transfrmation are replaced here by the more intuitive rotation and scalings . we compare the results obtained for two cases: the 'correct' case of fitting the car model to an actual car image  and fitting it to a telephone. the results are summarized in table 1. an examination of the data reveals that a marked difference exists between the 


table 1: the mean values and standard deviations for the parameters of the combined transformation applied to the different car model parts in two cases: deforming that model into a car object  and into a telephone. 
standard deviations in the two cases. the criterion used can be extended in several ways. for example  one may use the spatial variance of these parameters  i.e.  giving a larger weight to the difference between the parameters of two parts if they are closer together. nevertheless  the results seem promising  considering the simplicity of the computation that is performed. 
1 	summary 
in this work we use the alignment approach to devise a scheme for classifying objects according to shape similarity. while the complex classes such as  furniture  require more abstract  symbolic  information  it appears that basic level classification is often based on shape similarity of the type used in this work. 
　the central theme of the method is that objects can be classified by applying compensating transformations to simple pictorial descriptions  essentially two dimensional   used as the internal models. the classification scheme proceeds through the following three steps. first  a rough alignment process compensates for overall shift  scaling  and rotation. an important aspect of this stage is that it does not rely on global parameters that are sensitive to occlusion. second  an adjustment transformation is applied by matching features  corners . different parts  defined in the model only  may undergo different transformations. third  the quality of the match is evaluated based on the variability of the transformations applied to different parts. despite the simplicity of the implemented procedures  results of applying this scheme support the notion that applying compensating transformations to pictorial models can play a useful role in detecting shape similarities among object  and in object classification. 
