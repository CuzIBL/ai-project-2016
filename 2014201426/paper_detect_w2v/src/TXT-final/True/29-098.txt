 
a new variant on key feature object recognition is presented. it is applied to optimal matching problems involving 1d line segment models and data. a single criterion function ranks both key features and complete object model matches. empirical studies suggest that the key feature algorithm has run times which are dramatically less than a more general random starts local search algorithm. however  they also show the key feature algorithm to be brittle: failing on some apparently simple problems  while local search appears to be robust. 
1 	introduction 
to find an object  recognition algorithms often first seek small sets of features which predict the object's presence. this ideas is articulated by roberts  roberts  1  and is the heart of local feature focus  bolles and cain  1 . it suggests the value of perceptual organization  lowe  1  and is fundamental to the alignment approach  huttenlocher and ullman  1 . it is also a basic component of geometric hashing  lamdan et a/.  1  and continues to be refined  olson  1 . 
　here we present a variant upon this general theme which searches for an optimal match by first searching for good matches between triples of object model and image features. for simplicity  we call this our key feature algorithm since the search for a good triple may be thought of as a search for a key feature which determines the rest of the match. the key feature algorithm uses the same criterion function as our local search line matching algorithms  beveridge et al  1; beveridge  1; beveridge et al  1  and this enables us to report on a set of side-by-side comparisons between the two. since these two algorithms search the space of possible matches in very different ways  the comparison highlights strengths and weaknesses of each. 
   *this work was sponsored by the defense advanced research projects agency  darpa  image understanding program under grants daah1-g-1 and daah1-1  monitored by the u. s. army research office  and the national science foundation under grants cda-1 and iri-1 
1 	vision 
　the greatest difference between the two algorithms is in how they initiate search. the key feature algorithm initiates indepdent searches from each of a set of ranked key features. local search starts a number of independent trials from randomly selected matches. the key feature algorithm relies on one or more of the key features belonging to the best match. in contrast  local search makes no effort to find consistent initial matches and instead relies upon iterative improvement guided by updated global alignment to move search from the random match to one which is good. randomness is important to local search because the probability of failing to find a good solution over multiple trials drops exponentially as the number of trials increases. this same random sampling methodology has been used in the ransac algorithm to find consistent subsets of object model and image features  fischler and bolles  1 . 
　we begin by reviewing our formulation of 1d line segment matching as a combinatorial optimization task which has as its goal finding matches such as the one shown in figure 1. next the random-starts local search and key feature algorithms are described and their performance is compared on the example 
just shown as well as on a series of controlled test problems. 
measures how well the model fits the data. e measures how much of the model is omitted from the match. 


	 c  	 d  
figure 1: optimal match to fragmented data  a  aerial photograph  b  segments  burns et al.  1   c  model  d  best match. 
　to evaluate e  a 1d similarity transformation best fitting the model to the data is computed. the fitting criterion is a weighted sum of integrated perpendicular distance between infinitely extended model lines and their corresponding data line segments: eres c  is a normalized function of the residual error after fitting. the best fit for any c  neglecting underconstrained cases  is computed by solving a quadratic polynomial. eom c  is computed by transforming the model to the best-fit configuration and measuring how well the data covers the model. the weighting term a dictates how far a data segment can be from a model segment and still be included in an optimal match. if  for instance  then the data may be up to 1 pixels from the model. 
1 random starts local search 
perhaps the simplest algorithm is steepest-descent on a 'hamming-distance-t neighborhood. this is so named because any correspondence mapping c may be represented by a bitstring of length n  where n  a 'i' v in position j of the bitstring indicates that the jth pair in the set 1 is part of the match c. the n neighbors of c are generated by successively toggling each bit. hence  the neighborhood contains all matches created by either 1  adding a single pair of model-data features not already in the match or 1  removing a single pair currently in the match. 
　steepest-descent local search using this neighborhood computes e for all n neighbors of the current match c  and moves to the neighbor yielding the greatest improvement: the greatest drop in e. search terminates at a local optimum when no neighbor is better than the current match. 
　because local search often becomes stuck at undesirable local optima  it is common practice to run multiple trials. each trial is started from a randomly chosen initial match the random selection of  is biased to choose  on average  data segments for each model segment. specifically  let hm be the number of pairs in s which contain a model segment to. each of these pairs is included in ci with independent probability our experience suggests = 1 is a good choice  thus binding on average 1 data segments to each model segment. 
　over t trials  the probability of failing to find a good match drops as an exponential function oft. let p1 be the probability of finding a good solution on a single trial. the probability of failing to find a good match in t trials is: 
		 1  
for geometric models  there is often a well defined and known best solution and we can characterize algorithm performance in terms of the probability p1 of finding the best match in a given trial. from p1   the required number of trials t1 needed to solve a particular problem to a preset level of confidence q1 may be derived from equation 1: 
  
1 key feature matching 
the local search algorithm just presented can be turned into a 
key feature matching algorithm by initializing search from k carefully selected key feature matches. let f be the set of key features  and let a specific key feature fi be defined as follows: 
		 1  
the local search neighborhood is modified to consider only the addition of pairs. to understand this simplification  consider local search initiated from a key feature  fully contained within the optimal match: 
		 1  
simply by adding pairs local search will typically arrive at the best match c*. search is further simplified by noting that each match to a key feature  constrains where the model is placed relative to the data. thus  search can be carried out in a constrained search space which includes only pairs consistent with the specific model placement defined by 
　as a final step  the top 1 matches found by searching from each of the k key features are used to initialize a single trial of random starts local search. this final step can both add and remove pairs of segments and proves to be important in some cases. without this step  key feature matching can be close to the optimal match and still miss it. in principle  this final pass of local search could allow the key feature algorithm to find 
	bever1dge  graves  & steinborn 	1 

the best match even when no key feature satisfies equation 1. however  as a rule of thumb  the key feature algorithm will fail to find the optimal match when this condition is not met  i.e. no key feature is a subset of the optimal match c*. 
1 	choosing key features 
a variety of strategies has been suggested for defining sets of key features.  lowe  1  strongly advocated broad and general perceptual invariants derived from the basic rules of physics and imaging.  huttenlocher and ullman  1  took a more model-based approach: predicting which features are key in part upon specific object geometry. 
　however  both lowe and huttenlocher separated the task of selecting possible key features from that of matching complete models to image data. in contrast  we use the same criterion function  e  to both rank our selection of possible key features and to evaluate the optimality of final matches. 
　as in past work  the ability of a key feature to constrain the pose of the object is critical. for 1d line matching  where models can be rotated  translated and scaled  the two obvious choices are pairs of matched segments and triples of matched segments. triples are more reliable predictors of pose. 
1 	spatially proximate triples 
if there are n possible pairings between model and data features in the set 1  then there are n1 possible triples. for typical problems presented below  n  1. it is impractical to enumerate and rank 1 1 triples; clearly  some filter must be used. we use a very general heuristic: proximal lines in a model are likely to be proximal in the data. this idea is by no means new   lowe  1  suggests this approach and then dismisses it as too unfocused for his purposes. 
　filtering by spatial proximity will generate on the order of n ranked triples. model and data segments are analyzed independently to find the nearest neighbors of each. for each model line   determine the closest two neighbors and m i 1 as defined by euclidean distance 
when matching segments m to d  each pair of segments form two spatially proximate triples f  and f1: 
since each of the n pairs of model and data segments in s leads to 1 triples  there are 1n spatially proximate triples in the initial set of key features f. 
　in keeping with the assumption that some key features are better than others  order the set f from lowest to highest match error; 
		 1  
1 	vision 
the question arises  'how deep into the ranked set f should we search '. past work has typically assumed that it is not necessary to go very deep into the set f. our experiments take a conservative approach and use the best n triples. 
1 comparing approaches 
we present comparative performance data on two test problems suites produced by a monte carlo problem generator. the advantage of this data is that it allows for controlled testing under well specified conditions. we also consider performance on the real world problem shown in figure 1. 
　the first set of tests uses the six models shown in figure 1a. to create datasets d  model segments are randomly scaled and placed in the data images. in addition  the model line segments are fragmented  skewed and potentially omitted. finally  random clutter and structured clutter are added to the data. structured clutter consists of more highly degraded copies of the same object model. examples are shown in figures 1b and 1c. this dataset and results for it are available through our website 

　a second set of tests use data images with 1 randomly placed segments. models are created by randomly drawing 1  1 or 1 of these segments. this dataset tests the reliability of the proximity heuristic as the ratio of model to clutter lines varies. 
1 	six corrupted models 
the models shown in figure 1a exhibit characteristics known to make matching difficult. for example  the pole is an interesting case for the key feature algorithm because of its simplicity: there exists only one possible model triple for the key feature algorithm to exploit. the dandelion exhibits a 1 fold near symmetry; model symmetry complicates matching for many well established techniques  grimson  1 . the leaf presents an example where model and data line segments approximate a curved contour. for this model  a many-to-many mapping between model and image features is needed to account for breakpoints at different positions along the curve. 
　a monte carlo simulator produces corrupted image data. the simulator rotates  translates and scales the model so placement and size are unknown. model segments are also fragmented and skewed. in 1 of the problems  1 1 and 1 additional clutter segments are randomly placed about the image. a sampling of this data is shown in figure 1b. in the other 1 problems  1  1  1 and 1 additional corrupted model instances are added. a sampling of this data is shown in figure 1c. 
　the steepest-descent local search algorithm has been run for 1 trials on all 1 problems. the key feature algorithm has been run using the best n triples. the resulting run times and quality of results are indicated in figure 1a. problem instances are grouped along the x axis by model type  with run 


figure 1: models and data for controlled tests  a  six stick figure models  b  random clutter test data  c  multiple instances test data. 

figure 1: relative algorithm performance  a  the 1 problems using the six models shown in figure 1a  b  1 problems using randomly 

selected models. 
time plotted on the y axis. those problems where key features failed to find the optimal match are indicated in the separated band at the top of the plot. 
two critical types of information are conveyed by this plot. 
first  over 1 trials  the random starts algorithm never failed to find the optimal match at least once. conversely  the key feature algorithm failed in 1 out of the 1 cases. second  the key feature algorithm is dramatically more efficient in terms of run time  typically taking less than l/1th of the time required by the random starts local search algorithm. 
　the key feature algorithm fails on the simpler object models: the pole and rectangle  . two factors may explain these failures. first  simulator fragments model line segments with a fixed probability per unit length. hence  longer segments fragment more. the rectangle and pole have the longest indi-
     1 on symmetric models such as the rectangle and building in figure 1  symmetric matches are treated as equivalent. 
vidual segments  and therefore the highest degree of fragmentation. too much fragmentation prevents the proximal triples algorithm from finding triples which participate in the best match. the second factor is that there are fewer total features on the simpler models  further reducing the opportunities for the algorithm to find a good key feature. 
　the statement that the local search algorithm did not fail requires some elaboration. based upon the 1 trials  the probability of success  has been estimated for each individual problem. using these estimates  it is more proper to say that the required number of trials to find the best match with 1% confidence   from equation 1  never exceeds 1. 
　actually  far fewer than 1 trials are required for most of the problems in the test suite. less than 1 trials are required for 1 out of the 1 problems  and only 1 problems require more than 1 trials. likewise for the key feature algorithm  it is typically not necessary to consider all n triples. 
	beveridge  graves  & steinborn 	1 

the run times shown in figure 1a are intentionally conservative  making no assumption that there is a priori knowledge of how hard a specific problem is to solve. 
　looking at the actual work needed per problem  the key feature algorithm finds the best match from the first ranked triple in 1 out of the 1 cases. there are 1 cases where a triple in the top 1 leads directly to the best match. finally  in the 1 remaining cases  simply running the steepest-descent algorithm where only pairs are added misses the best match. however  in these cases  the final application of random starts local search locks onto the globally best match. this is counted as a success for the key feature algorithm. 
	 c  e = 1 	 d e = 1 
figure 1: examples of the key feature algorithm missing the best match  a  best match  b  best found by key feature  c  best match  d  best found by key feature. 
　it is interesting to observe how the key feature algorithm is lead astray. figure 1a shows the optimal match found by random starts local search for the three rectangles problem. figure 1b shows the best match found by the key feature algorithm. this is the 1nd best match found by the local search algorithm. figure 1c shows the optimal match found by random starts local search for the case of 1 random clutter lines. figure 1d shows the best match found by the key feature algorithm: again  it is the 1nd best match found by local search. this case is interesting because the coincidently placed long clutter line creates a 'false' rectangle which traps the key feature algorithm. 
1 	randomly placed lines 
the probability that the key feature algorithm will succeed depends upon the likelihood that a triple f in f belongs to 
1 	vision 
the best match c*. this likelihood  in turn  depends upon the probability that the two segments closest to a data segment coincide with the two closest segments in the model. as the results above suggest  this is a good but not perfect heuristic. 
　one way to think about this likelihood is to consider how the ratio of clutter to model features varies as more or fewer segments are drawn from a fixed size set of data segments. to illustrate  a set of 1 test problems has been generated  each with 1 randomly placed non-intersecting line segments. ten distinct sets of 1 random segments are used  and models are created by randomly selecting 1 1 or 1 from each set. 
the results for these 1 problems are shown in figure 1b. 
they support the hypothesis that a higher clutter ratio causes the key feature algorithm to become unreliable. the key feature algorithm fails on 1 out of 1 of the 1-segment models  while only failing on 1 out of 1 for the 1-segment models. 
　because the local search algorithm does not depend upon key features  we do not expect it to have difficulty with these problems. figure 1b shows that this is in fact the case  with local search solving all 1 problems reliably in less than 1 trials. moreover  it may surprise some readers to know that the average ta for 1% confidence decreases as these models get larger. for the problem instances with models of 1 segments  while for the models of 1 segments t-test on this data indicates that the drop is statistically significant: t = 1 and p  1. while far fewer than 1 trials are actually needed  for the sake of consistency  the times reported in figure 1b are all based upon 1 trials. 
it should now be apparent that the key feature algorithm has profound computational advantages over random starts local search. when it succeeds  it requires one to two orders of magnitude less computation. real world examples can be found where it works well  including many of the fairly clean 1d line matching problems common in the literature. 
　let us now consider a very difficult problem  the building shown in figure 1. there are 1 model line segments and 1 data segments  generating 1 possible pairs of segments. more important in terms of problem difficulty  there are multiple instances of buildings interacting with other buildings and roads  creating a combinatorial explosion of possible partial matches. also  the globally best match 1 is highly fragmented and must be pieced together by a search algorithm before it appears more attractive than many of the other more obvious partial rectangles. 
to fully test random starts local search on this problem  
1 trials have been run. the best match is found in only 
1 of these trials: the estimated probability of success 
     1 strictly speaking  we do not know the match shown in figure id to be the global optimum. however  through extensive study of this problem and our ability to eye-ball the results  we are relatively certain it is best. 

1 is tiny. required trials ts = 1 and average time to run a trial is 1 seconds. consequently  local search requires nearly 1 hours to reliably solve this problem. 

	 c  e = 1 	 d  e = 1 
figure 1: second through fifth ranked matches for aerial image  a  second best and match found using key features  b  third best  c  fourth best  d  fifth best. 
　while random starts local search has allowed us to find this best match  1 hours is far too much time for any practical online system to spend looking for one building. in contrast  the key feature algorithm is very fast  completing in a matter of minutes. however  it fails to find the best match. instead  it finds the match shown in figure 1a. this is not totally uninteresting  since according to the local search algorithm  this is the second best match in the image. however  it has a markedly larger match error: 1 as opposed to 1 for the global optimum. local search also  in some sense  finds two other buildings: figures 1c and 1d show the fourth and fifth ranked matches. figure1b shows that the third ranked match is a variant on the global optimum  sharing many of the same features. 
1 	conclusion 
we have presented a variant upon key feature matching. it operates within an optimal matching framework and uses the same criterion function to rank both key features and final ob-
ject model matches. on controlled problem sets  it has been shown that the underlying proximity heuristic yields key features which allow an algorithm to quickly find many but not all optimal matches. in an example of the cost versus generality trade-off typical in artificial intelligence  random starts local search is shown to be both more robust and more computationally demanding. 
