
in this paper we propose the dynamic weighting a*  dwa*  search algorithm for solving map problems in bayesian networks. by exploiting asymmetries in the distribution of map variables  the algorithm is able to greatly reduce the search space and offer excellent performanceboth in terms of accuracy and efficiency.
1 introduction
the maximum a posteriori assignment  map  is the problem of finding the most probable instantiation of a set of variables given partial evidence on the remaining variables in a bayesian network. one specialization of map that has received much attention is the most probable explanation  mpe  problem. mpe is the problem of finding the most probable assignment of a set of variables given full evidence on the remaining variables. map turns out to be a much more difficult problem compared to mpe and computing the probability of evidence. particularly  the decision problem for mpe is np-complete while the corresponding map problem is nppp-complete  park  1 . map is more useful than mpe for providing explanations. for instance  in diagnosis  we are generally only interested in the configuration of fault variables given some observations. there may be many other variables that have not been observed that are outside the scope of our interest.
모in this paper  we introduce the dynamic weighting a*  dwa*  search algorithm for solving map that is generally more efficient than existing algorithms. the algorithm explores asymmetries among all possible assignments in the joint probability distribution of the map variables. typically  a small fraction of the assignments can be expected to cover a large portion of the total probability space with the remaining assignments having practically negligibleprobability  druzdzel  1 . also  the dwa* uses dynamic weighting based on greedy guess  park and darwiche  1; yuan et al.  1  as the heuristic function. although it is theoretically not admissible  admissible heuristic should offer an upper bound on the map   the heuristic significantly reduces the size of the search tree  while rarely pruning away the optimal solutions.
1 map
the map problem is defined as follows: let m be the set of map variables  the configuration of which is what we are interested in; e is the set of evidence  namely the variables whose states we have observed; the remainder of the variables  denoted by s  are variables whose states we neither know nor care about. given an assignment e of variables e  the map problem is that of finding the assignment m of variables m which maximizes the probability p m | e   while the mpe problem is the special case of map with s being
empty  i.e. 
	 .	 1 
모in general  in bayesian networks  we use the conditional probability table  cpt  뷋 as the potential over a variable and its parent nodes. the notation 뷋e stands for the potential in which we have fixed the value of e 뫍 e. then the probability of map with 붯 as its cpts turns out to be a real number:
	 .	 1 
모in equation 1  summation commutes with summation  and maximization commutes with maximization. however  summation does not commute with maximization and vice versa. therefore  it is obligatory to do summation before any maximization. the order is called the elimination order. the size of the largest clique minus 1 in a jointree constructed based on an elimination order is called the induced width. the induced width of the best elimination order is called the treewidth. however  for the map problems in which the set s and the m are both non-empty  the order is constrained. then the constrained elimination order is known as the constrained treewidth. generally  the constrained treewidth is much larger than treewidth  leading the problem beyond the limit of feasibility for complexmodels. specifically  for some map problems  variableelimination on polytrees is subject to the constrained treewidth  which requires exponential time  while mpe problems can be computed in linear time  park and darwiche  1 .
모a very efficient approximate search-based algorithm based on local search  proposed by park and darwiche   is capable of solving map efficiently. an exact method  based on branch-and-bound depth-first search  proposed by park and darwiche   performs quite well when the search space is not too large. another approximate algorithm proposed more recently by yuan et al.  is a reheated annealing map algorithm. it is somewhat slower on simple networks but it is capable of handling difficult cases that exact methods cannot tackle.
1 solving map using dynamic weighting a*
we propose in this section an algorithm for solving map using dynamic weighting a* search  which incorporates the dynamic weighting  pohl  1  in the heuristic function  relevance reasoning  druzdzel and suermondt  1  and dynamic ordering in the search tree.
1 a* search
map can be solved by a* search in the probability tree that is composed of all the variables in the map set. the nodes in the search tree represent partial assignments of the map variables m. the root node represents an empty assignment. the map variables will be instantiated in a certain order. if a variable x in the set of map variables m is instantiated at the ith place using its jth state  it will be denoted as mij. leaves of the search tree correspond to the last map variable that has been instantiated. the vector of instantiated states of each map variable is called an assignment or a scenario.
모we compute the probability of assignments while searching the whole probability tree using chain rule. for each inner node  the newly instantiated node will be added to the evidence set  i.e.  the evidence set will be extended to mij 뫋 e. then the probability of the map problem which consists of n map variables can be presented as follows:
p m | e  = p mni | m1j m1k ...m n 1 t e  ...p m1k | m1j e p m1j | e  .
suppose that we are in the xth layer of the search tree and preparing for instantiating the xth map variables. then the function above can be rewritten as follows:

모the general idea of the dynamic weighting a* search is that during the search  in each inner node of the probability tree  we can compute the exact value of item  a  in the function above. we can estimate the heuristic value of the item  b  for the map variables that have not been instantiated given the initial evidence set and the map variables that have been instantiated as the new evidence. in order to fit the typical format of the cost function of a* search  we can take the logarithm of the equation above  which will not change its monotonicity. then we get f n  = g n + h n   where g n  and h n  are obtained from the logarithmic transformation of items  a  and  b  respectively. g n  gives the exact cost from the start node to node in the nth layer of the search tree  and h n  is the estimated cost of the best search path from the nth layer to the leaf nodes of the search tree. in order to guarantee the optimality of the solution  h n  should be admissible  which in this case means that it should be an upper-bound on the value of any assignment with the currently instantiated map variables as its elements.
1 heuristic function with dynamic weighting
the a* search is known for its completeness and optimality. for each search step  we only expand the node in the frontier with the largest value of f n .
definition 1 a heuristic function h1 is said to be more informed than h1 if both are admissible and h1 is closer to the optimal cost. for the map problem  the probability of the optimal assignment popt   h1   h1.
theorem 1 if h1 is more informed than h1 then dominates a1   nilsson .  pearl  1 
모the power of the heuristic function is measured by the amount of pruning induced by h n  and depends on the accuracy of this estimate. if h n  estimates the completion cost precisely  h n  = popt   then a* will only expand nodes on the optimal path. on the other hand  if no heuristic at all is used  for the map problem this amounts to h n  = 1   then a uniform-cost search ensues  which is far less efficient. so it is critical for us to find an admissble and tight h n  to get both accurate and efficient solutions.
greedy guess
if each variable in the map set m is conditionally independent of all the rest of map variables  this is called exhaustive independence   then the map problem amounts to a simple computation based on the greedy chain rule. we instantiate the map variable in the current search layer to the state with the largest probability and repeat this for each of the remaining map variables one by one. the probability of map is then

모the requirement of exhaustive independence is too strict for most of the map problems to be calculated by using the function above. simulation results show that in practice  when this requirement is violated  the product is still extremely close to the map probability  yuan et al.  1 . this suggests that it can be potentially used as a heuristic function for map.
모the curve greedy guess estimate in figure 1 shows that with the increase of the number of map variables  the ratio between the greedy guess and the accurate estimate of the optimal probability diverges from the ideal ratio one although not always monotonically.
dynamic weighting
since the greedy guess is a tight lower bound on the optimal probability of map  it is possible to compensate for the error between the greedy guess and the optimal probability. we can achieve this by adding a weight to the greedy guess such that their product is equal or larger than the optimal probability for each inner node in the search tree under the following assumption:
 
모where  is the minimum weight that can guarantee the heuristic function to be admissible. figure 1 shows that if we just keep  constant  neglecting the changes of the estimate accuracy with the increase of the map variables  the estimate function and the optimal probabilitycan be represented by the curve constant weighting heuristic. obviously  the problem with this idea is that it is less informed when the search progresses  as there are fewer map variables to estimate.
모dynamic weighting  pohl  1  is an efficient tool for improving the efficiency of a* search. if applied properly  it will keep the heuristic function admissible while remaining tight on the optimal probability. for map  in shallow layers of the search tree we get more map variables than in deeper layers. hence  the greedy estimate will be more likely to diverge from the optimal probability. we propose the following dynamic weighting heuristic function for the xth layer of the search tree of n map variables:

rather than keeping the weight constant throughout the search  we dynamically change it  so as to make it less heavy as the search goes deeper. in the last step of the search  x = n 1   the weight will be zero  since the greedy guess for only one map variable is exact and then the cost function f n-1  is equal to the probability of the assignment. figure 1 shows an empirical comparison of greedy guess  constant  and dynamic weighting heuristics against accurate estimates of the probability. we see that the dynamic weighting heuristic is more informed than constant weighting.

figure 1: constant weighting heuristic and dynamic weighting heuristic based on greedy guess.
1 searching with inadmissible heuristics
since the minimum weight  that can guarantee the heuristic function to be admissible is unknown before the map problem is solved  and it may vary in different cases  we normally set 붸 to be a safe parameter which is supposed to be larger than   in our experiments  we set 붸 to be 1 . however  what if 붸 is accidentally smaller then  leading the weighted heuristic to be inadmissible  suppose there are two candidate assignments: s1 and s1 with probability p1 and p1 respectively  among which s1 is the optimal assignment that the algorithm fails to find. and s1 is now in the last step of search  which will lead to a suboptimal solution. we skip the logarithm in the function for the sake of clarity here  then the cost function f is a product of transformed g and h instead of their sum . f1 = g1 몫 h1 and f1 = g1 몫 h1.
the error introduced by an inadmissible h1 is f1   f1. the algorithm will then find s1 instead of s1  i.e.  f1   f1   g1 몫 h1   g1 몫 h1.
since s1 is now in the last step of search  f1 = p1  section
1 . now suppose that we have an ideal heuristic function
  which leads to. then we have:
.
모it is clear that only when the ratio between the probability of suboptimal assignment and the optimal one is larger than the ratio between the inadmissible heuristic function and the ideal one  may the algorithm find a suboptimal solution.
모because of large asymmetries among probabilities that are further amplified by their multiplicative combination  druzdzel  1   we can expect that for most of the cases  the ratios between p1 and p1 are far less than 1. even though the heuristic function will sometimes break the rule of admissibility  if only the greedy guess is not too divergent from the ideal estimate  the algorithm will still not diverge from the optimal probability. our simulation results also confirm the robustness of the algorithm in finding optimal solutions.
1 improvements to the algorithm
there are several techniques that can improve the efficiency of the basic a* algorithm.
relevance reasoning
the main problem faced by our approach is the complexity of probabilistic inference. the critical factor in exact inference schemes for bayesian networks is the topology of the underlyinggraph and  more specifically  its connectivity. relevance reasoning  druzdzel and suermondt  1  is a technique based on d-separation and other simple and computational efficient techniques for pruning irrelevant parts of a bayesian network and can yield sub-networksthat are smaller and less densely connected than the original network. for map  our focus is the set of variables m and the evidence set e. parts of the model that are probabilistically independent from the nodes in m given the observed evidence e are computationally irrelevant to reasoning about the map problem.
dynamic ordering
as the search tree is constructed dynamically  we have the freedom to order the variables in a way that will improve the efficiency of the dwa* search. expanding nodes with the largest asymmetries in their marginal probability distributions lead to early cut-off of less promising branches of the search tree. we use the entropy of the marginal probability distributions as a measure of asymmetry.
1 experimental results
to test dwa*  we compared its performance on map problems in real bayesian networks against those of current state of the art map algorithms: the p-loc and p-sys algorithms  park and darwiche  1; 1  in samiam  and annealedmap  yuan et al.  1  in smile. we implemented dwa* in c++ and performed our tests on a 1 ghz pentium d windows xp computer with 1gb ram. we used the default parameters and settings for all the three algorithms above during comparison  unless otherwise stated.
1 experimental design
the bayesian networks that we used in our experiments included alarm  beinlich et al.  1   barley  kristensen and rasmussen  1   cpcs1 and cpcs1  pradhan et al.  1   hailfinder  abramson et al.  1   munin  diabetes  andreassen et al.  1   andes  conati et al.  1   pathfinder  and win1pts  heckerman et al.  1   some of which were constructed for diagnosis. we also tested the algorithms on two very large proprietary diagnostic networks built at the hrl laboratories  hrl1 and hrl1 . the statistics for all networks are summarized in table 1. we divide the networks into three groups:  1  small and middle-sized   1  large but tractable  and  1  hard networks.
groupnetwork#nodes#arcsalarm1cpcs11cpcs11hailfinder1pathfinder1andes1win1pts11munin11hrl1 1 1hrl1 1 1barley1diabetes1table 1: statistics for the bayesian networks that we use.
모for each network  we randomly generated 1 cases. for each case  we randomly chose 1 map variables from the root nodes or all of them if there were fewer than 1 root nodes. we chose the same number of evidence nodes from the leaf nodes. to set evidence  we sampled from the prior probability distribution of a bayesian network in its topological order and cast the states of the sample to the evidence nodes. following previous tests of map algorithms  we set the search time limit to be 1 seconds  1 minutes .
1 results for the first and second group
in the first experiment  we ran the p-loc  p-sys  annealedmap and dwa* on all the networks in the first and second group  and all four algorithms generated results within the time limit. the p-sys is an exact algorithm. so table 1 only reports the number of map problems that were solved optimally by the p-loc  annealedmap and dwa*. the dwa* found all optimal solutions. the p-loc missed only one case on andes and the annealedmap missed one on hailfinder and two cases on andes.
p-loca-mapdwa*alarm11cpcs11cpcs11hailfinder11pathfinder11andes11win1pts11munin11hrl11hrl11table 1: the number of cases that were solved optimally out of 1 random cases for the first and second groups of networks.
모since both annealedmap and p-loc failed to find all optimal solutions in andes  we studied the performanceof the four algorithms as a function of the number of map variables  we randomly generated 1 cases for each number of map variables .
#mapp-sysp-loca-map1111111timeout11timeout11timeout11timeout11timeout1table 1: the number of cases for which the existing algorithms found smaller probabilities than a* search in network andes.
모because the search time of p-sys increased quickly with the number of map variables  and it failed to generate any result when the number of map variables reached 1  while dwa* found all largest probabilities  we compared all the other three algorithms against dwa*. with the increase of the number of map variables  both p-loc and annealedmap turned out to be less accurate than dwa* on andes. when the number of map variables was above 1  there were about 1% cases of p-loc and 1% cases in which annealedmap found smaller probabilities than dwa*. we notice from table 1 that p-loc spent less time than dwa* when using its default settings for andes  so we increased the search steps of p-loc such that it spent the same amount of time as dwa* in order to make a fair comparison. however  in practice the search time is not continuous in the numberof search steps  so we just chose parameters for p-loc such that it spent slightly more time than dwa*. table 1 shows the comparison results. we can see that after increasing the search steps of p-loc  dwa* still maintains better accuracy.
#mapp-loc dwa*p-loc dwa*111111111111table 1: the number of cases that the p-loc found larger/smaller probabilities than dwa* in network andes when spending slightly more time than dwa*.
모in addition to the precision of the results  we also compared the efficiency of the algorithms. table 1 reports the average running time of the four algorithms on the first and the second groups of networks. for the first group  the an-
p-sysp-loca-mapa*alarm1111cpcs1.1.1.1.1cpcs1.1.1.1.1hailfinder1111pathfinder1111andes1111win1pts1111munin1111hrl1.1.1.1.1hrl1.1.1.1.1table 1: average running time in seconds of the p-sys  p-
loc  annealedmap and dwa* algorithms on the first and second group of networks.
nealedmap  p-loc and p-sys algorithms showed similar efficiency on all except the cpcs1 and andes networks. dwa* generated solutions within the shortest time on the average. its smaller variance of the search time indicates that dwa* is more stable across different networks.
모for the second group  which consists of large bayesian networks  p-sys  annealedmap and dwa* were all efficient. dwa* search still spent shortest time on the average  while the p-loc was much slower on the hrl1 network.
1 results for the third group
the third group consisted of two complexbayesian networks: barley and diabetes  many nodes of which have more than 1 different states. because the p-sys algorithm did not produce results within the time limit  the only available measure of accuracy was a relative one: which of the algorithms found an assignment with a higher probability. table 1 lists the number of cases that were solved differently between the p-loc  annealedmap  and dwa* algorithms. pl  pa and p  stand for the probability of map solutions found by p-loc  annealedmap and dwa* respectively.
p  pl/p  plp  pa/p  pabarley11diabetes11table 1: the number of cases that are solved differently from p-loc  annealedmap and dwa*.
모for barley  the accuracy of the three algorithms was quite similar. however  for diabetes dwa* was more accurate: it found solutions with largest probabilities for all 1 cases  while p-loc failed to find 1 and annealedmap failed to find 1 of them.
p-sysp-loca-mapa*barleytimeout111diabetestimeout111table 1: average running time in seconds of the p-sys  ploc  annealedmap and dwa* on the third groups.
dwa* turned out to be slower than p-loc and an-
nealedmap on barley but more efficient on diabetes  see table 1 .
1 results for incremental map test
our last experiment focused on the robustness of the four algorithms to the number of map variables. in this experiment  we set the number of evidence variables to be 1 and generated map problems with an increasing number of map nodes and ran four algorithms on these cases. we chose the munin network  because it seemed the hardest network among the group 1 & 1 and had sufficiently large sets of root and leaf nodes. the running time for each of the cases are shown in figure 1. typically  p-sys and p-loc need more running time in face of more complex problems  while annealedmap and dwa* seem more robust in comparison.

figure 1: plot of the running time of the p-sys  p-loc 
annealedmap and dwa* algorithms when increasing the number of map nodes on the munin network.
1 discussion
finding map in bayesian networks is hard. by exploiting asymmetries among the probabilities of possible assignments of joint probability distributions of map variables  the dwa* is able to greatly reduce the search space and lead to efficient and accurate solution of the map problem. our experiments show that generally  dwa* search is more efficient than the existent algorithms. especially for large and complex bayesian networks  when the exact algorithm fails to generate any result within a reasonable time  dwa* can still provide accurate solutions efficiently. further extension of this research is to apply the dwa* to the k-map problem  which is to find k most probable assignments for map variables. it is very convenient for the dwa* algorithm to achieve that  since after finding the most probable assignment  the algorithm keeps all the candidate assignments in the search frontier. we can expect that the additional search time will be sublinear in k. solving the k-map problem gives additional insurance against missing the optimal solutions  as there is a very good chance that if it is missed at first  it will show up among the following k-1 solutions.
acknowledgements
this research was supported by the air force office of scientific research grants f1-1and fa1- 1 and by intel research. we thank leon rothkrantz for his support during the research. we thank adnan darwiche and keith cascio for providing us with the latest version of the p-sys and p-loc algorithms. all experimental data have been obtained using smile  a bayesian inference engine developed at the decision systems laboratory and available at http://genie.sis.pitt.edu/.
