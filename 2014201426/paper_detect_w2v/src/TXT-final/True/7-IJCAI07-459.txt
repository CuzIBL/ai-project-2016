
understanding user interests from text documents can provide support to personalized information recommendation services. typically  these services automatically infer the user profile  a structured model of the user interests  from documents that were already deemed relevant by the user. traditional keyword-basedapproachesare unable to capture the semantics of the user interests. this work proposes the integration of linguistic knowledge in the process of learning semantic user profiles that capture concepts concerning user interests. the proposed strategy consists of two steps. the first one is based on a word sense disambiguation technique that exploits the lexical database wordnet to select  among all the possible meanings  senses  of a polysemous word  the correct one. in the second step  a na： ve bayes approach learns semantic sensebased user profiles as binary text classifiers  userlikes and user-dislikes  from disambiguated documents. experiments have been conducted to compare the performance obtained by keyword-based profiles to that obtained by sense-based profiles. both the classification accuracy and the effectiveness of the ranking imposed by the two different kinds of profile on the documents to be recommended have been considered. the main outcome is that the classification accuracy is increased with no improvement on the ranking. the conclusion is that the integration of linguistic knowledge in the learning process improves the classification of those documents whose classification score is close to the likes / dislikes threshold  the items for which the classification is highly uncertain .
1 introduction
personalized systems adapt their behavior to individual users by learning their preferences during the interaction in order to construct a user profile  that can be later exploited in the search process. traditional keyword-based approaches are primarily driven by a string-matching operation: if a string  or some morphological variant  is found in both the profile and the document  a match occurs and the document is considered relevant. string matching suffers from problems of polysemy  the presence of multiple meanings for one word  and synonymy  multiple words having the same meaning. the result is that  due to synonymy  relevant information can be missed if the profile does not contain the exact keywords in the documents  while  due to polysemy  wrong documents could be deemed relevant. these problems call for alternative methods to learn more accurate profiles that capture concepts expressing user interests from relevant documents. these semantic profiles should contain references to concepts defined in lexicons or ontologies. this paper describes an approach in which user profiles are obtained by machine learning techniques integrated with a word sense disambiguation  wsd  strategy based on the wordnet lexical database  miller  1; fellbaum  1 . the paper is organized as follows: after a brief discussion about the main works related to our research  in section 1 the wsd strategy proposed to represent documents by using wordnet is described. section 1 presents the na： ve bayes text categorization method we adopted to build wordnet-based user profiles. this method is implemented by the content-based profiling system item recommender  itr . an experimental sessions has been carried out in order to evaluate the proposed approach in a movie recommending scenario. the main results are presented in section 1. conclusions and future work are discussed in section 1.
1 related work
our research was mainly inspired by the following works. syskill & webert  pazzani and billsus  1  learns user profiles as bayesian classifiers able to recommend web pages  but it represents documents using keywords. libra  mooney and roy  1  adopts a bayesian classifier to produce content-based book recommendations by exploiting product descriptions obtained from the web pages of the amazon online digital store. documents are represented by using keywords and are subdivided into slots  each one corresponding to a specific section of the document. like syskill & webert 
the main limitation of this work is that keywords are used to represent documents. conversely  siteif  magnini and strapparava  1  exploits a sense-based document representation to build a user profile as a semantic network whose nodes represent senses of the words in documents requested by the user. the semantic network is built by assigning each node with a score that is inversely proportional to its frequency over all the corpus. thus  the score is higher for less frequent senses  and this prevents very common meanings from becoming too prevailing in the user model. in our approach  a probability distribution of the senses  found in the corpus of the documents rated by the user  is learned. ontoseek  guarino et al.  1  is a system designed for content-based information retrieval from online yellow pages and product catalogs  which explored the role of linguistic ontologies in knowledge-retrieval systems. that approach has shown that structured content representations  coupled with linguistic ontologies  can increase both recall and precision of content-based retrieval systems. by taking into account the lessons learned by the aforementioned works  itr has been conceived as a text classifier able: 1  to deal with a sense-based document representation obtained by exploiting a linguistic ontology; 1  to learn a bayesian profile from documents subdivided into slots. the strategy we devise in order to shift from a keyword-based document representation to a sense-based one  is to integrate lexical knowledge in the indexing step of training documents. several methods have been proposed to accomplish this task. scott and matwin  included wordnet information at the feature level by expanding each word in the training set with all its synonyms in wordnet in order to avoid a wsd process. this approach has shown a decrease of effectiveness in the obtained classifier  mostly due to the word ambiguity problem. therefore  it suggests that some kind of disambiguation is required. bloedhorn and hotho  experiment with various settings for mapping words to senses: no wsd  most frequent sense as provided by wordnet  wsd based on context. they found positive results on the reuters 1  the ohsumed1 and the faodoc1 corpora. none of the previous approaches for embedding wsd in classification has taken into account the fact that wordnet is a hierarchical thesaurus. a distinctive feature of our work is the adoption of a similarity measure that takes into account the hierarchical structure of wordnet.
1 using wordnet to represent documents
we consider the problem of learning user profiles as a binary text categorizationtask: each documenthas to be classified as interesting or not with respect to the user preferences. the set of categories is c = {c+  c }  where c+ is the positive class  user-likes  and c  the negative one  user-dislikes . there are several ways in which content can be represented in order to be used as a basis for the learning component and there exists a variety of machine learning methods that could be exploited for inferring user profiles. we propose a strategy to learn sense-based profiles that consists of two steps. this section describes the first one  that is  a wsd technique that exploits the word senses in wordnet to represent documents.
in the second step  described in section 1  a na： ve bayes approach learns sense-based user profiles as binary text classifiers  user-likes and user-dislikes  from disambiguated documents. a thorough experimental evaluation of that idea in the context of a hybrid  content-based / collaborative  recommender system has been carried out in  degemmis et al.  1 .
1 the jigsaw algorithm for word sense disambiguation
textual documents cannot be directly interpreted by learning algorithms. an indexing procedure that maps a document di into a compact representation of its content must be applied. a typical choice for document indexing is the classical bag-of-words  bow  approach  where each document is represented as a feature vector counting the number of occurrences of different words as features  sebastiani  1 . we extend the bow model to a model in which each document is represented by the senses corresponding to the words in its content and their respective occurrences. this sense-based document representation is exploited by the learning algorithm to build semantic user profiles. here   sense  is used as a synonym of  meaning . any implementation of sensebased document indexing must solve the problem that  while words occur in a document  meanings do not  since they are often hidden in the context. therefore  a procedure is needed for assigning senses to words. this task  known as word sense disambiguation  consists in determining which of the senses of an ambiguous word is invoked in a particular use of that word  manning and schu：tze  1 .
　the goal of a wsd algorithm is to associate a word wi occurring in a document d with its appropriatemeaning or sense s  by exploiting the context c in which wi is found  commonly defined as a set of words that precede and follow wi. the sense s is selected from a predefined set of possibilities  usually known as sense inventory. in the proposed algorithm  the sense inventory is obtained from wordnet  version 1.1 . wordnet was designed to establish connections between four types of parts of speech  pos : noun  verb  adjective  and adverb. the basic building block for wordnet is the synset  synonym set   which represents a specific meaning of a word. the specific meaning of one word under one type of pos is called a sense. synsets are equivalent to senses  which are structures containing sets of words with synonymous meanings. each synset has a gloss  a short textual description that defines the concept represented by the synset. for example  the words night  nighttime and dark constitute a single synset that has the following gloss:  the time after sunset and before sunrise while it is dark outside . synsets are connected through a series of relations: antonymy  opposites   hyponymy/hypernymy  is-a   meronymy  part-of   etc. jigsaw is a wsd algorithm based on the idea of combining three different strategies to disambiguate nouns  verbs  adjectives and adverbs. the motivation behind our approach is that the effectiveness of the wsd algorithms is strongly influenced by the pos tag of the target word. an adaptation of lesk dictionary-based wsd algorithm has been used to disambiguate adjectives and adverbs  banerjee and pedersen  1   an adaptation of the resnik algorithm has been used to disambiguate nouns  resnik  1   while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that wordnet utilizes to describe the usage of the verb. the algorithm disambiguates only words which belong to at least one synset. jigsaw takes as input a document d = {w1  w1  ...  wh} and will output a list of wordnet synsets x = {s1  s1  ...  sk}  k ＋ h  in which each element si is obtained by disambiguating the target word wi based on the information obtained from wordnet about a few immediately surrounding words. we define the context c of the target word to be a window of n words to the left and another n words to the right  for a total of 1n surrounding words. the algorithm is based on three different procedures for nouns  verbs  adverbs and adjectives  called jigsawnouns  jigsawverbs  jigsawothers  respectively. the pos tag of each word is computed by the hmmbased tagger acopost t1. jigsaw proceeds in several iterations by using the disambiguation results of the previous iteration to reduce the complexity of the next one. first  jigsaw performs the jigsawnouns procedure. then  verbs are disambiguated by jigsawverbs by exploiting the words already disambiguated by jigsawnouns. finally  the jigsawothers procedure is executed. more details for each one of the above mentioned procedures follow.
jigsawnouns
the algorithm assigns to wi the most appropriate synset sih among the sense inventory wi for wi. it computes the similarity between each sik in the sense inventory and the context for wi. the method differs from the original algorithm by resnik  in the use of the similarity measure. we adopted the leacock-chodorow measure   which is based on the length of the path between concepts in an isa hierarchy. the idea behind this measure is that similarity between synsets a and b is inversely proportional to their distance in the wordnet is-a hierarchy. the distance is computed by counting the number of nodes in the shortest path joining a with b  by passing through their most specific subsumer . the similarity function is: sinsim a b  =  log np/1d   where np is the number of nodes in the shortest path p from a to b  and d is the maximum depth of the taxonomy  d = 1  in wordnet 1.1 . the procedure starts by defining the context c of wi as the set of words having the same pos tag and found in the same sentence as wi. next  the algorithm identifies both the sense inventory for wi and the sense inventory wj  for each word wj in c. the sense inventory t for the whole context c is given by the union of all wj. jigsawnouns measures the similarity between each candidate sense sik （ wi and each sense sh （ t. the sense assigned to wi is the one with the highest similarity score.
jigsawverbs
before describing the jigsawverbs procedure the description of a synset must be defined. it is the string obtained by concatenating the gloss and the sentences that wordnet uses to explain the usage of a word. for example  the gloss for the synset corresponding to the sense n.1 of the verb look  {look appear seem} is  give a certain impression or have a certain outward aspect   while some examples of usage of the verb are:  she seems to be sleeping ;  this appears to be a very difficult problem . the description of the synset is  give a certain impression or have a certain outward aspect she seems to be sleeping this appears to be a very difficult problem . first  the jigsawverbs includes in the context c for the target verb wi all the nouns in the window of 1n words surrounding wi. for each candidate synset sik of wi  the algorithm computes nouns i k   that is the set of nouns in the description for sik. in the above example  nouns look 1 ={impression  aspect  problem}. then  for each wj in c and each synset sik  the following value is computed: maxjk = maxwl（nouns i k  {sinsim wj wl }  1 
in other words  maxjk is the highest similarity value for wj  with respect to the nouns related to the k-th sense for wi. finally  a score for each sik is computed:
		 1 
where r k  is the ranking of sik  synsets in wordnet are ranked according to their frequency of usage  and g posj  is a gaussian factor related to the position of wj with respect to wi in the original text that gives a higher weight to words near the target word. the synset assigned to wi is the one with the highest   value.
jigsawothers
this procedure is based on the wsd algorithm proposed in  banerjee and pedersen  1 . the idea is to compare the glosses of each candidate sense for the target word to the glosses of all the words in its context. let wi be the sense inventory for the target word wi. for each sik （ wi  jigsawothers computes the string targetglossik that contains the words in the gloss of sik. then  the procedure computes the string contextglossi  which contains the words in the glosses of all the synsets corresponding to each word in the context for wi. finally  the procedure computes the overlap between contextglossi and targetglossik  and assigns the synset with the highest overlap score to wi. this score is computed by counting the words that occur both in targetglossik and in contextglossi. the jigsaw algorithm was evaluated according to the parameters of the senseval initiative1  that provides a forum where the wsd systems are assessed against disambiguated datasets. in order to measure the capability of disambiguating a complete text  the  all words task  for english was chosen. jigsaw reaches the fourth position in that task  by achieving precision and recall equal to 1%. this result assures that our wsd algorithm can be configured to have high precision  and thus would add very little noise in the training set. due to space limitations  the details of the experiments are not reported.
1 keyword-based and synset-based document representation
the wsd procedure described in the previous section is adopted to obtain a synset-based vector space representation that we called bag-of-synsets  bos . in this model  a synset vector instead of a word vector represents a document. another key feature of the approach is that each document is represented by a set of m slots  where each slot is a textual field corresponding to a specific feature of the document  in an attempt to take also into account the document structure. according to the bos model  the text in each slot is represented by counting separately the occurrences of a synset in the slots in which it occurs. more formally  assume that we have a collection of n documents. let m be the index of the slot  for n = 1 ... n  the n-th document dn is reduced to m bags of synsets  one for each slot:
  m=1  1  ...  m
where is the k-th synset in slot sm of document dn and dnm is the total number of synsets appearing in the m-th slot of document dn. for all n  k and  which is the vocabulary for the slot sm  the set of all different synsets found in slot sm . document dn is finally represented in the vector space by m synset-frequency vectors:

whereis the weight of the synset tk in the slot sm of document dn  and can be computed in different ways: it can be simply the number of times synset tk appears in slot sm  as we used in our experiments  or a more complex tf-idf score. our hypothesis is that the proposed document representation helps to obtain profiles able to recommend documentssemantically closer to the user interests. the difference with respect to keyword-based profiles is that synset unique identifiers replace words.
1 a na： ve bayes method for user profiling
item recommender  itr  uses a na： ve bayes text categorization algorithm to build profiles as binary classifiers  userlikes vs user-dislikes . the induced probabilistic model estimates the a posteriori probability  p cj|di   of document di belonging to class cj as follows:
		 1 
where n di tk  is the number of times token tk occurs in document di. in itr  each document is encoded as a vector of bos in the synset-based representation  or as a vector of bow in the keyword-based representation  one bos  or bow  for each slot. therefore  equation  1  becomes:
		 1 
where s= {s1  s1  ...  s|s|} is the set of slots  bim is the bos or the bow in the slot sm of di  nkim is the number of occurrences of token tk in bim. when the system is trained on bow-represented documents tokens tk in bim are words  and the induced categorization model relies on word frequencies. conversely  when training is performed on bos-represented documents  tokens are synsets  and the induced model relies on synset frequencies. to calculate  1   the system has to estimate p cj  and p tk|cj sm  in the training phase. the documents used to train the system are rated on a discrete scale from 1 to max  where max is the maximum rating that can be assigned to a document. according to an idea proposed in  mooney and roy  1   each training document di is labeled with two scores  a  user-likes  score and a  userdislikes  score w i   obtained from the original rating r:
	;		 1 
　the scores in  1  are exploited for weighting the occurrences of tokens in the documents and to estimate their probabilities from the training set tr. the prior probabilities of the classes are computed according to the following equation:
		 1 
witten-bell smoothing  is adopted to compute p tk|cj sm   by taking into account that documents are structured into slots and that token occurrences are weighted using scores in equation  1 :
 otherwise
 1 
where n tk cj sm  is the count of the weighted occurrences of token tk in the slot sm in the training data for class cj  vcj is the total number of unique tokens in class cj  and v is the total number of unique tokens across all classes. n tk cj sm  is computed as follows:
		 1 
in  1   nkim is the number of occurrences of token tk in slot sm of document di. the sum of all n tk cj sm  in the denominator of equation  1  denotes the total weighted length of the slot sm in class cj. in other words  p  tk|cj sm  is estimated as the ratio between the weighted occurrences of tk in slot sm of class cj and the total weighted length of the slot. the final outcome of the learning process is a probabilistic model used to classify a new document in the class c+ or c . this model is the user profile  which includes those tokens that turn out to be most indicative of the user preferences  according to the value of the conditional probabilities in  1 .
1 experimental evaluation
the goal of the experiments was to compare the performance of synset-based user profiles to that of keyword-based profiles. experiments were carried out on a content-based extension of the eachmovie dataset1  a collection of 1 textual descriptions of movies rated by 1users on a 1-point scale  1 . the contentinformation for each moviewas collected from the internet movie database1 by using a crawler that gathered the title  the director  the genre  that is the category of the movie  the keywords  the summary and the cast. movies are subdivided into different genres: action  animation  classic  artforeign  comedy  drama  family  horror  romance  thriller. for each genre or category  a set of 1 users was randomly selected among users that rated n items  1 ＋ n ＋ 1 in that movie category  only for genre 'animation'  the number of users that rated n movies was 1  due to the low number of movies in that genre . in this way  for each category  a dataset of at least 1 triples  user  movie  rating  was obtained  at least 1 for 'animation' . table 1 summarizes the data used for the experiments.
idgenrenumber ratings% pos% neg1action111animation111art foreign111classic111comedy111drama111family111horror111romance111thriller1111table 1: 1 'genre' datasets obtained from eachmovie
　tokenization  stopword elimination and stemming have been applied to index the documents according to the bow model. the content of slots title  director and cast was only tokenized because the elimination of the stopwords produced some unexpected results. for example  slots containing exclusively stopwords  such as  it  or  e.t.   became empty. moreover  it does not make sense to apply stemming and stopword elimination on proper names. documents have been processed by the jigsaw algorithm and indexed according to the bos model  obtaining a 1% feature reduction. this is mainly due to the fact that synonym words are represented by the same synset. keyword-based profiles were inferred by learning from bow-represented documents  whilst synset-based profiles were obtained from bos-represented documents. as itr is conceived as a text classifier  its effectiveness is evaluated by the well-known classification accuracy measures precision and recall  sebastiani  1 . also used is f1 measure  a combinationof precision and recall. we adopted the normalized distance-based performance measure  ndpm   yao  1  to measure the distance between the ranking imposed on documents by the user ratings and the ranking predicted by itr  that ranks documents accord-

1
　　eachmovie dataset no longer available for download  see the grouplens home page for a new version named movielens  originally based on this dataset:
http://www.cs.umn.edu/research/grouplens/ 1
imdb  http://www.imdb.com
idprecisionrecallf1ndpmbowbosbowbosbowbosbowbos1.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.11111111111111111table 1: performance of itr on 1 different datasets
ing to the a-posteriori probability of the class likes. values range from 1  agreement  to 1  disagreement . the adoption of both classification accuracy and rank accuracy metrics gives us the possibility of evaluating both whether the system is able to recommend relevant documents and how these documents are ranked. in all the experiments  a movie description di is considered relevant by a user if the rating is greater or equal to 1  while itr considers a description relevant if p c+|di    1  computed as in equation  1 . we executed one run of the experiment for each user in the dataset. each run consisted in: 1  selecting the documents and the corresponding ratings given by the user; 1  splitting the selected data into a training set tr and a test set ts; 1  using tr for learning the corresponding user profile; 1  evaluating the predictive accuracy of the induced profile on ts  using the aforementioned measures. the methodology adopted for obtaining tr and ts was the 1-fold cross validation. table 1 shows the results reported over all 1 genres by itr.
　a significant improvement of bos over bow both in precision  +1%  and recall  +1%  can be noticed. the bos model outperforms the bow model specifically on datasets 1  +1% of precision  +1% of recall   1  +1% of precision  +1% of recall   1  +1% of precision  +1% of recall . only on dataset 1 no improvement can be observed  probably because precision and recall are already very high. it could be noticed from the ndpm values that the relevant / not relevant classification accuracy is increased without improving the ranking. this result can be explained by the example in table 1  in which each column reports the ratings or scores of the items and the corresponding positions in the ranking.
　let ru be the ranking imposed by the user u on a set of 1 items  ra the ranking computed by a  and rb the ranking computed by method b  ratings ranging between 1 and 1 - classification scores ranging between 1 and 1 . an item is considered relevant if the rating r   1  symmetrically  if the ranking score s − 1 . method a has a better classification accuracy compared to method b  recall=1  precision=1 vs. recall=1  precision=1 . ndpm is almost the same for both methods because the two rankings ra and rb are
very similar. the difference is that i1 is ranked above i1 in ra  whilst i1 is ranked above i1 in rb. the general conclusion is that method a  bos model  has improved the classification of items whose score  and ratings  are close to the relevant / not relevant threshold  items for which the classi-
itemrurarbi1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 i1  1 1  1 1  1 table 1: example of situation in which classification accuracy is increased without improving ranking
fication is highly uncertain . a wilcoxon signed ranked test  p   1  has been performed to validate the results. each genre dataset has been considered as a single trial for the test. results confirmedthat there is a statistically significant difference in favor of the bos model compared to the bow model as regards precision  recall and f1-measure. conversely  the two models are equivalent in defining the ranking of the preferred movies according to the score for the class  likes .
1 conclusions and future work
we presented a system that exploits a bayesian learning method to induce semantic user profiles from documents represented by wordnet synsets suggested by the wsd algorithm jigsaw. our hypothesis is that  replacing words with synsets in the indexing phase  produces a more effective document representation  which can help learning algorithms to infer moreaccurateuser profiles. our approachhas been evaluated in a movie recommending scenario. results showed that the integration of the wordnet linguistic knowledge in the learning process improves the classification of those documents for which the classification is highly uncertain. as a future work  we plan to exploit not only the wordnet hierarchy  but also domain ontologies in order to realize a more powerful document indexing.
