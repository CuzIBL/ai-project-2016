
we study learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers. in this framework  complex dependencies among the output variables are capturedby constraints and dictate which global labels can be inferred. we compare two strategies  learning independent classifiers and inference based training  by observing their behaviors in different conditions. experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed.
1 introduction
making decisions in real world problems involves assigning values to sets of variables where a complex and expressive structure can influence  or even dictate  what assignments are possible. for example  in the task of identifying named entities in a sentence  prediction is governed by constraints like  entities do not overlap.  another example exists in scene interpretation tasks where predictions must respect constraints that could arise from the nature of the data or task  such as  humans have two arms  two legs  and one head. 
¡¡there exist at least three fundamentally different solutions to learning classifiers over structured output. in the first  structure is ignored; local classifiers are learned and used to predict each output component separately. in the second  learning is decoupled from the task of maintaining structured output. estimators are used to produce global output consistent with the structural constraints only after they are learned for each output variable separately. discriminative hmm  conditional models  punyakanok and roth  1; mccallum et al.  1  and many dynamic programming based schemes used in the context of sequential predictions fall into the this category. the third class of solutions incorporates dependencies among the variables into the learning process to directly induce estimators that optimize a global performance measure. traditionally these solutions were generative; however recent developments have produced discriminative models of this type  including conditional random fields  lafferty et al.  1   perceptron-based learning of structured output  collins  1; carreras and ma`rquez  1  and max-margin markov networks which allow incorporating markovian assumptions among output variables  taskar et al.  1 .
¡¡incorporating constraints during training can lead to solutions that directly optimize the true objective function  and hence  should perform better. nonetheless  most real world applications using this technique do not show significant advantages  if any. therefore  it is important to discover the tradeoffs of using each of the above schemes.
¡¡in this paper  we compare three learning schemes. in the first  classifiers are learned independently  learning only  lo    in the second  inference is used to maintain structural consistency only after learning  learning plus inference  l+i    and finally inference is used while learning the parameters of the classifier  inference based training  ibt  . in semantic role labeling  srl   it was observed  punyakanok et al.  1; carreras and ma`rquez  1  that when the local classification problems are easy to learn  l+i outperforms ibt. however  when using a reduced feature space where the problem was no longer  locally  separable  ibt could overcome the poor local classifications to yield accurate global classifications.
¡¡section 1 provides the formal definition of our problem. for example  in section 1  we compare the three learning schemes using the online perceptron algorithm applied in the three settings  see  collins  1  for details . all three settings use the same linear representation  and l+i and ibt share the same decision function space. our conjectures of the relative performance between different schemes are presented in section 1. despite the fact that ibt is a more powerful technique  in section 1  we provide an experiment that shows how l+i can outperform ibt when there exist accurate local classifiers that do not depend on structure  or when there are too few examples to learn complex structural dependencies. this is also theoretically justified in section 1.
1 background
structured output classification problems have many flavors. in this paper  we focus on problems where it is natural both to split the task into many smaller classification tasks and to solve directly as a single task. in section 1  we consider the semantic role-labeling problem  where the input x are natural language features and the output y is the position and type of a semantic-role in the sentence. for this problem  one can either learn a set of local functions such as  is this phrase an argument of 'run'   or a global classifier to predict all semantic-roles at once. in addition  natural structural constraints dictate  for example  that no two semantic roles for a single verb can overlap. other structural constraints  as well as linguistic constraints yield a restricted output space in which the classifiers operate.
¡¡in general  given an assignment x ¡Ê xnx to a collection of input variables  x =  x1 ... xnx   the structured classification problem involves identifying the  best  assignment y ¡Ê yny to a collection of output variables y =  y1 ... yny  that are consistent with a defined structure on y. this structure can be thought of as constraining the output space to a smaller space c yny    yny  where c : 1y  ¡ú 1y  constrains the output space to be structurally consistent.
¡¡in this paper  a structured output classifier is a function h : xnx ¡ú yny  that uses a global scoring function  f : xnx ¡Á yny ¡ú ir to assign scores to each possible example/label pair. given input x  it is hoped that the correct output y achieves the highest score among consistent outputs: y  = h x  = argmax f x y1    1 
y1¡Êc yny 
where nx and ny depend on the example at hand. in addition  we view the global scoring function as a composition of a set of local scoring functions {fy x t }y¡Êy  where fy : xnx ¡Á {1 ... ny} ¡ú ir. each function represents the score or confidence that output variable yt takes value y:
                           ny f x  y1 ... yny   = xfyt x t 
t=1
¡¡inference is the task of determining an optimal assignment y given an assignment x. for sequential structure of constraints  polynomial-time algorithms such as viterbi or cscl  punyakanok and roth  1  are typically used for efficient inference. for general structure of constraints  a generic search method  e.g.  beam search  may be applied. recently  integer programming has also been shown to be an effective inference approach in several nlp applications  roth and yih  1; punyakanok et al.  1 .
¡¡in this paper  we consider classifiers with linear representation. linear local classifiers are linear functions  fy x t  = ¦Áy ¡¤ ¦µy x t   where ¦Áy ¡Ê irdy is a weight vector and ¦µy x t  ¡Ê irdy is a feature vector. then  it is easy to show that the global scoring function can be written in the familiar form f x y  = ¦Á ¡¤ ¦µ x y   where ¦µy x y  = pnt=1y ¦µyt x t i{yt=y} is an accumulation over all output variables of features occurring for class y  ¦Á =  ¦Á1 ... ¦Á|y|  is concatenation of the ¦Áy's  and ¦µ x y  =  ¦µ1 x y  ... ¦µ|y| x y   is the concatenation of the ¦µy x y 's. then  the global classifier is h x  = y  = argmax ¦Á ¡¤ ¦µ x y1 .
y1¡Êc yny 
1 learning
we present several ways to learn the scoring function parameters differing in whether or not the structure-based inference process is leveraged during training. learning consists of choosing a function h : x  ¡ú y  from some hypothesis space  h. typically  the data is supplied as a set d = { x1 y1  ...  xm ym } from a distribution px y over x  ¡Áy . while these concepts are very general  we focus on online learningof linear representationsusing a variant of the perceptron algorithm  see  collins  1  .
¡¡learning local classifiers: learning stand-alone local classifiers is perhaps the most straightforward setting. no knowledge of the inference procedure is used. rather  for each example  x y  ¡Ê d  the learning algorithm must ensure that fyt x t    fy1 x t  for all t = 1 ... ny and all y1= yt. in figure 1 a   an online perceptron-style algorithm is presented where no global constraints are used. see  harpeled et al.  1  for details and section 1 for experiments.
¡¡learning global classifiers: we seek to train classifiers so they will produce the correct global classification. to this end  the key difference from learning locally is that feedback from the inference process determines which classifiers to modify so that together  the classifiers and the inference procedure yield the desired result. as in  collins  1; carreras and ma`rquez  1   we train according to a global criterion. the algorithm presented here is an online procedure  where at each step a subset of the classifiers are updated according to inference feedback. see figure 1 b  for details of a perceptron-like algorithm for learning with inference feedback.
¡¡note that in practice it is common for problems to be modeled in such a way that local classifiers are dependent on part of the output as part of their input. this sort of interaction can be incorporated directly to the algorithm for learning a global classifier as long as an appropriate inference process is used. in addition  to provide a fair comparison between lo  l+i  and ibp in this setting one must take care to ensure that the learning algorithms are appropriate for this task. in order to remain focused on the problem of training with and without inference feedback  the experiments and analysis presented concern only the local classifiers without interaction.
1 conjectures
in this section  we investigate the relative performance of classifier systems learned with and without inference feedback. there are many competing factors. initially  if the local classification problems are  easy   then it is likely that learning local classifiers only  lo  can yield the most accurate classifiers. however  an accurate model of the structural constraints could additionally increase performance  learning plus inference  l+i  . as the local problems become more difficult to learn  an accurate model of the structure becomes more important  and can  perhaps  overcome sub-optimal local classifiers. despite the existence of a global solution  as the local classification problems become increasingly difficult  it is unlikely that structure based inference can fix poor classifiers learned locally. in this case  only training with inference feedback  ibt  can be expected to perform well.
algorithm onlinelocallearning input: dx y ¡Ê {x  ¡Á y }m
output: {fy}y¡Êy ¡Ê h
	y	|¦µy|
	initialize ¦Á ¡Ê ir	for y ¡Ê y
repeat until converge
for each  x y  ¡Ê dx y do for t = 1 ... ny do y t = argmaxy ¦Áy ¡¤ ¦µy x t  if y t
¦Á t = ¦Á t +¦µ t x t  ¦Áy t = ¦Áy t   ¦µy t x t  a  without inference feedback
algorithm onlinegloballearning input: dx y ¡Ê {x  ¡Á y }m output: {fy}y¡Êy ¡Ê h
initialize ¦Á ¡Ê ir|¦µ|
repeat until converge
for each  x y  ¡Ê dx y do y  = argmaxy¡Êc yny  ¦Á ¡¤ ¦µ x y  if y  1= y then
¦Á = ¦Á+¦µ x y    ¦µ x y   b  with inference feedback
figure 1: algorithms for learning without and with inference feedback. the key difference lies in the inference step  i.e. argmax . inference while learning locally is trivial and the prediction is made simply by considering each label locally. learning globally uses a global inference  i.e. argmaxy¡Êc yny   to predict global labels.
¡¡as a first attempt to formalize the difficulty of classification tasks  we define separability and learnability. a classifier  f ¡Ê h  globally separates a data set d iff for all examples  x y  ¡Ê d  f x y    f x y1  for all y1 ¡Ê yny   y and locally separates d iff for all examples  x y  ¡Ê d  fyt x t    fy x t  for all y ¡Ê y   yt  and all y1 ¡Ê yny   y. a learning algorithm a is a function from data sets to a h. we say that d is globally  locally  learnable by a if there exists an f ¡Ê h such that f globally  locally  separates d.
¡¡the following simple relationships exist between local and global learning: 1. local separability implies global separability  but the inverse is not true; 1. local separability implies local and global learnability; 1. global separability implies global learnability  but not local learnability. as a result  it is clear that if there exist learningalgorithms to learn global separations  then given enough examples  ibt will outperform l+i. however  learning examples are often limited either because they are expensive to label or because some learning algorithms simply do not scale well to many examples. with a fixed number of examples  l+i can outperform ibt.
claim 1 with a fixed number of examples:
1. if the local classification tasks are separable  then l+ioutperforms ibt.
1. if the task is globally separable  but not locally separable then ibt outperforms l+i only with sufficient examples. this number correlates with the degree of the separability of the local classifiers.
1 experiments
we present experimentsto show how the relativeperformance of learning plus inference  l+i  compares to inference based training  ibt  when the quality of the local classifiers and amount of training data varies.
1 synthetic data
in our experiment  each example x is a set of c points in ddimensional real space  where x =  x1 x1 ... xc  ¡Ê ird ¡Á
... ¡Á ird and its label is a sequence of binary variable  y =  y1 ... yc  ¡Ê {1}c  labeled according to: y = h x  = argmaxc xyifi xi     1   yi fi xi   y¡Êc y  	i
where c yc  is a subset of {1}c imposing a random constraint1 on y  and fi xi  = wixi+¦Èi. each fi correspondsto a local classifier yi = gi xi  = ifi xi  1. clearly  the dataset generated from this hypothesis is globally linearly separable. to vary the difficulty of local classification  we generate examples with various degree of linear separability of the local classifiers by controlling the fraction ¦Ê of the data where h x  1= g x  =  g1 x1  ... gc xc  -examples whose labels  if generated by local classifiers independently  violate the constraints  i.e. g x  ¡Ê/ c yc  .
¡¡figure 1 compares the performance of different learning strategies relative to the number of training examples used. in all experiments  c = 1  the true hypothesis is picked at random  and c yc  is a random subset with half of the size of yc. training is halted when a cycle complete with no errors  or 1 cycles is reached. the performance is averaged over 1 trials. figure 1 a  shows the locally linearly separable case where l+i outperforms ibt. figure 1 c  shows results for the case with the most difficult local classification tasks ¦Ê = 1  where ibt outperforms l+i. figure 1 b  shows the case where data is not totally locally linearly separable ¦Ê = 1 . in this case  l+i outperforms ibt when the number of training examples is small. in all cases  inference helps.
1 real-world data
in this section  we present experiments on two real-world problems from natural language processing - semantic role labeling and noun phrase identification.
semantic-role labeling
semantic role labeling  srl  is believed to be an important task toward natural language understanding  and has immediate applications in tasks such information extraction and

 a  ¦Ê = 1 d = 1

 b  ¦Ê = 1 d = 1

 c  ¦Ê = 1 d = 1
figure 1: comparison of different learning strategies in various degrees of difficulties of the local classifiers. ¦Ê = 1 implies locally linearly separability. higher ¦Ê indicates harder local classification.
question answering. the goal is to identify  for each verb in the sentence  all the constituents which fill a semantic role  and determine their argument types  such as agent  patient  instrument  as well as adjuncts such as locative  temporal  manner  etc. for example  given a sentence   i left my pearls to my daughter-in-law in my will   the goal is to identify different arguments of the verb left which yields the output:
 a1 i   v left    a1 my pearls   a1 to my daughter-in-law 

figure 1: results on the semantic-role labeling  srl  problem. as the number of features increases  the difficulty of the local classification problem becomes easier  and the independently learned classifiers  lo  perform well  especially when inference is used after learning  l+i . using inference during training  ibt  can aid performance when the learning problem is more difficult  few features .
 am-loc in my will .
here a1 represents leaver  a1 represents thing left  a1 represents benefactor  am-loc is an adjunct indicating the location of the action  and v determines the verb.
¡¡we model the problem using classifiers that map constituent candidates to one of 1 different types  such as fao and fa1  kingsbury and palmer  1; carreras and ma`rquez  1 . however  local multiclass decisions are insufficient. structural constraints are necessary to ensure  for example  that no arguments can overlap or embed each other. in order to include both structural and linguistic constraints  we use a general inference procedure based on integer linear programming  punyakanok et al.  1 . we use data providedin the conll-1shared task  carreras and ma`rquez  1   but we restrict our focus to sentences that have greater than five arguments. in addition  to simplify the problem  we assume the boundaries of the constituents are given - the task is mainly to assign the argument types.
¡¡the experiments clearly show that ibt outperforms locally learned lo and l+i when the local classifiers are inseparable and difficult to learn. the difficulty of local learning was controlled by varying the number of input features. with more features  the linear classifier are more expressive and can learn effectively and l+i outperforms ibt. with less features the problem becomes more difficult and ibt outperforms l+i. see figure 1.
noun phrase labeling
noun phrase identification involves the identification of phrases or of words that participate in a syntactic relationship. specifically  we use the standard base noun phrases  np  data set  ramshaw and marcus  1  taken from the wall street journal corpus in the penn treebank  marcus et al.  1 .
the phrase identifier consists of two classifiers: one that

figure 1: results on the nounphrase  np  identificationproblem.
detects the beginning  f   and a second that detects the end  f  of a phrase. the outcome of these classifiers are then combined in a way that satisfies structural constraints constraints  e.g. non-overlapping   using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifiers' outcomes  punyakanok and roth  1 .
¡¡in this case  l+i trains each classifier independently  and only during evaluation  the inference is used. on the other hand  ibt incorporates the inference into the training. for each sentence  each word position is processed by the classifiers  and their outcomes are used by the inference process to infer the final prediction. the classifiers are then updated based on the final prediction not on their own prediction before the inference.
¡¡as in the previous experiment  figure 1 shows performance of two systems varied by the number of features. unlike the previous experiment  the number of features in each experiment was determined by the frequency of occurrence. less frequent features are pruned to make the task more difficult. the results are similar to the srl task in that only when the problem becomes difficult ibt outperforms l+i.
1 bound prediction
in this section  we use standard vc-style generalization boundsfrom learning theoryto gain intuition into when learning locally  lo and l+i  may outperform learning globally  ibt  by comparing the expressivity and complexity of each hypothesis space. when learning globally  it is possible to learn concepts that may be difficult to learn locally  since the global constraints are not available to the local algorithms. on the other hand  while the global hypothesis space is more expressive  it has a substantially larger representation. here we develop two bounds-both for linear classifiers on a restricted problem. the first upper bounds the generalization error for learning locally by assuming various degrees of separability. the second provides an improved generalization bound for globally learned classifiers by assuming separability in the more expressive global hypothesis space.
¡¡we begin by defining the growth function to measure the effective size of the hypothesis space.
definition 1  growth function  for a given hypothesis class h consisting of functions h : x ¡ú y  the growth function  nh m   counts the maximum number of ways to label any data set of size m:
	nh m  =	sup	|{ h x1  ... h xm  |h ¡Ê h}|
x1 ... xm¡Êxm
¡¡the well-known vc-style generalization bound expresses expected error    of the best hypothesis  hopt on unseen data. in the following theorem adapted from  anthonyand bartlett  1  theorem 1   we directly write the growth function into the bound 
theorem 1 suppose that h is a set of functions from a set x to a set y with growth function nh m . let hopt ¡Ê h be the hypothesis that minimizes sample error on a sample of size m drawn from an unknown  but fixed probability distribution. then  with probability 1   ¦Ä
1 log nh 1m   + log 1/¦Ä  
	 ¡Ü opt +	.	 1 
m
¡¡for simplicity  we first describe the setting in which a separate function is learned for each of a fixed number  c  of output variables  as in section 1 . here  each example has c components in input x =  x1 ... xc  ¡Ê ird ¡Á ... ¡Á ird and output y =  y1 ... yc  ¡Ê {1}c.
¡¡given a dataset d  the aim is to learn a set of linear scoring functions fi xi  = wixi  where wi ¡Ê ird for each i = 1 ... c. for lo and l+i  the setting is simple: find a set of weight vectors that  for each component  satisfy yiwixi   1 for all examples  x y  ¡Ê d. for ibt  we find a set of classifiers such that pi yiwixi   pi yi1wixi for all y1= y  and that satisfy the constraints  y1 ¡Ê c yc  .
¡¡as previously noted  when learning local classifiers independently  lo and l+i   one can only guarantee convergence when each local problem is separable - however  it is often the case that global constraints render these problems inseparable. therefore  there is a lower bound  opt  on the optimal error achievable. since each component is a separate learning problem  the generalization error is thus
corollary 1 when h is the set of separating hyperplanes in ird 
1 dlog  em/d   + log 1/¦Ä  
	 ¡Ü opt +	.	 1 
m
proof sketch: we show that nh m  ¡Ü  em/d d when h is the class of threshold linear functions in d dimensions. nh m  is precisely the maximum number of continuous regions an arrangement of m halfspaces in ird  which is 1pdi=1 m i 1 ¡Ü 1 e m   1 /d d. for m   1  the result holds. see  anthony and bartlett  1  theorem 1  for details.	
¡¡on the other hand  when learning collectively with ibt  examples consist of the full vector x ¡Ê ircd. in this setting  convergenceis guaranteed  if  of course  such a function

	1	1	1
number of examples	1 x 1
figure 1: the vc-style generalization bounds predict that ibt will eventually outperform lo if the local classifiers are unable to find consistent classification  opt   1  accuracy   1 . however  if the local classifiers are learnable  opt = 1  accuracy = 1   lo will perform well.
exists . thus  the optimal error when training with ibt is opt = 1. however  the output of the global classification is now the entire output vector. therefore  the growth function must account for exponentially many outputs. corollary 1 when h is the set of decision functions over {1}c  defined by argmaxy1¡Êc {1}c  pci=1 yiwixi  where w =  w1 ... wc  ¡Ê ircd 

	 r	.	 1 
m
proof sketch: in this setting  we must count the effective hypothesis space - which is the effective number of different classifiers in weight space  ircd. as before  this is done by constructing an arrangement of halfspaces in the weight space. specifically  each halfspace is defined by a single
  x y  y1  pair that defines the region where pi yiwixi   i yi1wixi. because there are potentially c 1c  ¡Ü 1c output labels and the weight space is cd-dimensional  the growth function is the size of the arrangement of c1c halfspaces in ircd. therefore nh m  ¡Ü  em1c/cd cd. 
¡¡figure 1 shows a comparison between these two bounds  where the generalizationboundcurves on accuracyare shown for ibt  corollary 1  and for lo and l+i  corollary 1  with opt ¡Ê {1 1 1}. one can see that when separable  the accuracy=1 curve  opt = 1  in the figure outperforms ibt. however  when the problems are locally inseparable  ibt will eventually converge  whereas lo and l+i will not - these results match the synthetic experiment results in figure 1. notice the relationship between ¦Ê and opt. when ¦Ê = 1  both the local and global problems are separable and opt = 1 as ¦Ê increases  the global problem remains separable and the local problems are inseparable  opt   1 .
1 conclusion
we studied the tradeoffs between three common learning schemes for structured outputs  i.e. learning without the knowledge about structure  lo   using inference only after learning  l+i   and learning with inference feedback  ibt .
we provided experiments on both real-world and synthetic data as well as a theoretical justification that support our main clams. - first  when the local classification is linearly separable  l+i outperforms ibt  and second  as the local problems become more difficult and are no longer linearly separable  ibt outperforms l+i  but only with sufficient number of training examples. in the future  we will seek a similar comparison for the more general setting where nontrivial interaction between local classifiers is allowed  and thus  local separability does not imply global separability.
1 acknowledgments
we grateful dash optimization for the free academic use of xpressmp. this research is supported by the advanced research and development activity  arda 's advanced question answering for intelligence  aquaint  program  a doi grant under the reflex program  nsf grants itr-iis-1  itr-iis-1 and iis1  and an onr muri award.
