 
huge amounts of data are stored in autonomous  geographically distributed sources. the discovery of previously unknown  implicit and valuable knowledge is a key aspect of the exploitation of such sources. in recent years several approaches to knowledge discovery and data mining  and in particular to clustering  have been developed  but only a few of them are designed for distributed data sources. we propose a novel distributed clustering algorithm based on non-parametric kernel density estimation  which takes into account the issues of privacy and communication costs that arise in a distributed environment. 
1 	introduction 
knowledge discovery is a process aiming at the extraction of previously unknown and implicit knowledge out of large databases  which may potentially be of added value for some given application  fayyad et al.  1 . 
　data mining  which is devoted to the automated extraction of unknown patterns from given data  is a central element among the steps of the overall knowledge discovery process; the steps include preparation of the data to be analyzed as well as evaluation and visualization of the discovered knowledge. the large variety of data mining techniques which have been developed over the past decade include: methods for patternbased similarity search  cluster analysis  decision-tree based classification  generalization taking the data cube or attributeoriented induction approach  and mining of association rules  chen et al.   1  
　the increasing demand to scale up to massive data sets which are inherently distributed over networks with limited bandwidth and computational resources has led to methods for parallel and distributed knowledge discovery  kargupta et al.  1   the related pattern extraction problem in distributed knowledge discovery is referred to as distributed data mining. distributed data mining is expected to perform partial analysis of data at individual sites and then to send the outcome as partial result to other sites where it is sometimes aggregated to the global result. 
　one of the most common approaches of business applications to perform distributed data mining is to centralize distributed data into a data warehouse on which to apply the usual data mining techniques. data warehousing is a popular technology which integrates data from multiple data sources into a single repository in order to efficiently execute complex analysis queries  moro and sartori  1 . however  despite its commercial success  this approach may be impractical or even impossible for some business settings  for instance: 
  when huge amounts of data are  frequently  produced at different sites and the cost for their centralization cannot scale in terms of communication  storage and computation; 
  whenever data owners cannot or do not want to release information  for instance to protect privacy or because disclosing such information may result in a competitive advantage or a considerable commercial added value. 
　one of the most studied data mining techniques in centralized environments is data clustering. the goal of this technique is to decompose or partition a data set into groups such that both intra-group similarity and inter-group dissimilarity are maximized. despite the success of data clustering in centralized environments  only a few approaches to the problem in a distributed environment are available to date. 
　in this work we present kdec  a novel approach to distributed data clustering based on sampling density estimates. in kdec each data source transmits an estimate of the probability density function of its local data to a helper site  and then executes a density based clustering algorithm that is driven by the overall density estimate  which is built by the helper from the samples of the local densities. 
　the paper is organized as follows. in section 1 we describe related work and highlight differences with respect to our approach. section 1 and 1 present the kdec scheme to distributed data clustering. finally  section 1 concludes the paper and outlines ongoing and future research work. 
1 	related work 
in  johnson and kargupta  1  a tree clustering approach is taken to build a global dendrogram from individual dendrograms that are computed at local data sites subject to a given set of requirements. in contrast to the approach presented in this paper  the distributed data sets are assumed to be heterogeneous  therefore every site has access only to a subset of the features of an object. the proposed solution implements a distributed version of the single-link clustering algorithm which generates clusters that are substantially different from the ones generated by density-based methods. in particular  it suffers from the so-called chaining effect  by which any of two well separated and internally homogeneous groups of objects connected only by a dense sequence of objects are regarded as a single cluster.  kargupta et al  1  proposes a technique for distributed principal component analysis  collective pca. it is shown that the technique satisfies efficiency and data security requirements and can be integrated with existing clustering methods in order to cluster distributed  highdimensional heterogeneous data. since the dimensionality of the data is reduced prior to clustering by applying pca  the approach is orthogonal to ours. another related research direction deals with incremental clustering algorithms. the birch izhang et a/.  1  and related bubble method 
 ganti et al  1   compute the most accurate clustering  given the amount of memory available  while minimizing the number of i/o operations. it uses a dynamic index structure of nodes that store synthetic  constant-time maintainable summaries of sets of data objects. the method is sufficiently scalable requiring 1 n ogn  time and linear i/o. however  since it uses the centroid to incrementally aggregate objects  the method exhibits a strong bias towards globular clusters. incrementaldbscan lester et a/.  1  is a dynamic clustering method supporting both insertions and deletions  which is shown to be equivalent to the well-known static dbscan algorithm. since in turn dbscan can be shown to be equivalent to a method based on density estimation when the kernel function is the square pulse and the clusters are density-based  incrementaldbscan is less general than methods based on kernel density estimates. its time complexity is 1 n ogn . 
1 data clustering 
1 the cluster analysis problem 
cluster analysis is a a descriptive data mining task which aims at decomposing or partitioning a usually multivariate data set into groups such that the data objects in one group are similar to each other and are different as possible from those in other groups. therefore  a clustering algorithm j  -  is a mapping from any data set s of objects to a clustering of 1  that is  a collection of pairwise disjoint subsets of 1. clustering techniques inherently hinge on the notion of distance between data objects to be grouped  and all we need to know is the set of interobject distances but not the values of any of the data object variables. several techniques for data clustering are available but must be matched by the developer to the objectives of the considered clustering task  grabmeier and rudolph  1 . in partition-based clustering  for example  the task is to partition a given data set into multiple disjoint sets of data objects such that the objects within each set are as homogeneous as possible. homogeneity here is captured by an appropriate cluster scoring function. another option is based on the intuition that homogeneity is expected to be high in densely populated regions of the given data set. consequently  searching for clusters may be reduced to searching for dense regions of the data space which are more likely to be populated by data objects. that leads us to the approach of density estimation based clustering. 
1 density estimation based clustering 
in density estimation  de  based clustering the search for densely populated regions is accomplished by estimating a so-called probability density function from which the given data set is assumed to have arisen. many techniques for de-based clustering are available from the vast kdd literature  ankerst et al  1; ester et al  1; schikuta  1; hinneburg and keim  1  and statistics  silverman  1 . 
in both areas  the proposed clustering methods require the computation of a non-parametric estimation of the density function from the data. one important family of nonparametric estimates is known as kernel estimators. the idea is to estimate a density function by defining the density at any data object as being proportional to a weighted sum of all objects in the data set  where the weights are defined by an appropriately chosen kernel function. in the following we introduce kernel-based density estimation  parzen  1; silverman  1  and our approach to density estimation based clustering. 
let us assume a set  of data 
points or objects. kernel estimators originate from the intuition that the higher the number of neighbouring data ob-
jects x; of some given object   the higher the density at this object x. however  there can be many ways of capturing and weighting the influence of data objects. when given the distance between one data object x and another jc  as an argument  the influence of jf; may be quantified by using a so called kernel function. a kernel function k  x  is a real-valued  non-negative function on r which has finite integral over r. when computing a kernel-based density estimation of the data set 1  any element jf - in s is regarded as to exert more influence on some  than elements which are farther from  than the element. accordingly  kernel functions are often non-increasing with . prominent examples of kernel functions are the square pulse function and the gaussian function 
　a kernel-based density estimate is defined  modulo a normalization factor  as the sum over all data objects xi in s of the distances between scaled by a factor a  called window width  and weighted by the kernel function k: 
		 1  
　the influence of data objects and the smoothness of the estimate is controlled by both the window width h and the shape of kernel k  h controls the smoothness of the estimate  whereas k determines the decay of the influence of a data object according to the distance. even if the number n of data objects is very large  in practice it is not necessary to compute n distances for calculating the kernel density estimate at a given object x. in fact  the value of commonly used kernel functions is negligible for distances larger than a few h units; 

learning 

it may even be zero if the kernel has bounded support  as it is the case  for example  for the square pulse. using kernelbased density estimation  it is straightforward to decompose the clustering problem into three phases as follows. 
1. choose a window width h and a kernel function k. 
1. compute the kernel-based density estimate from the given data set. 
1. detect regions of the data space where the value of the estimate is high and group all data objects of space regions into corresponding clusters. 
　in the literature  many different definitions of cluster have been proposed formalizing the clusters referred to in step 1 above. a density-based cluster  ester el al  1  collects all data objects included in a region where density exceeds a threshold. center-defined clusters  hinneburg and kcim  1  are based on the idea that every local maximum of corresponds to a cluster including all data objects which can be connected to the maximum by a continuous  uphill path in the graph of  p. finally  an arbitrary-shape cluster lhinneburg and keim  1  is the union of center-defined clusters having their maxima connected by a continuous path whose density exceeds a threshold. 
　algorithm 1  de-cluster  implements the computation of center-defined clusters by a climbing procedure driven by the density estimate. the main procedure is decluster  taking as inputs an instance s of the class of data objects  the kernel function k  the window width h  and returning a clustering represented by c  which stores a mapping from each x  to the unique integer label ofxif 's cluster. it is assumed that s is an instance of a class which provides the following methods: get i  to access object x  given index i  nq k x  and radius to retrieve  given  the indexes and maximum distance of. nearest neighbours. uphill computes the steepest direction on the graph of the estimated density as the versor of its gradient  computed by function degradient  cf.  hinneburg and keim  1  . uphill then moves in that direction a fractions .   of the distance s.radius  of the k th nearest neighbour off  and finally returns the index of the nearest neighbour in s of the reached position. every nested call to fixedpoint marks the current object x  as visited and calls uphill to get the index of the next data object xj. if such object has already been visited  the proximity of a local maximum has been reached and j is taken as new cluster label. otherwise  is inductively assumed to lie at the bottom end of a path leading to the proximity of a local maximum  and to be already labeled accordingly.  if  is not marked as clustered  a recursive call ensures that the assumption holds.  
　the complexity of the de-cluster algorithm is that of calling n - s.count times fixedpoint. at the beginning of every iteration in decluster  the sets of clustered and visited ob-
jects are equal. fixedpoint is never called with a clustered object as argument  and visits unclustered objects at most once. therefore  even if the number of visited data objects in one call of fixedpoint is bounded only by n  the number of visited data objects in all calls is still only n. for each visited object a single k-nearest neighbour query suffices to compute the gradient and the next uphill object. the methods  can be efficiently implemented by equipping the class of s with a spatial access method like the kd-  or mvp-  or m-tree. therefore  the time complexity of declusler is 1 nq n    where q n  is the cost of a k nearest neighbour query in any such access method. note that in many practical cases  q n  is very close to log tv. 
1 	distributed data clustering 
the body of work on applications of data clustering in distributed environments  the problem of so called distributed data clustering  ddc   is comparatively small. in this section we adopt the kernel density estimation based clustering approach presented above for the distributed case assuming homogeneous data  which means that a data object cannot be split across two sites. 
1 	the ddc problem 
we define the problem of homogeneous distributed data clustering for a clustering algorithm a as follows. let be a data set of objects. let 1 ...  m  be a finite set of sites. each site stores one data set dj  and it will be assumed that . the ddc problem is to find a site clustering cj residing in the data space of l   for such that 
 i .  correctness requirement  
 ii . time and communications costs are minimized  efficiency requirement  
 iii . at the end of the computation  the size of the subset of s which has been transferred out of the data space of any site  is minimized  privacy requirement . 

the traditional solution to the homogeneous distributed data clustering problem is to simply collect all the distributed data sets dj into one centralized repository where their union s is computed  and the clustering c of the union s is computed and transmitted to the sites. such approach  however  does not satisfy our problem's requirements both in terms of privacy and efficiency. we therefore propose a different approach yielding a kernel density estimation based clustering scheme  called kdec. 
1 	the kdec scheme for ddc 
the key idea of the kdec scheme is based on the following observation: although the density estimate computed on each local data set gives information on the distribution of the objects in the data set  it conceals the objects themselves. moreover  the local density estimate can be coded to provide a more compact representation of the data set for the purpose of transmission. in the sequel  we tacitly assume that all sites lj agree on using a global kernel function k and a global window width h. we will therefore omit k and h from our notation  and write . density estimates in the form of equation  1  are additive  i.e. the global density estimate  can be decomposed info the sum of the site density estimates  one estimate for every data set dj. 
		 1  
thus  the local density estimates can be transmitted to and summed up at a distinguished helper site yielding the global estimate which can be returned to all sites. each site lj may then apply  in its local data space  the hill-climbing technique of algorithm 1  de-ciuster  to assign clusters to the local data objects. there is nevertheless a weakness in such a plan: the definition of a density estimate explicitly refers to all the data objects j ;. hence  knowing how to manipulate the estimate entails knowing the data objects  which contradicts the privacy requirement. however  only an intensional  algebraic definition of the estimate includes knowledge of the data ob-
jects. multidimensional sampling theory provides the basis for an alternative extensional representation of the estimate which makes no explicit reference to the data objects. 
　the theoretical idea of sampling is to represent a function / by a sampling series  that is  a summation of suitable expansion functions weighted by the values of / at a discrete subset of its domain  higgins  1 . in the following  let the 
/th coordinate of 	be denoted by 	and let diag 
	= 	  denote th  	diagonal matrix 
having diagonal w  i.e.  defined by 	if 
diag further l e t ' a vector of sampling periods. the sampled form of at intervals is the s e q u e n c e d e f i n e d by 
 1  
where   is the inner product between vectors. therefore  is the sequence of the values of  at all the real  ndimensional vectors whose i th coordinates are spaced by a multiple of the i th sampling period ti  1 ... /!. the sampled forms of the local density estimates are defined in a similar way by ... m. it is immediate to see by  1  that addivity holds for the sampled forms: 
 1  
therefore  after receiving the sampled forms 	of the 
m density estimates  the helper site can compute by  1  the sampled form of the overall estimate and transmit it to the sites lj. sites lj can then cluster local data with respect to the overall density estimate  using the gradient of the sampling series 
 1  
as needed in the hill-climbing function. 
　we briefly discuss the extent to which the series  1  can be used to represent it is well known that  under mild conditions  sampling a function is an invertible transformation if  for every coordinate i = 1 ... n  there is a frequency  such that the fourier transform of g differs from zero only in the interval   and the samples are computed with a period not greater than  cf.  higgins  1  . under these assumptions  the value of the sampling series computed at x equals g x . unfortunately  most popular kernel functions  hence summations of kernel functions  do not satisfy these hypotheses since the support of their fourier transform is unbounded. consequently sampling density estimates yields an information loss. however  it can be shown that the fourier transform of a kernel density estimate is negligible everywhere except for  not greater than  therefore  the global density estimate can be reconstructed from its samples by  1  introducing only a small error if 
　it is worth noting that the infinite series  1  need not be approximated  if it has finitely many nonzero terms. the latter case holds if the used kernel function has a bounded support  since the density estimate will also have bounded support. if  however  the kernel function has unbounded support  like the gaussian kernel  then the density estimate can be approximated by regarding its value to be zero everywhere except inside an appropriately chosen bounded region. 
　according to this approach  we propose the following algorithmic kdec scheme for computing the kernel density estimation based clusters for local data spaces at m distributed data sites lj  see algorithm 1 . every local site runs the procedure sitedecluster whereas the helper site runs helper. sitedecluster is passed a reference h to the helper and the local data set d  and returns a clustering in a class instance c. helper is passed a list of references l to the local sites. the procedure sitenegotiate carries out a negotiation with the other local sites through helpernegotiate at the helper site to determine the sampling periods x  the bounding corners of the sampling rectangle   the kernel k  and 
learning 


the window width h. the negotiation procedures contain appropriate handshaking primitives to ensure that all sites participate and exit negotiations only if an agreement has been reached. each local site computes the sampled form of the estimate of d  by calling function sample  and sends it to the helper.  function de computes the density estimate and is omitted for brevity.  the helper receives the sampled estimates  sums them by sampling indexes into a global sample  and returns it to all sites. procedures send and receive implement appropriate blocking and handshaking to ensure the transmission takes place. each local site uses the global sample in functions fixedpoint and uphill to compute the values of the gradient of the global density estimate. function seriesgradient can be easily derived from  1 . local sites perform a de-cluster algorithm to compute the corresponding local data clusters. the details of the hill-climbing strategy are however different from algorithm 1 because the sites are allowed access to local data objects only. uphill advances a fraction 1 of the gradient in its direction  if the gradient's norm exceeds a threshold e. if a of the object returmed by uphill contains an already clustered data object  the current cluster label id is set from that object's label. otherwise  checking whether uphill returned the same space object signals to fixedpoint that the proximity of the local maximum has been reached. the maximum is marked by adding the current space object x as a dummy object to the local data set; this ensures that subsequent paths converging to the same local maximum will use the same cluster label as the current path. method d.add{  returns the identifier of the added object  which is used as current cluster label. if neither case holds  the label id is obtained by a recursive call. finally  all objects in a small neighbourhood of the current object are labeled by id. note that adding dummy objects has eflect only on the range queries  and does not modify the density estimate. 
1 	complexity of the kdec scheme 
in terms of the complexity in computation and communication one crucial point of the kdec scheme is how many samples have to be computed and transferred among the sites. in most cases  to obtain good density estimates  h must not be less than a small multiple of the smallest object distance. as ti ~ h/1  the number of samples should rarely exceed the number of objects  if only space regions where the density estimate is not negligible are sampled. since the size of a sample is usually much smaller than the size of a data ob-
ject  the overall communication costs of our ddc approach will be in most cases significantly lower than in a centralized approach. of course  the precise number of samples depends on the bounding region that is being sampled by every site. in algorithm 1 the site lj determines autonomosly the rectangle that contains the computed samples. 
　the computational costs of the kdec scheme in terms of used cpu cycles and i/o do not exceed the one in the centralized approach where clustering is performed on data collected in a single repository. the computational complexity is linear in the number of samples. the precise cost of computation of any kdec-based ddc algorithm as an instance of the proposed scheme largely depends also on the used kernel function and local clustering algorithm. the de-cluster algorithm we developed for the kdec scheme in section 1 is of complexity 1 nq n    where q n  is the cost of a nearest neighbour query  which in practical cases is close to logjv . algorithm 1 implements a slightly different approach in the hill-climbing function than algorithm 1  since the function does not use data objects to direct the uphill path. however  preliminary results of experiments conducted on a prototype implementation show good scalability of the approach  in terms of number of executed range queries. 
1 	conclusion 
due to the explosion in the number of autonomous data sources there is a growing need for effective approaches to distributed knowledge discovery and data mining. in this paper we have presented kdec  a novel scheme for distributed data clustering which computes the density estimation  to per-

form the clustering  from sampled forms of local densities at each data source site. 
　the approach exploits statistical density estimation and information theoretic sampling to minimize communications between sites. moreover  the privacy of data is preserved to a large extent by never transmitting data values but kernel based density estimation samples outside the site of origin. the approach does not require cpu and i/o costs significantly higher than a similar centralized approach and its communication costs may be lower. ongoing research focuses in particular on implementations of a multiagent system for kdec-based distributed data clustering in a peer-to-peer network  and investigation on methods to mitigate the risk of security and privacy violations in distributed data mining environments. 
