. our understanding of how to use large amounts of knowledge to enhance the problem solving capabilities of computer programs is quite limited. over the past several years a number of knowledge based systems have been developed  and this experience has provided us with a handfull of techniques we can apply in a few domains. what we don't have yet is much of an appreciation of why these techniques work or of the limits of their usefulness in order to take this next step  we need more data. unfoitunately  the analyses of the expert systems currently being built tend to ignore questions that could provide precisely the data needed. this paper proposes a few questions that it might be worth asking and shows how answers to those questions could begin to give us the understanding we lack. 
1. introduction 
if those of us involved in expert systems research have a common goal  surely it is to understand how to use large amounts of knowledge in solving problems. we have made some progress in the 1s progress promised to be rapid; a small number of interesting systems were developed  and their developers described the tasks the systems could perform and the various techniques used. over the past few years  many more people have become involved  and expert systems are now springing up all over. unfortunately  this dramatic growth in the number of expert systems has not  as far as i can see  been accompanied by even a modest growth in our understanding of the relationship between knowledge and search. the problem is a lack of data. though the number of expert systems that have been developed is now sufficient to allow us to begin to deepen our understanding  the information available about those systems is inadequate. 
i am aware of only a few attempts to draw general lessons from an examination of a number of expert systems. a paper by feigenbaum focuses on several of the systems developed at stanford's heuristic programming project  feigenbaum 1 . although the paper gives some indication of the nature of the task each system addresses  its primary purpose is to provide an existence proof that the by now well known tools of the knowledge engineer can be used with success in a variety of domains. more recently  a group of experienced expert system builders wrote a paper which offers a map linking task characteristics with techniques for solving the task  stefik 1 ; the paper is one of the chapters in a forthcoming book on expert systems  hayes roth 1 . while the paper will be a valuable guide to people tryiny to construct al application systems  several of the systems discussed are by no stretch of the imagination knowledge based; thus the relationship between some of the prescribed techniques and knowledgeintensive tasks is not at all clear. two other works containing the insights of experienced expert systems builders will soon be available  buchanan 1a    buchanan 1b . though both works provide a number of important insights  they focus almost exclusively on a single task domain. neither indicates very clearly the scope of the various lessons learned. 
this paper assumes that what distinguishes expert systems from other al systems  and hence what makes thorn interesting  is that they briny relatively large amounts of knowledge to bear in solving problems. the claim made here is that a few basic aspects of task domains are the sources of the significant variety one finds among expert systems. if systems are described in terms of these aspects  we will gain an understanding of the various roles knowledge can play and an appreciation of the factors that define these roles. in the next section  four aspects of task domains are identified  and then in the third section  three expert systems are described in terms of those aspects. on the basis of these descriptions  several hypotheses about knowledge and its roles in problem solving are generated. the four aspects identified are supposed to be taken seriously; though the set is undoubtedly incomplete  we cannot understand the roles knowledge plays in problem solving without attending to those four aspects. the hypotheses are not to be taken seriously; their function is to suggest what we might learn about knowledge if we had a large number of systems to serve as data points. since each hypothesis was generated on the basis of at most three data points  all any of them can do is tentatively point the way. 
1. figuring out the questions 
in order to understand the roles knowledge plays in any particular task domain  it is necessary to know at least the following sorts of things about the task: 
  compartmentalizability of the task knowledge. 
  uncertainty of the task information. 
  applicability of the task knowledge. 
  thickness of the task knowledge. 
how and to what extent task knowledge can be compartmentalized depends almost exclusively on the structure of the task; the more compartmentalized the knowledge  the narrower its scope and the simpler the choice of what pieces of knowledge to apply. knowledge operates on information made available by the task environment; the more uncertain that information  the less control the knowledge has. each piece of knowledge has limited applicability; a piece of knowledge is more or less powerful depending on how precisely it specifies the set of situations in which it is applicable. finally  many different pieces of knowledge may be relevant in the same situation; the thicker the task knowledge  the more knowledge there is to bring to bear at any given time. the rest of this section further elaborates each of these aspects. 
1. the compartmentalizability of knowledge 
before much sense can be made of compartmentalizability  it is necessary to find a structure that can serve as a compartment. one plausible structure is the subtask. when human experts talk about what they do  they frequently first locate themselves by naming some subtask and then indicate what conditions have to be satisfied in order for some behavior to be appropriate. the subtask name appears to stand for a set of conditions all of which must be satisfied before any of the behaviors associated with that subtask are appropriate. typically subtasks are organized hierarchically; higher level subtasks are partitioned into a set of smaller  more focused subtasks. it is not clear what the criteria for being a primitive subtask are; presumeably experts differ somewhat with respect to the level to which they decompose the task. but since experts can communicate successfully with others using subtask names  it is unlikely that the decompositions differ to a very great extent. pieces of knowledge with the same common set of applicability conditions are distinguished from other pieces of knowledge associated with other subtasks. but within the subtask there are no such obvious boundries; in that sense  all of the knowledge associated with the subtask is potentially relevant whenever the subtask is being performed. 
human experts are not at all good at estimating how much knowledge is associated with each subtask. and in fact there is no apparent way of getting a reasonable estimate that does not involve building a system which performs the task. without an estimate of the amount of knowledge associated with each subtask  it is impossible to understand the access requirements the task imposes. if there is very little knowledge associated with each subtask  it may be possible to statically impose a structure on that knowledge. the more knowledge that is relevant to a subtask. the more likely it is that there will be issues of how to determine dynamically when to apply which pieces of knowledge. 
1. the uncertainty of information 
two ways in which the task environment strongly shapes the nature of the task are with  1  the reliability of the data it provides  and  1  the points in time  relative to the task  at which the data become available. the unreliability of data is a more or less serious issue depending on a variety of factors. if the agent can assume the data are correct  whether or not they are   then true reliability is irrelevant. if the task environment provides a measure of reliability for each datum  then  unless the measure is itself unreliable  reliability is only a minor issue. if there is ambiguity about the reliability of at least some of the data  but there are ways of determining whether a datum is incorrect within the time frame imposed by the task  then although data reliability is a serious issue  it is at least more tractable than the case in which unreliability cannot be recognized. 
if the task environment provides additional data to the agent during the course of the task  it makes a difference who initiates whatever interaction there might be. if the task environment is passive and interactions are initiated only by the agent  the most significant question is what information does the agent have access to  eg  can the agent ask for new information and clarification or confirmation of data already provided   if the task environment is active and intitiates interactions with the agent  the issue is whether new data can conflict with data provided earlier; if so  a significant question is whether the conflicting data are generated independently of what is happening in the task or because of the effects of the task on the task environment. 
1. the applicability of knowledge 
in any given situation  an agent may have some pieces of knowledge it is certain are applicable  other pieces of knowledge it suspects may be applicable  and presumeably a great deal of knowledge it is certain is not applicable. i will use the term  applicability factor  to distinguish among these three cases. if the conditions on the applicability of an action have an applicability factor of 1  performing the action will result in a transition to a state that is on a solution path. if the conditions of applicability have an applicability factor less than 1 but greater than 1  there is some likelihood that performing the action will result in a transition to a state on a solution path but an applicability factor less than 1 signals that if the agent performs the action  it may have to backtrack. 
there are three reasons why an agent might misuse its 
1 j. mcdermott 
knowledge:  1  the agent does not have sufficient information about the current situation  ie  the conditions of applicability on an action are not satisfied    1  it has misleading information about the current situation  or  1  it does not know the precise conditions under which its knowledge is relevant  le  the applicability factor is less than 1 . in the first case  if the agent can recognize when it is lacking such information and can get the exisimg information from its external environment  it will not fail to apply relevant knowledge. in the third case  it may be that the agent is simply ignorant; if the conditions under which the knowledge is relevant can be more precisely defined  then the conditions of applicability may be able to be elaborated to the point that the applicability factor becomes 1; see  doyle 1  for a more thorough discussion of this point. however  if the conditions under which a piece of knowledge is relevant are highly complex or if no one knows them  the possibility of misapplying the knowledge will remain. in such cases it may be possible to extract from an expert or otherwise acquire a reasonably accurate measure of the likelihood that applying the knowledge will result in a state on the solution path; if so. the applicability factor can be taken into account in guiding the search. 
it is important to distinguish between tasks in which the agent can know when it has accomplished the task  achieved its goal  and tasks in which this is not possible. in tasks in which the agent uses only knowledge whose applicability factor is 1  it knows when it has accomplished its task because it sees there is nothing more to do in tasks where some of all of the knowledge used has an applicability factor less than 1  if the agent knows enough about its goal to determine whether it is on a path leading to that goal  then although it may have to search extensively  it will recognize when it has achieved the goal; in such cases  the only role for applicability factors is to make search more efficient. in tasks in which the agent cannot know whether il has achieved its goal  the applicability factors not only can help direct the search  but can also serve as indirect measures of the likelihood that the goal has been achieved. 
1. the thickness of knowledge 
task knowledge is thick if the same situations can evoke a wide variety of actions depending on the agent's meta-goals. if  for example  the agent can deal with its task at a number of levels of abstraction  its knowledge is thicker than that of an agent who can deal with the task at fewer levels of abstraction. a more interesting example of how the same situation can invoke different knowledge arises when we ask how the association between an action and the conditions of applicability on that action are justified. if someone who claims some piece of knowledge k is always relevant in situation s is asked to justify that claim  the knowledge k' that is used as justification is also relevant in situation s; k' has the same function as k  but presumeably is more basic and thus relevant in many situations besides s. at the limit is analogy. if problems that arise while a task is being performed can be solved with knowledge having no direct connection to the task  endless opportunities for creative problem solving open up. 
1. converting expert systems to data points 
in this section  the aspects of task domains identified above are used to analyze three knowledge based systems; the results of the analyses are several hypotheses about the effects on problem solving competence of large amounts of domain knowledge. since only three systems are considered  the analyses will collectively prove to be woefully inadequate  and the hypotheses will be at best suggestive. it will hopefully be clear  however  that if the set of aspects were extended a bit and if fifty or a hundred data points were used instead of three  a number of interesting hypotheses could be generated. 
the three systems we will consider were  or are being  developed at cmu for digital equipment corporation. rl is a computer system configurer  mcdermott 1a ; given a set of components  it adds any support components that might be missing and determines what the spatial relationship among all ot the components should be. xsel is a salesperson's assistant  mcdermott 1b ; it helps the salesperson select a set of components tailored to the customer's needs and design a floor layout for the resulting system. the xsel user can call on r1 to configure or reconfigure the current set of component selections and can provide information to r1 which insures that the resulting configuration will satisfy special requirements of individual customers  mcdermott 1 . ptrans is a manufacturing management assistant  haley 1 ; it helps insure that the inevitable problems which arise and threaten to delay the delivery of systems to customers are resolved satisfactorily in time  xsel will interact with ptrans so that a delivery date can be confirmed as soon as an order is booked. 
rvs original task was to generate the configuration diagram and do the order-completeness checking required before a vax 1 order could be built in one of digital's final assembly and test plants; the original version of ri was tested in the fall of 1 and began to be used on a regular basis in january  1  to configure all'vax-1 orders. the information available to r1 consisted of just the set of components  lineitems  ordered by the customer; no information was available about how the customer intended to use various components or how he intended to lay them out on the floor. as xsei. was being developed  it became clear that it could collect and pass on to r1 information that would enable r1 to tailor its configurations to the requirements of individual customers; consequently. r1 was extended to be able to use this additional information. more recently rvs knowledge has been augmented  by developers at digital  so that it can now configure systems other than the vax 1. the version of r1 discussed below is the version currently used by digital. 
xsels development is occurring in two stages. the initial version of the system assists salespeople with both component selection and floor layout design; the assistance provided in component selection assumes the user already has a good appreciation of the customer's computing needs. this version of xsel began to be field tested in may  1. though its capabilities have been strongly shaped by a userdesign group which was formed early in its development  no significant redefinition of xsl l's task has occurred. a major extension to xsfl is being developed at cmu; the extended system will be an expert in sizing a customer's computing needs. the version of xsel discussed in this paper is the version which includes the yet to be refined sizing capability. 
petrans will ultimately be able to assist with the management tasks that arise from the time an order is received by digital's manufacturing organization until it is delivered to the customer. the initial version of pirans  howevei  deals only with management issues which arise in final assembly and test plants. these plants build complex systems from components produced by high-volume plants. the principal management tasks are determining when and where on the floor to build each system  insuring that the necessary parts are on hand when it is time to issue them to the floor  and tracking the progress of each system on the floor so that problems can be resolved as they arise. in order to perform these tasks  ptrains must be able to construct plans and then modify these plans appropriately as unforeseen circumstances arise. the initial version of ptrans began to be tested al digital in july  1  and is the version discussed in this paper. 
all three of these systems are implemented in ops1  forgy 1 . ops1 is a general purpose rule-based language; like other rule based languages  ops1 provides a rule memory  a global working memory  and an interpreter which tests the rules to determine which ones are satisfied by the descriptions in working memory. an ops1 rule is an if-then statement consisting of a set of patterns which can be matched by the descriptions in working memory and a set of actions which modify working memory when the rule is applied. on each cycle  the interpreter selects a satisfied rule and applies it. since applying a rule results in changes to working memory  different subsets of rules are satisfied on successive cycles. ops1 does not impose any organization on rule memory; all rules are evaluated on every cycle. 
1 . the compartmentalizability of knowledge rts 1 subtasks conform quite closely to the 1 subtasks human configuration experts identify. r1 has about 1 rules distributed among these 1 subtasks. on the average  then  there are 1 rules  pieces of domain knowledge  associated with each subtask; though there is some variation in the amount of knowledge associated with each subtask  for most of the subtasks there are between 1 and 1  relevant rules  and no subtask has more than 1 rules associated with it. typically 1 or 1 of the rules associated with each subtask recognize when other subtasks must be performed and enable those subtasks by depositing the subtask names in working memory. almost none of rts knowledge is relevant to more than one subtask. 
xsel has about 1 rules distributed among 1 subtasks. 
all but about 1 of its rules are relevant to only a single subtask; thus  on the average  there are about 1 rules associated with each subtask. at any given time  typically 1 of the 1 rules relevant to more than one subtask are potentially relevant. thus the total number of ru'es potentially relevant at any given time is  on the average  about 1. 
the tasks ptrans assists with are performed simultaneously by a number of different people. the responsibility of some of these agents is to insure that the information other agents use is as accurate as possible. since these monitoring tasks are not associated exclusively with any particular part of the process  their relevance comes and goes as the various tasks which rely on the monitoring tasks come and go. f trans has about 1 rules approximately 1 of these are distributed among 1 subtasks; thus there are about 1 rules associated with each subtask. at any given time. 1 of the 1 rules relevant to more than one subtask are potentially relevant. thus the total number of rules potentially relevant at any given time is  on the average  about 1. the 1 demon rules associated with each task are not easily compartmentalized. the mean number of demons that any two subtasks have in common is 1 and the mode is 1  a few subtasks have no demons in common and a lew have more than 1 in common. thus whatever structure might characterize the interrelationships among the demons  it appears to have nothing to do with the task structure. we have had little success so far in identifying any organizational principles at all. 
one might expect in tasks like the configuration task  which have a fairly well-defined structure  that only a relatively small amount of knowledge would be potentially relevant at any given time. but the data for all three systems appears to support the following hypothesis: 
h1: only a relatively small amount of an expert's knowledge is potentially relevant in any given situation. 
1 j. mcdermott 
the intriguing question is to what extent this is true of more general problem solvers. it would seem plausible that in domains in which there is not a clear focus on a small set of issues  ie  in which there are not strong expectations about what sot of events might occur   much more knowledge would be potentially relevant. what is not at all clear is how much more and whether there is anything at all in common between the sort of compartmentalization of knowledge that we find in expert systems and that which we could expect to find in general problem solvers. 
since the only system with a significant amount of knowledge not associated with specific subtasks is ptrans and since the knowledge it has that is not associated with specific subtasks appears to be structureless  a possible hypothesis is: 
h1: subtasks serve as the dominant organizing principle for an expert's knowledge. 
much of the knowledge that an expert has appears to be relevant in the context of only a single subtask. because subtasks often form a 'hierarchy  this is not quite as restrictive as it may sound; knowledge that is relevant in several subtasks will be associated with a higher level  subsuming subtask. when a piece of knowledge is useful in a variety of subtasks that have no common ancestor  there is no obvious way to characterize the relationship among those uses. 
1. the uncertainty of information 
decause the configuration task as originally defined for rl did not include direct inter action with a salesperson or customer  the only data available to rl was that provided at the beginning of the task. this input consists of a list of quantity/component name pairs and sometimes other information describing customer specific configuration requirements. the major uncertainty associated with the data is whether the set of specified components are orderable  play together  and are complete; and these uncertainties are precisely those which the task is there to resolve. any changes to the order  or additional information which a salesperson or customer might want to provide  occur after the task has been performed; such changes define a new configuration task  rather than reflecting uncertainty in the data for the initial task. 
whether xsel is providing assistance with component selection or with floor layout  it asks the user for some initial information. in the case of component selection  the information consists  possibly of a mixture  of direct and indirect measures of the customer's computing needs together with an indication of how much the customer is willing to pay to satisfy those needs. in the case of floor layout  the information consists of descriptions of the rooms which will house the system and possibly customer p