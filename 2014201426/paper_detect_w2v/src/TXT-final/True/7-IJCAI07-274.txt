
question classification  i.e.  putting the questions into several semantic categories  is very important for question answering. this paper introduces a new application of using subtree mining for question classification problem. first  we formulate this problem as classifying a tree to a certain label among a set of labels. we then present a use of subtrees in the forest created by the training data to the tree classification problem in which maximum entropy and a boosting model are used as classifiers. experiments on standard question classification data show that the uses of subtrees along with either maximum entropy or boosting models are promising. the results indicate that our method achieves a comparable or even better performance than kernel methods and also improves testing efficiency.
1 introduction
what a current information retrieval system can do is just  document retrieval   i.e.  given some keywords it only returns the relevant documents that contain the keywords.
모however  what a user really wants is often a precise answer to a question. for instance  given the question  who was the first american in space    what a user really wants is the answer  alan shepard   but not to read through lots of documents that contain the words  first    american  and  space  etc.
모in order to correctly answer a question  usually one needs to understand what the question was asked. question classification can not only impose some constraints on the plausible answers but also suggest different processing strategies. for instance  if the system understands that the question  who was the first american in space   asks for a person name  the search space of plausible answers will be significantly reduced. in fact  almost all the open-domain question answering systems include a question classification module. the accuracy of question classification is very important to the overall performance of the question answering system.
모question classification is a little different from document classification  because a question contains a small number of words  while a document can have a large number of words.
thus  common words like  what    else   etc. are considered to be  stop-words  and omitted as a dimension reduction step in the process of creating features in document. however  these words are very important for question classification. also  word frequencies play an important role in document classification while those frequencies are usually equal to 1 in a question  thus  they do not significantly contribute to the performance of classification. furthermore  the shortness of question makes the feature space sparse  and thus  makes the feature selection problem more difficult and different from document classification.
모there are two main approaches for question classification. the first approach does question classification using handcrafted rules which are reported in  voorhees  1  voorhees  1  voorhees  1 . although it is possible to manually write some heuristic rules for the task  it usually requires tremendous amount of tedious work. one has to correctly figure out various forms of each specific type of questions in order to achieve reasonable accuracy for a manually constructed classification program.
모the second approach applies machine learning to question classification problems such as in  radev et al  1   the machine learning tool ripper has been utilized for this problem. the author defined 1 question categories  trained and tested their question classification on trec dataset. the reported accuracy of their system is 1%  which is not high enough for question classification. li  li and roth  1  presented the use of snow method in which the good performance of their system depends on the feature called  relwords   related word  which are constructed semi-automatically but not automatically. the author also reported several useful features for the question classification task including words  parts of speech  chunking labels  and named entity labels. in addition  they indicated that hierarchical classification methods is natural to the question classification problem. a question is classified by two classifiers  the first one classifies it into a coarse category  the second classifies it into a fine category.
모zhang and lee zhang and lee  1  employed tree kernel with a svm classifier and indicated that the syntactic information as well as tree kernel is suitable for this problem. however  the authors showed the performance only for the coarse category  and there was no report on classifying questions to fine categories.
모on the other hand  the connection of data mining techniques with machine learning problem recently shows that data mining can help improve the performance of classifier. morhishita  morhishita  1  showed that the use of association rule mining can be helpful for enhancing the accuracy of the boosting algorithm. berzal et al also  f. berzal et al   1  presented a family of decision list based on association rules mined from training data and reported a good result in the uci data set  li and roth  1 .
모mining all subtrees can be helpful for re-ranking in the syntactic parsing problem  kudo et al  1  as well as ensemble learning in semantic parsing  nguyen et al  1 . along with the success  the natural question is whether or not subtree mining can be helpful for question classification problem  this paperinvestigatesthe connectionof subtree mining to the maximum entropy and the boosting model. we formulate the question classification as a tree classification problem. we then present an efficient method for utilizing subtrees from the forests created by the training data in the maximum entropy model  subtree-mem  and a boosting model
 subtree-boost  for the tree classification problem.
모the remainder of this paper is organized as follows: section 1 formulates the tree classification problem in which a maximum entropy model and a boosting model using subtree features are presented. section 1 discusses efficient subtree mining methods. section 1 shows experimental results and section 1 gives some conclusions and plans for future work.
1 classifier for trees
assume that each sentence in a question is parsed into a tree. a tree can be a sequence of word  a dependency tree  and a syntactic tree.
모the problemof question classification is equivalentto classifying a syntactic tree into a set of given categories.
1 definition
the tree classification problem is to induce a mapping f x  :
x 뫸 {1 ... k}  from given training examples t =  is a labeled ordered tree and
yi 뫍 {1 ... k} is a class label associated with each training data. the important characteristic is that the input example xi is represented not as a numerical feature vector  bag-ofwords  but a labeled ordered tree.
모a labeled ordered tree is a tree where each node is associated with a label and is ordered among its siblings  that is  there are a first child  second child  third child  etc.
모let t and u be labeled ordered trees. we say that t matches u  or t is a subtree of u  t   u    if there exists a one-to-one function 뷍 from nodes in t to u  satisfying the conditions:  1 뷍 preserves the parent-daughter relation   1 뷍 preserves the sibling relation   1 뷍 preserves the labels.
모let t and x be labeled ordered trees  and y be a class label  yi 뫍 {1 ... k}   a decision stump classifier for trees is given by:
1t   x
1otherwise
모figure 1 shows an example of a labeled ordered tree and its subtree and non-subtree.

figure 1: labeled ordered tree and subtree relation
모decision stump functions are observed on the training data  the set of trees and their labels   then these functions are incorporated to a maximum entropy model and a boosting model.
1 boosting with subtree features
the decision stumps classifiers for trees are too inaccurate to be applied to real applications  since the final decision relies on the existence of a single tree. however  accuracies can be boosted by the boosting algorithm schapire  1  boosting repeatedly calls a given weak learner to finally produce hypothesis f  which is a linear combination of k hypotheses produced by the prior weak learners  i e.:

모a weak learner is built at each iteration k with different distributions or weights    where
 . the weights are calculated in such a
way that hard examples are focused on more than easier examples.

모there exist many boosting algorithm variants  however  the original and the best known algorithm is adaboost schapire  1 . for this reason we used the adaboost algorithm with the decision stumps serving as weak functions.
1 maximum entropy model with subtree features
maximum entropy models  berger et al  1   do not make unnecessary independence assumptions. therefore  we are able to optimally integrate together whatever sources of knowledge we believe to be potentially useful to the transformation task within the maximum entropy model. the maximum entropy model will be able to exploit those features which are beneficial and effectively ignore those that are irrelevant. we model the probability of a class c given a vector of features x according to the me formulation:
		 1 
모here zx is the normalization constant  fi c x  is a feature function which maps each class and vector element to a binary feature  n is the total number of features  and 뷂i is a weight for a given feature function. these weights 뷂i for features fi c x  are estimated by using an optimization technique such as the l-bfgs algorithm  liu and nocedal  1 . in the current version of maximum entropy model using subtrees  we define each feature function fi c x  using subtrees x and a class label c. the value of a feature function is received as +1 if  c x  is observed in the training data. otherwise  it is received 1. under the current framework of maximum entropy model  all subtrees mined are incorporated to the mem as these feature functions mentioned above.
1 efficient subtree mining
our goal is to use subtrees efficiently in the mem or boosting models. since the number of subtrees within the corpus is large a method for generating all subtrees in the corpus seems intractable.
모fortunately  zaki  zaki  1  proposed an efficient method  rightmost-extension  to enumerate all subtrees from a given tree. first  the algorithm starts with a set of trees consisting of single nodes  and then expands a given tree of size  k .. 1  by attaching a new node to this tree to obtain trees of size k. however  it would be inefficient to expand nodes at arbitrary positions of the tree  as duplicated enumeration is inevitable. the algorithm  rightmost extension  avoids such duplicated enumerations by restricting the position of attachment. to mine subtrees we used the parameters bellow:
  minsup: the minimum frequency of a subtree in the data
  maxpt: the maximum depth of a subtree
  minpt: the minimum depth of a subtree
table 1 shows an example of mining results.
table 1: subtrees mined from the corpus
frequencysubtree1 vp vbzfeatures  pp  1 vp vp vbnconsidered   1 vp vp vbncredited   1 vp vp vbndealt   1 vp vp vbninvented   모the subtree mining using the right most extension is used in both maximum entropy model and boosting model. the following subsection will describe how these subtrees can be used in mem and boosting.
1 subtree selection for boosting
the subtree selection algorithm for boosting is based on the decision stump function. we will eliminate a subtree if its gain value is not appropriate for the boosting process. we borrow the definition from  kudo et al   1  kudo et al  1  as shown bellow:
let t = {  x1 y1 d1   ...   xl yl dl  } be training
data  where xi is a tree and yi is a labeled associated with xi and di is a normalized weight assigned to xi. given t find the optimal rule   t1 y1   that maximizes the gain value.
모the most naive and exhaustive method  in which we first enumerate all subtrees f and then calculate the gains for all subtrees  is usually impractical  since the number of subtrees is exponential to its size. the method to find the optimal rule can be modeled as a variant of the branch-and-bound algorithm  and is summarized in the following strategies:
  define a canonical search space in which a whole set of subtrees of a set of trees can be enumerated.
  find the optimal rule by traversing this search space.
  prune search space by proposing a criterion with respect to the upper bound of the gain.
모in order to define a canonical search space  we applied the efficient mining subtree method as described in  zaki  1 . after that  we used the branch-and-boundalgorithm in which we defined the upper bound for each gain function in the process of adding a new subtree. to define a bound for each subtree  we based our calculation on the following theorem
 morhishita  1 
모for any  the gain of is bounded by 뷃 t 

모note that this equation is only used for binary classification. to adapt it to the multi classification problem one can use some common strategies such as one-vs-all  one-vs-one  and error correcting code to consider the multi classification problem as the combination of several binary classifications. in this paper we focus on the use of one-vs-all for multi-class problems. hence the subtrees can be selected by using the bound mentioned above along with binary classification using boosting.
모we can efficiently prune the search space spanned by right most extension using the upper bound of gain 뷃 t . during the traverse of the subtree lattice built by the recursive process of rightmost extension  we always maintain the temporally suboptimal gain 붻 among all gains calculated previously. if 뷃 t    delta   the gain of any super-treeis no greater than delta   and therefore we can safely prune the search space spanned from the subtree t. otherwise  we can not prune this branch. the algorithm for selecting subtrees is listed in algorithm 1.
모note that |t| means the length of a given subtree t. in addition  algorithm1 is presented as in an algorithmfork classes but in the current work we applied it to the binary class problem.
모after training the model using subtrees  we used the onevs-all strategy to obtain a label for a given question.
1 subtree selection for mem
count cut-off method
the subtree selection for maximum entropy model is conducted based on a right most extension way. each subtree is algorithm 1: find optimal rule
mined using the algorithm described in  zaki  1  in which a frequency  a maximum length  and minimum length of a subtree is used to select subtrees. the frequency of a subtree can suitably be used for the count cut-off feature selection method in maximum entropy models. under the current framework of maximum entropy model we used only the simple feature selection method using count cut off method. however  the results we gained in question classification are promising.
using boosting
in order to find effective subtree features for maximum entropy model  we applied the subtree boosting selection as presented in the previous section. we obtained all subtree features after the running of the subtrees boosting algorithm for performing on data set of one vs all strategy. in fact we obtained 1 data set for 1 class labels in the question classification data. these subtree features after applying the boosting tree algorithm in each data set will be combined to our set features for maximum entropy models. the advantage of using the boosting subtrees with maximum entropy models is that we can find an optimization combination of subtree features under the maximum entropy principle using the efficient optimization algorithm such as the l-bfgs algorithm  liu and nocedal  1 . in my opinion  the combination of subtree features using maximu entropy models might better than the linear combination as the mechanism of adaboost.
1 experimental results
the experimentswere conducted on a pentium iv at 1 ghz. we tested the proposed models on the standard data similarly experiments in previous work which was collected from four sources: 1 english questions published by usc  about 1 manually constructed questions for a few rare classes  trec 1 and trec 1 questions and also 1 questions from trec 1. the standard data is used by almost previous work on question classification  zhang and lee  1  li and roth  1 . the labels of each question is followed the twolayered question taxonomy proposed by  li and roth  1   which contains 1 coarse grained category and 1 fine grained categories  as shown bellow. each coarse grained category contains a non-overlapping set of fine grained categories.
  abbrs : abbreviation  expansion
  desc : definition  description  manner  reason
  enty : animal  body  color  creation  currency  disease/medical  event  food  instrument  language  letter  other  plant  product  religion  sport  substance  symbol  technique  term  vehicle  word
  hum: description  group  individual  title
  loc: city  country  mountain  other  state
  num: code  count  date  distance  money  order  other  percent  period  speed  temperature  size  weight
모in order to obtain subtrees for our proposed methods  we initially used the chaniak parser  e. charniak  1  to parse all sentences within the training and testing data. we obtained a new training data consisting of a set of trees along with their labels. we then mined subtrees using the right most extension algorithm  zaki  1 . table 1 shows a running example of using maximum entropy models with subtrees. to train our maximum entropy model we used the l-bfgs algorithm  liu and nocedal  1  with the gaussian for smoothing.
table 1: a running example of using maximum entropy model with subtrees
featuresweightenty  adjp advp  1enty  adjp inof  1enty  adjp jjcelebrated  1hum  adjp jjcommon  1hum  adjp jjdead  1enty:substance  np adjp rbsmost  jjcommon   1num:count  adjp pp infor   1num:count  np dtthe  nnnumber  1모to evaluate the proposed methods in question classification  we used the evaluation method presented in  li and roth  1  zhang and lee  1  as the formula below.
#of correctpredictions
accuracy = 
#of predictions
we conducted the following experiments to confirm our advantage of using subtree mining for classification with the maximum entropy model and boosting model. the experiments were done using significant test with 1% confident interval. table 1 shows the accuracy of our subtree maximum entropy model  st-mem   subtree boosting model  stboost   the combination of subtree boosting and maximum entropy st-mb   and the tree kernel svm  svm-tk  for the coarse grained category  respectively. it shows that the boosting model achieved competitive results to that of svm tree kernel. the st-mb outperforms the svm-tk and achieve the best accuracy.
table 1: question classification accuracy using subtrees  under the coarse grained category definition  total 1 labels 
st-memst-boostst-mbsvm-tk1111table 1: question classification accuracy using subtrees  under the fine grained category definition  total 1 labels .
st-memst-boostst-mbmemsvm11111모table 1 shows the performance of subtree-mem  subtreeboost  and subtree-mb for question classification. it also shows the result of using word features for maximum entropy models  mem  and support vector machine models  svm . the result of using svm tree kernel is not reported in the original paper  zhang and lee  1 . the author did not report the performance of svm using tree kernel on the fine grained category. now on our paper we reported the performance of fine grained category using subtrees under the maximum entropy and the boosting model. we see that without using semantic features such as relation words we can obtain a better result in comparison with previous work. table 1 indicates that the subtree-mb obtained the highest accuracy in comparison with other methods since it utilizes both the advantage of subtree features selection using boosting and the maximum entropy principle.
모since the subtree information and other semantic features such as the named entity types and the relation words do not overlap  mixing these features together might improve the performance of question classification task.
모the results also indicate that boosting model outperformed maximum model using subtrees and the computational time of mem is faster than that of boosting model.
모the training time of maximum entropy models is approximately 1 times faster than the boosting model. we needed approximately 1 hours to finish all the training process of boosting models for 1 training data examples. while using the maximum entropy model we needed only approximately 1 hours. the computational times of combination subtree boosting features and maximum entropy models is comparable to the subtree boosting model.
모the testing using mem is also approximately 1 times faster in comparison with that of boosting model. we guess that the reason is the boosting model had to find optimal rules during the training process. in addition  the larger number of classes  1 labels  might affect to the computational time because of the strategy of one-vs-all method.
모table 1 depicted the results of boosting model and maximum model using subtrees in which the minsup and maxpt mean the frequency and the maximum depth of a subtree  respectively. this table reported that there is a relation between the subtree mined parameters and the performance of table 1: question classification accuracy using subtrees  under the fine grained category definition  total 1 labels .
parameterssubtree-boostsubtree-memmaxpt=1 minsup=1.1.1maxpt=1 minsup=1.1.1maxpt=1 minsup=1.1.1the boosting and maximum entropy model. so  the mathematical formulation for this relation is worth investigating.
모the subtree mining approach to question classification relies on a parser to get the syntactic trees. however  most parsers  including the charniak's parser we used in the above experiments  are not targeted to parse questions. those parsers are usually trained on the penn treebank corpus  which is composed of manually labeled newswire text. therefore it is not surprising that those parsers can not achieve a high accuracy in parsing questions  because there are only very few questions in the training corpus. in  hermjakob  1   the accuracy of question parsing dramatically improves when complementing the penn treebank corpus  marcus et al  1  with an additional 1 labeled questions for training. we believe that a better parser is beneficial to our approach.
모summarily  all experiments show that the subtree mined from the corpus is very useful for the task of question classification. it also indicated that subtrees can be used as feature functions in maximum entropy model and weak function as in a boosting model. in addition  in the paper we give an alternative method for using subtrees in question classification. as the results showed in our experiment  we can conclude that mining subtrees is significantly contribution to the performance of question classification.
모in the paper  we investigate the use of syntactic tree representation which do not overlap the named entity types and the relation words  so other tree structure representation with the combination of syntactic and semantic information be beneficial to our approach. in future  we would like to investigate the use of semantic parsing such as semantic role labelling to the task.
1 conclusions
this paper proposes a method which allows incorporating subtrees mined from training data to the question classification problem. we formulate the question classification as the problem of classifying a tree into a prefixed categories. we then proposed the use of maximum entropy and booting model with subtrees as feature and weak functions by considering subtree mining as subtree feature selection process.
모experimental results show that our boosting model achieved a high accuracy in comparison to previous work without using semantic features. the boosting model outperformed the maximum entropy model using subtrees but its computational times is slower more than 1 times in comparison with that of the proposed maximum model. in addition  the combination of using boosting and maximum entropy models with subtree features achieved substantially better results in comparison with other methods in term of either accuracy or computational times performance.
모future work will also be focused on extending our method to a version of using semi-supervised learning that can efficiently be learnt by using labeled and unlabeled data. we also plan to exploit the use of hierarchical classification model and wordnet semantic information as described in li and roth  1  li and roth  1 .
모furthermore  we hope to extend this work to support interactive question answering. in this task  the question answering system could be able to interact with users to lead to more variations of questions but with more contextual information. our subtree mining method does not depend on the task of question classification. so  we believe that our method is suitable for other tasks  i.e text classification  text summarization  etc  where its data can be represented as a tree structure.
acknowledgments
we would like to thank to anonymous reviewers for helpful discussions and comments on the manuscript. thanks to the cognitive computation group at uiuc for opening their datasets. thank to mary ann m at jaist for correcting grammatical errors in the paper.
모the work on this paper was supported by the jaist 1 century coe program  verifiable and evolvable e-society .
