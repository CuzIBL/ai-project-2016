 
an ensemble is a classifier created by combining the predictions of multiple component classifiers. we present a new method for combining classifiers into an ensemble based on a simple estimation of each classifier's competence. the classifiers are grouped into an ordered list where each classifier has a corresponding threshold. to classify an example  the first classifier on the list is consulted and if that classifier's confidence for predicting the example is above the classifier's threshold  then that classifier's prediction is used. otherwise  the next classifier and its threshold is consulted and so on. if none of the classifiers predicts the example above its confidence threshold then the class of the example is predicted by averaging all of the component classifier predictions. the key to this method is the selection of the confidence threshold for each classifier. we have implemented this method in a system called sequel which has been applied to the task of recognizing volcanos in sar images of venus. in this domain  sequel outperforms each individual classifier as well as the simple approach of using an ensemble constructed from the average prediction of all the classifiers. 
1 introduction 
a popular method for creating an accurate classifier from a set of training data is to train several different classifiers on the training data  and then to combine the predictions of these classifiers into a single prediction breiman  1; drucker et a/.  1; wolpert  1 . the resulting classifier is generally referred to as an ensemble because it is made up of component classifiers. in this paper we present a novel method for combining the predictions of several classifiers. for each classifier we calculate a prediction threshold and a confidence factor associated with that threshold. our approach orders the 
   * current address: department of computer and systems sciences  stockholm university  sweden. 
1 	learning 
richard maclin department of computer science university of minnesota duluth  minnesota 1 
classifiers by decreasing confidence. to make a prediction  the most confident classifier is consulted and if its prediction is above its threshold  it is considered competent to make a prediction  then that prediction is used  otherwise the next most confident classifier is consulted  then the third most confident classifier  and so on. if no classifier is competent then the average of all of the classifiers' predictions is used. 
　to test our approach we developed a system  referred to as sequel  sequence learner   which we have applied to the difficult task of detecting volcanos from radar images of venus. in our experiments we demonstrate that our approach generally performs as well or better than any of its component classifiers and outperforms the standard ensemble approach of simply averaging the predictions of the component classifiers. 
1 background 
a number of researchers have demonstrated that ensembles are generally more accurate than any of their component classifiers  breiman  1; clemen  1; quinlan  1; wolpert  1; zhang et a/.  1 . figure 1 shows a basic framework for combining classifiers. using an ensemble  the class of an example is predicted by first classifying the example with each of the component classifiers and then combining the resulting predictions into a single classification. to create an ensemble  a user generally must focus on two aspects:  1  which classifiers to use as components of the ensemble; and  1  how to combine their individual predictions into one. hansen and salamon  demonstrated that un-

	figure 1: 	basic framework for combining classifiers. 

der certain assumptions  the accuracy of an ensemble increases with the number of classifiers combined. for each example where the average error rate is less than 1% for the distribution of possible classifiers  they show that in the limit the expected error on that example can be reduced to zero. of course  since not all patterns will necessarily share this characteristic  e.g.  outliers may be predicted at more than 1% error   the error rate over all the patterns cannot necessarily be reduced to zero. but if we assume a significant percentage of the patterns are predicted with less than 1% average error  gains in generalization will be achieved. a key assumption of hansen and salamon's analysis is that the classifiers combined should be independent in their production of errors. krogh and vedelsby  1  expanded on this notion to show that the error for an ensemble is related to the generalization error of the the classifiers plus how much disagreement there is between the different classifiers. 
　thus much research on selecting appropriate classifiers to combine has focused on selecting classifiers that are accurate in the predictions  but differ in where they are accurate. methods for approaching this problem include using different classification methods  training on subsets of the data set  training on different sets of input features  and using different subsets of the training set for training the classifiers  breiman  1; drucker et akl.  
1; hansen and salamon  1; hashem et al.  1; krogh and vedelsby  1; maclin and shavlik  1 . 
　in the present application  sequel combined classifiers that were trained using different input features  see section 1 . 
　the second aspect of creating an ensemble is the choice of the function for combining the predictions of the component classifiers  kearns and seung  1 . examples of combination functions include voting schemes  hansen and salamon  1   simple averages  lincoln and skrzypek  1   weighted average schemes  perrone and cooper  1; rogova  1   and schemes for training combiners  rost and sander  1; wolpert  1; zhang et al.  1 . clemen  1  demonstrated that in the absence of knowledge concerning a specific problem  almost any reasonable method  including the simple ones such as voting or using a weighted average  will result in an effective ensemble. 
　sequel applies a sequence of classifiers starting with its  best  individual classifier. each classifier is allowed to operate only if its predictions are above a certain threshold  otherwise the decision is left to the remainder of the classifiers. if none of the classifiers can reach a competent decision  the default method of using the simple average of all the classifiers' predictions is applied  see section 1 . 
　sequel can also be thought of as being related to the hierarchical mixture of experts approach  jacobs et al  1; jordan and jacobs  1; nowlan and hin-
ton  1 . in a mixture of experts approach a group of sub-classifiers is trained so that each sub-classifier will become an  expert  on a different portion of the input space. our approach differs in that our training mechanism is simpler  each classifier simply trains on the entire problem  but as a result our component classifiers may have significant overlap in their expertise. on the other hand  in our approach we use the overall average to predict difficult cases  which may be better than trying to train an  expert  for these cases. 
　the method implemented in sequel has an intuitive justification and empirical results from real world datasets show considerable improvement relative to both any individual classifier as well as to an ensemble constructed from the simple average of all classifiers. 
1 the sequence learner 
sequel implements a method for combining the predictions of k classifiers trained on n examples. the method assumes that each classifier fk produces a probability estimate so that fk x  gives the probability that x is an instance of a target concept c. each classifier's threshold  is: the probability given to the negative example with the highest probability.1 a classifier is considered competent for an example x if 
　each classifier's confidence score is the number of positive training examples  divided by the total number of training examples in the region above the threshold. a classifier's score can be thought of as the probability of making a correct decision for those examples where the classifier is competent 
　the classifier with the highest confidence score will label some part of the training data as  sure instances of concept c   the part of the training data that has been given a probability of at least . to determine the next classifier in the sequence all of the  sure  examples labeled by the first classifier are removed and the classifier with the highest confidence score for the remaining examples is chosen as the most confident classifier. this process is then repeated until the best classifier's confidence score is less than a predetermined threshold  we are currently using a threshold of 1 . 
　the output from this method is a list of classifiers and their corresponding thresholds to be applied in order  

and a default method to apply on samples that cannot be labeled by the sequence. in our experiments  the default method is the simple average of all the classifiers. 
　for each new sample  apply the the first classifier in the sequence and see if it gives a probability that lies above its threshold. if so  assign that probability to the sample  otherwise leave it to the second classifier. if the second classifier gives the sample a probability that lies above its threshold then multiply that probability with the first classifier's threshold and let the product 
     1  this holds under the condition that there exists a positive example with higher probability  otherwise the threshold will be equal to the probability given to the positive example with the highest probability. 
	asker & maclin 	1 

figure 1: sample of part of an image from the magellan sar image set that contains a number of small volcanos. 
be the output of the combined classifier. if the second best classifier cannot decide  give it to the third best and so on until there are no more classifiers in the sequence. it is important that the ordering between the classifiers is maintained. therefore  prediction f* x  that is produced by the best classifier  with index kx  is multiplied by the product of the thresholds of all previous classifiers. 

this maintains the ordering but has the effect that we can no longer think of the numbers as probabilities. 
1 the domain 
sequel has been applied to the problem of identifying small volcanos in sar  synthetic aperture radar  images of venus collected by the magellan spacecraft  saunders  1 . this problem is of interest because it is important scientifically and because the huge volume of data and the high dimensionality of the data  images  make this problem very difficult. the use of an ensemble of different classifiers is further motivated by the fact that no existing system has yet been able to do a satisfactory mapping of the approximately 1 small volcanos in the images. 
　venus is of special interest to scientists because in geological terms venus is very similar to earth  so a better understanding of venus will provide us with information about earth. volcanos are interesting because they are a widespread feature of the surface of venus and because they are detectable in the magellan images. volcanos are also interesting  because if a mapping of all of the volcanos covering the surface of venus existed  scientists would be able to infer geological properties from the number  clustering  size  etc. of the volcanos. 
　the magellan spacecraft transmitted back to earth approximately 1 sar  synthetic aperture radar  
1 	learning 

figure 1: example volcanos from 1 different clusters  right  and their respective cluster centers  left . each row represents a sample of volcanos that have been clustered together using k-means. 
images covering 1% of the venusian surface. this produced more data than had been produced from all previous space probes combined. the sar images are 1 by 1 pixel images with a resolution of 1m per pixel. figure 1 contains a sample of part of an image from the 
magellan sar dataset. the very volume of the data set makes the problem of labeling all of the volcanos on venus infeasible for humans. 
   even with computer classification  the amount of data in the dataset is extremely large. the jartool system  burl et a/.  1; 1  which was developed by jpl  is an automatic tool developed for this classification process. jartool is trained by first filtering the data in a pre-pass method called the focus of attention  foa . the foa is a simple method for selecting particular sized sub-areas  generally small squares  of the image based on their responses to a matched filter. this model has two positive effects:  1  it greatly reduces the number of data points to consider in later stages; and  1  it causes each candidate volcano to be roughly  centered  in the sub-image. the first effect is very important since even when a relatively small sub-image  say 1 pixels by 1 pixels  is used to recognize volcanos  the resulting sub-image still has a large number of features  1 pixel values in this case . the major disadvantage of using the foa model is that by pre-selecting a small number of sub-images from the original image the resulting set of sub-images may not include all of the volcanos labeled by the experts. 
   once the foa model has been applied the problem of volcano detection is one of determining which of the sub-regions of the image  also called regions of interest  returned by the foa actually contain volcanos. 
   the original jartool method controlled for the highdimensional space using the principal-component analysis method discussed below to extract a reduced set of features. after the dimensionality reduction step  the resulting features were then used to train a gaussian classification method  qda or quadratic discrimination analysis  to distinguish between actual volcanos and non volcanos. jartool's performance is comparable to that of the single best classifier shown in figures 1 and 1. 


figure 1: results from our experiments  the single best classifier and an ensemble constructed from the average of all classifiers on a set of 1 images. results are graphed by the number of misclassified non-volcanos allowed per km1. as the number of allowable misclassified non-volcanos increases the total percentage of actual volcanos increases. 

note that due to the use of the foa model the resulting classifier has an upper limit in its accuracy that is less than 1%  since some of the actual volcanos are left out. 
1 feature engineering 
to create a set of classifiers for our ensemble that perform differently on different portions of the data set we varied the set of features used to create the classifiers. 
　to produce a volcano detector our algorithm must be able to label a set of small images as being either volcanos or not volcanos. since these sub-images consist of a large number of pixels  the resulting input space has high dimensionality  and the set of possible features becomes immense. thus  all the component classifiers use principal component analysis  pca   fukunaga  1; jolloffe  1  to reduce the high dimensional feature space that the examples are drawn from. 
　pca has been widely used in statistical data analysis  image processing and pattern recognition. for example  turk and pentland used it for face recognition  1 . 
pca provides the highest eigenvalue eigenvectors of the data covariance matrix to be used as the new features. in this way the high-dimensional feature space can be projected down onto a more tractable sub-space of less dimensionality. 
　one weakness with the scheme proposed in jartool is the fact that it is based on the assumption that all volcanos look enough alike to be selected by a single filter in the foa step and to be classified by a single classifier. in practice  there exists a variety of subtypes of volcanos  each with its own visual characteristics. figure 1 shows some different types of volcanos. the volcanos in each row on the right hand side of the figure are taken from different clusters. instead of training one single classifier to distinguish between typical volcanos and non-volcanos  we trained a collection of different classifiers  each with its own particular set of features  and then created an ensemble of all the classifiers. in order to do so we first used k-means clustering to partition the volcanos in the training data into a number of clusters. the volcano images of each such cluster were then analyzed with principal component analysis to produce a set of features that best describe the volcanos of each cluster. 
1 experiments 
we started out by examining a set of 1 images that have been labeled and examined in previous work  burl et a/.  1 . we used these images as a means for eval-
uating which combinations of features and classification methods to use. these images contain 1 volcanos  1 of which are recognized by the foa model. the foa model also produces 1 sub-images that match the filter but are not volcanos. to produce classifiers for these images we divided the images up into six sets and performed six-fold cross-validation on each of these sets  i.e.  using the volcanos/non-volcanos of each set as a fold . 
　we began our study by focusing on selecting an initial feature representation. we then performed experiments on the preliminary set of 1 images varying the number of clusters  the size  the scaling of the sub-images  and the number of principal components used by the classi-
	asker & maclin 	1 


figure 1: results from sequel  the best single classifier and one simple ensemble on the set of new 1 images. 

fier. parameter sensitivity tests indicated the selection of the following combinations: 1 and 1 principal components  1 and 1 clusters  and a scaling factor  controlling the size of the sub-image  of 1  1  and 1. we also added an additional feature based on knowledge of the domain  a line filter value that notes the presence of lines in the image - these lines can easily distract the foa model. all in all this produced 1 different classifiers. 
　once we had chosen the different combinations of features and classifiers we intended to use in our ensemble based on experiments with the first dataset  we then tested our method on another set of 1 labeled images separate from the original set of 1 images. 
　for all of our results we show a curve with the percentage of detected true positives relative to the number of detected false positives per km1. this is done by successively lowering the threshold for predicting a volcano and determining how many true volcanos are included versus how many  false positive  volcanos are included  i.e.  as the threshold lowers more of the actual volcanos are included  but more  false positives  may appear . 
　sequel produced results which are shown in figure 1. the resulting classifier outperforms the original jartool method  burl et a/.  1; 1  and even outperforms any individual classifier and an ensemble constructed from the average of all classifiers. of course  these results are for a dataset where we have performed significant exploration to select input features  etc. so it is not surprising that we perform well. to test our method further we trained the same 1 classifiers on the original 1 images  created a sequence of classifiers  and applied it to the set of 1 new images. the results from these experiments are shown in figure 1. in this experiment too 
1 	learning 
 but more surprisingly   sequel outperforms the best single classifier  the simple average ensemble as well as the original jartool  jartools performance is equivalent to the best single participating classifier . 
1 conclusions 
we have presented a new method for creating an ensemble of classifiers that is based on an estimation of each classifier's competence. output from the method is a sequence consisting of either individual classifiers or combinations of classifiers together with a threshold for each member of the sequence. 
　the advantage of this approach is that we do not have to settle on a particular classification method  but can combine multiple methods to produce a classifier that outperforms any individual classification method. to produce our ensemble we made use of domain knowledge  in this case  a set of preliminary data  in forming our ensemble. sequel implements a function for combining the component classifiers that takes advantage of the abilities of each of the component classifiers. 
　the result of our approach is a method for combining classifiers in an ensemble that is simple and that outperforms any single participating classifier. furthermore  the constructed ensemble constitutes the best known classifier to date  for this difficult domain  an achievement worth mentioning  since several man-years have already been spent to produce a good classifier for this domain  burl et al  1; 1 . under these circumstances  even an increase in accuracy as little as 1 or 1 % can be considered a great success. 
a c k n o w l e d g m e n t s 
this work was performed by the jet propulsion laboratory  california institute of technology  under contract with the national aeronautics and space administration  nasa . the work was partially supported by a grant from the swedish research council for engineering sciences  l. asker  and by a nasa/asee summer faculty fellowship  r. maclin . we would like to thank 
m. burl  p. smyth  and u. fayyad for their assistance with the volcano dataset. we would also like to thank a. gray  j. roden  m. turmon  e. mjolsness  j. gratch  s. minton and d. wolpert for useful discussions that helped develop the ideas presented in this paper. 
