 
we report here on our experiments with post  part of 
speech tagger  to address problems of ambiguity and of understanding unknown words. part of speech tagging  per se  is a well understood problem. our paper reports experiments in three important areas: handling unknown words  limiting the size of the training set  and returning a set of the most likely tags for each word rather than a single tag. we describe the algorithms that we used and the specific results of our experiments on wall street journal articles and on muc terrorist messages. 
1. 	introduction1 
　natural language processing  and al in general  have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge. our own natural language database query systems  janus1  parlance tm 1 and delphi1  use these techniques quite successfully. however  as we move from the problem of understanding queries in fixed domains to processing open text for applications such as data extraction  we have found rule-based techniques too brittle  and the amount of work necessary to build them intractable  especially when attempting to use the same system on multiple domains. 
　we report in this paper on one application of probabilistic models to language processing  the assignment of part of speech to words in open text. the effectiveness of such models is well known  derose  1; church  1; kupiec  1; jelinek  1  and they are currently in use in parsers  e.g. de marcken  1 . our work is an incremental improvement on these models in two ways:  1  we have 
1
 	the work reported here was supported by the advanced 
research projects agency and was monitored by the rome air development center under contract no. f1-d-oo1. the views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies  whether expressed or implied  of the defense advanced research projects agency or the united states government. 
1
 weischedel  et al. 1. 
1
 parlance is a trademark of bbn systems and technologies. 
1
 stallard  1. 
1 	natural language 
run experiments regarding the amount of training data needed in moving to a new domain;  1  we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and  1  we have applied the forward-backward algorithm to accurately compute the most likely tag set. we describe post and its algorithms and then we describe our extensions  showing the results of our experiments. 
1. post: using probabilities to tag part of speech 
　predicting the part of speech of a word is one straightforward way to use probabilities. many words are several ways ambiguous  such as the following: 
around table: adjective a round of cheese: noun to round out your interests: verb to work the year round: adverb 
even in context  part of speech can be ambiguous  as in the famous example:  time flies.  where both words are two ways ambiguous  resulting in two grammatical interpretations as sentences. 
　models predicting part of speech can serve to cut down the search space a parser must consider in processing known words and make the selection among alternatives more accurate. furthermore  they can be used as one input to more complex strategies for inferring lexical and semantic information about unknown words. 
1 	the n-gram model 
　if we want to determine the most likely syntactic part of speech or tag for each word in a sentence  we can formulate a probabilistic tagging model. let us assume that we want to know the most likely tag sequence  t  given a particular word sequence  w. using bayes' rule we can write the a 
　posteriori probability of tag sequence t given word sequence 
as 
p wlt  = pft pfw  p w  
where p t  is the a priori probability of tag sequence t  p wit  is the conditional probability of word sequence w occurring given that a sequence of tags t occurred  and p w  is the unconditioned probability of word sequence w. then  in principle  we can consider all possible tag sequences  evaluate the a posteriori probability of each  and choose the one that is highest. since w is the same for all hypothesized tag sequences  we can disregard p w . 
　we can rewrite the probability of each sequence as a product of the conditional probabilities of each word or tag given all of the previous tags. 

　now  we can make the approximation that each tag depends only the immediately preceding tags  say the two preceding tags for a tri-tag model   and that the word depends only on the tag. 

that is  once we know the tag that will be used  we gain no further information about the likely word from knowing the previous tags or words. this model is called a markov model  and the assumption is frequently called the markov independence assumption. 
　if we have sufficient training data then we can estimate the tag n-gram sequence probabilities and the probability of each word given a tag  lexical probabilities . we use robust estimation techniques that take care of the cases of unobserved events  i.e. sequences of tags that have not occurred in the training data . however  in real-world problems  we also are likely to have words that were never observed at all in the training data. the model given above can still be used  simply by defining a generic new word called  unknown-word . the system can then guess at the tag of the unknown word primarily using the tag sequence probabilities. we return to the problem of unknown words in section 1. 
using a tagged corpus to train the model is called 
 supervised training   since a human has prepared the correct training data. we conducted supervised training to derive both a bi-tag and a tri-tag model based on a corpus from the university of pennsylvania. the upenn corpus  which was created as part of the treebank project  santorini  1  consists of wall street journal  wsj  articles. each word or punctuation mark has been tagged with one of 1 parts of speech1  as shown in the following example: 
terms/nns were/vbd not/rb disclosed/vbn  
　a bi-tag model predicts the relative likelihood of a particular tag given the preceding tag  e.g. how likely is the tag vbd on the second word in the above example  given that the previous word was tagged nns. a tri-tag model predicts the relative likelihood of a particular tag given the two preceding tags  e.g. how likely is the tag rb on the 
　of the 1 parts of speech  1 are word tags and 1 punctuation tags. of the word tags  1 are tags for open class words and 1 for closed class words. 
1  nns is plural noun; vbd is past tense verb; rb is adverbial; vbn is past participle verb. 
third word in the above example  given that the two previous words were tagged nns and vbd. while the bi-tag model is faster at processing time  the tri-tag model has a lower error rate. 
the algorithm for supervised training is straightforward. 
one counts for each possible pair of tags  the number of times that the pair was followed by each possible third tag  and then derives from those counts a probabilistic tri-tag model. one also estimates from the training data the conditional probability of each particular word given a 
known tag  e.g.  how likely is the word  terms  if the tag is nns ; this is called the  word emit  probability. the probabilities were padded to avoid setting the probability for unseen tri-tags or unseen word senses to zero. 
　given these probabilities  one can then find the most likely tag sequence for a given word sequence. using the viterbi algorithm  we selected the path whose overall probability was highest  and then took the tag predictions from that path. we replicated the earlier results that this process is able to predict the parts of speech with only a 1% error rate when the possible parts of speech of each the words in the corpus are known. this is in fact about the rate of discrepancies among human taggers on the treebank project  marcus et al.t 1 . 
1 	quantity of training data 
　while supervised training is shown here to be very effective  it requires a correctly tagged corpus. the perceived size of the corpus required affected de marcken's choice of a bi-tag rather than a tri-tag model  de marcken  1  we have done some experiments to quantify how much tagged data is really necessary. 
　in these experiments  we demonstrated that the training set can  in fact  be much smaller than might have been expected. one rule of thumb suggests that the training set needs to be large enough to contain on average 1 instances of each type of tag sequence in order for their probabilities to be estimated with reasonable accuracy. this would imply that a tri-tag model using 1 possible parts of speech would need a bit more than a million words of training. however  we found that much less training data was necessary. 
　it can be shown that if the average number of tokens of each tri-gram that has been observed is 1  then the lower bound on the probability of new tri-grams is 1. thus the likelihood of a new tri-gram is fairly low. 
　while theoretically the set of possible events is all permutations of the tags  in practice only a relatively small number of tri-lag sequences actually occur. out of about 1 possible triples  we found only 1 unique triples when we trained on 1 words  and about 1 when we trained on 1 1 words. thus  even though an additional 1 sequences are observed in the full training set  they are so rare  1%  that they do not significantly affect the overall accuracy. 
　in our initial experiments  which were limited to known words  the error rate for a supervised tri-tag model increased only from 1% to 1% when the size of the training set was reduced from 1 million words to 1 words. all that is really necessary  recalling the rule of thumb  is enough 
	meteer  schwartz  and weischedel 	1 

training to allow for 1 of each of the tag sequences that do occur. 
　this result is applicable to new tag sets  subdomains  or languages. we simply continue to increase the amount of training data until the number of training tokens is at least 1 times the number of different sequences observed so far. alternatively  we can stop when the singleton events account for a small enough percentage  say 1%  of the total data. thus  in applications such as tagging  where a significant number of the theoretically possible events do not occur in practice  we can use supervised training of probabilistic models without needing prohibitively large corpora.1 
1 . 	u n k n o w n w o r d s 
　sources of open-ended text  such as a newswire  present natural language processing technology with a major challenge: what to do with words the system has never seen before. current technology depends on handcrafted linguistic and domain knowledge. for instance  the system that performed most successfully in the evaluation of software to extract data from text at the 1nd message understanding conference held at the naval ocean systems center  june  1  would simply halt processing a sentence when a new word was encountered. 
　determining the part of speech of an unknown word can help the system to know how the word functions in the sentence  for instance  that it is a verb slating an action or stale of affairs  that it is a common noun stating a class of persons  places  or things  that it is a proper noun naming a particular person  place  or thing  etc. if it can do that well  then more precise classification and understanding is feasible. 
　using the upenn set of parts of speech  unknown words can be in any of the 1 open-class parts of speech. the tritag model can be used to estimate the most probable one. random choice among the 1 open classes would be expected to show an error rate for new words of 1%. the best previously reported error rale was 1%  kuhn & de mori  1 . 
　in our first tests using the tri-tag model we showed an error rate of only 1%. however  this model only took into account the context of the word  and no information about the word itself. in many languages  including english  the word endings give strong indicators of the part of speech. furthermore  capitalization information  when available  can help to indicate whether a word is a proper noun. 
' of course  performance of post is also affected by the estimates of p wi i ti  for known words and unknown words. how to estimate p wj i tj  for unknown words is covered in the next section. for an observed word  a small training set of 1 words may still be adequate for estimates of p wj i tj . 
we found that by treating words observed only once as if they had not been observed at all  and are thus handled by the probabilistic models for unknown words  that performance actually increased slightly. this suggests that adequate performance can be obtained from a relatively small training set. 
1 	natural language 
　we employed a probabilistic model that takes into account features of the word in determining the likelihood of the word given a part of speech. this was used instead of the  word emit  probabilities p wi 1ti  for known words. to estimate p wi iti  for an unknown word  we first determined the features we thought would distinguish parts of speech. there are four independent1 categories of features: inflectional endings  derivational endings  hyphenation  and capitalization. our initial test had three inflectional endings  -ed  -s  -ing   and 1 derivational endings   including -ion  i   .  .  . capitalization has four values  in our system  + initial + capitalized  - initial + capitalized  etc.  in order to take into account the first word of a sentence. we can incorporate these features of the word into the probability that this particular word will occur given a particular tag using 

　we estimate the probability of each ending for each tag directly from supervised training data. while these probabilities are not strictly independent  the approximation is good enough to make a marked difference in classification of unknown words. as the results in figure 1 shows  the use of the orthographic endings of the words reduces the error rate on the unknown words by a factor of 1. 

figure 1: decreasing error rate with use of word features 
　we tested capitalization separately  since some data  such as that in the third message understanding conference  is upper case only. titles and bibliographies will cause similar distortions in a system trained on mixed case and 
1  these are not necessarily independent  though we are treating them as such for our tests. 

using capitalization as a feature. interestingly  the capitalization feature contributed very little to the reduction in error rates  whereas using the word features contributed a great deal. 
　previous efforts  church  1; de marcken  1  have dealt with unknown words using various heuristics. for instance  church's program parts has a prepass prior to applying the tri-tag probability model that predicts proper nouns based on capitalization. the new aspects of our work are  1  incorporating the treatment of unknown words uniformly within the probability model   1  approximating the component probabilities for unknowns directly from the training data  and  1  measuring the contribution of the tritag model  of the ending  and of capitalization. 
　in sum  adding a probability model of typical endings of words to the tri-tag model has yielded an accuracy of 1% for unknown words. adding a model of capitalization to the other two models further increased the accuracy to 1%. the total effect of bbn's model has been a reduction of a factor of five in the error rate of the best previously reported performance. 
1. 	k-best tag sets 
　an alternative mode of running post is to return the set of most likely tags for each word  rather than a single tag for each. 
　in our first test  the system returned the sequence of most likely tags for the sentence. this has the advantage of eliminating ambiguity; however  even with a rather low error rate of 1%  there are cases in which the system returns the wrong tag  which can be fatal for a parsing system trying to deal with sentences averaging more than 1 words in length. 
　de marcken  developed an approximate method for finding multiple tags for each word given the preceding words and one following word. we addressed this problem by adding the ability of the tagger to return for each word an ordered list of tags  marked by their probability using the forward backward algorithm  baum & eagon  1 . that yields a more precise method of determining the probability of each possible tag since it sums over all possible tag sequences  taking into account the entire sentence and not just the preceding tags. the forward backward algorithm is normally used in unsupervised training to estimate the model that finds the maximum likelihood of the parameters of that model. the exact probability of a particular tag given a particular word is computed directly by the product of the  forward  and  backward  probabilities to that tag  divided by the probability of the word sequence given this model. 
　the following example shows k-best tagging output  with the correct tag for each word marked in bold. note that the probabilities are in natural log base e. thus for each difference of 1  there is a factor of 1 in the probability. 

	figure 1: 	k-best tags and probabilities 
　in two of the words   controls  and  computerized   the first tag is not the correct one. however  in all instances the correct tag is included in the set. note the first word   bailey   is unknown to the system  therefore  all of the open class tags are possible. 
　in order to reduce the ambiguity further  we tested various ways to limit how many tags were returned based on their probabilities. often one tag is very likely and the others  while possible  are given a low probability  as in the word  in  above. therefore  we tried removing all tags whose probability was less than some arbitrary threshold  similar to de marcken's  factor    for example removing all tags whose likelihood is more than e1 less likely than the most likely tag. so only tags within the threshold 1 of the most likely would be included  i.e. if the most likely tag had a log probability of -1  only tags with a log probability greater than -1 would be included . this reduced the ambiguity for known words from 1 tags per word to 1  and for unknown words  from 1 to 1. 
　however  the negative side of using cut offs is that the correct tag may be excluded. note that a cut off of 1 would exclude the correct tag for the word  controls  above. by changing the cut off to 1  we are sure to include all the correct tags in this example  but the ambiguity for known words raises from 1 to 1 and for unknown words from 1 to 1  for an ambiguity rating of 1 overall. 
　we are continuing experiments to determine the most effective way of limiting the number of tags returned  and hence decreasing ambiguity  while ensuring that the correct tag is likely to be in the set. balancing the tradeoff between ambiguity and accuracy is very dependent on the use the tagging will be put to  both on the component that the tagged text directly feeds into  such as a parser that can efficiently follow many parses  but cannot recover easily from errors vs. one capable of returning a partial parse  and on the application  such as an application requiring high accuracy  data base query  vs. one requiring high speed  processing newswire text as it comes in . 
	meteer  schwartz  and weischedel 	1 

1. 	moving to a new domain 
　in all of the tests discussed so far  we both trained and tested on sets of articles in the same domain  the wall street journal texts used in the penn treebank project. however  an important measure of the usefulness of the system is how well it performs in other domains. while we would not expect high performance in radically different kinds of text  such as transcriptions of conversations or technical manuals  we would hope for similar performance on newspaper articles from different sources and on other topics. 
we tested this hypothesis using data from the third 
message understanding conference  muc-1 . the goal of 
muc-1 is to extract data from texts on terrorism in latin 
american countries. the texts are mainly newspaper articles  although there are some transcriptions of interviews and speeches. the university of pennsylvania treebank project tagged four hundred muc messages  approximately 1 words   which we divided into 1% training and 1% testing. 
　for our first test  we used the original probability tables trained on the wall street journal articles. we then retrained the probabilities on the muc messages and ran a second test  with an average improvement of three percentage points in both bi- and tri- tags. the full results are shown below; 1% of the words in the test were unknown: 
bitags: 
overall error rate: 
number of correct tags: 
number of incorrect tags: 
error rate for known words: 
error rate for unknown words: 
trttags: 
overall error rate: 
number of correct tags: 
number of incorrect tags: 
error rate for known words: 
error rate for unknown words: 
figure 1: comparison of original and trained probabilities 
　while the results using the new tables are an improvement in these first-best tests  we saw the best results using k-best mode  which obtained a .1% error rate. we ran several tests using our k-best algorithm with various thresholds. as described in section 1  the threshold limits how many tags are returned based on their probabilities. while this reduces the ambiguity compared to considering all possibilities  it also increases the error rate. figure 1 shows this tradeoff from effectively no threshold  on the right hand side of the graph  shown in the figure as a threshold of 1   which has a .1% error rate and an ambiguity of 1  through a cut off of 1  which has a error rate of 1  but an ambiguity of nearly zero-i.e. one tag pre word.  note the far left of the graph is the error rate for a cut off of 1  that is  only considering the first of the k-best tags  which is approximately the same as the bi-tag error rate shown in figure 1.  


figure 1: comparison of thresholds for k- best 
1 	natural language 

1. u s i n g d i c t i o n a r i e s 	r e f e r e n c e s 

　in all of the results reported here  we are using word/part of speech tables derived from training  rather than on-line dictionaries to determine the possible tags for a given word. the advantage of the tables is that the training provides the probability of a word given a tag  whereas the dictionary makes no distinctions between common and uncommon uses of a word. the disadvantage of this is that uses of a word that did not occur in the training set will be unknown to the system. for example  in the training portion of the wsj corpus  the word  put  only occurred as verb. 
however  in our test set  it occurred as a noun in the compound  put option . since for efficiency reasons  we only consider those tags known to be possible for a word  this will cause an error. 
　we are currently integrating on-line dictionaries into the system  so that alternative word senses will be considered  while still not opening the set of tags considered for a known word to all open class tags. this will not completely eliminate the problem  since words are often used in novel ways  as in this example from a public radio plea for funds:  you can mastercard your pledge. . we will be rerunning the experiments reported here to evaluate the effect of using on-line dictionaries. 
1. f u t u r e d i r e c t i o n s 
　in the work reported here  we have evaluated post in the laboratory  evaluating its results against the work of people doing the same task. however  the real test of such a system is how well it functions as a component in a larger system. can it make a parser work faster and more accurately  can it help to extract certain kinds of phrases from unrestricted text  we are currently running these experiments by making post a part of existing systems. it is being run as a preprocessor to grishman's proteus system for the muc-1 competition  grishman & sterling  1 . preliminary results showed it sped up proteus by a factor of two in one-best mode and by a factor of 1% with a 
　threshold of t=1. it is also being integrated into a new message processing system at bbn. the results of these experiments will provide us with new directions and ideas both for improving post and for other ways to integrate probabilistic models into natural language processing systems. 
a c k n o w l e d g e m e n t s 
　we would like to acknowledge lance ramshaw for his work on post and the results on the size of the training set. we have also benefited from discussions with ken church. 
baum  l.e. and eagon  	j.a. an inequality with 
applications to statistical estimation for probabilistic 
functions of markov processes and to a model of ecology  amer. math soc. bulletin  1 :1  1. 
church  k. a. stochastic parts program and noun phrase 
parser for unrestricted text. proceedings of the second 
conference on applied natural language processing  pages 1. acl  1. 
de marcken  c.g. parsing the lob corpus. proceedings of the 1th annual meeting of the association for computational linguistics  pages 1. 1. 
derose  s.j. grammatical category disambiguation by statistical optimization. computational linguistics 1: 1  1. 
grishman  r.  and j. sterling. preference semantics for 
message understanding. proceedings of the speech and natural language workshop  pages 1  oct 1. 
jelinek  f. self organizing language modeling for speech recognition. unpublished technical report  1. ibm tj. watson research center  yorktown heights  n.y. 
kuhn  r.  and de mori  r.  a cache-based natural 
language model for speech recognition. ieee transactions on pattern analysis and machine intelligence 1  pages 1. 1. 
kupiec  j. augmenting a hidden markov model for phrasedependent word tagging. in proceedings of the speech and natural language workshop  pages 1. oct. 1. 
santorini  b. 	annotation manual for the penn treebank 
project. technical report. cis department. university of pennsylvania. may 1. 
stallard  d. unification-based semantic interpretation in the bbn spoken language system. in proceedings of the speech and natural language workshop  pages 1. oct. 1. 
marcus  m.  santorini  b. & magerman   first steps 
towards an annotated database of american english  in 
langendoen & marcus   readings for tagging linguistic information in a text corpus   tutorial for the 1th 
annual meeting of the association for computational linguistics. june 1. 
weischcdel  r.  bobrow  r.  ayuso  d.  and ramshaw  l. 
portability in the janus natural language interface  in 
	speech 	and natural 	language  	morgan 	kaufman 
publishers  inc. p. 1. oct. 1. 
	meteer. schwartz  and weischedel 	1 
