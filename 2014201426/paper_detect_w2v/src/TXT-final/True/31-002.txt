 
recent work regarding the statistics of natural images has revealed that the dominant eigenvectors of arbitrary natural images closely approximate various oriented derivative-ofgaussian functions; these functions have also been shown to provide the best fit to the receptive field profiles of cells in the primate striate cortex. we propose a scheme for expressioninvariant face recognition that employs a fixed set of these  natural  basis functions to generate multiscale iconic representations of human faces. using a fixed set of basis functions obviates the need for recomputing eigenvectors  a step that was necessary in some previous approaches employing principal component analysis  pca  for recognition  while at the same time retaining the redundancy-reducing properties of pca. a face is represented by a set of iconic representations automatically extracted from an input image. the description thus obtained is stored in a topographically-organized sparse distributed memory that is based on a model of human long-term memory first proposed by kanerva. we describe experimental results for an implementation of the method on a pipeline image processor that is capable of achieving near real-time recognition by exploiting the processor's frame-rate convolution capability for indexing purposes. 
1 	introduction 
the problem of object recognition has been a central subject in the field of computer vision. an especially interesting albeit difficult subproblem is that of recognizing human faces. in addition to the difficulties posed by changing viewing conditions  computational methods for face recognition have had to confront the fact that faces are complex non-rigid stimuli that defy easy geometric characterizations and form a dense cluster in the multidimensional space of input images. one of the 
　*this work is supported by nsf research grant no. cda1  nih/phs research grant no. 1 r1 rr1  and a grant from the human science frontiers program. 
1 	action and perception 
most important issues in face recognition has therefore been the representation of faces. early schemes for face recognition utilized geometrical representations; prominent features such as eyes  nose  mouth  and chin were detected and geometrical models of faces given by feature vectors whose dimensions  for instance  denoted the relative positions of the facial features were used for the purposes of recognition  bledsoe  1; kanade  1 . recently  researchers have reported successful results using photometric representations i.e. representations that are computed directly from the intensity values of the input image. some prominent examples include face representations based on biologically-motivated gabor filter  jets   buhmann et al.  1   randomly placed zeroth-order gaussian kernels  edelman et a/.  1   isodensity maps  nakamura et a/.  1   and principal component analysis  pca   turk and pentland  1; pentland et al.  1 . 
　this paper explores the use of an iconic representation of human faces that exploits the dimensionalityreducing properties of pca. however  unlike previous approaches employing pca for recognition  turk and pentland  1; murase and nayar  1   our approach uses a fixed set of basis functions that are learned during an initial  development  phase; the costly and time consuming step of having to recompute basis functions when new faces  or other objects  are encountered is thereby avoided. in addition  the basis functions used to generate face representations are based on pca of localized natural image patches at multiple scales rather than pca of 
entire face images at a single scale; the localized nature of the representation helps to make it tolerant to minor changes in facial expressions and partial occlusions while the multiscale structure allows strategies for scale invariance. 
　the iconic face representations are formed from ndimensional photometric feature vectors comprised of the responses of m derivative of gaussian basis functions at a range of orientations  each at k scales  n = mk ; for the experiments in the paper  nine basis filters at five scales were used generating a forty-five element response vector characterizing the local image region at a point of interest. we have previously shown these iconic feature vectors to be useful for active vision  rao and 
ballard  1a   visuomotor learning  rao and ballard  1b   and general object indexing  rao and ballard  1c . here  we show that such a representation may be used for the difficult problem of expression-invariant face recognition as well. 
　a face is represented by a collection of iconic feature vectors automatically extracted from specific locations in the input image  section 1 . a topographicallyorganized sparse distributed memory is used to learn the association between the appearance of a face as given by its feature vectors and the identity of the face  section 1 . implementation of the recognition scheme is achieved using a datacube mv1 pipeline image processor for both real-time visual preprocessing as well as indexing into the face database  utilizing frame-rate convolutions for distance computations  section 1 . we present preliminary results on the performance of the method on a face database of 1 images from 1 different persons exhibiting a range of facial expressions; a recognition rate of 1% was achieved by the method when a set of 1 points were used for characterizing a face. 
1 natural basis functions 
images of natural scenes  unlike random collections of pixels  are characterized by a high degree of statistical regularity. for instance  pixel values in a given neighborhood tend to be highly correlated owing to the morphological consistency of objects. thus  a pixel-wise representation of objects obtained from a camera is highly redundant and some form of redundancy reduction is desirable. 

in principle  all n   n1 eigenvectors are needed in order to completely represent the input image set but due to the statistics of natural images  it is usually the case that only a small number m of eigenvectors  m    n  account for almost all of the variance in the input data. thus  by using only the first m dominant eigenvectors as basis functions  or orthogonal axes  for projecting new inputs  considerable computational savings can be achieved. 
　turk and pentland  l1l  used pca to synthesize the eigenvectors   eigenfaces   of a training set of face images; they achieve recognition by using a templatematching strategy with the vectors obtained by projecting new face images along a small number of eigenfaces. murase and nayar  applied pca to the problem of object recognition and pose estimation; they represent objects as manifolds in the low-dimensional subspace   eigenspace   formed by the dominant eigenvectors of a set of training images and achieve recognition by finding the manifold that is closest to the projection of an input image in the eigenspace formed by all objects. 
1 	unsupervised learning of basis functions 
the methods of  turk and pentland  1  and  murase and nayar  1  both require recomputation of the eigenvectors when new faces/objects are encountered. it is therefore natural to ask what the results of pca would be if one were to take the above process to its limit i.e. to perform pca on a set j  ...  jn of arbitrary natural images containing a wide variety of natural and manmade stimuli. recently  hancock et al.  used a neural network introduced by sanger  to extract the first few eigenvectors of an ensemble of natural images. they discovered that the eigenvectors were very close approximations of different oriented derivative-ofgaussian operators. 
　we employed sanger's pca network to ascertain whether the results of hancock et al. remained true for collections of images containing equal proportions of natural and man-made stimuli. in particular  we used 1 x 1 gaussian-windowed image patches obtained by scanning across a number of arbitrary images of natural scenes  figure 1  a  . suppose / represents an input mean-centered image patch and wj represents the weight vector from the input layer  which in our case consists of 1 units  to the output unit j. sanger's pca network uses linear output units i.e. 


after 1 presentations. these nine eigenvectors alone account for as much as 1% of the input variance. it is clear that regardless of the scale of analysis  the eigenvectors closely approximate different oriented derivativeof-gaussian operators. 
　in summary  the derivative-of-gaussian filters are well-suited for use as natural basis functions for generalpurpose visual recognition because:  a  they are obtained as result of applying pca to arbitrary collections of images containing diverse elementary features from natural as well as man-made structures rather than 
just the images of particular objects or faces. in addition  correlation filters generated via pca have been shown to maximize signal-to-noise ratio and yield much sharper correlation peaks than traditional raw image cross-correlation techniques  kumar et a/.  1 ;  b  they form the class of real-valued functions that simultaneously minimize the product of the standard deviation of the spatial position sensitivity and spatial frequency sensitivity   gabor  1  p. 1; and  c  they are endorsed by neurobiological studies  young  1  which show that the different order derivative-ofgaussian functions provide the best fit to primate cortical receptive field profiles among the different functions suggested in the literature. 
1 iconic representations of faces 
the iconic representations used in our recognition scheme are based on the natural basis functions mentioned in the previous section. the exact number and type of gaussian derivative basis functions used is motivated by the need to make the representations invariant to rotations in the image plane. this invariance can be achieved by exploiting the property of steerability  freeman and adelson  1  and using a minimal basis set of two first-order directional derivatives at 1＜ and 1＜  three second-order derivatives at 1＜  1＜ and 1＜  and four third-order derivatives oriented at 1＜  1＜  1＜  and 1＜  figure 1  c  . we omit the zeroth order to reduce illumination dependence and do not use higher orders since variance of the higher-order filters can be expected to approach that of image noise as suggested by the results of pca. the use of the non-orthogonal oriented filters also obviates the use of mixed derivatives  some of which were obtained in figure 1  b   since the other oriented filters yield a complete basis. 
1 	representing image regions 
the current implementation uses nine gaussian directional derivatives denoted by 
where n denotes the order of the filter and 1n refers to the preferred orientation of the filter. the response of an image patch / centered at  xo yo  to a particular basis filter gtj can be obtained by convolving the image patch with the filter : 
  1  
　the iconic representation for the local image patch centered at  x1 yo  is formed by combining into a single high-dimensional vector the responses from the nine basis filters  each  in the current implementation  at five different scales: 
		 1  
where i = 1 1 denotes the order of the filter  j = 1 ...  i -i- 1 denotes the different filters per order  and s = smin ...  smax denotes the different scales  as given by the levels of a low-pass filtered image pyramid . the use of multiple scales increases the perspicuity of the representation and allows interpolation strategies for scale invariance  see  rao and ballard  1a  for more details . in addition  the high-dimensionality of the vectors makes them remarkably robust to noise due to the orthogonality inherent in high-dimensional spaces: given any vector  most of the other vectors in the space tend to be relatively uncorrelated with the given vector  rao and ballard  1c . 
　the iconic representations can be made invariant to rotations in the image plane  for a fixed scale  by exploiting the property of steerability  freeman and adel-

                                             son  1 . the current orientation is computed using 1the class of complex-valued functions that minimize this the first-order responses as: 
conjoint localization metric are the well-known gabor ele-
mentary functions  gabor  1 . 		 1  
1 	action and perception 

 1  
with j' - 1 ...  1. note that this normalization procedure does not apply to rotations in 1d. modest rotations in depth can be handled as noise by the representation but larger 1d rotations require the use of responses from multiple views as we have shown in  rao and ballard  1c . this view-based approach is similar in spirit to the one used by beymer   see also  pentland et a/.  1  . 
1 	representing faces 
the response vectors described in the previous section serve as iconic descriptions of individual image regions. in order to represent a given model face with a set of such vectors  the problem of selecting suitable points within a face from which model response vectors can be extracted must be addressed. in our current implementation  we apply the following simple strategy after the approximate boundary of the face is determined  by using  for instance  stereo and a technique such as zero disparity filtering  coombs  1  : 
   a face is represented by response vectors from the centroid of the face and each of the points lying on the intersections of radial lines with concentric circles of exponentially increasing radii centered on the centroid. 
for the task of face recognition using the iconic representations discussed above  two requirements need to be met:  a  a mode of long-term storage for the representations of model faces  and  b  a method for learning the association between the representation of a face and the identity of the face. both these requirements are met by using a modified form of sparse distributed memory  sdm   first proposed by kanerva  1; 1  as a model of human long-term memory. 
1 	operation of the sdm 
the organization of sparse distributed memory  figure 1  is similar to that of conventional random access memory. there exists an array of data storage locations  each identified by a number  the address of the location . however  the address vectors are usually high-dimensional and hence only a sparse subset of the address space is used for identifying data locations. suppose the sdm contains m storage locations where the address vectors are p-ary and n-dimensional  and the data vectors are derived from the set {-1  l}k. for our current purposes  the feature vectors describing faces correspond to the addresses of the sdm while the data being associated with the feature vectors represent the identities of the faces  each person being defined by a small interval of possible values. specific values of the 
	rao and ballard 	1 
data vector may be interpreted as corresponding to a particular facial expression of a person. 
organization of the address space 
kanerva suggests randomly picking m unique vectors 
ai from the pn possible address vectors for addressing each of the m data storage locations of the memory. however  in our case  the set of response vectors will be clustered in many correlated groups distributed over a large portion of the response vector space. therefore  if addresses are picked randomly  a large number of locations will never be activated while a number of locations will be selected so often that their contents will resemble noise. the solution is to pick addresses according to the distribution of the data  keeler  1 . in our case  we simply use an initial subset of the training response vectors as the addresses. when all address locations have subsequently been filled  the address space can be allowed to self-organize using the well-known competitive hebbian learning rule: given a new input vector f  the closest addresses ak are adapted according to 
		 1  
where n   1 is a gain term and g is a radial basis function  poggio and girosi  1  that weights the second summand according to the distance between r and ak. however  for the experiments in this paper  we did not employ the above self-organization rule. 
activation of data locations 
the distance between response vectors r1 and r1 is defined to be their normalized dot product  or correlation : 
		 1 
given a response vector r for indexing into the memory  all storage locations whose addresses lie within a distance of d from r are selected  the selected locations are indicated by ones in the vector s*in figure 1 . note that in general  an arbitrary radial basis function such as a gaussian may be used to obtain the components of s instead of a strict binary threshold function. this allows for smoother interpolation between stored data vectors especially when they are used to indicate facial expression in addition to identity. 
hebbian learning of identity 
during the training phase  the input response vector r is used to find s and the associated identity vector d is added to the previous contents of each of the selected storage locations. this corresponds to a form of generalized hebbian learning as pointed out in  keeler  1 . note that this is different from a conventional memory where addresses are required to exactly match for selection and previous contents are overwritten with new data. 
retrieval of identity 
after training  the memory can be used to yield hypotheses for the identity of the object given a response vector 
r. first  the locations selected for r are found as above and the values of these selected locations are added in 
1 	action and perception 

parallel  vector addition  to yield a sum vector 1 containing the k sums. these k sums are thresholded at 1 to obtain the data vector d i.e. di = 1 if si   1 and d't = -1 otherwise. 
　the statistically reconstructed data vector should closely resemble the original data vector  or some linear combination of the stored vectors in the case of interpolation between stored facial expressions  provided the capacity of the sdm  kanerva  1  has not been exceeded. the intuitive reason for this is as follows: when storing d using r  each of the selected locations receives one copy of the data. during retrieval with an address close to r  say r  most of the locations that were selected with r are also selected with r. thus  the sum vector contains most of the copies of d  plus copies of other different words; however  due to the orthogonality of the address space for large n  these extraneous copies are much fewer than the number of copies of d. this biases the sum vector in the direction of d and hence  d is output with a high probability  see  kanerva  1  for a more rigorous argument based on a signal-to-noise ratio analysis . 
1 	topographical organization of memory 
topology of the model points  as given by the concentric circular template  figure 1   is preserved by using separate sdms for storing vectors from each of the sparse number of locations on a face. the final output of the memory is obtained from the cumulative sum vector over the sdms for the different facial locations. this arrangement offers at least two advantages over using a single sdm for storing vectors as proposed earlier in  rao and ballard  1c :  a  the crosstalk between response vectors from different locations on a face is eliminated  and  b  a given response vector from a facial location needs to compared to only the model vectors in the sdm for that location  thereby speeding up the recognition pro-
1 	implementation 
the face recognition scheme described above has been implemented using an active vision system comprised of 

the university of rochester  ur  binocular head which 
 figure 1  provides input to a datacube maxvideo tm  mv1 pipeline image-processing system capable of performing convolutions at frame-rate  1/sec . the pitch of the two-eye platform is controlled by a single servo motor while separate motors control each camera's pan angle  thereby providing independent vergence control. this allows strategies for figure-ground segmentation of faces using  for instance  zero disparity filtering  coombs  1 ; once a face has been approximately segmented from the background  the concentric circular template of points can be centered on the centroid of the face. given an input face image  the mv1 executes nine convolutions with the different 1 gaussian derivative kernels on a low-pass filtered five-level pyramid of the input image; filter responses are then extracted from each of the sparse number of points whose coordinates are given by the concentric circular template. 

menting memory directly within the pipeline image pro- discriminability between the first face and the others; the us cessing system itself and using convolutions for distance of multiple vectors at other facial locations resolves furthe 　traditionally  the most time-consuming step during object indexing has been linearly accessing the large number of object representations in memory. however  our implementation greatly optimizes this step by implefigure 1: discrimination ability of the feature vet tors. the graph shows a plot of correlation between th vector from the approximate centroid  marked by '+'  of th first face and the corresponding vectors for the other face: in this case  a threshold d  for the sdm  of upto 1 retair 

computations. during indexing  an input response vector is loaded into the 1 convolution kernel and convolved with a  memory surface  containing the stored model vectors; the closest vectors can be selected by simply thresholding the results of the convolution. 
1 	experimental results 
the first experiment  figure 1  illustrates the discrimination ability of the feature vectors. the vector extracted from a point near the approximate centroid of a given face was compared with those for five other faces. it is clear that the five vectors are relatively uncorrected with the vector for the original face  the closest vector having a correlation of 1. for the sdms  thresholds d in the range 1-1 were found to yield satisfactory results. further discriminability is obtained by using more than one vector per face from different facial locations as previously discussed. 
　the next experiment examines the effect of varying facial expressions on the iconic feature vectors. figure 1 shows a set of face images of a person exhibiting a range of facial expressions. the correlation between the vectors for two different points on the neutral face image and the corresponding vectors for the other images is plotted below. the graph indicates that as expected  vectors for some facial points change much more than the others though the correlation still remains relatively high  above 1 . this motivates the need for using a small number of images of a person under varying facial expressions for training the memory in order to achieve expression-invariant recognition. due to the interpolation inherent in the output of the sdm  the output of the memory can then be interpreted as an indication of facial expression in addition to the identity of the person. 
　the third experiment tests vulnerability to occlusions. figure 1 shows a sequence of face images with increasing facial occlusions; a plot below shows the correlations beambiguities. 
tween the iconic vectors for two different points on the first face image and the corresponding vectors for the other images. the results seem to indicate that modest occlusions  as in images 1  1  and 1  can be handled but larger occlusions  such as in 1  may require other strategies such as the one suggested in  ballard and rao  1 . the results also motivate the need for using more than one point per face in order to be able to compensate for partial occlusions near specific facial locations. 
　finally  we tested the recognition performance of the method by training the memory on a face database consisting of images of 1 persons  figure 1  a   exhibiting 1 different facial expressions  as in figure 1  a  . all images were of size 1 x 1  greyscale  1 bit quantized  and taken under normal  overhead  illumination conditions with the face only approximately centered in the image frame. for testing the method  we used face images of the persons exhibiting facial expressions which were not used in the training set. figure 1  b  and  c  give examples of success and failure of the method respectively. figure 1  d  shows the recognition rate  the fraction of 1 test faces correctly recognized  plotted as a function of the number of points used per training face. a peak performance of 1% was achieved when 1 facial points were used; only 1 of the 1 test faces were incorrectly classified  with the correct identity finishing second in 1 of these 1 cases. 
1 	conclusions and future work 
a new approach to the problem of face recognition was proposed which uses iconic representations of faces as input to a topographically-organized sparse distributed memory. the iconic feature vectors are attractive as representations of faces because: 
	rao and ballard 	1 

  they simultaneously achieve the dual goals of dimensionality-reduction and orthogonality. 
  they are tolerant to modest changes in facial fea-tures or expressions due to the large number of measurements incorporated in the representations. 
  they allow simple strategies for rotation normaliza-tion and scale invariance1. 
  they can be computed efficiently on pipeline image processors such as the datacube mv1. 
  they facilitate real-time indexing of large face databases by allowing strategies such as using convolutions for distance computations. 
the sparse distributed memory model of kanerva was used for learning the association between feature vectors of a face and its identity. this model enjoys several favorable properties such as the ability to interpolate between stored facial expressions/views of a person  theoretically constant indexing time  due to the fixed number m of storage locations   possibly greater storage capacity over conventional linear memory  and anthropomorphic learning behavior in addition to the favorable properties  such as fault tolerance  that are known to accrue to distributed representations. in addition  recognition memory was topographically organized  thereby reducing crosstalk and speeding up the indexing process. 
　the method is clearly computation intensive; however  the recent availability of pipeline image processors significantly ameliorates this drawback since the ability of these processors to perform convolutions at frame rate  1/sec  can be effectively exploited. for example  the feature vectors for a face can be extracted after only 1 convolutions  four for generating the low-pass filtered five-level pyramid and nine for the basis filter kernels  
i.e. in approximately half a second. indexing into the sdms using convolutions for distance computations further optimizes the recognition process. storing upto 1 
1 see  rao and ballard  1a  for more details. 
1 	action and perception 
vectors for a face may seem extravagant but note that this choice still results in considerable savings over the alternative of pixel-wise storage of images  1 x 1 versus 1 x 1 . an interesting question is whether the method will fail when extremely large model bases of faces are used. however  the use of more than one vector per face potentially allows an extremely large number of persons to be handled. kanerva  estimates the capacity of the sdm to be about 1% of the number of storage locations; thus  even with only 1 storage locations for each sdm  the number of potentially distinguishable items is still 1 which is an extremely large number even after ruling out a significant proportion of the possible combinations as being unlikely to be encountered in practice. the accuracy of the above naive estimate clearly depends on the extent to which the filter response vectors are shared between the different stored model faces; while there exists some sharing in general due to the similarity of certain facial features across persons  we believe that the possible use of self-organization within the address space of the sdms will greatly help in further extending the capacity of the memory. 
　as described in section 1  preliminary results using the proposed method have been encouraging. future work will involve augmentation of the filter responses with color information  using  for instance  a variety of color-opponent gaussian center-surround mechanisms derived from unsupervised learning along the rgb planes   motion-based segmentation and recognition of persons  and further testing of the method on large face databases. 
