 
many problems of practical interest can be solved using tree search methods because carefully tuned successor ordering heuristics guide the search toward regions of the space that are likely to contain solutions. for some problems  the heuristics often lead directly to a solution- but not always. limited discrepancy search addresses the problem of what to do when the heuristics fail. 
our intuition is that a failing heuristic might well have succeeded if it were not for a small number of  wrong turns  along the way. for a binary tree of height d  there are only d ways the heuristic could make a single wrong turn  and only d d-i /1 ways it could make two. a small number of wrong turns can be overcome by systematically searching all paths that differ from the heuristic path in at most a small number of decision points  or  discrepancies.  limited discrepancy search is a backtracking algorithm that searches the nodes of the tree in increasing order of such discrepancies. we show formally and experimentally that limited discrepancy search can be expected to outperform existing approaches. 
1 introduction 
in practice  many search problems have spaces that are too large to search exhaustively. one can often find solutions while searching only a small fraction of the space by relying on carefully tuned heuristics to guide the search toward regions of the space that are likely to contain solutions. for many problems  heuristics can lead directly to a solution-most of the time. in this paper  we consider what to do when the heuristics fail. 
　we will focus our attention on procedures for tree search. our objective is simple: for search problems with heuristically ordered successors  we will develop a search procedure that is more likely to find a solution in any given time limit than existing methods such as chronological backtracking and iterative sampling  langley  1 . the outline of this paper is as follows: in the next section  we discuss existing algorithms. limited discrepancy search  lds  is introduced in section 1 and compared to existing techniques in section 1. we discuss variations of lds that we believe will be useful for solving realistic problems in section 1. we conclude by presenting our experimental results in section 1. 
1 existing strategies 
consider a tree search problem for which the successor ordering heuristic is so good that it almost always leads directly to a solution. such problems are common both in practice and in areas of ai research such as planning and scheduling  smith and cheng  1; wilkins  1 . if the heuristic is good enough  one might be satisfied with an algorithm that follows the heuristic and just gives up if the heuristic fails to lead to a solution  an algorithm we will call  1-samp   harvey  1; smith and cheng  1 . if the performance of 1-samp is not satisfactory  however  one is confronted with the question of what search algorithm to use instead. iterative sampling and backtracking are two candidates. 
1 	iterative sampling 
iterative sampling  langley  1   or isamp  is the simple idea of following random paths  or probes  from the root until eventually a path that leads to a solution is discovered. at each node on a path  one of the successors is selected at random and expanded. then one of its successors is selected at random  and so on until a goal node or dead end is reached. if the path ends at a dead end  isamp starts a new probe  beginning again at the root. 
　since the algorithm samples with replacement  there is a uniform chance of finding a goal node on any particular probe. provided that there is a goal node somewhere in the space  it follows that the probability of find a goal node increases uniformly toward 1 as the number of probes grows without limit. 
　iterative sampling has been shown to be effective on problems where the solution density is high  crawford and baker  1   but its performance as a fallback procedure for 1-samp is questionable because it ignores the successor-ordering heuristic. if the heuristic were the key to solving the problem despite a low solution density  one 
	harvey and ginsberg 	1 
would not expect iterative sampling to be effective.1 
1 	backtracking 
an alternative fallback procedure is simply to backtrack chronologically when 1-samp fails. our experiments in section 1 with scheduling show that this approach provides little improvement over 1-samp itself  and the analysis of mistakes provides an explanation  harvey  1   there is a reasonable chance that  somewhere early in 1-samp's path  it made a mistake by selecting a successor that had no goal nodes in the entire subtree below it. once this early mistake is made and the successor's subtree is committed to  none of the subsequent decisions makes any difference. 
　if the subtree below a mistake is large  chronological backtracking will spend all of the allowed run time exploring the empty subtree  without ever returning to the last decision that actually matters. if one is counting on the heuristics to find a goal node in a small fraction of the search space  then chronological backtracking puts a tremendous burden on the heuristics early in the search and a relatively light burden on the heuristics deep in the search. unfortunately  for many problems the heuristics are least reliable early in the search  before making decisions that reduce the problem to a size for which the heuristics become reliable. because of the uneven reliance on the heuristics  it is unlikely that chronological backtracking is making the best use of the heuristic information. 
1 	discrepancies 
let us return to the search problems for which the successor ordering heuristic is a good one. our intuition is that  when 1-samp fails  the heuristic probably would have led to a solution if only it had not made one or two  wrong turns  that got it off track. it ought to be possible to systematically follow the heuristic at all but one decision point. if that fails  we can follow the heuristic at all but two decision points. if the number of wrong turns is small  we will find a solution fairly quickly using this approach. 
　we call the decision points at which we do not follow the heuristic  discrepancies.  limited discrepancy search embodies the idea of iteratively searching the space with a limit on the number of discrepancies allowed on any path. the first iteration  with a limit of zero discrepancies  is just like 1-samp. the next iteration searches all possibilities with at most one discrepancy  and so on. 
　the algorithm is shown in figure 1. we will assume the search tree is binary. successors is a function that returns a list of the either zero or two successors  with the heuristic preference first. 
　in figure 1  x is the discrepancy limit. we iteratively call lds-probe  increasing x each time. lds-probe does a depth-first search traversal of the tree  limiting the number of discrepancies to x. when eventually x 
   1 we have experimented with biasing the random selection of successors according to the heuristic  but our results suggest this is not a viable approach  harvey  1 . 

figure 1: limited discrepancy search. 
reaches d  the maximum depth of the tree  lds-probe searches the entire tree exhaustively. thus the search is guaranteed to find a goal node if one exists and is guaranteed to terminate if there are no goal nodes. 
　since each iteration of lds-probe limits the number of discrepancies to x instead of restricting the search to those nodes with exactly x discrepancies  iteration n reexamines the nodes considered by previous iterations  see figure 1 . as with other iterative techniques  however  the final iteration is far and away the most expensive and the redundancy is therefore not a significant factor in the complexity of the search. 
　figure 1 shows a trace of lds exhaustively searching a full binary tree of height three. the heuristic orders nodes left to right. the twenty pictures show all the paths to depth three  in order. the dotted lines and open circles represent nodes that were not backtracked over since the previous picture  so the trace can be followed by examining at the pictures in sequence. counting all the black circles gives the total number of nodes expanded in the search  forty. 
　in general  the number of nodes expanded by lds with a discrepancy limit of x is bounded by d*+1  for iteration x  there are at most dx fringe nodes  with each path to a fringe node expanding at most d nodes . if d is large  the cost of any single iteration dominates the summed costs of the preceding ones. 
1 comparison with existing methods 
in practice  of course  we will typically not have time to search the space exhaustively. we would therefore like to know the likelihood of finding a solution  using the various methods  in the amount of time we are actually willing to wait. we will make this question precise by formalizing what we mean for the heuristic to make a  wrong turn.  
1 	wrong turns 
for simplicity  we will consider only the case of a full binary tree. the two children of each choice point are 


assumed to be in the order of heuristic preference. we will further assume that if a choice point has a goal node in the subtree below it  then with probability p  the heuristic probabihty  its first child has a goal node in its subtree. if the first child does not have a goal  the other child must have a goal since the choice point has only two children. in this case the heuristic has made a wrong turn by putting the children in the  wrong  order. 
　the notion of a wrong turn is closely related to the mistake probability. we define a bad node to be a node that does not have any goal nodes in its subtree. we define a mistake to be a bad node whose parent is not bad. the mistake probability  m  is the probability that a randomly selected child of a good node is bad  harvey  1 . if the heuristic orders successors randomly  the heuristic probability is the complement of the mistake probability  p = 1 - m. if the heuristic does better than random selection  p   1 - m. 
　figure 1 shows the four possibilities for a node and its children. an x indicates a bad node  a solid dot a good node. in the figure  p is the probability that a node is in class x or y  the two classes with good left children  given that it is not in class w  the only class where the parent is bad . the mistake probability m is one half the probability that a node is in class y or z  the classes with one mistake child  given that it is not in class w. 
　experimentally  it appears that m is generally fairly constant throughout many search trees  harvey  1 . 

in order to simplify our analyses  we will assume that p is constant as well  although the experimental evidence is that p tends to increase somewhat as we search the tree because most heuristics are more accurate at deep nodes than at shallow ones. 
　the chance of finding a solution on a random path to depth d  i.e.  using isamp  is simply  1 - m d. using heuristics and assuming a constant p  1-samp has probability pd of finding a solution on its one and only path. 
　this observation allows us to estimate p by running 1samp on a large training set of problems from the domain of interest. let s be the success rate of 1-samp on the training set. since the probability of success for 1-samp is pd  we have p - slld. if s is small  the training set may have to be impractically large to get a reliable estimate. for some problems  though  s is not small. heuristics developed for job shop scheduling have been shown to yield a probability s that is nearly one for small research problems  smith and cheng  1 . we have found in earlier experimental work on the same problems  harvey  1  that even standard csp heuristics can yield a success rate of about 1%. on larger scheduling problems  vaessens et a/.  1  the success rate of 1-samp is less  but more 
sophisticated heuristics from operations research keep 1samp competitive with other search techniques  cheng and smith  1 . 
1 	theoretical results 
given specific values for m and for p  figures 1 and 1 show the theoretical probability of success as a function of time for iterative sampling  isamp   chronological backtracking  dps   and limited discrepancy search  lds  for various heuristic probabilities p.1 the graphs show the probability of finding a solution in some number of probes i  where we define a probe to be a search until a dead end is reached for isamp or lds  or simply a search of an additional d nodes for dfs. the number of probes is limited by the height of the tree because the combinatorics of solving the problem beyond the one-discrepancy limit are intractable. the analyses are biased toward dfs because depth-first search is given the highest of the heuristic probabilities shown in each figure. 
　figure 1 shows results for a problem of height 1  with a mistake probability m of 1. the problem has about a billion fringe nodes of which a few more than a million are goals.1 with a solution density of 1  we would expect iterative sampling to sample about 1 fringe nodes before finding a solution  1  to be 
　1  the combinatoric manipulations underlying these figures are quite involved and appear elsewhere  harvey  1 . 
   1 d
thc number of goals is  1 - 1m  . 
1 

exact .1 by many accounts  a problem with a solution density of 1 is a fairly easy problem. it takes only 1 = 1  1 nodes  on average  to find a solution using iterative sampling. the expected number of probes is slightly more than the number of probes required to have a 1% chance of finding a solution  1   1 = 1  1 
　in practice  we may be interested in the number of nodes required to find a solution with a higher probability of success. the number of nodes required by iterative sampling for a success probability of 1 on this problem is 1   1 = 1  1. compare this to the performance of limited discrepancy search. for p = 1  lds has probability of success 1 with just eleven probes  or 1 nodes. the savings  nearly a factor of forty  depends on the heuristic to order successors correctly seven out of eight times when one of the successors is a mistake. 
　for p = 1 = 1-to  the heuristic orders the successors correctly half the time  no better than random selection. the p = 1 curve in the figure  almost completely obscured by the isamp curve  shows that the performance of lds is slightly worse than iterative sampling under these conditions. for p = 1  1  and 1 the heuristic orders nodes correctly five  six  and seven out of eight times. the curves show that the expected performance of lds increases dramatically with the better p. 
　the dps curve for p = 1 rises only marginally above the probability 1 that its first fringe node is a goal.1 the futility of dps is even clearer in the deeper search shown in figure 1. 
　the problem of figure 1 has height 1  and approximately 1 nodes. the density of solutions for m - 1 is about 1 x 1. iterative sampling needs 1 probes  1 million nodes  to have a 1% chance of suc-

cess. if  as in the earlier problem  the heuristic orders nodes correctly seven out of eight times  p - 1 for m = 1   lds has a similar chance with just twenty probes  1 nodes   a savings of three orders of magnitude over iterative sampling. the savings is similar if a success probability of 1 is desired instead. 
　for higher probabilities of success  the three orders of magnitude savings is more doubtful  though perhaps not as doubtful as the graph seems to suggest. the one discrepancy iteration ends after 1 probes. the later probes of the one discrepancy iteration have much of their paths in common  so the likelihood that one of these later probes succeeds given that the others failed is small. after 1 probes  though  the two discrepancy iteration begins to explore  fresh  paths again. consequently  we would expect the lds curve to rise steadily once again where the graph leaves off.1 
1 variations and extensions 
the reason we have focused on analyzing the early iterations of limited discrepancy search is that we believe in practice they are the only iterations that matter. earlier  we argued on intuitive grounds that they would be more important than the later iterations. we will now take the position that in practice the later iterations don't matter at all. the reason is that if the objective is to maximize the probability of finding a solution in a given number of nodes  there are always better things to do than use those nodes on later iterations of limited discrepancy search. 
　this section discusses a few of the more promising choices. since some involve combinations with other techniques and others depend on search space properties that are difficult to quantify  this discussion will be less precise than that of earlier sections. here  we will view limited discrepancy search as a tool that can be used in combination with other techniques to craft an effective search procedure for a given real world problem. 
   as remarked earlier  we are unfortunately unable to verify this essentially theoretical claim because the combinatorics overwhelm us. 

1 variable reordering 
constraint satisfaction problems and sat problems are formulated as tree search by fixing an order for the variables to be instantiated or determining the order dynamically as the search progresses. in either case  a node in the search tree is a choice point for the possible instantiations of a particular variable. if an effective heuristic does not solve the problem with a limit of one discrepancy for some chosen variable order  it may still solve the problem with one discrepancy given a different variable order. the  wrong turn  instantiations that the heuristic makes on the first variable order may even follow from unit propagation on the second. this suggests the simple technique of repeating the one discrepancy iteration of lds with different variable orders. when variable order is determined dynamically  it may suffice simply to begin the search with a different variable on each iteration. a similar technique improves the efficiency of depth-first search as well  see section 1 . 
1 using different heuristics 
if multiple heuristics exist for a particular problem  one can try repeating the one- or two-discrepancy limit iterations of lds with different heuristics. if one heuristic is unlucky and makes more than two wrong turns on a given problem  some other heuristic may be luckier. in general  what is hard for one heuristic may not be hard for another. lds is an effective way to give one heuristic a reasonable chance before switching to another. 
1 combining lds with bounded backtrack search 
lds can also be combined with bounded backtrack search  bbs   harvey  1  to produce an algorithm that does not count  small  discrepancies  those that fail quickly  toward the discrepancy limit. this algorithm can also be viewed as modifying the heuristic to avoid choices that can be seen to fail using a fixed lookahead.  the algorithm itself appears in appendix a.  
　the combined lds-bbs algorithm outperforms both lds and bbs on job shop scheduling problems. in fact  
lds-bbs appears to be the algorithm of choice among all systematic backtracking strategies in this domain. there is a compelling theoretical argument for this. many mistakes result in quick  if not immediate  failures. if a heuristic makes few wrong turns to begin with  it makes even fewer wrong turns that exceed the backtrack bound. adding a bounded backtrack enables limited discrepancy search to discover solutions with a discrepancy limit of no more than the number of wrong turns that actually exceed the backtrack bound  potentially reducing the number of required iterations. since the cost of each lds iteration grows by a factor of d  the savings can be substantial. the added cost of the backtrack bound is relatively insignificant. adding a backtrack bound of one node can cost at most a factor of 1. a backtrack bound of / costs at most a factor 1l and  for small /  is likely to be cheaper than the cost of an additional iteration. this upper bound is conservative since the heuristic  by assumption  makes few mistakes. 
1 local optimization using lds 
for problems like scheduling  lds can also be used to search the neighborhood of an existing solution. the one discrepancy iteration of lds is modified to begin following the path of the previous best solution instead of following the heuristic. at the depth of the discrepancy  the algorithm diverges from the previous solution and follows the heuristic for the remaining decisions. if the path ends in a solution that is better than the previous best  it can be adopted immediately or stored as a contender for the basis of the next iteration. 
this variation of lds requires some measure of the 
 goodness  of a solution. for scheduling problems  the schedule length is often the appropriate measure. searching for a schedule that takes less than time l} if successful  produces a schedule that takes time l'. a set of standard lds iterations can be repeated with the lower time bound l'  or the optimization variant of lds can be applied to consider variations of the previous schedule that differ by at most one discrepancy.1 
1 non-boolean problems 
finally  we should at least comment on the possible extension of lds to constraint-satisfaction problems involving variables with domain sizes larger than 1. although we have focussed on boolean problems in this paper  in part because the most natural encoding of jobshop scheduling problems is boolean  smith and cheng  1    the technique can obviously be applied in a wider setting. there are a variety of choices that will need to be made  however: should the one-discrepancy search include every alternate value for the variable that violates the heuristic  or only the single next most attractive choice  if the number of nodes expanded is to increase by a factor of no more than d on each iteration  we will need to take the latter view. 
1 experimental results 
our experimental results comparing limited discrepancy search with chronological backtracking and iterative sampling are based on a set of thirteen job shop scheduling problems taken from a recent survey of operations research techniques  vaessens et a/.  1 .1 each of the problems involves scheduling the tasks that might be involved in producing widgets in a manufacturing setting: each job ji needs to be performed on a particular machine m* and takes time ti. there are constaints indicating that some jobs need to be completed before others can be started  and so on. 
　the most effective encoding of problems such as these focusses directly on the resource contentions that arise; if two jobs ji and jk require the same machine  we introduce a variable pik indicating whether it is job ji or job jk that uses the machine first  smith and cheng  1 . 
1  alternatively  the time bound can be adjusted by binary division. a single iteration of lds  though  is not a decision procedure  so failure to find a schedule for a given time bound is no proof that no such schedule exists. 
1 the problems can be obtained by sending a message to 
o.rlibraryoic.ac.uk. 
1 

because these variables are boolean  the search space is far smaller than it would be if we were to make the variables the start times of the various jobs themselves. 
our experimental work formulated each problem as a 
csp with a loose bound on the schedule length. we then iteratively repeated the search  decreasing the bound each time to slightly less than the length of the last schedule found. we recorded the length of the best schedule found as a function of the total number of nodes expanded until reaching a final cutoff of 1 nodes per problem  see figure 1 . 

　at any given node cutoff m   1  each algorithm had completed some number of iterations for each problem  resulting in schedules of various lengths. we evaluated the schedules by these lengths  measuring their percent above the optimal length for each problem.1 we took the average percent above optimal  a function of m  as the overall measure of the performance of the search algorithms. 
　in figure 1  lds is clearly superior to chronological backtracking and iterative sampling. we chose this particular benchmark  though  so that we could also compare our results with other scheduling research in artificial intelligence and operations research. on this benchmark  contemporary or scheduling programs score in the range 1% to 1% above optimal  vaessens et a/.  1 . although the performance of our implementation does not match the best of these programs  it appears to be in the same range. 
　our scheduling implementation uses general csp heuristics  which are weak by scheduling standards. rel-
   1  the optimal lengths were taken to be the best reported lengths as of november  1. 
ative to the larger pool of programs  our implementation appears to be comparable using limited discrepancy search but disastrous using chronological backtracking and iterative sampling. since limited discrepancy search relies heavily on the heuristics  we expect that the combination of lds with the more accurate heuristics of the dedicated scheduling programs would have the best performance overall. experiments in this vein are under way. 
　we also experimented with a variety of nonsystematic algorithms  harvey  1 . depth-first search with restarts  iterative broadening  and bounded backtrack search scored 1%  1%  and 1% above optimal on the benchmark and all outperformed pure limited discrepancy search slightly  lds was also 1% above optimal .1 
　however  since all of these nonsystematic methods rely less on the heuristics than lds  we believe that lds is likely to benefit more significantly from future improvements to the heuristics. as we commented in section 1  limited discrepancy search can also be combined with bounded backtrack search. the results are shown in figure 1 

　the combination of limited discrepancy with bounded backtrack search had the best performance of all the systematic and nonsystematic methods we tested. at 1% over optimal  its performance with a four-node backtrack 
   1  although the overall difference of 1% between the best and worst of these algorithms may appear slight  it is substantial in this domain  since the problems can be expected to become exponentially more difficult as one approaches the crossover point corresponding to optimality  crawford and auton  1 . 1  the parameter / in the figure is the depth of backtrack allowed in checking for heuristics that led to dead ends. 

bound is respectable when compared to the dedicated scheduling programs. 
1 	conclusion 
we have shown both theoretically and experimentally that limited discrepancy search is an effective way to exploit heuristic information in tree search problems. it is more effective than either chronological backtracking or iterative sampling  and we have attempted to explain why. 
　the scheduling problems that we used in our experiments  while large by contemporary research standards  are not large relative to the types and sizes of scheduling problems it would be useful to solve in the real world. because of the complexity of scheduling  it is likely that the challenge of scaling up from research problems to real world problems will be met more quickly by advances in heuristics than by the evolution of brute force methods. we expect that in the future  techniques that depend on heuristics yet recover gracefully by searching alternatives when the heuristics fail will be the methods of choice for solving real world problems. 
acknowledgement 
this work has been supported by the air force office of scientific research under contract 1 and by arpa/rome labs under contracts f1-c-1 and f1-c-1. the authors would like to thank to andrew baker  ari jonsson  jimi crawford and david etherington for valuable feedback in the course of this research. 

