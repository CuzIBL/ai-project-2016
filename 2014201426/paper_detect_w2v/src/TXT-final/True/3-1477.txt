
object and scene learning and recognition is a major issue in computer vision  in robotics and in cognitive sciences. this paper presents the principles and results of an approach which extracts structured view-based representations for multi-purpose recognition. the structures are hierarchical and distributed and provide for generalization and categorization. a tracking process enables to bind views over time and to link consecutive views. scenes can also be recognized using objects as components. illustrative results are presented.
1 introduction
object and scene learning and recognition is a major issue in computer vision  in robotics  in neuroscience  and in cognitive sciences as well. and one of the main questions to this respect is how to extract knowledge from 1d patterns of light in the camera or the retina.
　for the recognition of objects  two models were proposed in the literature. in the 'image-based' or 'view-based' model  an object is represented as a collection of view-specific local features  poggio and edelman  1; ullman  1; tarr and bulthoff  1; riesenhuber and poggio  1：	 . representations are organized in trees in which a set of viewtuned units constitutes the weighted inputs to a higher level object-invariant unit. each unit measures the similarity between the input image and its stored view  and the higher level unit computes the weigthed sum from its incoming connections. if the resulting value reaches a given threshold  then the learned object is recognized. riesenhuber's hmax model of object recognition in the ventral visual stream of primates also proposes a similar grouping method where higher level cells compute the maximum response of view-tuned cells  riesenhuber and poggio  1 .
　the second model is based on structural descriptions. one of the most important approaches is the  recognition-bycomponents   rbc  model  biederman  1  in which each

 
　　the work described in this paper was partially conducted within the eu integrated project cogniron   the cognitive companion   funded by the european commission division fp1-ist future and emerging technologies under contract fp1.
 
supported by the european social fund.object is represented as a collection of volume parts. thus  there is no need of multiple view-tuned representations since 1d models can be virtually rotated and compared to the input image. moreover  the use of such a model can make the recognition invariant to illumination and color.
　we can identify certain properties of the modeling process for efficient learning and recognition. firstly  the training and thus the construction of new representations must be sufficiently fast. lack of speed is the principal cutoff of models based on structural descriptions because building 1d models from images remains a non trivial task. secondly  an efficient modeling process must organize knowledge in a structured way. a first means of organizing the data is to build categorical representations. categorical structures make it possible to obtain capacities of class generalization at low cost. indeed  to effectively exploit the extracted data  those must be describable at various levels of specificity: a bottle can be described as a container  a plastic object for recycling  etc. such a structure can also accelerate recognition that deals with large collections of stored objects by reducing the number of candidate object models. another mode to structure meanings is the decomposition of a representation as the set of its parts. thanks to structural descriptions  rbc offers more robustness  in particular with respect to noise and occlusion. moreover  it is interesting to share components among several structural descriptions to save memory: one could use the representation of a wheel to describe either a car  a truck or a bike. thirdly  a learning system for knowledge extraction and structuring must allow the addition of new representations at non-prohibitive memory cost and without explosion of complexity. this open-endedness property can be approached by using the two kinds of data structures referred to above  in which a representation can be factorized in categories or shared as components.
　while considering object recognition one must mention important results achieved by template-based approaches for object classification and recognition  lecun et al.  1 . nearest neighbor methods  support vector machines and convolutional networks provide efficient solutions but the needs of structured knowledge  reusability  incremental and autonomous learning are several points addressed here which are not  to the best of the authors' knowledge  well dealt with by the pre-cited methods.
this paper presents an efficient approach  paquier  1   which is partially implemented  for building structured representations without using 1d primitives and exhibiting the properties mentioned above. the paper is organized as follows. section 1 presents the model we choose to both extract and organize representations. each essential property is presented and illustrated by an example produced by the implemented system. section 1 introduces the view-binding algorithm and the incremental building of object-invariant detectors. the use of objects as landmarks for buidling structured representation of scenes is presented in section 1. we conclude in section 1.
1 architecture for view-based recognition
we will first describe the map structure which constitutes the basic building block of our model  and discuss its inherent shift-invariant property. in the learning process  the maps will specialize in certain features  and we will present the way we associate them in order to prevent redundancy in this specialization and structure extracted features.
1 maps and inter-maps connectivity the map: a set of local classifiers

figure 1: a map is a collection of units sharing the same set of weights  kernel . each unit is composed as a three stage calculation pipeline. it receives signals incoming from previous layers for integration and lateral signals from maps situated on the same layer for competition.
　in order to satisfy real-time constraints  the learning and recognition system must be able to update its representations at a frequency that is compatible with the modifications of the environment  and process images at a rather high frequency  e.g.  1 fps . with this end in view we chose to implement an integrate-and-fire model in local classifiers built as calculation pipelines  figure 1 .
　we call a map a collection of local classifiers called units  organized in a retinotopic1 way. each map is associated with one or more afferent maps which are the locations of its units' input domains  or their receptive fields . receptive fields of each unit are static and each unit is thus associated to a set of afferent units in each afferent map. we denote  i the set of afferent units of a given unit ui.
　at time t and for a given unit ui  information flows from its receptive fields through a set of learning weights wi t . learning weights are organized in kernels  and there are as many weight kernels as afferent maps. all the units of a map are sharing the same set of weights  thus they can detect and learn a pattern which is at different positions in the input maps. as shown in figure 1  the coordinates of a unit in a given map and its receptive field in the afferent map are the same. therefore the position of an active unit corresponds to the position of the detected pattern  which provides for a
　shift-invariance property.
　this type of mapping has been previously introduced by fukushima's neocognitron and was successfully applied to handwritten digit recognition  fukushima  1 . figure 1 illustrates this intrinsic map feature. in this experiment  we produce an image containing a set of four randomly distributed letters  k  p  s  u . after less than a minute  the system has learned autonomously its own distributed representation of the input image. each resulting map can extract its learned feature at any location in the input image. to obtain this result we must also provide our system with an inter-maps communication channel  so that maps don't learn the same feature several times. this procedure is called  local competition  and is introduced next.
a  weight kernels evolution
map1
map1
map1
map1
b  final pattern representation
	input image	map1  u 	map1 p 	map1  s 	map1  k 

figure 1: letter extraction from noisy image input  1% random noise  1% scale and angle variation  1 steps  1 images per second . a  weight kernels evolution across time. b  input image and associated maps activations.
preventing redundancy with competition
y
figure 1: hierarchical maps connectivity and local competition. a  the letter  l  is detected as a relative positioning of horizontal and vertical bars. b  input image containing the letters  l  and  t : inter-maps competition with local inhibition leads to distinct specialisations.
one previously mentioned requirement concerns the structural decomposition into several components. to achieve such a decomposition  different maps observing the same inputs must  decide  to specialize into distinct component detectors. the undergoing process is illustrated in figures 1.a and 1.b. in this example  we want to train two maps to detect the letters  l  and  t  in the input image. a first step of extraction is composed of a layer of two maps which respectively detect vertical and horizontal bars in their inputs. when the letter  l  is present in the image  figure 1.a   the pattern is decomposed as the positions of these two local orientations. the relative positioning of the two patterns is then detectable and can be learned by a dedicated map on the next layer  layer1 .
　from this point  if we add another letter   t   to the input image and a second learning map to the third layer to learn it  figure 1.b   then we must prevent this new map from learning the  l  pattern one more time. this differentiation is achieved by using a local competition in which we compare map detection values and choose the best fitted  see section 1 for the computation of the detection value .
　right after calculating its detection value  every unit of each map broadcasts it to all other units on maps of the same layer and at the same coordinates. thus  every unit whose coordinates correspond to the position of the letter receives incoming values at the 'max' stage of its pipeline architecture  see figure 1 . the 'max' stage  as its name indicates  computes the maximum incoming value and sends it to its own 'learn' stage. the learning process is then able to compare the local unit activation incoming from the 'integrate & fire' stage to distant activations. by allowing learning only to the best fitted unit  we ensure that no map could learn one pattern if another is already specialized to detect it. we illustrate this inter-maps competition on figure 1.b with the dark discs which represent the blocking signal.
connecting maps to build distributed and hierarchical representations
we adopted a layered hierarchical architecture for several reasons. firstly we need this architecture for categorization. as our units achieve linear separation in their input set  complex features cannot be extracted with a single layer. this is a known limitation of the single layer perceptron that cannot simulate an exclusive disjunction  logical xor . the second reason is reusability. a huge number of extracted features are encountered in different shapes: oriented segments  arcs of circles or color blobs are building blocks for more complex images. since extracted information is shared in the network  we avoid redundancy and the resulting computational overhead. following the same idea  similar meanings should be encoded using shared sets of units. for example  the internal representations of a truck and of a car should intersect. this intersection representing the shared meaning could contain the internal representation of the four wheels  the steering wheel  etc. thanks to this distributed representation architecture  it is easier to pool similar objects into cross categories  e.g.  wheeled vehicules .
　this capability of building hierarchical and shared representations is shown in figure 1. in this experiment we trained a network to recognize faces using a set of ten different im-

figure 1: an example of a hierarchical representation of faces. a  network connectivity showing weight kernels and maps activities. layer1  layer1  layer1  layer1 respectively extract contrasts  oriented segments  eyes and mouth  and face. b  recognition robustness for different subjects and variable postures.
ages of each of 1 distinct subjects1. in the same way as in the previous example of letter extraction  layer1 learns to detect the position of each eye in one map and the position of the mouth in the other one  sharing the same segments extraction in the previous layer  layer1 . in layer1  we train another map which receives as inputs the detection values of the mouth and eyes specialized maps. as a result  this last map learns the representation of the face with relative positioning of eyes and mouth. we also see in this example that the resulting face recognition is robust to high variations in subject posture and morphology. nevertheless  this kind of recognition only applies to images with limited face orientations. in section 1  we will propose additional methods to recognize 1d objects based on the pooling of overlapping views.
1 integration  firing and learning
now that we have presented the global mechanisms involving the computing units  we present the internal workings of the system more precisely and the computation of the detection values mentioned in section 1.
integration
given a unit ui  we denote βi t  （  1  its calculated output burst at time t. the burst is the detection value mentioned before. let wij t  （ wi t  be the learning weight associated to the connection between units ui and uj  and  i+ t  a subset of afferent  i defined as:
wi t  = {wij t  uj （  i} 1             +i  t  = {uj （  i βj t    1} we also define three weight sums as follows : 1 si  t  = x | wij t  |
wij t （wi t  1 siβ+ t  = x wij t 
uj（ +i  t  1 si+ t  = x wij t  1 wij t （wi+ t 
where wi+ t  = {wij t  uj （  i and wij t    1}.
then the integrated potential pi at time t+1  which expresses a level of similarity between the learnt pattern and the inputs  is given by:
		 1 
where αp （  1  is a fixed potential leak.

figure 1: burst sampling distribution and cumulative distribution evolution. a  burst sampling distributions computed at steps 1  1 and 1. burst values are discretized within 1 levels. b  corresponding cumulative distribution used as a transfer function for integration.
firing
output bursts βi generated by a unit ui are produced both for integration processes situated on the the next layer and for its own  learn  stage  see figure 1 . in the first case  burst levels are not taken into account since all positive bursts participate to integration  see equation 1 . in the second case  we previously explained that we need to compare units activation in order to find the best fitted map in a competition process. the burst level is thus computed as follows:
 
	1	if pi t    ti 
	βi t  =	 1 
	fi t pi t  	otherwise.
 where ti is a fixed threshold and fi is an adaptive transfer function whose variations across time are illustrated by figure 1.b. the underlying idea of this function is that  as units are learning  their output function must converge to a binarylike response corresponding to a more strict linear separation. at each step we compute a sample distribution of the input burst by discretizing the output burst value in 1 levels. the transfer function fi is in fact the associative cumulative distribution at time t:
	fi t x  = x fi t b 	 1 
1 b＋x
where function fi is a sampling distribution of burst levels updated at each step  figure 1.a . we thus obtain a sigmoidal transfer function which permits a binary-like classification behavior and a more effective competition process. until now  object detection is internally represented as the activation of a limited set of computing units. due to our connectivity scheme  recognitions are indeed only reflected at positions located near the centre of the learned pattern. however patterns presence are more than just their center since the whole surface they cover is relevant. so we increase the pool of activated units to cover a wider surface. as a first approach  we chose a simple disc shape whose surface is given by si  t .
learning
we use in our model a hebbian-like learning rule in which units tend to learn patterns responsible for their activation. in other words  a learning process occurs when one unit has both fired and won the competition in its layer. moreover  we compute a stochastic standard deviation σij for each weight wij which is computed and used as coefficient in the learning calculation as follows:
μij t + 1  =  1   αw μij t  + αw | γij t  |
 σij t + 1 =  1   αw σij t  + αw μij t + 1    γij t + 1  1 wij t + 1  =  1   αw wij t  + αw 1   1σij t + 1  γij t + 1 
where μij is a stochastic mean and αw the learning rate.
hence we ensure that weights corresponding to noisy inputs  with high standard deviation  will tend to 1 and then won't take part in the representation. γij is a function of input burst of unit i which takes its values in { 1 1}. maps among the same layer are in competition as shown in figure 1. if at time t  an input burst from unit j is received γij t  = 1  else if a burst is received from any other afferent unit then γij t  =  1. finally if no burst was present γij t  = 1.
1 from views to objects
in this section  we consider the 1d-object recognition problem. let us consider a camera rotating around an object. for a given orientation we can train a dedicated map and obtain enough robustness to detect this view  say within about a 1 degrees angle interval centered on the learnt prototype. wider intervals can be obtained with  simple  objects  such as balls or paper cups  whose views are relatively invariant during view-point modification  while turning around them. our approch is to pool overlapping view detectors. then  a viewinvariant  or object-tuned  detector can be obtained simply by computing the maximum view-tuned map response  riesenhuber and poggio  1 . this will provide for continuous object recognition.

figure 1: object invariant detection with view-tuned maps. a  maps activities overlap to produce continuous detection. b  resulting learning weights. these are composed of 1 kernels for each map which correspond to 1 afferent oriented segmentdetectors situated on previous layer  see figure 1 .
　assuming the support of an external module producing the initial object detection  this could be simply a user's selection on the input image  but this module could be based on saliency  motion  etc...   we must resolve several remaining problems. firstly  the object-tuned map must keep track of the object during point of view modifications  temporal binding . secondly  as we have no initial information about the object's complexity and  therefore cannot anticipate the exact number of required view-tuned maps  we must find a method which incrementally adds new maps to the network and associate them together.
map-tracker : short-term memory for temporal binding
the different views of the same object tend to occur close together in time and space  tarr and bulthoff  1：  . several authors proposed to take advantage of this property  stinger and rolls  1; wallis  1 . we particularly want to mention stinger and rolls's hypothesis about the functional architecture and the operation of the ventral visual system tested in their model called visnet . in this model  each activation of a view-invariant neuron is maintained during a short period of time. the pooling is then achieved by using an associative memory to link temporal memory trace and current view detection. although this method seems biologically plausible  it seems difficult to apply to real-time robotics. as the view-toobject membership is not a priori known  there is no criteria to decide whether or not a newly specialized map corresponds indeed to the view of an object. if no preselection is applied  the size of the associative memory could exceed computable capacities. so  it is crucial to restrict map specialization to relevant views. a solution comes from a tracking approach. in this domain  temporal coherence is also used but in another way. the aim of tracking is not so much extracting knowledge but rather following the position of a collection of pixels. temporal coherence can here be exploited by considering that persitent informations are preserved between two frames and can thus be extracted. we propose to use the map architecture for pixel tracking in order to follow the position of the object and use this information as a first step for an incremental view-tuned map creation.
　we can obtain the desired tracking capability by increasing the learning rate αw of a map  see section 1   which we will call tracker-map  to a value near 1. the tracker-map is trained at the time the first object position is produced by the external module. we call this process  one-shot learning  because of the use of a high learning rate. in the next frame  thanks to the robustness of our model  the tracker-map can detect the object even if it has started to move. according to the hebbian rule  this detection is followed by a learning phase. then  another learning occurs which almost completly renews the stored prototype due to the high learning rate. from now the process can restart allowing continous detection. trackermaps can be viewed as short-term memories that never reach stable representations.
temporal binding
now that we can use temporal coherence to track the object position  we must resolve the remaining problem of building incrementally view-specific representations of the tracked object. to reach this goal we granted tracker-maps the capability of creating new maps during runtime. as we are in the context of moving objects and/or moving point of views  oneshot learning  learning with high learning rate will also be used to catch object's views during movement. the very specific resulting prototype will be refined as view-tuned maps compute standard learning rates. the binding algorithm develops as follows: if no view-tuned map recognizes the object at the position of the tracker-map's detection during a short period of time  we use a 1 frames time-out   which is initially the case  then the tracker creates a new map and forces it to learn the unknown view with a one-shot-learning signal. if a view is detected  because a map has previously learnt it  then a standard learning process occurs in this map and the detection produces a one-shot learning signal for the tracker-map in order to focus tracking on this view. this process ensures that long-term memory of a specific view has a higher priority during object detection than the tracker. this algorithm has been used in the experiment of figure 1 in which the image of a rotating object has been used to train a three-layers network. in figure 1.a  we see that the tracker always detects the object  thanks to the feedback from long-term to short-term memory  and that activity of map1 is about 1 times longer than the others. indeed  this map learned to detect orientations that are rather invariant during rotation. our model can thus adapt the number of required maps depending on the complexity of objects.

figure 1: representing scenes as arrangements of objects in an office environment. a  examples of object-invariant detections from different points of view learn. b  results of scene recognition by relative positioning of object-invariant detections.1 objects as landmarks for scene recognition
we present in this section experimental results of our system in the context of scene recognition. we can define a scene as a particular arrangement of objects. we demonstrated previously with face recognition that our architecture is able to construct hierarchical representations of an image. we can use this same technique in order to learn a scene as relative arrangement of objects. for this experiment we observed a standard office environment including 1 basic objects: a paper basket  a phone and a chair. in a first phase  object representations are learnt on the fly using temporal coherence by the means of tracker-maps  section 1 . the resulting network then detects each object simultaneously in view-specific maps and in the corresponding object-invariant tracker-map. in a second phase  three higher level maps connected to all object-invariant detectors learn representations of the scene from three different points of view. figure 1.a shows the outputs of the three object-invariant detectors built as presented in section 1 with natural images. without needing any additional algorithm  our model extracts a view-point specific representation of the scene in a hierarchical structure  figure 1.b . from now  a location-invariant map can be trained thanks to tracker-maps for scene-invariant maps pooling as explained in section 1. these multi-layered architecture allowed by parallel-pipeline calculations is the key property which permits to combine view-based representations and structural descriptions.
1 conclusion
in this paper we have introduced several methods and algorithms to extract knowledge from visual data. our model is able to build representations by decomposition of image features into local components structured in a global hierarchy of concepts. such decompositions are minimal requirements for both sharing representation in order to save memory and computational time  and providing generalization capabilities due to the fact that extracted components can be used as description blocks to recognize new elements. the main advantage of our model is that one does not need any prior knowledge or model to build robust dedicated detectors of simple to complex features in a unified language. thus  building heterogeneous libraries of meanings becomes feasible. these representations could be then provided as input to a symbolic reasoning level.
