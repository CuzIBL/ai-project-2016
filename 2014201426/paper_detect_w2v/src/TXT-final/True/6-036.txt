 
in constraint programming one models a problem by stating constraints on acceptable solutions. the constraint model is then usually solved by interleaving backtracking search and constraint propagation. previous studies have demonstrated that designing special purpose constraint propagators for commonly occurring constraints can significantly improve the efficiency of a constraint programming approach. in this paper we present a fast  simple algorithm for bounds consistency propagation of the alldifferent constraint. the algorithm has the same worst case behavior as the previous best algorithm but is much faster in practice. using a variety of benchmark and random problems  we show that our algorithm outperforms existing bounds consistency algorithms and also outperforms-on problems with an easily identifiable property-state-ofthe-art commercial implementations of propagators for stronger forms of local consistency. 
1 introduction 
many interesting problems can be modeled and solved using constraint programming. in this approach one models a problem by stating constraints on acceptable solutions  where a constraint is simply a relation among several unknowns or variables  each taking a value in a given domain. the constraint model is then usually solved by interleaving backtracking search and constraint propagation. in constraint propagation the constraints are used to reduce the domains of the variables by ensuring that the values in their domains are locally consistent with the constraints. 
　previous studies have demonstrated that designing special purpose constraint propagators for commonly occurring constraints can significantly improve the efficiency of a constraint programming approach  e.g.   regin  1; stergiou and walsh  1  . in this paper we study constraint propagators for the alldifferent constraint. an alldifferent constraint over a set of variables states that the variables must be pairwise different. the alldifferent constraint is widely used in practice and because of its importance is offered as a builtin constraint in most  if not all  major commercial and researchbased constraint systems. 
　several constraint propagation algorithms for the alldifferent constraint have been developed  ranging from weaker to stronger forms of local consistency  see  van hoeve  1  for an excellent survey . regin  gives an 1 n 1   1   algorithm for domain consistency of the alldifferent constraint  where n is the number of variables  that is based on relating alldifferent constraints to matchings. leconte  gives an 1 n1  algorithm for range consistency  a weaker form of consistency than domain consistency  that is based on identifying hall intervals. puget   building upon the work of leconte   gives an o n logn  algorithm for bounds consistency  which is in turn a weaker form of local consistency than range consistency. mehlhorn and thiel   building upon the work of regin  give an algorithm for bounds consistency that is 1 n  plus the time needed to sort the bounds of the domains  and thus has the same worstcase behavior as puget's algorithm in the general case. 
　in this paper we present a fast and simple algorithm for bounds consistency propagation of the alldifferent constraint. the algorithm has the same worst case behavior as the previous best algorithm but is much faster in practice. using a variety of benchmark and random problems  we show that our algorithm outperforms existing bounds consistency algorithms and also outperforms-on problems with an easily identifiable property-state-of-the-art commercial implementations of propagators for stronger forms of local consistency. 
　a longer version of the paper containing proofs and additional experimentation is available  lopez-ortiz et al.  1 . 
1 background 
a constraint satisfaction problem  csp  consists of a set of n variables  a finite domain dom  of possible values for each variable xi{ and a collection of m constraints  
 . each constraint c is a constraint over some set of variables  denoted by vars{c   that specifies the allowed combinations of values for the variables in vars c . given a constraint  1  we use the notation t c to denote a tuple t-an assignment of a value to each of the variables in vars c -that satisfies the constraint c. we use the notation t x  to denote the value assigned to variable x by the tuple t. a solution to a csp is an assignment of a value to each variable that satisfies all of the constraints. 
　we assume in this paper that the domains are totally ordered. the minimum and maximum values in the domain 

constraints 	1 

dom x  of a variable x are denoted by min dom x   and max dom x    and the interval notation  a  b  is used as a shorthand for the set of values {a a+ 1 ...b}. 
　csps are usually solved by interleaving backtracking search and constraint propagation. during the backtracking search when a variable is assigned a value  constraint propagation ensures that the values in the domains of the unassigned variables arc  locally consistent  with the constraints. with a'   a causes the same update. thus  for the purpose of updating lower bounds  it suffices to restrict attention to leftmaximal hall intervals: those  a  b  for which a is minimal. 
　puget's algorithm first sorts the variables in increasing order of max;. we assume for convenience that max -   maxj  for i   j. the algorithm then processes each of the variables in turn  maintaining a set of counters which count how many of the variables processed so far have a minimum bound of at least k. more precisely  after processing x -  the counter 

cik denotes the cardinality of the set {j   i : minj    k}. the algorithm stores the counters in a balanced binary tree  allowing updates in 1 log n.  time per variable. 
　conceptually  our algorithm is similar to puget's. the difference is in the maintenance of the counters. the key observation is that not all counters are relevant. 
1 	constraints 

  maxsorted  o . . n-1  : holds intervals sorted by max. 
  bounds  1 . .nb+l : sorted array of ruin's and max's. 
  t  1 . .nb+l : holds the critical capacity pointers; that is  t  i  points to the predecessor of/ in the bounds list. 
  d  1 . .nb+l  holds the differences between critical ca-pacities; i.e.  the difference of capacities between interval i and its predecessor in t viz. t   i   . 
  h 1. .nb+l  holds the hall interval pointers; i.e.  if h  i    i then the half-open interval  bounds  h  i     bounds  i   forms a hall interval  and otherwise holds a pointer to the hall interval it belongs to. this hall in-

1 	updating bounds 
finding hall intervals is only part of the solution. we also need to efficiently update the bounds. for this we use another linked list structure  in which indices inside a hall interval point to the location representing its upper end  while those outside of any hall interval point left toward the next such index. we store the list of bounds in a sorted array named bounds. intervals are hereafter numbered by their order of occurrence in this array. the linked list is implemented as an array t using indices to the bounds as pointers. the differences between critical capacities appearing above the arrows in example 1 are stored in an array d. the algorithm shown in figure 1 solves one half of the problem: updating all lower bounds. variable n holds the number of intervals  terval is represented by a tree  with the root containing the value of its right end. 
the algorithm uses two functions for retrieving/updating pointer information  namely: pathmax a  x  which follows the chain x  a  x   a  a  x     ...  until it stops increasing  returning the maximum found and pathset  a  x  y  z  which sets each of the entries a  x   a  a  x     a  w  
to z  where w is such that a  w  equals y. the values minrank and maxrank give the index in array bounds of the min and  max-hi  of an interval. 
constraints 	1 　the algorithm examines each interval in turn  sorted by their upper bounds. it then updates capacities accordingly  followed by path compression operations on the underlying data structures. at each step we test for failure  a negative capacity  or a newly discovered hall interval  a zero capacity  which indicates that the width of the interval is equal to the number of variables whose domain falls within that interval . 
example 1 table 1 shows a trace of the algorithm for updating lower bounds  figure 1  when applied to the csp from examples 1 & 1. each row represents an iteration where a variable is processed. in the first graph the nodes are the elements of the vector bounds. the arrows illustrate the content of the vector t and the numbers over them are given by the vector d. the nodes of the second graph are also the values found in vector bounds but the arrows are given by the vector h that keeps track of the hall intervals. 
table 1: trace of the example. 
1 	time complexity 
the running time of the algorithm is dominated by the various calls to pathmax and pathset. since each chain followed in a pathmax call is also followed in a subsequent p a t h set call  we can restrict our analysis to the time spent in the 
1 
latter. consider the right-running chains in array t. lemma 1 shows that all but a logarithmic number of indices see a rise in value as a result of a path compression operation. 

this implies that a linear number of path compressions take at most 1 n log n  steps. the situation with array h is similar. it follows then that the algorithm runs in time 1 n log n . 
　the theoretical performance of the algorithm can be improved further by observing that the union operations are always performed over sets whose bounds appear consecutively in a left to right ordering. this is known as the interval union-
find problem. gabow and tarjan  gave a linear time solution in a ram computer provided that the keys fit in a single word of memory. this is a reasonable assumption in current architectures with 1 or 1 bit words. using this technique we obtain a linear time algorithm which matches the theoretical performance of mehlhorn and thiel's solution. we implemented this algorithm on the intel x1 architecture using direct assembly code calls from a c++ program. however  in practice  the 1 n log n  solution outperformed both mehlhorn and thiel's algorithm and the algorithm using the interval union find data structure  see  lopez-ortiz et al.  1  for additional discussion . 
1 	experimental results 
we implemented our new bounds consistency algorithm  denoted hereafter as bc  and mehlhorn and thiel's bounds consistency algorithm  denoted mt  using the 1log solver c++ library  version 1  ilog s. a.  1. the ilog solver already provides implementations of leconte's range consistency algorithm  denoted rc   regin's domain consistency algorithm  denoted dc   and an algorithm that simply removes the value of an instantiated variable from the domains of the remaining variables  denoted as vc  for value consistency . to compare against puget's bounds consistency algorithm  we use the runtime results reported by puget   1  for rc and our own runtime results for rc as calibration points. we believe this is valid as puget also uses a similar vintage of 
ilog solver and when we compared  we were careful to use the same constraint models and variable orderings. 
　we compared the algorithms experimentally on various benchmark and random problems. all the experiments were run on a 1 mhz pentium ii with 1 mb of main memory. each reported runtime is the average of 1 runs except for random problems where 1 runs were performed. 
constraints 


figure 1: time  sec.  to first solution for pathological problems. 
bc propagator offers a clear performance improvement over propagators for stronger forms of local consistency  see figure 1 . comparing against the best previous bounds consistency algorithms  our bc propagator is approx. 1 times faster than mt and  using rc as our calibration point to compare against the experimental results reported by puget   approx. 1 times faster than puget's algorithm. 
　we next consider the golomb ruler problem  see  gent and walsh  1   problem 1 . following smith et al.  we modeled the problem using auxiliary variables  their  ternary and all-different model   and we used the lexicographic variable ordering. this appears to be the same model as puget  uses in his experiments as the number of fails for each problem and each propagator are the same. here  our bc propagator is approximately 1 times faster than the next fastest propagator used in our experiments  see table 1  and  again using rc as our calibration point  approximately 1 times faster than puget's bounds consistency algorithm. 

table 1: time  sec.  to optimal solution for golomb rulers. 
　we next consider instruction scheduling problems for single-issue processors with arbitrary latencies. instruction scheduling is one of the most important steps for improving the performance of object code produced by a compiler. briefly  in the model for these problems there are n variables  one for each instruction to be scheduled  latency constraints of the form xi    xj + d where d is some small integer value  a single alldifferent constraint over all n variables  and redundant constraints called  distance constraints  in our experiments  we used fifteen representative hard problems that were taken from the spec1 floating point  spec1 floating point and mediabench benchmarks. the minimum do-

table 1: time  sec.  to optimal solution for instruction scheduling problems. a blank entry means the problem was not solved within a 1 minute time bound. 
main size variable ordering heuristic was used in the search  see table 1 . on these problem too  our bc propagator offers a clear performance improvement over the other propagators. 
　to systematically study the scaling behavior of the algorithms  we next consider random problems. the problems consisted of a single alldifferent constraint over n variables and each variable xi had its initial domain set to  a.b   where a and b  a   1  were chosen uniformly at random from  1  n . the problems were solved using the lexicographic variable ordering. in these  pure  problems nearly all of the run-time is due to the alldifferent propagators  and one can clearly see the quadratic behavior of the rc and dc propagators and the nearly linear incremental behavior of the bc propagator  see figure 1 . on these problems  vc  not shown  could not solve even the smallest problems {n = 1  within a 1 minute time bound and mt  also not shown  was 1 - 1 times slower than our bc propagator. 
　having demonstrated the practicality of our algorithm  we next study the limits of its applicability. schulte and stuckey  investigate cases where it can be proven a priori that maintaining bounds consistency during the search  rather than a stronger form of local consistency such as domain consistency  does not increase the size of the search space. the golomb ruler problem is one such example. in general  of course  this is not the case and using bounds consistency can exponentially increase the search space. 
　to systematically study the range of applicability of the algorithms  we next consider random problems with holes in the domains of the variables. the problems consisted of a single alldifferent constraint over n variables. the domain of each variable was set in two steps. first  the initial domain of the variable was set to  a  1   where a and b  a   b  were chosen uniformly at random from  1  n . second  each of the values a-|-1 ...  b- 1 is removed from the domain with some given probability p. the resulting problems were then solved using both the lexicographic and the minimum domain size variable ordering heuristics. these problems are trivial for 

constraints 	1 

figure 1: time  sec.  to first solution or to detect inconsistency for random problems. 
domain consistency  but not so for bounds and range consistency. we recorded the percentage that were not solved by bc and rc within a fixed time bound  see figure 1 . if there are no holes in the domains of the variables  then bounds consistency is equivalent to range and domain consistency. as the number of holes increases  the performance of bounds and range consistency decreases and they become less appropriate choices. the range of applicability of bc can be extended somewhat following a suggestion by puget  of combining bounds consistency with value consistency  denoted as bc+ and mt+ . on these problems  bc  bc+  and rc are theoretically equivalent when using the lexicographic variable ordering and bc+ and rc are experimentally equivalent when using minimum domain  see figure 1 . 
　we also performed experiments on n-queens  quasigroup existence  and sport league scheduling problems. interestingly  in these experiments  rc was never the propagator of choice. on problems where holes arise in the domains  dc was the best choice  except for on n-queens problems  where vc was considerably faster   and on problems where holes do not arise in the domains  bc was the clear best choice. clearly  whether the domains have holes in them is a property that is easily identified and tracked during the search. thus  the best choice of propagator could be automatically selected  rather than left to the constraint modeler to specify as is currently the case. 
1 	conclusions 
we presented an improved bounds consistency constraint propagation algorithm for the important alldifferent constraint. using a variety of benchmark and random problems  we showed that our algorithm significantly outperforms the previous best bounds consistency algorithms for this constraint and can also significantly outperform propagators for stronger forms of local consistency. 
acknowledgements 
we thank kent wilken for providing the instruction scheduling problems used in our experiments. 
figure 1: percentage not solved within a cutoff of 1 seconds for problems with 1 variables. the cutoff was chosen to be the value that was at least two orders of magnitude slower than dc  the fastest propagator on these problems. 
