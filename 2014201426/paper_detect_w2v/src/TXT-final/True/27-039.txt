 
text classification  the grouping of texts into several clusters  has been used as a means of improving both the efficiency and the effectivedess of text retrieval/categorization in this paper we propose a hierarchical clustering algorithm that constructs a bet of clusters having the maximum bayesian posterior probability  the probability that the given texts are classified into clusters we call the algorithm hierarchical bayesian clustering  hbc  the advantages of hbc are experimentally verified from several viewpoints  1  hbc can re-construct the original clusters more accurately than do other non probabilistic algorithms  1  when a probabilistic text categorization is extended to a cluster-based one  the use of hbc offers better performance than does the use of non probabilistic algorithms 
1 	introduction 
text classification.  the grouping of texts into several clusters  has been used as a means of improving both the efficiency and the effectiveness of ttzt retrieval/categorization  jardine and van rjjabergen  
1  van rijsbergen and croft  1  croft  1  willett  1  for example  to retrieve texts relevant to a user's request  a simple strategy would be to search all the texts in a database by calculating a measure of the relevance of each text to the request this exhaustive search  however  would require more computation for larger databases text classification helps to reduce the number of comparisons in an exhaustive search by clustering  grouping  similar textb into clusters in advance and comparing the request with the representative of each cluster clustering is also assumed to improve the accuracy of retrieval  but this assumption has not been verified the retrieval/categorization model that incorporates text classification as a preliminary process is often called cluster-based text retrieval/categorization 
¡¡in this paper we propose a probabilistic algorithm of hierarchical clustering and compare it with other clustering algorithms almost all previous algorithms  such as single-link method and ward's method  use the measure 
1 	natural language 
for automatic text classification 
takenobu t o k u n a g a 
department of computer science 
tokyo institute of technology 
ookayama  meguro 
   tokyo 1  japan takedes titech ac jp 
of distance between two objects and merge the closer ones  cormack  1  anderberg  1  griffiths et a/  1  willett  1   our algorithm  though  constructs a set of clusters that has the maximum bayesian posterior probability  the probability that the given objects are classified into clusters this maximization is a general form of the well known maximum likelihood estimation  and we call the algorithm hierarchical bayesian clus tenng  hbc  
¡¡probabilistic models are becoming popular in the field of text retrieval/categorization owing to their solid formal grounding in probability theory  croft  1  fuhr  1  kwok  1  lewis  1  they retrieve those texts that have larger posterior probabilities of being relevant to a request when these models are extended to cluster-based text retrieval/categorization  however  the algorithm used for text clustering has still been a non probabilistic one  croft  1  we think that better performance could be obtained by using exactly the same criterion in both clustering and retrieval/categorization  that is  searching for the maximum posterior probability in this paper we verify this assumption through preliminary experiments where we compare a probabilistic text categorization using hbc and the same categorization using non probabilistic clustering algorithms 
¡¡the term  categorization  in this paper refers to the assignment of documents to predefined categories  whereas  classification  and  clustering  refers to only grouping of documents without identifying the meaning of groups 
	1 	hierarchical bayesian clustering 
like most agglomerative clustering algorithms  cormack  1  anderberg  1  griffiths et al  1  willett  1   hbc constructs a cluster hierarchy  also called dendrogram  from bottom to top by merging two clusters at a time at the begimng  i e   at the bottom level in a dendrogram   each datum belongb to a cluster whose only member is the datum itself for every pair of clusters  hbc calculates the probability of merging the pair and selects for the next merge the best one for which this probability is highest this merge step takes place n - 1 times for a collection of n data the last merge produces a single cluster containing the entire data set 


	iwayama and tokunaga 	1 

figure 1 mutual information at each merge step 
we compare hbc with other two representative hierarchical clustering algorithms  anderberg  1   singlelink method and ward'e method  from the viewpoint that how accurately the original  i e   true in a sense  classification can be reconstructed from scratch 
¡¡the data set we used was a dictionary of contemporary words  in japanese   called gendar yogo no kisotijih gk   jiyukokuminsya  1  it contains 1 word entries  each of which is classified into one of 1 categories the length of texts explaining each word varies from 1 to 1 characters  in kanji  and averages 1 characters to eliminate the effect of noise  we 
excluded smaller entries and smaller categories according to two thresholds1 
  for an entry  the number of terms in the entry should be more than 1 
  for a category  the number of entries that belong to the category should be more than 1 
there remained 1 entries classified into 1 categories all texts were tagged by a japanese morphological analyzer called juman  matsumoto  1   and each entry 
became a set of terms  l e   nouns  with relative frequencies 
¡¡from these 1 entries  we constructed three cluster hierarchies using single-link method  ward's method  and hbc  and we compared these hierarchies first we calculated the mutual information 1 c a  between a 
set of constructed clusters c and the set of the original categories a for each entry d  a category a € a had been assigned by gk's editors beforehand and a cluster c € c is assigned by a clustering algorithm so there are two classifications one by human experts and tbe other by a clustering algorithm since mutual information i c a  defines the amount of information that is commonly contained in c and a  the larger the value i c a  has  the more closely the set of clusters c approximates the original set of categories a formally  
¡¡¡¡1 in preliminary experiments  variations of tbe two thresh olds had utile affect on the overall results we set the threshold values because the memory available to run the program was limited 
1 	natural language 

figure 1 average accuracy at each merge step 
pa becomes 1 as the merging steps proceed  na decreases and pa increases monotonically at the end  since all the entries are contained in the single cluster  pa becomes 1 but na becomes 1 usually  the best point for both na and pa lies between the beginning and the end from the figure  we see that hbc offers the best upper bound of the averaged accuracy in addition  since the optimal set of clusters obtained by hbc is the smallest  hbc could construct a set of clusters that has 
the most generality  i e  the highest predictive power  for unseen data - from the standpoint of the mdl principle  rissanen  1  stating that smaller models are better 
	1 	evaluation in text categorization 
   text categonzation is the classification of documents with respect to a set of predefined categories we use text clustering to improve the efficiency of a text categorization based on memory based reasoning  mbr   a k nearest neighbor search   masand et a/  1  mbr solves a new task by looking up examples of tasks similar to the new task  stanfill and waltz  1  for example  to attach categories to a new document  mbr searches the nearest it documents in a large set of already categorized documents  called training data  and uses the categories attached to the searched k documents to determine categories of the new document although this strategy offers promising performance  masand et al  1   mbr requires a large amount of computational power for calculating a measure of the similarity between a new document and every example and for sorting the similarities for practical applications  parallel computing is usually necessary 
¡¡we use a clustering algorithm for preliminary processing to reduce the number of comparisons by partitioning a large amount of training data into several clusters  jardine and van rijsbergen  1  van rysbergen and croft  1  croft  1  willett  1  the original mbr searches the k nearest documents  but the clusterbased mbr searches the k nearest clusters of documents the documents in the searched k clusters are used to at tach categories to a new document this pre-clustering 
	iwayama and tokunaga 	1 

procedure could eliminate a great deal of computation if the number of clusters ib relatively small in addition  if the accuracy of the cluster-based mbr is comparable to that of the original mbr  the cluster-based version would be an efficient approximation of the original mbr this section describes our experimental investigate of how well hbc and ward's method can help to approximate the performance of the original mbr 
¡¡in experiments we used the gk data set described in section 1 to divide the data set into two bets  one for examples with categories  training data  and the other for evaluation  test data   we used 1-fold crossvalidation the 1 above-threshold entries were randomly divided into four bets  one used for test data and the remaining three used for training data for each entry in the test data  an text categorization model assigned a category using the training data if the category assigned by the model was the same as the category assigned by gk's editors  we said that the categorization was correct we used the accuracy of categorization to measure the performance of a model since there were four variations of test data depending on which set of the four we select  we did four series of experiments and averaged the results 
¡¡to see the performance of the original mbr  we calculated the probability  for every pair 
consisting of a test entry d1ei  and a training entry d| r q i n by using a method similar to that used in deducing the eq  1   bee  iwayama and tokunaga  1  for more details  to assign a category to each test entry dtettl we searched the k nearest entries besed on the calculated probabilities each dtrain in the k nearest entries voted on the originally assigned category when the weight of the voting was log  the category with 
the most votes was assigned to the test entry .  figure 1 shows the result of the accuracy obtained when varying k from 1 to the maximum value  i e   the number of the given training data  we can see that more correct categories are attached as k  the number of near-
est neighbors  increases but also that there is more noise 
after jfc reaches some value 
1 	natural language 
¡¡for cluster-based mbr we first constructed a cluster hierarchy from the given training data by using each of the two clustering algorithms  hbc and ward's method from a constructed hierarchy we selected a set of clusters  at some merge step for each 
pair consisting of a test entry d tlt and a cluster c  € c we calculated  by using the same method as in the original mbr and extracted the nearest k clusters from c all the members in the nearest k clusters become the approximated nearest entries of the test entry using the same voting strategy as in the original mbr  a category was assigned to each test entry figure 1 shows the results when the number of selected clusters  n  was set to 1  1  and 1 the original mbr curve is also plotted for reference we can see that the accuracy of the cluster-based mbr with hbc approaches that of the original mbr as the number of constructed clusters increases note that the original mbr corresponds to the extreme case where n is the maximum value  i e   the number of training data  we can also see the advantage of hbc over ward's method cluster-based mbr with hbc approximates the original mbr sufficiently well whereas the cluster-based mbr with ward's method provides poor performance  especially when the number of nearest neighbors is small 
1 	discussion 
we have proposed a new hierarchical clustering algorithm that is based on maximizing the bayesian posterior probability the advantages of this algorithm have been experimentally verified from several viewpoints  including its performance within an actual application of text categorization in summary  
  from the standpoints of the mutual information and the classification accuracy  hbc can re-construct the original clusters more accurately than do the single-link method and ward's method 
  when a probabilistic text categorization is extended to a cluster-based one  the use of hbc offeres better performance than does the use of non probabilistic algorithm like ward's method 

since the hbc algorithm itself is general and not restricted to text clustering  it could be easily applied to various applications  such as automatic thesaurus construction we have in fact used the algorithm to construct noun hierarchies  a kind of thesaurus  using grammatical relations and have verified their contribution to disambiguation tasks  tokunaga et al  1  
¡¡one issue we have not discussed in this paper is lhe selection of an appropriate set of clusters from a constructed dendrogram for a collection of n data there are n possibilities of the set  each of which corresponds to a merge step generally  a larger set of clusters  at earlier merge step  can express the given data more precisely but has poorer predictability for unseen data on the other hand  a smaller set  at later merge step  has the opposite characteristics selecting the optimal set of clusters is a crucial issue for applications in an application of cluster-based mbr  described in section 1   the amount of computation is also affected by the size of the set larger sets needs more comparisons than do smaller sets for this general issue of model selection  several statistical measures  such as aic  akaike  1  and mdl  rissanen  1  have been proposed since our algorithm is based on probability theory  it would be easy to introduce these statistical measures into our algorithm 
acknowledgments 
the authors are grateful to hiroshi motodafor beneficial discussions 
r e f e r e n c e s 
 akaike  1  h akaike a new look at the statistical model identification ieee trans on automatic control  1  1  1 
 anderberg  1  m r anderberg cluster analysis for applications academic press  1 
 cormack  1} r m cormack a review of classification journal of ike royal statistical society  1-
1  1 
 croft  1  w b croft a model of cluster searching based on classification information systems  1- 1  i1 
 croft  1  w b croft document representation in probabilistic models of information retrieval journal of the american society for information science  
1  1  1 
 dubes and jain  1  r dubes and a k jam validity studies in clustering methodologies pattern recognition  1-1  1 
 fuhr  1  n fuhr models for retrieval with probabilistic indexing information processing & retrieval 1 1  1 
 griffiths ct al  1  a griffiths  l a robinson  and p willett hierarchic agglomerative clustering methods for automatic document classification journal of documentation  1  1  1 
 iwayama and tokunaga  1  m iwayama and t tokunaga a probabilistic model for text categorization based on a single random variable with multiple values in proceedings of 1th conference on applied natural language processing  pages 1  1 
 jardine and van rijsbergen  1  n j ax dine and c j van rijsbergen the use of hierarchic clustering in information retrieval information storage and retrieval  1-1  1 
 jiyukokuminsya  1  jiyukokuminsya gendai yogo no kibotisiki jiyukokuminsya  1  in japanese  
 kwok  1  k l kwok experiments with a component theory of probabilistic information retrieval based on single terms as document components acm transactions on information systems  1  1  1 
 lewis  1  d d lewis an evaluation of phrasal and clustered representation on a text categorization task in proceedings of the annual international a cm si-
gir conference on research and development ir. information retrieval  pages 1  1 
 masand et al  1  b mas and  g lin off  and d waltz classifying news stones using memory based reasoning in proceedings of the annual international acm si gir conference on research and development in information retrieval  pages 1  
1 
 matsumoto  1  y matsumoto juman users manual kyoto university and nara institute of science and technology  1 
 rissanen  1  j rissanen stochastic complexity in statistical inquiry world scientific publishing  1 
 stanfill and waltz  1  c stanfill and d waltz toward memory-based reasoning communications of the acm  1  1  1 
 tokunaga et al  1  t tokunaga  m iwayama  and h tanaka automatic thesaurus construction based on grammatical relations in proceedings of the inter 
	national joint 	conference 	on 	artificial intelligence  
1 
 van rijrbergen and croft  1  c j van rijsbergen and w b crofl document clustering an evaluation of some experiments with the granfield 1 collection 
information processing & management  1-1  1 
 willett  1  p willett similarity coefficients and weighting functions for automatic document classification an empirical comparison international classification  1  1  1 
 willett  1  p willett recent trends in hierarchic document clustering a critical review information processing & management  1  1  1 
	iwayama and tokunaga 	1 
