planning for weakly-coupled partially observable stochastic games
	anyuan guo	victor lesser
university of massachusetts amherst
department of computer science
1 governor's drive  amherst  ma 1
{anyuan  lesser} cs.umass.edu

1 introduction
partially observable stochastic games  posgs  provide a powerful framework for modeling multi-agent interactions. while elegant and expressive  the framework has been shown to be computationally intractable  bernstein et al.  1 . an exact dynamic programming algorithm for posgs has been developed recently  but due to high computational demands  it has only been demonstrated to work on extremely small problems. several approximate approaches have been developed  emery-montemerlo et al.  1; nair et al.  1   but they lack strong theoretical guarantees. in light of these theoretical and practical limitations  there is a need to identify special classes of posgs that can be solved tractably.
¡¡one dimension along which computational gain can be leveraged is by exploiting the independence present in the problem dynamics. in this paper  we examine a class of posgs where the agents only interact loosely by restricting one another's available actions. specifically  rather than having fixed action sets  each agent's action set is a function of the global state. the agents are independent from one another otherwise  i.e. they have separate transition and reward functions that do not interact. this class of problems arises frequently in practice. many real world domains are inhabited by self-interested agents that act to achieve their individual goals. they may not affect each other in any way  except for occasionally putting restrictions on what each other can do.
¡¡the best-known solution concepts in game theory are that of computing nash equilibrium and performing strategy elimination. our work addresses both of these solution concepts. first  we show that finding a nash equilibrium where each agent achieves reward at least k is np-hard. another contribution of this work is that we present an exact algorithm for performing iterated elimination of dominated strategies. it is able to solve much larger problems than exact algorithms for the general class by exploiting problem structure.
1 posgs with state-dependent action sets
in this section  we will present a formal definition of our model. an n-agent posg with state-dependent action sets can be defined as h{si} {s1i} {ai} {bi} {pi} {ri}i  where 
  si is the state space of agent i. s = s1 ¡Ás1 ¡Á...¡Ásn denotes the joint state set.
  s1i ¡Ê   si  is the initial state distribution of agent i.
  ai is the action set of agent i. a = a1 ¡Áa1 ¡Á...¡Áan denotes the joint action set.
  bi : s ¡ú 1ai is the available action function that maps a joint state to the set of available actions for agent i. bi s    ai for all s ¡Ê si.
  pi : si ¡Á ai ¡Á si ¡ú  1  is the transition function of agent i. the joint transition function is completely
	=	i=1 pi si|si ai .
  ri : si ¡Á ai ¡ú   is the reward function for agent i. this states that the rewards of the agents depend only on the local state and the action taken by agent i.
¡¡in our model  each agent has a complete view of its own local state. a local policy for each agent is a mapping from the local state and the available action set to an action in that set. a joint policy is a tuple of local policies  one for each agent.
definition 1 a local policy ¦Ði  for agent i of an n-agent posg with state-dependent action sets  is a mapping from pairs of local states and local action sets hsi ai s i to actions in the current local action set ai s   where s = hs1 ... si ... sni.
¡¡here is an example of a posg with state-dependent action sets. two autonomous rovers are exploring an unknown terrain and collecting rock samples. there is a river with a narrow bridge on it that only allows one rover to pass at a time. to simplify the illustration  we will assume the rovers have two states {on land  on bridge}  and three actions each {move  get on bridge  pick up rock}. the rovers receive reward for the number of rocks picked up. the state-dependent action set is used to constrain the rover's actions such that only one is allowed on the bridge at a time. for example  the available actions for rover 1 are specified as follows  b1 hs1 = on land s1 = on landi  = {move  get on bridge  pick up rock}  b1 hs1 = on land s1 = on bridgei  ={move  pick up rock}  and analogously for rover 1.
1 complexity
we state our decision problem  denoted as posg-ne  as follows: given a posg with state-dependent action sets  g = h{si} {s1i} {ai} {bi} {pi} {ri}i  does there exist a nash equilibrium where all agents have expected utility at least k 
theorem 1 posg-ne is np-hard even in problems involving 1 agents and a horizon of 1.
1 iterated action elimination algorithm
we present an algorithm that performs iterated elimination of dominated strategies. the algorithm is able to work directly with the compact representation of a posg without first converting it to the double exponentially large normal form representation. in this algorithm  first  we first fix the action sets of each agent at each state to the optimistic and pessimistic action sets. the idea behind this is that although an agent cannot predict exactly what actions will be available at each state  it can find out what actions would be available in the best and worst case scenario. in the best case  none of the actions that can be restricted by the state of the other agents are  and in the worst case  all of the actions that can be restricted are in fact unavailable.

table 1: the iterated action elimination algorithm.
beforeafter# statespruningpruning1 11 111 ¡Á11.1 ¡Á11.1 ¡Á1 1
table 1: average size of the policy space before and after action elimination for problems with 1 constrained states.
¡¡once we fix the available action sets of each agent at all the states  the agents no longer depend on each other in any way. our model decomposes to a set of mdps. for each agent  we solve two mdps  one using the optimistic action sets and the other using the pessimistic action sets. the solutions will provide us with an upper and a lower bound on the actual value function of each agent. with these upper and lower bounds on the expected values of each state  we can now derive bounds on the expected action values. at each state  dominated actions are pruned. we iterate the action elimination procedure until no more actions can be pruned. since the number of actions at each state is finite  the procedure will eventually converge.
theorem 1 the iterated action elimination algorithm corresponds to the iterated elimination of very weakly dominated strategies.
1 experiments
we tested the algorithm on a simplified 1-agent autonomous rover exploration scenario. the size of each agent's local state space varies from 1 to 1. we introduced up to three constrained states in each agent's state space. 1 trials were run for each size of state space and number of constraints. table 1 shows detailed results for problems with three constrained states. for problems with one and two constrained states  the final policy space ranges from 1 to 1 and 1 to 1 respectively. in all three cases  the iterated action elimination algorithm is able to reduce the size of the policy space by several orders of magnitude.
