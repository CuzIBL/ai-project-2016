
in this paper  we show how using the dirichlet process mixture model as a generative model of data sets provides a simple and effective method for transfer learning. in particular  we present a hierarchical extension of the classic naive bayes classifier that couples multiple naive bayes classifiers by placing a dirichlet process prior over their parameters and show how recent advances in approximate inference in the dirichlet process mixture model enable efficient inference. we evaluate the resulting model in a meeting domain  in which the system decides  based on a learned model of the user's behavior  whether to accept or reject the request on his or her behalf. the extended model outperforms the standard naive bayes model by using data from other users to influence its predictions.
1 introduction
in machine learning  we are often confronted with multiple  related data sets and asked to make predictions. for example  in spam filtering  a typical data set consists of thousands of labeled emails belonging to a collection of users. in this sense  we have multiple data sets-one for each user. should we combine the data sets and ignore the prior knowledgethat different users labeled each email  if we combine the data from a group of users who roughly agree on the definition of spam  we will have increased the available training data from which to make predictions. however  if the preferences within a population of users are heterogeneous  then we should expect that simply collapsing the data into an undifferentiated collection will make our predictions worse.
¡¡the process of using data from unrelated or partially related tasks is known as transfer learning or multi-task learning and has a growing literature  baxter  1; guestrin et al.  1; thrun  1; xue et al.  1 . while humans effortlessly use experience from related tasks to improve their performance at novel tasks  machines must be given precise instructions on how to make such connections. in this paper  we introduce such a set of instructions  based on the statistical assumption that there exists some partition of the tasks into clusters such that the data for all tasks in a cluster are identically distributed. ultimately  any such model of sharing must be evaluated on real data  and  to that end  we evaluate the resulting model in a meeting domain. the learned system decides  based on training data for a user  whether to accept or reject the request on his or her behalf. the model that shares data outperforms its no-sharing counterpart by using data from other users to influence its predictions.
¡¡when faced with a classification task on a single data set  well-studied techniques abound  boser et al.  1; lafferty et al.  1 . a popular classifier that works well in practice  despite its simplicity  is the naive bayes classifier  maron  1 . we can extend this classifier to the multi-task setting by training one classifier for each cluster in the latent partition. to handle uncertainty in the number of clusters and their membership  we define a generative process for data sets that induces clustering. at the heart of this process is a non-parametric prior known as the dirichlet process. this prior couples the parameters of the naive bayes classifiers attached to each data set. this approach extends the applicability of the naive bayes classifier to the domain of multi-task learning when the tasks are defined over the same input space.
¡¡bayesian inferenceunderthis clustered naive bayes model combines the contribution of every partition of the data sets  weighing each by the partition's posterior probability. however  the sum over partitions is intractable and  therefore  we employ recent work by heller and ghahramani  1a  to implement an approximate inference algorithm. the result is efficient  task-level transfer learning.
1 models
in this paper  we concentrate on classification settings where the features xf  f = 1 ... f and labels y are drawn from finite sets vf l  respectively. our goal is to learn the relationship between the input features and output labels in order to predict a label given an unseen combination of features.
¡¡consider u tasks  each with nu training examples composed of a label y and a feature vector x. to make the discussion more concrete  assume each task is associated with a different user performing some common task. therefore  we will write d u  to represent the feature vectors x u i  and corresponding labels y  u i   i = 1 ... nu  associated with the u-th user. then d is the entire collection of data across all users  and d u j  =  x u j  y  u j   is the j-th data point for the u-th user.
¡¡let mu y denote the number of data points labeled y ¡Ê l in the data set associated with the u-th user and let nu y f x denote the number of instances in the u-th data set where the f-th feature takes the value x ¡Ê vf when its parentlabel takes value y ¡Ê l. let  and
. we can now write the general form of the probability mass function of the data conditioned on the parameters ¦È and ¦Õ of the naive bayes model:

in  a   p d|¦È ¦Õ  is expanded into a product of terms  one for each data set  reflecting that the data sets are independentconditioned on the parameterization. step  b  assumes the data points are exchangeable; in particular  the label/feature pairs are independent of one another given the parameterization. in step  c   we have made use of the naive bayes assumption that the features are conditionally independent given the label. finally  in step  d   we have used the fact that the distributions are multinomials. the maximum likelihood parameterizations are
.
¡¡because each data set is parameterized separately  it is no surprise that the maximum likelihood parameterization for each data set depends only on the data in that data set. in order to induce sharing  we must somehow constrain the parameterization across users. in the bayesian setting  the prior distribution f ¦È ¦Õ  can be used to enforce such constraints. given a prior  the resulting joint distribution on the data is
.
both models introduced in this section are completely specified by a prior distribution over the parameters of the naive bayes model. as we will see  different priors result in different types of sharing.
1 no-sharing baseline model
we have already seen that a ml parameterizationof the naive bayes model ignores related data sets. in the bayesian setting  any prior density f over the entire set of parameters that factors into densities for each user's parameters will result in no sharing. in particular 
 
is equivalentto the statement that the parameters for each user are independent of the parameters for other users. under this

figure 1: graphical models for the  a  no-sharing and  b  clustered naive bayes  right  models. each user has its own parameterization in the no-sharing model. the parameters of the clustered naive bayes model are drawn from a dirichlet process. here  the intermediate measure g has been integrated out.
assumption of independence  training the entire collection of models is identical to training each model separately on its own data set. we therefore call this model the no-sharing model.
¡¡having specified that the prior factors into parameter distributions for each user  we must specify the actual parameter distributions for each user. a reasonable  and tractable  class of distributions over multinomial parameters are the dirichlet distributions which are conjugate to the multinomials. therefore  the distribution over ¦Õu  which takes values in the |l|simplex  is
.
similarly  the distribution over ¦Èu y f  which takes values in
|vf|-simplex  is
.
we can write the resulting model compactly as a generative process:1
	¦Õu ¡« dir  ¦Áu y : y ¡Ê l  	 1 
y  u n  | ¦Õu ¡« discrete ¦Õu  ¦Èu y f ¡« dir  ¦Âu y f x : x ¡Ê vf  
xf u n  | y  u n  {¦Èu y f :  y ¡Ê l} ¡« discrete ¦Èu  y  u n   f 
the no-sharing model will function as a baseline against which we can compare alternative models that induce sharing. we turn now to specifying a model that induces sharing.
1 the clustered naive bayes model
a plausible assumption about a collection of related data sets is that they can be clustered into closely-related groups. to make this more precise  we will consider two tasks t1 and t1 over some space x ¡Á y to be related if the data associated with both tasks are identically distributed. while this is a very coarse model of relatedness  it leads to improved predictive performance with limited training data.
¡¡as a first step we must specify a distribution over partitions of tasks. there are several properties we would like this distribution to have: first  we want exchangeability of tasks  users ; e.g.  the probability should not depend on the ordering  i.e. identity  of the tasks  users ; second  we want exchangeability of clusters; e.g.  the probability should not depend on the ordering/naming of the clusters; finally  we want consistency; e.g.  a priori  the  hypothetical  existence of an unobserved task should not affect the probability that any group of tasks are clustered together.
¡¡the chinese restaurant process  crp  is a stochastic process that induces a distribution over partitions that satisfies all these requirements  aldous  1 . the following metaphor was used to define the process: imagine a restaurant with a countably infinite number of indistinguishable tables. the first customer sits at an arbitrary empty table. subsequent customers sit at an occupied table with probability proportional to the number of customers already seated at that table and sit at an arbitrary  new table with probability proportional to a parameter ¦Á   1. the resulting  seating chart  partitions the customers. it can be shown that  in expectation  the number of occupied tables after n customers is ¦¨ logn   antoniak  1; navarro et al.  1 .
¡¡the tasks within each cluster of the partition will share the same parameterization. extending our generative model  imagine that when a new user enters the restaurant and sits at a new table  they draw a complete parameterization of their naive bayes model from some base distribution. this parameterization is then associated with their table. if a user sits at an occupied table  they adopt the parameterization already associated with the table. therefore  everyone at the same table uses the same rules for predicting.
¡¡this generative process corresponds with the well known dirichlet process mixture model  dpm  and has been used very successfully to model latent groups  ferguson  1 . the underlying dirichlet process has two parameters  a mixing parameter ¦Á  which corresponds to the same parameter of the crp  and a base distribution  from which the parameters are drawn at each new table. it is important to specify that we draw a complete parameterization of all the feature distributions  ¦Èy f  at each table. we have decided not to share the marginal distributions  ¦Õ  because we are most interested in knowledge relating the features and labels.
¡¡again  we can represent the model compactly by specifying the generative process:
¦Õu ¡« dir  ¦Áu y : y ¡Ê l  
y  u n  | ¦Õ ¡« discrete ¦Õu 


xf u n  | y  u n  {¦Èu y f :  y ¡Ê l} ¡« discrete ¦È u y  u n   f 
the discrete measure g is a draw from the dirichlet process; in practice  this random variable is marginalized over. because the tasks are being clustered  we have named this model the clustered naive bayes model  and denote its distribution function over the parameters as fcnb . we now explain how to use the model to make predictions given training data.
1 approximate inference
given labelled training data d u  = {y  u i  x u i }1¡Üi¡Ünu for all tasks u ¡Ê {1 ... u} and an unlabeled feature vector for some task v  we would like to compute the posterior distribution of its label.
using bayes rule  and ignoring normalization constants 

where d is the data set where we imagine that the  v nv + 1 -th data point has label y. therefore  bayesian inference requires that we marginalize over the parameters  including the latent partitions of the dirichlet process. having chosen conjugate priors  the base distribution can be analytically marginalized. however  the sum over all partitions makes exact inference under the dirichlet process mixture model intractable. while markov chain monte carlo and variational techniques are the most popular approaches  this paper uses a simple  recently-proposed approximation to the dpm known as bayesian hierarchical clustering  bhc   heller and ghahramani  1a   which approximates the sum over all partitions by first greedily generating a hierarchical clustering of the tasks and then efficiently summing over the exponential number of partitions  consistent  with this hierarchy. this approach leads to a simple  yet efficient  algorithm for achieving task-level transfer.
¡¡consider a rooted binary tree t where each task is associated with a leaf. it will be convenient to identify each internal node  tn  with the set of leaves descending from that node. a tree-consistent partition of the tasks is any partition such that each subset correspondsexactly with a nodein the graph  figure 1 . it can seen that  given any rooted tree over more than two objects  the set of all tree-consistent partitions is a strict subset of the set of all partitions. exact inference under the dpm requires that we marginalize over the latent partition  requiring a sum over the super-exponential number of partitions. the bhc approximation works by efficiently computing the sum over the exponential number of tree-consistent partitions  using a divide-and-conquer approach to combine the results from each subtree. intuitively  if the tree is chosen carefully  then the set of tree-consistent partitions will capture most of the mass in the posterior. bhc tries to find such a tree by combining bayesian model selection with a greedy heuristic.
¡¡just as in classic  agglomerative clustering  duda et al.  1   bhc starts with all objects assigned to their own cluster and then merges these clusters one by one  implicitly
1
{1}
{1}
{1 1}
{1 1}
{1 1}
{1 1}
{1 1 1}
inconsistent
figure 1: all tree-consistent partitions represented both as sets of nodes  left  and collection of leaves  right   and one partition that is not tree-consistent  the sets of leaves  is not representable by an internal node .
forming a tree that records which merges were performed. however  instead of using a distance metric and merging the nearest clusters  bhc merges the two clusters that maximize a statistical hypothesis test. at each step  the algorithm must determine which pair in the set of clusters t1 t1 ... tm to merge next. consider two particular clusters ti and tj and let di and dj be the set of tasks in each respectively. the bhc algorithm calculates the posterior probability that these two clusters are in fact a single cluster tk = ti +tj. specifically  the hypothesis hk is that the data in dk = di ¡È dj are identically distributed with respect to the base model  in this case  some naive bayes model . the probability of the data in this new cluster under hk  p dk|hk  is simply the marginal likelihood of the data.
¡¡the alternative hypothesis  h¡¥k is that the data di and dj are  in fact  split into two or more clusters. computing the probability associated with this hypothesis would normally require a sum over the super-exponentialnumber of partitions associated with the tasks in di and dj. however  the clever trick of the bhc algorithm is to restrict its attention to treeconsistent partitions. therefore  the probability of the data dk = di ¡È dj under h¡¥k  p dk|h¡¥k  = p di|ti p dj|tj   where p di|ti  is the probability of the data associated with the tree ti. let ¦Ðk = p hk  be the prior probability of the cluster tk. then  we can write p dk|tk  recursively
 p dk|tk  = ¦Ðkp dk|hk + 1 ¦Ðk p di|ti p dj|tj .  1  then the posterior probability of hk is
	.	 1 
we now present the bhc algorithm  whose output is sufficient for approximate bayesian predictions under our model.
input data d = {d 1  ... d n } model p x|y ¦È  and prior density f ¦È 
initialize number of clusters c=n  and
       di = {d i } for i = 1 ... n while c   1 do
find the pair di and dj with the highest posterior probability of hk where tk = ti + tj.
merge dk ¡û dk ¡È dj  tk ¡û  ti tj 
     delete di and dj  c ¡û c   1 end while
¡¡heller and ghahramani  1a  show that a specific choice of the prior ¦Ðk = p hk  leads to an approximate inference scheme for the dpm. let ¦Á be the corresponding parameter from the dpm. then  we can calculate the prior probability for each cluster tj in the tree built by bhc.
initialize each leaf i to have di = ¦Á  ¦Ði = 1 for each internal node k do
dk = ¦Á¦£ nk  + dleftkdrightk ¦Ðk = ¦Á¦£ dknk 
end for
having built a tree that approximates the posterior distribution over partitions  we can use the tree to compute the posterior probability of an unseen label. assume we have an unlabeled example xk associated with the k-th task. let ak be the set of nodes along the path from the node k to the root in the tree generated by the bhc algorithm  e.g. in figure 1  a1 = {1 1} . note that the elements ti ¡Ê ak correspond to all clusters that task k participates in across all treeconsistent partitions. our predictive distribution for y will then be the weighted average of the predictive distributions for each partition:

where is the predictive distribution under the base model after combining the data from all the tasks in cluster k.
¡¡while the computational complexity of posterior computation is quadratic in the number of tasks  heller and ghahramani  1b  have proposed o nlogn  and o n  randomized variants.
1 results
the utility of the type of sharing that the clustered naive bayes supports can only be assessed on real data sets. to that end  we evaluated the model's predictions on a meeting classification data set collected by rosenstein et al.  1 . the data set is split across 1 users from multiple universities  an industry lab and a military training exercise. in total  there are 1 labeled meeting requests  with 1 meeting requests per user. in the meeting acceptance task  we aim to predict whether a user would accept or reject an unseen meeting request based on a small set of features that describe various aspects of the meeting.
¡¡to evaluate the clustered model  we assessed its predictive performance in a transfer learning setting  predicting labels for a user with sparse data  having observed all the labeled data for the remaining users. in particular  we calculated the receiver-operator characteristic  roc  curve having trained on 1 1 1 1  and 1 training examples for each user  conditioned on knowledge of all labels for the remaining users . each curve was generated according to results from twenty random partitions of the users' data into training and testing sets. figure 1 plots the area under the roc curve as a measure of classification performance versus the number of training examples.

figure 1: area under the curve  auc  vs. training size for five representative users. the auc varies between 1  always correct   1  always wrong   and 1  chance . for each experiment we label the  map  cluster of users to which the user belongs. if the cluster remains the same for several experiments  we omit all but the first mention. the first three examples illustrate improved predictive performance. the last two examples demonstrate that it is possible for performance to drop below that of the baseline model.
mmmmmmmmppppsspsspppp mmmmmmmmppppppppspsss mmmmmmmmppppppsppspss mmmmmmmmpppppppsspsps
figure 1: progression of trees found by bhc for 1  1  1 and 1 examples per user. short vertical edges indicate that two tasks are strongly related. long vertical edges indicate that the tasks are unrelated. key:  m ilitary   p rofessor   s ri researcher.
¡¡from the 1 users  we have selected five representative samples. the first three examples  users 1  1 and 1  show how the model performs when it is able to use related user's data to make predictions. with a single labeled data point  the model groups user 1 with two other military personnel  users 1 and 1 . while at each step the model makes predictions by averaging over all tree-consistent partitions  the map partition listed in the figure has the largest contribution. for user 1  the map partition changes at each step  providing superior predictive performance. however  for the third user in the second figure  the model chooses and sticks with the map partition that groups the first and third user. in the third example  user 1 is grouped with user 1 initially  and then again later on. roughly one third of the users witnessed improved initial performance that tapered off as the number of examples grew.
¡¡the fourth example  user 1  illustrates that  in some cases  the initial performancefor a user with very few samples is not improved because there are no good candidate related users with which to cluster. finally  the last example shows one of the four cases where predictions using the clustered model leads to worse performance. in this specific case  the model groups the user 1 with user 1. it is not until 1 samples that the model recovers from this mistake and achieves equal performance.
¡¡figure 1 shows the trees and corresponding partitions recovered by the bhc algorithm as the number of training examples for each user is increased. inspecting the partitions  they fall along understandable lines; military personnel are

figure 1: the clustered model has more area under the roc curve than the standard model when there is less data available. after 1 training examples  the standard model has enough data to match the performance of the clustered model. dotted lines are standard error.
most often grouped with other military personnel  and professors and sri researchers are grouped together until there is enough data to warrant splitting them apart.
¡¡figure 1 shows the relative performance of the clustered versus standard naive bayes model. the clustered variant outperforms the standard model when faced with very few examples. after 1 examples  the models perform roughly equivalently  although the standard model enjoys a slight advantage that does not grow with more examples.
1 related work
some of the earliest work related to transfer learning focused on sequential transfer in neural networks  using weights from networks trained on related data to bias the learning of networks on novel tasks  caruana  1; pratt  1 . more recently  these ideas have been applied to modern supervised learning algorithms  like support vector machines  wu and dietterich  1 . more work must be done to understand the connection between these approaches and the kind of sharing one can expect from the clustered naive bayes model.
¡¡this work is related to a large body of transfer learning research conducted in the hierarchical bayesian framework  in which common prior distributions are used to tie together model components across multiple data sets. the clustered model can be seen as an extension of the model first presented by rosenstein et al.  1  for achieving transfer with the naive bayes model. in that work  they fit a dirichlet distribution for each shared parameter across all users. unfortunately  because the dirichlet distribution cannot fit arbitrary bimodal distributions  the model cannothandle more than one cluster  i.e. each parameter is shared completely on not at all. the model presented in this paper can handle any number of users by modelling the density over parameters using a dirichlet process prior. it is possible to loosely interpret the resulting clustered naive bayes model as grouping tasks based on a marginal likelihood metric. from this viewpoint  this work is related to transfer-learning research which aims to first determine which tasks are relevant before attempting transfer  thrun and o'sullivan  1 .
¡¡ferguson  1  was the first to study the dirichlet process and show that it can  simply speaking  model any other distribution arbitrarily closely. the dirichlet process has been successfully applied to generative models of documents  blei et al.  1   genes  dahl  1   and visual scenes  sudderth et al.  1 . teh et al.  1  introduced the hierarchical dirichlet process  which achieves transfer in document modeling across multiple corpora. the work closest in spirit to this paper was presented recently by xue et al.  1 . they investigate coupling the parameters of multiple logistic regression models together using the dirichlet process prior and derive a variational method for performing inference. in the same way  the clustered naive bayes model we introduce uses a dirichlet process prior to tie the parameters of several naive bayes models together for the purpose of transfer learning. there are several important differences: first  the logistic regression model is discriminative  meaning that it does not model the distribution of the inputs. instead  it only models the distribution of the output labels conditioned on the inputs. as a result  it cannot take advantage of unlabeled data. second  in the clustered naive bayes model  the data sets are clustered with respect to a generative model which defines a probability distribution over both the inputs. as a result  the clustered naive bayes model could be used in a semi-supervised setting. implicit in this choice is the assumption that similar feature distributions are associated with similar predictive distributions. this assumption must be judged for each task: for the meeting acceptance task  the generative model of sharing is appropriate and leads to improvedresults.
1 conclusion
the central goal of this paper was to evaluate the clustered naive bayes model in a transfer-learning setting. to evaluate the model  we measured its performance on a real-world meeting acceptance task  and showed that the clustered model can use related users' data to provide better prediction even with very few examples.
¡¡the clustered naive bayes model uses a dirichlet process prior to couple the parameters of several models applied to separate tasks. this approach is immediately applicable to any collection of tasks whose data are modelled by the same parameterized family of distributions  whether those models are generative or discriminative. this paper suggests that clustering parameters with the dirichlet process is worthwhile and can improve prediction performance in situations where we are presented with multiple  related tasks. a theoretical question that deserves attention is whether we can get improved generalization bounds using this technique. a logical next step is to investigate this model of sharing on more sophisticated base models and to relax the assumption that users are exactly identical.
