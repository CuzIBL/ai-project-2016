 
traditional single-agent search algorithms usually make simplifying assumptions  single search agent  stationary target  complete knowledge of the state  and sufficient time . there are algorithms for relaxing one or two of these constraints; in this paper we want to relax all four. the application domain is to have multiple search agents cooperate to pursue and capture a moving target. agents are allowed to communicate with each other. for solving multiple agents moving target  mamt  applications  we present a framework for specifying a family of suitable search algorithms. this paper investigates several effective approaches for solving problem instances in this domain. 
1 introduction 
in the 1 steven spielberg movie minority report  john anderton  played by tom cruise  is on the run. in one sequence  anderton is hiding in a building  and his pursuers unleash a team of mini-robots to flush him out. the robot team separates  each covering a different part of the building. anderton  realizing the danger  stops fleeing and comes up with a unique solution - he submerges himself in a bathtub of water so as to avoid the robotic detectors. sadly  he can only hold his breath for so long  before he has to emerge and is found by the robots. 
　the classic algorithms  such as a*  are effective for solving search problems that satisfy the properties: one search agent  the agent has perfect information about the environment  the environment and goal state do not change  and enough time is given to make an optimal decision. relaxing even one of the assumptions gives rise to new algorithms  e.g.  moving target search  ishida and korf  1    real-time search  korf  1   and d*  stentz  1  . many realworld problems do not satisfy all of these properties. in this paper we use an application domain that breaks all of them. 
　consider the task of multiple agents having to pursue and capture a moving target; for example a squad of policemen chasing a villain. we want to design a test environment that is as realistic as possible. we assume a grid with obstacles. agents can only  see  what is directly visible to them. agents are allowed to communicate with any agent that they can see. as the target flees  it may become obscured from sight. 
　the agent's knowledge of the locations of the target and other agents may be fuzzy  since some of them may be hidden from sight . given whatever knowledge of the target and other agents is available  an agent must decide how to pursue the target. the target's possible locations provide information of where to search; the other agents' possible locations provide information that can be used to coordinate the search effort. the challenge is to have the agents act autonomously to catch the target as quickly as possible. 
　this paper makes the following contributions: mamt a challenging application domain for exploring issues related to multiple agents pursuing a moving target  a framework for expressing real-time search algorithms for the mamt domain  and several solutions that allow an agent to act autonomously  using information about the possible positions of the target and other agents in the decision-making process . 
　more details are available at www. cs . u a l b e r t a . ca/ ~ j o n a t h a n / p a p e r s / a i . 1 1 . h t m l . 
1 literature 
there are a family of a* algorithms that relax solution optimally by requiring the agent to make the best decision possible given limited search resources  e.g.  time   korf  1 . the minimin lookahead search algorithm uses a fixed-depth search  d moves   keeping track of the moves that lead to the  heuristically  best d-move outcome. real-time a*  rta*  is an a* variant that uses the results produced by the minimin lookahead search as heuristic values in order to guide the search towards achieving the goal  korf  1 . moving tar-
get search  mts  is an lrta* variant that allows a moving target  ishida and korf  1 . an assumption in most mts papers is that the target moves slower than the agent. without this requirement  the target can stay ahead of the agent and possibly elude capture. there are many real-time search variants  e.g.  lpa*  koenig and likhachev  1  . for the most part  these are orthogonal to our work. the difficult part for an agent is deciding on the search goal; once achieved  then any of several different search algorithms can be used. 
　having multiple agents participating in a search is a topic of recent interest. robocup is an example  but that work has more limited scope  and agents  players  generally have global knowledge. 

1 	poster papers 


figure 1: framework for mamt solutions 
1 	problem description 
the mamt domain has the following properties. agents and target: multiple agents pursuing a single moving target. grid: m x n in size  with randomly-placed obstacles. all agents and the target have complete knowledge of the grid topology. moving: all moves are horizontal or vertical and arc made simultaneously. starting position: the target always starts in the middle of the grid. the agents are all placed in the lower left corner of the grid. the target is visible to at least one agent. vision: can  see  anything that is in an unobstructed direct line. communication: between moves  agents communicate with any agent that is visible to them. the agents exchange information as to where they believe the target and other agents are located. objective: catch the target in the fewest number of moves. 
1 	multiple agent moving target search 
the intent of this work is not to build a new search algorithm. 
rather  we want to plug standard search algorithms into a framework that  given a goal selection  will find the  best  way to reach the objective. 
　when an agent does not know the exact position of the target  from vision or communication   it must maintain a belief set of where the target might be. the belief set can take into account the topology of the grid  knowledge of the opponent  and the time since the last known target location. as the time increases since the last sighting  the knowledge of where the target is gets fuzzier. an agent should choose its search area based on its beliefs about the target and other agents. 
　figure 1 shows the four-step method that is the framework used for specifying our solution algorithms. there is no  right  way of solving any of these steps. in the following we detail several algorithm alternatives. 
1 	belief set 
whenever an agent knows the exact location of the target  that agent's belief set contains only one location  otherwise it can grow. since this is a real-time search application  and real agents have limited memory  the belief set size is limited. before a move  each agent sends its belief set information about the target and other agents to any agent it can see  who may  in turn forward it to agents that they see . 
　we implemented three strategies for maintaining the belief set. all-scenarios. expand the current belief set to include all possible locations that can be reached in one more move. 
region belief set. this set has the same update as the allscenarios belief set. after the search goal is chosen  the agent commits to only consider beliefs that arc connected to the goal location. single-location: the agent maintains a single belief  one grid square . the belief is updated by choosing a random direction and moving the belief in that direction until an obstacle is encountered or new target information is available. 
we use a simple greedy algorithm to approximate the four 
 corners  of the variable shaped belief set. this subset of the belief set is called the filtered belief set. 
1 	goal selection 
each agent selects a goal from their filtered belief set. randomly selecting a location from this set is an obvious control strategy to implement. however  a more intelligent strategy is needed one that considers information about other agents. 
　the difference metric is used to identify a goal that ideally is  a  closest to the agent and yet  b  farthest from the closest other agent. for each location in the filtered belief set  we compute two metrics: the distance from the agent to the belief  and the minimum distance from the belief to the last known agent positions. these values can be determined by search  expensive  or by heuristics  inaccurate . 
　for each location in the filtered belief set  the agent computes the difference of the above two values  and chooses the one with the minimum difference. the idea is that the agent should assist the other agents by covering the possible escapes of the target that are hard for the other agents to reach. 
1 	search 
given a goal  each agent performs a search to find a  best  move that progresses towards that goal. since this has to be a real-time algorithm  the search algorithm is allocated a fixed number of search nodes  approximating a fixed amount of time per decision . the manhattan distance is used as the search evaluation function. 
　we experimented with two search algorithms. singleagent minimin search  korf  1 : here the agent uses single-agent search to find the shortest path to the goal location. the search has the effect of selecting the move that tries to chase the target. adversarial search: the search can alternate between moves for the agent  move towards the target  and target  move away from the agent . this becomes an alpha-beta search  where the agent tries to minimize the minimax value of the search  the distance from the target . 
1 	comments 
for non-trivial belief sets  as long as information is known about other agent's positions the agents will separate to avoid redundancy in the search. each agent wanders about the grid trying to maximize their coverage  as a function of what they know  about the target and other agents . in many cases  especially for large mazes  an agent may go a long time without getting an update on the target's position  effectively negating the effectiveness of the belief set. 
1 	experiments 
there are a variety of target strategies that can be investigated. the strategy used is a weighted combination of four 

poster papers 	1 

	figure 1: comparing solutions 	figure 1: varying size and # of agents figure 1: varying the obstacle density 

sub-strategies: distance  mobility  visibility  and random. the maximum scoring move is selected. the weights were handtuned based on the perceived realism of the target's behavior. 
　solutions to mamt instances were tested using grids of sizes from 1 x 1 to 1 x 1. grids had obstacles randomly placed  occupying 1% to 1%  in increments of 1%  of the space. the experiments used 1 to 1 agents. each pursuer was allocated 1 search nodes to make its decision. if search was used to compute the difference metric  then half the search nodes were allocated to this task  and the other half to move selection. agents were allowed a maximum belief set size of 1. an experiment ended when one of the pursuers caught the target  or when a maximum number of moves was reached. for an n x n grid  the maximum was set to 1 x n. with few exceptions  the target was caught in less than 1 x n moves  or not at all. 
　figure 1 compares several different solutions. the belief set was maintained using one of: all-scenarios  all   region  and single-location  single . except for a single-location belief set  the choice of goal was done using the difference metric based on either manhattan distance  no search  - d md   single-agent search - d sas   or alpha-beta - d ab . having chosen a goal  single-agent search  sas  or alpha-beta  ab  was used to select the best move. the graph shows the percentage of problems where the target was caught  1 trials with different random seeds per data point  as a function of the maze size. 
　the control experiment is randomly selecting a goal  single-location   and using single-agent search  sas  to decide on the move choice. not surprisingly  this gets poor performance. region was expected to perform quite well  but instead its results were mixed  not shown . 
　a1-d md -sas and a1 d sas  sas performed best in all our experiments  with a preference for the manhattan difference metric. that the difference heuristic gets the best performance is gratifying  since it is better informed by using beliefs about the other agents. using a simple heuristic appears to be as good as or better than using search for determining the  best  search goal. the simplest way of computing the difference metric - using static manhattan distance instead of more accurate search - also leads to shorter solution lengths  up to 1% on average . this shows that it is more beneficial to invest search effort in move selection than in goal selection. 
　alpha-beta is out-performed by single-agent search. since alpha-beta takes into account the target's moves  it cannot reach the search depths that single-agent search can and  hence  usually yields a lesser-quality solution. 
　in cases where the target eluded capture  a familiar pattern emerged. the target would stay hidden in a small area  and the agent's knowledge of where the target was became obsolete and effectively useless. the agents would independently wander about  hoping to find the target. in the real world  if such a scenario arises  the agents should wait until more help arrives and then begin the search anew  going through the entire grid systematically. 
　figure 1 shows that more agents are better than fewer  using all d md  sas . note that as the number of agents increases  the number of nodes per agent per search in the goal selection gets smaller  recall the total search size is limited . even so  adding more agents is beneficial despite less resources available. 
　figure 1 shows that as the mazes become more congested with obstacles  it gets harder for the agents to find the target. essentially  a higher percent of obstacles gives the target more opportunities to hide. however  at 1% of obstacles  our target starts having problems with avoiding the dead ends and is sometimes caught more easily. 
　several control experiments were also done  and they gave predictable results. simplistic targets  e.g.  random  or avoid  were much easier to catch. 
1 	future work and conclusions 
this problem domain is rich in possibilities  and can be extended to increase the  realism  of the simulations. some examples include: multiple moving targets  more realistic communication  creating  human-like  target behavior  and opponent modeling. the framework used of this research has many opportunities for interesting extensions. 
1 	acknowledgments 
this research was supported by nserc  icore  and iris. 