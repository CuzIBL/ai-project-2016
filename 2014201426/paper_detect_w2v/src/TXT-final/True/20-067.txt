 
new reasoning techniques for dealing with uncertainty in expert systems have been embedded in rum  a reasoning with uncertainty module. rum is an integrated software tool based on a frame system  kee  that is implemented in an object oriented language. rum's capabilities are subdivided into three layers: representation  inference  and control. 
the representation layer is based on frame-like data structures that capture the uncertainty information used in the inference layer and the uncertainty mete-information used in the control layer. linguistic probabilities are used to describe the lower and upper bounds of the certainty measure attached to a well formed formula. the source and the conditions under which the information was obtained represent the non-numerical meta-information. the inference layer provides the uncertainty calculi with which to perform the intersection  detachment  union  and pooling of information. five uncertainty calculi  based on their underlying triangular norms  are used in this layer. the control layer uses the meta-information to select the appropriate calculus for each context and to resolve eventual ignorance or conflict in the information. this layer enables the programmer to declaratively express the local  context dependent  meta-knowledge that will substitute for the global assumptions traditionally used in uncertain reasoning. 
rum has been tested in a sequence of experiments in both naval and aerial situation assessment  consisting of correlating reports and tracks  locating and classifying platforms  and identifying intents and threats. 
	i. 	introduction 
in most realistic situations  the information available to the decision maker is incomplete and uncertain. in automated reasoning systems  these two facets of the information have usually been treated independently. theories and techniques for dealing with incomplete  but precise  information have evolved into the development of non-monotonic logics  truth maintenance systems  tms  and reason maintenance systems  rms . 
    this work wis partially supported by the defense advanced research projects agency  darpa  under usaf/rome air development center contract f1-c-1. views and conclusions contained in this paper are those of the authors and should not be interpreted as representing the official opinion or policy of darpa or the u.s. government 
theories and techniques for dealing with uncertain  but complete  information have either been adapted from other fields such as probability theory  based on unrealistic global assumptions  or proposed as ad hoc solution without formal justifications  bonissone  1 . 
     the trend followed by most approaches for reasoning with uncertainty has shown an almost complete disregard for the fundamental issues of automated reasoning  such as the proper representation of the information  the allowable inference paradigms suitable for the representation  and the control of such inferences. the majority of the approaches to reasoning with uncertainty do not properly cover these issues. some approaches lack expressiveness in their representation paradigm. other approaches require unrealistic assumptions to provide uniform combining rules defining the plausible inferences. 
     specifically  the non-numerical approaches  cohen and grinberg  1b  cohen and grinberg  1a  doyle  1  are inadequate to represent and summarize measures of uncertainty. the numerical approaches  on the other hand  generally tend to impose some restrictions upon the type and structure of the information  e.g.  mutual exclusiveness of hypotheses  conditional independence of evidence . most numerical approaches represent uncertainty as a precise quantity  scalar or interval  on a given scale. they require the user or expert to provide a precise yet consistent numerical assessment of the uncertainty of the atomic data and of their relations. the output produced by these systems is the result of laborious computations  guided by well-defined calculi  and appears to be equally precise. however  given the difficulty in consistently eliciting such numerical values from the user  it is clear that these models of uncertainty require an unrealistic level of precision that does not actually represent a real assessment of the uncertainty. 
     with few exceptions  such as mrs  genesereth  1   the control of the inference process in most expert systems has been procedurally embedded in the inference engine  thus preventing any opportunistic and dynamic change in ordering inferences and in aggregating uncertainty. usually  the same set of aggregation operators  i.e.  the same uncertainty calculus  is selected a priori and is used uniformly for any inference made by the expert system. in the few numerical approaches where conflicting information is detected  shafer  1   conflict handling is done in the inference layer  where the conflict resolution procedure is embedded in the same com* 
	bonissone  gans  and oscksr 	1 
bining rules. this procedure consists of simply removing the conflicting part of the information. the non-conflicting portion is then normalized and propagated as if the conflict never existed. 
     it is our claim that the formalism for reasoning with uncertainty must exhibit the same structural  layered  decomposition typical of other automated reasoning methodologies  i.e.  it must address each of the three layers of representation  inference  and control. the formalism must also be based on sound theoretical foundations to guarantee its general applicability to a variety of reasoning tasks. the proposed layered approach will be suitable to integration with reason maintenance systems that provide a distinction between the object logic theory  inference layer  and the meta logic theory  control layer . 
     this paper describes the theory  design  implementation  and testing of rum  a reasoning with uncertainty module  whose layered architecture reflects the above concerns. the next two sections  1 and 1  summarize rum's underlying theory and design. the remaining two sections  1 and 1  describe an experiment in situation assessment used to test rum and present the conclusions of this work. 
	ii. 	rum's underlying theory 
preliminary theoretical results were presented in two previous publications  bonissone and decker  1  bonissone  1 . this section summarizes some of those results and provides a unified framework for their interpretation and use in rum's architecture. a philosophical motivation for the rum's three layer organization can also be found in  bonissone  1. 
a. 	term sets of linguistic probabilities 
in expert system applications  users and experts must frequently provide subjective assessments of probability. due to the difficulty of eliciting precise and consistent numerical certainty values  we have suggested the use of term sets of linguistic probability. each term set determines the granularity  the finest level of specificity  of the measure of certainty that the user/expert can consistently provide. 
a term set of linguistic probabilities is the set of symbols 
 the meaning of each term l  l is 
represented by a fuzzy number on the  1  interval. a computationally efficient way to characterize a fuzzy number is to use a parametric representation ot its membership function. this parametric representation  bonissone  1  bonissone and decker  1  is achieved by the 1-tuple the first two parameters indicate the interval in which the membership value is 1; the third and fourth parameters indicate the left and right width of the distribution. linear functions are used to define the slopes. therefore  the membership function ult x  is defined as 
　　for compactness of notation  we will denote the meaning of the term set element lt parametrically. table 1 illustrates one of four default rum term sets  the nine element l-nine.* 

table 1: the nine element term set l-nine 
     rum's representation layer allows the user to characterize the lower and upper bounds of the certainty of a fact  or the sufficiency and necessity of a rule  by using elements of a selected term set. 
b. 	triangular norms 
triangular norms  t-norms  and triangular conorms  tconorms  are the most general families of binary functions that satisfy the requirements of the conjunction and disjunction operators  respectively  bonissone and decker  1. a 
t-norm is defined as a mapping which is monotonic  commutative  and associative. the boundary conditions of a t-norm satisfy the truth tables of the logical and operator. the t-conorms are defined in terms of the t-norms and a negation operator  by using a generalization of demorgan's duality. thus  for a suitable negation operator  such as 

　　we have seen that the use of term sets determines the granularity with which the input certainty is described. this granularity limits the ability to differentiate between two similar calculi; only a finite  small subset of the infinite number of calculi that can be generated from a parametrized t-norm family produces notably different results. 
     this result has been confirmed by an experiment  bonissone and decker  1  where eleven different calculi of uncertainty  represented by their corresponding t-norms  were analyzed. the experiment showed that five equivalence classes were needed to represent  or reasonably approximate  any tnorm. the corresponding five uncertainty calculi were defined by the common negation operator and the de-

	morgan pair 	for the following values 
1 	reasoning 

table 1: five uncertainty calculi 

of pin table 1:** 
　　rum's inference layer provides the user with a selection of the five t-norm based calculi described above. 
	iii. 	rum's layered architecture 
rum's architecture is based on three layers: representation  inference  and control. the first layer  the representation layer  includes the structure required to capture information used in the inference layer and meta-information used in the control layer. in this structure  linguistic probabilities are used to describe the lower and upper bounds of the certainty measure associated with a well-formed formula  wff . non-numerical meta-information  describing the source and the conditions under which information was obtained  is represented in this layer along with numerical meta-information describing the amount of ignorance and consistency. 
　　the second layer  the inference layer  includes five uncertainty calculi based on their underlying triangular norms  t-norms . any operation required by an uncertainty calculus can be expressed in terms of its t-norm and a negation operator. note that t-norm-based calculi have various computational advantages: they are truth-functional  commutative  and associative. therefore  if numerical computations to evaluate t-norm-based expressions are carried out at run-time  the above properties ensure that any result can be directly computed front the individual value of each argument; that the result is independent from the order of the arguments; and that for more than two arguments  the evaluation of t-norm expressions can be done recursively. 
     the third layer  the control layer  includes the functions required to select the calculus appropriate for each context and to resolve ignorance or conflict in the information. these functions rely on local  i.e.  context-dependent  knowledge about the information  meta-knowledge . the scope of the calculus selection and ignorance/conflict resolution is limited to the context  knowledge base subset  for which the meta-knowledge is available. figure 1 illustrates rum's architecture. the following sections describe rum's functions attached to each of the three layers. 
is one of six parametrized families of t-norms discussed in 
 bonissone and decker  1 . it was originally defined by schweizer and sklar  schweizer and sklar  1  and exhibits both complete coverage of the t-norm space and numerical stability. 
a. 	the representation layer 
1. rum's wff system 
　　rum is an integrated software tool based on a frame system  kee tm    that is implemented in an object-oriented language. rum's wff system modifies kee's representation of a wff. rum's wff is the pair which is the description of a variable in the problem domain. for each wff  a corresponding certainty unit is created. the unit contains a list of the considered values that the variable may take  and for each of these values it maintains the lower and upper bounds of its certainty  an ignorance measure  a consistency measure  and the evidence source. the ignorance measure is computed as the area of the fuzzy interval formed by the lower and upper bounds  while the consistency measure checks to see if the lower and upper bounds have crossed. 
　　rum's wff system allows the user to express arbitrary uncertainty granularity by providing the flexibility to mix precise and imprecise measures of certainty  crisp numbers or intervals  fuzzy numbers or intervals  linguistic values  in defining both the input certainties and the rule strengths. 
1. rum's rule system 
　　rum's rule system replaces kee rule system-1 capabilities by incorporating uncertainty information in the inference scheme. the uncertain information is described in the certainty units of the wffs  represented in rum's wff system  and in the degrees of necessity and sufficiency attached to each rule. a rule is represented by a frame with several slots. these slots include the name of the rule; the lists of contexts  premises  and conclusions; the rule's sufficiency and necessity; and the t-norm to be used for aggregation. all slots  except the name  premises  and consequences  have default values. the contexts  premises  and conclusions can comprise values  variables  rum predicates and arbitrary lisp functions. rules with unbound variables art instantiated with the necessary environment to produce rule instances. 
     the t-norm specified with the rule is used to aggregate the certainties of the rule premises and to perform detachment  which computes the certainty of the conclusion given the sufficiency and necessity of the rule . it defaults to which is the min function. the associated t-conorm is used to aggregate the certainties of identical conclusions inferred by multiple rule instances derived from the same rule. these are often subsumptive  and the value defaults to  the max function. finally  each seperate consequence of a rule has a specified 
t-conorm that will be used to aggregate the consequence with 
bonimorw  gans  and decktr 1 


figure 1: rum's three layer architecture 
identical consequences derived from different rules. 
b. the inference layer 
	1. 	calculi operations 
　　for each calculus  four operations are defined in rum's rule system: premise evaluation  conclusion detachment  conclusion aggregation  and source consensus. each operation in a calculus can be completely defined by a triangular norm and a negation operator  in logic any boolean expression can be rewritten in terms of an intersection and complementation operator . the four operations are defined as follows: 
　　premise evaluation: the premise evaluation operation determines the degree to which all the clauses in the rule premise have been satisfied by the matching wffs. let bt and bt indi-
cate the lower and upper bounds of the certainty of condition 
i in the premise of a given rule. then the aggregated premise certainty range  b  b  is defined as 　　the degrees of sufficiency and necessity  respectively  indicate the amount of certainty with which the rule premise implies its conclusion and vice versa. the sufficiency degree is used with modus ponens to provide the lower bound of the 
conclusion. the necessity degree is used with modus tollens to obtain a lower bound for the complement of the conclusion  which can be transformed into an upper bound for the 
conclusion itself . 
     conclusion aggregation: the conclusion aggregation operation determines the consolidated degree to which the conclusion is believed if supported by more than one path in the rule deduction graph  i.e.  by more than one rule instance. it is also possible to have various groups of deductive paths  i.e.  various sets of rule instances  all supporting the same conclusion. each group of deductive paths can have a distinct conclusion aggregation operator associated with it. let the ranges indicate the certainty lower and upper bounds of the same conclusion inferred by various rule instances be-longing to the same group. then  for each group of deductive paths  the range  d  d  of the aggregated conclusion is defined 
　　conclusion detachment: the conclusion detachment operation indicates the certainty with which the conclusion can as and necessity  respectively  of the given rule  and let  b  b  be the computed premise certainty range. then the range 
indicating the lower and upper bound for the certainty of the conclusion inferred by such a rule  is defined as 
　　rum distinguishes between rule instances generated from the same rule and rule instances derived from different rules. rule instances generated from the same rule are aggregated first  to take into account the normally large amount of redundancy that such instances entail. rule instances derived be asserted  given the strength and appropriateness of the rule. let s and n be the lower bounds of the degree of sufficiency 
1 

from different rules are subsequently aggregated taking into account the knowledge about the presence or lack of positive or negative correlation that characterizes the various rules. 
　　source consensus: the source consensus operation reflects the fusion of the certainty measures of the same evidence a provided by different sources. the evidence can be an observed fact  or a deduced fact. in the former case  the fusion occurs before the evidence is used as an input in the deduction process. in the latter case  the fusion occurs after the evidence has been aggregated by each group of deductive paths. the source consensus operation reduces the ignorance about the certainty of a  by producing an interval that is always smaller or equal to the smallest interval provided by any of the information sources. if there is an inconsistency among some of the sources  the resulting certainty intervals will be disjoint  thus introducing a conflict into the aggregated result. 
the certainty lower and upper bounds of the same conclusion provided by different sources of information. the result  obtained from fusing all the assertions about a  is calculated by taking the intersection of the certainty intervals: 

c. 	the control layer 
	1. 	calculi selection 
　　rum's rule system uses a set of t-norm-based calculi to handle uncertain information. the calculus used by each rule instance is inherited from the rule subclass. the calculus can be modified through kee's user interface or programmatically  e.g.  by an active value or a task  see section 1. . class inheritance can also be used to modify the degree of sufficiency and necessity of all the rule members of the same class. 
the calculi selection process consists of two assignments. 
the first assignment indicates the t-norm with which the premise evaluation and the conclusion detachment will be computed. such an assignment is made for each rule and is passed  through inheritance  to all rule instances derived from a rule. 
　　the second assignment indicates the t-conorm  represented by its dual t-norm  with which the conclusion aggregation will be computed. this assignment is made for each subset of rule instances generated from different rules that assert the same conclusion. 
   the characteristics of a t-norm will determine how it is used. the t-norm assigned to each rule for premise evaluation and conclusion detachment will be a function of the decision maker's attitude toward risk. the ordering of the t-norms  which is identical to the ordering of parameter p in the schweizer and sklar family of t-norms  reflects the ordering from a conservative attitude to a nonconservative one from the definition of the calculi operations  we can see that t  will generate the smallest premise evaluation and the weakest conclusion detachment  i.e.  the widest uncertainty interval attached to the rule's conclusion.  t-norms generated by larger values of p will exhibit less drastic behaviors and will produce nested intervals with their detachment operations.  will generate the largest premise evaluation and the strongest conclusion detachment  the smallest certainty interval . 
   for the second assignment  the t-norm that aggregates the subsets of rule instances  derived from different rules and asserting the same conclusion  will be a function of the lack or presence of positive/negative correlation among the rules in each subset. the ordering of the t-norms reflects the transition from the case of extreme negative correlation or mutual exclusiveness  through independence i t o the case of extreme positive correlation or subsumption 
　　in all the assignments  a set of selection rules will express the meta-knowledge about the context  i.e  the task's relevance to the decision maker and the subsets of deduction rules used to solve that task  see section 1. . the selection rules will select the t-norms that better reflect the desired attitude toward risk and the perceived amount of correlation to be used in such a context. 
1. belief revision 
     an initial implementation of the belief revision of uncertain information is available in the control layer of rum's rule system. for any conclusion made by a rule  the belief revision mechanism monitors the changes in the certainty measures of the wffs that constitute the conclusion's support and the changes in the calculus used to compute the cached conclusion certainty measure. validity flags are inexpensively propagated through the rule deduction graph  which includes both wffs and rules. five flag values are used: good  guaranteeing validity; bad  level i   indicating unreliability propagated to the ith level; inconsistent  indicating conflict; not applicable  indicating that the rule context is not active; and 
ignorant  indicating that the information is too vague to be useful. 
     the belief revision system offers both backward and forward processing. a lazy evaluation  running in backward mode  recomputes the certainty measures of the modified wffs that are required to answer a given query. this mode   called reasoning under pressure  is used when the system or the user decide that they are dealing with time-critical tasks. breadthfirst  forward mode processing recomputes the certainty measures of the modified wffs  attempting to restore the integrity of the rule deduction graph. this mode is used by the system when time is not critical. 
1. rule firing control via context activation 
     a rule context is defined as a set of conditions that must be satisfied before the rule can be considered for premise evaluation. a user-definable threshold can be attached to each rule context  either by local definition or by inheritance from a rule class. the semantics of a context c attached to an inference rule  establishing the weak logical equivalence between a and b  is given by the following expression: 

     it is important to note that the inference symbol  in the production rule  is interpreted as a  weak  material 
	bonlssone  gans  and decker 	1 
implication operator in multiple-valued logics. the value s is the lower bound of the degree of sufficiency of the implication. this is different from the idea of conditioning  i.e.  
 in the production rule is in-
terpreted as a  weak  logical equivalence operator in multiplevalued logics  where s and n are the lower bounds of sufficiency and necessity  respectively. this  weak  logical equivalence is an if-and-only-if rule that could be decomposed in the following two rules:  equivalent to 
rum's rules are of the type where c indicates the context of the rule and -  represents a strong material implication. 
the context mechanism provides the following features: 
1. by activating/deactivating subsets of the kb  it limits the number of rules that will be considered relevant at any given time  thus increasing the overall system efficiency. 
1. by only considering the rules relevant to a given situation  it allows the knowledge engineer to effectively use the necessary conditions in the rule's premise. it is now possible to distinguish between the failure of a necessary test  described in the premise  and the failure of the rule's applicability  traditionally described by other clauses in the same premise and now explicitly represented in the context . 
1. by using predicates on the control-level wffs  it provides the required programmability for defining flexible control strategies  such as causing sequences of rules to be executed  firing default rules  ordering and handling timedependent information  etc. 
1. by using hierarchical contexts  an organizing principle is created for easing knowledge acquisition. 
1. rule tracing 
　　a user-definable threshold pair can be attached to each rule  either by local definition or by inheritance from a rule class. the threshold pair defines a lower and upper bound of certainty to be compared with the the certainty interval of the detached conclusion. if either threshold is exceeded  the firing mechanism will execute any user-defined lisp function attached to the rule  before the conclusion is detached. this property will enable the user to build any desired tracing facilities  or to create markers for use by the contexts of other rules. 
iv. testing rum 
rum's capabilities were first tested in october 1  as part of an experiment in simulated naval situation assessment. figure 1 illustrates the configuration of the components used in such experiment. 
a. 	an object-based simulation environment 
the simulation of the scenario was implemented in lotta  an object-oriented symbolic battle management simulator that 
1 
maintains time-varying situations in a multi-player antagonistic game  bonissone et al.t 1 . a development environment centered around lotta was the testbed used to test the new techniques in reasoning with uncertainty. the development environment is composed of four basic modules: the window manager  a map-like window-oriented user interface; the annotation system  an intelligent database for lotta; lotta  the simulator that executes commands and maintains internal states; and keela   kee to lotta interface  the link between lotta and rum. 
b. information fusion and situation assessment 
the situation assessment problem is composed of several tasks in which uncertainty pervades both the input data and the knowledge bases. given a platform  aircraft  ship  tank  in a potentially hostile environment  the process of situation assessment consists of the following tasks: 
  sensor data must be collected from various sources and described as reports 
  time-stamped sensor reports must be consolidated into tracks  each track is the trace of an object followed by a given sensor  
  tracks associated to the same object must be fused into a platform 
  the detected platform must be classified and identified  by class and type  
  node organization  formation of the identified platforms   use of special equipment  and maneuvering must be recognized 
  using the knowledge of the opponent's doctrines and rules of engagement  the recognized formation and observed use of special equipment must be explained by a probable intent  which is then translated into a threat assessment. 
     the first four tasks constitute what is generally known as information fusion  and they define the scope of the simplified example described in the next section. 
c. 	a simplified example in information fusion 
in one experiment  a modified version of the naval situation assessment scenario used by nosc to test stammer and stammer1  mccall et al.  1  j.p. ferranti  1  was created. in it  a single missile cruiser faced three platforms that were either patrol hydrofoils or merchant ships. the cruiser's task was to track  correlate  and classify each detected object. both passive and active sensors on the cruiser were run twice  generating sensor reports which were grouped into tracks for each sensor. plausible correlations were then made between tracks to group them into detected platforms. using the rum knowledge base  see the example in the next section  on this information  the platforms were correctly identified. 


figure 1: architecture for simulated naval situation assessment 

	1. 	example of a rum rule 
　　the rum knowledge base  kb  used in this example is composed of approximately forty rules  each of which can be instantiated by new sensor reports  new tracks  or new platforms. a representative sample of such a kb is provided by the following rule. 
english version of rule-1  identifying submarines : 
assuming that sonar was used to generate a sensor report  which together with other reports generated by the same sensor has been attached to a track associated with a platform   if the detected platform has a low noise emission  and is located at a depth of at least twenty meters  then it is extremely likely that the detected platform is a submarine. otherwise  the detected platform may not be a submarine. 
rum's version of the same rule: 

	v. 	remarks and conclusions 
we have implemented a layered architecture for reasoning with uncertainty. in the representation layer we have used framelike data structures attached to each variable. each frame contains a list of the values that were considered for the variable. 
for each of these values  rum maintains uncertainty information  such as the lower and upper bounds  and uncertainty meta-information  such as a measure of ignorance  a measure of consistency  and the evidence source. 
     in the inference layer  we have used t-norm based calculi that are truth functional and associative. the truth functionality of the calculi entails low computational complexity and relatively inexpensive belief revision. the associativity of the calculi entails recursive problem decomposition. the formulae for detachment have been derived from multiple-valued logics  by interpreting the inference symbol of the production rules as material implication  rather than conditioning. instead of adopting a unique calculus  based on global assumptions  and uniformly using it in the knowledge base  we can select any of the five t-norm based calculi and locally apply them to any given rule subset. 
　　in the control layer  we have implemented the capabilities of selecting the appropriate calculus based on the calculi characteristics  context independent information  and on the available meta-information describing the context and the user's attitude toward risk  context dependent information . we have also implemented a belief revision mechanism that can be frugal for time-critical situations  operating in a depthfirst backward mode  or exhaustive  operating in a breadth-first forward mode . rule firing is controlled via a context activation mechanism that reduces the number of rules that will be considered relevant at any given time. 
　　future work on rum will focus on extensions of the control layer capabilities. specifically  we will define a metalanguage for describing the policies  i.e.  meta-rules  for cal-
	bonissone  gans  and dtcker 	1 

cuius selection  ignorance reduction  and conflict removal; we will also extend the meta-language to enable the use of the context mechanism from the meta-rules. finally  we will start addressing real-time requirements by incorporating the timeliness of information as another constraint in the control of reasoning represented in the control layer. 
