 
the computation of the first complete approximations of game-theoretic optimal strategies for fullscale poker is addressed. several abstraction techniques are combined to represent the game of 1player texas hold'em  having size o 1   using closely related models each having size 1o1 . despite the reduction in size by a factor of 1 billion  the resulting models retain the key properties and structure of the real game. linear programming solutions to the abstracted game are used to create substantially improved poker-playing programs  able to defeat strong human players and be competitive against world-class opponents. 
1 introduction 
mathematical game theory was introduced by john von neumann in the 1s  and has since become one of the foundations of modern economics  von neumann and morgenstern  1. von neumann used the game of poker as a basic model for 1-player zero-sum adversarial games  and proved the first fundamental result  the famous minimax theorem. a few years later  john nash added results for iv-player noncooperative games  for which he later won the nobel prize  nash  1 . many decision problems can be modeled using game theory  and it has been employed in a wide variety of domains in recent years. 
　of particular interest is the existence of optimal solutions  or nash equilibria. an optimal solution provides a randomized mixed strategy  basically a recipe of how to play in each possible situation. using this strategy ensures that an agent will obtain at least the game-theoretic value of the game  regardless of the opponent's strategy. unfortunately  finding exact optimal solutions is limited to relatively small problem sizes  and is not practical for most real domains. 
　this paper explores the use of highly abstracted mathematical models which capture the most essential properties of the real domain  such that an exact solution to the smaller problem provides a useful approximation of an optimal strategy for the real domain. the application domain used is the game of poker  specifically texas hold'em  the most popular form of casino poker and the poker variant used to determine the world champion at the annual world series of poker. 
multiagent systems 
　due to the computational limitations involved  only simplified poker variations have been solved in the past  e.g.  kuhn  1; sakaguchi and sakai  1 . while these are of theoretical interest  the same methods are not feasible for real games  which are too large by many orders of magnitude   roller and pfeffer  1j . 
　 shi and littman  1  investigated abstraction techniques to reduce the large search space and complexity of the problem  using a simplified variant of poker.  takusagawa  1  created near-optimal strategies for the play of three specific hold'em flops and betting sequences.  selby  1  computed an optimal solution for the abbreviated game of 
pre flop hold1 em. 
　using new abstraction techniques  we have produced viable  pseudo-optimal  strategies for the game of 1-player texas hold'em. the resulting poker-playing programs have demonstrated a tremendous improvement in performance. whereas the previous best poker programs were easily beaten by any competent human player  the new programs are capable of defeating very strong players  and can hold their own against world-class opposition. 
　although some domain-specific knowledge is an asset in creating accurate reduced-scale models  analogous methods can be developed for many other imperfect information domains and generalized game trees. we describe a general method of problem reformulation that permits the independent solution of sub-trees by estimating the conditional probabilities needed as input for each computation. 
this paper makes the following contributions: 
1. abstraction techniques that can reduce an o 1  poker search space to a manageable 1o1   without losing the most important properties of the game. 
1. a poker-playing program that is a major improvement over previous efforts  and is capable of competing with world-class opposition. 
1 game theory 
game theory encompasses all forms of competition between two or more agents. unlike chess or checkers  poker is a game of imperfect information and chance outcomes. it can be represented with an imperfect information game tree having chance nodes and decision nodes  which are grouped into information sets. 
　
　since the nodes in this tree are not independent  divideand-conquer methods for computing sub-trees  such as the alpha-beta algorithm  are not applicable. for a more detailed description of imperfect information game tree structure  see  koller and megiddo  1 . 
　a strategy is a set of riles for choosing an action at every decision node of the tree. in general  this will be a randomized mixed strategy  which is a probability distribution over the various alternatives. a player must use the same policy across all nodes in the same information set  since from that player's perspective they are indistinguishable from each other  differing only in the hidden information component . 
　the conventional method for solving such a problem is to convert the descriptive representation  or extensive form  into a system of linear equations  which is then solved by a linear programming  lp  system such as the simplex algorithm. the optimal solutions are computed simultaneously for all players  ensuring the best worst-case outcome for each player. traditionally  the conversion to normal form was accompanied by an exponential blow-up in the size of the problem  meaning that only very small problem instances could be solved in practice.  roller et ai  1  described an alternate lp representation  called sequence form  which exploits the common property of perfect recall  wherein all players know the preceding history of the game   to obtain a system of equations and unknowns that is only linear in the size of the game tree. this exponential reduction in representation has re-opened the possibility of using game-theoretic analysis for many domains. however  since the game tree itself can be very large  the lp solution method is still limited to moderate problem sizes  normally less than a billion nodes . 
1 texas holctem 
a game  or hand  of texas hold'em consists of four stages  each followed by a round of betting: 
　preflop: each player is dealt two private cards face down  the hole cards . 
　flop: three community cards  shared by all players  are dealt face up. 
t irn: a single community card is dealt face up. 
river: a final community card is dealt face up. 
　after the betting  all active players reveal their hole cards for the showdown. the player with the best five-card poker hand formed from their two private cards and the five public cards wins all the money wagered  ties are possible . 
　the game starts off with two forced bets  the blinds  put into the pot. when it is a player's turn to act  they must either bet/raise  increase their investment in the pot   check/call  match what the opponent has bet or raised   or fold  quit and surrender all money contributed to the pot . 
　the best-known non-commercial texas hold'em program is poki. it has been playing online since 1 and has earned an impressive winning record  albeit against generally weak opposition  billings et al.y 1 . the system's abilities are based on enumeration and simulation techniques  expert knowledge  and opponent modeling. the program's weaknesses are easily exploited by strong players  especially in the 1-player game. 
texas 
hold'em 
1  
figure 1: branching factors for hold'em and abstractions. 
1 	abstractions 
texas hold'em has an easily identifiable structure  alternating between chance nodes and betting rounds in four distinct stages. a high-level view of the imperfect information game tree is shown in figure 1. 
　hold'em can be reformulated to produce similar but much smaller games. the objective is to reduce the scale of the problem without severely altering the fundamental structure of the game  or the resulting optimal strategies. there are many ways of doing this  varying in the overall reduction and in the accuracy of the resulting approximation. 
　some of the most accurate abstractions include suit equivalence isomorphisms  offering a reduction of at most a factor of 1! = 1   rank equivalence  only under certain conditions   and rank near-equivalence. the optimal solutions to these abstracted problems will either be exactly the same or will have a small bounded error  which we refer to as near-optimal solutions. unfortunately  the abstractions which produce an exact or near-exact reformulation do not produce the very large reductions required to make full-scale poker tractable. 
　a common method for controlling the game size is deck reduction. using less than the standard 1-card deck greatly reduces the branching factor at chance nodes. other methods include reducing the number of cards in a player's hand  e.g. from a 1-card hand to a 1-card hand   and reducing the number of board cards  e.g. a 1-card flop   as was done by ishi and littman  1  for the game of rhode island hold'em.  koller and pfeffer  1  used such parameters to generate a wide variety of tractable games to solve with their gala system. 
　we have used a number of small and intermediate sized games  ranging from eight cards  two suits  four ranks  to 1 cards  three suits  eight ranks  for the purpose of studying abstraction methods  comparing the results with known exact or near-optimal solutions. however  these smaller games are not suitable for use as an approximation for texas hold'em  as the underlying structures of the games are different. to produce good playing strategies for full-scale poker  we look for abstractions of the real game which do not alter that basic 
　
1 	multiagent systems 
　
structure. 
　the abstraction techniques used in practice are powerful in terms of reducing the problem size  and subsume those previously mentioned. however  since they are also much cruder  we call their solutions pseudo-optimal  to emphasize that there is no guarantee that the resulting approximations will be accurate  or even reasonable. some will be low-risk propositions  while others will require empirical testing to determine if they have merit. 
1 	betting round reduction 
the standard rules of limit hold'em allow for a maximum of four bets per player per round.l thus in 1-player limit poker there are 1 possible betting sequences  of which two do not occur in practice.1 of the remaining 1 sequences  1 end in a fold  leading to a terminal node in the game tree   and 1 end in a call  carrying forward to the next chance node . using 

and capital letters for the second player  the tree of possible betting sequences for each round is: 

　we call this local collection of decision nodes a betting tree  and represent it diagramatically with a triangle. 
　with betting round reduction  each player is allowed a maximum of three bets per round  thereby eliminating the last two sequences in each line. the effective branching factor of the betting tree is reduced from nine to seven. this does not appear to have a substantial effect on play  or on the expected value  ev  for each player. this observation has been verified experimentally. in contrast  we computed the corresponding postflop models with a maximum of two bets per player per round  and found radical changes to the optimal strategies  strongly suggesting that that level of abstraction is not safe. 
1 	elimination of betting rounds 
large reductions in the size of a poker game tree can be obtained by elimination of betting rounds. there are several ways to do this  and they generally have a significant impact on the nature of the game. first  the game may be truncated  by eliminating the last round or rounds. in hold'em  ignoring the last board card and the final betting round produces a 1-round model of the actual 1-round game. the solution to the 1-round model loses some of the subtlety involved in the true optimal strategy  but the degradation applies primarily to advanced tactics on the turn. there is a smaller effect on the flop strategy  and the strategy for the first betting round may have no significant changes  since it incorporates all the outcomes of two future betting rounds. we use this particular abstraction to define an appropriate strategy for play in the first round  and thus call it a preflop model  see figure 1 . 
   'some rules allow unlimited raises when only two players arc involved. however  occasions with more than three legitimate raises are relatively rare  and do not greatly alter an optimal strategy. 
   technically  a player may fold even though there is no outstanding bet. this is logically dominated by not folding  and therefore does not occur in an optimal strategy  and is almost never seen in practice. 
multiagent systems 
　the effect of truncation can be lessened through the use of expected value leaf nodes. instead of ending the game abruptly and awarding the pot to the strongest hand at that moment  we compute an average conclusion over all possible chance outcomes. for a 1-round model ending on the turn  we roll-out all 1 possible river cards  assuming no further betting  or alternately  assuming one bet per player for the last round . each player is awarded a fraction of the pot  corresponding to their probability of winning the hand. in a 1round preflop model  we roll-out all 1-card combinations of the turn and river. 
　the most extreme form of truncation results in a 1 -round model  with no foresight of future betting rounds. since each future round provides a refinement to the approximation  this will not reflect a correct strategy for the real game. in particular  betting plans that extend over more than one round  such as deferring the raise of a very strong hand  are lost entirely. nevertheless  even these simplistic models can be useful when combined with expected value leaf nodes. 
　alex selby computed an optimal solution for the game of preflop hold'em  which consists of only the first betting round followed by an ev roll-out of the five board cards to determine the winner  selby  1 . although there are some serious limitations in the strategy based on this 1-round model  we have incorporated the selby preflop system into one of our programs  psoptil  as described later in this section. 
　in contrast to truncating rounds  we can bypass certain early stages of the game. we frequently use postflop models  which ignore the preflop betting round  and use a single fixed flop of three cards  sec figure 1 . 
　it is natural to consider the idea of independent betting rounds  where each phase of the game is treated in isolation. unfortunately  the betting history from previous rounds will almost always contain contextual information that is critical to making appropriate decisions. the probability distribution over the hands for each player is strongly dependent on the path that led to that decision point  so it cannot be ignored without risking a considerable loss of information. however  the naive independence assumption can be viable in certain circumstances  and we do implicitly use it in the design of psoptil to bridge the gap between the 1-round preflop model and the 1-round postflop model. 
　another possible abstraction we explored was merging two or more rounds into a single round  such as creating a combined 1-card turn/river. however  it is not clear what the appropriate bet size should be for this composite round. in any case  the solutions for these models  over a full range of possible bet sizes   all turned out to be substantially different from their 1-round counterparts  and the method was therefore re-
jected. 
1 	composition of preflop and postflop models 
although the nodes of an imperfect information game tree are not independent in general  some decomposition is possible. for example  the sub-trees resulting from different preflop betting sequences can no longer have nodes that belong to the 
　
same information set.1 the sub-trees for our postflop models can be computed in isolation  provided that the appropriate preconditions are given as input. unfortunately  knowing the correct conditional probabilities would normally entail solving the whole game  so there would be no advantage to the decomposition. 
　for simple postflop models  we dispense with the prior probabilities. for the postflop models used in psoptio and psoptil  we simply ignore the implications of the preflop betting actions  and assume a uniform distribution over all possible hands for each player. different postflop solutions were computed for initial pot sizes of two  four  six  and eight bets  corresponding to preflop sequences with zero  one  two  or three raises  but ignoring which player initially made each raise . in psoptil  the four postflop solutions are simply appended to the selby preflop strategy  figure 1 . although these simplifying assumptions are technically wrong  the resulting play is still surprisingly effective. 
　a better way to compose postflop models is to estimate the conditional probabilities  using the solution to a preflop model. with a tractable preflop model  we have a means of estimating an appropriate strategy at the root  and thereby determine the consequent probability distributions. 
　in psoptil  a 1-round preflop model was designed and solved. the resulting pseudo-optimal strategy for the preflop  which was significantly different from the selby strategy  was used to determine the corresponding distribution of hands for each player in each context. this provided the necessary input parameters for each of the seven preflop betting sequences that carry over to the flop stage. since each of these postflop models has been given  an approximation of  the perfect recall knowledge of the full game  they are fully compatible with each other  and are properly integrated under the umbrella of the preflop model  figure 1 . in theory  this should be equivalent to computing the much larger tree  but it is limited by the accuracy and appropriateness of the proposed preflop betting model. 
1 	abstraction by bucketing 
the most important method of abstraction for the computation of our pseudo-optimal strategies is called bucketing. this is an extension of the natural and intuitive concept that has been applied many times in previous research  e.g.  sklansky and malmuth  1   takusagawa  1  ishi and littman  1  . the set of all possible hands is partitioned into equivalence classes  also called buckets or bins . a many-to-one mapping function determines which hands will be grouped together. ideally  the hands should be grouped according to strategic similarity  meaning that they can all be played in a similar manner without much loss in ev. 
if every hand was played with a particular pure strategy 
 ie. only one of the available choices   then a perfect mapping function would group all hands that follow the same plan  and 
　　1 to see this  each decision node of the tree can be labeled with all the cards known to that player  and the full path that led to that node. nodes with identical labels differ only in the hidden information  and are therefore in the same information set. since the betting history is different for these sub-trees  none of the nodes are interdependent. 
preflop psoptil 
flop 
turn bet 
river 

figure 1: composition of psoptil and psoptil. 
1 equivalence classes for each player would be sufficient for each betting round. however  since a mixed strategy may be indicated for optimal play in some cases  we would like to group hands that have a similar probability distribution over action plans. 
　one obvious but rather crude bucketing function is to group all hands according to strength  ie. its rank with respect to all possible hands  or the probability of currently being in the lead . this can be improved by considering the roll-out of all future cards  giving an  unweighted  estimate of the chance of winning the hand. 
　however  this is only a one-dimensional view of hand types  in what can be considered to be an iv-dimensional space of strategies  with a vast number of different ways to classify them. a superior practical method would be to project the set of all hands onto a two-dimensional space consisting of  roll-out  hand strength and hand potential  similar to the hand assessment used in poki   billings et al  1  . clusters in the resulting scattergram suggest reasonable groups of hands to be treated similarly. 
　we eventually settled on a simple compromise. with n available buckets  we allocate n - 1 to roll-out hand strength. the number of hand types in each class is not uniform; the classes for the strongest hands are smaller than those for mediocre and weak hands  allowing for better discrimination of the smaller fractions of hands that should be raised or reraised. 
　one special bucket is designated for hands that are low in strength but have high potential  such as good draws to a flush or straight. this plays an important role in identifying good hands to use for bluffing  known as semi-bluffs in  sklansky and malmuth  1  . comparing postflop solutions that use six strength buckets to solutions with five strength plus one high-potential bucket  we see that most bluffs in the latter are taken from the special bucket  which is sometimes played in the same way as the strongest bucket. this confirmed our expectations that the high-potential bucket would improve the selection of hands for various betting tactics  and increase the overall ev. 
　
1 	multiagent systems 
　
original 
bucketing 
transition 
probabilities 
next round bucketing 
figure 1: transition probabilities  six buckets per player . 
　the number of buckets that can be used in conjunction with a 1-round model is very small  typically six or seven for each player  ie. 1 or 1 pairs of bucket assignments . obviously this results in a very coarse-grained abstract game  but it may not be substantially different from the number of distinctions an average human player might make. regardless  it is the best we can currently do given the computational constraints of this approach. 
　the final thing needed to sever the abstract game from the underlying real game tree are the transition probabilities. the chance node between the flop and turn represents a particular card being dealt from the remaining stock of 1 cards. in the abstract game  there are no cards  only buckets. the effect of the turn card in the abstract game is to dictate the probability of moving from one pair of buckets on the flop to any pair of buckets on the turn. thus the collection of chance nodes in the game tree is represented by an  n x n  to  n x n  transition network as shown in figure 1. for postflop models  this can be estimated by walking the entire tree  enumerating all transitions for a small number of characteristic flops. for preflop models  the full enumeration is more expensive  encompassing all {1 choose 1} = 1 possible flops   so it is estimated either by sampling  or by  parallel  enumeration of a truncated tree. 
　for a 1-round postflop model  we can comfortably solve abstract games with up to seven buckets for each player in each round. changing the distribution of buckets  such as six for the flop  seven for the turn  and eight for the river  does not appear to significantly affect the quality of the solutions  better or worse. 
　the final linear programming solution produces a large table of mixed strategies  probabilities for fold  call  or raise  for every reachable scenario in the abstract game. to use this  the poker-playing program looks for the corresponding situation based on the same hand strength and potential measures  and randomly selects an action from the mixed strategy. 
the large lp computations typically take less than a day 
 using cplex with the barrier method   and use up to two gigabytes of ram. larger problems will exceed available memory  which is common for large lp systems. certain lp techniques such as constraint generation could potentially extend the range of solvable instances considerably  but this would probably only allow the use of one or two additional buckets per player. 
multiagent systems 
1 experiments 
1 	testing against computer players 
a series of matches between computer programs was conducted  with the results shown in table 1. win rates are measured in small bets per hand  sb/h . each match was run for at least 1 games  and over 1 games in some cases . the variance per game depends greatly on the styles of the two players involved  but is typically +/- 1 sb. the standard deviation for each match outcome is not shown  but is normally less than +/- 1 sb/h. 
the  bot players  were: 
　psoptil  composed of a hand-crafted 1-round preflop model  providing conditional probability distributions to each of seven 1-round postflop models  figure 1 . all models in this prototype used six buckets per player per round. 
　psoptil  composed of four 1-round postflop models under the naive uniform distribution assumption  with 1 buckets per player per round. selby's optimal solution fox preflop hold'em is used to play the preflop   selby  1  . 
　psoptio  composed of a single 1-round postflop model  wrongly assuming uniform distributions and an initial pot size of two bets  with seven buckets per player per round. this program used an always-call policy for the preflop betting round. 
　poki  the university of alberta poker program. this older version of poki was not designed to play the 1-player game  and can be defeated rather easily  but is a useful benchmark. 
　anti-poki  a rule-based program designed to beat poki by exploiting its weaknesses and vulnerabilities in the 1-player game. any specific counter-strategy can be even more vulnerable to adaptive players. 
　aadapti  a relatively simple adaptive player  capable of slowly learning and exploiting persistent patterns in play. 
always call  very weak benchmark strategy. 
always raise  very weak benchmark strategy. 
　it is important to understand that a game-theoretic optimal player is  in principle  not designed to win. its purpose is to not lose. an implicit assumption is that the opponent is also playing optimally  and nothing can be gained by observing the opponent for patterns or weaknesses. 
in a simple game like roshambo  also known as rock-
paper-scissors   playing the optimal strategy ensures a breakeven result  regardless of what the opponent does  and is therefore insufficient to defeat weak opponents  or to win a tournament   billings  1  . poker is more complex  and in theory an optimal player can win  but only if the opponent makes dominated errors. any time a player makes any choice that is part of a randomized mixed strategy of some game-theoretic optimal policy  that decision is not dominated. in other words  it is possible to play in a highly sub-optimal manner  but still break even against an optimal player  because those choices are not strictly dominated. 
　since the pseudo-optimal strategies do no opponent modeling  there is no guarantee that they will be especially effective against very bad or highly predictable players. they must rely only on these fundamental strategic errors  and the margin of victory might be relatively modest as a result. 
　

table 1: computer v.v computer matches  sb/h . 
　
　the critical question is whether such errors are common in practice. there is no definitive answer to this question yet  but preliminary evidence suggests that dominated errors occur often enough to gain a measurable ev advantage over weaker players  but may not be very common in the play of very good players. 
　the first tests of the viability of pseudo-optimal solutions were done with psoptio playing postflop hold'em  where both players agree to simply call in the preflop  thereby matching the exact pre-conditions for the postflop solution . in those preliminary tests  a poker master  the first author  played more than 1 hands  and was 'inable to defeat the pseudo-optimal strategy. in contrast  poki had been beaten consistently at a rate of over 1 sb/h  which is more than would be lost by simply folding every hand . 
　using the same no-bet preflop policy  psoptio was able to defeat poki at a rate of+1 sb/h  compared to +1 sb/h for the full game including preflop   and defeated aadapti at +1 sb/h  compared to +1 sb/h for the full game . 
　all of the pseudo-optimal players play substantially better than any previously existing computer programs. even psoptio  which is not designed to play the full game  earns enough from the postflop betting rounds to offset the ev losses from the preflop round  where it never raises good hands  nor folds bad ones . 
　it is suspicious that psoptil outperformed psoptil  which in principle should be a better approximation. subsequent analysis of the play of psoptil revealed some programming errors  and also suggested that the bucket assignments for the preflop model were flawed. this may have resulted in an inaccurate pseudo-optimal preflop strategy  and consequent imbalances in the prior distributions used as input for the postflop models. we expect that this will be rectified in future versions  and that the psoptil design will surpass psoptil in performance. 
1 	testing against human players 
while these results are encouraging  none of the non-pseudooptimal computer opponents are better than intermediate strength at 1-player texas hold'em. therefore  matches were conducted against human opponents. 
　more than 1 participants volunteered to play against the pseudo-optimal players on our public web applet  www. cs . ualberta. ca/  'games/poker/   including many experienced players  a few masters  and one world-class player. the programs provided some fun opposition  and ended up with a winning record overall. the results are summa-

table 1: human vs psoptil matches. 
rized in table 1 and table 1.  master-1 is the first author  experienced-1 is the third author . 
　in most cases  the relatively short length of the match leaves a high degree of uncertainty in the outcome  limiting how much can be safely concluded. nevertheless  some players did appear to have a definite edge  while others were clearly losing. 
　a number of interesting observations were made over the course of these games. it was obvious that most people had a lot of difficulty learning and adjusting to the computer's style of play. in poker  knowing the basic approach of the opponent is essential  since it will dictate how to properly handle many situations that arise. some players wrongly attributed intelligence where none was present. after losing a 1game match  one experienced player commented  the bot has 
　
1 	multiagent systems 
　

hand1 played 
figure 1: progress of the  thecount  vs psoptij 
mc figured out now   indicating that its opponent model was accurate  when in fact the pseudo-optimal player is oblivious and does no modeling at all. 
　it was also evident that these programs do considerably better in practice than might be expected  due to the emotional frailty of their human opponents. many players commented that playing against the pseudo-optimal opponent was an exasperating experience. the bot routinely makes unconventional plays that confuse and confound humans. invariably  some of these  bizarre  plays happen to coincide with a lucky escape  and several of these bad beats in quick succession will often cause strong emotional reactions  sometimes referred to as  going on tilt  . the level of play generally goes down sharply in these circumstances. 
　this suggests that a perfect game-theoretic optimal poker player could perhaps beat even the best humans in the long run  because any ev lost in moments of weakness would never be regained. however  the win rate for such a program could still be quite small  giving it only a slight advantage. thus it would be unable to exert its superiority convincingly over the short term  such as the few hundred hands of one session  or over the course of a world championship tournament. since even the best human players arc known to have biases and weaknesses  opponent modeling will almost certainly be necessary to produce a program that surpasses all human players. 
1 	testing against a world-class player 
the elite poker expert was gautam rao  who is known as 
 thecount  or  countdracula  in the world of popular online poker rooms. mr. rao is the #1 all-time winner in the history of the oldest online game  by an enormous margin over all other players  both in total earnings and in dollar-per-hand rate. his particular specialty is in short-handed games with five or fewer players. he is recognized as one of the best players in the world in these games  and is also exceptional at 1-player hold'em. like many top-flight players  he has a dynamic ultra-aggressive style. 
mr. rao agreed to play an exhibition match against 
multiagent systems 
psoptil  playing more than 1 hands over the course of several days. the graph in figure 1 shows the progression of the match. 
　the pseudo-optimal player started with some good fortune  but lost at a rate of about -1 sb/h over the next 1 hands. then there was a sudden reversal  following a series of fortuitous outcomes for the program. although  thecount  is renown for his mental toughness  an uncommon run of bad luck can be very frustrating even for the most experienced players. mr. rao believes he played below his best level during that stage  which contributed to a dramatic drop where he lost 1 sb in less than 1 hands. mr. rao resumed play the following day  but was unable to recover the losses  slipping further to -1 sb after 1 hands. at this point he stopped play and did a careful reassessment. 
　it was clear that his normal style for maximizing income against typical human opponents was not effective against the pseudo-optimal player. whereas human players would normally succumb to a lot of pressure from aggressive betting  the bot was willing to call all the way to the showdown with as little as a jack or queen high card. that kind of play would be folly against most opponents  but is appropriate against an extremely aggressive opponent. most human players fail to make the necessary adjustment under these atypical conditions  but the program has no sense of fear. 
　mr. rao changed his approach to be less aggressive  with immediate rewards  as shown by the +1 sb increase over the next 1 hands  some of which he credited to a good run of cards . mr. rao was able to utilize his knowledge that the computer player did not do any opponent modeling. knowing this allows a human player to systematically probe for weaknesses  without any fear of being punished for playing in a methodical and highly predictable manner  since an oblivious opponent does not exploit those patterns and biases. 
　although he enjoyed much more success in the match from that point forward  there were still some  adventures   such as the sharp decline at 1 hands. poker is a game of very high variance  especially between two opponents with sharp styles  as can be seen by the dramatic swings over the course of this match. although 1 games may seem like a lot  mr. rao's victory in this match was still not statistically conclusive. 
　we now believe that a human poker master can eventually gain a sizable advantage over these pseudo-optimal prototypes  perhaps -1.1 sb/h or more is sustainable . however  it requires a good understanding of the design of the program and its resulting weaknesses. that knowledge is difficult to learn during normal play  due to the good information hiding provided by an appropriate mixture of plans and tactics. this  cloud of confusion  is a natural barrier to opponent learning. it would be even more difficult to learn against an adaptive program with good opponent modeling  since any methodical testing by the human would be easily exploited. this is in stark contrast to typical human opponents  who can often be accurately modeled after only a small number of hands. 
1 	conclusions and future work 
the pseudo-optimal players presented in this paper are the first complete approximations of a game-theoretic optimal strategy for a full-scale variation of real poker. 
　
　several abstraction techniques were explored  resulting in the reasonably accurate representation of a large imperfect information game tree having o 1  nodes with a small collection of models of size o 1 . despite these massive reductions and simplifications  the resulting programs play respectably. for the first time ever  computer programs are not completely outclassed by strong human opposition in the game of 1-player texas hold'em. 
　useful abstractions included betting tree reductions  truncation of betting rounds combined with ev leaf nodes  and bypassing betting rounds. a 1-round model anchored at the root provided a pseudo-optimal strategy for the preflop round  which in turn provided the proper contextual information needed to determine conditional probabilities for postflop models. the most powerful abstractions for reducing the 
problem size were based on bucketing  a method for partitioning all possible holdings according to strategic similarity. although these methods exploit the particular structure of the 
texas hold'em game tree  the principles are general enough to be applied to a wide variety of imperfect information domains. 
　many refinements and improvements will be made to the basic techniques in the coming months. further testing will also continue  since accurate assessment in a high variance domain is always difficult. 
　the next stage of the research will be to apply these techniques to obtain approximations of nash equilibria for tvplayer texas hold'em. this promises to be a challenging extension  since multi-player games have many properties that do not exist in the 1-player game. 
　finally  having reasonable approximations of optimal strategies does not lessen the importance of good opponent modeling. learning against an adaptive adversary in a stochastic game is a challenging problem  and there will be many ideas to explore in combining the two different forms of information. that will likely be the key difference between a program that can compete with the best  and a program that surpasses all human players. quoting  thecount : 
 you have a very strong program. once you add opponent modeling to it  it will kill everyone. ' 
acknowledgments 
the authors would like to thank gautam rao  sridhar mutyala  and the other poker players for donating their valuable time. we also wish to thank daphne roller  michael littman  matthew ginsberg  rich sutton  david mcallester  mason malmuth  and david sklansky for their valuable insights in past discussions. 
　this research was supported in part by grants from the natural sciences and engineering research council of canada  nserc   the alberta informatics circle of research excellence  icore   and an izaak walton killam memorial postgraduate scholarship. 
