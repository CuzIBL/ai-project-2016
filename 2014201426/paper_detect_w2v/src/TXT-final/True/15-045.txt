 
1n-ate is an on-going project aimed at developing expert consultant systems for guiding a novice technician through each step of an electronics troubleshooting session. one goal of the project is to automatically produce  given a set of initial symptoms  a binary  pass/fail  decision tree of testpoints to be checked by the technician. this paper discusses our initial approach using a modified game tree search technique  the gamma miniaverage method. one of the parameters which guides this search technique - the cost of each test - is stored a priori the two other parameters that guide it - the conditional probability of test outcomes and the proximity to a solution - are provided by a dynamic model of an expert troubleshooter's beliefs about what in the device is good and what is bad. this model of beliefs is updated using probabilistic  tp.st-result   
plausible-consequences  rules these rules are either provided by an expert technician  or approximated by a 
model-guided rule generator the model that guides the generation of rules is a simple block diagram of the unit under test  uut  augmented with component failure rates. 
1. introduction 
　　in-ate is an on-going project aimed at developing expert consultant systems for guiding novice technicians through each step of an electronics trouble-shooting session this paper describes the current  mark i implementation of the in-ate system. the basic design of this implementation grew out of a series of preliminary conversations with our domain expert - a master technician. using a tektronix model 1 oscilloscope as a potential unit under test  uut  for particular examples  these discussions focused on the general types of information used in troubleshooting electronic equipment. to achieve an implementation in a reasonable amount of time  we chose for inclusion a proper yet important subset  namely: 
circuit topology cost of individual tests component failure rates conditional probability of test results proximity to a solution 
in the remainder of this introduction  section 1  we describe their importance in electronics troubleshooting  and briefly how they have been incorporated in the current implementation. the final section  section 1  of this paper briefly discusses how other important types of information  e.g.  component function  could be added to the current system. the body cf this paper  sections 1 and 1  describes in detail the current system - a novel application of heuristic search guided by a rule-based reasoning component that makes use of both traditional  expert-supplied rules and rules approximated by a novel  model-guided rule generator. 
　　in principle  a technician can isolate the faulty components of an electronic device with an exhaustive search by testing every discrete  low level component  e.g.  resistor or transistor.  this simple linear search can find all faults and in fact is often done when there are relatively few components to test. when there are a great many discrete components  the technician can take advantage of the hierarchical organization that designers generally impose upon complex and sophisticated circuits in this tree-like organization the top-most branches represent the major subsystems  such as the power supply  while the bottom leaves represent the discrete components  such as the individual resistors and transistors. this organization of the search space permits the technician to consider  at each level of abstraction  relatively few components. the in-ate system  as well as others  e.g.   davis  1b  and 
 genesereth  1    attempts to mimic the troubleshooter by starting the search for faults at the top of the component hierarchy  and then slowly moving down to lower and lower levels. 
　　at each level in this abstraction hierarchy tree  the troubleshooter must decide where to test first  and then based on the outcome of that test  where to test next  etc. in the computer programs that guide automatic test 
equipment  ate  these decisions are stored tin a binary {pass/fail  decision tree. each node in the decision tree represents the best test to make next  given the test outcomes represented by the arcs which lead to it from the root node. the root node represents the initial symptoms. this is similar to the decision trees produced by artificial intelligence game tree search algorithms  such as the a* algorithm and the alpha-beta minimax algorithm  see  nilsson  1  for an excellent discussion of these algorithms.  in these decision trees each node represents the best move for a player to make next the arcs which lead to each node represent his opponent's responses to his earlier moves. 
     the a* algorithm can find an solution tree which is optimal  in cost  for and/or trees  but requires a depthfirst search to termination  eg.  by looking ahead until a faulty component is found.  when there are many possible tests in a circuit  or many possible moves in a game  the a* algorithm can be impractical* and a shallow  suboptimal search strategy may be required. the alpha-beta minimax method can allow this to be done efficiently  yet the alphabeta method does not take into account the cost of each test. while the cost of moves in a game may be inconsequential  the cost of a test in troubleshooting can be crucial. 
* even in this case  where there are only two branches at each  and  node  i.e.  pass /fail   it has in fact been shown that the general problem of finding an optimal solution tree is np-hard  see  hyafil and rivest  1  and also 
 loveland  1 .  

1 r. cantone et al. 
     still another problem with using any minimax method for this application is the built-in assumption that the  and  branches in the and/or search space are decided by an opponent in physical systems  such as electronic devices  the  and  branches are governed by nature  or chance  and can be estimated using probability. this observation by slagle and lee motivated their creation of the gamma miniaverage method  slagle and lee  1 . in the miniaverage method  the backed-up value of an  and  branch is not a maximum  but a weighted average weighted by the conditional probabilities associated with each branch. it allows for efficient pruning of the search space with cut-offs  similar to alpha-beta cut-ofls  called gamma cut-offs. it also allows for termination of the search when the cost of continuing the search exceeds the cost of simply replacing the suspect components. 
     while the cost of each test can often be estimated a priori  two other parameters that guide the gamma miniaverage method - the conditional probability of a test outcome  discussed above  and a static evaluation function  which estimates the proximity to a solution - are based on the results of earlier tests. even with only 1 possible outcomes  i.e.  pass/fail  for a test  for n possible tests there may be as many as 1n possible combinations to consider  since each test could either pass  fail  or not be made.  rather than require that values for these two parameters be established a priori for all possible combinations of test results  in-ate estimates these values through rule-based simulation. 
     figure 1 shows the overall structure of the rule-based reasoning component  which is the heart of the in-ate system. test results  real or hypothetical  are introduced one at a time to the inference engine given a test result  the inference engine uses a single  triggered rule to update a dynamic  a posteriori model of an expert technician's beliefs about what in the uut could be bad and what should be good. 

     in the traditional expert systems approach  the rules used to update these beliefs would come from a stored  a 
priori collection of expert-supplied symptom --  possiblecause rules. randall davis  1a. 1b  has pointed out that the task of extracting a separate rule from the expert for every conceivable symptom that might arise in a sophisticated piece of electronics equipment is unrealistic. because of this  davis stresses the importance of an alternative means with which to reason - a model of the system being diagnosed. davis argues that expert systems should have both compiled rules  which he calls  compiled experience   and the ability to reason from a model. in-ate incorporates these two knowledge sources through its small expert-supplied  device-specific a priori rulebase  and a model-guided  device-independent rule generator. 
     the rule generator can dynamically produce rules  with the same syntax as the compiled rules  by consulting an a priori model of the uut. the model of the uut that has been implemented  thus far  is an augmented block diagram* - augmented in the sense that it contains such non-traditional information as component failure rates although the rules that in-ate generates from its model may be less accurate than expert-supplied rules  the generated rules can fill in whatever gaps exist in the expertsupplied rulebase  and then only when needed. the motivation for this design was a statement by our domain expert that in troubleshooting any sophisticated piece of electronic equipment  including equipment with which he is very familiar  a set of block/schematic diagrams is indispensible he also says that given a good set of such diagrams he can troubleshoot essentially any piece of analog electronics the rule generator tries to capture this generality by producing approximations of the rules which normally make up  compiled experience   
     the rules in in-ate  both compiled and generated  are probabilistic  or  more properly  evidential  rules of belief each rule is triggered by a single test result. the consequences of each rule are probabilistic assignments of guilt or innocence  given this one particular finding  to various components and lines in the uut. these probabilistic assignments are used by the system's inference engine to update the system's a posteriori model of beliefs about what in the uut is good and what is bad. since the system does not make use of a  single fault assumption  that at most one of the uut's components is faulty  for each component and line the system gathers both evidence that is it good and evidence that it is bad. and  because of the uncertainty inherent with approximated probabilities  the probabilities associated with these two mutually exclusive possibilities may not necessarily sum to 1. the system combines evidence from separate  independent sources  i.e.  test results  using dempster's rule  shafer  1 . dempster's rule does allow for a sum less than 1  yet reduces to the traditional bayesian approach when the sum is exactly 1. 
     dempster's rule works fine for combining evidence from sources that are independent. but because of circuit connectivity  test results are not always independent. in in-ate  non-independent test results are defined as those that are taken from a common path in the uut circuitry. the interpretation assigned by the inference engine given two non-independent results depends upon whether the tests passed or failed and upon which result is upstream or downstream from the other. for example  if two nonindependent results are both bad  good   then only the more useful - the upstream  downstream  result - is retained and interpreted by the inference engine. as another example  when one passed test result later clears away the blame from some component - assigned as a result of an earlier failed test result - that blame is removed from the component and the original failed test result is re-interpreted in light of this new information. this form of non-monotonicity has been implemented using the basic truth maintenance facility in the electronics simulation system el  stallman and sussman  1 . 
     the current in-ate system is made up of two basic components. section 1 of this paper discusses the probabilistic rule-based reasoning component that guides the heuristic search component discussed in section 1. section 1 is a discussion of future plans. 
* fault isolation and testability analysis based upon block diagrams has come to be known as logic modeling or logic model analysis. one notable example of this is the stamp system  simpson and balaban  1  which makes use of information theory to generate  in a proprietary manner  binary decision trees or the type generated by the in-ate system. 

r. cantone et al. 1 
good. 

1 r. cantone et al. 
re-distributed among the other n-m components. similarly  when the plausible blame attributed to some line l  by an instance of rule-lb  is later confirmed by a failed test result  then the blame that had been assigned to the m components that line l feeds  by the corresponding instance of rule-la  should be removed* and re-distributed to the other n- m components using a new instance of rule-la - one triggered by the new test result. 
　　this form of non-monotonicity has been implemented using the basic truth maintenance system incorporated in the electronics simulation system el  stallman and sussman  1   borrowing the terminology of stall man and sussman. at each moment an assertion can be believed with some certainty  and labeled in  if there is some wellfounded support behind it  i.e.  a test result   otherwise it ls labeled out an assertion that is less than completely certain can move from in to out by the introduction of contradictory evidence that is completely certain. to 
implement this capability we store with each discrete piece of evidence that is less than completely certain the test result and rule that originally introduced it to in. this ls the only information necessary for removing and reevaluating the plausible consequences of a rule in light of new information 
1. deciding the best test 
　　the a posteriori model of beliefs has two basic purposes firstly  the system can present to the user a list of suspect components  ordered by their current probability of being faulty this can either be done at any time  upon request from the user  or automatically  when the maximum such probabability exceeds some user established threshold secondly  the system can use the model of beliefs  in a hypothetical mode  to help decide the best test for the user to make next if there are no initial symptoms 
the best test model of beliefs can be used to construct  a 
priori  a binary decision of best tests to be performed at each step of such a troubleshooting session if there are initial symptoms  a custom tailored binary decision tree which takes these initial  test results  into account can be computed at the beginning of the troubleshooting session  or dynamically recomputed if the user later contributes  through his own initiative  other test results  
　　the best test is basically that test which  for its cost  will bring the system closest to isolating the fault to a single component. since there are two possible outcomes of each test  i.e..pass or fail   the system must consider both of them separately. it does this by simulating the effect that each such outcome would have on its model of beliefs. a heuristic static evaluation function is applied  in each case  to the model of beliefs to estimate how far the system would then be to having isolated the problem to one faulty component. 
     we have applied the gamma miniaverage method in a straight-forward manner  and refer the reader to  slagle and lee  1  for a detailed discussion of the method itself.  the nodes in the search tree represent  on alternating levels  the possible tests that can be performed and the potential outcomes o1 of the possible tests. a static evaluation function g applied to an outcome node on the 
frontier of the search represents the loss  or cost  of terminating at that point. the value of the static evaluation function g which we have empirically chosen is the size of the component ambiguity set  the still suspect components.  g could also represent the total cost of replacing all of the components in the ambiguity set  if this data 
were known. 
* although that blame is removed from these components  the components are not cleared since this test result pro-
vides no evidence that they are good  but merely that one of their inputs is bad. 


     to allow for multiple levels of look-ahead  this backing-up process can be repeated as many levels as resources permit not only can these backed-up miniaverage values be used for estimating the best test to make next  but using gamma cut-offs  this search can be done efficiently. 
1. future work 
     the algorithms have been implemented in franz lisp on a vax 1 computer and produces reasonably good decision trees for block diagrams that include series  parallel  and feedback circuits. for example  in the case of a simple series circuit where all test costs are equal  the system reduces to the standard half-split method  i.e.  binary search.  in the more realistic case of a 1dimensional circuit  where there is a real ambiguity about where the  middle  is  the system performs much better than the half-split method. in the next few months this basic system will first be applied to troubleshooting a standard oscilloscope  the tektronix model 1  and its performance will be evaluated by our domain expert. to test the generality of the system  it will then be applied to troubleshooting a sophisticated piece of military electronics  such as the wlq-1 intercept receiver. 
     we will also be adding a learning component  in the style of the self-improving diagnostics system sid  hughes aircraft corp.. i1   to retain and periodically refine generated rules. the probabilities within a rule  initially assigned by the rule generator  would be changed to reflect the actual distribution encountered by the system over time. 
     a more difficult improvement that we will be exploring is a more informed rule generator. we will attempt to incorporate an ability to reason with knowledge about component function  in addition to circuit topology and component failure rates.  also the rule generator should know when to use assumptions that narrow the set of possible faults  e.g.  that faults are always upstream from a bad line  but are not always correct  see  davis  1b  . 
r. cantone et al. 1 
acknowledgements 
     we would like to thank our domain expert  master technician mr. mark hulbert  for his time and patience  prof. kenneth de jong. prof. michael gaynor and dr. james 
slagle for their advice; dr. randall schumaker  nava1r  for directing our attention toward the problem of automatic test program generation  atpg   and dr. jude franklin  especially  for introducing us to the general problem of electronics troubleshooting 
