 
　　　a general system to simulate human cognitive processes is described. the f o u r - p a r t system comprises a nodespace to store the network s t r u c t u r e ; a supervisor; a t r a n s i t i o n network parser; and an i n t e r p r e t e r . the method by which noun phrases operate and the process for the determiner   t h e   is presented. an analysis of verb structures i l l u s t r a t e s how network structures can 
be constructed from p r i m i t i v e verb d e f i n i t i o n s that get at the underlying structures of p a r t i c u l a r verbs. 	the 
paper concludes with an i l l u s t r a t i o n of a problem in question-asking. 
a model of human memory 
　　　we have constructed a large general simulation of human language and long-term memory on the premise that the study of the i n t e r - r e l a t i o n s h i p s among psychological processes w i l l lead to more i n s i g h t i n t o human cogn i t i o n and memory. the general implementation is bas i c a l l y complete  and a v a r i e t y of users are s t a r t i n g 
to study s p e c i f i c psychological tasks  language understanding; c h i l d r e n ' s development of language; p r i m i t i v e 
verb s t r u c t u r e ; reading; inference; game playing--go and gomoku; visual representation and memory; l e a r n i n g ; and question answering . it is s t i l l too e a r l y to r e port on the r e s u l t s of the psychological i n v e s t i g a t i o n . . therefore  t h i s paper is a progress report on the system and the underlying psychological p r i n c i p l e s . 
　　　the major guidelines have come from our attempts to represent long-term memory s t r u c t u r e s . we know that people r a p i d l y forget the d e t a i l s about the surface s t r u c t u r e of an experience but r e t a i n the meaning or i n t e r p r e t a t i o n of that experience i n d e f i n i t e l y . we a l so know that r e t r i e v a l of an experience from memory is u s u a l l y a reconstruction which is h e a v i l y biased by the person's general knowledge of the world. thus  general world knowledge should i n t e r a c t with s p e c i f i c event knowledge in such a way that d i s t i n c t i o n between the two is not possible. the representation should allow paraphrase. f i n a l l y   the l i m i t a t i o n s of human working storage  or short-term memory  probably comprise a f u n damental property of the system  one that should be viewed as an e s s e n t i a l   p o s i t i v e component  not as simp l y a performance l i m i t a t i o n . 
the computer system 
　　　the basic system consists e s s e n t i a l l y of f o u r f i x e d components: 1  a nodespace in which our network structures are stored; 1  a supervisor which allows us d i r e c t access to various portions of the nodespace; 1  a parser which converts s t r i n g s of words i n t o network s t r u c t u r e s ; 1  an i n t e r p r e t e r which processes sections of the nodespace and c a r r i e s out any s t r a t e g i e s which were stored in that p o r t i o n of the nodespace. the system is w r i t t e n in algol on the burroughs 1 at the u n i v e r s i t y of c a l i f o r n i a   san diego. the simulations are done in our own e n g l i s h - l i k e language  with a l l statements entered through the parser. the language is called sol   f o r semantic operating language-pronounced   s o u l     and it is s p e c i f i c a l l y designed f o r manipulating and t r a v e r s i n g the network structures of the data base. because we wish many d i f f e r e n t psychological simulations to be handled by the one system  we have made it reasonably general and r e a d i l y extendable so that any of the psychological hypotheses under study can be simulated and tested in i t s own specialized 
m i n i - w o r l d . 
　　　the representation of actions and concepts. 	the representation to be described here is presented in 
more d e t a i l and w i t h more j u s t i f i c a t i o n in the papers by rumelhart  lindsay & norman1 and norman1. basically  we use a network representation with nodes connected to other nodes by labeled  directed r e l a t i o n s . because each r e l a t i o n also has an inverse  the network is bid i r e c t i o n a l . 
　　　events are s p e c i f i e d in a similar way  except that actions require arguments. thus  the node that represents an action may have o b l i g a t o r y r e l a t i o n s leading from i t   specifying such things as the agent  l o c a t i o n   and object of that a c t i o n . 
　　　most actions and concepts in the network have a s i n g l e primary node  or type node  that encodes i t s d e f i n i t i o n   and numerous secondary nodes  or token nodes  that represent s p e c i f i c instances of the primary one. almost a l l encodings of s p e c i f i c scenes are done by means of secondary nodes. 
　　　the basic u n i t in the memory space is the scenario: an action that consists of events  agents  l o c a t i o n s   and objects. to i l l u s t r a t e the representational system  consider the sentence 
             peter put the package on the t a b l e . figure 1 shows a possible simple encoding f o r t h i s sentence which includes some of the underlying structures of the a c t i o n . 
	figure 1. 	peter put the package on the t a b l e . 
the sol language 
　　　the parsing process is based on three independently motivated p r i n c i p l e s . f i r s t   the parsing procedures are represented as an augmented recursive t r a n s i t i o n 
network 	  f o l l o w i n g the work of woods and k a p l a n 1 * 1 ' 1   . 
second  the parser is based around a  case grammar    a f t e r fillmore 1   and has  case frames  and  argument c o n s t r a i n t s   associated w i t h many l e x i c a l items.  here some of the methods suggested by schank1 can be used.  t h i r d   the parsing is based on the idea that it is the task of each noun phrase to f i n d i t s own r e f e r e n t in memory if it e x i s t s or else to create a new s t r u c t u r e in the data base. thus  c e r t a i n l e x i c a l items such as determiners  a d j e c t i v e s   and pronouns are defined by the s t r a t e g i e s f o r f i n d i n g the proper r e f e r e n t . 
　　　argument frames. associated with every predicate word is an argument frame which indicates which and how many arguments must e x i s t . for example  associated w i t h the verb move might be the f o l l o w i n g set of arguments; 1  a causal mover  called here an agent ; 
1  a moved object  obj ; 1  an i n i t i a l l o c a t i o n  fromloc ; 1  a terminal l o c a t i o n  t1-l1c ; 1  a means of moving  method ; and 1  a time of occurrence  at-time . we denote the argument frame as f o l l o w s : 
1              agent x moves y  fr1m-l1c ll t1-l1c l1 method m at-time t   . those arguments enclosed in parentheses are taken to be o p t i o n a l ; the others are r e q u i r e d . associated with each case name   e . g .   from-loc or method  is a l i s t of prepositions which can occur at the surface l e v e l to i n d i c a t e or mark t h a t argument. each label also is 
associated with a set of semantic characteristics which can be interrogated during the parse. the prepositions and the semantic characteristics can be used together to disambiguate which of the variety of concepts a given noun phrase is representing. 
　　　certain verbs  particularly those talking about ideas  sometimes take whole sentences as arguments. such arguments are referred to in our system as prepositional arguments  proposition . thus  the argument frame for one sense of the verb make  as in the sentence  freddy made his brother come home   takes a propositional argument and has the argument frame 
agent x make proposition y  method 
                 m at-time t  where y stands for some transformed version of an ent i r e sentence. 
     at every -point during the parse the goal is to find and correctly f i l l the argument slots of the predicate word in question. if some arguments do not f i t into the frame of the sense of the predicate word in question  a new sense of the predicate word is tried until either a f i t occurs  or no more senses exist  in which case  the parse f a i l s   . 
　　　operators. one important class of words in our language analysis is the class we call operators. operators are nouns that take arguments  usually prepositional phrases  and thus have associated case frames. operators can be verb based nouns such as destruction in the destruction of the city by the enemy-destruction is an operator with its two arguments f i l l e d by the following noun phrases. an operator is also a relational noun such as father  as in the sentence   b i l l is the father of henry.  here  father is analyzed as an operator with one argument. the existence of case frames for these nouns as well as verbs reduces substantially the ambiguity of prepositional modification. 
disambiguating the referent 
　　　one of the major problems in the analysis of natural language is determining the exact referents of a phrase. most of the complexities of such words as the come from the difficulties of determining just what concept is being referred to. in the sol system the 
parser automatically invokes the procedural definition of the which  in turn  performs an active search through the data base to determine the referent as each noun phrase is analyzed. we illustrate here how this is done by going through the strategies that comprise the procedural definition for the. in rough form  the process is this: f i r s t   if the phrase is an operator  then it contains the procedures for its own disambiguation which should be performed before doing anything else. if that is not the case  then we determine whether the object being referred to is unique within the data base  for if it i s   no particular problem exists. if these two strategies f a i l   then we see whether or not immediate context helps  and if that f a i l s   we look to see whether or not there is a relative clause that can do the job. now look at this in detail. 
　　　operators. if the unknown phrase is an operator  then it is necessary to determine whether or not to perform the operation or to refer to the value of the operation. thus  with the phrase the father of john the operator father has not been evaluated  so f i r s t we execute the routine for father  passing john to it as an argument  and then return to the parser with the result of that operation  presumably  the name of the 
person who is john's father . if father is being used in its nominal sense  however  as in  i told the father to give the toothbrush to the daughter   then we are referring to the value that a previous execution of the operator had returned. 
　　　unique instances. if a given concept is unique to the data base  then it can be unambiguously found whenever referred to with a determiner. thus  if the memory system knows of only one ocean  to t e l l it  the sun set over the ocean  is completely unambiguous  not because the system is intelligent  but rather because it doesn't know enough to be confused. tell it about the existence of a second ocean  or a second sun  and this strategy will not work  but the following ones might . 
       foregrounding. chafe1  suggests that many problems in disambiguation are handled by context in a man-
ner that he calls  foregrounding.  if the recent context has been about  fred's kitchen   then the objects in that particular kitchen are foregrounded even though 
they have never been mentioned specifically. foreground establishes local context. in our system each concept that can be brought to the foreground has associated with it a specific l i s t of items. as new sentences pass through the parser  they initiate the appropriate foreground l i s t s . 
       note that foreground has several hierarchical levels  for the context includes the general overall topic under discussion  the specific details  and the environmental setting of the speakers. thus  in this paper we could now talk of  this conference  or  this parser   both of which would be disambiguated by foreground-like operations  but each would be at different 
levels. 
       short-term memory. 	we can also look back in short-term memory to determine if any of the recent sentences help disambiguate the referent. 	at the moment  we look back over the last five sentences. 	eventually  we intend to have a more reasonable simulation of human short-term memory processes  so that only top-
ics that could reasonably be expected s t i l l to remain in active short-term memory could be disambiguated this way. 
       search. if all this fails  it is s t i l l possible that an intelligent search among the concepts discussed recently  or foregrounded recently  could disambiguate the referent. this strategy has not yet been implemented  primarily because its use depends upon the operation of a search routine that is not yet fully operational.  the search routine is a simultaneous breadthf i r s t search emanating from as many nodes as are specified  returning with a path that links all the nodes in the search space. that path is evaluated for its logical properties and the search process is either terminated or continued.  
       clauses. a common method of disambiguation is by the use of clauses  as in the phrase the girl  whom  i saw in the park. this method of disambiguation is clearly an important part of normal english  it has been deleted from the existing the routines because the search routines do not yet work. but it is an important enough process to warrant further discussion here. consider the sentence  i see the girl with the telescope.  as it now stands the sentence is incomplete and  therefore  ambiguous: we need some context. suppose that the following information is known by the system. 
jane  mary  cynthia  and helen are girls. 
mary has a telescope. 
these data are represented in the left part of figure 
1. 
figure 1. 
1        the analysis of the sentence  i see the g i r l with the telescope  is simple until we reach the phrase the g i r l . thus  we can recognize i a s the subject of the verb see.  the model has only one person with whom it converses  namely you. the change in designation of the subject to the case relation of agent occurs with the construction of the deep parse ana construction of a permanent memory segment.  the analysis of the is complex because a l l the strategies discussed so far would f a i l . we need to look at the clause with the telescope. a search of the data base reveals that only one g i r l possesses a telescope; now we have dis-
ambiguated the referent  see figure 1 . 
     a different result would occur had the contextual information in the data base been the following. 
mary is a g i r l . 
i got a telescope on tuesday. 
the resulting analysis is shown in figure 1. 
figure 1. 
　　　the major difference between the analyses shown in figures 1 and 1 is that in the latter the phrase with the telescope is neither needed to help disambiguate the referent for the girl nor is it consistent with the known information about mary. hence  the referent program completes its action with one phrase left unanaly1ed. when control returns to the parser  this phrase is s t i l l left. the parser then checks it against the possible frame for the verb see and  in this case  finds that it can be used as the instrument of seeing. again  the sentence is analyzed with no difficulty and with no recognition by the parser that an alternative analysis was possible. 
defining verbs 
　　　at this point the general description of the system is complete. one more specific point is appropriate to discuss here  however. the basic premise underlying the linguistic analysis is that we can represent the meaning of verbs as network structures built from a limited set of semantic primitives. here we wish to illustrate one analysis of verbs and 'their underlying primitives  both to show how we believe the linguistic structures should be represented and also to demonstrate several features of the sol language. 
     at least three different aspects of verb meanings can be distinguished: states; changes of states; and causes of these changes. the stative component of a verb conveys that fixed relationship which holds among its arguments for a specified period of time. the change component of a verb tells that a change in state has occurred. the causative component communicates the source of  or reason for  the change. these different verb components are not all present in all verbs  but all components may appear in a single lexical item. 
　　　in the remainder of this section we show how we represent these various semantic components and how we can express the definitions of particular lexical items in such a way that the primitive representation for that item is automatically computed whenever it appears in a sentence.1 
　　　statiyes. the simplest semantic component of verbs is the stative component. this component merely communicates the information that a particular state of the world holds from some i n i t i a l time to some final time. the simple locative is an example of a verb 
which seems to have only stative components. 	for ex-
ample ; 
a stadium was located in the park from 1 until 1. 	 1  
sentence  1  presumably communicates nothing more than that a particular relationship held between a stadium and a park for some period of time. we represent this meaning by an underlying locative primitive called *loc  the names of our primitive predicates are preceded with asterisks in order to differentiate them from surface lexical items . figure 1 illustrates the network representation we give to sentence  1 . 
figure 1  
　　　we want to define *loc and locate in such a way that when the meaning of locate is computed   i . e .   the definition is executed   we have the structure given in figure 1 generated in the nodespace and associated with sentence  1 - to accomplish that  we f i r s t define *loc so that it generates the appropriate structure. then 
we define locate in terms of *loc. 	first the definition of *loc: 
define as predicate *loc. 
x *loc at-loc l  from-time t1 to-time t1 . return with newtoken for  *loc   subj  x  at-loc  l  from-time  tl  to-time  t1. 
in this definition  the i n i t i a l line calls the special defining mode of the parser which sets up the basic node structure for the definition of a new concept. it also accepts the sentences that follow as instructions for processes which are executed each time the newly defined structure is actually used. the term predicate 
is the syntactic class to which *loc is being assigned. this class includes all relational terms which can stand as the main relational term of a sentence. the second line of the definition gives the argument frame for the definition. in this example  the structure that *l1c returns is a newly constructed token node  secondary node  for the primitive with the appropriate argument values inserted in place. 
     now we can define the stative sense of the verb locate: 
define as predicate locate. 
x locate at-loc l  prom-time tl to-time t1 . iswhen x *loc at l from t1 to t1. 
 other senses of locate can also be defined  but they are not shown in this example.  note here that when the definition of locate is invoked  a statement involving *loc is asserted. whenever this happens  the definition of *loc is invoked and a structure similar to that in figure 1 is generated. this structure is then passed back through the definition of locate and in this case returned hack to be associated with the surface proposition from which it was invoked. thus  the structure generated by *l1c becomes associated with the use of the verb locate. the term is when is an action of sol which carries out the details of passing back the newly constructed structures. 
       change-of-states. the next simplest type of verb component is that of the change of state where no par-
ticular causative component is specified or implied. for example: 
the train moved out of the station 
	at 1 o'clock. 	c1  
in this sentence the subject  train  is the object of moved  not the causative agent. letting  change he the underlying primitive indicating change of state  we i l lustrate the network structure for sentence  1  in fig-
ure 1. 
figure s. 
       we want to define *change in such a way that it constructs structures like those shown in figure 1. the features of these structures are: 1  indicate that the former state  from-state  terminated at the time of the change; 1  indicate that the final state  to-state  was initiated at the time of the change; 1  construct and return with a new token node for change with each of the arguments f i l l e d with the appropriate structures. the sol definition of  change is this: 
define *change as operator. 
*change from-state s1 to-state s1 at-time t. 
understand that s1 ended at t. 
understand that s1 started at t. 
return with newtoken for  *change   fromstate  si  to-state  s1  at-time  t. 
       we are now ready to define the intransitive  i.e.  non-causative  sense of the verb move. 	we call this 

sense m1ve1 to distinguish it from the general sense of move which contains a causative component. the noncausative sense simply indicates a change from one locative state to another one. the sol definition for m1ve1 is this: 
define as predicate m1ve1. 
x m1ve1  from-loc l1 to-loc l1 at-time t . 
iswhen a  change from the state that x is located at l1 to the state that x is located at l1 occurs at t. 
note that when this definition is evaluated  it invokes *loc twice  through the two uses of locate  and passes the structures built by *l1c to *change where the final structure of the form in figure 1 is put together and then associated with the current invocation of move. 
　　　causatives. the prototypical causal verb is  of course  the verb cause itself. the complexity of the causal component of verbs stems from the fact that there are at least three qualitatively different sorts of causes of events. as an illustration  consider the following five sentences: 
the cowboy caused ambrose to wake by putting water on him. 
the cowboy caused ambrose to wake with  1a  a bucket of water.  1b  the cowboy caused ambrose to wake.  1c  the water caused ambrose to wake. 
ambrose was awakened by water being  1d  put on him.  1e  sentence  1a  illustrates the specification of all three types of causes: 1  the agentive cause  the cow-
boy ; 1  the instrumental cause  the water ; 1  the method  the putting of the water . sentences  1b  1e  illustrate some of the surface forms in which these causes can appear. we hold the basic underlying model of causatives to be that  someone does something with some instrument.  if the event is fully specified  then that event is taken to be the cause; otherwise a dummy act  *d1  is inserted into the structure. figure 1a-e gives the network representations for the sentences  1a - 1e . 
figure 1a-e 
note in 1a that the structure for put  from figure 1  is the event causing ambrose to wake. when the event is not known it is replaced by *d1 with the agent or instrument properly f i l l e d i n . 
     we are now in a position to define cause in such a way that the proper causative structure will be generated whenever the definition of cause is executed: 
define as predicate cause. 
agent x cause proposition y  method m instrument i at-time t . 
if m is specified  understand that m started at t  evaluate m  call m  act   
else call newtoken for   d 1    agent  x  instrument  i  act. 
understand that y started at t. 
evaluate y. 
return with a newtoken for  *cause   event  act  result  y. 
in this definition we f i r s t check to see whether the method is specified; if so  we say that it was i n i t i ated at the time of the cause  compute the structure associated with the method  by evaluating the procedure mj  and save that structure in a variable called 
act. 	in case the method is unspecified  we build a dummy action and store it in act. we then compute the structure for y  the caused event  by evaluating the 
procedure for y . using the predicate for the primitive sense of cause  we now link the causative event to the resultant event. finally  the procedure returns with a structure that represents the entire definition. 
     now that we have defined the primitives for the three basic types of components  we can use these as 
building blocks to define ever broader classes of 
verbs with increasingly natural definitions. we can  for example  define the verb move as it appears on the surface. the sol definition of move is this: 
define as predicate move. 
 agent x  moves y  from-loc l1 to-loc l1 method m at-time t . 
if x is not specified  iswhen y move! from l1 to l1 at t  
else iswhen x caused y to movel from l1 
to l1 by m at t. 
here move is defined only in terms of the intransitive move  move1  and cause. similarly  we can define the verb put in terms of move so that the structure illustrated in figure 1 is produced: 
define as predicate put. 
asent x puts y at-loc l  at-time t . iswhen x moves y to l at t. 
       note that these definitions do more than simply rewrite one verb in terms of another. the important point about the entire memory model is the type of representational structure that is constructed with the network. with these verb definitions  the primitives build new structures and modify old information. thus  in the definition of move  the last line performs the 
processes for cause and also the processes defined for m1ve1. cause both builds a structure for the causal factors and also performs whatever processes are represented by m  the method. the process for m is passed as an argument down from the original sentence that was entered through the parser  through the definitional structure for move  and finally to the definitional structure for cause. there it is finally executed  building whatever network structure the method m represents. 
the three drugstores problem 
       in this section we give an example of one problem being analyzed by our research group. a major feature of the way that a person views the events of the world is in terms of their causal factors. that is  we tend to disbelieve that an event could simply happen by itself; rather  we tend to believe that an event must have a cause. the tendency to give causal reasons for events is important because it affects the ways in 
which people make use of information. to illustrate the point  we analyze the three drugstores problem. 
       the basic problem before us was eloquently posed by abelson and reich. we paraphrase their version of the problem in this way: 
suppose an individual says a sentence such as  
	 i went to three drugstores.  	 1  
a response based on syntax only might be  
	 how did you go to three drugstores   	 1  
a response based on some semantics might be  
 what useful things did you buy in three 
	drugstores   	 1  
but the most natural response ought to be  
 how come the f i r s t two drugstores didn't 
	have what you wanted   	 1  
　　　solving the drugstore problem. just what must the required processes look like to be able to solve the drugstore problem  to solve the f i r s t few levels a l l that is needed is a pattern-match program that examines the structure of the verb of the sentence and compares the allowable arguments with those actually presented. thus  in the sentence   i went to a drugstore   we see that the to-location is provided but not the from-location  the method  or the time. thus  it is really a simple matter to construct questions like  1 . 
　　　to be more intelligent a basic decision must be made: should the missing information be requested  the answer is usually no. in normal conversation information is omitted either because it is assumed to be provided by the preceding or following context or because it is unimportant to the conversation. the patternmatch routines  inside a procedure called comprehend  f i l l in information by examining the structure of preceding sentences. sometimes the information in prior sentences might be appropriate to later ones  and sometimes the information given in the present sentence might f i l l in missing arguments from previous sentences. when missing arguments are noticed  an attempt is made to answer the implicit question provided by their absence through an examination of the data base. in addition  the present input is examined to see whether it can f i l l arguments missing in the data base being constructed from the conversation. 
　　　so far  we have simply investigated a simple means for f i l l i n g out the syntactic pattern for verbs  albeit with some sophistication in determining when to ask for mors information. the next step is more complex. suppose we wish to determine why someone has gone to the drugstore. again  we should not simply have to ask why  but rather determine the general reasons for going to the stores. for this point the comprehend routine must be intelligent enough to examine a more general data base. now a fair amount of inference is required: we need to match the basic paradigm with the specific information given by the parsed sentence. this is not easy when one considers that many different paradigms w i l l probably be stored. if the sentence had been   john went to a shoestore   then the same analysis should clearly not yield the query   what did john buy at the shoestore   the comprehend routine must be flexible enough to solve this part of the problem by itself. a large amount of world knowledge is needed to solve the general problem. 
　　　this brief analysis shows that in order to have intelligent conversation it is necessary to be able to generate internal questions and their answers  whenever information is missing some attempt must be made to f i l l in the gap  sometimes by asking appropriate questions  but usually by internal problem solving. in general  information should not be requested by means of a question unless there is some actual need for it at the mo-
ment. moreover  it would appear that the information should be asked from the very highest level down. thus  the f i r s t question asked should refer to the motive and results of the operations being described. only later should specific details of the method be asked. 
　　　in the implementation of the memory model system at the time of this writing  a l l the levels of analysis can not yet be performed. basically  the implementation is complete up to the level of the sophisticated internal answering of questions. thus  it has been an easy matter to implement a question answering routine to ask questions like the following for the input sentence: how did john go to the drugstore  what did he do afterwards  with whom did he go  at the moment  the basic routines to ask such questions as  what did he buy at the drugstore   are close to operation  but the construction of the system that can ask the question o r i ginally posed   how come the f i r s t two drugstores didn't have what you wanted   s t i l l remains some distance away. the memory representation provides a rich environ-
merit for simulating human cognitive processes. 	the major ideas have been implemented  yielding an active network representation with an english parser that allows interaction with the network and ready extendabili t y . 	actual simulations of human cognitive tasks have just begun  and although work is in progress in a variety of areas  no large system has yet been completed. however  for a description of the use of this system in human problem  solving  see the paper by eisenstadt and kareev. 



figure 1 
1 


1 

figure 1 

1 
1 





1 



1 







