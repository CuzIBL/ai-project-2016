
time series data abounds in real world problems. measuring the similarity of time series is a key to solving these problems. one state of the art measure is the longest common subsequence. this measure advocates using the length of the longest common subsequence as an indication of similarity between sequences  but ignores information contained in the second  third  ...  longest subsequences.
in order to capture the common information in sequences maximally we propose a novel measure of sequence similarity - the number of all common subsequences. we show that this measure satisfies the common properties of similarity functions. calculating this measure is not trivial as a brute force approach is exponentialin time. we present a novel dynamic programming algorithm to calculate this number in polynomial time. we also suggest a different way of extending a class of such measures to multidimensional  real-valued time series  in the spirit of probabilistic metric spaces.
we conducted an experimental study on the new similarity measure and the extension method for classification. it was found that both the new similarity and the extension method are consistently competitive.
1 introduction
a time series is a sequence 1 of symbols or real numbers  representing the measurements of a variable at  usually  equal time intervals  gunopulos and das  1 . when more than one variable are involved we get a multidimensional time series. examples of time series include: stock price movements  volume of sales over time  daily temperature readings  ecg signals  and gene expressions. in tasks involving time series we need to consider the issues of indexing and searching  agrawal et al.  1; goldin et al.  1  and similarity measuring  gunopulos and das  1  - the latter being the focus of this paper. one state of the art measure is the longest common subsequence  lcs   hirschberg  1; bergroth et al.  1 .
¡¡a subsequence is obtained from a sequence by removing 1 or more symbols  and a common subsequence of a set of sequences is a subsequence of every sequence in the set. lcs advocates the use of the longest common subsequence as an indication of the similarity relationship between sequences. however the common information shared between sequences is more than that contained in the longest common subsequence. the second longest  third longest and  in general  all common subsequences contain some common information to a certain degree. the use of all possible common information between sequences is intuitively appealing and may provide a fuller picture about the similarity relationship between the sequences. how to measure all such common information between two sequences 
¡¡the above question can be answered by addressing a new problem  all common subsequences  acs  - counting the number of all common subsequences. we expect that two sequences are more similar to each other if they have more common subsequences. acs is conceptually novel as far as we know. to motivate acs  we consider three sequences {¦Á ¦Â ¦Ã} = {cbabca bcabac abcade}. the set of all common subsequences of ¦Á and ¦Â is  ignoring the empty se-
 
                             cb  cba  cbac  cbc  cc}. the set of all common subsequences of ¦Á and ¦Ã is: {a  aa  ab  aba  abc  abca  ac  aca  b  ba  bc  bca  c  ca}. clearly lcs ¦Á ¦Â  = lcs ¦Á ¦Ã  = 1. however acs ¦Á ¦Â  = 1  acs ¦Á ¦Ã  = 1  suggesting there is a relatively higher degree of commonality in ¦Á and ¦Â than in ¦Á and ¦Ã. however this difference can not be revealed by lcs.
¡¡solving the acs problem is not trivial. a brute-force approach has an exponential time complexity as we need to verify all subsequences. we take a dynamic programming approach  and develop a novel algorithm to calculate acs between two sequences in polynomial time.
¡¡in order to evaluate acs as a similarity measure we need to extend acs  as well as lcs  to real-valued  multidimensional time series as both lcs and acs are defined in terms of symbolic sequences. we suggest a novel method for this purpose  which requires determination of equality/inequality. this is done in the spirit of probabilistic metric spaces  where we consider the probability that two data points are the same. this extension method is different from the threshold-based extension method  vlachos et al.  1   where two data points in a multidimensional time series are deemed equal if they are close enough  judged by a threshold  in every dimension. under this extension the acs similarity is evaluated in an experimental study with some real-valued  multidimensional time series. the study shows that acs is favorably comparable to lcs in general  and offers some property that is absent from lcs. additionally the extension method is sound and competitive consistently.
¡¡the rest of the paper is organized as follows. the longest common subsequence concept is briefly reviewed in next section as it inspired the work presented here. our all common subsequence concept is presented in section 1  along with a dynamic programmingalgorithm to calculate acs. a method is presented in section 1  which can be used to extend acs as well as lcs to multidimensional  real-valued time series. evaluation is discussed in section 1 and related work is discussed in section 1  followed by some concluding remarks.
1 longest common subsequence
the longest common subsequence  as a computer science problem  is concerned with the search for an efficient method of finding the longest common subsequence of two or more sequences. a milestone in the study of lcs is the development of dynamic programming algorithms  hirschberg  1; bergroth et al.  1   which calculates lcs in polynomial time.
¡¡the dynamic programming algorithm for calculating lcs was originally designed to work for one dimensional sequences of discrete values. it was extended in  vlachos et al.  1  to work for multidimensional  real-valuedsequences as follows:
¡¡let a and b be two d-dimensional sequences with length n and m respectively: a =  a1 ¡¤¡¤¡¤  an  and b =  b1 ¡¤¡¤¡¤  bm   where ai =  ai 1 ¡¤¡¤¡¤  ai d   and bj =
 bj 1 ¡¤¡¤¡¤  bj d . let head a  =  a1 ¡¤¡¤¡¤  an 1  and head b  =  b1 ¡¤¡¤¡¤  bm 1 . then the lcs similarity between a and b is  vlachos et al.  1 :
 1 
 otherwise
where ¦Ä is an integer that controls matching in time  and  is a real number that controls matching in space. the lcs algorithm  thus restricted  has a computationalcomplexity of o ¦Ä n + m   if we only allow a matching window ¦Ä in time  das et al.  1 . the advantage of lcs is that it supports elastic and imprecise matches.
1 all common subsequence
here we consider sequences of discrete symbols. in the next section we will consider sequences of multidimensional  realvalued sequences.
¡¡let a be a finite set of symbols. a sequence ¦Á is an ordered set  where si ¡Ê a. for simplicity we also write the above sequence as s1 ¡¤¡¤¡¤sn. the empty sequence is an empty set and is denoted by . the data space v is then the set of all sequences.
¡¡consider two symbols a b in a sequence ¦Á. looking from left to right  if a comes before b we write a ¡Ü¦Á b. consider two sequences ¦Á and ¦Â. if ¦Á can be obtained by deleting some symbols from ¦Â  we say ¦Á is a subsequence of ¦Â  or  equivalently  ¦Â is a supersequence of ¦Á  and we write it is obvious that the  relation is reflexive and transitive.
¡¡a common subsequenceof two sequences is a subsequence of both sequences. for two identical sequences  they have the maximal number of common subsequences relative to their length. for two different sequences  they have the minimal number of common subsequences. therefore the number of common subsequences is an indication of similarity. whether or not it is a good similarity indicator is unclear  but will be evaluated in a later section. we call this all common subsequences  acs  similarity. for two sequences ¦Á and ¦Â  we write acs ¦Á ¦Â  for the acs similarity of the two sequences. lemma 1. the acs similarity has the following properties:
  acs ¦Á ¦Â  ¡Ý 1.
  acs ¦Á ¦Â  ¡Ü acs ¦Á ¦Á  and acs ¦Á ¦Â  ¡Ü acs ¦Â ¦Â .
  acs is symmetric  i.e.  acs ¦Á ¦Â  = acs ¦Â ¦Á ;
  acs ¦Á ¦Â  = acs ¦Ár ¦Âr   where ¦Ár is the reverse sequence of ¦Á.
¡¡the first two properties are common to all similarity measures  osborne and bridge  1   but the last two are additional. the proof of the lemma follows directly from the definition of acs. the following lemma shows how many subsequences there are in one sequence  which can be proved by mathematical induction.
lemma 1. for a sequence of length m  the number of its subsequences is  which includes the empty sequence.
¡¡the relationship between sequence and subsequence is shown in the following lemmas. let n ¦Á|¦Ã  be the set of subsequences of ¦Á that are supersequences of ¦Ã  and n ¦Á  be the set of all subsequences of ¦Á.
lemma 1. consider. then

proof.  : suppose. consider x ¡Ê n ¦Á|¦Ã1 . then
. since the relation  is clearly transitive we have
. therefore x ¡Ê n ¦Á|¦Ã1 . we then have n ¦Á|¦Ã1    n ¦Á|¦Ã1 .
  : suppose n ¦Á|¦Ã1    n ¦Á|¦Ã1 . by definition we have for allfor all y ¡Ê n ¦Á|¦Ã1 .
to show that  we assume it is not true. then
n ¦Á|¦Ã1   but ¦Ã1 ¡Ê n ¦Á|¦Ã1 . therefore it is not possible n ¦Á|¦Ã1    n ¦Á|¦Ã1   contradicting the assumption. 
lemma 1. if and only if n ¦Á    n ¦Â .
proof.  : suppose. consider x ¡Ê n ¦Á . then x is a subsequence of. by the transitivity of  we haveis a subsequence of ¦Â. therefore we have x ¡Ê n ¦Â . this proves n ¦Á    n ¦Â .
 : suppose n ¦Á    n ¦Â . to show thatwe assume it is not true. then. however ¦Á ¡Ê n ¦Á . therefore it is not possible that n ¦Á    n ¦Â   contradicting the assumption. this proves. 
1	calculating the number of all common subsequences
now we discuss how to calculate the number of all common subsequences  acs a b   of any two sequences a and b. since the number of subsequences of a sequence is exponential in its length  a straightforward brute force approach is clearly not feasible. we turn our attention to dynamic programming.
¡¡ dynamic programming can be thought of as being the reverse of recursion. recursion is a top-down mechanism - we take a problem  split it up  and solve the smaller problems that are created. dynamic programming is a bottom-up mechanism - we solve all possible small problems and then combine them to obtain solutions for bigger problems. the reason that this may be better is that  using recursion  it is possible that we may solve a small subproblem many times. using dynamic programming  we solve it once.    hirschberg  1 
theorem 1. consider two sequences  a =  a1 ¡¤¡¤¡¤  am  and b =  b1 ¡¤¡¤¡¤  bn . let n i j  be the number of common subsequences of  a1 ¡¤¡¤¡¤  ai  and  b1 ¡¤¡¤¡¤  bj   i.e.  the prefixes of sequences a and b of lengths i and j. then
n i j  = n i   1 j   1  ¡Á 1  if ai = bj
n i j  = n i   1 j  + n i j   1    n i   1 j   1  
if
consequently acs a b  = n m n .
proof. let s i   1 j   1  be the set of all common subsequences between  a1 ¡¤¡¤¡¤  ai 1  and  b1 ¡¤¡¤¡¤  bj 1 . so n i   1 j   1  = |s i   1 j   1 |.
¡¡if ai = bj  then s i j  = s i 1 j 1 ¡Ès i 1 j 1 ai. here s i   1 j   1 ai = {xai : x ¡Ê s i   1 j   1 }. therefore n i j  = n i   1 j   1  ¡Á 1.
¡¡if  then some new common subsequences may be added to s i j   1  or s i   1 j  on top of s i   1 j   1 . therefore s i j  = s i j 1 ¡Ès i 1 j  s i 1 j 1 . consequently we have n i j  = n i j 1 +n i 1 j   n i   1 j   1 .	

algorithm 1: calacs - calculate all common subsequence

input: two sequences a and b.
output: the number of all common subsequences acs a b .
1 begin
1 for i = 1 to |a| do n i 1  = 1
1 for j = 1 to |b| do n 1 j  = 1
1 for i = 1 to |a| do
1do
1then
n i j  = n i   1 j   1  ¡Á 1
1 else n i j  =
n i   1 j  + n i j   1    n i   1 j   1 
1 end
1 end
1 acs a b  = n |a| |b| 
1 end

note that the ¦Ä parameter as discussed in section 1 can also be used together with the above calacs algorithm.
¡¡the acs a b  is unbounded. we can normalize it by the number of all subsequences  which can be calculated as in lemma 1.
1 extension of lcs and acs
the acs is presented in the previous section on the assumption that a sequence is an ordered set of symbols or discrete values. when a sequence is real-valued or multidimensional  the dynamic programming algorithm for calculating acs must be extended.
¡¡this is also the case for lcs  but a method is presented in  vlachos et al.  1  to extend lcs. this extension method can be directly used for acs but  as discussed in section 1  the threshold  is hard to choose and is usually different for different data. this is not desirable  keogh et al.  1 .
¡¡here we present a different solution for the problem of extending lcs and acs  possibly some other similarity/distance measures as well  where we need to decide if two points are equal  equality checking . our solution is in the spirit of probabilistic metric spaces  schweizer and sklar  1   where the line between 'equal' and 'not equal' is probabilistic. consider two sequences a =  a1 ¡¤¡¤¡¤  am  and b =  b1 ¡¤¡¤¡¤  bn . for any pair ai and bj we need to decide  at some point in the lcs and acs algorithms  if they are equal. the answer to this question may not be deterministic  especially when real numbers are involved. in general the question can be answered by measuring the probability  prob ai = bj   that they are equal 1.
¡¡a general template for both lcs and acs is the following. the distance/similarity between a sequence  a1 ¡¤¡¤¡¤  ai  and another sequence  b1 ¡¤¡¤¡¤  bj  is
 1 if ai = bj
	ds1	otherwise
in the case of lcs  ds1 = 1+ds i 1 j 1   and ds1 = max ds i 1 j  ds i j 1  . in the case of acs  ds1 = ds i   1 j   1  ¡Á 1  and ds1 = ds i j   1  + ds i   1 j    ds i   1 j   1 .
we propose to use the following template instead:

where. this approach avoids using the threshold .
¡¡the question now is how to determine prob ai = bj . in the case of symbols or discrete values  we can simply and reasonably take prob ai = bj  = 1 if ai = bj  and prob ai = bj  = 1 otherwise. in the case of real numbers  either 1-dimensional or multidimensional  we can estimate prob ai = bj  by the distance between ai and bj normalized to the range of  1 . note that the distance can be the euclidean distance or other distance. this is a simplistic approach  but a more involved approach is beyond the scope of this paper.
1 experimental evaluation
we conducted an experimental study to evaluate the acs similarity measure along with the extension method. in this study we demonstrate the utility of acs for classification in an experiment on some public datasets. we consider the following multidimensional  real-valued time series: ecg1c  ecg1c and auslan.
¡¡the ecg datasets are from the bidmc congestive heart failure database. the original database contains two ecg signals  but keogh et al  keogh et al.  1  separated the signals and create two datasets  ecg1c - ecg clustering.txt and ecg1c - ecg clustering 1class.txt  of one-dimensional time series. each instance of 1 contiguous data points  about 1 heartbeats  of each signal is randomly extracted from each long ecg signals of each patient  class . twenty instances are extracted from each class  resulting in eighty total instances for each dataset.
¡¡the australian sign language  auslan  data is available from  keogh and folias  1 . we consider a clean  preprocessed version of auslan. there are 1 classes: 'come'  'girl'  'man'  'maybe'  'mine'  'name'  'read'  'right'  'science'  'thank'. there are 1 examples in each class  and all classes come from one signer. all time series are of the same length  by interpolation .
¡¡auslan is an interesting and challenging dataset as  it is multivariate  and without exploiting the multivariate property it is hard to do much better than random guessing.   keogh and folias  1  we use the following similarity/distance measures on the datasets: euclidean  euc   longest common subsequence  lcs  and all common subsequences  acs . because all of the datasets are real-valued and some are multidimensional  these distance/similarity measures need to be extended. we consider the following combinations:
  euc: the standard euclidean distance  used in our extended similarity measures  lcs weuc  acs weuc .
  ncm: the ncm similarity  see  wang and dubitzky  1    used in our extended similarity measures  lcs wncm  acs wncm .
  lcs vhgk: the extended lcs presented in  vlachos et al.  1   see section 1 .
  acsvhgk: acs extended in a way similar to lcs vhgk.
  lcs weuc: lcs extended as in eq.1  where the probability is estimated  simplistically  as the normalized euclidean distance.
  acs w euc: acs extended as in eq.1  where the probability is estimated as the normalized euclidean distance.
  lcs wncm: where the probability is the normalized ncm.
  acs w ncm: where the probability is the normalized ncm.
¡¡we used the leave-one-out evaluation strategy in the knearest neighbors  knn  classification framework  meaning that we classify every time series based on all other time series and then we measure the classification accuracy. there are two parameters in the experiment:   which are required by acs and lcs. for simplicity we set ¦Ä = 1 for all measures  without loss of generality. for  we testexperimented on samples of the data with various values and set it to a value that gives the best overall result  that is 
. additionally we set parameterk  requiredfor knn  to 1 1 1 1.
¡¡experimental results are presented in tables 1  1  1  and 1  from which we observe the following:
  when used as part of acs or lcs  ncm gives consistently and
  acs has an edge over lcs  especially for relatively large k: acs w euc   lcs weuc and acs wncm   lcs wncm.
  the extension method presented in this paper  see section 1  has an edge over that suggested in  vlachos et al.  1   especially for small k values: lcs w euc   lcs vhgk.
¡¡we believe that the superior performanceof acs over lcs is due to the fact that acs is able to capture more common information in a pair of sequences than lcs.
1 related work
paper  lin et al.  1  presents a novel measure of dissimilarity between two time sequences - the degree of difference for pattern p  weighted by the confidence of the pattern  in two time series. given two time series  s1 and s1  they are first of all represented as symbolic sax sequences. a pattern is a subsequence in the sax representation of a time series. without weighting  the dissimilarity is

where fi p  is the frequency of pattern p in sequence si.
¡¡paper  yang et al.  1  introduces a new measure of similarity/distance between symbolic sequences. the measure is based on counting the occurrences of m-bit  i.e.  consisting of m symbols  words  i.e.  subsequences  in a symbolic sequence. without weighting by the probability of each word in a sequence  the distance between two sequences s1 and s1 is defined as follows:

where ri wk  is the rank of subsequence wk 1 in sequence si.
¡¡dsim and dm are both related to acs in the sense they consider all common subsequences in s1 and s1. however there are significant differences:
  conceptually both dsim and dm measure the average  weighted  difference of the frequency of a subsequence p in s1 and the frequency of p in s1  while acs counts the number of common subsequences of two sequences.
  they all need to look through a large  exponential search space for subsequences  but acs has a dynamic programming algorithm to calculate the similarity in polynomial time.
  acs can be applied to sequences of real numbers without the need of transformation  while dm can not. to use dsim for sequences of real numbers  the symbol sax representation  a type of transformation is needed.
1 conclusion
this paper presents a conceptually novel similarity measure for time series data - all common subsequences or acs. it is intended to maximally capture the common information shared by two time series. a novel dynamic programming algorithm is presented to calculate the number of all common subsequences.
¡¡to use acs and other equality-checking based sequence similarity measures  e.g.  lcs  to handle multidimensional  read-valued time series  we need to extend these measures. this paper presents an extension method  which is inspired by the theory of probabilistic metric spaces. we seek to use the probability that two points are equal in calculating the similarity when a time series is real-valued or multidimensional.
¡¡we conducted an experimental study to evaluate the new similarity measure and the extension method. our experimental results show that acs is favorably comparable to lcs  and it displays a distinctive feature - stable performance as more data are considered for decision  i.e.  as k gets larger   making acs suitable for those tasks whose aim is to find a set of ranked sequences  e.g.  information retrieval . the results also show that the extension method presented in this paper works well  and appears better than that discussed in  vlachos et al.  1 .
¡¡we feel that the way in which we estimate the probability of two data points being equal is a bit crude. in our future work we intend to develop more involved methods in order to make it more effective and efficient.
