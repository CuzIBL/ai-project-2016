recognition and depth perception 
of objects in real world scenes* robert j. douglass 
department of computer sciences 
university of wisconsin-madison 
　　　automatons and other robotic applications which are designed to move around and interact with their physical environment need a computer vision system for recognizing and understanding the spatial relationships of objects in real world scenes. the perceptual system must be able to identify salient objects in a scene  develop an understanding of their spatial relationships  and maintain continuity from one view to the next as either the objects or the system's camera moves through the scene. 
　　　outlined here and described in more detail in douglass  1  is a system which has been implemented in simula and tested on hand coded outdoor scenes of simple subjects such as houses 
and automobiles. it uses a recognition cone  a segmentation algorithm for dividing a scene into similar regions and a routine for constructing a three dimensional world model. visual inference routines interpret perspective  shadows  highlights  occlusions  shading and texture gradients  and monocular motion parallax. other visual knowledge is added with long term models and short term object representations. 
　　　the f i n a l program w i l l be tested on color photographs of outdoor scenes using as input a 
　　　series of views of the same scene from different angles which approximates what an automaton would  see  as it moves down a street. 
scene description program 
　　　the program's recognition cone  after uhr: 1  is a parallel-serial cone structure consisting of a number of processing layers. the f i r s t layer contains an array of light i n tensities in three colors as digitized from the system's camera. successive layers average the picture  compute several measures of texture and color  and detect edges and angles. in the higher layers of the cone  object names are assigned to various areas based on the presence of certain configurations of edges and lower level features. the last layer contains a number pf textural  color  and edge descriptors for each point of the array and one or more possible interpretations for that point. ' 
　　　the output of the cone is segmented into regions by a region growing algorithm similar to yakimovsky's  which uses a zobrist-thompson grouping operator to estimate the probability of an edge between two points in the picture array. the region grower builds a description of each segment as it is grown including texture  color  size  adjacent segments  order of connectivity  brightness  and a l i s t of possible 
*this work is partially supported by the national science foundation  nsf grant mcs1 
v i s i o n - 1 : 
1 
interpretations for that segment. 
　　　the segments produced by the region grower are built into a three dimensional world model by a placement routine  the heart of the whole system. this routine uses the descriptions of the segments  a set of long term models of objects  the visual inference routines  and any contents of the world model from the previous view to form the segments into three dimensional surfaces representing objects in the scene. the routine begins by making an i n i t i a l approximation to the segments depth or orientation in space and by selecting the highest weighted object name from the l i s t of possible interpretations produced by the recognition cone. each inference routine then successively improves the placement of the segment by using information in the long term object models  comparing the current placement with the previous contents of the short term memory  or examining the placement and interpretation of neighboring segments. 
　　　the visual inference routines assist in interpreting the objects'positions in the scene with general information about the relationship between the visual properties of the physical surfaces in the real world and the manifestations of these properties on the program's digitized input array. the routines contain heuristics for a  deciding when one segment is occluding another  in shadow  or highlighted  and b  i n terpreting shading and texture gradients  linear perspective  and motion parallax due to the movement of the camera between successive views of the scene. 
　　　long term object models are used by both the visual inference routines and the placement routine. they provide a general description of shape  size  orientation  and expected context for the object within a scene and contain weights for each part of the description indicating the 
system's confidence. for example  if the occlusion routine  part of the visual inference routine  finds a segment labeled window adjacent to a segment labeled w a l l   it w i l l consult the object model for a window and find the window segment has a high probability of touching the wall segment and is therefore lying in the same plane rather than occluding i t . 
　　　the system integrates a sequence of slowly changing views of one scene over time by using a motion parallax routine. segments from one view of the scene are matched with segments from the next view to compute their shift. this shift and the parameters describing the camera's movement between the two views is used to compute the orientation of a segment in space. 
