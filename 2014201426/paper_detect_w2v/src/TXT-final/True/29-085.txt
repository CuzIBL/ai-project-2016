 
in tree search  depth-first search  dfs  often uses ordering successor heuristics. if the heuristic makes a mistake ordering a bad successor  without goals in its subtree  before good ones  with goals in their subtrees   dfs has to unsuccessfully traverse the whole bad subtree before finding a goal. to prevent this useless work  we present a new strategy called interleaved depth-
first search  idfs   which searches depth-first several subtrees -called active- in parallel. idfs assumes a single processor on which it interleaves dfs on active subtrees. when idfs finds a mistake  it traverses partially the bad subtree. idfs does not reexpand nodes and uses a memory amount linear in search depth  with a bounded number of active subtrees . idfs outperforms dfs if the heuristic improves from the first to the second tree level. experimental results on hard solvable problems confirm the practical validity of idfs. 
1 	introduction 
in tree search with bounded depth  depth-first search  dfs  is widely used because it is simple and its space requirement is linear in search depth. often  dfs is combined with some heuristic which estimates the likelihood of goal existence in subtrees rooted at internal nodes. in this case  dfs is called ordered because node successors are ordered from left to right by decreasing heuristic value. we say that an internal node is good if the subtree below it has goal nodes  and bad otherwise. with this definition  heuristic ordering tries to move good successors to the left and bad successors to the right. given that dfs starts searching on the left  heuristic ordering tries to speed up goal finding. 
   'this research is supported by the spanish cicyt proyect tic1-c1. 
search 
　heuristic advice is not perfect and  in occasions  it may be wrong. a heuristic makes a mistake when it orders a bad successor before a good one. ordered dfs falls into a mistake when it visits the bad node. if mistakes appear in the ordering of current node successors  ordered dfs falls into them as it progresses from left to right  until it finds a good node. falling into a mistake at shallow levels of the search tree is quite costly  because dfs is forced to unsuccessfully traverse a large subtree without goals. falling into a mistake at deep levels is less costly  because dfs has to traverse a smaller subtree. typically  heuristics provide better advice at deep levels than at shallow levels  so mistakes are more likely to appear at shallow levels  where they are more costly to recover. the presence of mistakes  specially at shallow levels  is a weak point for dfs performance  against which this algorithm has no defense. 
　in this paper  we present a new approach to prevent - at least partially- ordered dfs falling into mistakes. we call this approach interleaved depth-first search  idfs   and it works as follows. while dfs searches sequentially - from left to right - subtrees at each tree level  idfs searches in parallel several subtrees at some tree level. calling active those subtrees being searched in parallel  
idfs searches depth-first the current active subtree until it finds a leaf. if it is a goal  search terminates. otherwise  the state of the current subtree is recorded and added to a fifo queue of active subtrees  from which it will be taken later to resume search at the point it was suspended. idfs selects another active subtree as the new current subtree and repeats the process. idfs simulates parallel dfs on active subtrees using a single processor. in this sense  idfs interleaves depth-first search on active subtrees. if the heuristic orders successors with mistakes  idfs avoids falling completely into them by distributing search among active subtrees  which are expected to include some good subtree. 
　idfs provides advantages over dfs in average performance on hard solvable problems. performance improvement is directly related to mistake occurence: the more mistakes there are  the higher the performance improvement of idfs over dfs. if no mistakes occur dfs is the best algorithm. idfs is meaninful for solvable problems only. unsolvable problems require both idfs and dfs to traverse the whole search tree to detect goal absence  so they will expand the same nodes -although in different order- causing no difference in performance1. idfs provides practical advantages over dfs on hard problems  where many mistakes are likely to occur at shallow levels. on easy problems  heuristics make few mistakes and idfs benefits do not pay off the overhead of searching simultaneously active subtrees. on the other hand  idfs requires more memory than dfs because it has to store active subtrees. at shallow levels  each active subtree requires almost as much storage as single dfs  so its number should be bounded to allow for the practical applicability of this algorithm. 
　this paper is organized as follows. in section 1 we describe related approaches. in section 1 we provide a detailed explanation of the idfs algorithm  analyzing its performance in section 1. in section 1 we give experimental results of this algorithm on different search problems  showing its practical applicability. finally  in section 1 we summarize the contributions of this work. 
1 	related work 
in the context of binary search trees  the idea of mistakes was introduced in the limited discrepancy search algorithm  lds   harvey and ginsberg  1 . given a heuristic to order successors  a discrepancy is not to follow the heuristic preference at some node  that is  to select the right successor as the next node. lds visits first the leftmost leaf of the tree  following a path without discrepancies. if it is not a goal  lds visits leaves with at most one discrepancy in their paths  visiting first paths with discrepancies at early levels  where the heuristic is supossed to be less accurate. if no goal is found  lds visits leaves with at most two discrepancies in their paths  etc. the proccess continues until it reaches the maximum number of discrepancies and the whole tree is searched. lds space requirement is linear in search depth and it has to reexpand nodes previously visited. from the analysis given in  harvey and ginsberg  1   lds outperforms dfs on easy problems. 
　this approach has been enhanced by the improved limited discrepancy search algorithm  ilds   korf  1 . at the iteration for k discrepancies  lds generates all paths with k or less discrepancies. but all paths with less than k discrepancies have already been visited in previous iterations. this is corrected by ilds  which at each iteration generates only those paths with exactly k discrepancies. 
l
   in fact  idfs will require more time than dfs due to overhead of switching active subtrees. 

figure 1: example search tree. first line is pure idfs leaf visiting order; second line is limited idfs leaf visiting order  as explained in the text. 
however  different from lds  ilds visits first paths with discrepancies at deep levels. regarding results on number partitioning  dfs outperforms ilds when no perfect partition exists. on problems with perfect partitions  dfs outperforms ilds using a simple heuristic  while ilds outperforms dfs when using the kk heuristic. 
the efficiency of parallel dfs is analyzed in  rao and 
kumar  1 . regarding search on ordered-bounded binary trees  parallel dfs expands no more nodes on the average than sequential dfs. on easy problems  those in which heuristics give good advice at early levels  parallel dfs offers no advantadge over sequential dfs. on hard problems  parallel dfs obtains substantial speedup over sequential dfs. 
1 	interleaved depth-first search 
1 	p u r e idfs 
pure idfs interleaves search among all successor subtrees of any internal node at any level of the search tree. idfs searches depth-first in a subtree until it finds a leaf. if it is a goal  search terminates. otherwise  it stores the search state of those subtrees traversed by the leaf path  and switches to the next subtree at the earliest level of the search tree. successors of a node are ordered  from left to right  by decreasing value of some heuristic estimation of good node likelihood. successor list is circular  so the next successor of the rightmost is the leftmost. 
　pure idfs is better explained with the example of figure 1  a ternary tree of depth 1. idfs interleaves search among subtrees at level 1  a  b and c in turn  switching from one to the next after reaching a leaf. when searching a  idfs interleaves search among its successors d  e and f  switching from one to the next each time subtree a is searched. when searching d  idfs visits its leaf successors a  1 and c in turn  one each time subtree d is searched. the same process occurs for subtrees b and c. idfs execution is as follows. it expands 1  a  d and visits a  a non-goal leaf. idfs stops visiting subtrees d and a} stores their states and switches to b  the next subtree at level 1  where idfs repeats the process: it expands b  g and visits ;  a non-goal leaf. idfs stops visiting subtrees g and b  stores their states and switches to  1  

figure 1: pure idfs algorithm. 
the next subtree at level 1  where idfs repeats the process: it expands c  j and visits r  a non-goal leaf. idfs stops visiting subtrees j and c  stores their states and switches to a. now  idfs resumes search on a  moving to the next successor e. this process is repeated again and again until a goal is found or the whole tree is searched. the first line in figure 1 indicates the order in which pure idfs visits leaves in the example search tree. the algorithm for pure idfs appears in figure 1. it is a function that takes node s as input and can return: success: a goal has been found  
	failure: 	no goal found  tree exhausted  
	continue: 	no goal found  tree not exhausted. 
when this function returns continue  it modifies the state of s as a side-effect  to record the search progress in the subtree rooted at s. 
　in its current implementation  pure idfs requires an amount of memory exponential in search depth  which renders it unapplicable. we have presented pure idfs for clarity purposes. in the next subsection we introduce the practical version of idfs. 
1 	limited idfs 
limited idfs interleaves search among a limited number of successor subtrees at some levels of the search tree. limited idfs distinguishes between two kinds of levels: parallel levels  where search is interleaved among some of its subtrees  and sequential levels  where search is sequential. for simplicity  we will assume that parallel levels start at level 1 and they are consecutive. given a node with successors in a parallel level  we call active those successors  and their subtrees  on which idfs interleaves search. limited idfs performs as pure idfs on active subtrees at parallel levels  and as dfs on sequential levels until it reaches a leaf. if it is a goal  search terminates. otherwise  it stores current subtrees and switches to the next active subtree at the earliest parallel level. as in the pure case  active successors are 
search 

figure 1: limited idfs algorithm. 
heuristically ordered  and the next active sucessor of the rightmost is the leftmost. in the example of figure 1  assuming that the only parallel level is level 1 and the number of active subtrees is 1  limited idfs works as follows. it expands 1  taking a and b as active subtrees. it expands a  d and visits a  a non-goal leaf. it stops searching a  stores its state and switches to b  the next active successor at level 1  where it repeats the same process: it expands b  g and visits j  a non-goal leaf. idfs stops searching b  stores its state and switches to a  where it resumes depth-first search visiting 1  another non-goal leaf. idfs stops searching a and resumes search on bf where it visits k. the same process goes on until an active subtree  say a  is exhausted  assuming that no goal has been found . then  a is replaced by c. the whole process terminates after finding a goal or exhausting the complete search tree. the second line in figure 
1 indicates the order in which this execution of limited idfs visits leaves in the example search tree. the algorithm for limited idfs appears in figure 1. it is worth noting that limited idfs does not reexpand nodes and  if the number of active subtrees is bounded to a number independent from the problem size  limited idfs requires a memory space linear in search depth. 

1 	efficiency analysis 
in the following we provide a probabilistic efficiency analysis of limited idfs versus ordered dfs. following the approach of  rao and kumar  1   efficiency is evaluated by the average number of leaves visited by each algorithm. we work on a search tree of depth d and uniform branching factor 1 containing goal nodes. for simplicity  we assume that no pruning is done. at the first level  there are b subtrees ordered from left to right  and we denote as  the probability of goal existence in the i-th subtree. 
　regarding dfs  the probability of finding a goal in the subtree i is the probability of goal existence at subtree t  under the condition that no goal has been found in subtrees before i  that is  
if the problem has solution  the probability that dfs finds a goal in the last subtree depends only on probabilities that no goal has been found in previous subtrees. 
this probability is  
s d  is the average number of leaves visited by dfs in a tree of depth d with goals. f d  is the number of leaves visited by dfs in a tree of depth d without goals  it is equal to 
 1  
where summand i-th is the expected number of visited leaves by dfs if it finds the goal in the i-th subtree and not before. expressions s d - 1  and t are  
 1  
 1  
　regarding idfs  we assume that level 1 is the only parallel level and all its subtrees are active. 1 d  is the average number of leaves visited by idfs in a tree of depth d with goals. 1 d  is  

depends on the quality of the heuristic at level 1  while ti depends on the best quality of the heuristic at level 
1. given a problem and a heuristic  1dfs outperforms dfs if there is a significant difference of heuristic quality between level 1 and level 1  enough to satisfy expression  1 . observe that the best should be better than t  to overcome the factor otherwise  if heuristic quality is similar in both levels  dfs is the algorithm of choice. 
　in general  heuristic quality improves with tree depth. this quality improvement increases with problem difficulty. on easy problems  heuristics are able to give good advice at shallow levels  leaving no room for significant improvement as they move to deeper levels. on hard problems  heuristic advice is not very good at shallow levels  and it improves as it goes into deeper levels. hard problems are good candidates to satisfy condition  1   for which we believe that idfs is a suitable algorithm. 
　in the previous analysis we assume that all subtrees of first level are active. if we suppose that a goal exists in the first k subtrees  we take these subtrees as active and the whole analysis can be repeated. expressions  1   1  and  1  are truncated at the k-th subtree  instead of including contributions until the 1-th subtree   expression  1  remains invariant and the discussion about heuristic quality is restricted to the first k subtrees. 
1 	experimental results 
a binary csp is defined by a finite set of variables taking values on finite domains under a set of binary constraints. csps are search tree problems with fixed depth -the number of variables- for which several ordering heuristics exist  so they are adequate to test idfs performance. we have taken forward checking  fc   a dfs based algorithm  substituting its dfs structure by idfs  obtaining  we have tested fc and  on a variety of random csp instances. a random binary csp  prosser  1  is defined by  where n is the number of variables  m is the number of values for each variable  p  is the proportion of existing constraints  and  pi is the proportion of forbidden value pairs between two constrained variables. constrained variables and their forbidden value pairs are randomly selected. 
　in csps  heuristics to order successors are value ordering heuristics. we have used the highest support heuristic for value ordering  and the lowest support heuristic for variable ordering  larrosa and meseguer  1  1. they are computed incrementally  and the consistency checks performed in their computation are included in the results. 
　we have tested fc and   both using the same heuristics  on difficult solvable instances in the left side 
     1  in that paper  both heuristics were considered under the generic name of  lowest support . current names represent more faithfully the heuristic criterion behind each one. 
search 

figure 1: visited nodes by fc and  for four random csp classes 
of the of c o m p u t a t i o n a l   i n s i d e the  mushy region'1  smith and dyer  1   where solvable and unsolvable instances coexist   for the following problem classes   n  rn pi    

where constraint density varies from very low  a  to totally connected  d . for all experiments   takes 
level 1 as the only parallel level  with 1 active subtrees for class  a   1 active subtreees for classes  b  and  c   and 1 active subtrees for class  d . results are contained in figures 1  and 1  with p1 as the varying parameter. each point is the average over 1 solvable instances. regarding the number of visited nodes  figure 1   on the left side of the plots problems are relatively easy  and both fc and  visit a similar number of nodes. going to the right  problems become more difficult and the number of visited nodes increases  but the increment for fc is much higher than the increment for  note the logarithmic scale . in this part  outperforms fc in a factor varying from 1 to 1. the rightmost point corresponds to the peak  or the point previous to the peak when the density of solvable problems at the peak is too low . at this point problems are very difficult and the number of visited nodes is similar for both algorithms. regarding the number of consistency checks  figure 1   we observe a similar picture with a linear scale. on easy problems at the left  both algorithms perform a similar number of checks. when problems become harder  the number of consistency checks increases  but fc performs more consistency checks than in this part   outperforms fc in a factor from 1 

figure 1: consistency checks performed by fc and for four random csp classes. 
to 1. for very difficult problems at the rightmost point  both algorithms perform a similar number of consistency checks. it is worth noting that  always performs better than or approximately equal to fc  but never significantly worse. 
　these results can be interpreted in full agreement with the efficiency analysis of section 1. for easy problems  the heuristic is good enough at the first level of the tree to provide almost always the good advice. mistakes are very rare so 1dfs provides no advantages over dfs. when problems become harder  more and more mistakes occur. for these problems  the heuristic can improve when it is computed at the second level of the tree  with respect to the same heuristic computed at the first level. it seems to do so because idfs outperforms dfs on these problems  in both number of visited nodes and consistency checks. for the hardest problems  going down a single level in the tree is not enough to improve significantly the heuristic quality. this is in agreement with the experimental results  since idfs and dfs behave similarly on the hardest instances. 
1 	open issues and conclusions 
experimental results confirm the efectiveness of idfs over dfs on hard solvable problems. however  some important questions to use idfs as a general search procedure remains to be answered  such as the depth of parallel levels in the search tree  their distribution  consecutive or not  and their number. related to this is the number of active subtrees in each parallel level  which could vary from one subtree to another inside the same parallel level. dynamic selection of these parameters  to adjust the procedure to specific problem characteristics  can also be devised. besides  given the close relation between idfs benefits and heuristics  further knowledge about the variation of heuristic quality with search depth is needed. all these issues remain to be investigated in the future. 
　in  korf  1   korf concludes:  the weakness of lds is dilettantism  whereas the weakness of dfs is excessive diligence. an ideal algorithm should strike the right balance between these two properties.  we have conceived idfs with this aim  trying to keep the right balance between exploration of the search tree and exploitation of heuristic advice  inside a complete algorithm. as in the case of lds and ilds  idfs is a new search strategy which outperforms classical ones on several problem classes  demonstrating how imaginative ways of search can effectively improve the current state of the art in search procedures. 
acknowledgments 
i thank lluis godo  javier larrosa  carme torras and the anonimous reviewers for their constructive comments. i thank josep lluis arcos for latex support. i also thank romero donlo for her collaboration on the preparation of this work. 
