 
speculative execution of information gathering plans can dramatically reduce the effect of source i/o latencies on overall performance. however  the utility of speculation is closely tied to how accurately data values are predicted at runtime. caching is one approach that can be used to issue future predictions  but it scales poorly with large data sources and is unable to make intelligent predictions given previously unseen input data  even when there is an obvious relationship between past input and the output it generated. in this paper  we describe a novel way to combine classification and transduction for a more efficient and accurate value predic-
tion strategy  one capable of issuing predictions about previously unseen hints. we show how our approach results in significant speedups for plans that query multiple sources or sources that require multi-page navigation. 
1 introduction 
the performance of web information gathering plans can suffer because of i/o latencies associated with the remote sources queried by these plans. a single slow web source can bottleneck an entire plan and lead to poor execution time. when a plan requires multiple queries  either to the same source or to multiple sources   performance can be even worse  where the overhead is a function of the slowest sequence of sources queried. 
　when multiple queries are required  speculative plan execution  barish and knoblock 1b  can be used to dramatically reduce the impact of aggregate source latencies. the idea involves using data seen early in plan execution as a basis for issuing predictions about data likely to be needed during later parts of execution. this allows data dependency chains within the plan to be broken and parallelized  leading to significant speedups. 
　to maximize the utility of speculative execution  a 
　good value prediction strategy is necessary. the basic problem involves being able to use some hint h as the basis for issuing a predicted value v. one approach involves caching: we can note that particular hint hx corresponds to a particular value vv so that future receipt of hx can lead to prediction of vy. as a result  a plan that normally queries source s1 with hx and subsequently source sz with vv can be parallelized so that both si and s1 are queried in parallel  the latter speculatively. unfortunately  caching has two major drawbacks. first  it does not scale well when the domain of hints is large. a second drawback is the inability to deal with novel  previously unseen  hints  even when an obvious relationship exists between hint and predicted value. 
　in this paper  we present an alternative to caching that involves automatically learning predictors that combine classification and transduction in order to generate predictions from hints. our approach succeeds where caching fails: the predictors learned usually consume less space than that demanded by caching and they are capable of making reasonable predictions when presented with novel hints  the latter leading to better speedups. specifically  our work contributes the following: 
  an algorithm that learns efficient transducers capa-ble of a variety of string transformations. 
  an algorithm that combines classification and trans-duction to learn value predictors 
　the rest of this paper is organized as follows. the next section briefly reviews information gathering and provides a motivating example for speculative execution. in section 1  we describe how classification and transduction can be used to build efficient and intelligent predictors. section 1 describes our learning algorithms that combine both techniques. section 1 describes experimental results of using our approach. finally  section 1 details the related work. 
1 preliminaries 
web information gathering plans retrieve  combine  and manipulate data located in remote web sources. such plans consist of a partially-ordered graph of operators o1.ott connected in a producer/consumer fashion. each operator o  consumes a set of inputs a1.ap  fetches data or performs a computation based on that input  and produces one or more outputs b1.bq the types of operators used in information gathering plans vary  but most either retrieve or perform computations on data. 

al and data integration 	1 

　to better illustrate a web information gathering plan  we consider the example plan carlnfo  shown in figure 1. given constraints on car type and pricing  carlnfo locates the make and model which has a median price closest to that specified and then retrieves the full review of that car from the web site consumerguide.com. the plan consists of four wrapper operators that fetch and extract data from various parts of the remote source. specifically  the plan involves:  a  querying carsdirect.com for the car make and model having a median price closest to that specified   b  querying consumerguide for the resulting make and model   c  retrieving the link to the summary page for that car  using the link provided in the search results   and  d  retrieving the full review using the link provided on the summary page. 

figure 1: the carlnfo plan 
for example  for the input  sedan  $1   the car returned is  honda accord   the summary url for this car is  http://cg.com/summ/1.htm  and the full review url  http://cg.com/full/1.htm .l once at the full review url  the review text can be extracted. 
existing research on information agent executors 
 barish and knoblock 1a  and network query engines 
 hellerstein et al. 1  naughton et al 1  ives et al 1  has suggested a number of techniques for the efficient execution of information gathering plans. most of these systems work on the principle of streaming dataflow  which executes plan operators as fast as their data dependencies allow and supports the pipelining of data between operators. despite the benefits of streaming dataflow  plan execution can still be slow if there are many binding pattern relationships between sources in the plan. for example  notice that since steps  b    c   and  d  are dependent on the steps that precede them  the plan in figure 1 must be executed sequentially and profits little from the benefit of streaming dataflow. thus  if each source in figure 1 has an average latency of 1s  than the average plan execution time is the summation of source latencies  or  1s =  1s. 
1 speculative plan execution 
speculative execution is one technique that can be used to overcome the effects of aggregate latencies in information gathering plans containing multiple binding patterns. 
　as described in  barish and knoblock 1b   a plan is transformed into one capable of speculative plan execution by the insertion of two additional operators -
speculate and confirm - at various points the plan  
1  for the sake of brevity  we have abbreviated the urls associated with consumerguide.com  although we retain the key aspects of its structure . 

　　figure 1: carlnfo transformed for speculative execution based on an iterative analysis of the most expensive path to execute within that plan. for example  one possible result of transforming the plan in figure 1 for speculative execution is shown in figure 1. 
　as shown  the speculate operator receives copies of data sent to operators executing earlier in the plan. based on the hints it receives  speculate can generate predicted values for later operators that can be transmitted immediately to those operators. thus  the earlier and later parts of the plan can be parallelized. after the earlier operators finish executing  speculate can assess whether or not its initial predictions were correct and forward the results onto a confirm operator  which ensures that speculative data does not prematurely exit the plan or cause some other irreversible action to occur. finally  notice that figure 1 shows that speculation can be cascading: speculation about one operator can drive the speculation of another operator  leading to greater degrees of parallelism and thus arbitrary speedups. 
　as a result of the transformation shown in figure 1  execution would then proceed as follows. input data  such as  sedan  $1   would result in the retrieval of the initial search results in parallel with the predicted make and model - which would drive the predictions of summary and full-review urls so that all four retrievals  three speculative  were executed at once. if all predictions are correct  the resulting execution time can be reduced to only 1s plus the overhead to speculate  a maximum speedup of about 1. however  the average speedup depends on the average accuracy of prediction: the 
greater this accuracy  the higher the average speedup. 
1 value prediction strategies 
caching can be used to implement value prediction when speculatively executing plans such as carlnfo. unfortunately  caching does not allow predictions to be issued for unseen hints. as a result  the average accuracy of prediction can be low when the domain of possible hints is large. further  trying to achieve better accuracy under these conditions can require significant amounts of memory. in this section  we describe how a hybrid approach  consisting of classification and transduction  results in a more intelligent and space-efficient prediction strategy. 
1 classification 
classification involves extracting knowledge from a set of data  instances  that describes how the attributes of those instances are associated with a set of target classes. given a set of instances  classification rules can be learned so that future instances can be classified cor-

1 	al and data integration 

rectly. once learned  a classifier can also make reasonable predictions about new instances - a combination of attribute values that had not previously been seen. the ability for classification to accommodate new instances is intriguing for the speculative execution of information gathering plans because  unlike with caching  it is possible to make predictions about novel hints. 
　for example  consider the prediction of the make and model of a car in the carlnfo plan. it turns out that carsdirect.com returns the same answer {honda accord  for  sedan  as it does for other types  such as  all and  coupe   in the same price range. the association of the same make and model to multiple criteria combinations occurs somewhat frequently on carsdirect.com. 
　to see why classification is a more effective technique than caching for the prediction of the make and model  consider prediction of a car based on type and price: 
type price car sedan 1 saturn s series sedan 1 honda accord sedan 1 vw beetle coupe 1 saturn s series  coupe 1 honda accord coupe 1 vw beetle 　the data above is what a cache would contain. in contrast  a classifier like ld1  quinlan 1  would induce the following decision tree: 
p r i  = 1 : saturn s series  1  p r i   1 : 
	i 	p r i  = 1 : honda accord  1  
	i 	p r i   1 : vw beetle  1  
　when presented with an instance previously seen  such as  sedan  1   both the cache and the classifier would result in the same prediction:  honda accord . however  when presented with a new instance  such as 
 coupe  1   the cache would be unable to make a prediction. in contrast  the classifier would issue the correct prediction of  honda accord . any errors made by classification would be caught automatically later in execution by the confirm operator. 
　the decision tree above is also more space efficient than a cache for the same data. the cache requires storing 1 unique values. the decision tree above requires only storing 1 values  just those shown  plus the information required to describe tree structure and attribute value conditions  i.e. pri  = 1 . 
　in short  classifiers such as decision trees can function as better  more space-efficient predictors. and in the worst case  where each instance corresponds to a unique class  a classifier simply emulates a cache. 
finite sets corresponding to input and output alphabets  1 is the state-transition function that maps q x i to q  and a is the output function that maps q x a to a*. we are interested in a particular type of sequential transducer called a p-subsequential transducer that allows at most p output symbols to be appended to the output  i.e.  exist on the final state transition arc . 
　value prediction by transduction makes sense for web information gathering plans primarily because of how web sources organize information and how web requests 
 i.e.  http queries  are standardized. in the case of the former  web sources often use predictable hierarchies to catalog information. for example  in the carlnfo example  the summary url for the 1 honda accord was http://cg.com/summ/1.htm and the full review was http://cg.com/full/1.htm. notice that both urls use the same piece of dynamic information  1   but in different ways. by learning this transduction  we can then predict future full review urls for corresponding summary urls we have never previously seen. transducers can also allow us to predict http queries. for example  an http get query for the ibm stock chart is http.//finance.yahoo. com/q s-ibm&d=c. by exploiting the regularity of this url structure  the system can predict the url for the cisco systems  csco  chart. 
   in this paper  we define two new types of transducers that extend the traditional definition of p-subsequential transducers. the first is a high-level transducer  called a value transducer that describes how to construct the predicted value based on the regularity and transformations observed in a set of examples of past hints and values. value transducers build the predicted value through substring-level operations {insert  classify  and transduce . insert constructs the static parts of predicted values. classify categorizes hint information into part of a predicted value. finally  transduce transforms hint information into part of a predicted value. transduce uses a second type of special transducer  a hint transducer  in which the operations {accept  copy  replace  upper  lower  all function on individual characters of the hint and perform the same transformation as their name implies  with respect to the predicted value. 
　to illustrate  consider the transducers shown in figure 1  for predicting the full-review url in the carlnfo example. figure 1 shows the value transducer performs high-level operations - the insertion of substrings and the call to a lower-level transduction. the second transducer  in abbreviated form  uses the accept and copy opera-

al and data integration 	1 

tions to transform the part of the hint value into its proper point in the predicted value. thus  the first step builds the  http://cg.com/fulir part  the second step copies the  1  part and the third step appends the  .htm  part. 
   in short  transducers lend themselves to value prediction because of the way information is stored by and queried from web sources. they are a natural fit because urls are strings that are often the result of simple transformations on earlier input. transducers are also space efficient: once learned  they occupy a finite size and can be applied to an endless number of inputs. overall  for web content that cannot be queried directly  instead requiring an initial query and then further navigation   transducers serve as compact predictors that capitalize on the regularity of web queries and source structure. 
1 a unifying learning algorithm 
in this section  we present algorithms that induce value transducers  vts   which combine classification and transduction  as predictors for the speculative execution of information gathering plans. the predictors learned represent not only a hybridization of classification and transduction  but also of caching  since classifiers default to caches when there is no feature of the key that is a particularly good indicator of the resulting value. 
to learn a vt  the general approach consists of: 
1. for each attribute of the answer tuple  identify an sd template that distinguishes static from dynamic parts of the target string by analyzing the regularity between values of this attribute for all answers. 
1. for each static part  add an insert arc to the vt. 
1. for each dynamic part  determine if transduction can be used; if so  add a transduce arc to vt. 
1. if no transducer can be found  classify the dynamic part based on the relevant attributes of the hint and add a classify arc to the vt. 
we implemented this in the algorithm lkarn-valuetransducer  shown below. the algorithm takes a set of hints  a set of corresponding answers  and returns a vt that fits the data: 
1 function learn-value-transducer returns valuetransducer 
1 input: the set of hints h  the set of answers a 
1 vt -θ 
1 tmpl  - learn-sd-template  a   
1 foreach element e in tmpl 
1 if e is a static element 
1 add insert  e.value  arc to vt 
1 else if e is a dynamic element 
1 da  - the set of dynamic strings in a for this tmpl element 1 ht - learn-hint-transducer  //  da  
1 if 	ht !=1 
1 add transduce  ht  arc to vt 1 	else 
1 	c  - learn-classifier  //  da  
1 add classify  c  arc to vt 1 return vt 
1 end /* learn-value-transducer */ 
in this algorithm  learning a classifier can be achieved by decision tree induction algorithms such as ld1  quinlan 1 . learning the sd template and the hint transforming transducer  however  require unique algorithms. 
1 learning templates of string sets 
learning a vt requires first identifying a template for the target value that describes what parts are static and what parts are dynamic. next  each static part is replaced with insert operations and each dynamic part becomes a candidate for either transduction or classification. 
　to identify an sd template  we use an approach based on the longest common subsequence  lcs  between a set of values. first  an lcs identification algorithm similar to the one described by  hirschberg 1  is applied to the set of answer values. we then iterate through the lcs on each answer value to determine the set of possible static/dynamic templates that fit that answer. only those templates common to all are kept - from this  one of the set is returned  though all are valid . the algorithm that implements this  learn-sd-template  is shown below. 
1 function learn-sd-template returns template 
1 input: set of strings s 
1 tmpl  - 1 les  - get-lcs s  1 if seq !=θ 
1 tmplset  - 1 
1 foreach string s in s 
1 curtmplset  - extract-templates  s  les  
1 tmplset  - tmplset n curtmplset 
1 if tmplset !=1 
1 tmpl  - choose any member of tmplset /* all are valid */ 
1 return tmpl 1 end /* learn-sd-template */ 
1 learning hint transducers 
to learn a hint transducer  ht   we also make use of sdtemplate identification. however  instead of identifying a template that fits all answers  we identify templates that fit all hints. based on this template  we then construct an ht that accepts the static parts of the hint string and performs the character-level transformation on the dynamic 
parts. a sketch of the algorithm that implements this  learn-hint-transduckr  is shown below. 
1 function learn-hint-transducer returns hinttransducer 
1 input: the set of hints h  the set of resulting strings s 
1 use lcs to identify static parts between all h 
1 foreach h s pair  h  s  
1 h' - extraction of h replacing static chars with token 'a' 
1 a  - align  h' s  based on lowercased string edit distance 
1 annotate a with character level operations 
1 end 
1 re  - build a reg expr that fits all annotations  using lcs  
1 if re !=1 
1 return general predictive transducer based on re that accepts static sequences of h where necessary and transduces dynamic sequences. 
1 else 
1 return 1 
al and data integration 1 end /* learn-hint-transducer */ 
for example  suppose prior hints {dr. joe smith  dr. jane thomas  corresponded to values {joe s jane t . the algorithm would first identify the static part of the hints and rewrite the hints using the accept operation  i.e.  {aaaajoe smith  aaaajane thomas} where a refers to the operation 
accept. it would then align each hint and value based on string edit distance and annotate with character operations required for transformation to the observed values  resulting in {aaaaalccrldddd  aaaaalcccrlddddd}. 
next  it would use the lcs to build the regular expression 
{a*lc*rld*} fitting these examples and  from this  the ht  partial form shown  in figure 1: 

figure 1: partial form of the ht for the names example 
1 experimental results 
to evaluate our approach to value prediction  we compared it with caching on three sample plans that can benefit from speculative execution. these plans were chosen because all query multiple sources and/or multiple times within a source in order to retrieve information that is not possible to query directly. these plans normally require sequential execution; however  with speculative execution  significant speedup is possible. 
　these plans included carlnfo  which we have already described   replnfo  and phone info. replnfo  based on the plan described in  barish and knoblock 1b   queries the site vote-smart.org for the issue positions of u.s. federal representatives for a particular nine-digit zip code. its plan involves three queries: one for the list of representatives in the desired zip code  navigation to the profile page for each member  and navigation to their corresponding issue positions page. phonelnfo is a similar plan that takes a u.s. phone number  does a reverse lookup of that number  on superpages.com  to find the state of origin and then queries the us census  quickfacts.census.gov  about demographics for that state. when querying the census  additional navigation is required to get from the initial summary page about the state to the corresponding demographics details page. 
　all three were modified for speculative execution  with results similar to that shown in figure 1. we then learned predictors for each  based on the data observed during execution. the learning was done offline  after each execution  so that future executions could benefit from the resulting predictors. the carlnfo predictors  as described  involved classification  make/model/year to car summary page  and transduction  summary to full review page . replnfo required classification  nine-digit zip to political district page  and transduction  district to issue positions page . finally  phonelnfo required classification  phone number to state  and transduction  initial state facts page to detailed demographics page . 
　when learning the predictors  instances were drawn from typical distributions for that domain; for example  instances for replnfo were drawn from a list of addresses of individuals that contributed to presidential campaigns  obtained from the fec  - a distribution that closely approximates the u.s. geographic population distribution. similarly  the phone numbers used in phonelnfo came from a distribution of numbers for common last names. 
　we implemented speculative execution in theseus  a streaming dataflow system for web information gathering. since our tests involved thousands of requests  we ran our plans using theseus on local copies of the relevant data and simulated network latencies during retrieval. in doing so  we assumed each source had a latency of 1 seconds  note that the particular latency chosen does not matter - speedups will be the same . 
　our results focus on three measurements. the first  shown in table 1  show the average number of examples required in order to learn the correct transducer required by each of the plans. notice  that replnfo required more than phonelnfo or carlnfo because there was a higher likelihood of the examples sharing some part of their dynamic data in common - and this was extracted by the lcs-based algorithm as a static element. 

table 1: learning the correct transducer 
　the second set of results  shown in figure 1  focuses on the accuracy of classification  as measured over a 1fold cross-validation sample of the data  where no data in the test fold was in any of the training folds . as expected  domains with larger sets of discrete target classes  such as replnfo  required many more examples than those with smaller numbers of classes  like phonelnfo . 
　the final set of results show how the learning we have described resulted in better average plan execution 


al and data integration 	1 

speedups than did caching. due to space constraints  we show results from only one of the plans - carlnfo - in figure 1. this figure shows that  while the benefit of caching degrades significantly as the composition of future requests contains a greater number of unseen exam-
ples  the learning allows accurate predictions to be made - even for a mix containing entirely new requests. furthermore  as the number of examples increases  speedup from learning increases and the accuracy of speculation  even on 1% unseen instances  gradually increases. though omitted here  these same general trends hold for both rep info and phonelnfo as well  although to different degrees . 
1 	related work 
learning to speculatively execute programs has been well-studied in computer science. historically  however  computer architecture research has focused mostly on predicting control  e.g.  branch prediction   not data. while hardware-level value prediction has recently become an active area of research  the type of data being predicted  e.g.  memory locations  is different. 
　at a high-level  this work could be considered a form of speedup learning. in speedup learning  the goal is to improve problem solving performance through experience. past research has focused on a number of areas  including learning sequences of operators  fikes et al. 1  and learning control knowledge to aid in choosing what operators to execute next  minton 1 . although the use of learning in this work is for the purpose of value prediction  i.e.  how to execute   the goal of using learned knowledge to improve performance is the same. 
　there has not been any previous work on value prediction for information gathering systems.  hull et al. 1  proposed speculation in a decision flow framework  but one in which only control predictions were necessary. there has also been past work on information gathering with partial results  hellerstein et al 1; shanmugasundaram et al. 1   but these systems do not predict data values and instead use approximate values from intermediate aggregate operators in order to obtain approximate final results. 
　surprisingly  there has been little work on the learning of subsequential transducers. one existing algorithm is ostia  oncina et al. 1   which is able to induce traditional subsequential transducers capable of automating translations of decimal to roman numbers or english word spellings of numbers to their decimal equivalents. our work differs from ost1a mainly in that the transducers we learn capture the general process of a regular class of string transformations. after learning from only a few examples  our algorithm can achieve a high-degree of accuracy on such classes. in contrast  while ostia can learn more complex types of subsequential transducers  it can require a very large number of examples before it can learn the proper rule  gildea and jurafsky 1 . 
　the transducer learning algorithm suggested by  hsu and chang 1  viewed transduction as a means for information extraction. our use is similar in that one part of our approach involves extracting dynamic values from hints. however  the transducers we describe go beyond extraction - they transform the source string so that it can be integrated into a predicted value. in doing so  our transduction process also makes use of classification. 
　finally  while our use of classification applies to predicting any type of data value in an information gathering plan  our typical use of transduction is for the prediction of urls. other approaches have explored point-based  zukerman et al. 1  or path-based  su et al. 1  methods of url prediction  attempting to understand request models based on either time  the order of requests  or the associations between requests. however  unlike our approach  these techniques do not try to understand very general patterns in request content and thus cannot predict urls that have not been previously requested. 
1 conclusions 
successful speculative execution of information gathering plans is fundamentally linked with the ability to make good predictions. in this paper  we have described how a hybrid approach based on two simple techniques - classification and transduction - can be combined and applied to the problem. the approach we describe represents a 
hybridization of not only classification and transduction  but also of caching  since classifiers effectively function as caches when no classification is possible. 
our experimental results show that learning such pre-

dictors can lead to significant speedups when gathering information that must be queried indirectly. we believe that a bright future exists for data value prediction at the information gathering level  primarily because of the po-
tential speedup enabled by speculative execution and because of the availability of resources  i.e.  memory  that exist at higher levels of execution  enabling more sophisticated machine learning techniques to be applied. 
	1 	acknowledgements 
we wish to thank steven minton and cenk gazen for very helpful and inspiring discussions related to techniques for template induction. 
1 	al and data integration 

　this material is based upon work supported in part by the defense advanced research projects agency  darpa  and air force research laboratory under contract/agreement numbers f1-c-1 and f1-1  in part by the air force office of scientific research under grant numbers f1-1 and 
f1-1  in part by the united states air force under contract number f1-c-1  in part by gifts from the intel and microsoft corporations. the u.s.government is authorized to reproduce and distribute reports for governmental purposes notwithstanding any copy right annotation thereon. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of any of the above organizations or any person connected with them. 
