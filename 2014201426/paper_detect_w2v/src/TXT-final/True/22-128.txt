 
a new decision tree learning algorithm called idx is described. more general than existing algorithms  idx addresses issues of decision tree quality largely overlooked in the artificial intelligence and machine learning literature. decision tree size  error rate  and expected classification cost are just a few of the quality measures it can exploit. furthermore  decision trees of varying quality can be induced simply by adjusting the complexity of the algorithm. quality should be addressed during decision tree construction since retrospective pruning of the tree  or of a derived rule set  may be unable to compensate for inferior splitting decisions. the complexity of the algorithm  the basis for the heuristic it embodies  and the results of three different sets of experiments are described. 
1 	introduction 
incomplete knowledge is characteristic of interesting classifier learning problems. part of the reason for developing connectionist models  rumelhart et a/.  1   genetic classifier systems  holland  1   and bayesian classifiers ftou and gonzalez  1  is to contend with this incompleteness. analytical learners use strong background knowledge to reason about training instances. their problem is often one of finding the right formulation of existing knowledge. one analytical learning method can even deduce classifiers from single training instances  mitchell et al  1 . empirical learners  such as id1  quinlan  1   cart  breiman et a/.  1   and idx  do without extensive background knowledge. instead  they try to recognize and exploit regularities in larger training sets. 
1 	machine learning 　　decision trees are a popular representation for classifiers. the interior nodes of a decision tree are tests applied to instances during classification. branches from an interior node correspond to the possible test outcomes. classification begins with the application of the root node test  its outcome determining the branch to a succeeding 
* and rutgers university  department of computer science  new brunswick  nj 1. 
node. the process is recursively applied until a leaf node is reached. then the instance is labeled with the class of the leaf node  and the process halts. during construction the decision tree naturally directs training data downward to the leaves. in a given leaf node  one of the classes can often be identified as incorporating more of the training instances than any other class. this typically becomes the class for that leaf node. another common strategy is to choose the class that minimizes the average cost of misclassification. in my experiments the majority classification of the training data at each leaf has guided class selection. 
　　successful decision tree construction depends on the choice of good tests for the interior nodes.  it also depends on the decisions to stop splitting the training data and form leaf nodes  but this issue is not addressed here.  figure 1 illustrates the point. four types of instances arc found in the training population each having the given probability of each occurrence. three binary tests can be used. each tree classifies the data accurately. if the tests are pre-computed  the expected depth of the decision tree is the expected classification cost. then trees 1 and 1 are better than tree 1. in contrast  if tests correspond to pieces of special purpose hardware  minimizing the number of tests in the classifier is important  as in tree 1. other preference schemes are possible. 
　　the next section describes several other research projects in decision tree induction. each overlooks important issues and opportunities in reducing the cost of classification. section three describes two lookahead algorithms: gota  hartmann et al.  1  and idx. lookahead and a different heuristic function should help them outperform other algorithms. the fourth section presents the results of three sets of experiments comparing id1  quinlan  1   idx  and gota. section five discusses the complexity of idx  plus the positive and negative results of this research. section six summarizes and concludes the paper. 
1 	background 
hunt  marin  and stone  1  report a series of experiments in the induction of decision trees. their incremental  failure driven learners induce small binary decision trees  no more than 1 tests  from noiseless data. 

they varied several components of their algorithm  but did not experiment with information theoretic heuristics  largely because of computational limitations of the day. 
　　quinlan uses an information theoretic test selection heuristic in id1  1. a set of pre-classified training instances guides test selection  or splitting. shannon's measure of information /  gallager  1  measures the uncertainty in the classification of a sample given that it falls in a particular node. the entropy function h measures the average uncertainty in the classification given that an additional test is applied to split the node. at each step  id1 chooses the test that maximizes the drop in classification uncertainty. this heuristic is meant to produce efficient decision trees as measured by average depth. id1 generates decision tree 1 in figure 1. 
　　breiman  friedman  olshen  and stone  1 present an extensive study on the induction of decision trees using cart. classification costs are entailed in determining a classification; misclassification costs stem from incorrect classifications. a major concern of the cart team is reducing average misclassification cost. after generating large decision trees  they suggest pruning to reach a right sized tree. quinlan's recent work on generating production rules from decision trees  also uses pruning. breiman et al. reach the tentative conclusion   ... that within a wide range of splitting criteria the properties of the final tree selected are surprisingly insensitive to the choice of splitting rule. the criterion used to prune or recombine upward is much more important.  the degree to which this is valid depends on the setting for the finished application and on the nature of the costs involved. 
1 	generating better decision trees 
hartmann and his colleagues describe gota   an algorithm that uses lookahead to maximize an information theoretic heuristic function. control of splitting in gota is governed by branch levels  or horizontal slices of the decision tree. within each branch level  gota does an exhaustive search to find the best test sequence as measured by the heuristic function. the key point for this discussion is that branch levels cover the decision tree completely and do not overlap. the depth of a test within a decision tree is the number of tests between it and the root. consider the application of gota to a problem with four binary tests  using branch levels two tests deep. this would first entail an exhaustive search for the best combination of one depthzero test and two depth-one tests. these would be fixed prior to processing in the next branch level. gota would then search for the best combination of four depth-two tests and eight depth-three tests. 
	norton 	1 

　　gota's heuristic seeks to maximize the drop in uncertainty across the branch levels  much like id1. but in addition  it seeks to minimize the cost of the decision tree. gota maximizes the function f given below. it is a function of the uncertainty h in the classification of the various branch levels bl  and the cost g of extending the decision tree from branch level i-1 to branch level i. hartmann and his colleagues show that no disadvantage ensues from maximizing f over a single branch level rather than combining the results of maximizing over component branch levels. in fact better results are often obtained with the former approach. their result is valid for cost functions satisfying g a c    g a b  + g b c . size  average cost  and performance all satisfy the inequality. 

　　the f heuristic is frequently helpful in practice. the intuition behind the phenomenon involves the choice of tests at the bottom of the branch levels. consider using gota in the four test case mentioned above. when using two branch levels two tests deep  the choice of the depth-one tests is based only on the choice of the depth-zero test. what if all four tests are chosen by processing a single branch level four tests deep  the choice of the depth-one tests is now based  in part  on the candidates for depth-two and depth-three tests. the information gained by the extra search can only help  not hurt. 
　　but consider the eight-test case. if the largest branch level that can be processed is four tests deep  there will still be a problem at the border between levels. whereas gota uses branch levels that cover the decision tree but do not overlap  idx uses levels that do overlap. this allows idx to be better informed at test selection time. there is a second difference. processing of an entire branch level with gota results in the selection of tests throughout that branch level. in contrast  processing of a level in idx results only in the selection of the shallowest tests in that level. of course  nearly as many levels are required as the tree is deep. when using idx and branch levels four tests deep on the present case  the choice of depth-zero test is made on the basis of an exhaustive search for the best configuration of tests from depth zero to depth three. then the depth-one tests are chosen on the basis of an exhaustive search for the best configuration of tests from depth one to depth four. this continues until the lookahead reaches the leaves. the results of that last search are optimal  and therefore applied directly. 
1 	experimental results 
this section describes decision tree building experiments using jeffrey schlimmer's congressional voting record data  provided april 1  1  drawn from  congressional 
1 	machine learning 
quarterly  1 . the data set contains 1 samples  one for each congressional representative. each sample contains a representative's party affiliation  republican or democrat  and voting record on 1 key bills during 1. for simplicity the categories of votes have been reduced to yes  no  and    other . 1 tests were considered during experimentation: party affiliation and votes. each experiment excluded the test for which the decision tree was being built. 
1 decision tree size 
the first experiment deals with decision tree size  measured as the expected value of the number of tests needed to classify the instances in the entire data set. let idx n  stand for the use of idx with n-test lookahead. id1  idx l   idx 1   and idx 1  were used to generate decision trees for party affiliation  and for each tuple  vote  outcome   using the entire data set. the cost function used in idx measured the expected number of tests needed to extend the decision tree. because of the volume of data  only the regression lines are plotted in figure 1. since each test was given unit cost  id1 and idx l  performed identically. the performance of idx 1  was slightly better on average  particularly as larger trees were needed to correctly classify the training data. idx 1  did better still. let gota xi  x1  ...  stand for the use of a branch level including tests of depths 1 to x1 - 1  a branch level including tests of depths x1 to x1 - 1  and so on. gota 1 1  ...   and gota l 1  ...    evens and odds  were applied using the same cost function as idx. figure 1 plots regression lines for the results of gota evens  and gota odds  with the results of idx 1 . the three algorithms were nearly equivalent. each performed best  worst  or in between about as often as the others. 
1 robustness tests 
one effect of idx is structural. it tends to balance decision trees. the second experiment was meant to determine whether generality and error rates of decision trees generated with idx 1  differed systematically from those generated with id1. for each vote  decision trees were generated for the yes outcome using different fractions of the data for training: .1  .1  .1  .1  .1  .1  .1  .1  .1  .1  .1  .1  and .1 . the remaining data were used to compute false positive and false negative rates for the decision trees. for each vote and each training fraction  ten trials of this kind were performed and the results averaged. error rates obtained using idx 1  decision trees did not systematically vary from those obtained using id1 decision trees. 
1 cost of classification 
the third set of experiments highlights the importance of recognizing cost measures other than size  and demonstrates the proficiency of idx in doing so. for each tuple  vote  outcome  a decision tree was generated using id1. for each tree  five experimental runs were conducted. in each run 


	norton 	1 

random real valued costs between 1 and 1 were assigned independently to each test for each set of costs  idx l  and idx 1  were executed. the g function measured expected cost  not expected size. the expected classification costs for id1  idx l   and idx 1  are plotted versus test cost variance in figure 1. again because of the large volume of data  only the regression lines are shown. 
1 	discussion 
there was some uncertainty about how much of an effect lookahead would have on classification cost as measured by average decision tree depth. it is widely known that the heuristic used in id1 is a good one for minimizing that measure. the experiments did  however  show a consistent improvement using lookahead. regression suggests 1% and 1% improvements in decision tree depth on small  1 nodes  and large  1 nodes  decision trees when using two-level lookahead. three-level lookahead demonstrated between 1% and 1% improvement. while these gains may not seem impressive  consider a high volume task such as optical character recognition for a postal application. small improvements applied countless times can yield real benefits. 
　　the results of the comparison of gota and idx are somewhat disappointing  but they highlight something important about the heuristics in question. while enlarging levels is guaranteed not to reduce the heuristic measure  it does not imply the overall goal of minimizing classification cost. the limiting case is the obvious exception  when lookahead covers the entire tree. to its credit  idx provides a more flexible tool in the search for efficient decision trees. let idx xo  x1 ...  mean that tests at depth zero are based on xo level lookahead  that tests at depth one are based on x1 level lookahead  and so on. gota 1  1  1  is certainly equivalent to idx 1  1  1  1  1 . what idx does not provide is independent control of lookahead in disjoint subtrees within the same level. there may be circumstances where this will be important  and we will see that test cost variance-based heuristics are strong candidates. 
　　in the third set of experiments  id1 was expected to perform consistently despite test cost variance. after all  it does not recognize test costs in its heuristic. idx was generally expected to outperform id1 whenever test costs varied. in addition  the disparity between id1 and the idx algorithms was expected to increase as test cost variance increased. variation in test costs gives idx an opportunity to recognize when expensive tests can be delayed or avoided altogether. of course  test cost variance is not the only factor influencing performance. test relevance is at least as important. if too much of the contribution to the variance is from tests irrelevant to the classification  the variance effect will tend to be diminished. unfortunately  test relevance is difficult to capture since the algorithm has some freedom in selecting the tests it uses. to compensate for this problem the experiment called for a larger number of 
1 	machine learning 
trials. these expectations are born out in the regression lines of figure 1. 
　　the performance of idx was also expected to improve in the presence of test cost variance as the lookahead parameter increased. this was bom out but with diminishing returns as shown in figure 1. if an expensive test is essential for finding the correct classification  both algorithms will tend to push it to the bottom of the decision tree  trying to apply it less often. in the end  the necessary tests will be applied. since idx tends to balance decision trees  maximum depth does not change very much. 
therefore  expensive but essential tests will be delayed but eventually applied at similar depth in either case. nonessential tests are never used. this accounts for the effect of diminishing returns. 
     computing optimal decision trees is known to be npcomplete fhyafil and rivest  1 . calculation of a loose upper bound on the number of times idx partitions the entire data set is given below. parameters: branch levels l tests deep; t available tests. the bound assumes that costs can be computed independently for disjoint subtrees  a favorable but typical case. the bound is linear in the size of the data set. if the number of tests is large  the cost of additional lookahead will be substantial. one area for future research involves significant reduction in the number of tests considered at a given stage during the exhaustive search. the good news is that as test cost variance increases the need for large l decreases. 

the efficacy of lookahead with idx has been 
established  but the question remains whether a less complex tree generation technique followed by some type of retrospective pruning could accomplish the same ends. by  retrospective pruning  i mean any technique in which a constructive stage is followed only by pruning and by no other tree modification. the answer depends on the criticality of cost in the final application. breaking tree 1 of figure 1 into rules in the obvious way yields the two candidate rule sets in figure 1. pruning does not eliminate the x tests in either case. likewise  cost complexity pruning of the same tree  as done in  breiman et al.  1   does not eliminate test x unless the cost parameter swamps the error rate term. still  two-test lookahead suffices to eliminate the x lest as shown in tree 1. if reduction of classification costs has high priority in an application then lookahead will be important in the splitting criterion for decision tree construction. such cases are easy to conceive. in building a medical expert system for diagnosis  blood work and imaging will generally be preferred to exploratory surgery although the surgery greatly reduces uncertainty. invasive tests should be avoided as much as possible. techniques such as idx can be useful in this regard. 

1 	conclusions 
a new decision tree learning algorithm is described that is more general than id1 and gota. using lookahead often permits idx to make more informed decisions about test selection during decision tree construction. the complexity of the algorithm is largely controllable by the user. in practice  high quality decision trees are generated by using lookahead and by considering costs of classification. depending on the demands of the classification problem  accepting higher cost during decision tree construction can yield better decision trees than schemes using simpler splitting criteria and retrospective pruning. an important negative result regarding variance of test costs describes circumstances under which additional lookahead becomes less and less warranted. 
acknowledgements 
thanks go to my colleagues mike hudak  narendra gupta  and kangsuk lee for reviewing earlier versions of this paper. thanks also go to kevin kelly  and neeraj bhatnagar for acting as sounding boards for some of these ideas. 
