 
evaluation of incomplete algorithms that solve sat requires to generate hard satisfiable instances. 
for that purpose  the ksat uniform random generation is not usable. the other generators of satisfiable instances generate instances that are not intrinsically hard  or exhaustive tests have not been done for determining hard and easy areas. a simple method for generating random hard satisfiable instances is presented. instances are empirically shown to be hard for three classical methods: the  davis-putnam  procedure  which is complete   and the two incomplete methods: gsat and the break out method. moreover  a new method for escaping from local minima is presented. 
1 	introduction 
a crucial problem in practical ai development is the problem of satisfiability  called sat  of a finite set of propositional clauses. sat is a np-complete problem. which ai system does not use a satisfiability test or equivalent  constraint satisfaction problem  graph coloring  ...   formal reasoning is limited because there is no efficient propositional theorem prover. on one hand  the complete methods that solve sat have not really progressed since the resolution method and davis-putnam procedure  except that it has been shown that the choice of the heuristics is central  dubois et al.  1 . on the other hand  the incomplete methods have been really developed thanks to the results of gsat for the random instances  selman et al.  1 . 
　a threshold phenomenom has been brought to light for the generation of random instances  cheeseman et al.  1   making possible the generation of random hard instances. satisfiable instances are needed for evaluating the incomplete methods. but only 1% of the hardest instances are satisfiable. it has been shown  chvatal and szemeredi  1  that the unsatisfiable instances are hard  for the resolution  but it has not been shown  cha and iwama  1  that the satisfiable instances are difficult  for the local search for example . in works on evaluation of incomplete methods   konolige  1  showed the inefficiency of gsat on random structured instances  compared with davis-putnam procedure. but with few changes  gsat becomes efficient  kask and dechter  1 . may be it is 
1 	constraint satisfaction 
due to the fact that these structured instances are not intrisically hard. but  what is it the definition of intrisically hard  so  a main problem about sat is to determine if a set of instances is hard. there are no general definition and determination method of the complexity of a set of instances  except when a polynomial class is recognized . for this reason  empirical methods are used for showing the difficulty of the instances produced by a generator. a hard set of instances is a set of instances which are hard for the best known algorithms  namely the  davis-putnam  procedure 
 davis et al  1   gsat with random walk  selman and kautz  1  and the break out method  morris  1 . 
1 the ksat uniform random generation 
an instance of ksat is produced by the uniform random generation if each clause of the instance is randomly and independently picked out. these instances are generated from two parameters: the number of variables and the ratio. the ratio is defined as the number of clauses divided by the number of variables. the uniform generation of ksat instances is an interesting theoretical problem. a threshold phenomenom has been brought to light for the probability to pick out a satisfiable instance as a function of the ratio  cheeseman et al.  1   crawford and auton  1 . the threshold seems to appear when the probability to generate a satisfiable instance is equal to 1. the value of this critical ratio has been determined  empirically  as 1 for 1sat  1 for 1sat etc  mitchell et al.  1   dubois et al.  1 . a problem is to determine theoretically this threshold. near the threshold  if the ratio decreases  the probability to pick out a satisfiable instance quickly tends to 1; if the ratio increases  the probability to pick out a 
satisfiable instance quickly tends to 1. the more important the number of variables  the more quickly the probability tends to 1  or 1 . moreover  it has been shown  empirically  that the hardest instances are generated for a ratio value equal to the threshold. for other values  the instances are easier. 
　this generation method is used for evaluating complete or incomplete theorem provers. in the hardest region  it has been proved that inconsistent instances are difficult  for the resolution   chvatal and szemeredi  1   but there is no theoretical result for the difficulty of the satisfiable instances. futhermore  for generating hard instances  the higher the number of variables  the more the value for the 
ratio is precise. a little variation on the ratio can produce only satisfiable or only insatisfiable instances. 
   for example  if the theoretical value of the critical ratio for 1 variables is equal to 1 then the uniform generation would produced  for 1 variables and with a ratio equal to 1  more than 1% of satisfiable instances. in this case  the evaluation of a local search algorithm under the assumption  1% satisfiable  is a big mistake. 
　with a lot of variables  the complete methods are unusable. thus it is impossible to know the rate of success of the incomplete methods. the uniform generation is not a good means for evaluating the incomplete methods if the theoretical precise value of the threshold is not known. 
1 	satisfiable instances 
the algorithm ksat gen presented in  cha and iwama  
1  produces only random satisfiable instances. the clauses are randomly constructed in order to be satisfied by a 
given model. there are several generation parameters: number of variables  ratio and literal distribution  i.e. the number of occurrences of each literal in the generated set of clauses . in  cha and iwama  1   there is no result on the relation between the parameters and the difficulty of the instances. for not producing only easy instances  the studies on the uniform random generation have shown the importance of the value of the parameters. there are a lot of parameters for ksat gen  then it will be difficult to make exhaustive tests. 
   two other methods are proposed  cha and iwama  1  for generating always inconsistent sets of clauses or sets of clauses having one and only one model. the basic steps of these generators are the reversal resolution principle and the reversal subsumption simplification  a clause ci subsumes a clause  if the generation is reversed  a resolution proof can be easily obtained. the time of the generation of the sets of clauses must be limited. so there is a short proof  by resolution  of the empty clause for the inconsistent sets of clauses. and there is a short proof  by resolution  for each prime implicate for the sets of clauses having only one model. the smaller the generation time  the smaller the resolution proofs! this kind of instances is not intrinsically hard  because the resolution proof of the prime implicates is short. a very good resolution-based theorem prover would be able to solve easily this kind of instances. 
   let x be a propositional symbol; x is called a positive literal and is called a negative literal. one possibility for generating only consistent sets of clauses is to leave out the clauses that have only positive litterals  from a set of clauses generated by uniform random generation  this idea appears in  morris  1   rauzy  1  . for these instances  a model is obtained by assigning false to all the variables. unfortunately  this method often produces easy instances. it is due to the fact that the clauses with only positive literals are left out  so there more negative literals than positive literals. consequently  the davis-putnam procedure easily finds a model  with a good heuristic . 
moreover   rauzy  1  has noticed that only  nearly horn-renamable  instances are generated. to generate hard instances  new parameters must be introduced. 
　each clause has a negative literal. an  intelligent  algorithm would be able to show this property. to solved that problem  some literals of the instances have to be renamed  swap x and for some x   and then some clauses without negative literal appear. 
1 	how to be hard 
let us consider the case where the clauses without negative literals are left out. in order to control the proportion of positive and negative literals   rauzy  1  introduces parameters for determining the probability to pick out a given sign of a clause. for 1sat  there are four kinds of sign for a clause:  i  all the literals in the clause are positive   ii  two literals are positive and one is negative   iii  one literal is positive and two are negative   iv  all the literals are negative. so there are four parameters for the generation: the probability to pick out a clause  i    for a clause  ii   for a clause  iii  and a for a clause  iv . 
　the probability to pick out each sign of a clause is for the uniform random generation. with a probability equal to to pick out a clause which sign is  i   and with given probabilities for the others   rauzy  1  proposed two equations for generating instances  before renaming  with approximative  the same number of positive and negative literals: 
 1    since clauses with only positive literals are rejected  
 1    for the equilibrium between positive and negative literals  
   for the sake of simplicity  a restrictive strategy is chosen in our work. only one parameter is introduced: the probability to pick out a positive literal  called posp. the following algorithm is proposed: 

	castell & cayrol 	1 

1 	search algorithms 
the generator ksat satisfiable has been tested on three classical different methods. this generator has also been used to evaluate two new personal versions of the break out method. 
1 	complete method 
we use the classical  davis-putnam  procedure  dp   it is an enumeration of the interpretations. with sophisticated heuristics  it is the most efficient complete algorithm for the uniform random instances of ksat. for our tests  an implementation with heuristics of c-sat is used  dubois et al.  1 . c-sat generally solves in few seconds hard instances  from uniform random generation  having 1 variables. instances having 1 variables are solved in less than one hour. instances having more than 1 variables require some days  years or much more ... 
1 	incomplete methods 
these methods are incomplete because they cannot always find a model. but they can solve very large problems  selman et ah  1 . at the present time  the best algorithms for sat are based on the hill-climbing algorithm. for a given interpretation  these methods try to decrease  by local changes on the interpretation  the number of falsified clauses. a local change in the interpretation is an inversion of the truth value of a variable  called flip of the variable. for these methods  the crucial problem is to escape from a local minimum. 
   we used the break out method  bout   morris  1  and gsat with random walk  selman and kautz  1 . bout escapes from a local minimum by weigthing the falsified clauses on that local minimum. the results presented in  cha and iwama  1  show that this method seems to be better than gsat which is supposed to be the best incomplete method for solving sat. gsat  selman et al.  1  is the more popular incomplete algorithm for sat. there exists some refinements  in particular a notion of weighting  selman and kautz  1  as for bout. for the 
1 	constraint satisfaction 
tests  we used the version with random walk. a random walk is an elementary improvement for escaping from local minima: with a given probability  a variable that appears in a falsified clause is flipped. 
1 	escaping from a local maxima 
break out method with jump 
now an improvement of bout is presented. it is called the break out method with jump  boj . on a local minimum very often the same clauses are falsified. to avoid that  on a local minimum  all the variables that appear in the falsified clauses are flipped. thus several flips have to be done for falsifying again these clauses. 
   another change is introduced. in some cases  the current interpretation tends to the opposite of a model; in order to solve that  the following property is used: when the sum of the weights of the clauses that are totally satisfied  each literal of the clause is satisfied  is lower than the sum of the weights of the falsified clauses  if all the variables are flipped  the new interpretation is better. 
　the break out method with jump is represented by the following algorithm: 
ze 
	a 	specialized 	algorithm: 	mirror 
this variation of bout called mirror is presented because its efficiency is a mystery for us. its principle is simple. it is bout with only one new operation. on a local minimum  falsified clauses are normally weigthed and immediately after all the variables are flipped  it is the reason for the name mirror . in the hardest areas of the other incomplete algorithms  this method is amazingly efficient. but it has a lot of problems on obvious areas. moreover this method seems to be efficient only with our generator. 
1 	empirical results 
the figures appear at the end of the paper. 
　ksatjsatisfiable has been tested with a lot of values for nbv  ratio and posp  for the 1sat case. each point is 

calculated from a sample of 1 instances. only general results for 1 variables are presented  the results are similar for 1  1 and 1 variables . here we are interested in the behaviour of the different methods as well as the easy and hard instances of the generator. comparing the algorithms is outside the scope of the paper. 
   the complexity of davis-putnam procedure is measured in terms of the sum of the average number of calls  the number of unit propagations and the number of pure literal simplifications  a literal is a pure literal for a set of clauses if its complementary literal does not appear in this set . the complexity of the incomplete methods is measured in terms of the average number of tested interpretations. this measurement is more precise than the number of flips. indeed  in order to choose a best neighbour  a local search method inspects the interpretations in the neighbourhood of the current interpretation  explicitly or implicitly . therefore  this measurement enables us to differenciate algorithms which do not have the same kind of 
neighbourhood  or which do not explore the search space in the same way. 
   the most classical parameters are chosen for the incomplete algorithms. the initial weight of the clauses is fixed to one and a unit weight is added on local minima. the probability of the random walk for gsat is equal to 1. in order to avoid a too quick restart  no restart is realized. the programs are stopped after 1 flips  but gsat with random walk is stopped after 1 flips. it is a high value compared to values used in experimentations with uniform random generation. experimentations indicate that gsat with random walk needs in average 1 flips and almost no restart in order to prove the consistency of the instances composed of 1 variables generated at the threshold by the standard model  selman et al.  1 . 
1 	the hardest areas 
as for the ratio of instances solved  figure 1   for dp it is equal to 1%  which is not surprising since dp is a complete method. the performances of gsat with random walk are poor: close to 1% of instances solved for a ratio equal to 1 and posp = 1. bout behaves better but still have big weaknesses. boj behaves quite well  almost all instances are solved. moreover  boj has the same behaviour of the other algorithms  but the failure starts only at around 1 variables. as for complexity  figure 1   gsat  bout and boj have similary curves. the complexity of these algorithms grows with the ratio. it is more simple for dp  the form of the complexity is easy-hard-easy. it is like the uniform random generation. the hardest areas are located at a 
ratio=1 and  however  the same  ridge  of 
hardness is present in dp: when ppos increases with the ratio. 
   we could conclude that this model for generating consistent instances generates hard instances and that the region of the hardest instances follows a ridge which is common to all methods: when ppos increases with the ratio. 
but we are immediately going to check that this conclusion is erroneous. 
1 	calling the conclusions into question 
we must not conclude too quickly  even if some evidences are showed. we are going to show that the presence of a common ridge in the complexity curves is not directly linked with the hardness of instances but with the methods used. to show this  we are going to make again the tests with mirror  figure 1. moreover  when describing the dp procedure used in the experimentations  we omitted an important factor: the presence of a heuristics in the choice of the first subtree explored  heuristics of s-sat  dubois et al.   1  . we are also going to make again the tests with dp  but with a choice of the first subtree explored which is opposed to that proposed by the heuristics and which is random  figure 1. 
   the curves always show the presence of a ridge  but here posp decreases when the ratio increases! for a high posp value and a high ratio  we observe that the generated instances are easy for mirror. 1% of the instances are solved quickly. and for the hardest area of mirror  low posp value and a high ratio   the generated instances are easy for bout  boj and gsat: 1% are solved quickly. 
　when the first subtree explored by dp is selected randomly then the two ridges appear  figure 1. 
1 	conclusion 
with the ksat-consistent function  it is easy to generate hard satisfiable instances. these instances turn out to be practically much harder than instances generated by the standard random model. 1 variables are enough to make gsat+random walk and the break out method little efficient. the generated instances have no structure and yet they are very hard to solved  by almost the most popular algorithms . as for the efficiency of local search  the results of mirror bring into light gaps of classical approaches. there exists instances generally simple to solve  bymirror  which may be extremely difficult for methods such as gsat and the break out method. an important result of our tests is that incomplete algorithms must be evaluated with all the different areas  and not only on the hardest areas of a given algorithm. if mirror was tested only on the hardest area of bout or gsat  a mistake would be made on its efficiency. 
　new operations must be integrated to local search in order to make it more robust. the flip of the whole set of variables  mirror  is one of our proposals  but up to now this operation gives interesting results on this model of generation only. in order to capture the good characteristics of bout  gsat and mirror  the boj algorithm has been developed. on our generator  boj improves bout and seems also better on other kinds of instances. 
   we venture to conjecture that for the ksat-consistent function  the generated instances which are the hardest to solve are  independently from the method used  located on a ratio = 1 and  for a given algorithm  the 
	castell & cayrol 	1 

presence of a zone harder than this point brings into light the gaps of the evaluated method. 
   generally speaking  the ksat-consistent function shows figures that  as complete methods  local search has weaknesses and allows for instances that are not solvable in a reasonable amount of time. as for the uniform generation  the simplified model generation of satisfiable instances must be intensively studied. 
acknowledgments 
to elsa  claudette for helping us with english  jerome for helping us finishing the paper and the members of ressac. 
