 
¡¡¡¡¡¡the only technique available for solving many important problems is searching. since searching can be extremely costly  it is important to identify features that improve the efficiency of aearch algorithms. we compute the efficiency of simple backtracking  of an algorithm similar to backtracking except that it notices when the predicate is empty instead of noticing when it is unsatlsfiable  the empty predicate method   of the combination of simple backtracking with the empty predicate method  and of search rearrangement backtracking  the unit clause rule combined with backtracking . the analysis is done over two sets of random problems. we also consider the algorithm based on the pure literal rule that was analyzed by goldberg and the results he obtained. all these algorithms are simplifications of the complete putham-david procedure  which has not been analyzed as yet  although most of i t s components have been analyzed . the performances of the algorithms are compared and features that lead to efficient algorithms are identified. 
1. introduction 
     some problems can be aolved by direct calculation in an efficient  straightforward way  but for many important classes of problems the best known method is a controlled search for solutions. such searches  unfortunately  can consume extremely  exponentially  large amounts of time. efforts to study and improve search methods are therefore of considerable practical importance. 
* this research was supported in part by the national science foundation under grant no. mcs 1. 
     by carefully analyzing a particular set of problems it may be possible to find problemspecific information that can be used to control the search. this can be an excellent approach; in some cases it has led to algorithms that avoid searching altogether. often  however  after a l l problem-specific information has been used  excessive search time is s t i l l required. another approach is to study general search algorithms and identify features that lead to an efficient search. the two approaches are complementary; the best algorithm for a particular problem is often obtained by combining problem-specific techniques with the best general search methods. 
     here we report the i n i t i a l results of a systematic study of the average time performance of search methods. all methods have about the same worst case time  exponentially large . also  techniques that lead to an improvement in average performance often result in a minor degradation of worst-case behavior  so a study of worst-case behavior can be misleading. the average time performance of these algorithms can be much better than the worst-case performance. some of the methods we study lead to an exponential improvement in average search time. 
     the search methods we analyzed include simple backtracking  search rearrangement backtracking  1   an algorithm similar to backtracking except that it notices when the predicate is true instead of noticing when it is unsatlsfiable  the empty predicate method   and the empty predicate method combined with simple backtracking. our analysis shows that the empty predicate rule does not contribute much to the performance of search algorithms  and that search rearrangement backtracking can be much more efficient than ordinary backtracking when the typical problem contains a large number of clauses with few literals per clause. 
goldberg  analyzed a version of the putnam-
     
1 
     
davis procedure that essentially relies on the pure l i t e r a l rule. the analysis shows that the pure l i t e r a l rule can save a huge amount of time on problems that have a large number of literals per clause  but it also suggests that the rule is 
unimportant when the typical clause does not have many literals. 
     we hope to analyze the f u l l putnam-davis procedure. the points that remain to be analyzed are the effect of stopping the search when the f i r s t solution is found and the effect of the pure literal rule when the typical clause has only a few l i t e r a l s . the effect of combining various techniques also needs to be analyzed. 
     any problem in the class np can be expressed as a predicate in the form of eq.o . many examples of such problems are given in . we illustrate the encoding of problems in this form with the game of generalized instant insanity . the game is played with n cubes. each face of each cube is painted with some color. the object of the game is to form a stack of n cubes with each cube oriented so that each face of the stack consists of cube faces that have distinct colors. each cube has twenty-four possible orientations. since the order of the cubes in the stack is irrelevant  the problem is equivalent to the predicate 
		 1  
where ok is the orientation of cube k and r i j  o i f oj  is true if and only i f   when cube i has orientation o¡À and cube j has orientation o j   the pair of cubes forms a legal stack of height two   a l l faces of the stack are made up of distinct colors . 
     the most obvious method of searching for solutions to a predicate in this form is to try a l l combinations of values of the variables. this sort of exhaustive searoh is prohibitively slow for large problems. fortunately  the special form of eq  1 permits three types of improvements. first  since each relation is defined over a small subset of the variables  a relation may become false as soon as a l l of i t s variables have been assigned values. in this case no extension of the current partial assignment of values to variables can be a s o l u t i o n . such extensions need not be i n v e s t i g a t e d . this is the idea behind 
backtracking  
     a second improvement consists of looking for variables that should be assigned values early in the search. it is particularly helpful to find a variable a l l of whose values make a clause false  under the current partial assignment   or which has only one value which does not make a clause false. this is the basic idea of search 
rearrangement backtracking  1   and of the unit clause rule in the putnam-davis procedure   
     a third approach involves looking for the values of a variable that are most likely to lead to a solution. in some cases there is a value that makes a l l relations which depend on the variable true. in that case only that one value of the variable requires consideration. this is the basis of the pure l i t e r a l rule in the putnam-davis procedure. 
     these ideas have been used extensively to improve the average running time of search algorithms. we are studying algorithms that use various combinations of the ideas. to specify these algorithms we f i r s t give a common general procedure and then present the d e t a i l s that distinguish the various methods. the procedure uses a stack to keep track of which variables have been set. 
generalized search procedure 
1.  initialize   set each variable to undefined. set stack to empty  
1.  select variable   select as the current variable a variable that needs to be tested. if there is no such variable  go to step 1* push the current variable onto stack and mark a l l of its values as untested. 
1 
     
1.  select value.  for the current variable select an untested value that requires testing. if there is no such value  go to step 1. otherwise set the variable to that value and mark the value as tested. 
1.  test.  if some clause of the predicate is false under the present partial assignment of values to variables  go to step 1 if a l l solutions are desired  go to step 1. 
1.  solution.  the current assignment of values to variables is a solution  any remaining 
unasslgned variables may take on any of their values . if only one solution is desired  stop. if a l l solutions are desired  go to step 
1. 
1.  back up.  set the current variable to undefined. pop stack. the new current variable is at the top of stack. if stack is empty  stop. otherwise go to step 1. 
     many search algorithms simplify the predicate as they search. for example  in the putnam-davis procedure clauses that become true are dropped. the dropped clauses are restored when the algorithm backs up. 
     the algorithms that we have analyzed search for a l l solutions; they never stop at step 1. in the future  unless otherwise stated  we will assume that step 1 always goes to step 1. for these algorithms the order in which values are tested at step 1 is immaterial  since a l l values must eventually be tested. 
     simple backtracking is obtained from the generalized search procedure by selecting the variables in a fixed order at step 1 and the values in a fixed order at step 1* every variable and every value that is considered at step 1 requires testing. in search rearrangement backtracking  a simple test is used to select a variable at step 1. each value of each variable is tested  using the same test used in step 1   and the variable for which the fewest values pass the test is selected. more sophisticated search rearrangement algorithms  test combinations of values. 
     in the putnam-davis procedure variables are selected by examining the predicate directly rather than by testing the values of variables. this method of selection is more powerful  but it is also more d i f f i c u l t to program and requires more knowledge of the internal structure of the clauses. 
     the o r i g i n a l putnam-davis procedure was designed for predicates in conjunctive normal form. to describe a more general procedure we first give some  nonstandard  definitions. a relation r is a unit clause i f   under the current p a r t i a l 
assignment of values to variables  there is an unset variable v such that r is false for every value but one of v. we call v the associated variable of the unit clause. a variable v is relevant if there is a relation r whose value under the current partial assignment depends on v. a variable v is associated with a bus literal if under the current partial assignment there is some value x of v such that when v is assigned value x every r that depends on v is true. the value x is called a safe iftluft* 
     the generalized putnam-davis procedure is our general search procedure with the following modifications. in step 1  if possible  select a variable associated with a unit clause. otherwise  if possible  select a variable associated with a 
     pure l i t e r a l . only one safe value is tested. if there are no variables associated with pure literals or unit clauses  then select any relevant variable. if there are no relevant variables  go to step 1. the putnam-davis procedure stops when the first solution is found. 
     we also considered the empty predicate method  an algorithm that may be viewed as an extremely simplified version of the putnam-davis procedure. in this algorithm variables are selected in fixed order at step 1  as long as some variable is relevant  the selected variable is not necessarily relevant . all values are tested at step 1  and no tests are done at step 1  i.e. the test gives the result true  unless a l l relevant variables have been assigned values. the procedure searches for all solutions. the procedure is like backtracking except that it backs up when a l l the ra are true instead of when one of them is false. it works by noticing when the predicate can be simplified to the point where it is empty. 
1* random prpfrlfifflff 
     to do an average time analysis it is necessary to select a set of representative problems and a probability d i s t r i b u t i o n over the set. for backtracking it is not obvious what a  typical  problem is. in this paper we consider two types of random problem sets. both are instances of 
     
1 
     
conjunctive normal form predicates. for each type of problem set we give a method of forming a random clause; a random predicate is then the conjunction of t random clauses selected independently  thus  a random predicate may happen to contain two copies of the same clause . 
     in the f i r s t method of constructing random clauses each clause has s literals for some fixed s. the s l i t e r a l s are independently selected from the 1v possible literals. this method was used in our earlier analyses of backtracking algorithms  1 . 
     in the second method each l i t e r a l has probablity p of being in the clause for some fixed p. this method was used by goldberg . the two models are roughly equivalent when the parameters are set so that p z &/1v. 
we follow the original papers in computing 
 running time  in the two models. in our model we assume that the  running time  is equal to the number of binary nodes in the search tree. the actual running time increases more rapidly  by a factor of approximately v   but this error is unimportant compared to the exponential differences in the average running times of the various search algorithms. 
     in goldberg's model we assume that the time to process a node is equal to avt   where a is a constant  v is the number of unset variables  and t is the number of terms that are s t i l l being considered. both unary and binary nodes are counted. 
1. results 
     table 1 summarizes the exact results for the average  running time  of various search algorithms. most of the exact results are not in closed form  so it is difficult to understand their significance. an asymptotic analysis makes these results easier to interpret. to obtain an i n t e r e s t i n g asymptotic analysis  careful consideration must be given to how s  p  and t should vary as v increases. we believe that 
     
table 1. exact formulae for tha number of solutions a&d for tha   running t i n e   of various starching algorithms for two sodala of random problems. 

1 
     
keeping s fixed  or l e t t i n g p= a/v for fixed a  and l e t t i n g t = by for fixed b gives results similar to those for many interesting realistic problems. this keeps the individual terms small while l e t t i n g the number of terms increase with v. we also consider t s v for fixed a 1.  for example  generalized instant insanity can be coded with s s 1  five logical variables for each o  in the original problem  and t s  v/1  1 .  the f i r s t choice for t produces problems where the number of constraints increases proportionately to the number of variables. the second choice produces problems where the number of constraints increases more rapidly. 
     table 1 gives the approximate value of the logarithm of the average  running time  for each algorithm. the two models generate problems with the same number of solutions when a =  in 1  s. usually the form of the answers is the same for the two models  but goldberg's model generates problems that are much easier to solve by backtracking when t s v  for  the results show that the 
empty predicate method is not helpful when p -  1. a l i t t l e thought suggests that this method is less helpful than stopping the search when the f i r s t solution is found  because both approaches need a solution before they can save any time. 
     comparing the results for simple backtracking with those for search rearrangement on our model with t s v¡ã shows that search rearrangement saves about as much time as reducing the size of the clauses by one l i t e r a l . this exponential improvement indicates that search rearrangement can be much more effective than simple backtracking. further analysis is needed to determine how search rearrangement behaves for t = bv. 
     

1 
     
     the following table shows the results of setting s = 1 a = 1 in 1 : 1  and b= 
	 in 1   in  1-1  -1 z 1 	in the formulas 
from table 1. these values for the parameters lead to an interesting set of difficult problems where the t y p i c a l problem has about one solution regardless of the size of the problem  as is often the case for realistic d i f f i c u l t problems. the values in the table demonstrate clearly the exponential improvement that can result from using simple backtracking. 

1. goldberg*a results 
     in  goldberg reported an average time analysis of an algorithm based on using the pure l i t e r a l rule to simplify the predicate. when no pure literal is available it chooses a variable and creates two simplified predicates  one for each value of the variable. it uses the same stopping rule as the empty predicate method. the analysis is over the goldberg model as described above  but with p constant. we are preparing a joint paper with goldberg in which some minor errors in the derivation presented in  are corrected.  reference  is becoming a draft of this paper.  these analyses show that goldberg's procedure takes polynomial average time on his model. when p is fixed  the size of the clauses increases with the number of variables  and the number of solutions to a typical predicate also increases. the analysis does not apply to models with variable p  but it suggests that  for the case in which p approaches zero as v becomes large  the performance of goldberg's algorithm is not dramatically better than that of the empty predicate method. 
1. conclusions 
     the putnam-davis procedure is the source of many good ideas for improving the efficiency of search algorithms. it may be viewed as a combination of 1  backtracking  1  unit clause selection  one level aearoh rearrangement   1  empty predicate detection  1  pure l i t e r a l selection  and 1  stopping at the f i r s t solution. the methods of  1 and of this paper are adequate to analyze an algorithm with the f i r s t three features. to analyze the first four is more difficult. goldberg  analyzed the effect of the pure literal rule in an algorithm where it does not affect the order of selection of the variables. it is quite likely that his methods can be combined with those of  1 1  to analyze an algorithm that uses all of the first four features. no work has yet been done on the effect of stopping at the first solution. 
     the analyses of random problems show that backtracking is effective on problems that contain a large number of clauses with few literals per clause. when backtracking is e f f e c t i v e   backtracking combined with the unit clause rule is even more effective  but when backtracking is not effective adding the unit clause rule does not help much. the pure l i t e r a l rule works well when each clause contains a large number of literals. since such predicates usually have a large number of solutions  stopping at the f i r s t solution is also effective in this case . none of the analyzed methods is very effective for problems with a moderate number of literals per term and with the number of terms equal to several times the number of variables. 
     the most straightforward direction for future work is completion of the analysis of search rearrangement backtracking. the blank entries in table 1 for level one backtracking can be filled in by completing calculations similar to those which led to the f i r s t entry. the m u l t i - l e v e l backtracking algorithms appear to be even more efficient for large problems   but analyzing their performance is difficult. 
     backtracking algorithms are easy to use: once the general search algorithm has been coded the user need only provide the routine to test partial solutions. adding features of the putnam-davis procedure that manipulate the predicate requires more programming e f f o r t   but the resulting algorithm may also be much more powerful. analyses are needed to determine whether this is the case. 
     there may be algorithms that are both simpler to analyze and more powerful than the putnam-davis procedure. one weakness of the putnam-davis procedure is that it does not have any guidance concerning which variable to select in eases where 
     
1 
     
the pure l i t e r a l rule ami the unit clause rule oo not apply. a good technique is to select a variable from the shortest remaing clauae. a recent analysis by monien et al  shotted that a problem with three literals per term can always be salved in time 1v using an improvement on the putnam-davis algorithm that selects variables from the shortest clause. some improvement is also obtained for problems with more than three literals per term. other interesting techniques for modifying backtracking have been proposed 
 1 1 . 
     another area where progress is possible is in the use of rules to manipulate the predicate. subsumption can be combined with the putnam-davis procedure. substitution of equal quantities can also be used. 
     many of the techniques used in this paper were developed in earlier work  1 1 . each of the original papers analyses one algorithm on one model. here we apply the techniques to a variety of algorithms and models to determine how important various features are to efficient searching. a large number of ideas have been suggested for improving the efficiency of searching. if they were a l l combined the result would be a large  complex program  containing many parts that made l i t t l e or no contribution to its efficiency. analysis of average running time is a powerful technique for determining the value of proposed improvements. 
