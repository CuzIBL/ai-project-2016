. 
psychological experiments on children's development of spatial knowledge suggest experience at self-locomotion with visual tracking as important factors. yet  the mechanism underlying development is unknown. we propose a robot that learns to mentally track a target object  i.e.  maintaining a representation of an object's position when outside the field-of-view  as a model for spatial development. mental tracking is considered as prediction of an object's position given the previous environmental state and motor commands  and the current environment state resulting from movement. following jordan and rumelhart's  1  forward modeling architecture the system consists of two components: an inverse model of sensory input to desired motor commands; and a forward model of motor commands to desired sensory input  goals . the robot was tested on the  three cups  paradigm  where children are required to select the cup containing the hidden object under various movement conditions . consistent with child development  without the capacity for self-locomotion the robot's errors are self-center based. when given the ability of self-locomotion the robot responds allocentrically. 
1 	introduction 
this research challenges the traditional approach of theory construction in cognitive development by using the framework of robot learning. traditionally  researchers in cognitive development  e.g.  developmental psychologist  have focused on general and abstract descriptions of experimental data as explanations for their observations. however  developmental psychology is intrinsically limited with respect to the question  how does development occur    because of difficulties in the methodology  e.g.  scientists should not open an infant's head to check for internal representations  and should not control their everyday experiences . instead of real infants  we need a substitute that can be used for testing the theory and controlling conditions without ethical limitation. consequently  the requirement of a computer simulation can no longer be ignored. 
　over the past few decades several studies have been conducted on computational models of cognitive development. for example  klahr and wallance developed a computer model of acquisition of number conservation1 using self-modifying production system  klahr and wallance  1   drescher 
proposed a schema mechanism to elaborate and test piaget's theory from a constructivist's perspective 
 dresher  1l . however  what is lacking in these approaches is an account of the interaction between children and environment. consequently  models based on these approaches sometimes lack realism. we should pay more attention to the dynamics of cognitive development in the real world. 
　in contrast to these approaches  we propose using autonomous robots as the subject of cognitive development  and constructing computer programs by which robots can develop or learn analogously to infants. the advantage of using robots is twofold. first  we can utilize a robot's vision sensors and actuators as the inputs and outputs of the model. this forces us to use the same input stimuli and action goals as those of the infant  whereas the input and output representations of a computer simulation must be assumed. second  we can construct a theory absorbing activeness in cognitive development. recently  researchers have emphasized the importance of activeness  i.e.  mobility  of infants during development  thelen and smith  1 . however  the theory derived from this stream needs 
　　1 conservation is a term introduced by piaget for the child's understanding that quantitative aspects of a set of materials are not changed or affected by transformations of the display itself. 
	hiraki  sashima  & phillips 	1 

to be tested and refined in more detail. we believe that using a robot leads us to a more concrete theory. more recently  elman et  al. published an exciting book on development from a connectionist perspective  elman et a/.  1 . we follow their approach  but concentrate much more on interaction between individuals and environment. 
　as a first step to constructing a complete computational theory of cognitive development  we address the question of how infants relate to their spatial environments  and how this changes as the infant matures. to explore these issues  we focus on the change of mental tracking: the ability to update spatial relations between self and object without real  visual  tracking during the locomotion. we modeled the development of mental tracking as a learning task for a simulated robot  and conducted experiments simulating an infant's experience of locomotion. 
　the following sections describe our first results of modeling infant's spatial development. in section 1  psychological evidence for spatial development is introduced. in section 1  we elaborate our model for the development of mental tracking. section 1 describes an empirical experiment with a simulated robot. in section 1  we discuss the implications of our approach and future work. 
1 psychological evidence for spatial development 
1 	egocentrism in early infants 
piaget suggested that before infants are 1 year old  they exhibit a kind of sensorimotor egocentrism  piaget  1 . although the term egocentrism refers to young children's general tendency to view the world solely from their own perspective1  we focus on infant's egocentric behavior in the spatial environments and how egocentric behavior changes into the allocentric behavior that normal adults exhibit. in other words  we address the question of how infants relate to their spatial environment  and how this changes as the infant matures. 
　figure 1 shows an experiment designed to investigate infant's spatial searching  bower  1 . a doll  prize  was put inside one of three cups  in this case the middle one  and then the infant moved around the table. the doll's relative position from the infant's view was changed from 'middle' to 'right'. thus  the infant should look for the doll under the right cup. however  early infants frequently fail to 
　　1  piaget used the word egocentrism referring not only to spatial behavior but also to more genera  aspects of young children such as egocentic communication. 
1 	cognitive modeling 

figure 1: self centered representation of space. 
compensate for changes in their own spatial position. they continue to turn in the direction that previously led them toward the target  middle cup . the egocentric behavior in the searching task can be observed up to the age of about 1-months  but older children consistently show non-egocentric behavior on spatial tasks that involve searching for objects from different view points. 
　in the searching task the containers  cups  used to hide the prize were the same size and color. however  the actual environment provides much more information than position. in fact  infants can search the prize more correctly when distinctive cues  landmarks  are provided  acredelo  1 . 
　phenomena concerning egocentrism are quite controversial and there are still many on-going studies. however  it's important to note that egocentic encoding of the target's position is not so irrational for infants who cannot move around. pre-walking infants don't have to take into account changes in their own position in order to search for the target. 
1 	effect of locomotor experience 
so far  we have seen an interesting developmental change in infants spatial behavior. 1-year-olds behave egocentrically and 1-year-olds do not. what is the difference between 1-year-olds and 1-yearolds  how does egocentric behavior change into allocentric behavior  experience in moving around environments seems to be one of the factors that effects the difference in the searching task. kermoian and campos suggested the importance of locomotor experience  kermoian and campos  1 . infants who have experience in a walker or who can crawl are superior in spatial tasks to infants who have no such experiences. 
　acredelo  asams and goodwyn conducted experiments to test the role of self-locomotion as opposed to passive transport concerning infant's spatial cognition  acredelo et al.  1 . their results sug-
gested the importance of active movement with visual tracking. when 1-month-olds walked to the other side of a layout and have the opportunity to continually looked in the direction of a hidden prize  they looked in that direction more often and subsequently did better at turning toward the object from the new location than children who were carried. in contrast  when they could not see the prize as they walk from one position to the other  they were subsequently no better in turning toward it than children who were carried. based on these results they hypothesized self-produced motion leads to more effective deployment of visual attention. 
1 	learning to mentally track 
the psychological experiments mentioned previously suggest two important factors for the spatial development: 
  self-locomotor experience; and 
  visual tracking. 
　yet  it is still unclear what information is central in promoting the change from egocentric behavior to allocentric behavior. in acredelo's experiments  1-month-olds can behave correctly without visual tracking of the target object. this leads us to the necessity of modeling the mechanism of the change  taking into account of the effect of locomotion experience. 
　in this section  we focus on the change of the ability of updating the spatial relation between self and target object without visual tracking during locomotion. we call this ability mental tracking  and propose a learning architecture for mental tracking by which robots can learn it analogously to infants. firstly  we present our assumptions and identify the information that is available during the experience of self-locomotion. 
1 	formalization as a robot learning task 
the robot 
figure 1 shows a robot that was used for modeling infant spatial development. the robot is based on nomad 1  nomadic technologies inc . it can control two wheels and trunk orientation. the robot is equipped with a movable stereo-camera  sony evid1 x 1  that is connected with a vision processing unit that uses a fujitsu tracking module and dsp board tms1 x 1  for accelerating image processing. using these facilities  the robot can detect relative distance and orientation to the target. 

figure 1: a robot for modeling cognitive development. 
locomotion experience 
suppose that the above robot is the infant who has just started toddling. what types of information can the robot receive from walking  we assume that the self-locomotion experience of robots can be characterized by applying a next-state function / and an output function g successively. at time step n - 1 the robot produces motor command u n - 1 . in conjunction with the state of the environment x n - 1   the motor command determines the next state: 
		 1  
　corresponding to each state x n  there is also a sensation y n : 
		 1  
　we assume that the robot has access to the state of the environment: y{ n   can be seen as visual information directly obtained from the camera. the formalism is analogous to a standard state-action loop of mobile robot. 
the learning task for visual tracking now we model the experience of locomotion with visual tracking. locomotion experience with visual tracking can be modeled with a robot that moves while tracking the target with its movable camera and trunk. in other words  the task of visual tracking can be seen as generation of motor commands to the camera and the trunk to keep the target ob-
ject on the center of the visual image. note that we should consider two types of motor commands; one for moving and the other for visual tracking. 
　let be a desired sensation  and in this case the target object in the center of the visual image. 
	hirak1  sashima  & phillips 	1 

let be a motor command for moving1  and be a command for visual tracking. given the state  representing the current posture of the robot   the robot produces 
an action 

　the learning task for visual tracking with locomotion is to make appropriate adjustments to the input-to-action mapping h based on data obtained from interaction with the environment. note that we assume is also given. this is because the robot should know how to move to the next position. the robot produces independently of visual tracking. 
1 	the learning architecture for mental tracking 
so far  we have defined the learning task for visual tracking with locomotion: nonetheless  what we need is a model of mental tracking. it must be noted here that mental tracking can be accomplished by mentally simulating visual tracking. in other words  if the robot can learn to track the target while in motion  the robot can mentally track the target by applying input-to-action mapping h successively as an internal process. first  we present a learning architecture for visual tracking with locomotion  and then describe how to use the acquired knowledge for mental tracking. 
learning inverse model 
as mentioned above  the learning task for visual tracking is to determine a proper command 
given this is analogous to the so called inverse model in control system design. a controller receives the desired sensation as input and must find actions that cause actual sensations to be as close as possible to the desired sensation. the controller must invert the transformation from actions to sensations. 
　we developed this mechanism based on the neural network architecture proposed by jordan and rumelhart  jordan and rumelhart  1 . there are several reasons for using their approach. one of the advantages of this architecture is that we don't need an explicit teacher. the robot can use the difference between predicted position of the target and the next input of the target position as training data. another reason is that the architecture is capable of addressing the many-to-one mapping problem from actions to sensations. the robot shown in 
1
　　the motor command can be seen as the command to move the robot's two wheels. 
1 	cognitive modeling 
figure 1 must control at least two parameters  one for the camera and the other for the trunk to track the target. so there are infinite number of possible inverse models1. 
　using these features  the mechanism learns to produce appropriate motor command  to keep the target in the center of the visual image  
given the current state and a motor command to move to the next position 

figure 1: the learning mechanism for visual tracking with locomotion. 
　figure 1 shows the learning architecture for visual tracking.  denotes a desired position of the target in the visual image  denotes a proper command to the movable camera. y  n   denotes the actual position of the target in the visual image  and uv n - 1  denotes the actual motor command for the movable camera. 
　in order to learn the inverse model to keep the target in the center of the field-of-view  we need the difference between the proper command and actual command  to adjust the motor command: 
		 1  
　we used the method described in  jordan and rumelhart  1 . firstly the robot learns a forward model based on the difference between y* n   the output forward model  and y n . here the difference  1  can be acquired by backpropagating the difference between y* n  and y n  through the forward model. then  the robot learns the inverse model based on the difference  1 . 
the network architecture 
we implemented the above learning architecture using a feedforward network based on the block diagram shown in figure 1. the network is composed 
1 see  jordan and rumelhart  1  for more details. 

of two subnetworks: one for the inverse model; and the other for the forward model. the inverse model consists of 1 input  1 hidden and 1 output units. the forward model consists of 1 input  1 hidden and 1 output units. the output of the inverse model is taken as input to the forward model. 
mental tracking via acquired knowledge 
figure 1 illustrates the way to mentally track the target using the learned visual tracking. the shaded portion denotes the acquired knowledge for visual tracking. the command for moving around in the environment is denoted as 
　note that the robot uses the output of the forward model as the current state  instead of actual input from the camera  environment . in order to mentally track the target with locomotion  the robot produces virtual command  to the forward model as an internal process. 

figure 1: the architecture for the mental tracing. 
1 empirical experiments with a simulated robot 
1 	the setup 
in the following experiments  we simulated three stages of a child's development of motor skills with the robot by varying its permitted actions. in stage 1  the robot is only permitted head rotation. in stage 1  the robot can rotate both head and body. finally  in stage 1  the robot is also permitted selflocomotion  whereas in stages 1 and 1  locomotion was performed by an external agent. 
　for each stage  the forward and inverse models of the network were trained until: 
forward model prediction error was less than 1 for 1 training steps  or 1 training steps were completed. 
inverse model the difference between the proper command and actual command 
1  was less than 1 for 1 training steps  or 1 training steps were completed. 
1 	the three cups task 
following the  three cups  paradigm  bower  1  discussed previously  the robot is placed in front of three cups and shown which cup hides the target object1. the robot moves  or is moved  to a new position from which it must predict which cup hides the target object. 
1 	results 

figure 1: prediction rate after each stage  average over 1 runs . 
　figure 1 shows the rate of selection one of three cup after training for each of the three stages. the target cup  cup a  is black. the robot's performance at stages 1 and 1 was at chance level  1% . however  analysis showed that responses were consistent with an egocentric based prediction  not random choice. for example  from an allocentric perspective  a cup on the left-hand side of one's field of view would appear on the right-hand side if one views the cups from behind. from an egocentric perspective  however  one would predict the target as being on the left-hand side  which was the robot's prediction for stages 1 and 1. since the cups were arranged in the shape of an equilateral triangle  only 1% of positions will yield a correct prediction based on egocentric knowledge. under random choice there is no correlation between the relative 
　　1  this was done by labeling the target cup at the initial location. 
	hiraki  sashima  & phillips 	1 
positions of the selected cups before and after movement. in stage 1  when the robot also had control of translational movement  its predictive accuracy was above chance and egocentric levels  and more consistent with an allocentric based choice. thus  locomotion experience was important for learning to predict the target's position. 

figure 1: relative angular position of cups as a function of robot location at the end of stage 1. 

figure 1: relative angular position of cups as a function of robot location at the end of stage 1. 
　the contrast between egocentric behaviour  at stage 1  and allocentric behaviour  at stage 1  is made clearer by plotting cup position in the robot's field-of-view as a function of the robot's location. 
　figure 1 shows the angular position of each cup relative to the center of the robot's field of view at various locations around the cups. the robot was moved along the circumference of a circle enclosing the cups  see figure 1   and cup positions were recorded at l/1th intervals. for example  1 on the x-axis corresponds to the robot's initial po-
1 	cognitive modeling 
sition. for the y-axis  negative and positive values correspond to the left and right halves of the fieldof-view  respectively . for each location  the cup with the smallest angular position  in magnitude  is the selected cup. for example  at position 1  cup c was selected. as evident from figure 1  the robot always selected the leftmost cup as the target. in other words  the robot adjusted its head so that the leftmost cup  relative to the robot  was positioned at the center of the field-of-view  1 angular position . consequently  the other two cups appeared in the right half of the field-of-view. this is to say that the robot behaved egocentrically. crossovers on the graph  e.g.  position 1  occurred because there are 1 locations on the circumference of the enclosing circle for which one cup is occluded by another. 
figure 1 shows mental tracking at the end of stage 
1. as can be seen from the graph the target cup  cup a  remains closest to the center of the robot's field-of-view for most locations  i.e.  it behaved allocentrically . again  the exceptional cases  i.e.  crossovers  are due to occlusions. 
1 	discussion and future w o r k 
so far we have described mental tracking as a computational model of infant's spatial development. the simulation results support the evidence found in developmental psychology: the importance of self-locomotion. furthermore  the results of our simulation suggest that experience of selflocomotion with visual tracking can accelerate spatial development. this offers the key to an understanding of how egocentric behavior changes into allocentric behavior. 
　as for mental tracking  tani proposed a similar idea in the context of robot navigation  tani  1 . he developed a robot that is capable of mentally simulating action plans based on a forward modeling scheme using recurrent network learning. although he was not concerned with cognitive development  he did suggest its relevance to cognition. 
perhaps the closest approach to ours is the cog 
project of brooks and colleagues  brooks and lynn  
1 . they have been developing human-body like robots  humanoid. the idea of creating humanoids to investigate human cognition is very attractive in a sense that intelligence cannot come without body. we believe that the key concept of using robot for modeling cognitive development can be achieved even with simple mobile robots. 
　current simulations were limited to one step back in time  i.e.  target visible from the previous time step . for more complex environments  the target will be outside the field-of-view for indef-

inite periods. an obvious  and elegant  solution is to incorporate recurrent connections and have the network learn to remember positional information  e.g.   elman  1  . however  learning to maintain information over long periods in the absence of additional input is difficult without special learning techniques  e.g.  incremental learning   elman  1  . the extent to which mental tracking is maintenance of internal representations  or the search for alternative visual cues is an interesting research issue. and one that can best be addressed in real-world active environments such as we have proposed with the use of robots. 
　in our simulations  we divided child's development of motor skills into three stages. in general  however  motor skills develop more gradually  and interact with spatial development more tightly. more likely is that the development of motor skills and spatial knowledge interact both ways  thelen and smith  1 . we need to explore this kind of interaction in future work. 
1 conclusion 
in this paper  we addressed the question of how infants relate to their spatial environment  and how this changes as the infant matures. to explore these issues  we introduced mental tracking as a key concept  and propose a learning architecture for mental tracking analogous to infants. although there is much work to be done  we believe that the idea of using autonomous robots as the subject of development will open a new approach to modeling cognitive development. we take inspiration from recent work in robot vision  where the problem of making a robot see generated predictions leading to discoveries in insect vision  franceschini et al.  1 . we expect similar results for cognitive development. 
acknowledgement 
we thank hideki asoh for his comments on initial stage of this work. we also thank motoi suwa  kazuhisa niki and hideyuki nakashima for their support. 
