 
data-intensive web sites have created a new form of knowledge base  as richly structured bodies of data. several novel systems for creating dataintensive web sites support declarative specification of a site's structure and content  i.e.  the pages  the data available in each page  and the links between pages . declarative systems provide a platform on which a1 techniques can be developed that  further simplify the tasks of constructing and maintaining web sites. this paper addresses the prob-
lem of specifying and verifying integrity constraints on a web site's structure. we describe a language that can capture many practical constraints and an accompanying sound and complete verification algorithm. the algorithm has the important property that if the constraints are violated  it proposes fixes to either the constraints or to the site definition. finally  we establish tight bounds on the complexity of the verification problem we consider. 
1 	introduction 
data-intensive web sites have created a new form of knowledge base. they typically contain and integrate several bodies of data about the enterprise they are describing  and these bodies of data are linked into a rich structure. for example  a company's internal web site may contain data about its employees  linked to data about the products they produce and/or to the customers they serve. the data in a web site and the structure of the links in the site can be viewed as a richlv structured knowledge base. 
   the management of data-intensive web sites has received significant attention in the database community  fernandez et al.  1; atzeni et al.  1; aroccna and mendelzon  1; chiet et al.  1; paolini and fraternal   1 . the key insight of recent systems is to specify the structure and content of sites dedaratively. these systems separate and provide direct support for the three primary steps of site creation:  1  identifying and accessing the data served at the site   1  defining the site's structure  i.e.  the pages  the data in each page  and the links between pages   and  1  specifying 
1 	knowledge-based applications 
the h t m l rendering of the site's pages. step 1 is usually supported by a declarative  specification language. 
　web-site management systems based on declarative representations offer several benefits. first  since a site's 
structure and content are defined dedaratively  not procedurally by a program  it is easy to create multiple versions of a site. for example  it is possible to build 
internal and external views of an organization's site or to build sites tailored to novice or expert users. currently  creating multiple versions requires writing multiple sets of programs or manually creating different sets 
of h t m l files. second  these systems support the evolution of a site's structure. for example  to reorganize pages based on frequent usage patterns or to extend the site's content  we simply rewrite the site's specification. another advantage is efficient update of a site when its data sources change. 
   declarative web-site management systems also allow us to view a site's definition and its content as a knowledge base. a natural next step is to consider how reasoning techniques can further improve the process of build-
ing and maintaining web sites. we consider the reasoning problem of verifying integrity constraints over web sites. specifically  when the structure of a site becomes complex  it is hard for a designer to ensure that the site will satisfy a set of desired properties. for example  we may want to enforce that all pages are reachable from the root  every organization homepage points to the homepages of its sub-organizations  or proprietary data is not displayed on the external version of the site. a study on the usability of on-line stores  lohse and spiller  1  provides other constraints that if followed  would improve the site design. 
   for a verification tool to be useful  if must verify constraints against a site definition  not a particular instance of the site  because  1  we do not want to verify the constraints every time the site instance changes  and  1  if a web site is dynamically generated  an instance is never completely materialized making it is impossible to check the constraints. verifying the constraints on the site definition ensures that as long as the site is generated according to the definition  the constraints will be satisfied. for this reason  the verification problem requires reasoning  and not just applying a procedure to the site. furthermore  when the integrity constraints are not ver-

ified  the system should automatically propose a set of candidate modifications to the site definition. this raises a search problem in the space of possible modifications. 
　this paper makes the following contributions. first  we identify an important class of integrity constraints relevant to web sites. second  we describe a sound and complete algorithm for verifying the integrity constraints and an analysis of their complexity. the key feature of our algorithms is that they consider only the specification of the site's structure and content  not a particular instance of a site. hence  the verification is independent of changes to the underlying site  as long as they are generated by the same specification. finally  in cases where the verification algorithm shows that the constraints may be violated  it proposes a set of corrections to the web 
site's definition. 
　the problem we consider is closely related to the problem of knowledge-base verification  see  vvt'1  1  for a recent workshop . we follow the paradigm proposed in  levy and rousset  1   where algorithms for verification are based on query containment. however  whereas in  bevy and rousset  1  there was a 1 translation between the verification problem and query containment  a challenge in our case is to perform the appropriate transformation. 
   we believe that web-site management tools based on declarative specifications will pose several important at research problems in the near future. hence  one of the contributions of this paper is to bring the problem to the attention of our community. in. the last section  we mention other research problems in this context. 
1 	declarative management of web sites 
declarative systems for web-site management are based on the principle of separating three tasks:  1  the management  of the data underlying the site   1  the definition of the site's structure and the content  and  1  the graphical presentation of the site. the first step requires identifying the sources that contain the site's data. we refer to this data as the raw data. these sources may include databases  structured files  or pre-existing sites. we assume that we interact with each of these sources via a wrapper program that produces the necessary data in tabular form. here  we assume that the raw data is stored in a single relational database system. in the rest of the paper  we use an example that  is a small fragment of a publication's web site. fig. 1 contains the schema of the raw data and sample data. 
   the second step in building a web site requires specifying the site's structure. we describe a formalism for specifying this structure that captures features common to many declarative systems for web-site management  fernandez et al.  1; atzeni et al.  1; arocena and mendelzon  1; cluet et al.  1; paolini and fraternali  1 . we emphasize that the declarative specification is concerned with the logical model of the site as a set of nodes and links  not its graphical pre-

figure 1: the schema and data underlying the publication web site. 
sentation  i.e.  how each node is translated to h t m l   . wrhen using the systems above  the site designer also specifies the graphical presentation of each page  usually by a set of h t m l templates  each of which applies to a group of related pages. 
1 	specifying web-site structure 
in order to specify a site's structure  we need to state  1  what pages exist   1  what  data is available in each page  and  1  what links exist between pages. we specify the structure of site in a site definition. given a site definition and a database instance  applying the definition to the database produces an instance of the site  called a site graph. fig. 1 contains our example site definition and fig. 1 contains the resulting site graph. 

figure 1: the site definition for our example site 

figure 1: the example site graph. 
　a site definition is a graph whose nodes are labeled by variables or by functional terms of the form f x   where 
	fernandez  florescu  levy  and suciu 	1 

x is a  possibly empty  tuple of variables. functional nodes in the site definition represent sets of pages in the site graph. in our example  the node personpage y  represents the set of pages personpage p  where p is a constant in the database. non-functional nodes are leaves and have one incoming edge. they represent the data contained in the page that points to thern. for example  the node n represents the name of the person y. functional nodes that have no arguments represent unique pages  such as the root. 
   each functional node is labeled with a horn rule that defines the conditions for the existence of instances of the node. a rule's head is an atom of the form node f x    where node is a special predicate. for example  the rule for yearpage specifies that there will be a node for a year z if some article was published in year z. the rules for our example site are: 

edges in the site definition represent sets of links in the site graph. each edge has an associated a horn rule  which specifies the conditions for existence of a link between instances of source and destination nodes. the horn rules use the special predicate link. for example  the rule fourth below specifies that there is a link in the site graph between the page personpage y  and the page articlepage x  if y is an author of paper x. the third argument of the predicate link is the link's label in the site graph.1 we assume that in all of the rules this argument is always a constant. the rules for the links in our example are given below. 
link root    publications     publications'1  : -true. 
link root    people     people   : -true. 
link publications   yearpage z   	 year   	: 	-article -  z . 
link peoplc    personpage y    person   : -person y    . 
link per1onpage y  articlepage{x    article   : author v  a'   article x      . . 
link yearpage z   articlepage x    article    : article x     z . 
link articlepage x   personpage y    author   : - author y  x   person  y    . 
finally  the data contained in each page is also specified by horn rules. for every leaf associated with afunctional node  we associate a horn rule defining the contents of the leaf. the first rule below specifies that the name of a person will be contained in the appropriate person page: 
ltnk personpage y   n   name   	: 	-personiy  n . 
link articlepage x j  	 title   	: 	-article xj  	  . 
link{articlepage x  ps  	 ps   : -article a'     .   psfilc x ps . 
     1  this string denotes the name of the relationship between the nodes in the site graph  and not the anchor that will appear on the link in the actual site. anchors are omitted for clarity. 
1 	knowledge-based applications 
declarative specification of a web site offers many advantages: rapid modification of the site's structure; creation of multiple versions of the site for different classes of users; and  as we explore next  the ability to reason globally about the site's structure. in principle  restructuring a site or building another version requires modifying the set of rules that define the site  instead of modifying each page and its hard-wired links. 
1 	specifying integrity constraints 
although declarative specification can simplify the task of creating complex sites  the specification of a richly structured site can be long. for example  the specification of a customer-billing site using the strudel specification language  fernandez et al.  1  is 1 lines. the specification is more concise than the equivalent implementation in a scripting language  but still too large to determine without automated reasoning whether global constraints on the site are satisfied.  lohse and spiller  1  describes web sites for on-line stores. they argue that enforcing integrity constraints on such sites is critical to customer satisfaction and describe a set of such constraints. our goal is to take advantage of a site's declarative definition and develop algorithms for verifying that  a given definition only produces sites that satisfy the given set of constraints. for our example  some possible constraints include: 
1: all article pages are reachable from the root page. 
ic1: for every article  there is a link from its article page to its postscript source. 
1: if two articles have a common author  there is a path between the corresponding article pages. 
ic1: if two articles have been published in the same year  there is a path between the corresponding article pages. 
　we may also want to specify constraints that limit the length of a path between two nodes  or that force every path to a node to go through some distinguished set of nodes. we define our language for specifying these kinds of integrity constraints and formally define the verification problem. 
　integrity constraints express properties we would like the web site to have. since the web site is modeled as a graph  integrity constraints should be able to express the existence of certain paths between pages in the site. we express such paths using regular-path expressions. a regular-path expression over the set of constants c is formed by the following grammar  r  r1 and r1 denote regular-path expressions : 

in the grammar  a denotes a constant in c; not  a  matches any constant in c different from a. an   denotes any constant in c; a period denotes concatenation  and | denotes alternation. r*  denotes 1 or more repetitions of r. for example  a.b...c+ denotes the set of 

paths beginning with ab  then an arbitrary element of c and then any number of occurrences of c. we use * as a shorthand for   meaning an arbitrary path of length 1 or more. 
   regular-path expressions are used in path atoms of the form x  y  where r is a regular-path expression  and x and y are terms. the atom x  y is satisfied in a labeled directed graph g by each pair of nodes xyy for which there is path from x to y that satisfies the regular path expression r. 
   in principle  we can express integrity constraints using arbitrary formulas in first-order logic. however  our main goal here is to identify a more restricted language for which it is possible to develop sound and complete verification algorithms and which is expressive enough to model integrity constraints that are of practical interest. we consider integrity constraints that have the form  where and are conjunctions of path atoms  atoms of the relations of the raw data  and atoms of the relation node. variables that appear in both  and are assumed to be universally quantified  while the others are existentially quantified. the following sentences express the integrity constraints in our example. 
1: 
ic1: 
1: 
1:  
　given a particular site graph  it is straightforward to test whether an integrity constraint holds. however  our goal is to verify at the intentional level whether an integrity constraint  is guaranteed to hold  i.e.  given a site definition  test whether the integrity constraint will hold for all web sites that can be generated by 1v  for any possible database state. formally  our problem is the following. 
definition 1: be the relations in the schema of the raw data  be a site definition. let ic be an integrity constraint. we say that satisfies ic if for any given extension x of the relations ic is satisfied in the site graph resulting frorn and i. 
　in our example  ic1 is satisfied  because every article has a year of publication  and therefore is reachable through the yearpage. similarly  1 is also  satisfied. ic1 is not satisfied  because some articles may not have 
postscript sources. although ic1 is satisfied by the site graph in fig. 1  it is not necessarily satisfied for every site graph. 
　next  we describe a sound and complete verification algorithm  and show how the complexity of the verification problem changes with the form of the integrity constraints considered. 
1 	verification algorithm 
the crucial step of our verification algorithm is to translate the integrity constraint into a pair of datalog programs  and datalog  ullman  1  is a database query language where queries are specified by sets of horn rules  and the meaning of the query is given by the least fixpoint model of the database and the rules. our translation has the property that the integrity constraint is satisfied if and only if the datalog program is contained in the program informally  given two queries and  the query contains the query if 's result is a superset of 's result for any database instance. algorithms for query containment have been studied extensively in the database literature  ullman  1 . these algorithms can be viewed as logical-entailment  algorithms for specific classes of logi-
cal sentences  which is why they are useful in our context. our algorithm has two steps. 
1. given the integrity constraint   and the site create a pair of datalog queries and  
1. we use an extended query containment algorithm to test whet her  is contained in  if the containment holds  then the integrity constraint is guaranteed to hold. if not  the containment algorithm returns a set of candidate fixes. 
we describe each step in more detail. 
　the algorithm in fig. 1 translates either  into a datalog program. this step relies heavily on the possible paths specified in the structure of the site definition in order to generate  and  the subtle part of the translation concerns the path atoms. given a path atom  y  y  the translation builds in a bottomup fashion a datalog program that defines a relation corresponding to each of the subexpressions of r. the translation varies slightly depending on whether a' and y are variables  functional terms  and whether there is another conjunct of the form node x   node y  . in the figure  we show only the ease when a' and y are unary functional terms. 
　if our extended query-containment algorithm reports that  is contained in  then then the integrity constraint is guaranteed to hold. otherwise the containment algorithm returns a set of candidate fixes. the algorithm considers four kinds of fixes: 
  add conditions to  in the integrity constraint  
  remove conditions from the rules in the site defini-tion  
  modify by adding back arcs in the site definition  and 
  suggest a set of integrity constraints to enforce on the raw data  which guarantee that the constraints on the site will hold. 
the fixes are reported to the site designer  who can then decide how to proceed. due to space limitations  we 
	fernandez  florescu  levy  and suciu 	1 

only illustrate this phase of the algorithm through the example below. intuitively  the fixes are generated by searching through the possible modifications to  and  such that for the modified queries  the containment 
holds. 
algorithm ic-translate  
input:  is either the lhs or rhs of an 1c. is the site definition. arc the universally quantified variables in the ic. output: a datalog program defining the relation  

ii a x  is an atom; r is a site definition. algorithm atomtoprog  1  r  if a is of the form  
then return the datalog program: 
if a is of the form f x   g y  then return the datalog program constructed as follows: 
for every rule r  r of the form link f1  x1   f1 x1   v  : -body where the rules for f1 and f1 have bodies body1 body1 respectively  add the following rule: 
 : -body  body1  body1 
define an idb predicate for r by structural induction on r: if r is of the form  a   then 

the query predicate of the datalog program is defined by: qa x y :-qr f x g y . end atomtoprog 
figure 1: algorithm for translating the lhs or rhs of an integrity constraint into a datalog program. 
　consider the constraint 1 in our example  that requires a path from the root page to any article page. the translation step produces the following two datalog programs  whose query predicates are qths and qrhs. since the rhs of the constraint involves a path atom with the regular expression *  the datalog program of qrhs defines a predicate q* a'i  f1  x1  f1   the transitive closure of q    which describes the possible paths in the site graph between nodes of the form f1  x1  and f1 x1 - we also use the rules defining q* in the other parts of the example. 
1 	knowledge-based applications 
tained in the verification test succeeds. for ic1  the algorithm produces the following two programs  for which the containment fails. 

however  in this case  the algorithm will propose a correction to the integrity constraint  namely adding the conjunct psfile x y  to the left hand side  meaning that the constraint needs to hold only on articles that have a postscript source . 
finally  ic1 would result in the two programs: 

　in this case  the containment does not hold because paths between article pages in the site only go through the author pages  not through the year pages. hence  the algorithm will suggest to add a link from articlepage to either rootq  the publications    or to the corresponding yearpage  and would propose the appropriate query to put on the new link. 
1 	complexity of verification 
the algorithm described in the previous section provides a sound and complete verification algorithm in many important cases. this section characterizes these cases and establishes the complexity of the algorithm and of the verification problem. note that in all of the results  the complexity is measured in the size of the site definition and not the size of the underlying raw data. 
　the following theorem considers the case in which there are no cycles in the nodes in the site definition. 
theorem 1: let  be a site definition and ic be an integrity constraint of the form  assume that there are no cycles between nodes in the site definition. then  our verification algorithm is sound and complete and runs in non-deterministic polynomial time. the verification problem under these conditions is np-complete. 
　the following theorem permits cyclic site definitions  but requires that the left-hand side of the integrity constraint does not contain path atoms with kleene star. 

this is a common case  because the left-hand side usually refers to conditions on the raw data  not on the site graph. 
t h e o r e m 1: let  be a site definition and ic be an integrity constraint of the form  where  does not contain path atoms with kleene star. then  our verification algorithm is sound and complete and it runs in non-deterministic polynomial time. the verification 
problem 	under 	these 	conditions 	is 	np-complete. 
   the proof of the theorems is based on the fact that the size of  and  is polynomial in the size of  and the complexity of the corresponding containment algorithms. note that in general  containment of arbitrary recursive datalog is undecidable-  shmueli  1   but in the cases considered above  is always nonrecursive. note that if contains the interpreted predicates then the complexity of the problems in the theorems is  
1 	conclusions and related work 
web-site management systems based on declarative representations offer many opportunities for applying al research to improve the web-site construction and maintenance process. this paper considered the first such problem  namely the specification and verification of integrity constraints. we described a language for specifying a wide class of constraints and a sound and complete algorithm for verification. in addition  our algorithm suggests fixes to the site definition when the integrity constraint does not hold. 
   our work can be viewed as an extension of verification methods for rule-based knowledge-base systems. of that work  the most related. to ours is  levy and rousset  1  which first showed how to use query contain merit techniques for knowledge-base verification. in contrast to that work  where there was a direct mapping from the knowledge base to a query containment problem  an added challenge in our context is to develop the translation to containment.  sehmolze and snyder  1  considers the verification problem where rules may have side-effects  but those to not appear in our context.  ronsset  1  proposes an extensional approach to verifying constraints on snapshots of web sites  i.e.  directly on the site graphs . 
   finally  we mention two additional opportunities for new al problems in this context. the first  a generalization of the work we described here  is to specify the structure of web sites at an even higher level. whereas in our work we only checked whether certain integrity constraints hold for a given site definition  there may be cases that we would want to specify only integrity constraints for the site. the system would then consider the constraints and would propose a definition of the structure for the web site. the challenge is to choose among multiple structures that satisfy the given constraints. 
　the second problem concerns automatically restructuring web sites. the short experience in building web sites has already shown that it is a highly iterative process. even after the web site is up  designers will frequently want to restructure it after understanding the patterns with which users browse the site. perkowitz and etzioni  perkowitz and etzioni  1  have proposed the notion of adaptive web sites that restructure themselves automatically. we argue that declarative representations of web sites provide a basis on which to build adaptive web site techniques. in particular  once we have a model of a web site  we can analyze the user browsing patterns and propose meaningful ways to restructure the model  and hence the site itself. 
