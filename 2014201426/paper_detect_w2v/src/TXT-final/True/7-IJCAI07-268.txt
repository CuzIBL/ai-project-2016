
contact center agents typically respond to email queries from customers by selecting predefined answer templates that relate to the questions present in the customer query. in this paper we present a technique to automatically select the answer templates corresponding to a customer query email. given a set of query-response email pairs we find the associations between the actual questions and answers within them and use this information to map future questions to their answer templates. we evaluate the system on a small subset of the publicly available pine-info discussion list email archive and also on actual contact center data comprising customer queries  agent responses and templates.
1 introduction
contact center is a general term for help desks  information lines and customer service centers. many companies today operate contact centers to handle customer queries  where  customers are provided email support from a professional agent. contact centers have become a central focus of most companies as they allow them to be in direct contact with their customers to solve product and services related issues and also for grievance redress. a typical contact center agent handles a few hundred emails a day. typically agents are provided many templates that cover the different topics of the customer emails. when an agent gets a customer query she selects the answer template s  relevant to it and composes a response that is mailed back to the customer. figure 1 shows an example query  response/template pair.
　the work flow for processing emails at contact centers is as follows. when a customer sends in a query by email a human manually triages it and forwards it to the right agent. the agent has a number of predefined response templates from which she selects the most appropriate ones and composes a response email to the customer query by combining the templates and inserting additional text fragments. incoming mails are in free text format with very little structured content in them. usually the structured content are email fields like date  subject line  sender id  etc.  which anyway do not help in triaging the email query or in answering them. therefore  any
please provide the current status of the rebate
reimbursement for my phone purchases.i understand your concern regarding the mail-in rebate. for mail-in rebate reimbursement  please allow 1 weeks to receive it.i understand your concern regarding the mail-in rebate. for mail-in rebate reimbursement  please allow  replace this   ***weeks or months***   /replace this  to receive
it.　figure 1: an example query  response/template pair technique that tries to automatically handle the email queries needs to use natural language processing to handle it.
　today's contact centers handle a wide variety of domains such as computer sales and support  mobile phones and apparel. for each of these domains typical answer templates are defined and provided to the agents to aid them in composing their responses. contact centers have lots of such email data corresponding to each of the domains they handle. the customer queries along with the corresponding agent responses to them are stored and available in abundance.
　in this paper we have presented techniques to automatically answer customer emails by
1. extracting relevant portions from customer queries thatrequire a response
1. automatically matching customer queries to predefinedtemplates to compose a response
　we evaluate the performance of our technique and show that it achieves good accuracy on real life data sets.
1 background and related work
1 key phrase extraction
extracting sentences and phrases that contain important information from a document is called key phrase extraction. key phrase extraction based on learning from a tagged corpus has been widely explored  hirao et al.  1   frank et al.  1 . the problem of extracting key phrases from emails using parts of speech tagging has also been studied  tedmori et al.  1 . we mention this body of work in the context of this paper because in a given email it is important to identify the portions that correspond to questions and answers.
1 text similarity
text similarity has been used in information retrieval to determine documents similar to a query. typically similarity between two text segments is measured based on the number of similar lexical units that occur in both text segments  salton and lisk  1 . similarity measures have been used to find similar documents and sentences  metzler et al.  1 . however  lexical matching methods fail to take into account the semantic similarity of words. hence  i remained silent and i was quiet would not be matched. to overcome this several techniques based on measuring word similarity have been proposed. in their work  wu and palmer  wu and palmer  1  measure the similarity of words based on the depth of the two concepts relative to each other in wordnet 1. in  mihalcea et al.  1  several corpus and knowledge-based measures have been evaluated for measuring similarity based on the semantics of the words. in the context of this paper we need to identify similar questions and we need to match a question to its answer.
1 question answering
question answering  qa  has attracted a lot of research. the annual text retrieval conference  trec  has a qa track where text retrieval systems are evaluated on common  realworld text collections. the aim of a system in the trec qa task is to return answers themselves  rather than documents containing answers  in response to a question  voorhees and dang  1 . in essence what we are attempting to do in this paper is to answer the customer queries by looking at ways in which similar questions have been answered in the past.
1 contact center email processing
a lot of work hasebeenedone on automatic triaging of emails in contact centers  nenkova and bagga  1   busemann et al.  1 . in these a classifier is learnt based on existing cases and using features such as words in the mail  their parts of speech  type of sentence etc. so when new queries come in they are automatically routed to the correct agent. not much work has been done on automatically answering the email queries from customers. it is possible to learn a classifier that map questions to a set of answer templates  scheffer  1 . also there has been some work on automatically learning question-to-answer mappings from training instances  bickel and scheffer  1 . our work in this paper describes methods to automatically answer customer queries by selecting response templates.
1 contributions of this paper
in this paper we present techniques to automatically locate relevant questions in a customer query email and then map each of these to predefined answer templates. by putting together these templates a response email message is composed for the given query email. we automate the process of composing the response. we are not aware of work where a response is generated by putting together existing templates. our technique is novel in that all the questions in the query email are first identified and the associations between the questions and the corresponding answers in the response emails are identified from the given corpus. in this way even multiple questions in a query email will get identified and addressed. our technique shows good accuracy.
1 problem definition
we have a repository of matching query and response emails that have been exchanged between customers and call center agents. the responses have been composed out of a set of templates which have been provided to the agents. a single query may comprise many questions thus requiring multiple templates to be used in answering it. the problem is that of matching the individual questions in the queries to their corresponding templates.
　given the repository of query-response pairs and templates  the first problem is to learn the associations between questions and templates. let {q1 ... qm} be the query emails and {r1 ... rm} be the corresponding responses. a query qi comprises the questions {ql ... qm} which are matched with templates {tl ... tm} used to compose the response ri. the problem can now be defined as follows: given a query email qi we need to find the set of questions  qs  in it and then match these to the corresponding templates  ts  that are used to compose the response ri. now that the associations have been established we can tackle the problem of automatically composing responses for new queries. when a new query comes in  we need to identify the questions in it and then compose a response for it using the mappings that have previously been established.
1 email triaging
in a contact center typically customer query emails are manually triaged. in the triaging step  emails are forwarded to the concerned agent handling the particular class of queries. in some cases a single agent handles all the classes of queries and hence the emails are not actually classified. so the classification information is not available. we replace this step with automatic triaging. since obtaining labelled data for a new process is hard  we use clustering to first identify the different classes and then learn a classifier based on these classes. we classified the emails from the larger pool into an equal number of query and response clusters using text clustering by repeated bisection using cosine similarity. matching query and response clusters could contain different numbers of emails. so  they were subsequently matched based on maximal matching criteria. the cluster having the maximum match fraction was separated and the remaining clusters were rematched. this way a one-to-one mapping of the query and response clusters was obtained. the non-matching emails for each map were removed to create clean clusters having exact correspondence of query-response emails. an svm classifier was then trained on the classes created by the clustering. this classifier is then used to triage the emails.
1 extraction of question-answer pairs
given a query email we would like to extract the key questions in it. then we would like to map these questions to their answers in the response email and determine the templates that have been used.
1 identification of questions and answers
we identify the questions based on the presence of key phrases within them. we follow the approach of  frank et al.  1  to identify the key phrases of length up to 1 words. we use a set of training documents for which key phrases are known to generate a naive bayes model. this model is then used to find key phrases in a new document.
　in actual fact the tagged training documents were not available. so we first extracted the most frequent unigrams  bigrams and trigrams and selected from this list. these extracted phrases of length up to 1 words were then marked in the emails as being the key phrases. this tagged set was used to generate a naive bayes model for the key phrases.
　in a query email the questions are identified as those sentences that contain key phrases in them. similarly in the response email  the replies are identified as those sentences that contain key phrases in them.
1 mapping questions to answers
once the questions and responses have been identified we need to map each question to its corresponding response. a given query email may contain multiple questions and therefore the response may contain multiple answers. to accomplish this mapping we first partition each extracted sentence into its list of tokens. stop words are removed and the remaining words in every transcription are passed through a stemmer  using porter's stemming algorithm 1  to get the root form of every word e.g. call from called. now the similarity between each question and answer is calculated to find the correct map. in order to calculate the overlap between the two sentences  we used word overlap measure along with the score being adjusted to take into account the inverse document frequency  metzler et al.  1 . the measure is given as:

where  n = total number of words  |q ” r| = collection of common words of q and r after stop-word removal and stemming  dfw = document frequency of w  where w belongs to the common word set of q and r
　this gives us a score of similarity between question and answer. in addition  we used a heuristic that if a question is asked in the beginning  then the chances that its response would also be in the beginning are more. so  we biased them with a high weight as compared to those lying in the end which were penalized with a low weight.
so  the expression for score becomes:

where  sim q r  = similarity score as obtained from above  pos q  = position of the question in the set of questions of that query-email  n = number of questions in that queryemail  m = number of answers in that response-email.
　each answer is then mapped to a template. this is done by simply matching the answer with sentences in the templates. the agent typically makes very little changes to the template while composing an answer. as a result of these steps we obtain the question to template pairs.
　multiple questions can match the same template because different customers may ask the same question in different ways. hence  for example  while what is my account balance and can i know the balance in my account can be merged into one sentence  i need to know the carry forward from last month cannot easily be linked with the earlier sentences. hence while the mapping method gave the questionto-template mappings  we need to merge similar sentences and keep the minimal set of questions associated with a template. we prune the set of questions by removing questions that have a very high similarity score between them. so that when a new question comes in  we only need to compare it with this minimal set.
1 answering new questions
when we get a new query we first identify the questions in it by following the method outlined in section 1. each of these questions now needs to be answered. to compose the response email the answers to each of these questions are put together.
1 mapping new questions to existing questions
when a new question comes in we need to determine its similarity to a question we have seen earlier and for which we know the template. from the previous discussion we know that many different questions can map to the same answer template. if we are able to determine that the new question is similar to an existing question then we are done. the new question is mapped to the template for the existing question to which it is similar.
　the similarity between two concepts is given as  wu and palmer  1 :

　where  s and t denote the source and target words being compared. depth s  is the shortest distance from root node to a node s on the taxonomy where the synset of s lies lcs denotes the least common subsubmer of s and t. we are using wordnet.
　further we need to compute the similarity between the concepts present in the sentences that we want to match. the two sentences are first tokenized  stop words are removed and porter stemmed. parts of speech labels are assigned to the tokens. nouns  verbs  adjectives and adverbs were selected. in addition  we also keep the cardinals since numbers also play an important role in the understanding of text. we form 1 matrices  one for each of the five classes. the rows of the matrix are the tokens from one sentence and the columns are the tokens from the second sentence. each entry in the matrix sim s t  denotes the similarity as it has been obtained above for the pair. also  if a word s or t does not exist in the dictionary  then we use the edit distance similarity between the two words.
　the results from these are combined together in the following manner to obtain the overall similarity score between the two sentences
　the similarity between two sentences is determined as follows  mihalcea et al.  1 :

where  simm s qj  is the word in qj that belongs to the same class as s  noun or verb  etc.  and has the highest semantic similarity to the word s in qi.
　using the above similarity we compare the new question with the questions seen earlier. when a match is found we select the response template of the matched question as the template for the new question.
1 evaluation
in this section we present the results over real life data sets.
1 contact center data
we chose two sets of data to work on. the pine-info discussion list web archive 1contains emails of users reporting problems and responses from other users offering solutions and advice. the questions that users ask are about problems they face in using pine. other users offer solutions and advice to these problems. the pine-info dataset is arranged in the form of threads in which users ask questions and replies are made to them. this forms a thread of discussion on a topic. we choose the first email of the thread as query email as it contains the questions asked and the second email as the response as it contains responses to that email. it may not contain answers to all the questions asked as they may be answered in subsequent mails of the thread. we randomly picked a total of 1 query-response pairs from pine-info. the question sentences and answer sentences in these were marked along with the mappings between them. on average  a query email contains 1 questions and the first response email contains 1 answers and there are 1 question-answer pairs. we show a query and response pair from pine-info in figure 1. the actual question and answer that have been marked by hand are shown in bold. in the example shown two questions  two sentences  have been marked in the query email but only one answer is marked in the response email.
　the second data set used was obtained from an actual contact center process relating to mobile phone services. we obtained 1 query and response emails relating to the mobile phone support process. we also obtained the 1 answer templates that the agents are provided to aid them in answering the customer queries. on an average each response contains 1 templates  implying that there are about two key questions per customer query. in figure 1 a query is shown along with its actual response and the template used in composing the response.
i am working on creating an rpm for our university  and i need to put pine.conf into /usr/local/etc/pine.conf instead of /etc/pine.conf. i am wondering if there are any extra parameters that i can pass at build time to redirect pine to /usr/local/etc/pine.conf. if there isn't a parameter that i can pass does anyone know of a patch that would do something like that yes  what i would recommend you is that you take a look at the build script.
the port for redhat  includes a  altdocpaths = 1   which defines a few extra variables  in particular look at the -dsystempinerc= /etc/pine.conf  and so on. that should give you an idea of what to do.figure 1: a typical query  response pair from pine-info
i signed up for wireless web 1 or 1 months ago and have been charged $1 each bill. this time it was $1.
what is the problem i am sorry for the inconvenience caused to you. i have adjusted your account for $1. this will be reflected on your may bill.i am sorry for the inconvenience caused to you. i have adjusted your account for  replace this   ***$insert amount or minutes ***   /replace this . this will be reflected on your  replace this   ***month***   /replace this  bill.figure 1: a typical query  response/template pair
1 measures
we measure the accuracy of the system by comparing the email response generated by a human agent with that generated by our system. we use two criteria for comparison. in the first case  we say that the system is correct only if it generates the exact answer as the agent. in the second case we allow partial correctness. that is if the query contains two questions and the system response matched the agent response in one of them then the system is 1% correct for that query.
1 experiments
in this section we show the results of our techniques both on the pine dataset and the contact center emails. in pine there are no templates. so in effect we are only testing whether we are able to map the manually extracted questions and answers correctly. we used the 1 annotated pairs of query and response emails to measure this. as already mentioned we are looking for an answer in the first response to the query. hence  all questions in the query may not get addressed and it may not be possible to get all the mappings. in table 1 we show the numbers obtained. out of a total of 1 questions only 1 have been mapped to answers in the manual annotation process. using the method presented in section 1 we are able to find 1 of these maps correctly.
　for the contact center emails we tested whether our method could accurately generate the responses for the queries. we built our system using 1 of the 1 query-response pairs of emails available to us. we then tested using the remaining 1 pairs of emails. we ran two sets of experiments on these emails. in the first case we didnt use class information to generate the response emails and in the second case we triaged table 1: results on pine-info dataset for question-answer mapping
totaltotaltotalactualcorrect% correctmailsqnsansmapsmapsmaps111.1%the emails first to aid in the generation of templates.
　in table 1 we show the results for generating the response email without using any class information. hence  in this case  the question-to-template maps are established using the 1 training email pairs. for each template the questions associated with them are found using the techniques of section 1. for new questions in the testing set the response is automatically generated as discussed in section 1. we generated the exact response that the human generated in 1% of the cases. considering partial correctness this accuracy was about 1%.
table 1: results on contact center dataset without triaging
totaltrainingtestingtotal%%emailsetsetcorrectcorrectpartialpairsresp.resp.correct111.1%　for the case of triaging the emails. we first clustered the 1 query and response sets separately. we used cluto package 1 for doing text clustering. we experimented with all the available clustering functions in cluto but no one clustering algorithm consistently outperformed others. also  there was not much difference between various algorithms based on the available goodness metrics. hence  we used the default repeated bisection technique with cosine function as the similarity metric. the clustering results using cluto are shown in table 1. we chose to cluster into 1 clusters because other values for the total number of clusters resulted in lower purity numbers in cluto. we also generated the top descriptive feature for each cluster using cluto and found this to be very close to the actual label for that class which was assigned by a human. we found that the average number of templates varied quite a lot between the classes. many templates were used in two classes or sometimes even more. we used libsvm 1 to train an svm classifier on the clusters that we obtained. we used the rbf kernel with γ = 1 ρ =  1. this classifier was used for triaging the emails. the resulting overall accuracy for the classification was over 1%. in this case  the question-to-template maps are established using the training email pairs for each class separately. for each template the questions associated with them are found using the techniques of section 1. for new questions in the testing set the email is first triaged using the classifier that has been learnt. then the response is automatically generated as discussed in section 1. the questions in this email are only matched to questions from the same class. we generate the exact response that the human generated in table 1: results of clustering contact center dataset
actualdescriptivetotaltotaltemplatesclassclutono.templatesperlabelemailsin classemailrebaterebate11cancellingcancel11chargescharge11accountpayment11payment
general queriesphone11overall11over 1% of the cases. considering partial correctness this accuracy goes up to about 1%. these results are shown in table 1.
　it is seen that there is a marked jump from 1% to 1% in the accuracy numbers for complete match if the emails are triaged. we do not notice such a large jump in the case of partial correctness. the reasons for this could be twofold. firstly  the step of triaging the emails limits the search space over which the questions need to be matched. secondly  if a query lands up in a wrong cluster due to misclassification then it stands no chance of being partially correct because that particular class may not have the template for it.
table 1: results contact center dataset with triaging
clutototalclassification%%classno.accuracycorrectpartiallabelemailsresponsescorrectrebate1.1.1.1cancel1.1.1.1charge1.1.1.1payment1.1.1.1phone1.1.1.1overall1.1.1.1　also it should be mentioned here that often there are questions from customers in reply to which an agent does not use any of the existing templates but actually composes her own reply.
1 conclusions
in this paper we have introduced an automatic email response generation technique. given a corpus of query-response email pairs and answer templates  we determine the typical questions for the given answer templates. when a new query email is presented we identify the questions in it and find the nearest questions to these that had been identified along with their answer templates. the answer templates relating to the questions are put together and composed into a response.
　the accuracies obtained using the methods presented in this paper point to the fact that such a system is practically possible and useful to a contact center agent. when composing a response to a customer query the agent has to select from a few hundred templates. the system presented in this paper helps improve agent efficiency and speed. the templates need some modification and filling in which still has to be done by the agent.
　in future we plan to improve our system to handle questions for which the predefined templates do not work. we would also like to attempt to fill in some of the details in the templates to further aid the agent.
