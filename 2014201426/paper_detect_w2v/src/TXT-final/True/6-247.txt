 
previous algorithms to compute lexical chains suffer either from a lack of accuracy in word sense disambiguation  wsd  or from computational inefficiency. in this paper  we present a new lineartime algorithm for lexical chaining that adopts the assumption of one sense per discourse. our results show an improvement over previous algorithms when evaluated on a wsd task. 
1 introduction 
passages from spoken or written text have a quality of unity that arises in part from the surface properties of the text; syntactic and lexical devices can be used to create a sense of connectedness between sentences  a phenomenon known as textual cohesion  halliday and hasan  1 . of all cohesion devices  lexical cohesion is probably the most amenable to automatic identification  hoey  1 . lexical cohesion arises when words are related semantically  for example in reiteration relations between a term and a synonym or superordinate. 
　lexical chaining is the process of connecting semantically related words  creating a set of chains that represent different threads of cohesion through the text. this intermediate representation of text has been used in many natural language processing applications  including automatic summarization  barzilay and elhadad  1; silber and mccoy  1    information retrieval  al-halimi and kazman  1   intelligent spell checking  hirst and st-onge  1   topic segmentation  kan et al  1   and hypertext construction  green  1 . a first computational model of lexical chains was introduced by hirst and st-onge . this linear-time algorithm  however  suffers from inaccurate wsd  since their greedy strategy immediately disambiguates a word as it is first encountered. later research  barzilay and elhadad  1  significantly alleviated this problem at the cost of a worse running time  quadratic ; computational inefficiency is due to their processing of many possible combinations of word senses in the text in order to decide which assignment is the most likely. more recently  silber and mccoy  presented an efficient linear-time algorithm to compute lexical chains  which models barzilay's approach  but nonetheless has inaccuracies in wsd. 
　in this paper  we further investigate the automatic identification of lexical chains for subsequent use as an intermediate representation of text. in the next section  we propose a new algorithm that runs in linear time and adopts the assumption of one sense per discourse  gale et al.  1. we suggest that separating wsd from the actual chaining of words can increase the quality of chains. in the last section  we present an evaluation of the lexical chaining algorithm proposed in this paper  and compare it against  barzilay and elhadad  1; silber and mccoy  1  for the task of wsd. this evaluation shows that our algorithm performs significantly better than the other two. 
1 lexical chaining with a word sense disambiguation methodology 
our algorithm uses wordnet  miller  1  as a knowledge source to build chains of candidate words  nouns  that are related semantically. we assign a weight to each semantic relation; in our work semantic relations are restricted to hypernyms/hyponyms  e.g. between cat and feline  and siblings  hyponyms of hypernyms  e.g. dog and wolf . distance factors for each type of semantic relation prevent the linkage of words that are too far apart in the text; these factors are summarized in table 1. 
　the algorithm can be decomposed into three steps: building a representation of all possible interpretations of the text  disambiguating all words  and finally building the lexical chains. first  similarly to  silber and mccoy  1   we process the whole text and collect all semantic relations between candidate words under any of their respective senses. no disambiguation is done at this point; the only purpose is to build a representation used in the next stages of the algorithm. note that this is the only stage where the text is read; all later stages work on this implicit representation of possible interpretations  called a disambiguation graph  figure 1 . in this kind of graph  nodes represent word instances and weighted edges represent semantic relations. since wordnet doesn't relate words but senses  each node is divided into as many senses as the noun has  and each edge connects exactly two noun senses. 
　this representation can be built in linear time by first building an array indexed by senses of wordnet and processing a text sequentially  inserting a copy of each candidate word into 

1 	poster papers 

semantic relation 1 sent. 1 sent. 1 par. other synonym 
hypernym/hyponym sibling 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 table 1: weights of relations depending on the kind of semantic relationship and distance  in number of sentences or paragraphs  between two words. 

figure 1: a disambiguation graph  an implicit representation of word-sense combinations  in this example  all weights are equal to 1.  
all entries that are valid senses of this word  for example  in figure 1  the instances car and auto have been inserted under the same sense in the array . then  wc check whether the noun instance that was just inserted is semantically related to other nouns already present in the array. we do so by looking at hypernyms  hyponyms  and siblings  and if any of these related senses have non-empty entries in the array  we create the appropriate links in the disambiguation graph. for example  in figure 1  the algorithm finds an hyponymy relation between the noun taxi  under its unique sense in the array  and another entry in the array containing car and auto  so semantic links are added to the disambiguation graph between these two words and taxi  shown here attached to the array . processing all nouns this way  we can create all semantic links in the disambiguation graph in time 1{n   where n is the number of candidate words.  
　in the second step  we use the disambiguation graph to perform wsd  enforcing the constraint of one sense per 

figure 1: first pass of the algorithm: using an array  we can build the disambiguation graph in linear time. 
discourse. we perform the disambiguation of every word  instead of disambiguating word occurrences as in e.g.  hirst and st-onge  1; silber and mccoy  1 . we process all occurrences  nodes  of one word at a time  and sum the weight of all edges leaving these nodes under their different senses. the one sense of the word that is assigned the highest score  sum of weights  is considered the most probable sense. for example in figure 1  the sense of bank that has the highest score is financial institution. that sense is assigned to all occurrences of the word; in other words  we impose the constraint of one sense per discourse. in case of a tie between two or more senses  we select the one sense that comes first in wordnet  since wordnet orders the senses of a word by decreasing order of frequency. 
　the final step is to build the actual lexical chains by processing the entire disambiguation graph. at this point  we have already assigned a sense to each word  so the last step is to remove from the disambiguation graph all semantic links that connect words taken under their  assumed  wrong senses. once all such edges have been removed  we are left with the semantic links corresponding to a unique interpretation of the text  and the edges that remain in the graph are the actual lexical chains of our algorithm.l 
   the separation of wsd and lexical chaining into two different sub-tasks is important. all semantic relations  whether correct or incorrect  can be investigated in wsd without necessarily creating incorrect semantic relations in the chaining process. words are disambiguated by summing weights of semantic relations  but mistakenly counting edges relating words under wrong senses  as in figure 1 between fall and bank  doesn't necessarily have the undesirable effect of linking the two words in the same chain. our assumption is that summing edge weights generally helps in selecting the right senses  e.g. bank is disambiguated as a financial institution  'and fall and bank are thus prevented from appearing in the same chain. 
1 	evaluation 
the evaluation of lexical chains is generally difficult. even if they can be effectively used in many practical applications like automatic summarization  topic segmentation  and others  lexical chains are seldom desirable outputs in a realworld application  and it is unclear how to assess their quality independently of the underlying application in which they are used. for example  in summarization  it is hard to determine whether a good or bad performance comes from the efficiency of the lexical chaining algorithm or from the appropriateness of using lexical chains in that kind of application. in this section  we evaluate lexical chaining algorithms on the basis of wsd. this arguably is independent of any targeted 
   'our algorithm has some similarities with silber and mccoy's algorithm  but it is actually quite different. first  they process each noun instance separately; thus  nothing prevents a noun from having different senses in the same discourse. second  they process the entire text twice instead of once. in the second pass of their algorithm  they perform wsd and the actual chaining at the same time  whereas we postpone the chaining process until each word has been fully disambiguated. 

poster papers 	1 

algorithm accuracy barzilay and elhadad 
silber and mccoy 
galley and mckeown 1% 
1% 
1% table 1: accuracy of noun disambiguation on semcor. 
application  since any lexical chaining algorithm has to deal with the problem of wsd. we do not attempt to further evaluate other aspects of chains. 
   we tested three lexical chaining algorithms on the semantic concordance corpus  semcor   a corpus that was extracted from the brown corpus and semantically tagged with wordnet senses. we limited our evaluation to a set of 1 documents of that corpus; this represents about 1 nouns. wsd was evaluated on nouns  since all three algorithms that were tested  barzilay and elhadad; silber and mccoy  and ours  build lexical chains with nouns only. we used the original implementation of barzilay and elhadad's algorithm  but had to implement silber and mccoy's algorithm since we didn't have access to their source code. we tested the accuracy of wsd on the set of 1 nouns and obtained the results presented in table 1 accuracy by polysemy is displayed in figure 1. we can see that our algorithm outperforms barzilay and elhadad's  and a one-sided t-test1 of the null hypothesis of equal means shows significance at the .1 level  p = 1   1 -1 . barzilay and elhadad in turn outperform silber and mccoy  but this result is not significant at the basic .1 level  p = 1 . 
1 	conclusions 
in this paper  we presented an efficient linear-time algorithm to build lexical chains  showing that one sense per discourse can improve performance. we explained how the separation of wsd from the construction of the chains enables a simplification of the task and improves running time. the evaluation of our algorithm against two known lexical chaining algorithms shows that our algorithm is more accurate when it chooses the senses of nouns to include in lexical chains. the implementation of our algorithm is freely available for educational or research purposes at http://www.cs.columbia.edurgalley/research.html. 
acknowledgments 
we thank regina barzilay  the three anonymous reviewers  and the columbia natural language group members for helpful advice and comments. 
1
　　in barzilay and elhadad's algorithm  a word can sometimes belong to two different chains. in order to map each word to one single sense  we applied the strong chain sense disambiguation strategy described in  barzilay  1   i.e. picking the word sense used in the strongest chain . 
1
　　the samples in the t-test arc the wsd accuracies on each individual documents. 

figure 1: accuracy by polysemy of the three algorithms. 
