 
　　nearly all of the proposed parallel architectures for artificial intelligence applications use a multiple instruction  multiple data stream  mimd  approach. while this approach offers the greatest opportunity for ultimate exploitation of parallelism  it has been difficult to actually achieve a high level of parallelism in practice because most of the existing algorithms require a higher interprocessor communications bandwidth than the hardware can achieve. 
　　an alternate approach to parallelism is the use of single instruction  multiple data stream parallelism  simd  machines. while simd machines do not offer the same ultimate exploitation of parallelism as mimd architectures  they may  in fact  provide more useable parallelism. this is because they can be treated as serial machines with very long data words  so that existing algorithms may be more readily adapted in ways which better use available parallelism. we illustrate this concept with the cellular array processor  cap  being developed at the itt 
advanced technology center. this simd architecture is based on a rectangular processing array which accesses a single memory and which has very high speed data paths among the processing elements. we discuss the implementation and manipulation of data for two applications on the cap; the ops1 production system interpreter and a representation of an associative network. the expected performance of the cap in these applications will also be presented. 
introduction 
　　recently  work on more powerful hardware and architectures for artificial intelligence applications has focused on parallelism as a solution to the problem of the looming limits to increased circuit speed in monoprocessor architectures  deering  1 . while there are a great many ways in which parallelism as a concept can be applied to architectures  the variety of parallelism that has attracted the predominant attention of the artificial intelligence community has been one in which multiple processors are each executing their own instruction stream on their own set of data. among the designs based on this approach are c.mmp  wulf and bell  1  and the dado architecture 
 stolfo  1 . 
　　the major problem that these designs present is one of memory access. since memory speed is as big a bottleneck as processor speed  designs in which all processors share a common memory offer little opportunity for parallelism. consequently  current work on parallelism is focusing on distributing the computation among processing elements in such a way that each processor operates on its own set of data. work on parallel inference systems  for example  has attempted to develop algorithms such that all the information needed to infer each  single clause is on one processor.  this may involve copies of some of the information . a conjunction or disjunction is then computed by having each processor work separately on one of the clauses and combining the results. such  divide and combine  approaches are also the basis for parallelism in other paradigms  such as the one used by stolfo et. al. for ops1  stolfo  1 . 
　　while such algorithms can achieve a high degree of parallelism when they succeed  they do not work universally. frequently  there is a trade-off between processor utilisation and communication time. solutions that maximize processor utilization often result in slower overall computation time because of the time taken to communicate information - both instructions and data among processors. in the worst case  the performance is inferior to that of single processor systems. increasing the communication bandwidth is rarely cost effective and  ultimately  will run up against the same hardware factors which limit memory bandwidth in single processor designs. 
　　while high performance multiple instruction  multiple data stream  mimd  solutions may ultimately be found for many significant problems  equally high performance may be obtainable from architectures which trade reduced information communication requirements against a reduced degree of parallelism. we have been working with one such architecture  an simd  single instruction  multiple data 
　　
1 	r. brooks and r. lum 

figure 1. 
　　
stream  machine  which shows considerable potential for running some of the algorithms important to work in artificial intelligence. 
the cellular array processor 
architecture 
　　the cellular array processor  cap  is a general purpose simd computer. the overall architecture of the cap is shown in figure 1. originally  this architecture was designed for large scale numerical operations  particularly those involving floating point and vector operations. adapting it to artificial intelligence algorithms was facilitated by a change to the memory access structure. 
　　the heart of the cap is the associative array  composed of a square array of processing elements. each element is a one-bit  bit-slice* processor. the number of elements in the array can be optimized for particular applications in multiples of 1; thus  arrays can be built which are 1  1 or larger. 
　　within the array  the processing elements are conceptually organized into rows; each row behaves like a conventional processor with three exceptions: first  all rows execute the same instruction at the same time; this is accomplished by keeping all of the instructions in a single program memory and sending the instruction to all processors at once. a masking mechanism can be used to disable some of the rows or parts of the rows. 
　　second  there are instructions for data transfer between processors; for example  it is possible to instruct all processors to transfer the contents of one of their registers into a register of the row next to them. it is also possible to broadcast the contents of a register in one row to all of the other rows. 
　　third  operations can be performed in parallel on parts of a row. for example  a 1 bit row can perform two  simultaneous 1 bit integer adds on each half of a row. moreover  by setting most-significant and leastsignificant bits at different points in the row  it is possible to work with arbitrary row partitions. thus  a 1 bit row could be partitioned into two 1 bit pieces  and a 1 bit piece.  there are some restrictions on partitioning across rows and on minimum partition sizes.  
　　as with conventional processors  each row has its own memory. thus  there is effectively a stack of memory planes beneath the plane of processors. the amount of memory addressable by each plane is implementation dependent; the initial version  with 1 bit rows  will allow for 1k bytes per row  for an overall memory of 1m bytes. 
comparision to other s i m d architectures 
　　other simd machines  kuhn & padua  1  have been typically composed of complete processor units with small amounts of primary memory. communication among processors is via a common bus of limited bandwidth. the problem is partitioned so that each processor executes the same instruction stream but on different parts of the problem  or in the case of production systems  different rules . 
　　if the problem is not partitioned properly or if only part of the problem is executable at any one time  then some of the processors will have to wait until other processors have finished execution. this frequently prevents fully utilizing the speed gained in the parallel implementation of instructions. 
　　the cap  on the other hand  has a large primary memory for each processing row. broadcast of data to the rows takes place at instruction execution speed. since the data is organized along rows rather than into the depth of the memory  it is easier to allocate the data more evenly among the processors. further  since instruction decoding is done once for the entire array  there is no need to replicate instruction memories. this combination increases the likelihood that the processing array can be kept fully occupied more of the time than has been the case with past simd architectures. 
implementation 
　　the current implementation of the cap architecture is a vlsi chip set consisting of a processor chip  a controller chip  a barrel-shifter chip  a communications processor chip and an input/output processor chip. each array chip contains 1 one-bit cells. the chip is a custom design currently fabricated in 1-micron  n-well  double metal cmos. it contains approximately 1 transistors  and has an estimated die size of 1 mil. die  with 1 i/o pins. 
　　the array chip has been designed to have the following attributes: 
  highly regular structure 
  fixed and floating point arithmetic 
  logic  branching and masking instructions 
  1 mhz sixteen bit addition 
  1 mhz input/output shift rate 
in addition  each cell has it own control logic  
alu  a set of single bit registers  and simple i/o processor for external device dma to off-chip ram. the amount of off-chip ram is abo variable with typical sizes being 1k  1k  and 1k. the total available primary memory for the array is then n x 1 x m; where n is the number of chips times 1  the number of cells on each chip  times m  the amount of memory attached to each cell. 
	r. brooks and r. lum 	1 
applications 
　　to illustrate how the cap architecture can be used for artificial intelligence applications  we show how the cap can be used to implement some typical algorithms used in artificial intelligence work: the ops1/rete algorithm and association network algorithms. these will be discussed in terms of implementation and performance on the cap. 
o p s 1 / r e t e algorithm 
　　the rete algorithm for matching the left-hand sides of rules in the ops1 forward-chaining production system language is one of the few  well analyzed computations in artificial intelligence  forgy  gupta  newell & wedig  1 . forgy et. al. describe this algorithm as follows: 
　　＊the rete interpreter processes the left-hand sides of the productions prior to executing the system. it compiles the left-hand sides into a network that specifies the computations that the matcher has to perform in order to affect the mapping from changes in working memory to changes in the conflict set. the network is a dataflow graph. the input network consists of changes to working memory encoded in data structures called tokens. other tokens output from the network specify changes that must be made to the conflict set. as the tokens flow through the network  they activate the nodes  causing them to perform the necessary operations  creating new tokens that pass on to subsequent nodes in the network. the network contains essentially four kinds of nodes: 
　　  constant-test nodes: these nodes test constant features of working memory elements. they effectively implement a sorting network and process each element added to or deleted from working memory to determine which conditions the element matches. 
　　  memory nodes: these nodes maintain the matcher's state. they store lists of tokens that match individual conditions or groups of conditions. 
　　  two-input nodes: these nodes access the information stored by the memory nodes to determine whether groups of conditions are satisfied. for example  a two input node might access the lists of tokens that have been determined to match two conditions of some production individually and determine whether there are any pairs of tokens that match the two conditions together. in general  not all pairs will match because the left-hand side may specify constraints such as consistancy of variable bindings that have to hold between the two 
　　
1 	r. brooks and r. lum 
conditions. when a two-input node finds two tokens that match simultaneously  it builds a larger token that indicates that fact and passes it to subsequent nodes in the network. 
　　  terminal nodes. terminal nodes are concerned with changes to the conflict set. when one of these nodes is activated  it adds a production to or removes a production from the conflict set. the processing performed by the other nodes insures that these nodes are activated only when conflict set changes are required. ＊ 
in their analysis of several  large ops1 systems  
forgy and his colleagues conclude that the match process is the biggest component of ops1 run times  since it occurs every time a rule action takes place. further  processing the memory nodes and the twoinput nodes takes the largest portion of match time. they also point out that most changes are localized  with only a small proportion of nodes being activated for any rule action. 
c a p implementation of the rete a l g o r i t h m . 
the rete algorithm can be implemented on the 
cap in a form which utilizes the cap parallelism effectively. for purposes of describing this implementation  we will assume a 1 cap array  although the approach will work the same way for other array sizes. 
constant-test nodes. 
　　these can be handled in two different ways  depending on the characteristics of the tests that are needed. 
many values for a single attribute. 
　　here  the situation is one in which many rules test the same attribute of a particular working memory element  but where each rule matches a different value  or  alternately  there are many different classes of working memory element and different rules match different classes. in the cap  this is handled by testing the incoming token against many possible alternatives simultaneously. assuming that all of the alternative values are stored in one or more planes of the cap memory  the operations are as follows: 
　　1. broadcast the value from the incoming token to all rows of the cap and store it in a register. 
　　1. load the plane of constants into a cap register in each row. 
　　if there are 1 or fewer alternatives to be tested  then all of the tests can be made in one pass through these steps. if there are more  then steps 1 and 1 can be repeated for additional planes of alternatives. 
many constant values in a single lhs pattern. 
　　sometimes there will be many constant tests to be made on a single working memory element; this would occur if the working memory elements were very long with many attributes. in this situation  there is an alternative set of cap operations. to use them  all of the attribute values that belong to one left-hand side element are placed in the same planes of cap memory. the operations are then the same as those given above. 
　　both of these methods can be combined in a single search tree. thus  it may be desirable to test the classes of all of the elements at the top of the tree but then switch to testing multiple attributes within an element further down in the tree. 
m e m o r y and two-way nodes. 
　　in these nodes  the problem is one of testing an incoming token against the set of tokens that have arrived at the node earlier and are being stored there. in the cap implementation  the node memory will be set up in the following manner: 
　　1. if the tokens are smaller than the row length of the cap array  they will be placed contiguously within a plane of cap memory. if the memory is large  then multiple planes of memory may be used  but the planes need not be contiguous. 
　　1. if the tokens are larger than a row of cap array  the tokens will be split across multiple planes of memory  but each part of a token will occupy the same relative position within its plane. 
with this arrangement  the following sequence of 
cap operations can be used to test an incoming token against the tokens which are already in a node memory: 
　　1. load the first part of the incoming token into the vertical data register. 
　　1. broadcast this value to all rows of the cap and store it in a register. 
　　1. load the plane containing the first part of the tokens stored at the node into a cap register in each row. 
　　
1. compare the two registers in each row. 	1. compare the two registers in each row. 
　　
　　1. if there are no more parts  the matching rows are the memory tokens which match. 
　　1. if there are more parts  mask out the rows which did not match in the previous steps  and repeat the load and compare operations with successive parts. 
　　1. repeat steps 1 through 1 for all of the planes containing additional stored tokens. 
expected performance 
　　the cap implementation will affect the performance of the rete match algorithm in two ways: first  it will effectively reduce the number of constant nodes in the tree. currently  the number of constant nodes in the tree is proportionate to the product of the number of different attributes and the number of different values those attributes can take on. in the cap implementation  it will be proportionate only to the number of different attributes  provided the number of attributes is less than the row size of the cap. 
　　second  and  perhaps  most importantly  there will be a speed up in the performance of two-input nodes. since the time in processing these nodes is spent comparing an incoming token to the tokens already stored at the node  the ability of the cap to do multiple comparisons simultaneously could significantly improve performance. the amount of performance improvement will depend on the average number of tokens being stored in the node memories and will increase as the number increases. a factor equal to the number of rows of the cap will be the upper bound; thus  a 1 row cap will yield a maximum of a 1 fold improvement. 
　　overall  maximum parallelism will be achieved when the number of memory nodes is large and these nodes are filled much of the time and when the number of one-input  constant  nodes is large. worst case performance will occur when the number of oneinput nodes is small and there are many two-input nodes with no memories. this would be the situation if no variables were used on the left-hand sides of rules  but the left-hand sides had a large number of conditions. 
comparision to other ops1 architectures 
　　at least two other architectures have been proposed for forward production systems  particularly ops1.  the psm group  forgy et. al.  1  architecture is still in the the design analysis stage  so that no figures on the implementation are yet available.  the dado architecture has been well described and several different algorithms have been 
	r. brooks and r. lum 	1 
proposed for it  so that a preliminary comparision is possible. 
　　the dado architecture is based on a relatively large number of processors  such as 1  connected in a binary tree architecture  stolfo  1 . each processor is currently implemented using a commercially available processor  a modest amount of local memory  1k bytes  and a semi-custom i/o switch. since each of the processors operates in a conventional  serial manner  the system relies for speed improvement on distributing the computation out among the processors. 
　　if each of the 1 processors has an 1 bit word width  and if maximum parallelism is achieved  then the maximum processing capacity will be 1 bytes at the instruction execution rate of the individual processors. a 1 row by 1 bit cap array  running at higher clock rate  offers equivalent maximum processing capability with many fewer components. one important issue  then  is the extent to which the parallelism is useable. 
　　as was noted earlier  the cap will achieve best performance with large average node memory sizes. forgy et. al.  1  point out that this is the situation in the large ops1 systems they analyzed. the dado algorithm that most closely corresponds to this situation is algorithm 1  in which leaves of the tree contain one input tests  and the results of the test are broadcast up the tree. this means that  in general  fewer than 1 processors will be available for simultaneous work on two input nodes  that is  on any instruction cycle  only 1 bytes of data bandwidth will be available. in contrast  the full 1 bytes of cap bandwidth will be useable on these tests. hence  the cap should achieve roughly twice the performance of the dado machine. 
　　another difference will be in the memory available for large working memories or numbers of rules. the current dado design partitions the memory into relatively small units of 1k per processor. if a single node memory becomes very large  it may exceed the storage capacity of an individual processor  while still using only a small fraction of the overall memory of the machine. in the cap  in contrast  the memory behind each row is relatively large - 1k for the initial implementation   and it can be dynamically allocated among the different node memories. thus  the cap is less likely to run into memory allocation problems on large ops1 systems. 
association networks 
　　association networks  also called semantic networks  are one of the most widely investigated 
　　
1 	r. brooks and r. lum 
approaches to knowledge representation in artificial intelligence. a wide diversity of approaches and systems exist  findler  1 ; in some  for example  links between nodes indicate relationships among nodes; in others  links indicate positions in logical predicates. at the implementation level  though  nearly all of the formalisms use the following operations: 
　　1. find all of the node s  reacheable from a given node using a specified set of links or link types. 
　　1. find a path between a pair of nodes. 
　　there may be restrictions on the length of the paths and/or on the types of links used. 
　　in general  the links are stored as lists of pointers such as lisp property lists. consequently  the time to perform the operations tends to be heavily dependent of the total number of links emanating from a node since it may be necessary to traverse the entire list to find a link of a particular type. 
　　the cap can also be used to process association networks much faster than can be done with conventional serial processors. the approach that is used is to take advantage of the cap's ability to simultaneously perform operations on individual bits. instead of representing the links in the network as pointers  they are stored as bit matrices  one matrix for each type of link. each node in the network is represented by a row and a column in each matrix  and a 1 in a row-column intersection indicates that a link of the given type exists for the two corresponding objects. 
　　figure 1 gives an example of such a matrix. the relationships are predefined in the vertical and horizontal directions and the axis in the x and y directions represent the nodes of the list. in this example  the vertical or columnwise relationship represent the children of the member in the horizontal axis. a 1 in the searched column will yield all children of a certain parent. this axis can also be discriminated further by male/female categories whereby son/daughter information can be ascertained. the horizontal or row-wise relationship was defined as the parents of the searched person whose identity belongs to that row. 
　　from this simple example  it can be seen that this type of representation also commends itself to depth or iterative searches. by recursively tracing links of previous links  ancestral information for x number of generations can be found. family trees or paths may be traced and on either parent's lineage. in addition  

the search space for any row or column is restricted to only the pages which fall in the row or column of the searched individual. for example  if the query is to find all the children of namex and namex is on the fourth page  1th column  figure 1   then only the pages in the shaded area need be searched. because the matrices are sparse  it is important to be able to determine quickly if a column is blank. 
expected performance. 
　　assuming a 1 cap configuration  the cap will be able to simultaneously search 1 elements of a row or column in a single instruction sequence. thus  it will offer a substantial performance improvement over uniprocessor architectures that test one element per instruction sequence. 
comparision to other architectures. 
　　at least two other architectures have been proposed for similar tasks  the connection machine  hillis  1  and snap  dixit & moldovan  1 . both of these are vlsi implementations of associative memory architectures in which there is a very large array of processing elements with nearest neighbor interconnections. in both designs  the array is driven by an external processor which uses the array for data storage and retrieval. 
　　in these designs  there is a mapping from nodes in the association network to individual processing elements. in the connection machine  in which processing elements have 1 connections to their neighbors  a node is represented by a tree of processing elements in which the links in the tree are implemented as addresses stored in state vector of the processing element. in snap  the cells have a fourway connection and a content addressable memory that is used to store links; each processing element represents one node. 
　　the connection machine is partially an simd design  since all cells share the same rule table. though it is not as clear from the published descriptions  it appears that all snap cells also execute the same instructions. in contrast to the cap 
　　
r. brooks and r. lum 	1 

　　
though  the individual cells may operate asynchonously in both designs. thus  they may be simultaneously operating on different parts of an association network. 
the performance of these designs relative to the 
cap will depend on several factors: the first is overall array size. since these machines are driven by external processors  loading the arrays will depend on the speed of these processors. if the size of the network exceeds the size of the array  the effective processing speed may be considerably reduced. the cost effectiveness of array size will  in turn  depend on the effectiveness of the vlsi implementation. since the cap architecture uses conventional ram  the ability to directly access large networks may compensate for the reduced number of active processing elements. 
　　a second important factor will be the structure of the particular network and its effect on array congestion. if the network is partitioned into largely independent graphs with only a few nodes connecting the areas  then these connecting nodes can effectively be a bottleneck to many processing array operations and greatly reduce the effective parallelism. 
　　third  the connection machine and snap designs are only intended to search and manipulate links among nodes. any processing done on the nodes themselves are  again  handled by an external processor. in contrast  the cap is capable of a wide variety of arithmetic and logical operations  so that no interaction with an external processor is needed. 
conclusions 
　　the previous analyses have been intended to show that the cap is  at least  a competitive architecture for two  important artificial intelligence algorithms. other architectures for the same algorithms all have significantly more processing power when measured in terms of overall processing bandwidth  but potentially suffer from problems of bandwidth utilization caused by restrictions on common access to data. in contrast  the large memory of the cap and the large bandwidth of access to this memory may more than compensate for the reduced opportunities for parallelism. 
o h   yes  it does do floating point. 
　　finally  in making these analyses  we have deliberately understressed the cap's ability to do high speed arithmetic processing. for byte-wise integer operations  such as might be involved in picture processing  a 1 array could achieve a speed of over 1 mops. for floating point operations  the speed approaches 1 mflops. the ability to perform these operations on the same processor with high speed rule firing or fast association network searching might be very valuable in intelligent signal processing tasks such as scene analysis or speech recognition. 
