
we show that a simple procedure based on maximizing the number of informative content-words can produce some of the best reported results for multi-document summarization. we first assign a score to each term in the document cluster  using only frequency and position information  and then we find the set of sentences in the document cluster that maximizes the sum of these scores  subject to length constraints. our overall results are the best reported on the duc-1 summarization task for the rouge-1 score  and are the best  but not statistically significantly different from the best system in mse-1. our system is also substantially simpler than the previous best system.
1 introduction
in this paper  we show that a very simple approach to generic multi-document summarization can lead to some of the best reported results on this task. our system has two components: one component uses machine learning to compute scores for each word in the set of documents to be summarized  which is called the  document cluster  ; the other component uses a search algorithm to find the best set of sentences from the document cluster for maximizing the scores. despite the simplicity of the techniques  our results are among the best ever reported for multi-document summarization.
　multi-document summarization is an increasingly important task: as document collections grow larger  there is a greater need to summarize these documents to help users quickly find either the most important information overall  generic summarization  or the most relevant information to the user  topic-focused summarization . examples where multi-document summarization might be helpful include news  email threads  blogs  reviews  and search results.
　the design of our system is motivated by sumbasic  nenkova and vanderwende  1; nenkova et al.  1 . sumbasic is extremely simple  but its performance is within statistical noise of the best system of duc-1 and the second best system of mse-1  using the rouge-1 measure. sumbasic first computes the probability of each contentword  i.e.  verbs  nouns  adjectives and numbers  by simply counting its frequency in the document set. each sentence is scored as the average of the probabilities of the words in it. the summary is then generated through a simple greedy search algorithm: it iteratively selects the sentence with the highest-scoringcontent-word breaking ties by using the average score of the sentences. this continues until the maximum summary length has been reached. in order not to select the same or similar sentence multiple times  sumbasic updates probabilities of the words in the selected sentence by squaring them  modeling the likelihood of a word occurring twice in a summary.
　our system improves on sumbasic in three ways. first  in contrast to sumbasic which only uses frequency information  our approach also uses position information. second  we introduce a discriminative  machine-learning based algorithm to combine these information sources. third  instead of applying the heuristic greedy search of sumbasic  we formalize the content selection process as an optimization problem. specifically  we use a stack-decoding algorithm to find the summary that maximize the total scores of the contentwords it has  subject to the summary length constraint. each of these three improvements is empirically shown to lead to better summaries. this system achieves the best reported rouge-1 results for the duc-1 summarization task as well as for mse-1  although the difference is statistically significant only for duc 1.  another recent system  wan and yang  1  also reports excellent results on the duc1 task  but uses a different version of the rouge software  making comparisons difficult. 
　the rest of this paper is organized as follows. in section 1 we describe how we determined the importance of position information  and then we describe several term scoring mechanisms. next  in section 1  we give details of our optimization technique  including the stack decoder. in section 1  we give experimental results showing that both our new scoring method and our optimization technique lead to improvements  and that the combined system outperforms the best previous system on the rouge-1 metric. we then compare our system to related work in section 1  and finally section 1 concludes the paper.
1 scoring
in this section  we describe our techniques for scoring words. our starting point  sumbasic  scores words purely using frequency information. in section 1  we investigate additional possible features by looking for mismatches between human and machine-generated summaries. we identify position information as a key additional feature. then  in section 1  we examine two different ways to combine position information with frequency information  one of which can be described as generative  and the other of which as discriminative.
1 position and frequency features
as mentioned previously  exclusively using word frequency information  sumbasic can produce quite competitive multidocument summaries  nenkova et al.  1 . they focused on frequency information after comparing machine to human generated summaries  and finding that machine-generated summaries contained fewer frequent words than human generated ones. we wanted to build on this work by identifying other information sources that could be useful for a summarization system  in addition to word frequencies. in particular  we examined the summaries generated by sumbasic and human summaries written for the same documentset  and checked whether these two types of summaries had different properties. for example  if the number of capitalized words was substantially higher  or lower  in human summaries than in machine generated summaries  we would expect that capitalization information could help improve the automatic system. we looked at a number of different properties  including capitalization  word length  sentence length  and others. we found that the human and machine summaries had comparable values for all properties except one - the word positions.
　position information has been used quite frequently in single-document summarization. indeed  a simple baseline system that takes the first l sentences as the summary outperforms most summarization systems in the annual duc evaluation  barzilay and lee  1   and see also  zajic et al.  1  for use of position in scoring candidate summary sentences . for multi-document summarization in duc  position information has also been used as part of simple baseline systems: two baselines were constructed for multidocument summarization  both informed by position. one system took the first n words of the most recent news document in the collection as the baseline  while the other system was constructed by appending the first sentences of subsequent articles in the document cluster until the length limit was reached.
　in mutli-document summarization  various systems have used sentence position as a feature in scoring candidate sentences  e.g.   radev et al.  1; zajic et al.  1    but word position has not been explored thus far. at the level of words  some systems have used as a feature the number of words in the candidate sentence that are found to be signature tokens  i.e.  words found to be frequent near the beginning of an article and therefore likely to be highly informative  lin and hovy  1; mckeown et al.  1; schiffman  1; conroy et al.  1 . however  these signature tokens are computed based on a large corpus of news articles  not based on the word position in a small cluster of articles.
　to check for the importance of word position information in a given cluster of documents  we first needed to define a position measure. our procedure was as follows: for each word occurrence in a document cluster  we computed its position relative to the beginning of its document  e.g. 1 for the first word and 1 for the last. for each word  we then computed the average of its occurrences throughout the document cluster. we call this the average position of the word. a very frequent word occurring randomly throughout the documents would have an average position of about 1  but words that occur disproportionately at the beginnings of documents have an average position below 1.
　in particular  we compute the average position in the original document cluster for the terms in the summary. for human summaries  the value is about 1. for term frequencybased summaries generated by sumbasic  the value is around 1. compared to the value computed from the document cluster  which is 1  this fact re-confirms the importance of position. of course  a difference of .1 may or may not be important in practice: later  in section 1  we will give experimental results showing that our inclusion of this information does indeed lead to statistically significant improvements.
1 scoring the terms
knowing that frequency and location are important  the next question is how to incorporate them together to create a good scoring function for each term. in this section  we explore two different approaches: generative and discriminative.
generative scoring
the term scoring method used in sumbasic can be thought of as a generative model based on frequency alone. in this model  we assume that summaries are generated at random from the documentcluster accordingto the followingprocess. we select a term at random from the document cluster  with a probability proportional to the frequency of the word in the cluster. we add this term to the summary  and then delete all occurrences of the term from the cluster. we repeat this process until we have generated a summary of the required length. of course  a random set of words would be a horrible summary. in practice  the output summary consists of sentences with the highest probability according to the model  selected from the document cluster. in section 1  we describe in detail how we use these scores to create summaries.
　in addition to the abovemodel based only on frequency we also propose a generative model that favors words from the beginnings of documents. we tried additional experiments in which  instead of selecting words at random from the whole document  we select terms only from the very beginning of the document. for our experiments with duc-1  we used the first 1 words; for mse-1 where many of the articles were very short  we used the first 1 words.
　our final generative model combines the above two models  allowing a tradeoff between overall frequency  and frequency at the beginning of documents. this model assumes that the first step is to flip a biased coin. based on the biased coin flip  we either select our terms at random from the whole document cluster  or only from the beginnings of the document clusters. the effective probability value is therefore a linear interpolation of the values used in the two base models. after trying different bias terms  we found that setting it to 1 worked best empirically.
discriminative scoring
often  it has been found that for probabilistic approaches to natural language processing  discriminative approaches work better than generative ones. we train our discriminative model using the data for the duc-1 multi-documentsummarization task. that is  we learn the probability that a given term in the document will be in the summary. for each content word in a document cluster  we assign label 1 to it if it appears in the given human summary; otherwise  the label is 1. we then try to learn the probability that the term has label 1  given its features.
　the learning algorithm we chose is logistic regression  for two reasons. first  logistic regression predicts probabilities directly  in contrast to  say  perceptron or svms . second  logistic regression works well even with inconsistent labels  in contrast  to  say  large margin approaches like svms which can find such data difficult to train on . this is importantsince we have four different human summaries for each document cluster. when a term in the document cluster only appears in  say  three summaries  the training data will have four identical feature vectors representing this term. three of them will be labeled 1 and the other one will be labeled 1.
　we created 1 basic features using the frequency and position information. they are: #occr  the number of the occurrences in the document cluster   occrratio  #occr divided by the number of the content-words in the document cluster   avgoccrratio  calculate occrratio for each document in the cluster  and average them   minpos  find the position  starting from 1  where the content-word appears first in each document in the cluster  and use the smallest   avgminpos  similar to minpos  but use the average instead; if the contentword does not appear in a document  the position is the total number of content-words in this document   minrelpos  compute the relative first position  which is the minpos divided by the number of content-words in the document  and return the smallest of the cluster   and avgrelpos  similar to minrelpos  but use the avearge instead .
　for each of the basic features  we also create a corresponding log feature. suppose the basic feature is x; then the corresponding log feature is log 1 + x . we then expand the feature space by introducing conjunction features. for each pair of the above features  we use the product of the values as a new feature  which has a similar effect to using a degree-1 polynomial kernel.
1 optimization method
one relatively novel aspect of our system is that we have moved from an algorithmic description common in most systems  to a scoring description: potential summaries are given an overall score based on the scores of the included content words  and the goal is to find the summary with the best overall score. we first explain how we decide the overall score of a summary in section 1. then  in section 1  we describe the optimization procedure that searches for the best summary. finally  in sec 1  we describe a variation that does sentence simplification  to allow the system to use either full sentences  or sentences with certain less useful phrases removed.
1 scoring the summary
we considered two different methods for combining scores of words to get an overall score: a product-based method  and a sum-based method. consider the generative model of section 1. for a given summary  we could multiply together the probabilities from this model. finding the summary with the highest product of probabilities would give us the summary with the highest probability of being exactly correct  according to the generative model.
　on the other hand  in automatic evaluation metrics such as the rouge scores  we favor summaries that have the most words that also appear in the reference  i.e.  human  summaries. if the score of a content word represents the probability that the word appears in the human summary  then the summation of these scores can be thought of as the expected number of  correct  content words. we compared the sum and product methods  and we found that the sum method consistently worked better.
　our summary scoring principle is somewhat different from sumbasic in that we directly compute the score based on the words  while sumbasic weights sentences first and tries to select sentences which have a better total score.
1 finding the best summary
the iterative algorithm used in sumbasic can be thought of as a simple greedy algorithm that tries to find the best summary. however  this greedy algorithm rarely finds the best summary  even in the sense of optimizing the expected score. in addition  since it does not explicitly consider the maxlength cut-off threshold  which causes the end of the last sentence not to be used for scoring  the score of the final summary may be further impacted. we thus developed a more complex algorithm that could explicitly search for the best combination of sentences. our algorithm is based on a stack decoder  jelinek  1 . one typical problem of stack decoders is that they have trouble comparing hypotheses of different lengths. although it is sometimes solved with an a* search  paul  1   this requires finding an admissible cost function  which does not always exist. instead of using an a* search  we chose to use multiple stacks  with each stack representing hypotheses of different lengths  magerman  1 .
　our stack decoder method is shown in algorithm 1  which takes as input the set of all sentences from the document cluster  as well as the score array used to weight the sentences. the method uses maxlength stacks: one for each length  up to the maximum length of the summary. each stack contains our best summaries so far  of exactly that length.  the last stack  stack maxlength   may contain summaries longer than maxlength - but words past size maxlength are not considered as part of the scoring . there will be at most stacksize different hypotheses on any given stack.
　the algorithm proceeds by examining a particular stack. it looks at every solution on that stack  a solution is a set of sentences . it then tries to extend that solution with every sentence from the document cluster. these extensions are then placed on the stack of the appropriate length. in order to avoid an exponential blowup in the number of solutions on any given stack  we use a priority queue  and only keep the top stacksize highest scoring solutions on any given stack.
algorithm 1 stack decoder for multi-document summa-
rization

1: input: an array of sentences and scores for each term in the sentences
1: input: a maximum length maxlength
1: input: a maximum stacksize
1: typedef solution = a variable length array of sentence ids
1: let stack 1..maxlength  be a priority queue of solutions; each queue has at most stacksize solutions.
1: stack = the solution of length 1;
1: for i = 1 to maxlength   1 do 1: for all sol （ stack i  do
1:	for all s （ sentences do
1:	newlen = min i+length s  maxlength 
1:	newsol = sol “{s}
1: score = score of newsol counting each word once  and at most maxlength words
1:	insert newsol score into queue stack newlen  
pruning if necessary
1:	end for 1:	end for 1: end for
1: return best scoring solution in stack maxlength 

　notice that if we did not penalize words that occur more than once  and if we did not truncate the very last sentence as part of the scoring procedure  then this problem would be equivalent to the knapsack problem: how large a score can we pack into a maxlength word summary. without the noduplication limitation and last sentence truncation  and when using a stack size of 1  algorithm 1 devolves to the standard exact solution using dynamic programming for the knapsack problem.
　in section 1 we will compare the greedy algorithm in sumbasic to the stack decoder algorithm  and show that the stack decoder leads to reasonably large gains. note that this algorithm is fast: about 1 seconds per document cluster with a stack size of 1 on a standard pc.
1 summarization with simplified sentences
the goal of a summarization system is to produce summaries with as much content as possible given the length limit. therefore  if we can simplify sentences in the summary  removing phrases that have little or no expected value  we can make room for additional sentences that provide more value.
　for each sentence in the document cluster  our sentence simplification procedure eliminates various syntactic units based on predefined heuristic templates  such as removing noun appositives  gerundive clauses  nonrestrictive relative clauses  or intra-sentential attributions. unlike previous approaches that deterministically shorten sentences before or after sentence selection  e.g.   conroy et al.  1; siddharthan et al.  1; daume＞ iii and marcu  1    the simplified sentence in our approach does not replace the original sentence but is instead added to the sentence pool for the summarizer to choose from. the choice among the sentence alternatives provided by the simplification procedure is left entirely to the summarization component. a detailed description of our approach to sentence simplification can be found in  vanderwende et al.  1 .
1 experiments
in order to evaluate the performance of our systems  we use two data sets that have been used in recent multi-document summarization shared tasks: multi-document summarization  task 1  in duc-1 and the multilingual multi-document summarization task in mse-1. we first show the results of the purely extractive system on each of these tasks  and also show the effects of variations of the systems. next  we perform experiments using the sentence simplification system  showing additional improvements.
duc 1
in the multi-document summarization task in duc-1  participants are given 1 document clusters  where each cluster has 1 news articles discussing the same topic  and are asked to generate summaries of at most 1 words for each cluster. since the same task was also held in duc-1  but with different documents  we take the 1 data for development  especially for training the probabilities.
　we present the results of our system and sumbasic using different term scoring methods in table 1. in addition  we also compare them with the best system  peer1  and the baseline system  greedyline  in duc-1. as mentioned previously  greedyline simply takes the first 1 words of the most recent news article in the document cluster as the summary. for the evaluation  we use the rouge-1 metric  with stemming and stop-words removed   which has been shown to correlate well with human judgments  lin and hovy  1; lin  1  and which was found to have one of the best correlations with human judgments on the duc-1 data  over and yen  1 . in addition  we also report the performance on rouge-1  bigram overlap  and rouge-su1  skip bigram  metrics1.
　in the table  the stack systems use our stack decoder algorithm  while the basic systems use sumbasic's iterative algorithm. the suffixes of the system names indicate the types of term scoring methods used. discriminative training is represented by -train; using frequencies only is denoted as -freq; using frequencies in the first 1 words is -pos; and finally  -inter means the score is the average of full document frequencies and frequencies in the first 1 words.
　we performed paired t-tests comparing our systems to peer1  the previous best performing system  and to sumbasic  basic-freq in our terminology . the top three systems  stack-train  stack-inter  and basic-inter  were all significantly better on rouge-1 than peer1  p   .1 . on rouge-1 and rouge-su1  these three systems were just slightly worse than peer-1  but the differences were not significant. the top four systems were all significantly better than basic-freq  p   .1 . we have thus improved on both our baseline system and on the best previous system.
systemrouge-1rouge-1rouge-su1stack-train111stack-inter111basic-inter111basic-train111stack-freq111stack-pos111basic-pos111peer 1.1.1.1basic-freq111greedyline111table 1: duc-1: rouge-1  stop-words removed and stemmed   rouge-1 and rouge-su1  stemmed  scores
systemrouge-1rouge-1rouge-su1stack-inter111stack-pos111basic-train111stack-freq111stack-train111basic-inter111peer 1.1.1.1basic-pos111basic-freq111table 1: mse-1: rouge-1  stop-words removed and stemmed   rouge-1 and rouge-su1  stemmed  scores
　we also looked at the component improvements. in every case  stack decoding was better than basic  greedy  decoding  but the differences were not statistically significant. differences between a pure frequency approach and the trained approach  stack-train versus stack-freq and basic-train versus basic-freq  were both highly significant  p   .1 . the difference between basic-inter and basic-freq was highly significant  p   .1   showing that using position information does indeed lead to improvement.
mse 1
in 1  a different multi-document summarization task was conducted as part of the machine translation and summarization workshop at acl. participating systems produced a 1-word summary from a document cluster  which was a mixture of english and arabic news articles on the same topic  where the arabic documents are translated into english by automatic machine translation systems. in addition to this major difference  the news articles are generally shorter than those used in duc-1. ignoring the potential mistakes introduced by the machine translator  we ran our systems without specific modifications for this unusual setting  except counting the frequencies of the first 1 words instead of 1 in our position-based generative models   -pos and
-inter .
　as shown in table 1  on this data set  the mean rouge1 score of our best system  stack-inter  is better than the best participating system  peer 1  and the original version of sumbasic. on rouge-1 and rouge-su1  the scores of our systems are slightly lower. however  in all three metrics  all of the systems have no statistically significant difference.
we notice that the our stack-decoding summarizer with
systemrouge-1rouge-1rouge-su1stack-train-sim1  +1 11basic-train-sim1  +1 11stack-freq-sim1  +1 11basic-freq-sim1  +1 11stack-train111basic-train111stack-freq111basic-freq111table 1: duc-1: rouge-1  rouge-1 and rouge-su1
scores  with sentence simplification
the discriminatively trained term scores does not perform the best. this may be due to the fact the model is trained on the duc-1 data  which may be quite different from the data in mse-1.
sentence simplification for duc 1 and mse 1
next  we look at the effects of sentence simplification. table 1 shows the performance of different configurations of our summarizerwith sentence simplification  -sim   and without; the number in parentheses for the rouge-1 score is the amount of improvement. comparing the four pairs of configurations  e.g. stack-train-sim versus stack-train  etc.   we see that sentence simplification consistently raises the rouge1 score by about .1  while having essentially no impact on rouge-1 or rouge-su1. the difference in rouge-1 score was largest for stack-train-sim versus stack-train  and the difference was statistically significant  p   .1  for both this pair  and stack-freq-sim versus stack-freq. there appears to be a synergy to using our stack decoder with sentence simplification: the stack decoder allows the system to do a better job of choosing among the many candidate sentences  which include both all of the original sentences  and simplified versions of many sentences.
1 related work
in this section  we compare our work to related work on a number of aspects  such as scoring method  search method  etc.
　both sumbasic and our system focus on scoring individual words. in contrast  most existing system are sentence-based. these sentence-based systems use a variety of features  including: sentence position in the document  sentence length  sentence similarity to previously extracted sentences  usually using the maximal marginal relevance  mmr  framework  carbonell and goldstein  1    an explicit redundancy score  daume＞ iii and marcu  1   and sentence similarity to the document centroid. in the cases where individual words are considered during sentence selection  important words are identified through graph-based analysis where the nodes in the graph represent words  mani and bloedorn  1; erkan and radev  1   rather than through probabilistic measures such as those used in this work. in contrast to these complex systems  the only features we use are frequency and position-based.
　we combine the position and frequency information using either a simple generative or simple discriminative model. other summarization systems use heuristic methods for combining multiple features  or  as in  daume＞ iii and marcu  1   have trained parameters directly using various rouge metrics as an objective function.
　almost all previous multi-document summarization systems  including sumbasic  have used greedy or heuristic searches to choose which sentences to use  even when they had an explicit scoring function. in this paper  we formalize summarization as an optimization problem and present an explicit search algorithm  namely a stack decoder  to search for the best combination of sentences.
　a complete survey of summarization systems is beyond the scope of this paper  but it's worth describing in more detail the classy summarization system  which was the previous best system on the rouge-1 metric for the duc1 task  peer 1   and is now tied for best for the mse1 task  peer 1 . the classy  conroy et al.  1; 1  summarization system consists of two core components - a hidden markov model for selecting sentences from each document and a pivoted qr algorithm for generating a multi-documentsummary. the hmm has two kinds of states  which correspond to summary and non-summarysentences in a single document. the model uses just one feature  which is the number of signature terms in each sentence. these terms are decided by the log-likelihood statistic suggested by lin and hovy   derived based on a large set of documents in advance. in addition  the best number of the hmm states needs to be determined based on empirical testing  and the hmm model needs to be learned using training data. after applying the hmm  the top scoring sentences of each document form a weighted token-sentence matrix. a pivoted qr algorithm is then used for scoring and selecting sentences to form the output summary. in addition to these two core components  classy also incorporates a linguistic component as a preprocessing stage to provide the summarization engine simplified  shortened  sentences as input.
　very recently  wan and yang  1  proposed an approach to multi-document summarization based on affinity graphs. their method tried to identify semantic relationships between sentences and used a graph rank algorithm to compute the amount of information a subset of sentences contain. the best subset of sentences were then selected as the output summary using a greedy algorithm. their system was also very competitive and outperformed the best result in duc-1. however  because of the different rouge versions and parameter settings  we are not able to compare directly with their results.
1 conclusion
our results are the best reported on the duc-1 and mse1 multi-document summarization tasks for the rouge-1 score  although only for the duc-1 task is the difference statistically significant  and we could not compare to wan and yang  1  because of the different rouge versions used. on rouge-1 and rouge-su1  our system is second best  although the differences are not statistically significant. we have achieved these excellent results using a system that is substantially simpler than the previousbest system  classy.
in particular  our system used about 1 lines of code for the stack decoder  and less than 1 lines for the score computations  as well as pre-existing libraries for logistic regression and for finding content words.
　we achieved these results by enhancing an already fairly competitive summarization system  sumbasic  in several aspects. first  we showed that position information was not already sufficiently captured by sumbasic. second  we proposed different word scoring methods that combine the position information with frequency information. we gave a very easy-to-implement generative model that produces excellent results  and we described a somewhat more complex  but still straightforward  discriminatively-trained version that works even better. while a few other systems have explicitly optimized parameters  daume＞ iii and marcu  1; erkan and radev  1   we are not aware of any previous work optimizing for word scores. third  we described a simple search procedure using a stack decoder that can find the best sentences to form a summary  given the word scores. in contrast to more common approaches using heuristic or greedy methods  such as the iterative algorithm of sumbasic  the explicit search method is not only more principled and with a clear objective function  but also better empirically.
　as for future research  we would like to apply our method also in single-document summarization. given that position plays an even more important role in this task  we believe our system should be able to perform reasonably well and unify single-document and multi-document summarization tasks seamlessly. while frequency may be less informative given that there is less repetition  our discriminative model provides a way to incorporate other information that can be helpful in judging word importance  especially in the singledocumentsummarization setting. another area we would like to explore is to enhance the readability of the summary  an important issues existing in almost all purely extractive summarization systems. while the summary generated by our system is very informative  the coherence between sentences in the summary may be further enhanced by adjusting the order better or applying some semantic analysis to reconstruct the summary. finally  we would like to adapt our system to task-focused summarization problems  such as web search result snippets.
acknowledgments
we thank arul menezes  who helped us on the initial setting of the experiments. we are also grateful to anonymous reviewers for their valuable comments.
