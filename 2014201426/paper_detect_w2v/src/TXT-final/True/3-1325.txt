
the limited visual and computational resources available during the perception of a human action makes a visual attention mechanism essential. in this paper we propose an attention mechanism that combines the saliency of top-down  or goaldirected  elements  based on multiple hypotheses about the demonstrated action  with the saliency of bottom-up  or stimulus-driven  components. furthermore  we use the bottom-uppart to initialise the top-down  hence resulting in a selection of the behaviours that rightly require the limited computational resources. this attention mechanism is then combined with an action understanding model and implemented on a robot  where we examine its performance during the observation of object-directed human actions.
1 introduction
in an attempt to arrive at a definition for attention  tsotsos  in his review paper  tsotsos  1   arrives at the following proposal:  attention is a set of strategies that attempts to reduce the computational cost of the search processes inherent in visual perception . our work aims at producinga model of visual attention for dynamic scenes  emphasising the importance of top-down knowledge in directing the attention during action recognition.
　after introducing the two different  bottom-up and topdown  elements of attention  we will proceed to review a model of action understanding  demiris and johnson  1  that will make use of our attention model  to correctly allocate the limited resources available to it. subsequently  we will proceed to describe how the multiple hypotheses generated by our model while the human demonstration is unfolding can feed top-down signals to the attention mechanism to direct attention to the important aspects of the demonstration.
　furthermore  we propose a method for initialising the topdown part  using the saliency of the bottom-up part in our visual attention mechanism. we have implemented our model on an activmedia robot  running experiments observing a human acting upon various objects. these results will be presented and discussed  not only in terms of whether our attention model allocates the resources correctly  but also to see if it results in a faster recognition of the correct behaviour being demonstrated in the action understanding model.
1 background
work in cognitive science suggests that the control inputs to the attention mechanism can be divided into two categories: stimulus-driven  or  bottom-up   and goal-directed  or  topdown    van essen et al.  1 . a number of bottom-up attention models follow treisman's feature integration theory  treisman and gelade  1  by calculating the saliency for different low-level features of the object  e.g. colour  texture or movement. a winner-take-all approach is then used to decide on the most salient part of the scene  as in  koch and ullman  1    itti et al.  1    breazeal and scassellati  1  . top-down  on the other hand  covers the goaldirected factors  essentially the task-dependent part of the visual attention model. wolfe  wolfe and gancarz  1  produced a biologically inspired guided search model that controls the bottom-up features that are relevant to the current task by a top-down mechanism  through varying the weighting of the feature maps. however  it is not clear what the task relevant features are  particularly in the case of action recognition.
　our attention mechanism is inspired by wolfe's model  and integrates bottom-up elements with a top-down mechanism using behaviours1 and forward models that can guide the robot's attention according to the current task. a forward model is a function that  given the current state of the system and a control command to be applied on it as given by the behaviour  outputs the predicted next state. our top-down part of the attention model  when observing a demonstration  will make a prediction of the next state for a number of different possible behaviours  producing a confidence value for each  based on the observedaccuracy of the prediction. these confidence levels are important values that can be thought of as saliencies for the top-down mechanism of our attention model  hence producing a principled method of quantifying the top-down part of the attention mechanism.
　in the experimental section  we will demonstrate that this attention mechanism improves performance on action understanding mechanisms that use multiple behaviours and forfigure 1: action understanding model  demiris and johnson  1 .
ward models such as  demiris and johnson  1 . this will be done in two ways: first by cutting down the number of computational cycles required for identifying the correct behaviour and by directing the limited computational resources to the relevant parts of the human demonstration  instead of the whole scene. secondly  by using the saliency from the bottom-up part of our visual attention mechanism to initialise the top-down part  enabling it to select only the relevant behaviours to the demonstrated action  instead of activating and running all of them.
1 action understanding model
demiris's action understanding model  demiris and johnson  1   shown in figure 1  identifies the correct behaviour that is being observed by using forward models for a number of behaviours. by predicting what will happen next  and comparing it with what actually does happen next  from the behaviour that is being demonstrated   confidence levels are generated for each of the predicted behaviours. from these  a winner is selected by picking the predicted behaviour with the highest confidence level.
　the attention mechanism we propose in this paper can be used to cut computational costs on this action understanding model  demiris and johnson  1 . it would be far too computationally expensive to direct the attention of the observer towards all the parts of the demonstration to satisfy all the possible behaviours  tsotsos  1 . hence  the attention mechanism is used to restrict this  giving only one of the behaviours at a time the information it requires. using this attention mechanism  we managed to cut down substantially on the computational costs  yet it was also achieved without affecting the quality of the system to the extent of producing wrong outcomes.
　the success of our model will be demonstrated through comparisons of the new results with the results from the original action understanding model for a number of different behaviours. if  after having cut down on all the computational costs using our visual attention model  the final behaviour chosen in each situation with the previous model  demiris and johnson  1  remains the same  then our attention model will be deemed to have succeeded in its task.
figure 1: our bottom-up model of visual attention. the output is the focus of attention  foa 
1 the attention mechanism
in this section we will describe the design of our attention mechanism  and the integration of both bottom-up and topdown elements.
1 bottom-up
we have implemented a bottom-up model that is mainly based on wolfe's biologically inspired guided search 1 model of human visual attention and visual search  wolfe and gancarz  1 . this model uses treisman's feature integration theory  treisman and gelade  1  to construct a model of human visual attention. in this model  low-level filters are applied to various visual stimuli in order to produce individual feature maps in which high values indicate areas of interest.
　all of the individual feature maps are weighted and then summed into a single activation map. attention is guided to peaks in the activation map  because these represent the most salient areas in the scene. in our model  top-down task information can influence the bottom-up feature maps by changing the activation map through the modifying of the weights that are applied before the summation. our model is shown in figure 1. there are certain features that can make objects salient. for example  brightly coloured objects are a most typical example  or if they are movingin a way that can attract attention  e.g a sudden  irregular and fast movement. each of the bottom-up blocks in the model represents a certain feature that contributes towards the calculation of the saliency of an object. our implementation focuses on three bottom-up blocks that are feature detectors. these are: motion  colour and the size the object occupies in the image  which not only accounts for the actual size of an object  but also for the distance of the object from the camera  both of which are important in grabbing one's attention.
the remaining blocks in the model are:
  fovea-effect - this models the decrease in resolution away from the centre of the image  because our eyes' resolution decreases dramatically with the distance from the fovea  farid et al.  1 .

figure 1: the architecture of the top-down part of the model and how it integrates with the bottom-up part
  winner takes all  wta  selection as in itti's model  itti et al.  1 .
  an attention gate mechanism as in wolfe's model  wolfe and gancarz  1  which keeps a record of the two most salient areas in the scene.
  a saccade generationsystem as in wolfe's model  wolfe and gancarz  1 .
1 top-down
figure 1 shows our complete visual attention model which includes the top-down part. this figure specifically illustrates how the top-down information could influence the bottom-up part of the model  and vice versa.
　our top-down part of the model receives the list of behaviours from the action understanding model  described in the previous section  together with their confidence levels and their forward models. it must select only one behaviourout of the many behaviours to attend to at any given point in time. the block labelled  selecting one behaviour  in figure 1 has five inputs:
  behaviours - a list of hypotheses  potential behaviours that explain the demonstration  is passed in  one of which must be selected.
  confidence levels - the current confidence levels for each behaviour.
  resource allocation algorithms - we have performed experiments with different resource allocation algorithms  stallings  1  that can be employed to decide on how to distribute resources between the behaviours.
  internal representations of objects and actions - this block gives information about objects and how they move  and interact with other objects etc.
  purely bottom-upsaliency map - this is the saliency map representing the most salient object in the scene.
from the above five inputs  one behaviour must be chosen. the block labelled  where to attend for this behaviour   in figure 1 has three inputs:
  output from  selecting one behaviour  - the winner behaviour that is selected from the previous stage is passed on.
  internal representations of objects and actions - this block gives information about objects and how they move  and interact with other objects etc.
  bottom-up saliency map influenced by top-down - this is to give current information on where the attention of the model is.
the output of this block influences all the bottom-up calculation blocks in order to direct the attention of our model in such a way that it serves the current behaviour.
1 experimental setup
we implemented our model on an activmedia peoplebot robot  equipped with a pan-tilt-zoom camera  as well as a two degrees of freedom gripper  sonar and infrared sensors. only the camera was used in the following sets of experiments  and the saccade generation module was switched off as it was not needed.
　for these experiments  three objects were chosen: a hand  a coke can  and an orange. eight behaviours were then defined:
  behaviour 1 - pick coke can
  behaviour 1 - move coke can
  behaviour 1 - move hand away from coke can
  behaviour 1 - pick orange
  behaviour 1 - move orange
  behaviour 1 - move hand away from orange
  behaviour 1 - drop coke can
  behaviour 1 - drop orange
each of these behaviours has a correspondingforward model. figure 1 shows the arrangement for behaviour 1.

figure 1: behaviour1 - picking a coke can

figure 1: images to show bottom-upblock processing a scene of a hand picking a coke can
　all the other behaviours are implemented in the same way. forward models were hand coded using kinematic rules to output the prediction of the next state. the output from the forward model  which is a prediction of the actual next state  is compared with the next state. based on this comparison  a confidence level is generated for this behaviour by either rewarding it with one confidence point if the prediction is successful  or otherwise punishing it by subtracting one confidence point.
　figure 1 shows an input example of what the robot sees when a behaviour is carried out  in this case  it is the demonstration of a hand picking a coke can . a background was chosen where these object's colours were minimally present. these are only snapshots of some frames.
　the bottom-up block detects and tracks the presence of the coke can  the orange and/or the hand in the scene  depending on what the top-down part of the attention model requires. the output of this bottom-up block are the corresponding locations of where the hand  the coke can and/or the orange are in the scene. this information is then passed to the top-down part of the model for intelligent processing. the camshift algorithm  bradski  1  was used to assist in doing this. we used a hue and saturation histogram back-projection of camera images taken at a pixel resolution of 1〜1 and at 1 frames per second. the corresponding histograms of the three objects used in our experiments were pre-saved into the robot's memory  and used during the experiments as a simple method of object recognition.
　four different implementations were experimented with each of the eight behaviours:
  a pure implementation of the action understanding model without our attention model. hence there was no layer of intelligence to cut down on computational costs  i.e. each behaviour gets to carry out all the computations it requires at each frame.
  our attention model is added to the action understanding model using a  round robin  scheduling algorithm  equal time sharing   stallings  1  to select between the behaviours. therefore  each behaviour is processed every eighth frame  since there are eight behaviours in total.
  our attention model is added to the action understanding model using the strategy of  highest confidence level always wins   which means the behaviour with the previous highest confidence level gets the next computation.
  our attention model is added to the action understanding model using a combination of the  round robin  and the  highest confidence level always wins  strategies to select between the behaviours.
finally  we also ran another set of experiments by adding to these implementations initialisation for the top-down part using bottom-up saliencies.
1 experimental results
we used 1 differentvideos  performing1 experiments  using the different scheduling algorithms above  while varying some of the model's parameters. the results for behaviour 1 are shown in figure 1 as an example of a typical output to demonstratehow our system can work on top of the action understanding model  cutting down its computational costs by up to 1%  because every behaviour is now only being processed once in every 1 frames   and still producing the correct results to determine which behaviour is being demonstrated. but more importantly  it directs the limited computational resources to the relevant areas in the scene  instead of analysing the whole scene.
　in addition to this  the above results were substantially improved by adding initialisation to the top-down part using the bottom-up saliencies to our attention model . therefore  in the case of a scene where the orange does not exist  as shown in figure 1  our bottom-up block would detect the coke can and the hand as being salient objects. using previously saved colour histograms  our system recognises that these salient objects are a coke can and a hand. this result is passed to the top-down part of our attention model  which in turn will only select the behaviours that are involved with these objects  as opposed to previously selecting every behaviour that exists in the database.
　results from behaviour 1  where no orange was present in the scene  are shown in figure 1 as an example for this initialisation process of only selecting the relevant behaviours using the bottom-up saliencies. the results are compared to the previous implementation without this initialisation. it can be seen that this initialisation process speeds up the correct recognition of the correct behaviour. furthermore  it will also serve in allowing scalability to be added to our model.
　as can be seen from figure 1  our attention model not only still gives the correct results for the recognition of behaviour 1  but it does it with the saving of up to 1% of the total computational costs. furthermore  it returns better results in recognising the correct behaviour by isolating it from the other wrong behaviours  due to the focusing on the correct areas of the scene only  instead of the entire image. these successful results were also seen for all of the other seven behaviours that were tested.

　behaviour 1 in figure 1 shows that when a  round robin  scheduling algorithm is applied  the correct behaviour is still picked  but it ends up with a lower confidence value that is quite close to the other resulting confidence values of the other behaviours. the reduction in computational costs using this algorithm has resulted in a decrease on the separability of the behaviours. this is because there are n frames in the demonstrated scene  in these experiments  n is 1   and each behaviour is only processed once every m frames  where m is the number of behaviours  in these experiments  m is 1  meaning a total number of n/m computations per behaviour  which is 1 in these experiments  hence  only a maximum score of 1 for the winning behaviour . behaviour 1 in this case scores 1 out of 1  still enough to make it the winning behaviour  but much lower than the pure implementation of the action understanding model without the use of any attention mechanism.
　when  highest confidence level always wins  is used  the opposite effect to  round robin  can be seen: attention acts like an accelerator to the winning behaviour once it recognises who is the winner  suppressing all the others. the problem with purely using the scheduling algorithm of  highest confidence level always wins   is that it may not always initialise correctly  as can be seen in figure 1  hence taking some time before converging on the correct behaviour.
to alleviate this problem  we used the  round robin 
figure 1: behaviour 1 - move hand away from orange	scheduling algorithm as an initialisation step for the  highest confidence level always wins  which then acts as an ac-

celerator for the winning behaviour. hence   round robin  is applied only for half the length of the demonstration  1 frames in these experiments . then the  highest confidence level always wins  is applied as an accelerator to the winning behaviour. as can be seen from figure 1  it returns accurate and fast results for selecting the winning behaviour. this process may be seen as being equivalent to  playing it safe  at the beginning by looking everywhere  because we are not yet sure what is happening. but once we start to understand the scene better  we are more confident to focus on the relevant parts and ignoring the rest.
　results from figure 1 show significant improvements when using the bottom-up saliencies to initialise the top-down selection of the relevant behaviours. hence as a result  only the relevant behaviours are picked for processing  instead of picking all the behaviours in the robot's database. in this experiment  the robot has 1 behaviours in its database  1 of which involve a hand and a coke can  and the other 1 involve a hand and an orange. in the demonstrated video  the orange is not present  hence  all the 1 behaviours involving the orange  are immediately punished and not processed. the remaining 1 behaviours  instead of the total 1  are processed with the  round robin  scheduling algorithm at the beginning. great improvementcan be seen here in finding the correct behaviour sooner  at frame 1 instead of the previous frame 1. this can be thought of as enabling our model to recognise the correct behaviour being demonstrated easier and quicker in a less complicated scene with fewer objects.
1 conclusion
our attention model utilises both  resource scheduling algorithms  and initialisation of the top-down mechanism to cut down on the computational costs. we have found that a combination of the  round robin  and the  highest confidence level always wins  strategies  together with using the bottomup saliencies for initialising the top-down selection of the relevant behaviours to the demonstrated scene  worked very well.
　the computational costs needed to run our attention model are justified since the action-understanding model is aimed at having a large number of behaviours. as the number of behaviours increases therefore  the resultant search space makes the model indispensable. this is especially because the savings on the computational costs will also increase by  n 1 /n for n behaviours.
　we are working towards further optimisation of our model by considering optimal path algorithms. for example  if the sequence of planned allocations involves looking at the head  feet  and arm of a human demonstrator  the algorithm will try to minimise the saccade required and rearrange the allocations to accommodate that.
　optimisation based on predicting future requests will further enhance the performance of our model  and will be our next step in our investigationbetween attention and action understanding  with the ultimate goal of having robots that will efficiently understand our human actions.
acknowledgments
this research was funded by the donal w morphy trust scholarship and by the uk engineering and physical sciences research council and the royal society. the authors would like to thank the bioart team for their support and assistance  especially matthew johnson who assisted in the implementation of the camshift tracker.
