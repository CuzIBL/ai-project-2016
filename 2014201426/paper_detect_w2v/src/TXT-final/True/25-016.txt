 
scott  semantically constrained otter  is a resolution-based automatic theorem prover for first order logic. it is based on the high performance prover otter by w. mccune and also incorporates a model generator. this finds finite models which scott is able to use in a variety of ways to direct its proof search. clauses generated by the prover are in turn used as axioms of theories to be modelled. thus prover and model generator inform each other dynamically. this paper describes the algorithm and some sample results. 
　scott  semantically constrained otter  is a resolution based automatic theorem prover for first order logic. so much is hardly revolutionary. what is new in scott is the way in which it blends traditional theorem proving methods  best seen as purely syntactic  with techniques for semantic investigation more usually associated with constraint satisfaction problems. thus it bridges two aspects of the science of reasoning. it was made by marrying an existing high-performance theorem prover to an existing model generator. neither parent program was much modified in this process. the resulting combined system out-performs its parents on many problems  for some of which it is currently the most effective prover available. 
1 	the parents 
1 	syntax: o t t e r 
the theorem prover otter  written by w. mccune and based on earlier work by e. lusk  r. overbeek and others  is a product of argonne national laboratory and is widely regarded as the most powerful program of its type for certain classes of problem   mccune  1; lusk and mccune  1  . its basic method is forward chaining  applying a rule of inference r  seen here as a partial function on clauses  to generate new clauses as in figure 1. the clauses are divided into two disjoint sets  the set of support and the usable list initially the set of support is non-empty. clearly the proof search may terminate with a successful proof  or the set of support may be emptied  showing that there is no proof in the 

figure 1: basic o t t e r algorithm 
chosen search space  or it may fail to terminate. first order logic being undecidable  this is to be expected. 
　the simplest rule applied by otter is binary resolution  together with unification and factoring. more interesting and powerful variants include hyper-resolution  negative hyper-resolution and unit resulting resolution. for equational reasoning the available rules include the various forms of paramodulation and term rewriting  demodulation . 
　a crucial part of the algorithm is the decision as to whether each deduced formula is  new . in general this means that it is not subsumed by any formula already kept. much of the high performance of otter is due to its sophisticated techniques for reducing the time spent computing subsumption and related properties. these techniques are not the focus of the present paper. also not shown in the simple version of the algorithm in figure 1 is the rewriting due to demodulation and the like which may take place before the subsumption test. nor have back subsumption and back demodulation  whereby the generated clause is used to simplify the existing clause database  been made explicit  although in many applications they are important. 
1 	semantics: f i n d e r 
deduction is only one form of reasoning. another is the generation of models of a given theory. a model of 
slaney 

a theory shows that theory to be consistent  but it also shows much more. it is an account of what it might be like for the theory to be true. it divides the whole language into truths and falsehoods in a way consonant with the theory. thus  given an effective means of detecting the truth values it assigns to formulas  it can be used to reach out beyond the given basis to determine semanticproperties of all formulae. 
   if a theory has finite models  one way of generating them is to fix the domain as consisting of a few objects  say three objects  or thirty  and then to perform an exhaustive search in the space of functions definable by enumeration over that domain. a model is an interpretation of the non-logical symbols in the theory's language making all of its axioms true. since all of the structures in question are finite  it is easy to recognise models when they are found. there are  of course  more efficient and less efficient methods of searching. finder see  slaney  1   performs a backtracking search  building a database of facts about the search space to enable it to avoid ever having to backtrack twice for the same reason. the details are interesting but are not germane to the present application since any reasonable constraint satisfaction algorithm could be used to much the same effect provided it is not heavily dominated by the overheads of starting up a search. 
　finder accepts input in a fairly friendly format. it works with first order theories in clausal form couched in a many-sorted language. function symbols must be given first order types in terms of the sorts. there are a few pre-defined function symbols: for example  each sort comes with an identity relation  symbolised by ' = ' each sort is also equipped with a total order  symbolised with '   ' and ' '. evidently  this convention does no harm and it is often useful. a clause is a set of literals and is true iff for every assignment of elements from the domains to variables occurring in it one of those literals is true. 
   it is worth noting that what finder does is rather different from logic programming. in the first place  it imposes no order on the evaluation of clauses and has no 'flow of control'. the clauses meet each candidate model in a body; if they are all true then the model is good  while if any clause is false  the model is adjusted to deal with that badness  resulting in a new candidate. in the second place  finder is not able to show the nonexistence of models and hence cannot prove sets of clauses inconsistent. it returns a null result if there is no model within a specified finite search space  but always leaves open the possibility that there may be models outside that space. in the third place  it finds and specifies the models without any reference to the herbrand universe. the domain consists simply of the first object  the second object and so forth  any relationship with terms of the language being accidental. 
1 	putting it together 
there are several distinct ways in which semantic information such as the truth value of a formula in a model can be used to direct proof searches. the oldest and simplest is goal deletion. in attempting to show by backward chaining that a goal formula is a theorem of some theory  
1 	automated reasoning 

figure 1: basic scott algorithm 
we decompose the goal into simpler subgoals and work recursively on those. in typical cases  most of the subgoals are unprovable. having discovered that a subgoal cannot be reached  the proof search must backtrack and try another. hence techniques for rapidly detecting and deleting unprovable subgoals are valuable. one good technique is to test goals for truth in some simple model of the theory. any that are false are unprovable and may be deleted. see  ballantyne and bledsoe  1  and  thistlewaite et al  1  for some discussion. otter is a forward chaining prover  so goal deletion is not the main present concern  though it is fairly obvious how a method similar to that of scott could be used for goal deletion in backward chaining systems. 
　another use for semantic information is in the false preference strategy. this is appropriate to forward chaining proof search and uses some model or models in which the goal is false. the strategy is to prefer parent clauses of inferences to be ones false in the guiding model or models  on the thought that the goal is more likely to be deduced from clauses which imply it in the models than from those which do not. since this is only a heuristic  it can usually be shown not to affect the prover's completeness. scott  optionally  implements the false preference strategy in a straightforward way  testing each kept clause against a guiding model and assigning greater weight to clauses which are true in the model than to those which are false. 
　most interesting for present purposes is the idea of semantic resolution. given any model m  a simple the-
orem assures us that if there is a derivation of the empty clause from a set of clauses using unification  resolution and factoring then there is one in which no inference has parents both of which are true in m. this is a well worn result  for a fuller account of which see  chang and lee  1  or  slagle  1 . scott implements semantic resolution by means of two very simple amendments to otter's algorithm. these are underlined in figure 1. the safe clauses are those true in the guiding model. 
they are  safe  because they are known to form a con-

figure 1: basic tester algorithm 
sistent set whose immediate consequences may therefore safely be omitted. if the given clause g is safe then it may not react with other clauses unless they are unsafe. this cuts down the number of generated clauses sufficiently to have a marked effect on otter. 
before the guiding model can be used it must be found. 
the clause testing module which assigns labels to given clauses calls finder from time to time  to generate a model of a set of clauses or to return the information that no model was found within the delimited search space. the logic of the clause tester is given in figure 1. note that at any given time it is either in generating mode or in simple testing mode. at the start of the proof search it is in generating mode; after a while it stops trying to generate any new models and becomes just a tester. this is because model generation is expensive in comparison with testing and because there comes a time when no better model can be found without enlarging the search space to an unacceptable degree. the cutoff point at which the mode is switched may be set as a parameter  the default being after 1 clauses have been evaluated. also at any given time the tester has associated with it a theory t  the set of safe clauses so far  and a model m of that theory. the further procedure 'model' is the model generator  in effect finder  in scott's case  and its parameter is the theory to be modelled. it must be given a finite search space  so that it always completes even when it fails to find a model. in order to set up the semantic apparatus consistently  finder is called initially with the theory consisting of the clauses  if any  in the initial usable list. if it fails to model this  the whole proof attempt fails  since otherwise dynamic semantic resolution would not fit with the set of support algorithm. 
　two input scripts are needed for scott. one is the normal otter input consisting of the problem to be solved. it specifies the initial contents of the usable list and the set of support  together with any settings for otters optional parameters such as which rules to use  what weight limit to set and what results should be printed. the other is a finder input file  consisting of the sort and function specifications for the problem  together with any clauses in the initial usable list and optionally finder settings such as verbosity modes. expert knowledge about the problem domain may be added in the form of extra clauses to help direct finder to the models. in practice this facility is best used sparingly  as the program may make more intelligent choices of model than the  expert . any extra clauses given to finder are not communicated to the otter part of the prover  so they do not form part of the proof. 
1 	case studies 
a systematic evaluation of scott has yet to be made  since it has been designed and developed only in 1. the following sample results show its effect in three selected cases where otter is already one of the most effective theorem provers available. these involve condensed detachment axiomatisation of propositional logics and are among the hardest solved problems in the field. it would have been easy to show dramatic speedup effects in cases where otter does poorly-non-horn ground problems for example1 but it is of more interest and value to improve what otter does best. 
1 	classical pure implication 
one problem set for which otter is particularly suitable is determined by the single axiom for the classical propositional calculus of pure implication 

given by lukasiewicz. the problem is to derive from this some axioms known to be sufficient for the theory  using the rule of condensed detachment 

where  is the most general unifier of a and c. a convenient sufficient axiom set consists of these five:1 

a very simple otter input file for the first axiom  for example  reads as follows. 


the predicate p is for provability  and the function symbol i for implication. the goal is a skolemized denial of the axiom to be derived. otter input for the other four problems is similar  with the different goals of course. in practice there may be some further settings such as a weight limit and a limit on the number of distinct variables per clause. the fifth problem is difficult. to the best of my knowledge  otter is the only prover to have solved it automatically.1 
　the basic finder input used by scott in tandem with the otter input reads as follows. this is for the first problem; for the second  it is necessary only to declare another constant b and to change the goal. 

in fact  it pays to use some expert knowledge amounting to the fact that the implication relation may be expected to be acyclic. by embedding that relation in the default total order on the values we may cause the finder half of scott to avoid searching too many isomorphic subspaces. this alternative clause set is the one used for problem  1  and  with a change of goal  for problem  1 . 

for hard problems like  1  we shall probably also want to help finder along with some more expert knowledge about what models of implication logics are like. in the experiment reported here it was directed to a good model by being informed ahead of time of some of the clauses 
   1since this paragraph was written  the theorem proving group in the 'fifth generation' project icot in tokyo have reported a proof using their model generation theorem prover mgtp-n which however is closely modelled on otter. 
1 	automated reasoning 
 clauses generated clauses 
kept clauses 
given time 
 sec  1: otter 
1: scott 
factor 
1: otter 
1: scott 
factor 
1: otter 
1: scott 
factor 
1: otter 
1: scott factor 
1: otter 
1: scott 
factor 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 j 
1 
1 
1 
1 
1 
1 ' 
1 
1 
1 
1 figure 1: i m p problem: results 
which it would eventually have modelled anyway. the otter input was also changed a little  by imposing a length limit in order to cut down the number of clauses kept and by making it less verbose. 
　note that finder needs some termination condition such as a cardinality limit or a time limit. the more generous this limit  the longer scott will spend trying  and failing  to model inconsistent sets of clauses  but the more restrictive it is made the more likely scott is to miss some useful model. at this stage  the course between these two undesirable outcomes is steered manually  though clearly such heuristics are programmable. 
   as will be observed from figure 1  scott improves on otter on every measure. the 'factor' in each case is the figure for otter divided by that for scott. the counts of clauses give measures of the amount of work done. in each case about two thirds of the given clauses were labelled 'safe'  so approximately four ninths  two thirds squared  of clauses which would otherwise have been generated are avoided because their parents are safe. the amount of the time spent generating models and testing clauses for safety was 1% for problem 1 but only 1% for problem 1. in order to keep these experiments tidy for easy reporting  very simple finder input was used. where scott is used to tackle really serious problems  the option of giving it extra semantic information is clearly valuable. the input of expert knowledge and heuristics may be restricted to the semantic side  leaving the theorem prover itself clean. 
1 	extending intuitionist implication 
in  karpenko  1  karpenko raised the problem of finding an axiom which would strengthen intuitionist pure implication to classical pure implication but whose addition to various substructural systems would produce 

clauses 
generated clauses 
kept clauses 
given time 
 sec  natural 
otter scott 
factor 
tuned 
otter scott 
factor 1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 figure 1: intuitionist logic: results 
new substructural logics. there are several solutions to this problem  one of which is the formula 

where a * b is defined as 

the theorem-proving problem is to derive from this  together with some standard axioms for intuitionist implication  goal 1 of the set given in the last example: 

a suitable set of 'standard axioms' is the following. 

and again the problem is easily set up for otter to solve by enumerating consequences by condensed detachment. 
　observe that the second axiom assures us that the implication relation is transitive. hence we are entitled to add a new clause giving us the rule of condensed transitivity. 
where a is the most general unifier of b and c. the otter input clause for this reads: 

this is a useful proof-shortening device. figure 1 shows the results of running otter and scott on this problem  first without any special settings  except for a limit of three distinct variables per formula  then  after many experiments  with settings hand-tuned to help otter. it is worth noting that this problem was open until closed by scott. 
clauses 
generated clauses 
kept clauses 
given time  sec  otter scott 
factor 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 figure 1: many valued logic: results 
1 	many valued logic 
a similar problem to that presented by the lukasiewicz single axiom arises in the axiomatisation of his many valued logic. this time there is a negation connective as well as implication. the rule of inference is again condensed detachment  and the axioms are: 
 1  
 1  
 1  
 1  
the thought behind axiom  1  is that defines avb  so  1  amounts to the commutativity of disjunction. now the hard problem is to show that implication is a total order: 

in view of axiom 1  we may add the clause for condensed transitivity as before. next  it is not hard to see that the logic being axiomatised satisfies a rule of replacement of proved equivalents. we can secure the effect of replacement by adding a clause to make derived two way implications into equalities thus: 

together with settings to make otter add demodulators  rewrite rules  whenever it derives a directable equality. this  for instance will cause any subformula of the form to be rewritten as a  thus eliminating huge numbers of redundant equivalents. 
with careful setting of weight limits and the like  
otter can now solve the problem. it takes 1 hours 
1 minutes on a sparc-1  generating 1 clauses of which 1 are kept and 1 are given  added to ul . finder can easily be instructed to find a model of all the axioms except for the commutativity one  in which model negation is well-behaved in that  is everywhere the same as and in which the goal formula is false. the results of running scott with that finder file and with the same problem input as otter are interesting  figure 1 . overall performance actually degrades  despite the fact that over 1% of all given clauses are labelled 'safe'. 
　more useful for this problem is the false preference strategy. otter normally selects given clauses by weight  lightest first  taking the weight to be the number of symbols in the clause. scott applying the false preference strategy with a safe-weight of s tests each kept 
slaney 


value of safety weight s 
figure 1: false preference strategy 
clause before its insertion in the set of support and adds s to its weight if it is true in the current model. this causes selection of 'safe' clauses to be delayed. figure 1 shows the effect on otter's proof search. note that the scale is logarithmic. the case 1 = 1 is just otter  since dynamic semantic resolution was not used in this experiment. an improvement of almost two orders of magnitude is possible  but only if the choice of s is right. at present  no automatic method of finding an appropriate value for s is known  though it seems likely that the optimal value will be similar for cognate problems. 
1 	remarks 
scott brings semantic information into the service of forward chaining resolution proof search. looked at abstractly  this is obviously a move in the direction of intelligence. otter is powerful but blind. when asked to prove axiom  1  in the lukasiewicz implicational calculus problem  it starts performing exactly the same search as it does when asked for a proof of axiom  1 . in other words  it never refers to the goal except to check whether the proof is finished  so the goal has no effect on the search. the really remarkable fact is that such a strategy works at all! the effect of injecting models is to enable scott to look ahead. even a crude model carries information. whether through the false preference strategy  semantic resolution of a combination of the two  this information allows the goal to affect the direction of the search. in cases where this leads to incompleteness1 or inefficiency  it can be disabled since scott has otter as a sub-program  so nothing is lost in the move from 
1
　　scott's dynamic semantic resolution is obviously complete where the rule of inference is binary resolution  but incomplete in general for hyper-resolution and the like. its completeness for condensed detachment is an open question. 
1 	automated reasoning 
otter to scott. moreover  the addition a complex set of tools to otter  has opened new possibilities for heuristics on which research may usefully be focussed. 
   in theorem proving  there are no magic bullets. no one technique gives easy solutions to all problems. dynamic semantic resolution and the false preference strategy are like most other worthwhile ideas in the field in that they work spectacularly in a few cases and solidly across a fair range  have little or no effect in other cases and sometimes make matters worse. hence the present paper issues no claim to have solved all the problems. indeed  in that it opens questions about the effectiveness of semantic resolution relative to problem-specific models it may be taken to have posed some new ones. the significance of so doing depends in part on whether these are interesting problems. my feeling  for what it is worth  is that if we could explain both the successes and the failures of scott in its various configurations then we should understand the heuristics of first order theorem proving better than we do. meanwhile  whether it be seen as a theorem prover of a new type or simply as otter with yet another optional add-on  scott is both an intriguing departure and a power tool. 
