 
self-explanatory simulators have many potential applications  including supporting engineering activities  intelligent tutoring systems  and computer-based training systems. to fully realize this potential requires improving the technology to efficiently generate highly optimized simulators. this paper describes an algorithm for compiling selfexplanatory simulators that operates in polynomial time. it is capable of constructing self-explanatory simulators with thousands of parameters  which is an order of magnitude more complex than any previous technique. the algorithm is fully implemented  and we show evidence that suggests its performance is quadratic in the size of the system being simulated. we also analyze the tradeoffs between compilers and interpreters for self-explanatory simulation in terms of application-imposed constraints  and discuss plans for applications. 
1. 	introduction 
self-explanatory simulators  1 1  integrate qualitative and quantitative knowledge to produce both detailed descriptions and causal explanations of a system's behavior. they have many potential applications in intelligent tutoring systems and learning environments  1  1  and engineering tasks  1 . one step towards realizing this potential is developing techniques that can operate efficiently on systems involving many hundreds of parameters  so that  for instance  complex training simulators can be generated automatically. this paper describes a polynomial-time method for compiling self-explanatory simulators and shows that it operates successfully on models larger than most industrial applications require. this paper considers only initial-value simulations of lumped-element  ordinary differential-algebraic  systems. 
   section 1 reviews the basics of self-explanatory simulators. section 1 describes our new polynomial-time compilation technique. section 1 outlines the complexity analysis and section 1 summarizes empirical results  including evidence that its performance is quadratic in the size of the system being simulated. section 1 identifies tradeoffs in constructing self-explanatory simulators in light of task requirements. section 1 outlines our plans for future work. 
1. self-explanatory simulation: the basics 
traditional numerical simulators generate predictions of behavior via numerical computation using quantitative models of physical phenomena. most simulators are written by hand  although an increasing number are generated by 
1 	qualitative reasoning and diagnosis 
brian falkenhainer modeling research technology area xerox wilson center ms 1e 1 phillips road  webster  ny  1  usa 
domain-specific toolkits. modeling decisions  such as what phenomena are important to consider  how does the phenomena work  how can it be modeled quantitatively  and how to implement the quantitative model for efficient computer solution  are mostly made by hand. domain-specific toolkits provide reasonable solutions to the last two problems  and with appropriate libraries they can simplify the first problem. however  the choices of how to translate the physical description into the conceptual entities supported by the toolkit  and which quantitative model to use from the library to model an entity  are still made by hand. moreover  no existing toolkit provides the intuitive explanations used by scientists and engineers to describe how a system works. these intuitive  qualitative descriptions serve several important purposes in building and working with simulations. first  they guide the formulation of quantitative models by identifying what aspects of the physical situation are relevant. second  qualitative descriptions are used to check the results of a simulation  to ensure that it  makes sense.  thus two advantages of self-explanatory simulation are increased automation and better explanations . 
   self-explanatory simulators harness the formalisms of qualitative physics to automate the process of creating simulators. given an initial physical description  a qualitative analysis of the situation reveals what conceptual entities are relevant to the task and what causal factors affect a parameter under different circumstances. this information is then used  in concert with quantitative information in the domain theory  to construct numerical simulation programs for the system. by incorporating explicit representations of conceptual entities  such as physical processes  in the simulator  causal explanations can be given for the simulated behavior. moreover  the qualitative representations allow automating some of the  reality checks  that an expert would apply in evaluating a simulation. for instance  a simulation of a fluid system which reported a negative amount of liquid in a container is not producing realistic results. this ability is called self-monitoring. 
   figure i shows a typical architecture for selfexplanatory simulators. the state vectors are arrays of floating point and boolean values describing the state of the system at a particular point in time. the floating point values represent the values of continuous parameters  such as pressure and velocity. the boolean values represent the validity of statements about the physical system at that time  e.g.  if liquid exists inside a container or if a particular physical process is acting. given a state vector and a time 
increment  the evolver generates a state vector representing the state of the system after that time increment. 
　　in many physical systems  the equations governing the temporal evolution of the system's behavior can themselves change  as when phase changes occur or flows start and stop. these changes occur at limit points  which mark the boundaries between qualitatively distinct behaviors. the transition finder  again see figure 1  detects the occurrence of limit points. the controller uses the transition finder and evolver to  roll back  the simulation to include all limit points in the simulated behavior. this increases the accuracy of the simulation because it ensures that the appropriate sets of equations are always used  and increases the accuracy of explanations because it ensures that causally important events  e.g.  reaching a phase transition  are included in the simulated behavior. the controller also is responsible for recording a concise history describing the system's qualitative behavior over time. this concise history is used in conjunction with a structured explanation system  to provide hypertext explanations of the system's behavior over time  including physical  causal  and mathematical descriptions. the nogood checker generates a warning if qualitative constraints are violated by a state vector  thus providing self-monitoring. 
　　the first systems to generate self-explanatory simulators were compilers  doing all reasoning off-line to produce software that approached the speed of traditional simulators while providing causal explanations and self-monitoring. these compilation strategies were computationally expensive. simgen mkl  used envisioning  an exponential technique  for its qualitative analysis procedure. 
　　simgen mk1  exploited the observation that simulation authors never explicitly identify even a single global qualitative state of the system being simulated  hence qualitative simulation is unnecessary for simulation construction. qualitative analysis is still necessary to determine what physical phenomena are relevant  for instance  but most exponential reasoning steps could be eliminated. simgen mk1 could construct simulators of systems larger than any envisioning-based system ever could  i.e.  involving up to hundreds of parameters   such as a twenty stage distillation column . this advance in capabilities was not free: the tradeoff was that some explanatory capabilities  i.e.  efficient counterfactual reasoning  and selfmonitoring capabilities  i.e.  the guarantee that every numerical simulator state satisfied a legal qualitative state  were lost in moving from simgen mkl to simgen mk1. as section 1 explains  even simgen mk1 was subject to combinatorial explosions  because it was based on an atms. the techniques in this paper trade away more selfmonitoring to achieve polynomial-time performance. 
   an alternative to compiling self-explanatory simulators is to build interpreters that interleave model-building  model translation into executable code  and code execution  1 . for example  pika  uses mathematica and a causal ordering algorithm to decompose a set of equations into independent and dependent parameters and find an or-
der of computation. every state transition which changes the set of applicable model fragments reinvokes this reasoning to produce a new simulator.   uses an incremental constraint system to minimize this cost.  one motivation for 

interpreters was the perceived slowness of compiler techniques; by only building models for behaviors that are known to be relevant  presumably the overall time from formulation of the problem to the end of simulation would be reduced  even though the simulation time itself might be longer due to run-time reasoning. such systems are not themselves immune from combinatorial explosions  and have never been tested on examples as large as compilers  c.f.    but on small examples interpreters can exhibit impressive performance. 
　　it should be noted that the claim that pika is  1 times faster than simgen m k 1    is problematic  for three reasons. first  the domain theories used by each system were completely different. pika used just two model fragments  with built-in quantitative models containing example-specific numerical constants. this is not realistic. by contrast  in keeping with the goal of increased automation  the domain models used in simgen m k 1 were similar to others used in qualitative physics  with quantitative information added modularly. for instance  parameters such as fluid and thermal conductances and container sizes and shapes are explicit variables in our models that can be set by the simulation user at run-time. second  the hardware and software environments used by the two systems were completely different  making the comparison figures meaningless. finally  the factor of 1 claimed is based on one example only; the other example for which even roughly comparable data is available shows a difference smaller by a factor of 1. 
1. simgen m k 1 : compiling self-explanatory simulators in polynomial-time 
　　we have developed a polynomial-time algorithm for compiling self-explanatory simulators. the improvements in complexity are purchased at the cost of reduced selfmonitoring and less compile-time error checking. however  the ability to quickly generate simulators for systems containing thousands of parameters suggests that these costs are worth it. here we outline the algorithm and explain how it works. we begin by describing why using an atms  was the source of exponential behavior in simgen m k 1 . next we examine the inference services needed to generate simulators  and show polynomial-time methods for achiev-
ing them. finally  we outline the algorithm. 
f1rbus and falkenhainer 

1 atms: the exponential within 
atms' are often used in qualitative reasoning when many alternative situations are being explored . a qualitative state can be defined as a set of assumptions and their consequences  so that states  and partial states  can be concisely represented by atms labels . the labels in an atms database provide a simple and elegant inferential mechanism for simulation generation. for instance  the code needed to generate the truth of a proposition could be generated by interpreting the label as a disjunction of con-
junctions  with each assumption becoming a procedural test.  for instance  the boolean corresponding to a particular liquid flow occuring might be set to true if the boolean corresponding to the fluid path being aligned was true and the numerical value for the pressure in the source were greater than the numerical value for the pressure in the destination.  labels for causal relationships were used to infer what combinations of them could occur together  and hence what mathematical models were needed.  for instance  there can be several distinct models for flow rates  depending on whether or not one is considering conductances  but no two of these models can ever hold at the same time.  using labels made certain optimizations easy  such as proving that two distinct ordinal relationships were logically equivalent and thus allowing the same test to be used for each  which enhances reliability.  for example  in a spring-block oscillator the relationship between the length of the spring and its rest length determines the sign of the force.  
   unfortunately  even when we did not perform qualitative simulation at all  the number of environments generated by the atms grew exponentially with the size of the system modeled. empirically  we found that the source of this growth was the transitivity inference system  whose job it is to infer new ordinal relationships via transitivity and mark as inconsistent combinations of ordinal relations that violate transitivity. this makes sense because dependency networks in which assumptions are justified by other assumptions  and especially those containing cycles  lead to exponential label growth . the majority of assumptions in a typical analysis are ordinal relations  and cyclic dependencies are inherent in transitivity reasoning. transitivity reasoning cannot be avoided when using an atms in simulation generation  because without it the labels will include impossible combinations. we conclude that the atms must be abandoned to generate simulators in polynomial time. 
1 how to get what you really need without exponential performance 
what is the minimum reasoning needed to generate a simulator for a physical scenario   1  the model fragments of a domain theory must be instantiated to identify the relevant physical and conceptual entities and relationships  e.g.  the existence of contained fluids and phase change processes .  1  the causal and quantitative relationships that follow from them must be determined  to generate the appropriate causal accounts and mathematical models  e.g.  models that allow the level of a liquid to be computed given its mass  density  and specifications of its container .  1  the truth values of propositions corresponding to these relationships 
1 	qualitative reasoning and diagnosis holding and/or entities existing  e.g. whether the liquid exists at a particular time  and if so  what processes are acting on it  must be inferred to control the operation of the quantitative models. the performance of any compiler and the quality of the code it produces depends on how these questions are answered. for instance  if the shape of a container can be fixed at compile-time  then the model for liquid level depending on that shape can be  hard-wired   but if the shape is unknown  a run-time conditional must be inserted and code representing alternative models generated. 
   three techniques provide the inferential services needed for polynomial-time simulation generation.  1  reification of antecedents: when instantiating model fragments  create explicit assertions concerning their antecedents  in addition to installing the appropriate dependency network. one example is 
    l i q u i d - f l o w 	 c-s water l i q u i d f  p1 g  
:antecedents 
	 :and 	   	 a  amount-of-in water l i q u i d f   	zero  
 aligned pi  
	   	 a  pressure 	 c-s water l i q u i d f     
 a  pressure g        
    the creation of such assertions is automatic  and is transparent to the domain modeler.  such reified antecedents are also generated for causal relations and mathematical models. these antecedent assertions are used by the compiler in generating truth value tests for propositions.  1  symbolic evaluation: if the truth value of a proposition is known at compile-time  it is presumed to hold universally. for instance  if a valve is known to be open at compiletime  the simulator produced should always presume the valve is open  but otherwise the simulator should include explicit tests as to whether or not the valve is open and change its operation accordingly. a simple symbolic evaluator provides this information  using the reified antecedents and a logic-based tms . given a proposition  it returns true  false  or maybe  according to whether the proposition is universally true  universally false  or can vary at run-time.  1  deferred error checking: finding errors at compile-time can require exponential work. one example is proving that exactly one of the quantitative models for a specific phenomena must hold at any given simulated time. such exponential inferences can be avoided by substituting run-time conditionals. for instance  if there are n mathematical models of a phenomena  insert a conditional test that runs the appropriate model according to the simulator's current parameters  and signals an error if no model is appropriate. this reduces self-monitoring  in that finding such problems at compile-time would be useful. however  this solution is common practice in building standard simulators when its behavior might enter a regime for which the author lacks a good mathematical model. 
1 the simgen m k 1 algorithm 
the algorithm is outlined in figure 1. it is very similar to 
simgen m k 1  described in    so here we focus on how the above techniques are used. 
step 1: creation of the scenario model 
as before  we assume domain theories are written in a com-

positional modeling language  using qualitative process theory  for their qualitative aspects. since the cost of qualitative reasoning was the dominant cost in simgen mk.1 and m k 1   the tradeoffs in this step are crucial. the source of efficiency in interpreters like pika  and dme  is that they appear to do no qualitative reasoning beyond instantiating model fragments directly corresponding to equations. 
   step 1 uses a qualitative reasoner to instantiate model fragments and draw certain trivial conclusions  i.e.  if a b then a=b . no transitivity reasoning  influence resolution or limit analysis is attempted. we use tgizmo  a publicly available qp implementation  for our qualitative reasoner. we modified it in two ways. first  the patterndirected rules that implement many qp operations were simplified  stripping out inferences not needed by the compiler. second  we implemented antecedent reification by modifying the modeling language implementation to assert antecedents explicitly in the database as well as producing ltms clauses. 
step 1: constructing the simulator's state 
here the results of qualitative analysis are harvested to specify the contents of state vectors  the concise history  and 
1. create scenario model by instantiating model fragments from domain theory 
1. analyze scenario model to define simulator state vector and constituents of concise history and structured 
explanation system. 
1 extract physical and conceptual entities 
1 define boolean parameters for relevant propositions 
1 define numerical parameters and relevant comparisons 
1 extract influences to create causal ordering 
1. write simulator code 
1 simplify antecedents of boolean parameters 
1 compute update orders 
1.1 for numerical parameters  use causal ordering 
1.1 for boolean parameters  use equivalences and dependencies 
1 write evolver code 
1 write transition finder code 
1 write nogood checker code 
1 write structured explanation system 
figure 1: the s1mgen mk1 algorithm 

the explanation system. the numerical parameters are the quantities mentioned in the scenario model. boolean parameters are introduced for relevant propositions: the existence of individuals  exists  and quantities  quantity   the status of processes and views  active   and any ground propositions mentioned in their antecedents  recursively  except ordinal relations  which are computed by comparing numerical values . each boolean parameter has an associated antecedents statement  a necessary and sufficient condition for the truth of the corresponding proposition. 
   as noted above  any proposition known to be true in the scenario model is presumed to hold universally over any use of the simulator. the compiler does not generate simulator parameters for such propositions  although they are still woven into the explanation system appropriately. every proposition whose truth value is not known in the scenario yields a boolean parameter whose value must be ascertained at run-time. this technique allows the compiler to produce tighter code by exploiting domain constraints and user hints. the symbolic evaluator decides what propositions are static and simplifies antecedents containing them. 
step 1: writing the simulator code 
to achieve flexibility compositional modeling demands decomposing domain knowledge into small fragments. this can lead to long inference chains  which if proceduralized naively  would result in simulators containing redundant boolean parameters and run-time testing. step 1 simplifies inference chains in order to produce better code. we use two simplification techniques;  1  symbolic evaluation exploits compile-time knowledge to simplify expressions and  1  a minimal set of boolean parameters is found by dividing them into equivalence classes  based on their antecedents. that is  if a proposition a depends only on b  and b in turn depends only on c  and c either has an empty antecedent or an antecedent with more than one ground term  then a  b  and c would be in the same equivalence class  and c would be its canonical member. each equivalence class is represented in the simulation code by a single boolean parameter  although all original propositions are retained in the explanation system to maintain clarity. 
   in step 1  the state space assumption  common in engineering and satisfied by qp models   guarantees we can always divide the set of parameters into dependent and independent parts  with the independent parameters being those which are directly influenced  or uninfluenced  and with the dependent parameters computed from them. to gain a similar guarantee for boolean parameters we stipulate that the domain theory is condition grounded   an assumption satisfied by all domain theories we have seen in practice. an independent boolean parameter mentions no other boolean parameters in its antecedents. it could be universally true or false  or its value could be determined at run-time by ordinal relations  e.g.  the existence of a contained liquid depending on a non-zero amount of that substance in liquid form in the container  or by the simulation user's assumption  e.g.  the state of a valve . a boolean parameter that is not independent is dependent. the logical dependencies between boolean parameters define an ordering relation that can be used as the order of computation for 
them. 
   the overall structure of the code produced by the compiler in steps 1 through 1 is the same as in simgen mk1. the only change in step 1 is taking into account the simplifications in the boolean parameters  which is simple so we ignore it here. in evolvers  the effects of direct influences are calculated first to estimate derivatives  the dependent numerical parameters are then updated  followed by the boolean parameters. the main impact of restricted inferencing in writing evolvers arises in selecting quantitative models for updating dependent numerical parameters. for instance  a domain theory may have two quantitative mod-

els for the level of liquid as a function of mass  depending on whether the container is cylindrical or rectangular. if the compiler knows the shape of the container  via symbolic evaluation  it can install the appropriate model  otherwise it must provide both models in the simulator and write a run-
time conditional. the compiler must also handle models in which the equations governing a quantity vary over time. in simgen mk1 these cases were handled by using influence resolution to see what combinations of qualitative proportionalities could co-occur  constructing appropriate quantitative models for each combination or signaling a compiletime error if the domain theory failed to include an appropriate quantitative model. in simgen mk1 we instead retrieve all the quantitative models for a parameter not ruled out via symbolic evaluation and write code that selects the relevant model based on evaluating the models' antecedents in the current state vector. the evolver code includes a test for none of the known models being relevant  and generates 
a run-time error in such cases. 
   in generating transition finders  restricted inference can lead to moot run-time tests  corresponding to physically impossible transitions. however  symbolic evaluation catches most of them  and this is not a serious drawback because inequality tests are very cheap. generating nogood checkers is simplified: previous compilers generated code based on atms nogoods to detect impossible behaviors. much effort was wasted filtering the nogoods  since the vast majority of them were transitivity violations  which are irrelevant when ordinal relations are computed from known numerical parameters. simgen m k 1 simply uses the symbolic evaluation procedure to see what ordinal relations are known to be impossible and test for those. 
1. complexity analysis 
   a detailed complexity analysis is beyond the scope of mis paper  but see  ; here we settle for proving that the algorithm is polynomial. 
step 1: the only exponential behavior in previous compilers occurred in mis step  so its complexity is crucial. the time complexity is the sum of the time to instantiate model fragments and the time to draw conclusions with them. the cost of instantiation can be decomposed into two factors: the cost of pattern matching  and the size of the description produced by the operation of the system's rules. the cost of pattern-matching is polynomial in the number of antecedents . we assume that both the scenario model and the domain theory are finite  and that the number of new entities introduced by the domain theory for any scenario model is a polynomial function of the size of the scenario model. domain theories with exponential  or even unbounded  creativity are possible in theory   but never appear in practice. 
   the number of clauses instantiated about any particular statement is bounded by a polynomial  since it is a function of  a  the number of relevant domain theory statements  which is always small and certainly independent of the size of the scenario and  b  the number of entities introduced is polynomial. since the work of instantiation is the product of the number of instantiations and the work to perform each  the instantiation process is polynomial-time. furthermore  the dependency network so created is polynomial 
1 	qualitative reasoning and diagnosis 

in size  as a function of the size of the domain theory and 
scenario model. this means that the cost of inference remains polynomial in these factors  since we use an ltms  for which the cost of inference is worst-case linear in the size of the dependency network . we thus conclude that the time and space complexity of this step is polynomial. 
step 1: most of the work in this step consists of fetching information from the tgizmo database and constructing corresponding internal compiler datastructures  which is obviously polynomial time. the only other potentially expensive part of this computation is the symbolic evaluation procedure. symbolic evaluation of a ground term is performed by checking its ltms label  which is constant time. symbolic evaluation of a compound expression is a recursive analysis of the structure of the expression  ending in ground terms. the size of expressions is determined by 
the antecedents in the domain theory  and thus for any domain model a maximum size can be found for such expressions independent of the size of the scenario model. ergo symbolic evaluation is also polynomial in the size of the scenario model. 
   step 1: each of these computations involves simple polynomial-time operations  see    the most expensive being sorting the numerical parameters via the causal ordering  sorting the boolean parameters via logical dependencies  and computing the equivalence classes for boolean parameters. all of these are simple polynomial-time operations  operating over datastructures whose size is polynomial in the initial scenario description  so they are polynomial time as well. 


   s img en m k 1 is fully implemented  and has been tested successfully on the suite of examples described in . in all cases it is substantially faster than simgen mk1  as table 
i shows. the simulators it produces  like those of simgen mk1  operate at basically the speed of a traditional numerical simulator  with the only extra run-time overhead being the maintenance of a concise history for explanation generation. currently the compiler's output is common lisp without any numerical declarations  and even with this performance handicap  the simulators it produces run quite well on even small machines  i.e.  macintosh powerbooks . 
   to empirically demonstrate that simgen mk1's performance is polynomial time  we generated a set of test examples similar to those used in . that is  a scenario description of size n consists of an n by n grid of containers  connected in manhattan fashion by fluid paths. figure 1 illustrates for the three by three case. we generated a sequence of scenario descriptions  with n ranging from 1 to 1.  the reason we chose 1 as an upper bound is that the simulator which results contains just over 1 parameters  which is roughly three times the size of the steamer engine room numerical model .  extending the domain theory in   contained liquids include mass  volume  level  pressure  internal energy  and temperature as dynamical parameters  as well as other static parameters  e.g.  boiling temperature  specific heat  density . containers can be either cylindrical or rectangular  with appropriate numerical dimensions in each case. the liquid flow process affects both mass and internal energy. we then ran the compiler to produce simulators for each scenario  to see how its performance scaled. the results are show in table 1. in an n x n grid scenario  there are n containers and 1 n1~n  fluid paths  so the numbers of parts in these examples ranges from 1 to 1. the count for quantities includes both static and dynamic parameters  and the count for booleans includes both conditions controllable by the user  e.g.  the state of valves  and qualitative state parameters  such as whether or not a particular physical process is occurring. the proposition count is the number of statements in the simulator's explanation system. 
1. tradeoffs in self-explanatory simulators: compilers versus interpreters 
different applications entail different tradeoffs: some potential users have powerful workstations and can afford the best commercial software  e.g.  many engineering organizations   and some potential users have only hand-me-down computers and publicly available software  e.g.  most us schools . here we examine tradeoffs in self-explanatory simulation methods with respect to potential applications. 

table 1: simgen mk 1 data  linear chain of containers  ibm 
rs/1  1mb ram  lucid common lisp 1  
   broadly speaking  the computations associated with selfexplanatory simulations can be divided into three types:  1  model instantiation  in which the first-order domain theory is applied to the ground scenario description   1  model translation  in which the equations associated with a state are identified  analyzed  and converted into an executable form  and  1  model execution  in which numeric integration is used to derive behavior descriptions from initial values. the choice of compiler versus interpreter is mainly a choice of how to apportion these computations  with tradeoffs analogous to those of programming language interpreters and compilers. interpreters are more suited for highly interactive circumstances  where more effort is spent changing models than running them. exploratory and rapidprototyping environments for scientists and engineers formulating and testing models of new phenomena  and highly interactive construction kit simulation environments for education may be two such applications. compilers are more suitable for circumstances where the additional cost of compilation is offset by repeated use of the model  or when the environment for model execution cannot support the resources required by the development environment. compilers seem to have the edge in engineering analysis and design  where a small number of models are used many times  e.g.  in numerical optimization   and most educational software and training simulators  where maximum performance must be squeezed out of available hardware. 
   the cost of model generation is dominated by the expressiveness of modeling language and the amount of simulator optimization performed. in simgen mk1  the order of computation is specified as an inherent part of the domain theory due to the causal ordering imposed by qualitative process theory influences. thus  no algebraic manipulation is required at model generation time. other systems allow a domain theory to contain equations in an arbitrary form. thus  the equations must be sorted  using a causal ordering algorithm   and symbolically reformulated to match that sort. this technique provides the ease of using arbitrarily arithmetic expressions  but can lead to expensive processing for some classes of equations. furthermore  the time taken to switch models  1 seconds for a small model on a fast workstation  even with pika's incremental constraint algorithm suggests that switching delays for large models  e.g.  training simulators  could be unacceptable. 
   another way in which the modeling language affects potential applications is in the kinds of explanations that can be generated. domain theories that explicitly represent conceptual entities as well as equations can provide better explanations than those which do not. while in a few domains  e.g.  electronics  expert causal intuitions are not strongly directional  in many domains  e.g.  fluids  mechanics  thermodynamics  chemistry  etc.  expert causal intui-
tions are strongly directed   and there is no a priori guarantee that the accounts produced by causal ordering will match expert intuitions . using equation-based models reduces the overhead of formalizing expert intuitions  but at the cost of reduced explanation quality. using explicit qualitative representations provides an additional layer of explanations  but at the cost of increased domain theory development time. interestingly  tgizmo accounts for less than 1% of simgen mk1's time  so the penalty for using rich  compositional domain theories appears to be quite small. 
1. discussion 
previous work on self-explanatory simulation produced software that could compile systems up to a few hundred parameters. this paper describes a new algorithm for compiling self-explanatory simulators that extends the technology to systems involving thousands of parameters. we have shown  both theoretically and empirically  that selfexplanatory simulators can be compiled in polynomial time  as a function of the size of the input description and the domain theory. this advance was made possible by the observation that minimizing inference could substantially improve performance . these gains are not without costs: simgen mk1 does less self-monitoring and less compile-time error detection than previous versions  and the simulators produced can contain dead code. however  no explanatory capability is lost  and the ability to scale up to very large systems outweighs these drawbacks for most applications. even our current research implementation of simgen mk1 can  running on a powerbook  compile new simulators for small systems reasonably quickly. 
   one open question concerns the possibility of recovering most  if not all  of the self-monitoring and error checking of previous compilers by the judicious use of hints. many programming language compilers accept advice in the form of declarations. qualitative representations can be viewed as declarations  providing advice to self-explanatory simulators at the level of physics and mathematics rather than code. perhaps domain-specific and example-specific hints could replace the functionality provided by inference in earlier compilers. 
   we now believe that the remaining hurdles to using selfexplanatory simulators in applications are building domain theories and software engineering. we are working on two applications. first  we are building an articulate virtual laboratory for engineering thermodynamics  containing the kinds of components used in building power plants  refrigerators  and heat pumps  using a domain theory developed in collaboration with an expert in thermodynamics . second  we are also developing a tool for building training simulators  such as a self-explanatory simulator for a shipboard propulsion plant  to finally fulfill one of the early goals of qualitative physics . 
1. acknowledgments 
this research was supported by grants from nasa langley 
research center and from the computer science division of the office of naval research. we thank franz amadu for supplying us with a sample pika domain theory. 
1. 