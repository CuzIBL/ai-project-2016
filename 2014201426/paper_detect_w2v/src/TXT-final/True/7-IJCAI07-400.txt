
markov models are commonly used for joint inference of label sequences. unfortunately  inference scales quadratically in the number of labels  which is problematic for training methods where inference is repeatedly preformed and is the primary computational bottleneck for large label sets. recent work has used output coding to address this issue by converting a problem with many labels to a set of problems with binary labels. models were independently trained for each binary problem  at a much reduced computational cost  and then combined for joint inference over the original labels. here we revisit this idea and show through experiments on synthetic and benchmark data sets that the approach can perform poorly when it is critical to explicitly capture the markovian transition structure of the large-label problem. we then describe a simple cascade-training approach and show that it can improve performance on such problems with negligible computational overhead.
1 introduction
sequence labeling is the problem of assigning a class label to each element of an input sequence. we study supervised learning for sequence-labeling problems where the number of labels is large. markov models are widely used for sequence labeling  however  the time complexity of standard inference algorithms  such as viterbi and forward-backward  scales quadratically with the number of labels. when this number is large  this can be problematic when predictions must be made quickly. even more problematic is the fact that a number of recent approaches for training markov models  e.g.  lafferty et al.  1; tsochantaridis et al.  1; collins  1   repeatedly perform inference during learning. as the number of labels grows  such approaches quickly become computationally impractical.
　this problem has led to recent work that considers various approaches to reducing computation while maintaining high accuracy. one approach has been to integrate approximateinference techniques such as beam search into the learning process  collins and roark  1; pal et al.  1 . a second approach has been to consider learning subclasses of markov models that facilitate sub-quadratic inference-e.g. using linear embedding models  felzenszwalb et al.  1   or bounding the number of distinct transition costs  siddiqi and moore  1 . both of these approaches have demonstrated good performance on a number of sequence-labeling problems that satisfy the assumptions of the models.
　a third recent approach  which is the focus of this paper  is sequential error-correcting output coding  secoc   cohn et al.  1  motivated by the success of error-correcting output coding  ecoc  for non-sequential problems  dietterich and bakiri  1 . the idea is to use an output code to  reduce  a multi-label sequence-learningproblem to a set of binary-label problems. secoc solves each binary problem independently and then combines the learned models to make predictions over the original large label set. the computational cost of learning and inference scales linearly in the number of binary models and is independent of the number of labels. experiments on several data sets showed that secoc significantly reduced training and labeling time with little loss in accuracy.
　while the initial secoc results were encouraging  the study did not address secoc's general applicability and its potential limitations. for non-sequential learning  ecoc has been shown to be a formal reduction from multi-label to binary label classification  langford and beygelzimer  1 . one contribution of this paper is to show that this result does not hold for sequential learning. that is  there are sequence labeling problems such that for any outputcode  secoc performs poorly compared to directly learning on the large label set  even assuming optimal learning for the binary problems.
　given the theoretical limitations of secoc  and the prior empirical success  the main goals of this paper are: 1  to better understand the types of problems for which secoc is suited  1  to suggest a simple approach to overcoming the limitations of secoc and to evaluate its performance. we present experiments on synthetic and benchmark data sets. the results show that the originally introduced secoc performs poorly for problems where the markovian transition structure is important  but where the transition information is not captured well by the input features. these results suggest that when it is critical to explicitly capture markovian transition structure secoc may not be a good choice.
　in response we introduce a simple extension to secoc called cascaded secoc where the predictions of previously learned binary models are used as features for later models. cascading allows for richer transition structure to be encoded in the learned model with little computational overhead. our results show that cascading can significantly improve on secoc for problems where capturing the markovian structure is critical.
1 conditional random fields
we study the sequence-labeling problem  where the goal is to learn a classifier that when given an observation sequence x =  x1 x1 ... xt  returns a label sequence y =  y1 y1 ... yt  that assigns the correct class label to each observation. in this work  we will use conditional random fields  crfs  for sequence labeling. a  sequential  crf is a markov random field  geman and geman  1  over the label sequence y globally conditioned on the observation sequence x. the hammersley-clifford theorem states that the conditional distribution p y |x  has the following form:
 
where z x  is a normalization constant  and Ψ yt 1 yt x  is the potential function defined on the clique  yt 1 yt . for simplicity  we only consider the pair-wise cliques  yt 1 yt   i. e. chain-structured crfs  and assume that the potential function does not depend on position t. as in  lafferty et al.  1   the potential function is usually represented as a linear combination of binary features:
.
the functions f capture the relationship between the label yt and the input sequence x; the boolean functions g capture the transitions between yt 1 and yt  conditioned on x .
　many crf training algorithms have been proposed for training the parameters λ and γ  however  most all of these methods involve the repeated computation of the partition function z x  and/or maximizing over label sequences  which is usually done using the forward-backward and viterbi algorithms. the time complexity of these algorithms is o t ，lk+1   where l is the number of class labels  k is the order of markov model  and t is the sequence length. so even for first ordercrfs  training and inferencescale quadratically in the numberof class labels  which becomescomputationally demanding for large label sets. below we describe the use of output coding for combating this computational burden.
1 ecoc for sequence labeling
for non-sequential supervised classification  the idea of error-correcting output coding  ecoc  has been successfully used to solve multi-class problems  dietterich and bakiri  1 . a multi-class learning problem with training examples { xi yi } is reduced to a number of binaryclass learning problems by assigning each class label j a binary vector  or code word  cj of length n. a code matrix m is built by taking the code words as rows. each column bk in m can be regarded as a binary partition of the original label set  bk y  （ {1}. we can then learn a binary classifier hk using training examples { xi bk yi  }. to predict the label of a new instance x  we concatenate the predictions of each binary classifier to get a vector h x  =  h1 x  h1 x  ... hn x . the predicted label y is then given by y  = argminj Δ h x  cj   where Δ is some distance measure  such as hamming distance. in some implementations h x  stores probabilities rather than binary labels.
　this idea has recently been applied to sequence labeling problems under the name sequential error-correcting output coding  secoc   cohn et al.  1   with the motivation of reducing computation time for large label sets. in secoc  instead of training a multi-class crf  we train a binary-class crf hk for each column bk of a code matrix m. more specifically the training data for crf hk is given by { xi bk yi  }  where bk y   =  bk y1  bk y1  ... bk yt   is a binary label sequence. given a set of binary classifiers for a code matrix and an observation sequence x we compute the multilabel output sequence as follows. we first use the forwardbackward algorithm on each hk to compute the probability that each sequence position should be labeled 1. for each sequence element xt we then form a probability vector h xt  =  p1 ... pn  where pk is the probability computed by hk for xt. we then let the label for xt be the class label yt whose codeword is closest to h xt  based on l1 distance. the complexity of this inference process is just o n ， t  where n is the codeword length  which is typically much smaller than the number of labels squared. thus  secoc can significantly speed inference time for large label sets.
　prior work  cohn et al.  1  has demonstrated on several problems that secoc can significantly speed training time without significantly hurting accuracy. however  this work did not address the potential limitations of the secoc approach. a key characteristic of secoc is that each binary crf is trained completely independently of one another and each binary crf only sees a very coarse view of the multiclass label sequences. intuitively  it appears difficult to represent complex multi-class transition models between yt 1 and yt using such independent chains. this raises the fundamental question of the representational capacity of the secoc model. the following counter example gives a partial answer to this question  showing that the secoc model is unable to represent relatively simple multi-label transition models.
　secoc counter example. consider a simple markov model with three states y = {1 1} and deterministic transitions 1 ★ 1  1 ★ 1 and 1 ★ 1. a 1-by-1 diagonal code matrix is sufficient for capturing all non-trivial codewords for this label set-i. e. all non-trivial binary partitions of y. below we show an example label sequence and the corresponding binary code sequences.
	y1	y1	y1	y1	y1	y1	y1
　y	1	1	1	1 b1 y  	1	1	1	1 b1 y  	1	1	1	1 b1 y  	1	1	1	1
given a label sequence y = {1 1 1 1 ...}  a firstorder markov model learned for b1 y   will converge to p yt = 1|yt 1 = 1  = p yt = 1|yt 1 = 1  = 1. it can be shown that as the sequence length grows such a model will make i.i.d. predictions according to the stationary distribution that predicts 1 with probability 1. the same is true for b1 and b1. since the i.i.d. predictions between the binary crfs are independent  using these models to make predictions via secoc will yield a substantial error rate  even though the sequence is perfectly predictable. independent first-order binary transition models are simply unable to capture the transition structure in this problem. our experimental results will show that this deficiency is also exhibited in real data.
1 cascaded secoc training of crfs
each binary crf in secoc has very limited knowledge about the markovian transition structure. one possibility to improve this situation is to provide limited coupling between the binary crfs. one way to do this is to include observation features in each binary crf that record the binary predictions of previous crfs. we call this approach cascaded secoc  c-secoc   as opposed to independent secoc  i-secoc .
　given training examples s = { xi yi }  let yi j  be the prediction of the binary crf learned with the j-th binary partition bj and yit j  be the t-th element of yi j . to train the binary crf hk based on binary partition bk  each training example  xi yi  is extended to   where each xit is the union of the observation features xit and the predictions of the previous h binary crfs at sequence positions from t l to t + r  l = r = 1 in our experiments  except position t:
x 
.
we refer to h as the cascade history length. we do not use the previous binary predictions at position t as part of xit   since such features have significant autocorrelation which can easily lead to overfitting. to predicty for a given observation sequence x  we make predictions from the first binary crf to the last  feeding predictions into later binary crfs as appropriate  and then use the same decoding process as i-secoc.
　via the use of previous binary predictions  c-secoc has the potential to capture markovian transition structure that isecoc cannot. our experiments show that this is important for problems where the transition structure is critical to sequence labeling  but is not reflected in the observation features. the computational overhead of c-secoc over isecoc is to increase the number of observation features  which typically has negligible impact on the overall training time. as the cascade history grows  however  there is the potential for c-secoc to overfit with the additional features. we will discuss this issue further in the next section.
1 experimental results
we compare crfs trained using i-secoc  c-secoc  and beam search over the full label set. we consider two existing base crf algorithms: gradient-treeboosting gtb   dietterich et al.  1 andvotedperceptron vp  collins  1 . gtb is able to construct complex features from the primitive observations and labels  whereas vp is only able to combine the observations and labels linearly. thus  gtb has more expressive power  but can require substantially more computational effort. in all cases we use the forward-backward algorithm to make label-sequence predictions and measure accuracy according to the fraction of correctly labeled sequence elements. we used random code matrices constrained so that no columns are identical or complementary  and no class labels have the same code word.
　first  we consider a non-sequential baseline model denoted as  iid  which treats all sequence elements as independent examples  effectively using non-sequential ecoc at the sequence element level. in particular  we train iid using isecoc with zeroth-order binary crf  i. e. crfs with no transition model. this model allows us to assess the accuracy that can be attained by looking at only a window of observation features. second  we denote by  i-secoc  the use of i-secoc to train first-order binary crfs. third  we denote by  c-secoc h   the use of c-secoc to train first-order binary crfs using a cascade history of length h.
　summary of results. the results presented below justify five main claims: 1  i-secoc can fail to capture significant transition structures  leading to poor accuracy. such observations were not made in the original evaluation of i-secoc  cohn et al.  1 . 1  c-secoc can significantly improve on i-secoc through the use of cascade features. 1  the performance of c-secoc can depend strongly on the base crf algorithm. in particular  it appears critical that the algorithm be able to capture complex  non-linear interactions in the observation and cascade features. 1  c-secoc can improve on models trained using beam search when gtb is used as the base crf algorithm. 1  when using weaker base learning methods such as vp  beam search can outperform c-secoc.
1 nettalk data set
the nettalk task  sejnowski and rosenberg  1  is to assign a phoneme and stress symbol to each letter of a word so that the word is pronounced correctly. here the observations correspond to letters yielding a total of 1 binary observation features at each sequence position. labels correspond to phoneme-stress pairs yielding a total of 1 labels. we use the standard 1 training and 1 test sequences.
　comparing to i-secoc. figures 1a and 1b show the results for training our various models using gtb with window sizes 1 and 1. for window size 1  we see that i-secoc is able to significantly improve over iid  which indicates that i-secoc is able to capture useful transition structure to improve accuracy. however  we see that by increasing the cascade history length c-secoc is able to substantially improve over i-secoc. even with h = 1 the accuracy improves by over 1 percentage points. this indicates that the independent crf training strategy of i-secoc is unable to capture important transition structure in this problem. c-secoc is able to exploit the cascade history features in order to capture this transition information  which is particularly critical in this problem where apparently just using the informationin the observation window of length 1 is not sufficient to make accurate predictions  as indicated by the iid results .
　results for window size 1 are similar. however  the improvement of i-secoc over iid and of c-secoc over isecoc are smaller. this is expected since the larger window size spans multiple sequence positions  allowing the model to capture transition information using the observations alone  making the need for an explicit transition model less important. nevertheless  both secoc methods can capture useful transition structure that iid cannot  with c-secoc benefiting from the use of cascade features. for both window sizes  we see that c-secoc performs best for a particular cascade history length  and increasing beyond that length decreases accuracy by a small amount. this indicates that c-secoc can suffer from overfitting as the cascade history grows.
　figures 1c and 1d show corresponding results for vp. we still observe that including cascade features allows c-secoc
　
	 a  window size 1 with gtb	 b  window size 1 with gtb

 1
 1	 1	 1	 1	 1	 1	 1	 1	 1
length of code words
 c  window size 1 with vp 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
length of code words
 d  window size 1 with vp 1 1figure 1: nettalk data set with window sizes 1 and 1 trained by gtb and vp.
　
to improve upon i-secoc  though compared to gtb longer cascade histories are required to achieve improvement. no overfitting was observed up to cascade history length 1. however  we were able to observe overfitting after code length 1 by including all possible cascade history bits  denoted by c-secoc all . all of our overfitting results suggest that in practice a limited validation process should be used to select the cascade history for c-secoc. for large label sets  trying a small number of cascade history lengths is a much more practical alternative than training on the full label set.
　gtb versus vp. c-secoc using gtb performs significantly better than using vp. we explain this by noting that the critical feature of c-secoc is that the binary crfs are able exploit the cascade features in order to better capture the transition structure. vp considers only linear combinations of these features  whereas gtb is able to capture non-linear relationships by inducing complex combinations of these features and hence capturing richer transition structure. this indicates that when using c-secoc it can be important to use training methods such as gtb that are able to capture rich patterns in the observationsand hence of the cascade features. comparing to beam search. here we compare the performance of c-secoc with multi-label crf models trained using beam search in place of full-viterbi and forwardbackward inference  which is a common approach to achieving practical inference with large label sets. for beam search  we tried various beam-sizes within reasonable computational limits. figure 1 shows the accuracy/training-time trade-offs for the best beam-search results and the best results achieved for secoc with 1 code-word bits and various cascade histories. the graph shows the test set performance versus the amount of training time in cpu seconds. first notice that gtb with beam search is significantly worse than for vp. we believe this is because gtb requires forward-backwardinference during training whereas vp does not  and this is more adversely affected by beam search. rather  c-secoc using
gtb performs significantly better than vp with beam search.
1 noun phrase chunking
we consider the conll 1 noun phrase chunking  npc  shared task that involves labeling the words of sentences. this was one of the problems used in the original evaluation of i-secoc. there are 1 class labels  which are combinations of part-of-speech tagging labels and npc labels  and 1 observation features for each word in the sequences. there are 1 sequences in the training set and 1 sequences in the test set. due to the large number of observation features  we can not get good results for gtb using our current implementation and only present results for vp.
　comparing to i-secoc. as shown in figure 1a  for window size 1  i-secoc outperforms iid and incorporating cascade features allows c-secoc to outperform i-secoc by a small margin. again we see overfitting for c-secoc for larger numbers of cascade features. moving to window size 1 changes the story. here a large amount of observation information is included from the current and adjacent sentence positions  and as a result the iid performs as well as any of the secoc approaches. the large amount of observation information at each sequence position appears to capture enough transition information so that secoc gets little benefit from learning an explicit transition model. this suggests that the performance of the secoc methods in this domain are primarily a reflection of the ability of non-sequential ecoc. this is interesting given the i-secoc results in  cohn et al.  1   where all domains involved a large amount of local observation information. iid results were not reported there.

	 1	 1	 1	 1	 1	 1	 1	 1
	cpu seconds	cpu seconds
	 a  window size 1	 b  window size 1
figure 1: nettalk: comparison between secoc and beam search.

	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
	length of code words	length of code words
	 a  window size 1	 b  window size 1
figure 1: npc data set with window sizes 1 and 1 trained by vp.　comparing to beam search. we compare the accuracy versus training-timefor models trained with secoc and with beam search  using the already described experimental setup. figure 1 shows that beam search performs much better than the c-secoc methods within the same training time using vp as the base learning algorithm. we believe the poor performance of c-secoc compared to beam search is that using vp does not allow for rich patterns to be captured in the observations which include the cascade features. we hypothesize  as observed in nettalk  that c-secoc would perform as well or better than beam search given a base crf method that can capture complex patterns in the observations.
1 synthetic data sets
our results suggest that i-secoc does poorly when critical transition structure is not captured by the observation features and that c-secoc can improve on i-secoc in such cases. here we further evaluate these hypotheses.
　we generated data using a hidden markov model  hmm  with 1 labels/states {l1 ... l1} and 1 possible observations {o1 ... o1}. to specify the observation distribution  for each label li we randomly draw a set oi of ko observations not including oi. if the current state is li  then the hmm generates the observation oi with probability po and otherwise generates a randomly selected observation from oi. the observation model is made more important by increasing po and decreasing ko. the transition model is defined in a similar way. for each label li we randomly draw a set li of kl labels not including li. if the current state is li then the hmm sets the next state to li with probability pl and otherwise generates a random next state from li. increasing pl and decreasing kl increases the transition model importance.
　 transition  data set. here we use po = 1  ko = 1  pl = 1  kl = 1  so that the transition structure is very important and observationfeatures are quite uninformative. figure 1a shows the results for gtb training using window size 1. we see that i-secoc is significantly better than iid indicating that it is able to exploit transition structure not available to iid. however  including cascade features allows c-secoc to further improve performance  showing that i-secoc was unable to fully exploit information in the transition model. as in our previous experiments we observe that c-secoc does overfit for the largest number of cascade features. for window size 1  graph not shown  the results are similar except that the relative improvements are less since more observation information is available  making the transition modelless critical. these results mirror those for the nettalk data set.
　 both  data set. here we use po = 1  ko = 1  pl = 1  kl = 1 so that both the transition structure and observation features are very informative. figure 1b shows results for gtb with window size 1. the observation features provide a large amount of information and performance of iid is similar to that of the secoc variants. also we see that c-secoc is unable to improve on i-secoc in this case. this suggests that the observation information captures the bulk of the transition information  and the performance of the secoc methods is a reflection of non-sequential ecoc  rather than of their ability to explicitly capture transition structure.
1 summary

	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
	cpu seconds	cpu seconds
	 a  window size 1	 b  window size 1
figure 1: npc: comparison between secoc and beam search using vp.

	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
length of code words
 a  window size 1 on  transition  data set	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
length of code words
 b  window size 1 on  both  data set 1figure 1: synthetic data sets trained by gtb.we uncovered empirical and theoretical shortcomings of independently trained secoc. independent training of binary crfs can perform poorly when it is critical to explicitly capture complex transition models. we proposed cascaded secoc and showed that it can improve accuracy in such situations. we also showed that when using less powerful crf base learning algorithms  approaches other than secoc  e.g. beam search  may be preferable. future directions include efficient validation procedures for selecting cascade history lengths  incremental generation of code words  and a wide comparison of methods for dealing with large label sets.
acknowledgments
we thank john langford for discussion of the counter example to independent secoc and thomas dietterich for his support. this work was supported by nsf grant iis-1.
