: 
in this paper we analyse how noise can affect knowledge 
acquisition from a machine learning perspective. we present some methods to detect and treat noise that goes beyond modulating numerical coefficients and show that noise cannot be viewed as a single entity. there are several different types of noise and noise is not only wrong information. 
i. introduction 
formalizing the knowledge needed to solve a real world problem is far from being a trivial task. as noted in  clancey  1   mc dermott  1   knowledge acquisition  ka  is not the process of transfering a mental model that lies somewhere within the brain of a human expert  but the familiar scientific and engineering problem of formalizing a domain for the first time. 
the classical methods to achieve this result is based on the domain expert's ability to explain his  or her  behavior: 
- a knowledge engineer interview the expert and attempts to formalize his knowledge 
- the expert himself is trained to construct a computable model that is extentionally equivalent to his own model 
this is usually a cumbersome process because human experts are trained to solve a task and not to explain how they obtained the solution. furthermore  the final model frequently contain bugs and is only representative of that specific expert  two different experts almost never agree on what should go in the knowledge base . this causes several problems in particular those connected with the system's maintenance. 
an alternative to the traditionnal methods is to use 
machine learning  ml  tools. from a set of examples of expertise  the learning system automatically constructs the rules. several experiments have shown that very good results can be obtained this way  see for instance  michalski & chilauski  1   quintan  1  . these techniques partially solve the problem of 
this research has been partially supported by the european 
economic community  esprit-aip contract p1  the instil project  and by the greco and prc  intelligence artificielle . the other partners of instil are gec research  uk  and cognitech  f . 
1 	knowledge acquisition 
getting the expert to formalize his rules. however  the expert still plays a critical role since he must provide the vocabulary needed to describe the events  the language of description   the domain specific properties  valid axioms and constraints  default and common sense knowledge and so on   and that he must validate  or even provide  the set of training examples. 
in a real application  the data given to the system by the expert  whether it is obtained manually or automatically  contains noise  wrong information  lack of information  unreliable information . we view noise as a critical problem that must be solved to build an accurate knowledge base. 
the traditional approach to handling noise consist in attaching numerical coefficients such as certainty factors  buchanan & shortlife 1  to the rules. however  we feel that this approach does not solve all the problems relating to noise and that it generates some new ones  for example  it is difficult for a human expert to relate to a large set of such rules and to generate or validate these . 
in this paper  we examin different types of noise  which affect ka at different steps and which yield different procedures of detection and treatment.. some methods developed here are common to both traditional and automatic ka techniques  for example crossexamination of expertise  but we actually studied how noise affects ka from a ml perspective. the ml tools that are used for this research are the decision-tree maker neddie  corlett  1  and the generalizer maggy  manago  1 . these are respectively descendants of the algorithms id1  quinlan  1  and agape  kodratoff & ganascia  1 . 
the material presented in this paper is based on a research project which aims at automatically building a knowledge base to diagnose tomato plant diseases and to compare the result with an existing knowledge base that contains over 1 rules  this work started in 
october 1 and is done in collaboration with the 
british research laboratory of the general electric 
company  the french company cognitech and the french  institut national de recherche en agronomie  . the example base contains observed examples and expert generated examples. 
we are currently studying another large scale application  air traffic control  and the preliminary results brought a few contributions to this paper. a case library is currently being built  work started in october 
1  in collaboration with the  centre d'etudes de la navigation a1rienne : by looking at a radar picture  the human expert describes what he sees and explains his behavior. some of the material presented here was and is still being tested on a smaller application in law. the examples come from the outcome of 1 cases in a court of law  that describe the legal actions and the duties of the mayor of a city  work started in fall 1 . 
ii. getting the initial knowledge 
knowledge located at different levels must be obtained from the human experts  alexander et al  1 . in this paper  we have divided the ka process into the following three steps: 
1  expert interaction + literature on the domain 1  concept formation from examples  learning full descriptions of the high/intermediary level concepts  1  rule learning  learning diagnostic rules and metaknowledge  
the last two use the ml tools.the reasons for going through the second stage will be explained in section ii.a.1. most of the methods that we will describe for detecting and treating noise rely heavily on the ability to go back and forth between the first and second components. 
note that we call concept a function f that partition the space of instances into the instances that verify the function  the set of instances whose image is true  and those that do not. a low level concept is the basic building block  for instance the symptoms   a high level concept is what we are trying to learn  for example the diseases  and an intermediary level concept is something in between that is  interesting   for example  the diseases caused by a fungus . it is important to emphasize that there is an intentional component in a concept  its purpose . 
we will not describe in details the second step  see  kodratoff & al  1   manago & kodratoff  1  . the basic idea is to grow a decision tree with neddie and to generalize the clusters with maggy to obtain full descriptions of the concepts. the resulting system is then model-driven which makes it more robust with respect to noise  fu  1 . 
ii.a. interaction with the experts 
ii.a.1. getting the initial vocabulary 
there is no universal method to obtain the initial vocabulary. the descriptors can be collected bv searching the literature  by interviewing an expert  regoczei & plantinga  1  and so on. in our application  plant pathology  we started with an existing expert system which was built by an expert and we asked another expert to filter off the irrelevant  noisy  descriptors. a descriptor like bird-eye-spot was converted into  two round spots  the darkest one in the center of the other. 
cross-examination between several sources of knowledge for fighting noise will come up several times in this paper. this is similar to a technique used in picture analysis  computer vision  signal processing  to filter noisy pixels. two pictures are taken and and-ed together to remove false pixels or or-ed together to fill missing pixels. we take a  picture  of the domain with an expert and compare it with  pictures  taken by other experts and/or by the persons who will use the expert system. 
the choice of descriptors depends on the underlying logic used by the system. to represent the examples and the rules we use a unit-based first order logic language  nilsson 1  chapter 1 . first order logic enables using low-level concepts such as red and fruit. intermediate concepts such as fruit x  & 
red x  will not be represented as red-fruit x  unless it is  interesting   symbolizing switchover points  fu & buchanan  1  or intermediary subgoals  kodratoff et al  1  . as noted in  clancey  1   intermediate knowledge provides better explanations capabilities for the system and thereby increase the understandability for the user. nevertheless  we emphasize the use of low-level concepts to enable communication and cross-examination between different experts. it also removes some of the initial bias introduced by the expert who provided the language of description  utgoff  1 . 
the choice of what is a constant and what is a predicate is usually independant of the application. when there is only one person named mary in the universe  mary can be a constant. if there are several persons. the choice of the low-level concepts  or primitives  varies from one application to the other even in the same domain. for example  in the world of blocks  if the application is to move blocks around as in shrdlu  winograd  1   cube is a valid primitive. 
on the other hand  if in the system is to perform analysis of visual scenes in that same world of blocks the concept of cube will be described in term of concave/convex interior/boundary lines  waltz   1  etc... identifying low-level concepts can be fairly difficult  in our ai-traffic control application the expert has a hard time describing the knowledge contained in the radar picture even in natural language . 
the initial choice of primitives to represent knowledge is necessarily ad hoc  schank & carbonell  1 . thus the choice of descriptors is never considered as final and is modified depending on the results. we are very sensitive to the importance of a good set of descriptors and its relation to noisy data. a good choice of descriptors enables all things to be said cleanly  hayes  1  and lot of problems connected to noise can be solved by improving the language of description. in these cases  using numeric uncertainty 

is not appropriate. 
ii.a.1. object-level knowledge 
during stept we also obtain some domain specific knowledge  a model . this is: 
- generic objects  all fruits have a color  a size  a texture etc...  
- default properties  leaves are normally green  - relations between objects that always hold  a leaf is part of the plant  yellow is a light color  - axioms  if there are a lot of spots on a leaf  then the size of the spots must be small  
there are various ways to represent background knowledge. we have chosen to encode it using a frame-based language. for example  the frame-unit leaves has a slot color filled with  $default  green  . this property may be inherited by a specific instance of that frame  or an isa descendant  when information is missing  noise . slot fillers may be simple attributes or procedural calls  deamons . for example  the color slot of the tomato-fruit frameunit is filled with an $if-needed deamon: -if the fruit is young  the procedure returns green   if it is mature it returns red. 
we have implemented all axioms as deamons to deduce missing low-level concepts  noise . axioms could also be implemented as constraints  if the color of the fruit is not red  then the fruit is not mature  to detect wrong information  noise  when a constraint is violated. we have not yet implemented this in the system. 
then we must obtain the training example base. this is achieved by filling up questionnaires. the information contained in these can then be generalized to produce characteristic descriptions  full descriptions  of the concepts. 
ii.a.1. using characteristic descriptions 
in terms of version space  mitchell  1   a 
consultation rule is a generalization in the g set: or set of most general consistent generalizations. a lull description is a generalization in the s set or the set of most specific generalizations that cover all the positive examples. in perfect domains  the positive examples exactly represent the sufficient conditions for belonging to a concept and the negative examples the necessary conditions. thus  the g and s sets meet when all examples are processed. as noted in  wittgenstein  1   fu & buchanan  1   this almost never occur with natural concepts. as a consequence  there is a  gap    the version space is not empty. we view the lack of training examples as a very important kind of noise in ml 
high-level concepts are not merely descriptions of the information provided by the training examples but they also have a predictive power for classifying unseen events. when the unseen event is less general than the complete description  for instance  the event  yellow 
1 	knowledge acquisition 
spot on leaves  is less general than  symptom on leaves   then using full descriptions or consultation rules will yield the same results. when the unseen event is in the version space  i.e. more general than the full descriptions and less general than the consultation rules  then by using full descriptions the system will not be able to classify it   but if it uses consultation rules it will. this is not a positive feature since we do not have any information on what the class of that event might be. there can even be conflicting consultation rules that all claim the event. 
a solution is to use full descriptions  or use consultation rules and check if the answer matches a full description  and to learn incrementally when an event is not recognized  as done in the version space method . the expert is asked afterward for the class of the event and the system updates its set of full descriptions in order to recognize similar events in the future. note that on the long run the result will not be as good as the ones obtained by processing the example base in one shot and will depend on the order in which the events are presented. 
another problem for ml tools with generalizing consultation rules and/or cross-examination of such rules  is that they implicitly contain strategies in the form of intermediary subgoals. consider the following example where the system recognizes vehicles such as cars  motorcycles and bicycles. 

	expert 	1 	experl 1 
fig. 1: two decision trees to classify vehicles 
two experts build the decision tress of fig. 1 or give the equivalent production rules representation  corlet  1   quinlqn  1 . the two experts have different strategies. one prefer to first find if the vehicle belongs to the class  vehicle with an engine  and the other to the class  vehicle with two wheels . there are various reasons why their strategy may differ  for instance one of them could live in pekin where there are a lot more bicycles than cars . both sets of rules are correct and usable by an expert system but from a learning point of view  there are no relevant similarities among these. 
the problem with consultation rules  is that they contain a single explanation  chain of reasoning  of why an element belongs to a certain class. since there can be several other valid explanations  two experts may come up with different ones. if the system is to learn from 

rules given by different experts  each one must provide full descriptions of the concept. intermediate knowledge and strategies are then represented explicitely. 
for instance  a full description of the concept car  is  a colored vehicle with four wheels  which weights between 1 lbs and 1 lbs  which has a windshield  an engine  a rear view mirror  a hood and soon 
full description contain all the information that is common to the positive examples  or to the clusters of positive examlples  but they are generalized descriptions. the system does not need to memorize all the examples encountered as it is the case in the exemplar theory  smith & medin  1 . 
from the set of full descriptions  it is possible to dynamically grow alternative decision trees when a user is not able to answer a question  noise . for example  what if the user does not know whether or not there was an engine in the first decision tree  the system could then grow the second tree an find another path leading to the solution. 
the usual way of dealing with the problems mentioned above is to attach certainty factors to each characteristics. for example  we would have rules such as door imply car  of .1   truck  cf 1  and so on. however this is not always the optimal solution considering that: 
- we have to find methods to combine these rules. this is not easy when the independance assumption does not hold  when low level concepts can be related . for example  if the rule door-handle imply car  cf .1   truck  cf .1  is fired after the previous one  it sould not modify the state of the consultation. 
- when an unseen event is located in the version space  the result of the consulation is as meaningless as if the system would have used flat consultation rules. however  since there is some uncertainty associated with the conclusion  the problem is hidden. - when the statistical information brought by the example base is irrelevant  as it is the case when we ask for experts full descriptions  the cf cannot be properly computed. even with a true case library  the statistical data can vary over time or changed when moved in a different environment. 
we now study how noise affects the training examples and the full descriptions of the concepts. 
iii. 	n o i s e i n a r u l e b a s e 
noise is present in a knowledge base when it does not truly reflect the environment we want to learn from.we define noise as being: 
- erroneous information 
- missing information and bad language of description 
- unreliable data 
there are different sources of noise  different effects of noise and different kinds of noise. hence  it is rather difficult to speak about noise as a single entity. note that we are not concerned with noise in the expert's strategies  expert wondering off on the wrong track  since we are interested in full descriptions. 
there are two separated issues which are: - forming concepts  learning full descriptions  when the training data contains noise 
- generating high-level concepts that are robust with respect to noise. 
iii.a. unreliable information 
in iii.a.  we assume in that the examples are correct and that the noise does not originate from the language of description. unreliable data is noise that naturally originates from the low-level concepts themselves. we have identified several kinds: 
1.a  concept is hard to see. 
for instance  a discriminating feature of  colletotrichum coccodes  is the presence of tiny black marks on the roots  less than one millimiter in size which are hard to see and often missed during consultation. 
1.b  concept polymorphy  symbolic noise  this happen when two low-level concepts have a nonempty intersection and may be confused. for example  when does dark grey stops being grey to become black  
1c  concept requires a high skill of expertise to be identified 
while an expert is not likely to confuse a brown spot with a rotting tissue  an unskilled user is. note that experts usually know when to carefully look for noisy concepts of type 1.a or 1.b and that noise in these categories often falls into category 1 .c. 
we can detect noisy concepts of the type 1 .a  1 .b by cross-examination of information given by several experts. when experts disagree on the value to give to a specific descriptor  then the associated concept is probably unreliable. likewise  if the experts and the users disagree  then the concepts probably require skill to be identified. this is why we ask both experts and users to fill up the questionnaires and we compare the results. in the questionnaires  experts are also asked to evaluate how noisy the low level concept are  very reliable  reliable  not reliable . 
1.d  concept is costly to identify when a concept is costly  we cannot rely on the fact that the information will be given. for example  if some information on the state of the roots is requested  then the plant must be killed to get the information. cost could also be associated time elapsed to obtain the information  in air-traffic control  the expert system must take decision before the planes crash into each others  and so on. the cost of performing a test is given by the expert in the questionnaires. 
1.e  uncertainty in the measure of an attribute when testing numerical parameters  the measuring instrument might not be 1 % accurate. the system should not rely on the outcome of a test like  ls a b  when a is in the neighborood of b  zucker. 1v clearly  when a gets close to b  we do not want to rely heavily on the outcome of the test. a solution is to 
. replace the test  a b  by the two tests  a b-a  and  a b+a  where a is given by the expert and ignore the outcome of the test when a is within a of b. 
the fundamental idea to treat unreliable data is to delay testing the concept. as a consequence  the noisy tests are performed lower down in the decision tree  and their influence is limited  or not performed at all when there are other alternative tests. another way to treat this noise is to favor clusters where unreliable information is generalised. thus  if in my clusters i have brown spots and brown necrosis  polymorphic lowlevel concepts   the characteristic description of the cluster will be brown symptom and it will not be noisy anymore. the result is that the final rules will be more robust in presence of unreliable information. 
it is often the case that noisy descriptors are important descriptors that allow discriminating between two similar high level concepts  polymorphy of high-level concepts . nevertheless it happens thats sometimes the experts wants to show off and use concepts that are very difficult to identify to reach a conclusion while another one would have been simpler and equally good. note that a common practice when using rules with numeric uncertainty is to lower the certainty factor of rules that contain unreliable premises and the expert does not trust the answer to the questions. again  this hides the problem as opposed to solving it in a  clean  manner. 
v  randomness of natural phenomena 
as surprising as it may sound  we do not consider this as being an important type of noise. indeed  when learning from examples we assume that the real live case will occur in the training set and do not have to worry about this type of noise. 
iii.b. wrong information 
wrong information is human introduced mistakes. we assume that it is not due to unreliable information. this can be: 
1.a  giving the wrong value to a descriptor for example  the expert might be distracted when he write the description of an example 
1.b  describing a class and attributing the description to another 
this can occur for the same reasons as 1.a. however  the effect for the learning system will be much worse. 
1.c  giving too many descriptors 
this can hadden when our evpert uses bv mistake a 
1 	knowledge acquisition 
plant which has several diseases as a reference for a specific disease. it can also happen when the plant presents natural imperfection with are not due to the disease and which are incorrectly identified as a symptom of the disease. finally  it also happens because people mention concepts that are irrelevant for psychological reasons  pazzani  1 . 
this can introduce inconsistencies in the knowledge base in which case we can easily detect it and request that the expert solve the problem. otherwise  errors in the positive examples will cause over-generalizations  fu  1 . 
a method to handle false positive and negative examples is to allow some uncertainty in the clusters or the rules. this numeric method has been succesfully used in rl  fu  1  or by performing tree-pruning  quinlan  1 . in the last one  the error in classification introduced by tree pruning is lower than the error of trees that find total classifications. 
1.d  noise in the background knowledge 
we are currently totally empty handed to detect or treat this type of noise. this might be detectable by analysing bad consultations  assuming we have a way of detecting bad consultations  which we do not . iii.c. incomplete information 
this can be: 
1a  forgetting an example 
by asking examples to several experts  this problem will be reduced since it is unlikely that all the experts forget the same example. however  as we have seen in section ii.b the system checks that the conclusion he obtain match a full description of a concept and learns incrementally if it does not. 
1.b  forgetting a relevant descriptor this can happen because of lack of attention  the concept is naturally noisy  see iv.a.  or is a default value  this is solved by inheriting properties in our frames . it also happens that sometime people simply do not to mention relevant features for no apparent reasons  pazzani  1 . 
by using questionnaires  we push the experts to give all the information. 
1c  giving a value for a descriptor which is too general 
this can be treated in a similar manner as when the expert forgets the descriptor 
1d  high level concept is  fuzzy  this can be caused by the fact that the outcome of a test which is needed in order to discriminate between some diseases is unknown  lack of information  high cost associated with a test as in 1 .d . 
for example  finding which virus causes a disease requires some lab tests. the results of these lab tests will never be known by the person who uses the system. hence  as far as the system is concerned  it will never be able to discriminate between different viral diseases. our solution to the problem consists in 
collapsing the classes into a single one virus . 
1.e  missing background knowledge this can be detected and treated when the experts review the characteristic descriptions that have been automatically generated. when they disagree with the generalization  they must explain why and usually this generates new axioms on the domain. 
1.f  missing descriptors in the vocabulary consider the following example: 
e1 :  x:  isa plant   age mature   &  spotsl:  isa spot   number several   color white  
 form concave   &  fruitl  isa fruit   &  facel: 
 part-of 	fruit1   is exposed-to-sun   	=  sunburn 
e1:  x:  isa plant   age mature   &  spotsl:  isa 
spot   number several   color yellow  
 form concave   &  fruitl  isa fruit   &  facel: 
 part-of 	fruit1   is exposed-to-sun   	=  sunburn 
by climbing the generalization frame  one could conclude that a caracteristic description of sunburn is: there are light colored spots on the face of the fruit exposed to the sun. 
ce :  x:  isa plant   age mature   &  spotsl: 
 isa spot   number several   color greybeige   evolute-into spots1   form 
concave   	&  spots1:  isa spots   color 
brown   &  fruitl  isa fruit   &  facel:  part-of fruitl   is exposed-to-sun   &  leavesl:  isa 
leaves   &  symptoml:  isa symptom   =  narcosis-of-fruit-extremity 
since grey-beige is also a light color  the negative example is covered. let us assume that the origin of the problem is the generalization of the color attribute. one can solve this problem by introducing an intermediary level node g1 in the frame such that white and yellow are sons of g1  g1 is a son of light-color and grey-beige is not a son of g1. the generalization then becomes 
color g1  spots1  which rejects the negative example. this is similar to rl symbolization of taxonomy points  fu & buchanan  1  and shift of bias  utgoff  1 . 
this might not be the optimal solution  try to find a meaning for g1 . the correct solution  obtained through interraction with the experts  is that white and yellow belong to another frame which had been overlooked  the translucent frame. white and yellow are not only colors but they also color loss  when exposed too long to the sun  the colors fade away and become translucent . while in some case one can clearly see that the white or yellow is a color and not a color loss  this means that the proceeding solution does not hold   in some other cases it is more difficult  in other word  we cannot introduce two new colors such as translucent-white and 
translucent-yellow .of course  multiple links of this sort generate problems for inheriting properties. 
in the present case  the sunburn disease   the fact there are no symptoms on leaves is also relevant while the fact that the color does not evolve into brown is not. the correct generalization will then be : there are translucent spots on the part of the fruits exposed to the sun and no symptoms on the leaves. 
iv. conclusion 
when learning from noisy data  it is often though that nothing but numerical coefficients can take into account 
this uncertainty. we do not claim that one should never use statistical information  but that: 
1  the symbolic approach to ml  for example the version space approach  may be usefull in noisy situation when noise comes from a lack of training examples 
1  cross-examination of expertise and comparison of users'descriptions with experts'description can filter noise. 
1  finding good intermediary knowledge and knowledge representation  using domain specific 
knowledge  collecting examples by means of 
questionnaires can solve some types of noise 1  delaying unreliable tests as much as possible in a decision tree when building a decision tree can generate robust rules. 
1  each time noise is not of numeric nature  as presented in this paper  the introduction of coefficients may lead to some results but that these will not have any significance. 
acknowledgements 
we wish to thank jim blythe and rob corlett at the general electric company  thierry andro at cognitech and jean jacques cannat in our group for all the contribution they made to the work described in this paper. special thanks to bruce buchanan and to the helix group at stanford university for the useful comments they have made on this research. the learning tools used for this research are implemented in common lisp on a ti-explorer lisp machine and on a sun 1 workstation. 
