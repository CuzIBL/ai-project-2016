
this paper presents a new system  called the system  performing abductive reasoning within the framework of abductive logic programming. it is based on a hybrid computational model that implements the abductive search in terms of two tightly coupled processes: a reduction process of the highlevel logical representation to a lower-level constraint store and a lower-level constraint solving process. a set of initial  proof of principle  experiments demonstrate the versatility of the approach stemming from its declarative representation of problems and the good underlying computational behaviour of the system. the approach offers a general methodology of declarative problem solving in ai where an incremental and modular refinement of the high-level representation with extra domain knowledge can improve and scale the computational performance of the framework.
1	introduction
over the last two decades it has become clear that abduction can play a central role in addressing a variety of problems in artificial intelligence. these problems include diagnosis  poole et al.  1; console et al.  1   planning  missiaen et al.  1; kakas et al.  1; shanahan  1  knowledge assimilation and belief revision   inoue and sakama  1; pagnucco  1  multi-agent coordination  ciampolini et al.  1; kowalski and sadri  1  and knowledge intensive learning  muggleton  1; mooney  1 .
　the essential feature of this abductive approach to problem solving is the fact that it allows the application problems to be formalized directly in their high-level declarative representation. a close link therefore emerges between declarative problem solving in ai and the logical reasoning of abduction. however  despite this variety of applications for abduction and its potential benefits it has not been easy to develop general systems for abduction that are computationally effective for problems of practical size and complexity.
　this paper presents a new system  called the -system  that supports abductive reasoning within the framework of abductive logic programming  alp   kakas et al.  1; denecker and kakas  1 . in this framework problems are represented in a high-level declarative way with logic programming rules and classical first-order sentences  integrity constraints . our work aims to examine the possibility of developing alp into a declarative problem solving framework  suitable for a variety of ai problems  that is computationally viable for problems of practical scale.
　a principled implementation of the -system is developed based on a hybrid computational model that formalizes the abductive search for a solution in terms of two interleaving processes: the logical reduction of the high-level representation of the problem and lower-level constraint solving. the abductive search is linked tightly to the construction of an associated constraint store.
　to validate our approachwe have carried out a set of  proof of principle  experiments with an initial implementation of the system that aim to test  a  the general underlying computational behaviour of the -system on different domains and  b  the flexibility of the framework to incorporate additional problem specific knowledge in a modular and computationally enhancing way. in particular  some of these experiments are designed to test the extend to which the high-level representation of problems can be computationally improved via extra domain knowledge added to it. the experiments considered include constraint satisfaction problems and standard ai planning systems competition problems.
　the -system has been developed as a follow up of two earlier alp systems  the aclp system  kakas et al.  1  and sldnfac  denecker and schreye  1   bringing together features from these two systems. the main development in the design of the -system over these previous systems is the fact that the non-determinism in the abductive computation is now made explicit allowing the possibility of implementing this as a form of parameterized heuristic search coupled with a process of deterministic propagation of the state of computation. the -system is available from www.cs.kuleuven.ac.be/ dtai/kt.
1	declarative programming with abduction
declarative problem solving with a high-level representation of the problem at hand is closely related to abduction. the reason for this stems from the fact that in a declarative representation where one describes the expert knowledge on the problem domain rather than some method for its solution often  the task of solving the problems consists in filling the

missing informationfrom this representation pertaining to the goal at hand. typically  in a logical framework of representation the solution consists in finding the extension of some predicate s  which are incompletely specified in the representation. the high-leveltheory describingthe problemis then extended to a new one such that the problem goal is satisfied.
　computing such extensions of the theory representing our problem is an abductive task. indeed  abduction as a problem solving method assumes that the general data structure for the solution to a problem  or solution carrier  is at the predicate level and hence that a solution is described in the same terms and level as the problem itself.
　a framework for problem solving with abduction therefore needs to be expressing enough to allow high-level declarative representations of complex problems with missing or incomplete information. at the same time for such a framework to have a practical value it should provide ways of improving the computational effectiveness of this high-level representation for any particular problem. one such framework that combines representational expressiveness and computational flexibility is that of abductiveconstraint logic programming  aclp  a framework that integrates together abductive logic programming  alp  with methods from constraint logic programming  clp . the a-system is developed within this framework. let us briefly review the underlying framework of alp.
　a recent way to formalize alp is to view this as a special case of id-logic  denecker  1   a logic extending classical logic with inductive definitions. let a language of predicates    be given consisting of three disjoint types of predicates:  i  the defined predicates   ii  the open or abducible predicates and  iii  the constraint predicates. then a theory in alp is defined as follows.
definition 1 an abductive theory is a triple where:
is a constraint logic program where only defined pre-
dicates appearing in the head of these rules.
　is a set of ground abducible atoms over open predicates.
	is a set of first order formulae over	  called integrity
constraints.
　the representation of an application problem in an abductive theory splits  in this view of alp in terms of id-logic  into two parts. the set of rules in represents the expert's strong definitional knowledge of the problem  i.e. knowledge which fully determines one or a group of predicates  the defined predicates  in terms of other open predicates. each rule represents a case in which the defined predicate in the head will be true; the program is an exhaustive enumeration of the cases. the open or abducible predicates have no definition. but some general information about them may be available indirectly through the expert's weaker assertional knowledge of the problem. this is represented by the theory of integrity constraints. the set enumerates all the possible missing  values  for the open predicates. abductive solutions are build from giving a ground  partial  definition of these open predicates.
　the constraint predicates are defined  as in clp  by an underlying constraint theory which is independent of any particular abductive theory . we will assume that the constraint theory is a finite domain theory that also includes equality over logical terms. the formal details of this are beyond the scope of this paper.
in alp the abductive problem task is defined as follows.
definition 1 given an abductive theory and some query   consisting of a conjunction of literals over   an abductivesolution  or explanation for is a set of ground abducible atoms together with an answer substitution such that is consistent and:
　this definition is generic in that it defines the notion of an abductivesolution in terms of any givensemantics of standard  constraint  logic programming  lp . each particular choice of semantics defines its own entailment relation and hence its own notion of what is an abductive solution. in the context of id-logic  its inductive definition semantics essentially coincides with the well-founded model semantics  gelder et al.  1  for lp when this is a two valued model1. in the rest of this paper we will be adopting this well-founded model semantics for lp for the development of our -system.
　a computed abductive solution therefore gives a ground definition of the open predicates which in turn through the logic program    extends this to the defined predicates. as an example of an abductive theory we give below a part of the theory representing a planning domain. in this 
     	  is the only open predicate 	 	 	are constraint predicates and all others are defined predicates. the predicates	 	and	are simple predicates defining the finite domain of variables  e.g. using clp notation	for a problemwhere we have ten trucks.


1









　the initial state of the problem is given by a set of facts in the logic program including statements of the form    e.g.

1	-system and abductive search
the computation of an abductive solution in the -system can be seen as a process of reduction of the query    to a set of abducible hypotheses on the open predicates together with an associated constraint store of constraints over the general finite domain constraint theory supported by the particular alp framework. this is similar to the computation of clp with two important differences.
　first the reduction involves through the hypotheses also a reduction of the integrity constraints . effectively  when
a new abductive hypothesis is made the integrity constraints need to be re-evaluated to remain true. this satisfaction of the can result in new constraints for the constraint store and possibly new goals at the same level as the original query. the second difference with clp is the way that the constraint store is used in this reduction. as we will see in the next subsection  this is not merely a passive store of constraints to be evaluated at the end of the reduction but it is used actively during the reduction to affect the search. for example  it enables reductions to be pruned early by setting new constraints in provided that this remains satisfiable.
1	abductive inference in the	-system
the logical reduction of a query in the -system can be defined as a derivation for through a rewriting process between states. we give here in brief the formal details of this.
　a computational restriction on the form of the integrity constraints    of an abductive theory is imposed in the -system. these sentences must be  or should be transformed to classically equivalent sentences  of the form: where each is a literal. such an integrity constraint will also be called a denial or a negative goal and will be denoted by . also to simplify the presentation  we assume that each rule in is of the form
where all variables are  implicitly  uni-

versally quantified  referred to as free variables  and

is a conjunction of literals with free variables occurring in the head and occurring only in the body. rules are therefore in homogeneous form where no non-variable term can appear in their head e.g. a rule will be written equivalently as	.
　a state of the derivation consists of two types of elements: a set of literals  possibly with free variables   called positive goals  and a set of denials  called negative goals  of the form where is a conjunction

of literals and are free variables. denotes the set of abducible atoms in   i.e. positive goal atoms whose predicate is an open predicate and denotes the set of positive constraint atoms in . a derivation for starts with an initial state consisting of the literals of the query as positive goals and all the denials in as negative goals. the rewriting derivation proceeds by selecting a literal in a goal of and applying a suitable inference rule yielding a new state. the main inference rules are given by the following rewrite rules  different elements of are separated by  ;  :


	if	is a defined predicate and

a rule. if	is an open predicate and		.
.


	 	a defined predicate 

	where	for each rule

	   	 .

	 	an
	open predicate  where		for each

		an abduced atom in	   	 .

	if	is a constraint atom without
universally quantified variables.

	if	is a constraint atom
without universally quantified variables.

	if	does not contain universally
quantified variables.
	a successful derivation terminates with a state	such that:
1. contains positive goals only of the form of abducible atoms or constraint atoms 
1. negative goals in	are denials containing some open

atom which has already been selected and resolved with each abduced atom    and
1. the constraint store	of	is satisfiable.
　otherwise  the derivation flounders when universally quantified variables appear in the selected literal in a denial. if the derivation does not succeed or flounder then it fails. note that negation as failure in is simply re-written  via the 1rd rule  to a denial of the positive atom.
　let be the final state of a successful derivation. then any substitution that assigns a ground term to each free variable of and which satisfies the constraint store is called a solution substitution of . such a substitution always exists since is satisfiable for a successful derivation.
theorem 1 let	be an abductive theory s.t.
　　　　  a query  the final state of a successful derivation for and a solution substitution of . then the pair and is an abductive solution of .
1	abductive search
the search for an abductive solution in the -system based on the above proof theory  depends very closely on the com-
puted constraint store. typically  the abducible hypotheses made during the computation are non-ground atoms whose variables are restricted by the constraints in the associated constraint store of the computation. this constraint store plays a central role not only in carrying  together with the abducibles  the solution but also in controlling the abductive search.
　in many respects the computation can be viewed as a process of constructinga constraint store fromthe high-levelspecification and query of the abductive theory as any decision that we take  e.g. which abducible to introduce or how to satisfy an integrity constraint  extends differently the current constraint store. hence we can turn things around and use the constraint store to guide the decisions that we make in this abductive search.
the overall pattern of the abductive computation in the -system is as follows:
1. deterministic propagation of +ve and -ve goals
1. suspend and evaluate choices
 a  quasi-consistency of -ve goals
 b  evaluate choices on +ve goals
1. global set of choices: new +ve goals 
1. if	non-empty then return to  1 
1. otherwise  exit and ground solution.
　the computation starts with a phase of deterministic propagation where all of the current goals that have only one possible rewriting are reduced until no such goals are left in the resulting state of the computation. the purpose of this phase is two-fold:  a  to expose and collect all the choice points in the current state of the computationand  b  to propagate the construction of the constraint store with all new constraints that are necessarily imposed under the choices made so far in the previous iteration steps. the updated constraint store is checked for satisfiability during this phase as it grows. this can have a significant effect on the computation as it enables us to detect early the ensuing failure of the choices made prior to this before committing to other choices.
　all the choice points exposed by this deterministic phase are suspended and a new process of their evaluation begins. this explicit handling of the non-determinism in the computation allows the use of a parameterized form of heuristic search where a variety of different types of heuristics can be used. there are essentially two types of choices in the computation. these are  1  a choice of which rule from the program to use in rewriting a positive goal  cf. first and second rewrite rule  and  1  a choice of which way to satisfy a denial  cf. last three rewrite rules .
　the choice points of the second type are considered in a phase called quasi-consistency1 where we try to satisfy all the denials in the current state together if possible without the introduction of new positive goals. choices result in a new possible constraint store  that we can compute by a subsidiary phase of deterministic propagation. the  quality  of this ensuing constraint store under various criteria  e.g. tightness of its finite domain variables or minimal change in the current constraint store  forms a possible heuristic criterion. a second criterion concerns the number and complexity of the new positive goals  that a choice would produce. for example  preferring new goals which are deterministic or which introduce new abducibles whose associated denials would have a simple form are different possibilities that we are currently exploring here in the implementation of the -system.
　after the phase of quasi-consistency the computation examines the other choice points left suspended  namely positive goals which are non-deterministic  and again it can apply a form of heuristic evaluation to choose amongst the different possibilities. once a global decision is taken on all these choice points the computation either returns to the top to repeat the process  when the choice leads to new positive goals  or otherwise  it terminates successfully by labeling the constraint store and thus grounding the abductive solution.
　this global approach  of the -system to choice making where it tries to cover together as many choice points as possible results in more informed decisions leading to less backtracking and thus to a better computational performance. it also facilitates the use of heuristics by helping to parameterize them and make them available at the top level.
　an important parameter of the computation is the degree to which the constraint store is examined for its satisfiability  and other quality measures  during its construction. one possibility is to do a full check of the whole store at each point where this grows. but this can be costly as a large part of this maybe unnecessary. on the other hand an incomplete check may result in the late  after several choices  recognition of the inconsistency of the constraint store. an intermediate possibility is to localizing the check to the latest addition to the store  i.e to the variables of the current constraints and other variables connected to these by constraints already in the store. this induces a form of dynamic partition of the constraint store into connected components that reflects  to a certain extend  the high-level structure of the problem and query at hand and could help minimize the computation required to check the satisfiability of the store.
1	experiments with the a-system
a number of different experiments have been carried out to test the underlying general computational behaviour of  an initial implementation of  the -system and how this can be affected by extending the high-level representation of problems with additional information.
　the current version of the system is implemented as a meta-programon top of sicstus prolog1.1 and uses its finite domain constraint solver. it is developed as an experimentation tool to test different datastructures and strategies. currently a form of indexing on the abducibles and constraints is used in the datastructures. with respect to the parameters of the abductivesearch presented in the previous section the current version implements only a very basic form of heuristics. in particular  the methods used for checking the satisfiability of the constraint store during its construction are simple.
　a first set of experiments consists of problems whose specification can be reduced to a finite domain constraint store in one phase of reduction. such problems include standard constraint satisfaction problems  e.g. n-queens  graph coloring and scheduling problems. the aim here was to confirm that the execution will be deterministic and to compare the execution time with that of solving the problem directly in clp to see the overhead cost of the reduction of the high-level representation of the problem. the tables below show a sample of these experiments for the n-queens problem and a graph coloring problem. for the graph coloring experiment a set of 1-colorable planar graphs were constructed. the run times are split into the time needed to find an abductive solution and the subsequent time needed to ground this by the clp labeling. all experiments reported in this paper are done on a linux machine  1 mhz  with 1 mb of memory. n-queens
sizeabductive solution  s clp grounding1.1ms1.1ms1.1ms1.1msgraph coloring
size nodes abductive solution  s clp grounding1.1ms1.1ms1.1ms1.1ms　although the reduction of the whole specification is done in one step  no backtracking occurs over the abductive solution  in both experiments the construction time  abductive solution  increases clearly with the size. so the metainterpreter is some orders slower than the prolog below  which is used in a clp-program to set up the constraints.
　another set of experiments was carried out on planning problems. these are non-deterministic problems where the abductive computation and search of the -system can be tested. we selected from the latest aips1 planning competition two domains: blocks world and logistics. the next table shows the performance of the system on the blocks world domain. for this experiment we used a specification with one action move x y t   which denotes moving a block
x onto y at time t.
	problem	planlength	time s 

11.1-1111.1-1111.1
　the second considered domain is that of logistics. for this domain a specification with functors and the general event calculus is used. futhermore the specification is two layered: the top layer is a compact high level description of the logistics domain. the lower level actions  from the original aips strips description  are derived from the high level ones. the first table below shows the performance of the -system on problems where the system has been used to compute only high-level abstract plans. the length of the plans are given by the final time when the goal is achieved  first number in the column  and the number of abduced actions describing the actual plan. a blank entry indicates that the system was unable to find a solution within a given time limit.
　the results for the low level plans are presented in a second table. these plans are computed in a separated phase using as input the previous computed high level plans. this step expands the high level plans in number of time points and actions.
high level solutions
problemlengthtime s max time points1111-1/1.1111  * 1-1/1.11---111  * 1-1/1.1low level solutions
problemlengthtime111ms111ms111ms111ms111ms　some of the problems took a while to return a solution. because the finite domain solver is incomplete  the -system may go on with a branch although the constraint store is already unsatisfiable  see the marked entries in the above tables . we tried different satisfiability checks but none of them was overall successful. a parameter in such a check is the domain size of the variables  see the last column of the table of the high level plans : by decreasing the maximal numberof time points the satisfiability checkfails faster when the constraint store is insatisfiable. as consequence some problems could be solved in a reasonable time. currently  we are investigating how we can improve this satisfiablity check. compared to previous experiments with sldnfac and aclp  the -system is more reliable and more robust to changes in the order of execution of the query. however because the constraint solver is incomplete it still possible to have one derivation of -system failing to find a solution and another one finding it immediately. another main difference of the -system with these previous systems is its separation
of search and inference. this allows easy experimentation with several strategies and different degrees of deterministic propagation.
1	related work and conclusions
there are several other abductive systems for alp that have recently been developed. different methods have been used e.g. bottom up computation  iwayama and satoh  1   tabling  alferes et al.  1 andrewritingrules with the completion  fung and kowalski  1; kowalski et al.  1 . the -system with its two earlier alp systems  the aclp system and sldnfac  is to ourknowledgethe first abductive system that has paid particular attention to the computational aspects of abduction using constraint solving extensively to control the abductive search and enhance its computational behaviour.
　on a more abstract level our work is related to answer set programming asp   gelfond and lifschitz  1 . strong connections have been established  satoh and iwayama  1  between alp and asp. at the level of semantics these two frameworks are for a large class of theories  asp admits only a special type of integrity constraints  equivalent.
at the level of computation our construction of an abductive solution in the -system corresponds to the generation of a model in asp but there are some significant differences in the respective computational models. it is therefore instructive to develop systematic experiments to compare systems from these two approaches to declarative problem solving.
　the abductive search in the -system can be parametrized in a variety of ways to reflect the type of cooperation with constraint solver and the heuristics that are used. an important future development of the system is to structure further this parametric space and to study how more techniques from constraint programming and advanced heuristic search can be incorporated in order to improve the general computational behaviour of the framework on a variety of problems. in particular  we can study how recent heuristic methods for planning  bonet and geffner  1  can be generalized to the abductive computation of the -system.
　this general improvementof the underlyingcomputational efficiency of the alp framework  or indeed of any other declarative problem solving framework  is clearly limited as we are aiming to use the framework on a general variety of problems. a complementaryline of developmentof the -system and its associated alp framework concerns the study of how to providea programmingenvironmentwhere the user has the facility to incrementally refine her/his high-level representation of the problemin a modular way. a general methodology for declarative problem solving with abduction is emerging where the alp modeling environment provides the possibility for extra problem specific  declarative or control  knowledge to be included that would enhance its computational performance.
references
 alferes et al.  1  j. j. alferes  l. m. pereira  and t. swift. well-founded abduction via tabled dual programs. in proceedings of iclp-1  1.
 bonet and geffner  1  b. bonet and h. geffner. planning as heuristic search: new results. in proc. european conference on planning  ecp-1   1.
 ciampolini et al.  1  a. ciampolini  e. lamma  p. mello  and p. torroni. an implementation for abductive logic agents. in ai*ia'1  lnai vol. 1  1.
 console et al.  1  l. console  l. portinale  and d. theseider dupre＞. using compiled knowledge to guide and focus abductive diagnosis. ieee transactions on knowledge and data engineering  1  1 :1  1.
 denecker and kakas  1  m. denecker and a.c. kakas. abductive logic programming. special issue of jlp  vol 1  1   1.
 denecker and schreye  1  m. denecker and d. de schreye. sldnfa: an abductive procedure for abductive logic programs. logic programming  1 :1  1.
 denecker  1  m. denecker. extending classical logic with inductive definitions. in proceedings of cl-1  1  1.
 fung and kowalski  1  t.h. fung and r.a. kowalski. the iff procedure for abductive logic programming. logic programming  1 :1  1.
 gelder et al.  1  a. van gelder  k.a. ross  and j.s. schlipf. the well-foundedsemantics for general logic programs. journal of the acm  1 :1  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. classical negation in logic programs and disjunctive databases. new generation computing  pp. 1  1.
 inoue and sakama  1  k. inoueand c. sakama. abductive framework for nonmonotonic theory change. in proceedings of ijcai-1  pages 1  1.
 iwayama and satoh  1  n. iwayamaand k. satoh. computing abduction by using tms with top-down expectation. logic programming  1-1 :1  1.
 kakas et al.  1  a.c. kakas  r.a. kowalski  and f. toni. the role of abductionin logic programming. in d. gabbay  c. hogger  and j. robinson  editors  handbook of logic in ai and logic programming  volume 1  pages 1. oxford university press  1.
 kakas et al.  1  a.c. kakas  a. michael  and c. mourlas. aclp: abductive constraint logic programming. journal of logic programming: special issue on abductive logic programming  1-1 :1 1.
 kowalski and sadri  1  r.a. kowalski and f. sadri. from logic programming to multi-agent systems. annals of mathemathics and artificial intelligence  1.
 kowalski et al.  1  r.a. kowalski  f. toni  and g. wetzel. executing suspended logic programs. fundamenta informaticae  1 :1  1.
 missiaen et al.  1  l.r. missiaen  m. denecker  and m. bruynooghe. chica  an abductive planning system based on event calculus. journal of logic and computation  1 :1  1.
 mooney  1  r.j. mooney. integrating abduction and induction in machine learning. in abduction and induction: essays on their relation and integration  pages 1. kluwer academic press  1.
 muggleton  1  s. muggleton. theory completion in learning. in inductive logic programming  ilp-1  1.
 pagnucco  1  m. pagnucco. the role of abductive reasoning within the process of belief revision. phd thesis  dept. of computer science  university of sydney  1.
 poole et al.  1  d. poole  r. goebel  and r. aleliunas. theorist: a logical reasoning system for defaults and diagnosis. in the knowledge frontier: essays in the representation of knowledge  pages 1. springer-verlag  1.
 satoh and iwayama  1  k. satoh and n. iwayama. computing abduction by using the tms. in proc. of iclp'1  pages 1  1.
 shanahan  1  m. shanahan. an abductiveevent calculus planner. logic programming  1-1 :1  1.
a comparative study of logic programs with preference
torsten schaub1 and kewen wang
　institut fur informatik  universit：	at potsdam： postfach 1 1  d-1 potsdam  germany
{torsten kewen} cs.uni-potsdam.deabstract
we are interested in semantical underpinnings for existing approaches to preference handling in extended logic programming  within the framework of answer set programming . as a starting point  we explore three different approaches that have been recently proposed in the literature. because these approaches use rather different formal means  we furnish a series of uniform characterizations that allow us to gain insights into the relationships among these approaches. to be more precise  we provide different characterizations in terms of  i  fixpoints   ii  order preservation  and  iii  translations into standard logic programs. while the two former provide semantics for logic programming with preference information  the latter furnishes implementation techniques for these approaches.
1	introduction
numerous approaches to logic programming with preference information have been proposed in the literature. so far  however  there is no systematic account on their structural differences  finally leading to solid semantical underpinnings. we address this shortcoming by a comparative study of a distinguished class of approaches to preference handling. this class consists of selective approaches remaining within the complexity class of extended logic programming  under answer sets semantics . these approaches are selective insofar as they use preferences to distinguish certain  models  of the original program.
　we explore three different approaches that have been recently proposed in the literature  namely the ones in  brewka and eiter  1; delgrande et al.  1; wang et al.  1 . our investigation adopts characterization techniques found in the same literature in order to shed light on the relationships among these approaches. this provides us with different characterizations in terms of  i  fixpoints   ii  order preservation  and  iii  translations into standard logic programs. while the two former provide semantics for logic programming with preference information  the latter furnishes implementation techniques for these approaches. from another perspective  one can view  iii  as an axiomatization of the underlying strategy within the object language  while  i  may be regarded as a meta-level description of the corresponding construction process. one may view  ii  as the most semantical characterization because it tells us which  models  of the original program are selected by the respective preference handling strategy.
　we limit  also in view of  iii   our investigation to approaches to preference handling that remain within np. this excludes approach like the ones in  rintanen  1; zhang and foo  1  that step outside the complexity class of the underlying reasoning method. this applies also to the approach in  sakama and inoue  1   where preferences on literals are investigated. while the approach of  gelfond and son  1  remains within np  it advocates strategies that are non-selective. approaches that can be addressed within this framework include  baader and hollunder  1; brewka  1  that were originally proposed for default logic.
1	definitions and notation
we assume a basic familiarity with logic programming under answer set semantics  gelfond and lifschitz  1 . an extended logic program is a finite set of rules of the form
       l1 ○ l1 ... lm not lm+1 ... not ln   1  where n − m − 1  and each li  1 ＋ i ＋ n  is a literal  ie. either an atom a or its negation  a. the set of all literals is denoted by lit. given a rule r as in  1   we let head r  denote the head  l1  of r and body r  the body  {l1 ... lm  not lm+1 ... not ln}  of r. further  let body+ r  = {l1  ... lm} and body  r  = {lm+1 ... ln}. a program is called basic if body  r  =   for all its rules.
　we define the reduct of a rule r as r+ = head r  ○ body+ r . the reduct  Πx  of a program Π relative to a set x of literals is defined by
Πx = {r+ | r （ Π and body  r  ” x =  }.
a set of literals x is closed under a basic program Π iff for any r （ Π  head r  （ x whenever body+ r    x. we say that x is logically closed iff it is either consistent  ie. it does not contain both a literal a and its negation  a  or equals lit. the smallest set of literals which is both logically closed and closed under a basic program Π is denoted by cn Π .
finally  a set x of literals is an answer set of a program Π iff cn Πx  = x. in what follows  we deal with consistent answer sets only.
the set  of all generating rules of an answer set x from
Π is given by  body+ r    x and body  r  ” x =  }.
　as van gelder in   we define cΠ x  = cn Πx . note that the operator cΠ is anti-monotonic  which implies that the operator aΠ x  = cΠ cΠ x   is monotonic. a fixpoint of aΠ is called an alternating fixpoint for Π. different semantics are captured by distinguishing different groups of fixpoints of aΠ.
　a  statically  ordered logic program1 is a pair  Π     where Π is an extended logic program and     Π 〜 Π is an irreflexive and transitive relation. given  r1 r1 （ Π  the relation r1   r1 expresses that r1 has higher priority than r1
1	preferred alternating fixpoints
the notion of answer sets  without preference  is based on a reduction of extended logic programs to basic programs  without default negation . such a reduction is inapplicable when addressing conflicts by means of preference information since all conflicts between rules are simultaneously resolved when turning Π into Πx. rather conflict resolution must be characterized among the original rules in order to account for blockage between rules. that is  once the negative body body  r  is eliminated there is no way to detect whether head r1  （ body  r  holds in case of r   r1.
　such an approach is pursued in  wang et al.  1  for characterizing  preferred  answer sets. following earlier approaches based on default logic  baader and hollunder  1; brewka  1   this approach is based on the concept of activeness: let x y   lit be two sets of literals in an ordered logic program  Π   . a rule r in Π is active wrt the pair  x y    if body+ r    x and body  r  ” y =  .
definition 1  wang et al. 1  let  Π    be an ordered logic program and let x be a set of literals. we define
	x1	=	 	and for i − 1
xi+1	=	xi “ {head r |
i. r （ Π is active wrt  xi x  and  
ii. there is no rule r1 （ Π with r   r1      such that
 a  r1 is active wrt  x xi  and
 b  head r1  1（ xi	    
then  c Π    x  = si−1 xi if si−1 xi is consistent.
otherwise  c Π    x  = lit.
the idea is to apply a rule r only if the question of application has been settled for all higher-ranked rules r1. that is  if either its prerequisites will never be derivable  viz. body+ r1  1  x  or r1 is defeated by what has been derived so far  viz. body  r  ” xi =1	   or r1 or another rule with the same head have already applied  viz. head r1  （ xi.
　as its original cΠ  the operator c Π    is anti-monotonic. accordingly  we may define for any set x   lit  the alternating transformation of  Π    as a Π    x  =
c Π    c Π    x  . a fixpoint of a Π    is called an alternating fixpoint of  Π   . note that a Π    is monotonic.
　now  in analogy to van gelder   a semantical framework for ordered logic programs in terms of sets of alternating fixpoints can be defined. three different types of semantics are investigated in  wang et al.  1 :  i  preferred1 answer sets  viz. alternating fixpoints being also fixpoints of c Π   .  ii  preferred regular extensions  viz. maximal normal1 alternating fixpoints of  Π   .  iii  preferred well-founded model  viz. the least alternating fixpoint of  Π   .
　we put the prefix 'w-' whenever a distinction to other approaches is necessary.
　for illustration  consider the following ordered logic program  Π1    due to  baader and hollunder  1 :
r1 : r1 : r1 : r1 : r1 : f w f b p○
○
○
○
○p not f b not  w w not  f pr1   r1 1 observe that Π1 admits two answer sets: x = {p b  f w} and x1 = {p b f w}. as argued in  baader and hollunder  1   x is the unique w-preferred answer set. to see this  observe that
x1
x1
x1
x1
x1=
=
=
=
= p}	f} {xp b p b 1 =  xf w}
{
{x1
x1
x1
x1=
=
=
= p}
{p b1 } x1 x{ 1=note that w cannot be included into because r1 is active wrt  and r1 is preferred to r1.
1	compiling order preservation
a translation of ordered logic programs  Π    to standard ones Π1 is developed in  delgrande et al.  1 . the specific strategy used there ensures that the resulting program Π1 admits only those answer sets of the original program Π that are order preserving:
definition 1 let  Π    be a statically ordered program and let x be an answer set of Π.
　then  x is called  -preserving  if there exists an enumeration such that for every i j （ i we have that:
1. body+ ri    {head rj  | j   i}; and 1. if ri   rj  then j   i; and
  then
 a  body+ r1  1  x or  b  body  r1  ” {head rj  | j   i} 1=  .
condition 1 makes the property of groundedness1 explicit. although any standard answer set is generated by a grounded sequence of rules  we will see in the sequel that this property is weakened when preferences are at issue. condition 1 stipulates that hriii（i is compatible with    a property invariant to all of the considered approaches. lastly  condition 1 is comparable with condition ii in definition 1; it guarantees that rules can never be blocked by lower-ranked rules.
　as above  x = {p b  f w} is the only  -preserving answer set of Π1; it can be generated by the grounded sequences hr1 r1 r1 r1i and hr1 r1 r1 r1i both of which satisfy conditions 1 and 1. the only grounded sequence generating x1 = {p b f w}  namely hr1 r1 r1 r1i  violates 1b.
　the corresponding translation integrates ordering information into the logic program via a special-purpose predicate symbol  . this allows also for treating ordering information in a dynamic fashion. a logic program over a propositional language l is said to be dynamically ordered iff l contains the following pairwise disjoint categories:  i  a set n of terms serving as names for rules;  ii  a set at of  propositional  atoms of a program; and  iii  a set at  of preference atoms s   t  where s t （ n are names. for each such program Π  we assume furthermore a bijective function n ，  assigning to each rule r （ Π a name n r  （ n. to simplify notation  we usually write nr instead of n r   and we sometimes abbreviate nri by ni .
　an atom1 holds.nr  a statically ordered programnr1 （ at  amounts to asserting that Π    can r   r
thus be captured by programs containing preference atoms only among their facts; it is then expressed by the program Π “ { nr   nr1  ○ | r   r1}.
　given r   r1  one wants to ensure that r1 is considered before r  in the sense that  for a given answer set x  rule r1 is known to be applied or defeated ahead of r  cf. condition ii or 1 above  respectively . this is done by translating rules so that the order of rule application can be explicitly controlled. for this purpose  one needs to be able to detect when a rule has been applied or when a rule is defeated. for a rule r  there are two cases for it not to be applied: it may be that some literal in body+ r  does not appear in the answer set  or it may be that a literal in body  r  is in the answer set. for detecting non-applicability  i.e.  blockage   for each rule r in the given program Π  a new  special-purpose atom bl nr  is introduced. similarly  a special-purpose atom ap nr  is introduced to detect the case where a rule has been applied. for controlling application of rule r the atom ok nr  is introduced. informally  one concludes that it is ok to apply a rule just if it is ok with respect to every  -greater rule; for such a  -greater rule r1  this will be the case just when r1 is known to be blocked or applied.
　more formally  given a dynamically ordered program Π over l  let l+ be the language obtained from l by adding  for each r r1 （ Π  new pairwise distinct propositional atoms ap nr   bl nr   ok nr   and ok1 nr nr1 . then  the translation t maps an ordered program Π over l into a standard program t  Π  over l+ in the following way.
definition 1  delgrande et al. 1  let Π = {r1 ... rk} be a dynamically ordered logic program over l.
then  the logic program t  Π  over l+ is defined as
trules  for Π  = ls+r（（Πbodyτ r +  wherer   l τ（ rbody  consists of the following  r   and r1 r1 （ Π :
a1 r  :
   a1 r  : b1 r l+  :
b1 r l   : c1 r  :head r  ○ ap nr 
ap nr  ○ ok nr  body r 
bl nr  ○ ok nr  not l+ bl nr  ○ ok nr  l 
ok nr  ○ ok1 nr nr1  ... ok1 nr nrk not  nr   nr1 
c1 r r   : ok  nr nr1  ○  nr   nr1  ap nr1  c1 r r1  : ok1 nr nr1  ○  nr   nr1  bl nr1 
t r r1 r1  :	nr   nr1 ○ nr   nr1 nr1   nr1
as r r1  :   nr1   nr  ○ nr   nr1
we write t  Π    rather than t  Π1   whenever Π1 is the dynamically ordered program capturing  Π   .
　the first four rules of τ r  express applicability and blocking conditions of the original rules. the second group of rules encodes the strategy for handling preferences. the first of these rules  c1 r    quantifies  over the rules in Π. this is necessary when dealing with dynamic preferences since preferences may vary depending on the corresponding answer set. the three rules c1 r r1   c1 r r1   and c1 r r1  specify the pairwise dependency of rules in view of the given preference ordering: for any pair of rules r  r1 with nr   nr1  we derive ok1 nr nr1  whenever nr   nr1 fails to hold  or whenever either ap nr1  or bl nr1  is true. this allows us to derive ok nr   indicating that r may potentially be applied whenever we have for all r1 with nr   nr1 that r1 has been applied or cannot be applied. it is important to note that this is only one of many strategies for dealing with preferences: different strategies are obtainable by changing the specification of ok ，  and ok1 ， ，   as we will see below.
　as shown in  delgrande et al.  1   a set of literals x is a  -preserving answer set of Π iff x = y ”l for some answer set y of t  Π   . in the sequel  we refer to such answer sets as being d-preferred.
1	synthesis
the last two sections have exposed three rather different ways of characterizing preferred answer sets. despite their different characterizations  however  it turns out that the two approaches prefer similar answer sets.
1	characterizing d-preference
we start by providing a fixpoint definition for d-preference. for this purpose  we assume a bijective mapping rule ，  from rule heads to rules  that is  rule head r   = r; accordingly  rule {head r  | r （ r}  = r. such mappings can be defined in a bijective way by distinguishing different occurrences of literals.
definition 1 let  Π    be a statically ordered logic program and let x be a set of literals. we define
	x1	=	 	and for i − 1
xi+1	=	xi “ {head r |
i. r （ Π is active wrt  xi x  and     
ii. there is no rule r1 （ Π with r   r1
	such that	 
 a  r1 is active wrt  x xi  and
 b  r1（ rule xi 	    
then is consistent.
otherwise .
the difference between this definition and definition 1 manifests itself in iib. while d-preference requires that a higherranked rule has effectively applied  w-preference contents itself with the presence of the head of the rule  no matter whether this was supplied by the rule itself.
this difference is nicely illustrated by program  Π1   :
r1 :	a	○	not b	r1   r1	 1  r1 :	b	○ r1 :	a	○
while the only answer set {a b} is w-preferred set  there is no d-preferred answer set. this is the same with program  obtained by replacing r1 with.
　we have the following result providing three alternative characterizations of d-preferred answer sets.
theorem 1 let  Π    be a statically ordered logic program over l and let x be a consistent set of literals.
then  the following propositions are equivalent.
1. ;
1. x = y ” l for some answer set y of t  Π   ;
1. x is a  -preserving answer set of Π.
while the last result dealt with effective answer sets  the next one shows that applying operator c Πd     is equivalent to the application of van gelder's operator cΠ1 to the translated program t  Π    .
theorem 1 let  Π    be a statically ordered logic program over l and let x be a consistent set of literals over l.
　then  we have thatfor some set of literals y over l+ such that x = y ” l.
this result is important because it allows us to use the translation t  Π    for implementing further semantics by appeal to the alternating fixpoint idea.
1	characterizing w-preference
we start by showing how w-preference can be characterized in terms of order preservation.
definition 1 let  Π    be a statically ordered program and let x be an answer set of Π.
　then  x is called  w-preserving  if there exists an enumeration such that for every i j （ i we have that:
1.  a  body+ ri    {head rj  | j   i} or
 b  head ri  （ {head rj  | j   i}; and
1. if ri   rj  then j   i; and
  then
 a  body+ r1   1	x or
 b  body  r1  ” {head rj  | j   i} 1=   or  c  head r1  （ {head rj  | j   i}.
the primary difference of this concept of order preservation to the original one is clearly the weaker notion of groundedness. this involves the rules in   via condition 1b  as well as those in   via condition 1c . the rest of the definition is the same as in definition 1. for instance  answer set {a b} of Π1 is generated by the  w-preserving rule sequence hr1 r1i. note that r1 satisfies 1c but neither 1a nor 1b. for a complement  in  is dealt with via condition 1b.
　interestingly  this weaker notion of groundedness can be easily integrated into the translation given in the last section.
definition 1 given the same prerequisites as in definition 1.
　then  the logic program t w Π  over l+ is defined as t w Π  = sr（Πτ r  “ {c1 r r1  | r r1 （ Π}  where c1 r r1  : ok1 nr nr1  ○  nr   nr1  head r1 
the purpose of c1 r r1  is to eliminate rules from the preference handling process once their head has been derived.
we have the following result  showing in particular  how
w-preference is implementable via off-the-shelf logic programming systems.
theorem 1 let  Π    be a statically ordered logic program over l and let x be a consistent set of literals. then  the following propositions are equivalent.
1. c Π    x  = x;
1. x = y ” l for some answer set y of t w Π   ;
1. x is a  w-preserving answer set of Π.
　in analogy to what we have shown above  we have the following stronger result  opening the avenue for implementing more semantics based on w-preference:.
theorem 1 let  Π    be a statically ordered logic program over l and let x be a consistent set of literals over l.
　then  we have that c Π    x  = ct w Π    y   ” l for some set of literals y over l+ such that x = y ” l.
1	brewka and eiter's concept of preference
another approach to preference was proposed by brewka and eiter in . for brevity  we omit technical details and simply say that an answer set is b-preferred; the reader is referred to  brewka and eiter  1; 1  for details.
　this approach differs in two significant ways from the two approaches given above. first  the construction of answer sets is separated from verifying whether they respect the given preferences. interestingly  this verification is done on the basis of the prerequisite-free program obtained from the original one by  evaluating  body+ r  for each rule r wrt the separately constructed  standard  answer set. second  rules that putatively lead to counter-intuitive results are removed from the inference process. this is made explicit in  brewka and eiter  1   where the following filtering transformation is defined:1
zx Π  = Π   {r （ Π | head r  （ x body  r  ” x =1	 }
　　 1  then  by definition  an answer set of Π is b-preferred iff it is a b-preferred answer set of zx Π .
　the distinguishing example of this approach is given by program  Π1   :
r1 : r1 : r1 : b  ab○
○
○a not  b not b not  ar1   r1   r1 1 program Π1 has two standard answer sets  {a b} and {a  b}. while the former is b-preferred  neither of them is w- or dpreferred  see below . also  we note that both answer sets of program  Π1    are b-preferred  while only {p b  f w} is
w- and d-preferred.
　in order to shed some light on these differences  we start by providing a fixpoint characterization of b-preference:
definition 1 let  Π    be an ordered logic program and let
x be a set of literals. we define
x1= 	and for i − 1xi+1=xi “ {head r |
i. such thatr （ Π is active wrtr1 （ x xΠ with  andr   r1      
ii. there is no rule
 a  r1 is active wrt  x xi  and
 b  head r1  1（ xi	    then is consistent.
otherwise  c Π    x  = lit.
the difference between this definition1 and its predecessors manifests itself in condition i  where activeness is tested wrt  x x  instead of  xi x  as in definition 1 and 1. in fact  in example  1  it is the  unprovability of the  prerequisite a of the highest-ranked rule r1 that makes the construction of w- or d-preferred answer sets break down  cf. definition 1 and 1 . this is avoided with b-preference because once answer set {a b} is provided  its preference-compatibility is tested wrt the program obtained by replacing r1 with b ○ not  b.
   b-preference can be captured by means of the following notion of order preservation:
definition 1 let  Π    be a statically ordered program and let x be an answer set of Π.
　then  x is called  b-preserving  if there exists an enumeration such that  for every i j （ i  we have that:
1. if ri   rj  then j   i; and
  then
 a  body+ r1  1  x or
 b  body  r1  ” {head rj  | j   i} 1=   or  c  head r1  （ x.
this definition differs in two ways from its predecessors.
first  it drops any requirement on groundedness  expressed by condition 1 above. this corresponds to using  x x  instead of  xi x  in definition 1. hence  groundedness is fully disconnected from order preservation. in fact  observe that the b-preferred answer set {a b} of  Π1    is associated with the  b-preserving sequence hr1 r1i  while the standard answer set itself is generated by the grounded sequence hr1 r1i. second  condition 1c is more relaxed than in definition 1. that is  any rule r1 whose head is in x  as opposed to xi  is taken as  applied . apart from this  condition 1c also integrates the filter-conditions from  1 .1 for illustration  consider example  1  extended by r1   r1:
r1 : a ○ not b r1   r1   r1  1  r1 : b ○ r1 : a ○
while this program has no d- or w-preferred answer set  it has a b-preferred one: {a b} generated by hr1 r1i. the critical rule r1 is handled by 1c. as a net result  condition 1 is weaker than its counterpart in definition 1. we have the following results.
theorem 1 let  Π    be a statically ordered logic program over l and let x be a consistent answer set of Π.
then  the following propositions are equivalent.
1. x is b-preferred;
1. ;
1. x = y ” lb for some answer set y of t b Π   
	 where t	is defined in  delgrande et al.  1  ;
1. x is a  b-preserving answer set of Π.
unlike theorems 1 and 1  the last result stipulates that x must be an answer set of Π. this requirement can only be dropped in case 1  while all other cases rely on this property.
1	relationships
up to now  we have tried to clarify the structural differences between the respective approaches. this has led to homogeneous characterizations that allow us to compare the examined approaches in a uniform way. as a result  we obtain insights into the relationships among these approaches.
　first of all  we observe that all three approaches treat the blockage of  higher-ranked  rules in the same way. that is  a rule r1 is found to be blocked if either its prerequisites in body+ r1  are never derivable or if some member of body  r1  has been derived by higher-ranked or unrelated rules. this is reflected by the identity of conditions iia and 1a/b in all three approaches  respectively. although this is arguably a sensible strategy  it leads to the loss of preferred answer sets on programs like
r1 : a ○ not b r1   r1 r1 : b ○ .
　let us now discuss the differences among the approaches. the difference between d- and w-preference can be directly read off definition 1 and 1; it manifests itself in condition iib and leads to the following relationship.
theorem 1 every d-preferred answer set is w-preferred.
example  1  shows that the converse does not hold.
　interestingly  a similar relationship is obtained between wand b-preference. in fact  definition 1 can be interpreted as a weakening of definition 1 by dropping condition 1 and weakening condition 1  via 1c . we thus obtain the following result.
theorem 1 every w-preferred answer set is b-preferred.
example  1  shows that the converse does not hold. we obtain the following summarizing result by letting as Π  = {x | cΠ x  = x} and asp Π    = {x （
as Π  | x is p-preferred} for p = w  d  b.
theorem 1 let  Π    be a statically ordered logic program. then  we have:
asd Π      asw Π      asb Π      as Π 
in principle  this hierarchy is induced by a decreasing interaction between groundedness and preference. while dpreference requires the full compatibility of both concepts  this interaction is already weakened in w-preference  before it is fully abandoned in b-preference. this is nicely reflected by the evolution of condition 1 in definitions 1  1  and 1.
　notably  groundedness as such is not the ultimate distinguishing factor  as demonstrated by the fact that prerequisitefree programs do not necessarily lead to the same preferred answer sets  as witnessed in  1  and  1 . rather it is the degree of interaction between groundedness and preferences that makes the difference.
1	conclusion
the notion of preference seems to be pervasive in logic programming when it comes to knowledge representation. this is reflected by numerous approaches that aim at enhancing logic programming with preferences in order to improve knowledge representation capacities. despite the large variety of approaches  however  only very little attention has been paid to their structural differences and sameness  finally leading to solid semantical underpinnings.
　this work is a first step towards a systematic account to logic programming with preferences. we elaborated upon three different approaches that were originally defined in rather heterogenous ways. we obtained three alternative yet uniform ways of characterizing preferred answer sets  in terms of fixpoints  order preservation  and an axiomatic account . the underlying uniformity provided us with a deeper understanding of how and which answer sets are preferred in each approach. this has led to a clarification of their relationships and subtle differences. in particular  we revealed that the investigated approaches yield an increasing number of answer sets depending on how tight they connect preference to groundedness.
　an interesting technical result of this paper is given by the equivalences between the fixpoint operators and the standard logic programming operators applied to the correspondingly transformed programs  cf. theorem 1 and 1 . this opens the avenue for further concepts of preference handling on the basis of the alternating fixpoint theory and its issuing semantics. further research includes dynamic preferences and more efficient algorithms for different semantics in a unifying way.
acknowledgements. this work was supported by dfg under grant for 1-1  tp c.
references
 baader and hollunder  1  f. baader and b. hollunder. how to prefer more specific defaults in terminological default logic. in proc. ijcai'1  p 1  1.
 brewka and eiter  1  g. brewka and t. eiter. preferred answer sets for extended logic programs. artificial intelligence  1-1 :1  1.
 brewka and eiter  1  g. brewka and t. eiter. prioritizing default logic. in st. holldobler  ed ： intellectics and computational logic. kluwer  1. to appear.
 brewka  1  g. brewka. adding priorities and specificity to default logic. in l. pereira and d. pearce  eds  proc. jelia'1  p 1. springer  1.
 delgrande et al.  1  j. delgrande  t. schaub  and h. tompits. logic programs with compiled preferences. in proc. ecai 1  p 1. ios press  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. classical negation in logic programs and deductive databases. new generation computing  1-1  1.
 gelfond and son  1  m. gelfond and t. son. reasoning with prioritized defaults. in j. dix  l. pereira  and t. przymusinski  eds  workshop on logic programming and knowledge representation  p 1. springer  1.
 rintanen  1  j. rintanen. on specificity in default logic. in proc. ijcai'1  p 1. morgan kaufmann  1.
 sakama and inoue  1  c. sakama and k. inoue. representing priorities in logic programs. in m. maher  ed  proc. jcslp'1  p 1. mit press  1.
 van gelder  1  a. van gelder. the alternating fixpoint of logic programs with negation. j. computer and system science  1-1  1.
 wang et al.  1  k. wang  l. zhou  and f. lin. alternating fixpoint theory for logic programs with priority. in proc. int'l conf. computational logic  p 1. springer  1.
 zhang and foo  1  y. zhang and n. foo. answer sets for prioritized logic programs. in j. maluszynski  ed  proc. islp'1  p 1. mit press  1.
reasoning with infinite stable models
piero a. bonatti
dip. di tecnologie dell'informazione - universita` di milano
i-1 crema  italy bonatti dti.unimi.itabstract
the existing proof-theoretic and software tools for nonmonotonic reasoning can only handle finite domains. in this paper we introduce a class of normal logic programs  called finitary programs  whose domain may be infinite  and such that credulous and skeptical entailment under the stable model semantics are computable. finitary programs- that are characterized by two conditions on their dependency graph-are computationally complete  they can simulate arbitrary turing machines . further results include a compactness theorem and the proof that the two conditions defining finitary programs are  in some sense   minimal . the existing methods for automated nonmonotonic reasoning are either complete for finitary programs  or can be easily extended to cover them.
1	introduction
there is a complexity gap between propositional and firstorder non-monotonic theories. finite propositional theories are decidable  as in first-order logic   while the consequences of first-order theories are not recursively enumerable  in general. a computationally complete  turing-equivalent  layer is missing between the two classes of theories. for this reason  most of the research on automated reasoning and proof-theory for nonmonotonic logics has been focussed on finite propositional theories or equivalent formalisms  such as function-free logic programs. in this way  the ability of reasoning about recursive data structures and infinite domains  such as lists  trees  xml/html documents  time  and so on  is completely lost. this is a strong limitation  both for standard tasks-such as reasoning about action and change when time is explicitly represented-and for emerging applications-such as using xml document bases as knowledge bases.
　of course  there may exist interesting  semi-decidable fragments of first-order nonmonotonic logics. for example  in some cases  the second-order circumscription formula can be expressed as a finite first-order formula. similar examples are missing so far for default logic and autoepistemic logic.
　normal logic programs under the stable model semantics can be regarded as a fragment of both logics. in this paper we introduce a turing-equivalent class of programs  called finitary programs  that opens the way to effective default and autoepistemic reasoning about infinite domains. the main theoretical features of finitary programs are the following:
their domain may be infinite.
nontheless both credulous and skeptical reasoning are semi-decidable. ground queries are decidable.
a form of compactness holds.
finitary programs are computationally complete  i.e. each turing machine can be simulated by some finitary program.
finitary programs are defined by two conditions on their dependency graph  that are minimal  in the sense that if any condition were dropped  then semi-decidability and compactness would not be guaranteed anymore.
the paper is organized as follows: after a few technical preliminaries  section 1   we characterize the subprogram needed to reason about a given ground formula  section 1 . then we introduce finitary programs  section 1  and study their theoretical properties and expressiveness. in section 1  the completeness proof for the skeptical resolution calculus introduced in  bonatti  1   originally formulated for function-free programs  is extended to all finitary programs and a generalization thereof  almost finitary programs . after a brief sketch of how existing credulous reasoners can be extended to finitary programs  section 1   we conclude with a discussion of the results and related work. some of the proofs are omitted due to space limitations.
1	preliminaries
we assume the reader to be familiar with the classical theory of logic programming  lloyd  1 . normal logic programs  hereafter called simply  programs   are sets of rules of the form     such that is a logical atom and each     is a literal. as usual by  head  and  body  of such a rule we mean and   respectively. a program is positive if it contains no occurrences of . the ground instantiation of a program is denoted by . the gelfond-lifschitz transformation of program w.r.t. an herbrand interpretation  represented  as usual as a set of ground atoms  is obtained by removing from all the rules containing a negative literal such that   and by removing from the remaining rules all negative literals. an interpretation is a stable model of if is the least herbrand model of . a formula is credulously  resp. skeptically  entailed by iff is satisfied by some  resp. each  stable model of .
　the dependency graph of a program is a labelled directed graph whose vertices are the ground atoms of 's language. moreover  i  there exists an edge from to iff there is a rule with in the head and an occurrence of in the body; ii  such edge is labelled  negative  if occurs in the scope of   and  positive  otherwise. an atom depends positively  resp. negatively  on if there is a directed path from to in the dependency graph with an even  resp. odd  number of negativeedges. a program is callconsistent if no atom depends negatively on itself. by oddcycle we mean a cycle in the dependency graph with an odd number of negative edges. call-consistency coincides with the absence of odd-cycles. every call-consistent program has at least one stable model  dung  1 .
　in the context of normal logic programs  a splitting set for a program  lifschitz and turner  1  is a set of atoms containing all the atoms occurring in the body of any rule whose head is in . the set of rules
	whose head is in	-called the  bottom  of
w.r.t. -will be denoted by . by we denote the following partial evaluation of w.r.t. : remove from each rule such that some containing an atom of is false in   and remove from the remaining rules all the containing a member of . the following is a specialization to normal programs of a result in  lifschitz and turner  1 .
theorem 1  splitting theorem  let be a splitting set for a normal logic program . an interpretation is a sta-
ble model of	iff	  where
1. is a stable model of	  and
1. is a stable model of	.
1	relevant subprograms
this section contains the basic technical results needed to investigate decidability and undecidability issues. these results prove that a certain strict subset of suffices to decide credulous and skeptical entailment. intuitively  all is needed for inference are the rules affecting the behavior of odd-cycles  which may generate inconsistencies in the form of instability  and the definitions of the predicates on which the given goal depends.
definition 1  relevant universe and subprogram  the relevant universe for a ground formula  w.r.t. program    denoted by   is the set of all ground atoms such that the dependency graph of contains a path from to an atom occurring either in or in some odd-cycle of the graph. the relevant subprogram for a ground formula  w.r.t program    denoted by   is the set of all rules in whose head belongs to . 
example 1 consider the program consisting of the rules:
and let	. here

note that for all rules   if the head of belongs to then also all the other atoms in belong to
. the next proposition follows easily.
proposition 1 is a splitting set for   and coincides with .
now the basic technical results can be proved.
lemma 1 for all ground formulae   has a stable model iff has a stable model such that
.
proof.	  if  part  suppose	is a stable model of	.
by proposition 1 	is a splitting set for	and
	.	then  by the splitting theorem
 lifschitz and turner  1   there exist a stable model	of
         and a stable model	of	  such that	. by definition  no atom in	occurs in	  therefore	.
it follows that   and hence is a stable model of .
	  only if  part  suppose	has a stable model
   . by definition  all the atoms occurring in an oddcycle belong to . consequently  the dependency graph of contains no odd-cycles  i.e. is call-consistent. then  by a wellknown result in  dung  1   has a stable model . let . by the splitting theorem  the interpretation is a stable model of . moreover  since
	 cf. point 1  	.	
theorem 1 for all ground formulae	 
1. credulously entails	iff	does.
1. skeptically entails	iff	does.
proof. if credulously entails   then there exists a stable model of such that . by lemma 1  is a stable model of . moreover  since by definition contains all the atoms occurring in   must have the same truth value in and  
and hence . as a consequence credulously entails .
	conversely  suppose that	credulously entails	.
then there exists a stable model	of	such that
　　　　. by lemma 1 	has a stable model	such that	. then the models	and	must agree on the valuation of	 cf. the  only if  part of the proof  and hence	  which means that	credulously entails . this completes the proof of 1 .
to prove 1   we demonstrate the equivalent statement:
　does not skeptically entail skeptically entail .iff	does notthis statement is equivalent to:credulously entailsiff	credulously entails  that follows immediatelyfrom 1 .1	finitary programs
now we introduce a class of programs whose consequences are recursively enumerable  although their domain may be infinite.
definition 1  finitary programs  we say a program is finitary if the following conditions hold:
1. for each node of the dependency graph of   the set of all nodes such that depends  either positively or negatively  on is finite.
1. only a finite number of nodes of the dependency graph of occurs in an odd-cycle. 
for example  most classical programs on recursive data structures such as lists and trees  e.g. predicates member  append  reverse  satisfy the first condition. in these programs  the terms occurring in the body of a rule occur also in the head  typically as strict subterms of the head's arguments. this property clearly entails condition 1.
　the second condition is satisfied by most of the programs used for embedding np-hard problems into logic programs  cholewin＞ski et al.  1; eiter and gottlob  1; gottlob  1 . such programs can be  re formulated by using a single odd cycle involving one atom and defined by simple rules such as and  if does not occur elsewhere  then can be used as the logical constant false in the rest of the program .
　an example of finitary program without odd-cycles is illustrated in figure 1. it credulously entails a ground goal iff encodes a satisfiable formula. by adding rule we obtain another finitary program with one odd-cycle  such that is skeptically entailed iff the formula encoded by is a logical consequence of the one encoded by .
　the following proposition follows straightforwardly from the definitions of   and finitary programs. it will be needed to prove computability results.
proposition 1 if	is finitary then  for all ground goals	  and	are finite.
1	compactness
in classical first-order logic  an infinite set of formulae is inconsistent iff it contains an inconsistent finite subset. a similar property-that in general does not apply to nonmonotonic logics-is enjoyed by finitary programs  too.
definition 1 an unstable kernel for a program is a subset of with the following properties:
1. is downward closed  that is  for each atom occurring in   contains all the rules whose head is .
1. has no stable models.	
theorem 1  compactness  a finitary program has no stable models iff it has a finite unstable kernel.
proof. let be any ground atom in the language of . from lemma 1  it followsthat has no stable models iff has no stable models. clearly  is downward closed by definition. moreover  by proposition 1  is finite. therefore has no stable models iff is a finite unstable kernel of .	
1	decidability and semi-decidability of inference
hereafter we focus on the complexity of inference within the class of finitary programs. this subsection deals with upper bounds. we start with the proof that  for all ground goals  both credulous and skeptical inference are decidable.
theorem 1 for all finitary programs	and ground goals
　  both the problem of deciding whether is a credulous consequence of and the problem of deciding whether is a skeptical consequence of are decidable.
proof. by theorem 1  is a credulous  resp. skeptical  consequence of iff is a credulous  resp. skeptical  consequence of . moreover  by proposition 1  is finite  so the set of its stable models can be computed in finite time. it follows that the inference problems for and are both decidable.	
it follows easily that existentially quantified goals are semidecidable.
theorem 1 for all finitary programs and all goals   both the problem of deciding whether is a credulous consequence of and the problem of deciding whether is a skeptical consequence of are semi-decidable.
proof. the formula is credulously  res. skeptically  entailed by iff there exists a grounding substitution such that is credulously  res. skeptically  entailed by . the latter
problem is decidable  by theorem 1   and all grounding for can be recursively enumerated  so existential entailment can be reduced to a potentially infinite recursive sequence of decidable tests  that terminates if and only if some is entailed.	
we shall see in the following section that this is a strict bound  i.e. existential entailment can be undecidable  cf. corollary 1 .
1	minimality and expressiveness
next we focus on lower bounds to the complexity of inference for the class of finitary programs and relaxations thereof. the next two results show that both of the conditions in definition 1 are necessary for semi-decidability  i.e. definition 1 is in some sense minimal.
proposition 1 credulous and skeptical inference are not semi-decidable for the class of all programs satisfying condition 1 of definition 1.
proof. note that locally stratified programs trivially satisfy condition 1 of definition 1 while some of them may violate condition 1. for locally stratified programs  credulous and skeptical inference coincide with the well-founded semantics  gelfond and lifschitz  1  and are not semi-decidable  schlipf  1   so the proposition immediately follows. 

figure 1: a finitary program for sat　the other lower-bound results are based on positive programs that decide whether a given turing machine terminates using a fixed portion of its tape.
	let	be an arbitrary turing machine; let	and	be
　's set of states and vocabulary  respectively. recall that the actions of are defined by 1-tuples   where and are the current state and symbol  respectively  is
the symbol to be overwritten on   is the next state  and specifies 's head movement.
　consider the positive program in figure 1. the reader may easily verify that is entailed by iff there exists a finite computation of   starting from a configuration with state and tape described by   and using only the finite portion of tape corresponding to . here is a list representing a finite portion of the tape on the left of 's head  in reverse order; is the current symbol; is a  non reversed  finite portion of the tape on the right of 's head.
　program satisfies condition 1 of definition 1. to prove this  note that the recursive calls to predicate preserve the length of the portion of tape encoded in the head. therefore  for each ground atom   the number of atoms connected to by a directed path in
the dependency graph is bounded by	.
　the next two results are based on and prove that if condition 1 in definition 1 were dropped  then inference would not be semi-decidable anymore. more precisely  the theorems say that some inferences would be as complex as deciding the termination of an arbitrary turing machine.
theorem 1 for each turing machine with initial state and tape with non-blank portion   a program and a goal can be recursively constructed  such that satisfies only condition 1 in definition 1  and skeptically entails iff terminates.
proof.  sketch  let consist of the program defined in figure 1 plus the rules



where represents the blank symbol. satisfies condition 1 of definition 1  cf. the argument for  . condition 1 of definition 1 is violated  as there exist infinitely many odd-cycles  one for each ground atom . clearly  for any grounding substitution   the goal

can be derived from the subprogram and the clauses for blanklist iff terminates using the portion of represented by . at the same time  has a stable model iff the above goal cannot be derived from   because of the odd-cycle containing . it follows that for all consisting of a propositional symbol not occurring in   skeptically entails iff terminates. 
theorem 1 for each turing machine with initial state and tape with non-blank portion   a program and a goal can be recursively constructed  such that satisfies only condition 1 in definition 1  and credulously entails iff terminates.
proof. let   where is a new propositional symbol not occurring in   and let . clearly credulously entails iff has a stable model. since has a stable model iff terminates  cf. the previous proof   the theorem immediately follows. 
　the next result is based on a modification of that keeps track of the result of the computation. the modified program has one extra argument to return the final state of the tape. the theorem proves that the class of finitary programs is computationally complete by showing how any turing machine can be simulated by a suitable finitary program.
theorem 1 for each turing machine with initial state and tape with non-blank portion   a  positive  finitary program and a goal can be recursively constructed  in such a way that for all grounding substitutions   iff terminates and encodes the final tape of the computation.
corollary 1 the problems of deciding whether a finitary program credulously/skeptically entails an existentially quantified goal are not decidable.
1	resolution calculus
here the results of the previous sections are applied to the resolution calculus for skeptical stable model semantics introduced in  bonatti  1 . in the original paper  the resolution
calculus was proved complete w.r.t. function-free programs  soundness holds for all programs . in this section  we extend the completeness result to all finitary programs.
theorem 1 let be a finitary program  and let be a ground skeptical consequence of   where
and are sequences of literals. then the skeptical goal has a successful skeptical derivation from with answer substitution more general than .
for all 1-tuples for all 1-tuples if there exists no tuple forand.figure 1: a program deciding termination of a turing machine with bounded tapeproof. let . since is a ground skeptical consequence of   then is also a skeptical consequence of  by theorem 1 . moreover 
is finite  by proposition 1 . then  by the original completeness result  bonatti  1   has a ground skeptical derivation from . by standard lifting techniques  cf.  lloyd  1    a corresponding skeptical
derivation of from with answer substitution more general than can easily be obtained. 
example 1 consider again the program of example 1. this finitary program has no stable model  because of its third rule. figure 1 shows a successful skeptical derivation for the skeptical conclusion   with empty answer substitution  which means that is a skeptical consequence of  . the restricted split of type ii introduces two subgoals  each with a new hypothesis   and   respectively   obtained from an atom occurring in some odd-cycle. the contradiction rule replaces the left-hand side of a goal with the negation of some hypothesis. intuitively  the goal is proved by showing that the hypotheses on the right hand side cannot be satisfied. the failure rule rewrites a goal with one of its counter-supports. in this example the counter-support of is itself  obtained by negating the unique support of  . resolution can be performed either with a program rule or with one of the hypotheses  treated as facts . finally  the success rule removesgoals where there is nothing left to prove. as usual  the empty goal sequence is denoted by .	
　with the help of the resolution calculus it can be shown that a class of normal programs larger than finitary programs is turing equivalent. the extended class-called almost finitary-admits the kind of rules typically used to generate inconsistencies  without the restrictions of definition 1.
definition 1  almost finitary programs  a normal program is almost finitary iff it can be partitioned into two  disjoint  subsets  and   such that is finitary  and consists of rules of the form	
note that there is no restriction on 's rule variables. given a ground instance of a rule in  
may depend on infinitely many ground atoms  i.e.  condition 1 of definition 1 can be violated .
theorem 1 the skeptical resolution calculus is complete  in the sense of theorem 1  for all almost finitary programs.
corollary 1 the set of skeptical consequences of the form
               where	and	are sequences of literals  is semi-decidable for almost finitary programs.
remark 1 for almost finitary programs  ground credulous inference is not semi-decidable. this means that skeptical inference from almost finitary programs cannot be implemented through credulous inference. only direct approaches such as the resolution calculus are possible.
1	credulous automated reasoners
for function-free programs  there exist powerful automated reasoners based upon stable model generation  such as smodels  niemela： and simons  1  and dlv  eiter et al.  1 . internally  these engines operate on ground programs; for this reason they embody smart program instantiation routines. such routines are applied before any other form of reasoning. it seems not difficult to extend the instantiation routines to deal with all finitary programs  so that given a ground goal 1  only the relevant  ground and finite  subprogram
        or an equivalent subset thereof 1 is generated. the rest of the engine needs no modification. by theorem 1  soundness and completeness are preserved. in this way  our results can be used to extend the applicability range of this class of automated reasoners  with no modification to the core of their reasoning mechanisms.1
1	summary and related work
for the first time  semi-decidable fragments of the stable model semantics have been explored in depth  and related to the number of atoms involved in odd-cycles. the main contributions of this paper can be summarized by recalling that finitary programs  def. 1  are computationally complete  theorem 1   can deal with function symbols  and enjoy a compactness result  theorem 1  that guarantees semi-decidability of inference  theorem 1 . finitary programs are characterized by two conditions on their dependency graph that are minimal  in the sense that if any of them were dropped  then inference would not always be semidecidable  theorems 1  1 and 1 . we proved that the skeptical resolution calculus is complete for finitary and almost finitary programs  theorems 1 and 1   and sketched how other engines for nonmonotonic reasoning can be extended to deal with finitary programs  section 1 . we are currently extending our results to larger classes of programs  and to partial stable models.1
　the work on finitary programs contributes to providing nonmonotonic logics with classical proof- and model-

1
these engines can only deal with ground queries
1
　　the smart instantiation routines remove some non-applicable rule instances. 1
　　as noted by an anonymous referee  the same idea can be applied to function-free programs  in order to reduce the cost of program instantiation. 1
　　the latter idea and evidence to its feasibility have been suggested by an anonymous referee.
 initial goal 
restricted split of type ii
contradiction rule
failure rule
resolution with hypothesis
success rule
contradiction rule
resolution with
resolution with hypothesis
success rulefigure 1: a skeptical derivationtheoretic tools and results. work in a similar direction comprises some pretty standard axiomatizations based on hilbertstyle systems  levesque  1 and sequent calculi  olivetti  1; bonatti and olivetti  1 . these axiomatizations for default and autoepistemic logic should be extended to firstorder theories  perhaps adapting the techniques introduced in this paper. an infinitary proof-theory can be found in  milnikel  1 . in  rosati  1   issues related to decidable nomonotonic reasoning in mknf are investigated. in  cenzer et al.  1   sufficient conditions for the existence of recursively enumerable stable models are identified. they do not ensure that all stable models are r.e.  so skeptical and credulous reasoning are not semi-decidable.
acknowledgments
the author is grateful to the anonymous referees for their deep and careful reviews  and for their precious suggestions.
references
 bonatti and olivetti  1  p. a. bonatti and n. olivetti. a sequent calculus for skeptical default logic. in proceedings of tableaux'1  number 1 in lnai  pages 1. springer verlag  1.
 bonatti  1  p. a. bonatti. resolution for skeptical stable semantics. journal of automated reasoning. to appear. preliminary version in  dix et al.  1 .
 cenzer et al.  1  d. cenzer  j. b. remmel  and a. vanderbilt. locally determined logic programs. in proc. of lpnmr'1  number 1 in lnai  pages 1  berlin  1. springer verlag.
 cholewin＞ski et al.  1  p. cholewin＞ski  v. marek  a. mikitiuk  and m. truszczyn＞ski. experimenting with nonmonotonic reasoning. in proc. of iclp'1. mit press  1.
 dix et al.  1  j. dix  u. furbach  and a. nerode  editors. proc. of lpnmr'1  number 1 in lnai  berlin  1. springer verlag.
 dung  1  p. m. dung. on the relation between stable and well-founded semantics of logic programs. theoretical computer science  1-1  1.
 eiter and gottlob  1  t. eiter and g. gottlob. complexity results for disjunctive logic programming and applications to nonmonotonic logics. in proc. of ilps'1. mit press  1.
 eiter et al.  1  t. eiter  n. leone  c. mateis  g. pfeifer  and f. scarcello. a deductive system for nonmonotonic reasoning. in  dix et al.  1   1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. the stable model semantics for logic programming. in proc. of the 1th iclp  pages 1. mit press  1.
 gottlob  1  g. gottlob. complexity results for nonmonotonic logics. journal of logic and computation  1-1  1.
 levesque  1  h. j. levesque. all i know: a study in autoepistemic logic. artificial intelligence  1-1  1.
 lifschitz and turner  1  v. lifschitz and h. turner. splitting a logic program. in proc. of iclp'1  pages 1- 1. mit press  1.
 lifschitz  1  v. lifschitz. computing circumscription. in proceedings of ijcai'1  pages 1  1.
 lloyd  1  j. w. lloyd. foundations of logic programming. springer-verlag  1.
 milnikel  1  r. s. milnikel. nonomonotonic logic: a monotonic approach. phd thesis  cornell university  1.
 niemela： and simons  1  i. niemela： and p. simons. smodels - an implementation of the stable model and wellfounded semantics for normal lp. in  dix et al.  1   1.
 olivetti  1  n. olivetti. tableaux and sequent calculus for minimal entailment. journal of automated reasoning  1-1  1.
 rosati  1  r. rosati. towards first-order nonmonotonic reasoning. in proc. of lpnmr'1  number 1 in lnai  pages 1  berlin  1. springer verlag.
 schlipf  1  j. schlipf. the expressive power of the logic programming semantics. in proc. of pods'1  1.

logic programming and
theorem proving
theorem proving

splitting without backtracking
alexandre riazanov and andrei voronkov
the university of manchester

abstract
integrating the splitting rule into a saturation-based theorem prover may be highly beneficial for solving certain classes of first-order problems. the use of splitting in the context of saturation-based theorem proving based on explicit case analysis  as implemented in spass  employs backtracking which is difficult to implement as it affects design of the whole system. here we present a  cheap  and efficient technique for implementing splitting that does not use backtracking.
1	introduction
case analysis in the form of the -rule is the core of tableaubased theorem proving methods. if our aim is to refute a set of clauses   we can reduce this task to refuting two simpler sets: and . this is justified by the following argument. to show that is false in all the models of the formula   one can separately consider two cases:
all the models of and the ones not necessarily satisfying but satisfying . the saturation based theorem proving has adopted the case analysis principle in the form of splitting that can be formulated in the propositional case as the following inference rule:

where is a set of clauses  and are clauses. unlike most of the other inference rules in saturation-based theorem proving that only modify a set of clauses by adding and removing some clauses  this rule makes two sets of clauses out of one. now  to refute the original set   one must separately refute the two new sets: and
.
　in the propositional case  this rule together with unit resolution gives a complete inference system. finding a refutation in this system can be organised  for example  as depth-first exploring of branches. after finding a refutation on one of

　　partially supported by grants from epsrc and the faculty of science and technology  the university of manchester. the first author is also partially supported by an ors award.
the splitting branches  the procedure backtracks and proceeds with the second branch. in what follows  we call this procedure explicit case analysis. a simple example of a propositional derivation which combines the splitting rule with resolution on branches is shown in figure 1. the current search state is depicted as a tree. the nodes contain clauses common for all the branches below them. the clause or clauses to which an inference rule has been applied is put in a shaded box. for a moment  ignore the labels of the edges  for example .
　in the first-order case the splitting rule can be formulated in the same way as in the propositional case with an additional restriction: the clauses and do not have common variables. this rule can be combined with inference rules like resolution and paramodulation. combined with some resolution strategies  it gives decision procedures for fragments of first-order logic  see e.g.  fermu：ller et al.  1  .
　the cost of naive backtracking is very high  since the clause set can contain tens of thousands of clauses  so after several splits a lot of memory is needed to store backtrack points. therefore  in practice  the spass prover  weidenbach et al.  1   explicit case analysis is implemented by adding to each clause its split history: the set of splits used to obtain this clause  represented by an array of bits. the split history also helps to implement intelligent backtracking: by analyzing the split history one can check which splits have actually been used to obtain a contradiction on the current branch.clauses which do not belong to the currently exploited branch are put in a special waiting list  and popped back from the waiting list upon backtracking. there are complications caused by simplification by unit equalities belonging to a branch which can be illustrated by the following example.
example 1 consider the split on the left of this picture:
	..	...
	.	


figure 1: a derivation with splitting and unit resolutionwhere and have no common variables  in the simplification order used in the proof-search  and is a clause with an occurrence of . after the split the left branch contains two clauses and   so can be simplified and replaced by   but only on the left branch  since there is no clause on the right branch. this means that should still be kept on the right branch  so simplification by should be implemented according to the right-hand side of the picture.
　to achieve efficiency  modern theorem provers maintain one or more indexes on terms  literals  or clauses. to implement splitting  maintainance and retrieval algorithms on indexes should be changed. either one should have common indexes for all branches  or else indexes for the current branch only. in the former case  index retrieval algorithms should be changed to take into attention the splitting history. in the latter case  index maintainance algorithms should be changed. in either case  a significant amount of work on indexes is required upon backtracking.
　in general  backtracking in resolution-based theorem provers requires a nontrivial implementation. in the modern saturation-based theorem provers for first-order logic  splitting with backtracking is implemented only in spass  weidenbach et al.  1 .
　in this report we discuss an alternative to the explicit case analysis which can be implemented in resolution-based theorem provers without a significant overhead. we call it splitting without backtracking. the idea of splitting without backtracking can be formulated as follows. consider again the split of example 1  but let us mark the branches of the search tree by literals as follows. we take a new propositional symbol and mark the left branch with and the right branch with :
.
..

if there are several splits  we introduce a new propositional symbol for every split. now  instead of dealing with branches  we simply add to each clause on a branch all labels used on that branch. then the split can be described as the following inference rule which replaces a clause by two new clauses:
it is not hard to argue that such a splitting preserves the unsatisfiability of the set of clauses  we will prove it formally below . the effect of splitting is very similar to the effect of the explicit case analysis  but branches are no more needed  since we are still dealing with just one set of clauses. the following simplification of by on the left branch can now be described in at least two possible ways:
the second way corresponds to the simplification of example 1  but we prefer the first way because it gives simpler clauses.
　we will show that splitting without backtracking can simulate  in some sense  explicit case analysis. moreover  by using various selection functions one can simulate a parallel version of case analysis  in which branches are exploited simultaneously. apart from being easy to implement  splitting without backtracking has several other advantages: firstly  we obtain the effect of intelligent backtracking for free  and secondly  some simplification rules can move clauses between the branches thus avoiding repeated work on different branches.
　this article is organized as follows. in section 1 we define some fundamental notions of this paper  for example those of component and split. we also recall some notions of the theory of resolution. in section 1 we discuss several versions of the splitting rule  namely binary and hyper splitting  with or without naming. in section 1 we show how one simulate different strategies of splitting by using appropriate literal selection functions. in particular  we show how one can simulate explicit case analysis by using so-called blocking extension of selection functions. an efficient algorithm for finding the maximal split of a clause into components is given in the full version of this paper. in section 1 we discuss the clause variance problem: checking if a clause is a variant of another clause. we show that the clause variant problem is polynomial-time equivalent to the graph isomorphism problem. finally  in section 1 we discuss experiments carried out with vampire over a large collection of problems.
1	preliminaries
we assume a fixed signature . by we denote a countable set of predicate symbols of arity disjoint from . elements of will be denoted by and used as new names for clause components  or alternatively  as identifiers for branches.
　we call a clause a set of literals. sometimes clauses will be written as disjunction of their literals. we denote by the set of all variables occurring in an expression  e.g. clause . for a clause   the universal closure of   denoted by   is the formula   where are all variables of .
1	components and splits
definition 1  component  the clause is called a component of a clause if is nonempty 
           and	contains no predicate symbols in	. the component	is called minimal if no proper subset of	is a component in	. the component	is called trivial if is a clause of the signature	or empty.
note that every clause has at least one component: the subset of consisting of the literals of the signature .
definition 1  split  a split of a clause is any representation of as   where are different components of . the split is trivial if and is the trivial component of . the split is maximal if are all the minimal components of .
note that in the maximal split the subclause only consists of the literals of the signature . also  it is obvious that the maximal split of a clause is unique.
1	resolution
our technique for implementing splitting works in the context of a saturation procedure for the calculus of ordered resolution. for the purposes of this paper it is enough to recall only the nonground version of the ordered resolution rule itself  for other rules and calculi see e.g.  bachmair and ganzinger  1 . let be a simplification order on terms  extended in the usual way on literals. we assume that a selection function select in every nonempty clause either a negative literal  or a subset of positive literals  and if any positive literal is selected  then all maximal w.r.t. literals are selected too. then ordered resolution is the following rule:

where is a most general unifier of and   the atom is selected in the clause   the literal is selected in the clause   and there is no literal in greater than in the used simplification ordering.
1	splitting without backtracking
in this section we formulate several versions of splitting without backtracking and prove their soundness.
1	binary splitting
definition 1  binary splitting rule  let be a set of clauses of the signature   be a clause with a nontrivial component   and be an atom not occurring in . then the binary splitting rule is the following inference rule:
 1 
we also say that is obtained from by binary splitting.
theorem 1 the binary splitting rule preserves satisfiability.
proof. any model for	is also a model for	since	is a logical consequence of and	. in the reverse direction  a model	for can be obtained from a model	for by redefining	so that	.
　it follows from the proof of theorem 1 that the new predicate symbol introduced by an application of the binary splitting rule  see  1   can be regarded as a new name of the formula . this implies an optimization of binary splitting which allows one to reuse the new names. this optimization can be formalized as follows.
definition 1  binary splitting with naming  a naming function is any function from the set of nonempty clauses of the signature to the set with the following property: if and only if is a variant of .
　let be a set of clauses of the signature and be a clause with a nontrivial component . let also be a naming function. then the binary splitting rule with naming is the following inference rule:
 1 
one can also consider a modification of the binary splitting rule with naming where only the first application of the splitting introduces the clause   and all further applications of splitting with the same component simply replace by one clause .
　let us give an example that illustrates the difference between the binary splittings with and without naming. suppose that we have a set of clauses
such that	is a nontrivial component in both	and
　　　. using binary splitting  we can perform the following derivation:
using binary splitting with naming  we can perform a different derivation  assuming  .
　preservation of satisfiability in the case of binary splitting with naming is more difficult to formulate. we can only guarantee preservation of satisfiability under some natural conditions.
theorem 1 let be an inference system on sets of clauses with the following properties: every inference preserves the set of models  i.e. for every inference in this system  and have the same models. let be the extension of by binary splitting with naming. then for every derivation in such that is a set of clauses of the signature and every   the set is satisfiable if and only if so is .
proof. the proof essentially repeats the proof of theorem 1. take any model of of the signature and define its extension to the signature as follows: define in each new predicate symbol be equivalent to . then arguing as in the proof of theorem 1 one can show that is a model of each .
　in the converse direction  note that every is a logical consequence of . indeed  when is obtained from
by applying an inference of the original system   this holds by our assumption. when is obtained from by applying binary splitting with naming  1   this is also obvious  since contains the clause . note
that this proof can be easily adapted to the modified version of splitting. for the modified version in general it is not true any more that contains the clause   but one can show that is a logical consequence of .
　the conditions on the inference system are natural and not restrictive. for example  the standard proofs of completeness in the theory of resolution  bachmair and ganzinger  1  use inference systems with two kinds of rule: addition of a clause implied by other clauses  and removal of a clause implied by other  smaller  clauses. it is easy to see that every such inference system preserves models.
1	hyper splitting rule
similar to the binary splitting  we can define hyper splitting  in which we split more than one component off a clause.
definition 1  hyper splitting rule  let be a set of clauses of the signature and be a clause with nontrivial components . let be predicate symbols in occurring in neither this clause nor . then the hyper splitting rule is the following inference rule:
 1 
let also be a naming function. then the hyper splitting rule with naming is the following inference rule:
 1 
　hyper splitting can be interpreted as repeatedly applied binary splitting. one can prove that hyper splitting  with or without naming  preserves satisfiability in exactly the same way as for binary splitting.
1	literal selection and simulation of explicit case analysis
so far we only considered splitting as an inference rule. here we will consider splitting as part of an inference system. as the inference system we consider binary resolution with superposition and negative selection. we assume a simplification ordering in which every literal of the signature is greater than any atom in the signature . in order to define the inference system we have to define a selection function on clauses. we will define two kinds of selection functions: blocking and parallel. both will be obtained by extending an arbitrary selection function on the clauses of the signature to the clauses of the extended signature .
1	blocking extension of a selection function
definition 1  blocking extension  let be any selection function on the clauses of the signature . the blocking extension of is the selection function on the clauses of the signature defined as follows.  a  let be a clause containing a negative literal of the signature   then the maximal one among such literals is selected in .  b  let be a clause containing a positive literal of the signature and no negative literals in this signature. then this literal is selected only when is the maximal literal in . note that in this case solely consists of positive literals of the signature
.
it is not difficult to prove that we indeed obtain a selection function  since all literals of the signature are less than all literals of the signature . since we will usually assume some selection function on the clauses of   we will simply speak about the blocking selection function instead of the blocking extension of the selection function.
　now assume that we are using a blocking selection function. let us also assume that each time we apply the splitting rule  the newly introduced atom is greater than any atom previously introduced by splitting. let us show that the resulting inference system simulates  to some extent  explicit case analysis.
　consider a sequence of splits and applications of nonsimplifying inference rules. let us call the label of a branch the set of all positive literals that mark edges of this branch. we transform clauses used in a derivation with splitting as follows. to each clause that appears on a branch we add the label of this branch  obtaining the clause . we can show that each nonsimplifying inference that can be performed on a branch  can also be performed on the transformed clauses. for simplicity  we only consider the resolution rule.
	let	and	be two clauses in which
and are selected  be a most general unifier of and   and no literal in is greater than . then we can apply a resolution inference as follows.

let and be the labels of the branches of and respectively. then the corresponding transformed clauses will be and . by our definition of a blocking selection function and the order it is not hard to argue that and will be selected in the transformed clauses and that no literal in is greater than . therefore  we can apply the rule

it is not hard to argue that is the label of the common branch for and .
　it is interesting to observe what happens to the literals which do not belong to the currently investigated branch. suppose  for  simplicity  that we are working with the leftmost branch labelled by . then all clauses which are not on the current branch contain a negative literal of the signature   which by our choice of the selection function  will be selected. take any such clause  for example  . the only inference rule we can apply to this clause is resolution with a clause   where is greater than any literal of . but we can obtain such a clause only when the empty clause is obtained on the leftmost branch labelled by   so will be blocked for inferences until a contradiction on the leftmost branch is obtained. as soon as such a contradiction is obtained  we can  unblock  the clause by resolving it against and obtaining .
　thus  we obtain a nearly complete correspondence between search states of the explicit case analysis and the system with the binary splitting without backtracking. it is not an exact correspondence for several reasons. firstly  simplification rules behave in a slightly different way. secondly  splits done by splitting without backtracking often do not belong to particular branches  but are  shared  across different branches. to explain this effect  we show in figure 1 a derivation using binary splitting and a blocking selection function which corresponds to the derivation of figure 1. we preserve the same enumeration of inferences as in that figure. if a literal in the new signature is selected  we put it in front of the clause  for example means that is selected. for simplicity  we remove subsumed clauses and pure literals.
　after inference 1 of the derivation using explicit case analysis  we have a copy of the clause which was put on the right branch  after we performed a split on this clause on the left branch. in the derivation using binary splitting  the split of into and remains even when we finish working on the left branch.
1	parallel extension of a selection function
definition 1  parallel extension  let be any selection function on the clauses of the signature . the parallel extension of is a selection function on the clauses of the signature defined as follows. let be a clause of the form   where is a clause of the signature and is a clause of the signature . if is not empty  then selects in exactly the same literals as selects in . when is empty  the maximal literal from is selected.
in other words  the parallel extension always makes the selection in a clause ignoring the literals of of the signature
.
　again  it is not difficult to prove that we indeed obtain a selection function  since all literals of the signature are less than all literals of the signature . we will simply speak about a parallel selection function instead of the parallel extension of a selection function.
　a parallel selection function ignores the splitting history of clause completely  and thus gives the effect of parallel search on all branches.
1	subsumption resolution and splitting
one of the serious advantages of splitting without backtracking is that it can be productively combined with the simplification rule called subsumption resolution  bachmair and ganzinger  1 . essentially  subsumption resolution is a special case of binary resolution that guarantees that the resolvent subsumes one of the parents. we say that a clause subsumes a clause if there exists a substitution such that . it is known that clauses subsumed by other clauses can be removed from the search space.
　subsumption resolution is the following inference rule on sets of clauses:
where there exists a substitution	such that	and
　　　　　. the rule is called subsumption resolution because of its resemblance to subsumption: this rule is applicable if and only if subsumes . thus subsumption algorithms can be modified for finding pairs of clauses to which subsumption resolution is applicable.
　since subsumption resolution is a simplification rule  the clause or is replaced by a smaller clause
      it can be applied independently of the selection function or order on literals.
　subsumption resolution combined with splitting can give the following effect: literals can be moved across different branches. for example  consider two clauses and


figure 1: a derivation using binary splitting and blocking selection	such that	subsumes	. in explicit case analysis  these
clauses would belong to two different branches  because should be on the left branch of splitting on   while
on the right branch. if we use subsumption resolution  we can replace by the clause   which corresponds to moving from the right branch to the top.
　the effect of subsumption resolution is even stronger when is a variant of or coincides with . then we obtain two clauses and   which by subsumption resolution followed by subsumption will be replaced by . this has the effect of noting that belongs to two different branches  and putting it on top and can save a lot of resources compared to explicit case analysis. splitting with backtracking would repeat inferences with on different branches.
　let us show that subsumption resolution and splitting can  in some cases  give the effect of condensing a clause. condensing is defined in  joyner  jr.  1 . a clause is said to be obtained from a clause by condensing if is a proper subset of and subsumes .
example 1 let be a clause with variables in and be a sequence of variables disjoint from . then is a variant of . consider the clause . this clause can be condensed into . binary splitting will replace this clause by two clauses and application of subsumption resolution to these clauses yields the clause which subsumes each of these clauses. thus  binary splitting and subsumption resolution in this example give the effect of condensing.
1	complexity of splitting with naming
to implement splitting with naming  it is required to check whether a component is a variant of another component. in this section we provethat the complexity of checking whether one clause is a variant of another one is polynomial-time equivalent to the graph isomorphism problem.
definition 1  clause variance problem  clause variance is the following decision problem. an instance of this problem is a pair of clauses . the answer is  yes  if is a variant of .
theorem 1 the clause variance problem is polynomialtime equivalent to the graph isomorphism problem.
the proof is given in the full version of this paper.
　more generally  in practice we have to check whether a given component is a variant of a component in a set of components . in vampire we implement this many-to-one clause variance check using the code tree indexing described in  voronkov  1; riazanov and voronkov  1b . code trees are used in the implementation of many-to-one forward subsumption  it is not hard to modify them to implement the many-to-one clause variance.
1	experimental results
we implemented several versions splitting with backtracking in the new version of vampire  riazanov and voronkov  1 . in this section we present experimental results over 1 nonunit clause form problems. of these problems  1 problems come from the tptp library  sutcliffe and suttner  1  and 1 problems from the experiments in list software reuse described in  schumann and fischer  1 . of the 1 problems 1 contain equality. for problems with equality  in addition to splitting without backtracking we implemented the branch rewriting rule which replaces a clause by a clause   if a clause
with is present. the effect of this simplification rule is similar to the effect of simplification by unit equalities on a splitting branch in an explicit case analysis procedure. to support our viewpoint that we describe a  cheap  implementation of rewriting  we note that branch rewriting was implemented in less than 1 hours.
　we present the results for problems with and without equality separately. for all problems we used the same algorithm based on the limited resource strategy  riazanov and voronkov  1a  and the same selection function. this algorithm and selection function were chosen because they experimentally proved to be superior to other algorithms and selection functions when splitting was not used. when running vampire with splitting  we used the following parameters:
1. hyper splitting with a parallel selection function  p  vs.
binary splitting with no selection.
no11111111pnr1111111nr111111pn11111n1111pr111p11r1pnrnrpnnprprfigure 1: results on problems with equality
pn1111n111no11p1nnopfigure 1: results on problems without equality
1. naming  n  vs. no naming.
1. branch rewriting  r  vs. no branch rewriting.
we will use combinations of the three letters p  n  r  to denote different strategies used in the experiments. for example  pr means that the parallel version with branch rewriting but no naming was used. we use  no  to denote the strategy that uses no splitting at all  and the strategy that uses splitting without p r  and n  i.e. simple binary splitting with a blocking selection function.
　we used a linux-running pc with 1m ram and a 1mhz intel iii processor. for all problems we used the time limit of 1 minute. the chosen strategy without splitting solves 1 problems. the total number of problems solved with or without splitting is 1. so with splitting  vampire could solve 1 problems that could not be solved without splitting. of these 1 problems  1 are without equality and 1 with equality. figures 1 and 1 show the comparative behavior of various strategies depending on the settings of the parameters p  n  r on problems with and without equality  respectively. the pair in a row labelled by a strategy and the column labelled by a strategy means that there
were problems solved by but not solved by and problems solved by but not solved by . for example nr  naming + branch rewriting  could solve 1 problems with equality not solved by pn  parallel selection + naming   while pn could solve 1 problems with equality not solved by nr.
　the following conclusion can be derived from these results:
1. on problems with equality  vampire with any version of splitting is on the average weaker than vampire without splitting  while on problems without equality it is on the average stronger.
1. each of the following settings: naming  parallel splitting  and branch rewriting improve the results  by far the strongest one is naming.
　we believe that the performance of splitting without backtracking can be considerably improved  but optimized implementation  more experiments  and insights in the behavior of splitting are required.
references
 bachmair and ganzinger  1  l. bachmair and h. ganzinger. resolution theorem proving. in a. robinson and a. voronkov  editors  handbook of automated reasoning  volume i  chapter 1  pages 1. elsevier science  1.
 fermu：ller et al.  1  c. fermu：ller  a. leitsch  u. hustadt  and t. tammet. resolution decision procedures. in a. robinson and a. voronkov  editors  handbook of automated reasoning  volume ii  chapter 1  pages 1. elsevier science  1.
 joyner  jr.  1  william h. joyner  jr. resolution strategies as decision procedures. journal of the acm  1 :1  july 1.
 riazanov and voronkov  1a  a. riazanov and a. voronkov. limited resource strategy in resolution theorem proving. preprint cspp-1  department of computer science  university of manchester  october 1.
 riazanov and voronkov  1b  a. riazanov and a. voronkov. partially adaptive code trees. in m. ojeda-aciego  i.p. de guzma＞n  g. brewka  and l.m. pereira  editors  logics in artificial intelligence. european workshop  jelia 1  volume 1 of lecture notes in artificial intelligence  pages 1  ma＞laga  spain  1. springer verlag.
 riazanov and voronkov  1  a. riazanov and a. voronkov. vampire 1  system description . siena  italy  june 1. accepted to ijcar 1.
 schumann and fischer  1  j. schumann and b. fischer. nora/hammr: making deduction-based software component retrieval practical. in proc. automated software engineering  ase-1   pages 1  lake tahoe  november 1. ieee computer society press.
 sutcliffe and suttner  1  g. sutcliffe and c. suttner. the tptp problem library - cnf release v. 1.1. journal of automated reasoning  1   1.
 voronkov  1  a. voronkov. the anatomy of vampire: implementing bottom-up procedures with code trees. journal of automated reasoning  1 :1  1.
 weidenbach et al.  1  c. weidenbach  b. afshordel  u. brahm  c. cohrs  t. engel  e. keen  c. theobalt  and d. topic. system description: spass version 1.1. in h. ganzinger  editor  automated deduction-cade-1. 1th international conference on automated deduction  volume 1 of lecture notes in artificial intelligence  pages 1  trento  italy  july 1.
unsearchmo: eliminating redundant search space on backtracking for
forward chaining theorem proving
lifeng he
faculty of information science and technology
aichi prefectural university  aichi  1 japanabstract
this paper introduces how to eliminate redundant search space for forward chaining theorem proving as much as possible. we consider how to keep on minimal useful consequent atom sets for necessary branches in a proof tree. in the most cases  an unnecessary non-horn clause used for forward chaining will be split only once. the increase of the search space by invoking unnecessary forward chaining clauses will be nearly linear  not exponential anymore. in a certain sense  we  unsearch  more than necessary. we explain the principle of our method  and provide an example to show that our approach is powerful for forward chaining theorem proving.
1	introduction
automated reasoning is one of the most important topics for artificial intelligence and computer science. among others  theorem proving technologies for first-order predicate calculus have been attracting much interesting  robinson  1; loveland  1; manthey and bry  1 .
　it is well-known that if is an unsatisfiable subset of a clause set   it is generally easier to show unsatisfiable than to show unsatisfiable. obviously  the smaller such a is  the shorter a proof is. if forward chaining is used for reasoning about non-horn clauses  it means that we need not consider every violated clause  but only those that can help us to find a refutation. it is obvious that invoking unnecessary non-horn clauses will explode the search space exponentially.
　satchmo  satisfiability checking by model generation   manthey and bry  1  is potentially inefficient  since it uses all violated clauses to forward chaining. addressing to this problem  two strategies have been developed. one is utilizing an intelligent strategy to only select those relevantlike non-horn clauses for forward chaining  which is proposed in  ramsay  1   refined in  loveland et al.  1 
 satchmore  and further improved in  he et al.  1 

　　this work is partially supported by the japanese ministry of education  science  sports and culture and the artificial intelligence research promotion foundation of japan.
  -satchmore . the other method  proposed in  he  1   called i-satchmo  is eliminating redundant searching space after unnecessary non-horn clauses have been used for forward chaining by intelligent backtracking.
　as indicated in  he  1   there are mainly two problems about the approach to select a suitable non-horn clause for forward chaining. one is that it takes much overhead cost to decide whether a non-horn clause is suitable or not. in general  the stricter the checking is  the more expensive the overhead cost is. the other is that any such strategy is only effective in a limited range and certainly fails in some more complicated cases. in fact  a non-horn clause recognized as  relevant  by some advanced selecting strategy might be practically unnecessary to the refutation being made. in other words  there are always such irrelevant non-horn clauses that can pass any relevancy test. these problems have been illustrated on some examples in  he  1 .
　the principle of the method of eliminating redundant searching space by intelligent backtracking is very simple: if one of the consequent atoms of a forward chaining clause used for forward chaining is found to be useless to the reasoning on backtracking  the use of this clause is known as unnecessary  and the remaining processing over the clause's consequence is immediately abandoned. it only takes a little overhead cost to find irrelevant non-horn clauses and is compatible with the former strategy introduced above.
　however  marking those consequent atoms contributed to derive antecendents of irrelevant forward chaining clauses as useful and mixing up the useful consequent atoms for different branches in a proof tree  i-satchmo might make redudant search.
　this paper introduces a solution for these problems. as we will see  in our improved method  for each node in the proof tree  only those indispensable consequent atoms to derive the refutation at the node are marked as useful. thus  our method can show a refutation for a given unsatisfiable clause set without unsearching more than necessary  hence the acronym unsearchmo . in other words  our method is one of the most efficient strategies for forward chaining theorem proving.
1	satchmo and i-satchmo
we assume familiarity with the details relating to satchmo  manthey and bry  1   satchmore  loveland et al.  1  as well as -satchmore and limit ourselves to briefly reviewing the basic material needed for our presentation.
　in this paper  a problem for checking unsatisfiability is represented by means of positive implicational clauses  each of which has of the form	 
  . we refer to the implicit conjunction as the antecedent of the clause  with each being an antecedent atom. the implicit disjunction is referred to as the consequent of the clause  and each is a consequent atom. a clause with an empty consequent  i.e.    is called a negative clause and is written as   where means false. a clause with an empty antecedent  i.e.    is written with consequent part only if it is a fact  horn clause   and otherwise the antecedent atom true is added. moreover  a given clause set is divided into two subsets. one is   the backward chaining component that is a decidable horn clause set consisting of all negative clauses  facts and any horn clauses selected by user. the other is
     the forward chaining component that contains all the remaining clauses.
　if goal  a conjunction of atoms  can be proven to logically follow the clause set   we denote that with . on the other hand  if is a ground atom set and a ground atom  indicates that atom is a member of . because we utilizes forward chaining on non-horn clauses same as satchmo  all of clauses must hold the so-called rangerestricted property1 to guarantee the soundness for refutation.
　suppose that be a decidable horn clause set and a ground atom set  then a conjunction  disjunction  of ground atoms is satisfiable in if all  some  of its members can be derived from   and else unsatisfiable. a ground clause is said satisfiable in if is satisfiable or is unsatisfiable in   and else violated.
　it is well-known that a clause set is satisfiable if and only if we can find a model for . if is a model of   then each clause of is satisfiable in . based on such consideration  to check whether a given clause set is satisfiable  satchmo goes to construct a model for
this clause set by trying to satisfy all clauses in	.
　the reasoning made by satchmo to search a model for a given clause set can be graphically illustrated in a proof tree  abbreviated to pt hereafter  described as follows.
　for the current node	 initially the root node	   and the ground consequent atom set	 initially empty :
1. if	  create a leaf node	below node	.
the process for the branch terminates.
1. if   select an instantiated ground violated clause	from . if no such
clause exists  the process terminates to report that is satisfiable.


	figure 1: the proof tree	constructed by satchmo
1. for each a consequent atom of the selected violated clause  create a child node below the current node. taking node as new current node  call this procedure recursively in depth-first strategy with the augmented ground consequent atom set   where =
.
　for node	  where the ground consequent atom set is	  if all branches terminate in a leaf node	  it means that satisfying atom	there will lead to a refutation anyway. in other words 	cannot be extended as a model of	.
such node is said to be unsatisfiable. when the node is the root node   then is unsatisfiable. corresponding to creating node in   an atom is asserted into the reasoning database. on the other hand  when node is proven to be unsatisfiable  the atom will be retracted from the database. moreover  whenever a node is proven unsatisfiable  the reasoning process will backtrack to its parent node if any.
example 1 consider the following propositional clause set
.
　the proof tree generated by satchmo is shown in figure 1. it is obvious that satchmo made some redundant search. in fact  a refutation can be derived from the subset consisting of and the only clause .
　by the way  for the given clause set in this example  every violated clause used by satchmo is recognized as  relevant  by either satchmore or -satchmore  the proof tree generated by satchmore or -satchmore is the same as that shown in figure 1.
　the redundant search space can be cut down by intelligent backtracking presented in i-satchmo. as we have seen  a consequent atom is retracted from the reasoning database only if the node in the proof tree has been proven as unsatisfiable. suppose that be the violated
clause used for forward chaining at node in the proof tree  then  node holds child nodes     . if some consequent atom is found to be useless to the reasoning at the time being retracted from the database  hence node has been proven as unsatisfiable   i.e.  it is useless to construct the partial proof tree below node   the same partial proof tree can be directly constructed below node   and therefore 

	figure 1: the deriving tree for goal	at node
at this point  we know that node can be proven as unsatisfiable just in the same way made at node . since has been known as unsatisfiable  the remaining process for showing node unsatisfiable  i.e.  the process on the consequent atoms      if any   can be removed at once. on the other hand  the use of an clause is necessary only if each     is useful to constructing the partial tree below node .
　now we introduce i-satchmo how to decide what consequent atom is useful for the reasoning. obviously  a consequent atom asserted can only be used to derive goal or antecedents of violated clauses. therefore i-satchmo marks an asserted consequent atom as useful whenever it contributed to derive goal or an antecedent of a violated clause.
　the useful consequent atoms to derive goal can be found from the deriving tree of goal  he  1 .
definition 1 suppose that a give clause set be
     the current consequent atom set established by forward chaining  a goal such that . the deriving tree of goal with respect to   denoted by   is the result tree constructed as follows.
1. the root node is	.
1. suppose that	is a node in the tree being constructed.
	for each	clause	such that
　　　　  where is a most general unifier of and   create a child node
.
	for each atom	such that	  where
is a ground substitution  since is a ground atom   create a child node     where	.
1. repeat this procedure recursively until an empty leaf or a leaf such that all of its atoms hold     is derived. such a leaf is called successful deriving leaf.
　since is decidable and   can be finally constructed with finding a successful deriving leaf. the consequent atoms that contributed to derive from are those atoms listed in the successful deriving leaf if any.
example 1 suppose that be the clause set given in example 1. consider the leftmost node in figure 1  where . the deriving tree for goal  
           is shown in figure 1. to derive goal	  only	is useful.
　the algorithm of i-satchmo to construct pt for checking whether a given clause set is unsatisfiable can be described as follows.
　for the current node  initially the root node   and the corresponding consequent atom set  initially empty :
1. if   create a leaf node under the current node  which means that node is proven unsatisfiable. mark all consequent atoms listed in the successful deriving node in as useful.
1. if   select an instantiated ground violated clause from . if no such clause exists  the process terminates to report that is not unsatisfiable.
1. for the selected violated non-horn clause	;	;
     mark all consequent atoms listed in the successful deriving node in as useful. then  create a node below the current node for each such that
　　　　　. taking as the new current node  call this procedure recursively in depth-first strategy with the augmented consequent atom set = . however  if has been found to be useless on backtracking  instead of each node such that   a special leaf node is created  where means that the process from there is pruned away. on the other hand  if all
are marked as useful  the use of ; ; is known as necessary. in either case  node is proven as unsatisfiable.
example 1 suppose that be the clause set given in in example 1.
　in the branch	in figure 1  only atom	is used in forward chaining and marked as useful. in the same way  in the branch	  only atom	is used in forward chaining and marked as useful.
　since both and are proven to be unsatisfiable  its parent node  i.e.  is proven to be unsatisfiable. we backtrack to node to retract . at that time  we know is not marked as useful in the reasoning. therefore  it is useless for the reasoning and the use of the violated clause selected at node is unnecessary. i-satchmo prunes away the process to be made on the remaining consequent atom and backtracks to node at once.
　now  we know that is also not marked as useful in the reasoning  therefore the violated clause selected at the root node is unnecessary. accordingly  we stop to process the remaining consequent atom .
　in this way  the proof tree constructed by i-satchmo is shown in figure 1. as we see  unnecessary clauses are split only once.
1	unsearchmo
since i-satchmo marks an asserted consequent atom as useful whenever it contributes to derive thea antecendent of a violated clause  those consequent atoms contributed to unnecessary violated clauses are also marked as useful.
　this problem can be easily solved by improving the marking work as follows: an asserted consequent atom is marked as useful only when it contributes to derive goal or the

figure 1: the proof tree constructed by i-satchmo in example 1

figure 1: the proof tree constructed by i-satchmo for the clause set given in example 1
antecedent of a violated	clause that has been verified as necessary.
　moreover  without distinguishing the useful consequent atoms for different branches in proof tree  i-satchmo might mismark those consequent atoms that are useful for the refutation of unnecessary branches in a proof tree but useless for the necessary branches as  useful . in such cases  isatchmo also possibly makes some redundant search. let us see the following example.
example 1 consider the following propositional clause set
.
　the proof tree generated by i-satchmo is shown in figure 1.
	consider the leftmost node labeled with	  where
       the clause becomes violated and is used to forward chaining. since both and are useful for deriving   the use of the clause is found to be necessary to the refutation  and are marked as useful. in this way  the process for the consequent atom  the sibling atom of   and  the sibling atom of   are made  respectively.
　obviously  the process for node in the above example is unnecessary. although the consequent atom is useful for each branch from node   it is useless for any branch from node . it means that the refutation at node can be made without the consequent atom   that is  that can be made on the root node in the same way. therefore  the redundant process for the consequent atom can be eliminated.
　although the process made at the leftmost node is also unnecessary in a strict meaning  it is unavoidable unless we figure 1: finding the useful consequent atom set for parent node from those of its children nodes
select atom first to process at node or we first select the last non-horn clause for forward chaining. it is the wellknown nondetermination problem for theorem proving.
　now  we consider how to distinguish and manage the useful consequent atoms for different branches in a proof tree. without the loss of generality  shown as in figure 1  suppose that ; ; be the violated clause selected at node and be the useful consequent atom set relative to
for all such that . we consider how to derive the useful consequent atom set relative to node .
　since our reasoning procedure is depth-first one  is not taken into processing until node has been proven to be unsatisfiable. moreover  only when some of is found to be useless or all node are proven to be unsatisfiable  their parent node is proven to be unsatisfiable.
　as we will know from the algorithm of unsearchmo  when node is proven to be unsatisfiable  the useful consequent atom set for deriving the refutation at node has already been established. the useful consequent atom set for node is constructed according to the way how node is proven as unsatisfiable.
　if node is proven as unsatisfiable in the way that all of are proven as unsatisfiable  then the use of the clause ; ; is found to be necessary. in order to complete the refutation at node   the clause should be ably found to be violated there. therefore  the useful consequent atoms to derive the antecedent are useful for our refutation. moreover  in order to complete the refutation made at each node   each useful consequent atom for the refutation made at node   except itself  since it can be obtained by splitting ; ; at node    is also needed. concludingly  in this case  the useful consequent atom set can be derived as follows.
 1 
where is the useful consequent atom set for deriving the antecedent .
　on the other hand  if node is proven as unsatisfiable in the way that some of is found to be useless  then the use of the clause ; ; is found to be unnecessary for the refutation made on node   i.e.  . in other words  the refutation found at node can also be made at node without using the clause ; ; .
thus  the useful consequent atom set for deriving a refutation at node is exactly the same as   i.e. 
 1 
　in any case  when is derived  all of are removed from database.
　let us make a comparison of i-satchmo's method and our new strategy. when node is proven as unsatisfiable in the way that all of are proven as unsatisfiable  the two methods are exactly similar  the useful consequent atom set is obtained according to the formula  1 . however  when
node is proven as unsatisfiable in the way that node is found to be useless  the useful consequent atom set derived by i-satchmo is as follows.
 1 
obviously  although the consequent atoms contained in
　　　　　　 	 	are useful for the branches beginning from	 	 	  but useless for the refutation made at node	  and therefore is also useless to prove	unsatisfiable.
　as we have seen  if a consequent atom  except the last atom of the consequence  of a violated clause is found to be useful  then its next sibling atom will be taken into processing. therefore  the fewer the useful consequent atoms are  the more efficient a refutation is. it is quite clear that the useful consequent atom set for any node derived by our new strategy cannot be reduced any further  i.e.  it is a minimal one. since we can keep a minimal useful consequent atom set for a refutation  our new strategy is one of the most efficient approaches for forward chaining theorem proving.
　the algorithm of our prover unsearchmo to construct pt for checking whether a given clause set is unsatisfiable can be described as follows.
　for the current node  initially the root node   and its corresponding consequent atom set  initially empty :
1. if   create a leaf node below node   which means node is proven unsatisfiable. the useful consequent atom set for node     that consists of those consequent atoms listed in the successful deriving leaf in   is established.
1. if   select an instantiated ground violated clause from . if no such clause exists  is satisfiable.
1. for the selected violated non-horn clause	;	;
     create a node for all such that . taking as the new current node  call this procedure recursively in depth-first strategy with the augmented consequent atom set . however  if
is found to be useless on backtracking  instead of each node such that   create a special node
　. the useful consequent atom set for node     is established according to the formula  1 . on the other hand  if every for is found to be useful  the useful consequent atom set is established according to the formula  1 . in either case  node is proven unsatisfiable.
example 1 suppose that be the clause set given in in example 1. the proof tree constructed by unsearchmo is shown in figure 1. redundant search space has been eliminated.
　the proof of the correctness of our unsearchmo is not difficult. since unsearchmo just cuts down some branches in the proof tree constructed by satchmo  if satchmo can show a given clause set unsatisfiable  then unsearchmo can certainly show the same thing. this shows the completeness of unsearchmo.
　on the other hand  as we have indicated above  an asserted consequent atom is found useless at the time being retracted from the reasoning database  its parent node is able to be proven as unsatisfiable. therefore  all remaining process to show its parent node unsatisfiable can be removed without the loss of the soundness. the further details are omitted here since the lack of space.
　it is easy to implement unsearchmo by prolog. the code is omitted here also because of the lack of space.
1	an example
example 1 let be the following clause set  an extension of the benchmark problem syn1 given in tptp problem library  sutcliffe and suttner  1 .
　the first clause will be fully split by any of satchmo  satchmore and -satchmore. that means more than cases will be considered. as a result  we gave up the our tests on satchmo  satchmore and satchmore with a run of three days without termination on an intel pentiumiii/1mhz workstation  respectively.
　now  we see how i-satchmo and unsearchmo work. initially  the first clause is used for forward chaining with each substitution such that its antecedent becomes satisfiable. in this way  a branch
is generated.
　at node   the second clause with the substitution       i.e. 
	  is first split. for node	 
can be derived and the useful consequent atom set is established as   . at node   can

figure 1: the proof tree constructed by unsearchmo for the clause set given in example 1
not be derived  and the second	clause with the substi-
tution	 	 	  i.e. 
　　　　;   is split. at node   similar as node   is derived and the useful consequent atom set is   . for node   similar as node   no refutation can be derived  then the second non-horn clause will be split with the substitution  
	 	  and so on.
the second non-horn clause will be further split until is used for forward
chaining at node . then  at last a refutation can be derived from each of node 's child node simultaneously. the useful consequent atom set for the refutation at node is and that at node is just . according to the formula  1   the useful consequent atom set for node is constructed as	.
　if we do not distinguish the useful consequent atoms for different branches  as i-satchmo does  we will find that all -facts have been marked as useful. then  the same process made on each -fact   where each of   and is one of   and   would be repeated for the corresponding -fact  also for the corresponding -fact  . similar to other existing strategies  no answer can be obtained within a reasonable time.
　however  at node   where the useful consequent atom set is   unsearchmo finds useless for the refutation  i.e.  the clause
　　　　　　　　　　selected at node 's parent is not necessary. according to the formula  1   the useful consequent atom set derived at its sibling node
　　　　  i.e.    is abandoned  and the useful consequent atom set for node is still
.
	similar	backtracking	repeatedly	continues	to	node
         where the useful consequent atom set established by unsearchmo is still . the same process made on node will be also made on its sibling node and node   and the corresponding useful consequent atom set are found to be and
  respectively. the violated clause
	selected at node	is
found to be necessary.
　now  let us see what happens when unsearchmo backtracks to 's parent node   where  according to the formula  1   useful consequent atom set is surprisedly found to be empty! no other consequent atom is useful for our refutation! unsearchmo simply backtracks to the root node to conclude that the given clause set is unsatisfiable.
　with the help of distinguishing the useful consequent atoms for different branches  unsearchmo only takes 1 seconds to find its solution on an intel pentiumiii/1mhz workstation.
1	conclusion
in this paper  we have introduced how to eliminate redundant search space on backtracking for forward chaining theorem proving as much as possible. the redundant search branches are immediately pruned away when unnecessary ones are found on backtracking. at any reasoning point  only those necessary violated clauses are completely split for forward chaining. repeated search by invoking unnecessary nonhorn clauses can be effectively eliminated. the experimental result has shown that our method is powerful for forward chaining theorem proving.
　since our strategy prunes away unnecessary proof branches after unnecessary non-horn clauses are used for forward chaining  the violated non-horn clauses for forward chaining can be used in any order decided by any advance checking strategy. that is  we can incorporate the relevancy checking proposed in satchmore and the availability testing introduced in -satchmore into our strategies to improve the performance further.
　it is obvious that our method can be applied to disjunction logic programming  database . the principle of our method to eliminate redundant search space in forward chaining based prover can also be applied to reason about other logics whenever forward chaining strategy is used. this gives our method a wide application in automated reasoning field.
　as future works  we will use our prover to test those benchmark problems provided in tptp library  sutcliffe and suttner  1   to clear which class of problems will be helped by our approach.
references
 bry and yahya  1  bry  f. and yahya  a.: positive unit hyperresolution tableaux and their application to minimal model generation. journal of automated reasoning  1-1  1.
 he et al.  1  he  l.  chao  y.  simajiri  y.  seki  h. and itoh  h.: -satchmore: satchmore with availability checking. new generation computing  1  1.
 he  1  he  l.: i-satchmo: an improvement of satchmo. to appear in j. of automated reasoning.
 loveland  1  loveland  d.w.: mechanical theorem proving by model elimination. j. of the acm  1  1.
 loveland et al.  1  loveland  d.w.  reed  d.w. and wilson  d.s.: satchmore: satchmo with relevancy. journal of automated reasoning  1-1  1.
 manthey and bry  1  manthey  r. and bry  f.: satchmo: a theorem prover implemented in
prolog. in proceedings of 1th intl. conf. on automated deduction  1.
 ramsay  1  ramsay  a.: generating relevant models. journal of automated reasoning  1-1  1.
 robinson  1  robinson  j.a.: a machine-oriented logic based on the resolution principle. j. of ass. comput. mach.  1  1  1.
 sutcliffe and suttner  1  sutcliffe  g. and suttner  c.: http:	www.cs.jcu.edu.au   tptp
theorem proving with structured theories
sheila mcilraith  and eyal amir
department of computer science 
gates building  wing 1a
stanford university  stanford  ca 1  usa
{sheila.mcilraith eyal.amir} cs.stanford.eduabstract
motivated by the problem of query answering over multiple structured commonsense theories  we exploit graph-based techniques to improve the efficiency of theorem proving for structured theories. theories are organized into subtheories that are minimally connected by the literals they share. we present message-passing algorithms that reason over these theories using consequence finding  specializing our algorithms for the case of first-order resolution  and for batch and concurrent theorem proving. we provide an algorithm that restricts the interaction between subtheories by exploiting the polarity of literals. we attempt to minimize the reasoning within each individual partition by exploiting existing algorithms for focused incremental and general consequence finding. finally  we propose an algorithm that compiles each subtheory into one in a reduced sublanguage. we have proven the soundness and completeness of all of these algorithms.
1	introduction
theorem provers are becoming increasingly prevalent as query-answeringmachinery for reasoning over single or multiple large commonsense knowledge bases  kbs  . commonsense kbs  as exemplified by cycorp's cyc and the high performanceknowledgebase  hpkb  systems developedby stanford's knowledge systems lab and by sri often comprise tens/hundreds of thousands of logical axioms  embodying loosely coupled content in a variety of different subject domains. unlike mathematical theories  the original domain of automatedtheoremprovers  commonsensetheories are often highly structured and with large signatures  lending themselves to graph-based techniques for improving the efficiency of reasoning.
　graph-based algorithms are commonly used as a means of exploiting structure to improve the efficiency of reasoning in bayes nets  e.g.     constraint satisfaction problems  csps   e.g.    and most recently in logical reasoning  e.g.   1; 1; 1  . in all cases  the basic approach is to

  knowledge systems laboratory  ksl 
convert a graphical representation of the problem into a treestructured representation  where each node in the tree represents a tightly-connected subproblem  and the arcs represent the loose coupling between subproblems. inference is done locally at each node and the necessary information is propagated between nodes to provide a global solution. inference thus proves to be linear in the tree structure  and often worstcase exponential within the individual nodes.
　we leverage these ideas to perform more efficient sound and complete theorem proving over theories in first-order logic  fol  and propositional logic. in this paper we assume that we are given a first-order or propositional theory that is partitioned into subtheories that are minimally coupled  sharing minimal vocabulary. sometimes this partitioning is provided by the user because the problem requires reasoning over multiple kbs. other times  a partitioning is induced automatically to improve the efficiency of reasoning.  some automated techniques for performing this partitioning are discussed in  1; 1 .  this partitioning can be depicted as a graph in which each node represents a particular partition or subtheory and each arc represents shared vocabulary between subtheories. theorem proving is performed locally in each subtheory  and relevant information propagated to ensure sound and complete entailment in the global theory. to maximize the effectiveness of structure-based theorem proving we must 1  minimize the coupling between nodes of the tree to reduce information being passed  and 1  minimize local inference within each node  while  in both cases  preserving global soundness and completeness.
　in this paper we present message-passing algorithms that reason over partitioned theories  minimizing the number of messages sent between partitions and the local inference within partitions. we first extend the applicability of a message-passing algorithm presented in  to a larger class of local reasoning procedures. in section 1 we modify this algorithm to use first-order resolution as the local reasoning procedure. in section 1 we exploit lyndon's interpolation theorem to provide an algorithm that reduces the size of the communication languages connecting partitions by considering the polarity of literals. finally  in section 1 we attempt to minimize the reasoning within each partition using algorithms for focused and incremental consequence finding. we also provide an algorithm for compiling partitioned propositional theories into theories in a reduced sublanguage. we have proven the soundness and completeness of all of these algorithms with respect to reasoning procedures that are complete for consequence finding in a specified sublanguage. proofs omitted from this paper can be found at .
	1	partition-based logical reasoning
in this section we describe the basic framework adopted in this paper. we extend it with new soundness and completeness results that will enable us to minimize local inference.
             following   we say that {ai}i＋n is a partitioning of a logical theory a if a = si ai. each individual ai is a set of axioms called a partition  l ai  is its signature  the set of non-logical symbols   and l ai  is its language  the set of formulae built with l ai  . the partitions may share literals and axioms. a partitioning of a theory induces a graphical representation  g =  v e l   which we call the theory's intersection graph. each node of the intersection graph  i  represents an individual partition  ai   v = {1 ... n}   two nodes i j are linked by an edge if l ai  and l aj  have a non-logical symbol in common  e = { i j  | l ai  ” l aj  =1  }   and the edges are labeled with the set of symbols that the associated partitions ag replacementsshare  l i j  = l ai  ” l aj  . we refer to l i j  as the communication language between partitions ai and aj. we ensure that the intersection graph is connected by adding a minimal number of edges to e with empty labels  l i j =  . figure 1 illustrates a propositional theory a in clausal form  left-hand side  and its partitioning displayed as an intersection graph  right-handside .  figures 1  1 and 1 first appeared in . 

figure 1: partitioned theory a intersection graph g.
　figure 1 displays forward-m-p  fmp   a messagepassing algorithm for partition-based logical reasoning. it takes as input a partitioned theory  a  an associated graph structure g =  v e l   and a query formula q in l ak   and returns yes if the query was entailed by a. the algorithm uses procedures that generate consequences  consequence finders  as the local reasoning mechanism within each partition or graphical node. it passes a concluded formula to an adjacent node if the formula's signature is in the communication language l of the adjacent node  and that node is on the path to the node containing the query.
　recall  consequence finding  as opposed to proof finding  was defined by lee  to be the problem of finding all nontautological logical consequences of a theory or sentences that subsume them. a prime implicate generator is a popular example of a consequence finder 1.
　to determine the direction in which messages should be sent in the graph g  step 1 in fmp computes a strict partial order over nodes in the graph using the partitioning together with a query  q.
definition 1     given partitioned theory a = si＋n ai  associated graph g =  v e l  and query q （ l ak   let dist i j   i j （ v   be the length of the shortest path between nodes i j in g. then i   j iff dist i k    dist j k .
procedure forward-m-p  fmp  {ai}i＋n  g  q 
{ai}i＋n a partitioning of the theory a  g =  v e l  a graph describing the connections between the partitions  q a query in l ak   k ＋ n .
1. determine   as in definition 1.
1. concurrently 
 a  perform consequence finding for each of the partitions ai  i ＋ n.
 b  for every  i j  （ e such that i   j  for every consequence   of aj found  or   in aj   if   （l l i j    then add   to the set of axioms of ai.
 c  if q is provena in ak  return yes.

a
　　derive a subsuming formula or initially add  q to ak and derive inconsistency.figure 1: a forward message-passing algorithm.
　figure 1 illustrates an execution of the fmp algorithm using resolution as the consequence finder within a partition. as can be seen from the example  the partitioning reduces the number of possible inference steps by precluding the direct resolution of axioms residing in different partitions. indeed   showed that partition-based reasoning reduces the search space significantly  as a function of the size of the communication language between partitions.
　fmp is sound and complete if we guarantee some properties of the graph g and the consequence finders used for each partition. the graph g is required to be a tree that is properly labeled for a.
definition 1  proper labeling  a tree-structured representation  g =  v e l   of a partitioned theory a = {ai}i＋n is said to have a proper labeling  if for all  i j  （ e and b1 b1  the two subtheories of a on the two sides of the edge  i j  in g  it is true that l i j    l b1  ” l b1 .
　for example  every intersection graph that is a tree is properly labeled. a simple algorithm called break-cycles
using fmp to prove hot drink
part.	resolve	generatinga1 1 	  1 onpump ‥ water m1 a1 m1   1 ok pump ‥ water m1 a1 m1   1  water clause water passed from a1 to a1 m1 a1 m1    1 ok boiler … on boiler   steam m1 a1 m1    1  onboiler ‥ steam m1 a1 m1    1  steam clause steam passed from a1 to a1 m1 a1 1 	  1  steam ‥ teabag ‥ hot drink m1 a1 m1    1  steam ‥ hotdrink m1 a1 m1    m1 hot drink m1 figure 1: a proof of hot drink from a in figure 1 after asserting ok pump  1  in a1 and ok boiler  1   on boiler  1  in a1.
that transforms an intersection graph that is not tree into a properly labeled tree was presented in . note that the notion of proper labeling is equivalent  in this context  to the running intersection property used in bayes nets.
　the consequence finders applied to each partition i are required to be complete for li-generation for a sublanguage li   l ai  that depends on the graph g and the query q.
definition 1  completeness for l-generation  let a be a set of axioms  l   l a  a language  and r a consequence finder. let cr l a  be the consequences of a generated by r that are in l. r is complete for l-generation if for all   （ l  if a |=    then cr l a  |=  .
theorem 1  soundness and completeness  let a be a partitioned theory {ai}i＋n of arbitrary propositional or first-order formulae  g a tree that is properly labeled with respect to a  and q （ l ak   k ＋ n  a query. for all i ＋ n  let li = l l i j   for j such that  i j  （ e and j   i  there is only one such j   and let {ri}i＋n be reasoning procedures associated with partitions {ai}i＋n. if every ri is complete for li-generation then a |= q iff fmp {ai}i＋n  g  q  outputs yes.
　this soundness and completeness result improves upon a soundness and completeness result in  by allowing consequence finders that focus on consequences in the communication language between partitions. in certain cases  we can restrict consequence finding in fmp even further by using reasoners that are complete for l-consequence finding.
definition 1  completeness for l-consequence finding 
let a be a set of axioms  l   l a  a language  and r a consequence finder. r is complete for l-consequence finding iff for every   （ l that is not a tautology  a |=   iff there exists ψ （ l such that a `r ψ and ψ subsumes1 .
　observe that every reasoner that is complete for lconsequence finding is also complete for l-generation  for any language l that is closed under subsumption . the notion of a consequence finder restricting consequence generation to consequences in a designated sublanguage was discussed by inoue   and further developed by del val  and others. most results on the completeness of consequence finding exploit resolution-based reasoners  where completeness results for l-consequence finding are generally restricted to a clausal language l. the fmp reasoners in theorem 1 must be complete for li-generation in arbitrary fol languages  li. corollary 1 refines theorem 1 by restricting ai and li to propositional clausal languages and requiring reasoners to be complete for li-consequence finding rather than li-generation.
corollary 1  soundness and completeness  let a be a partitioned theory {ai}i＋n of propositional clauses  g a tree that is properly labeled with respect to a  and q （ l ak   k ＋ n  a query. let li = l l i j   for j such that  i j  （ e
and j   i  there is only one such j   and let {ri}i＋n be reasoning procedures associated with partitions {ai}i＋n. if every ri is complete for li-consequence finding then a |= q iff fmp {ai}i＋n  g  q  outputs yes.
　in section 1 we provide examples of reasoners that are complete for l-consequence finding and show how to exploit them to focus reasoning within a partition.
1	local inference using resolution
in this section  we specialize fmp to resolution-based consequence finders. resolution  is one of the most widely used reasoning methods for automated deduction  and more specifically for consequence finding. it requires the input formula to be in clausal form  i.e.  a conjunction of disjunctions of unquantified literals. the resolution rule is complete for consequence finding  e.g.   1; 1   and so is linear resolution and some of its variants  e.g.   .
　we present algorithm resolution-m-p  res-mp   that uses resolution  or resolution strategies   in figure 1. the rest of this section is devoted to explaining four different implementations for subroutine res-send    j  i   used by this procedure to send appropriate messages between partitions: the first implementation is for clausal propositional theories; the second is for clausal fol theories  with associated graph g  that is a properlylabeled trees and whose labels include all the functionand constant symbols of the language; the third is also for clausal fol theories  however it uses unskolemization and subsequent skolemization to generate the messages to be passed between partitions; the fourth is a refinement of the third for the same class of theories that avoids unskolemization.
　in the propositional case  subroutine res-send    j  i   implementation 1  simply adds   to ai  as done in fmp. if the resolution strategies being employed satisfy the conditions of corollary 1  then res-mp is sound and complete.
　in the fol case  implementing res-send requires more care. to illustrate  consider the case where resolution generates the clause p b x   b a constant symbol and x a variable . it also implicitly proves that  b p b x . res-mp
procedure resolution-m-p res-mp  {ai}i＋n  g  q 
{ai}i＋n a partitioned theory  g =  v e l  a graph  q a query formula in the language of l ak   k ＋ n .
1. determine   as in definition 1.
1. add the clausal form of  q to ak.
1. concurrently 
 a  perform resolution for each of the partitions ai  i ＋ n.
 b  for every  i j  （ e such that i   j  if partition aj includes the clause    as input or resolvent  and the predicates of   are in l l i j    then perform ressend    j  i .
 c  if q is proven in ak  return yes.figure 1: a resolution forward message-passing algorithm.
may need to send  b p b x  from one partition to another  but it cannot send p b x  if b is not in the communication language between partitions  for ground theories there is no such problem  see   . in the first-order case  completeness for consequence finding for a clausal first-order logic language  e.g.  lee's result for resolution  does not guarantee completeness for l-generation for the corresponding full fol language  l. this problem is also reflected in a slightly different statement of craig's interpolation theorem  that applies for resolution .
　a simple way of addressing this problem is to add all constant and function symbols to the communication language between everyconnectedset of partitions. this has the advantage of preserving soundness and completeness  and is simple to implement. in this case  subroutine res-send    j  i   implementation 1  simply adds   to ai  as done in fmp.
　in large systems that consist of many partitions  the addition of so many constant and function symbols to each of the other partitions has the potential to be computationally inefficient  leading to many unnecessary and irrelevant deduction steps. arguably  a more compelling way of addressing the problems associated with resolution for first-order theories is to infer the existential formula  b p b x  from p b x   send this formula to the proper partition and skolemize it there. for example  if   = p f g b   x  is the clause that ressend gets  replacing it with  b p b x  eliminates unnecessary work of the receiving partition.
　the process of conservatively replacing function and constant symbols by existentially quantified variables is called unskolemization or reverse skolemization and is discussed in  1; 1; 1 .  presents an algorithm u that is complete for our purposes and generalizes and simplifies an algorithm of .  space precludes inclusion of the algorithm. 
theorem 1    let v be a vocabulary and   ψ be formulae such that ψ （ l v   and     ψ. there exists f （ l v   that is generated by algorithm u such that f   ψ.
　thus  for every resolution strategy that is complete for lconsequence finding  unskolemizing   using procedure u for v = l i j  and then skolemizing the result gives us a combined procedure for message generation that is complete for lj-generation. this procedure can then be used readily in res-mp  or in fmp   upholding the soundness and completeness to that supplied by theorem 1. the subroutine res-send    j  i  that implements this approach is presented in figure 1. it replaces   with a a set of formulae in l l i j   that follows from  . it then skolemizes the resulting formulae for inclusion in ai.
procedure res-send    j  i 	 implementation 1    a formula  j i ＋ n.
1. unskolemize   into a set of formulae  Φ in l l i j    treating every symbol of l      l i j  as a skolem symbol.
1. for every  1 （ Φ  if  1 is not subsumed by a clause that is in ai  then add the skolemized version of  1 to the set of axioms of ai.
figure 1: subroutine res-send using unskolemization.
　procedure u may generate more than one formula for any given clause  . for example  if   =
p x f x  u g u    for l i j  = {p}  then we must generate both  x y u vp x y u v  and  u v x yp x y u v     entails both quantified formulae  and there is no one quantified formula that entails both of them . in our case we can avoid some of these quantified formulae by replacing the unskolemize and then skolemize process of res-send  implementation 1  with a procedure that produces a set of formulae directly  implementation 1 . it is presented in figure 1.
procedure res-send    j  i 	 implementation 1    a formula  j i ＋ n.
1. set s ○ l      l i j .
1. for every term instance  t = f t1 ... tk   in    if f （ s and t is not a subexpression of another term of   with f1 （ s  then replace t with  x ○ t  for some new variable  x  if k = 1  t is a constant symbol .
1. nondeterministicallya  for every pair of marked arguments  x ○ α    y ○ β   in    if α β are unifiable  then unify all occurrences of x y  i.e.  unify αi βi for all markings x ○ αi  y ○ βi .
1. for every marked argument  x ○ α  in    do
 a  collect all marked arguments with the same variable on the left-hand side of the  ○  sign. suppose these are x ○ α1 ... x ○ αl.
 b  let y1 ... yr be all the variables occurring in α1 ... αl. for every i ＋ l  replace  x ○ αi  with f y1 ... yr  in    for a fresh function symbol f  if r = 1  f is a fresh constant symbol .
1. add   to ai.

a
　　nondeterministically select the set of pairs for which to unify all occurrences of x y.figure 1: subroutine res-send without unskolemization.
　steps 1 of procedure res-send    j  i   implementation 1  correspond to similar steps in procedure u presented in   simplifying where appropriate for our setup. our procedure differs from unskolemizing procedures in step 1  where it stops short of replacing the skolem functions and constants with new existentially quantified variables. instead  it replaces them with new functions and constant symbols. the nondeterminism of step 1 is used to add all the possible combinations of unified terms  which is required to ensure completeness.
　for example  if   = p f g b   x  and l i j  = {p}  then res-send adds p a x  to ai  for a new constant symbol  a. if   = p x f x  u g u    for l i j  = {p}  then res-send adds p x h1 x  u h1 u   to ai  for new function symbols h1 h1. finally  if   = p x f x  u f g u     then res-send adds
p x f x  u h u   and p h1 u  h1 u  u h1 u   to ai  for h h1 h1 new function symbols.
theorem 1  soundness & completeness of res-mp  let a be the partitioned theory si＋n ai of propositional or first-order clauses  g a tree that is properly labeled with respect to a  and q （ l ak  k ＋ n  a sentence that is the query. a |= q iff applying res-mp {ai}i＋n  g  q   with implementation 1 of res-send  outputs yes.
proof sketch soundness and completeness of the algorithm follow from that of fmp  if we show that res-send  implementation 1  adds enough sentences  implying completeness  to ai that are implied by    thus sound  in the restricted language l l i j  .
　if we add all sentences   that are submitted to res-send to ai without any translation  then our soundness and completeness result for fmp applies  this is the case where we add all the constant and function symbols to all l i j  .
　we use theorem1 to provethat we add enoughsentences to ai. let  1 be a quantified formula that is the result of applying algorithm u to  . then   1 results from a clause c generated in step 1 of algorithm u  respectively  step 1 in res-send . in algorithm u  for each variable x  the markings  x ○ αi  in c are converted to a new variable that is existentially quantified immediately to the right of the quantification of the variables y1 ... yr.  1 is a result of ordering the quantifiers in a consistent manner to this rule  this process is done in steps 1 of algorithm u .
　step 1 of res-send performs the same kind of replacement that algorithm u performs  but uses new function symbols instead of new quantified variables. since each new quantified variable in  1 is to the right of the variables on which it depends  and our new function uses exactly those variables as arguments  then step 1 generates a clause c1 from c that entails  1. thus  the clauses added to ai by res-send entail all the clauses generated by unskolemizing   using u. from theorem 1  these clauses entail all the sentences in l l i j   that are implied by  .
　to see that the result is still sound  notice that the set of clauses that we add to ai has the same consequences as   in l l i j    i.e.  if we add those clauses to aj we get a conservative extension of aj . 
　resolution strategies that can be readily used in res-mp  while preservingcompleteness  include linear resolution   directional resolution  and lock resolution . strategies such as set-of-support and semantic resolution can be used with somewhat different treatments.
1	minimizing node coupling using polarity
fmp and res-mp use the communication language to determine relevant inference steps between formulae in connected partitions. this section improves the efficiency of fmp and res-mp by exploiting the polarity of predicates in our partitions to further constrain the communication language between partitions. this leads to a reduction in the number of messages that are passed between adjacent partitions  and thus a reduction in the search space size of the global reasoning problem. our results are predicated on lyndon's interpolation theorem   an extension to craig's theorem .
theorem 1  lyndon's interpolation theorem  let α β be sentences such that α ` β. then there exists a sentence γ such that α ` γ and γ ` β  and that every relation symbol that appears positively  negatively  in γ appears positively  negatively  in both α and β. γ is referred to as the interpolant of α and β.
　this theorem guarantees that fmp need only send clauses with literals that may be used in subsequent inference steps. for example  let {a1 a1} be a partitioned theory  g =  v = {1}  e ={ 1 }  l  be a graph  and q （ l a1   be a query. if fmp concluded p from a1  and p does not show positively in a1   q  i.e.  p does not show negatively in a1 and does not show positively in q   then there is no need to send the message p from a1 to a1.
　procedure polarize  figure 1  takes as input a partitioned theory  associated tree g =  v e l   and a query q. it returns a new graph g1 =  v e l1  that is minimal with respect to our interpretation of lyndon's interpolation theorem. the labels of the graph now include predicate/propositional symbols with associated polarities  the same symbol may appear both positively and negatively on an edge label . all function and object symbols that appeared in l also appear in l1 for the respective edges.
theorem 1  soundness and completeness  let a be a partitioned theory {ai}i＋n of arbitrary propositional or first-order formulae  g a tree that is properly labeled with respect to a  and q （ l ak   k ＋ n  a query. let g1 be the result of running polarize {ai}i＋n  g  q . let li = l l i j   for j such that  i j  （ e and j   i  there is only one such j   and let {ri}i＋n be reasoning procedures associated with partitions {ai}i＋n. if every ri is complete for li-generation then a |=   iff fmp {ai}i＋n  g1 q  outputs yes.
　darwiche  proposed a more restricted use of polarity in graph-based algorithms for propositional sat-search. his proposal is equivalentto first finding those propositionalsymbols that appear with a unique polarity throughout the theory and then assigning them the appropriate truth value. in contrast  our proposed exploitation of polarity is useful for both propositional and first-order theories  it is more effective in constraining inference steps  and is applicable to a broader class of message-passing algorithms problems. in particular 
procedure polarize {ai}i＋n  g  q 
{ai}i＋n a partitioning of the theory a  g =  v e l  a tree and q a query formula in l ak   k ＋ n .
1. for every i j （ v   set l1 i j  to be the set of object and function symbols that appear in l i j   if there are any.
1. rewrite {ai}i＋n such that all negations appear in front of literals  i.e.  in negation normal form .
1. determine   as in definition 1.
1. for all  i j  （ e such that i   j  for every predicate symbol p （ l i j  
 a  let v1 v1 be the two sets of vertices in v separated by i in g  with j （ v1.
 b  if    p appears in v1 then  if    p appears in q or     p appears in am  for some m （ v1  then add    p to l1 i j .
1. return g1 =  v e l1 .figure 1: constraining the communication language of {ai}i＋n by exploiting polarity.
our method is useful in cases where symbols appear with different polarities in different partitions.
1	minimizing local inference
to maximize the effectiveness of structure-based theorem proving  we must minimize local inference within each node of our tree-structured problem representation  while preserving global soundness and completeness. first-order and propositional consequence finding algorithms have been developed that limit deduction steps to those leading to interesting consequences  skipping deduction steps that do not.
　in the propositional case  the most popular algorithms are certain l- prime  implicate finders.  see  for an excellent survey.  sol-resolution  skipping ordered linear resolution   and sfk-resolution  skip-filtered  kernel resolution   are two first-order resolution-based l-consequence finders. sfk-resolution is complete for first-order l-consequence finding  reducing to directional resolution in the propositional case . in contrast  sol-resolution is not complete for first-order l-consequence finding  but is complete for first-orderincremental l-consequencefinding. given new input Φ  an incremental l-consequence finder finds the consequences of a“Φ that were not entailed by Φ alone. defining completeness for incremental l-consequence finding is analogous to definition 1.
　in the rest of this section  we propose strategies that exploit our graphical models and specialized consequence finding algorithms to improve the efficiency of reasoning. following the results in previoussections  using sfk-resolutionas a reasoner within partitions will preserve the soundness and completeness of the global problem while significantly reducing the number of inference steps. sfk-resolution can be used by all of the procedures below. unless otherwise noted  the algorithms we describe are limited to propositional theories because first-order consequence finders may fail to terminate  even for decidable cases of fol.
　the first strategy is compilation. figure 1 provides an algorithm  compile {ai}i＋n  g   that takes as input a partitioned theory {ai}i＋n and associated tree g  that is properly labeled  and outputs a compiled partitioned theory. each new partition is composed of the logical consequences of partition ai that are in the language lcommi  all the communication languages associated with ai. prime implicate finders have commonly been used for knowledge compilation  particularly in propositional cases. sfk-resolution can be used as the sound and complete l-consequence finder in step 1 of compile.
　knowledge compilation can often create a large theory. each partition produced by compile {ai}i＋n  g  will be of worst case size of o 1|l lcommi |  clauses. since our assumption is that partitions are produced to minimize communication between partitions  | l lcommi  | should be much smaller than | l ai  |. as a consequence  we might expect the compiled theory to be smaller than the original theory  though this is not guaranteed. under the further assumption that the theories in partitions are fairly static  the cost of compilation will be amortized over many queries. we discuss further options for compilation  including the use of partial compilation  in a longer paper.
procedure compile {ai}i＋n  g 
{ai}i＋n a partitioning of the theory a  g =  v e l  a tree with proper labeling for a. for each partition ai  for i = 1 ...  n 
1. let lcommi = l s i j （e l i j  
1. using a sound and complete l-consequence finder  perform lcommi-consequence finding on each partition ai  placing the output in a new partition a1i.
figure 1: a partition-based theory compilation algorithm.
proposition 1 let a = si＋n ai be a partitioned theory with associated tree g that is properly labeled for a. let
lcommi = l s i j （e l i j   . for all   （ li   lcommi  
 are the compiled
partitions output by compile {ai}i＋n  g .
　we may use our compiled theories in several different strategies for batch-style and concurrent theorem proving  as well as in our previous message-passing algorithms. figure 1 presents an algorithm for batch-style structure-based theorem proving. batch-mp takes as input a  possibly compiled  partitioned theory  associated tree g that is properly labeled  and query q. for each partition in order  it exploits focused l-consequence finding to compute all the relevant consequences of that theory. it passes the conclusions towards the partitionwith the query. this algorithmis verysimilar to the bucket elimination algorithm of . batch-mp preserves soundness and completeness of the global problem  while exploiting focused search within each partition.
theorem 1  soundness and completeness  let a be a set of clauses in propositional logic. let {ri}i＋n be the li-consequence finders associated with partitions {ai}i＋n
procedure batch-mp  {ai}i＋n  g  q 
{ai}i＋n a  compiled  partitioning of the theory a  g =  v e l  a properly labeled tree describing the connections between the partitions  q a query in l ak   k ＋ n .
1. if {ai}i＋n  is a compiled theory  replace partition ak with the partition ak from the uncompiled theory.
1. determine   as in definition 1.
1. let li = l l i j   for j such that  i j  （ e and j   ia.
1. following   in a decreasing order  for every  i j  （ e such that j   ia 
run the li-consequence finder on ai until it has exhausted its consequences  and add the consequences in li to aj.
1. if q is provenb in ak  return yes.

a
there is only one such j.
b
　　derive a subsuming formula or initially add  q to ak and derive inconsistency.figure 1: a batch-style message-passing algorithm.
in step 1 of batch-mp  {ai}i＋n g q . if every ri is complete for li-consequence finding then a |= q iff applying batch-mp {ai}i＋n g q  outputs yes.
　our final algorithm  concurrent-mp   figure 1   takes as input a  possibly compiled  partitioned theory  associated tree g that is properly labeled  and query q. it exploits incremental l-consequence finding in the output communication language of each partition to compute the relevant incremental consequences of that theory  and then passes them towards the partition with the query. once again  sfk-resolution can be used as the sound and complete lconsequence generator for the preprocessing  step 1 . in the case where the theory is compiled into propositional prime implicates  the consequences in li may simply be picked out of the existing consequences in ai. sol-resolution can be used as the sound and complete incremental l-consequence finder  step 1a . concurrent-mp preserves soundness and completeness of the global problem in the propositional case  while exploiting focused search within each partition.
theorem 1  soundness and completeness  let a be a set of clauses in propositional logic. let {ri}i＋n be the li-consequence finders associated with partitions {ai}i＋n in step 1 of concurrent-mp {ai}i＋n g q  and let
{r1i}i＋n be the incremental li-consequence finders associated with partitions {ai}i＋n in step 1 of concurrentmp {ai}i＋n g q . if every ri is complete for liconsequence finding  and every r1i is complete for incremental li-consequence finding then a |= q iff applying concurrent-mp {ai}i＋n g q  outputs yes.
1	related work
a numberof ai reasoningsystems exploit some type of structure to improve the efficiency of reasoning. while our exploitation of graph-based techniques is similar to that used in bayes nets  e.g.    our work is distinguished in that we
procedure concurrent-mp  {ai}i＋n  g  q 
{ai}i＋n a  compiled  partitioning of the theory a  g =  v e l  a properly labeled tree describing the connections between the partitions  q a query in l ak   k ＋ n .
1. determine   as in definition 1.
1. let li = l l i j   for j such that  i j  （ e and j   ia.
1. if {ai}i＋n  is a compiled theory  then replace partition ak with the partition ak from the uncompiled theory.
1. for every i ＋ n  run the li-consequence finder on partition ai until it has exhausted its consequences.
1. for every  i j  （ e such that j   ia  add the li-prime implicates to partition aj.
1. concurrently 
 a  for every  i j  （ e such that j   ia  perform incremental li-consequence finding for each of the partition ai and add the the consequences in li to aj.
 b  if q is provenb in ak  return yes.

a
there is only one such j.
b
　　derive a subsuming formula or initially add  q to ak and derive inconsistency.figure 1: a concurrent message-passing algorithm.
reason with logical rather than probabilistic theories  where notions of structure and independence take on different roles in reasoning. our work is most significantly distinguished from work on csps  e.g.    and more recently  logical reasoning  e.g.   1; 1   in that we reason with explicitly partitioned theories using message passing algorithms and our algorithms apply to fol as well as propositional theories.
　in the area of fol theorem proving  our work is related to research on parallel theorem proving  see surveys in  1; 1   and on combining logical systems  e.g.   1; 1  . most parallel theorem proverimplementationsare guided by lookahead and subgoals to decompose the search space dynamically or allow messages to be sent between different provers working in parallel  using heuristics to decide on which messages are relevant to each prover. these approaches typically look at decompositions into very few sub-problems. in addition  the first approach typically requires complete independence of the sub-spaces or the search is repeated on much of the space by several reasoners. in the second approach there is no clear methodology for deciding what messages should be sent and from which partition to which.
　the work on combining logical systems focuses on combinations of signature-disjoint theories  allowing the queries to include symbols from all signatures  and decision procedures suitable for those theories. all approaches either nondeterministically instantiate the  newly created  variables connecting the theories or restrict the theories to be convex  disjunctions are intuitionistic  and have information flowing back and forth between the theories. in contrast  we focus on the structure of interactions between theories with signatures that share symbols and the efficiency of reasoning with consequence finders and theorem provers. we do not have any restrictions on the language besides finiteness.
　work on formalizing and reasoning with context  see  for a survey  can be related to theorem proving with structured theories by viewing the contextual theories as interacting sets of theories. unfortunately  to introduce explicit contexts  a language that is more expressive than fol is needed. consequently  a number of researchers have focused on context for propositionallogic  while much of the reasoningwork has focused on proof checking  e.g.  getfol  .
1	summary
in this paper we exploited graph-based techniques to improve the efficiency of theorem proving for structured theories. theories were organized into subtheories that were minimally connected by the literals they share. we presented message-passing algorithms that reason over these theories using consequence finding  specializing our algorithms for the case of first-order resolution  and for batch and concurrent theorem proving. we provided an algorithm that restricts the interaction between subtheories by exploiting the polarity of literals. we attempted to minimize the reasoning within each individual partition by exploiting existing algorithms for focused incremental and general consequence finding. finally  we proposed an algorithm that compiles each subtheory into one in a reduced sublanguage. we have proven the soundness and completeness of all of these algorithms. the results presented in this paper contribute towards addressing the problem of reasoning efficiently with large or multiple structured commonsense theories.
acknowledgements
we wish to thank the anonymous ijcai reviewers for their thorough review of this paper  and alvaro del val and pierre marquis for helpful comments on the relationship between our work and previous work on consequence finding. this research was supported in part by darpa grant n1-c-1-p1 navy grant n1-c-1 and by darpa grant n1-c-1 rkf program .
references
 v. akman and m. surav. steps toward formalizing context. ai magazine  1 :1  1.
 e. amir. efficient approximation for triangulation of minimum treewidth. manuscript submitted for publication. available at http://www-formal.stanford.edu/eyal/papers/decompuai1.ps  1.
 e. amir and s. mcilraith. paritition-based logical reasoning. in proc. kr '1  pages 1. morgan kaufmann  1.
 f. baader and k. u. schulz. combination of constraint solvers for free and quasi-free structures. theoretical computer science  1 :1  1.
 w. w. bledsoe and a. m. ballantyne. unskolemizing. technical report memo atp-1  mathematics department  university of texas  austin  1.
 m. p. bonacina and j. hsiang. parallelization of deduction strategies: an analytical study. journal of automated reasoning  1-1  1.
 r. s. boyer. locking: a restriction of resolution. phd thesis  mathematics department  university of texas  austin  1.
 r. chadha and d. a. plaisted. finding logical consequences using unskolemization. in proceedings of ismis'1  volume 1 of lnai  pages 1. springer-verlag  1.
 p. cox and t. pietrzykowski. a complete nonredundant algorithm for reversed skolemization. theoretical computer science  1-1  1.
 w. craig. linear reasoning. a new form of the herbrandgentzen theorem. j. of symbolic logic  1-1  1.
 a. darwiche. utilizing knowledge-based semantics in graphbased algorithms. in proc. aaai '1  pages 1  1.
 r. dechter and j. pearl. tree clustering schemes for constraint processing. in proc. aaai '1  1.
 r. dechter and i. rish. directional resolution: the davisputnam procedure  revisited. in proc. kr '1  pages 1. morgan kaufmann  1.
 a. del val. a new method for consequence finding and compilation in restricted language. in proc. aaai '1  pages 1- 1. aaai press/mit press  1.
 j. denzinger and i. dahn. cooperating theorem provers. in w. bibel and p. schmitt  editors  automated deduction. a basis for applications.  volume 1  chapter 1  pages 1. kluwer  1.
 f. giunchiglia. getfol manual - getfol version 1. technical report dist-tr-1  dist - university of genoa  1. available at http://ftp.mrg.dist.unige.it/pub/mrgftp/1.ps.gz.
 k. inoue. linear resolution for consequence finding. artificial intelligence  1-1 :1  aug. 1.
 f. v. jensen  s. l. lauritzen  and k. g. olesen. bayesian updating in recursive graphical models by local computation. computational statistics quarterly  1-1  1.
 r. c.-t. lee. a completeness theorem and a computer program for finding theorems derivable from given axioms. phd thesis  university of california  berkeley  1.
 r. c. lyndon. an interpolation theorem in the predicate calculus. pacific journal of mathematics  1 :1  1.
 p. marquis. consequence finding algorithms. in algorithms for defeasible and uncertain reasoning  volume 1 of handbook on deafeasible reasoning and uncertainty management systems  pages 1. kluwer  1.
 s. mcilraith and e. amir. theorem proving with structured theories  full report . technical report ksl-1  ksl  computer science dept.  stanford u.  apr. 1.
 e. minicozzi and r. reiter. a note on linear resolution strategies in consequence-finding. artificial intelligence  1- 1  1.
 g. nelson and d. c. oppen. simplification by cooperating decision procedures. acm trans. on programming languages and systems  1 :1  oct. 1.
 i. rish and r. dechter. resolution versus search: two strategies for sat. journal of automated reasoning  1-1 :1- 1  1.
 j. a. robinson. a machine-oriented logic based on the resolution principle. j. of the acm  1 :1  1.
 j. r. slagle. interpolation theorems for resolution in lower predicate calculus. j. of the acm  1 :1  july 1.

logic programming and
theorem proving
answer set programming

experimenting with heuristics for answer set programming
	wolfgang faber	nicola leone
inst. f. informationssysteme 1  tu wien	dep. of mathematics  univ. of calabria
a-1 wien  austria 1 rende  cs   italy faber kr.tuwien.ac.at leone unical.it
gerald pfeifer
inst. f. informationssysteme 1  tu wien
a-1 wien  austria pfeifer dbai.tuwien.ac.at

abstract
answer set programming  asp  is a novel programming paradigm  which allows to solve problems in a simple and highly declarative way. the language of asp  function-free disjunctive logic programming  is very expressive and allows to represent even problems of high complexity  every problem in the complexity class  .
as for sat solvers  the heuristic for the selection of the branching literal  i.e.  the criterion determining the literal to be assumed true at a given stage of the computation  dramatically affects the performance of an asp system. while heuristics for sat have received a fair deal of research in ai  only little work in heuristics for asp has been done so far.
in this paper  we extend to the asp framework a number of heuristics which have been successfully employed in existing systems  and we compare them experimentally. to this end  we implement such heuristics in the asp system dlv  and we evaluate their respective efficiency on a number of benchmark problems taken from various domains. the experiments show interesting results  and evidence a couple of promising heuristic criteria for asp  which sensibly outperform the heuristic of dlv.
1	introduction
answer set programming  asp  is a declarative approach to programming  which has been recently proposed in the area of nonmonotonic reasoning and logic programming. the knowledge representation language of asp is very expressive: function-free logic programs with classical negation where disjunction is allowed in the heads of the rules and nonmonotonic negation may occur in the bodies of the rules. the intended models of an asp program  i.e.  the semantics of the

　　this work was supported by fwf  austrian science funds  under the project z1-inf and p1.
program  are subset-minimal models which are  grounded  in a precise sense  they are called answer sets  gelfond and lifschitz  1 . the idea of answer set programming is to represent a given computational problem by an asp program whose answer sets correspond to solutions  and then use an answer set solver to find such a solution  lifschitz  1 .
　as an example  consider the well-known problem of 1colorability  which is the assignment of three colors to the nodes of a graph in such a way that adjacent nodes have different colors. this problem is known to be np-complete.
suppose that the nodes and the edges are represented by a set of facts with predicates  unary  and  binary   respectively. then  the following asp program allows us to determine the admissible ways of coloring the given graph.
 rule above states that every node of the graph is colored red or yellow or green  while forbids the assignment of the same color to any adjacent nodes. the minimality of answer sets guarantees that every node is assigned only one color. thus  there is a one-to-one correspondence between the solutions of the 1-coloring problem and the answer sets of . the graph is 1-colorable if and only if has some answer set.
　an advantage of answer set programming over sat-based programming is that problems can be encoded more easily in the asp language than in propositional cnf formulas  thanks to the nonmonotonic character of disjunction and negation as failure  lifschitz  1 . importantly  due to the support for variables  every problem in the complexity class  i.e.  in
  can be directly encoded in an asp program which

can then be used to solve all problem instances in a uniform way  eiter et al.  1 .
　the high expressivenessof answer set programming comes at the price of a high computational cost in the worst case  which makes the implementation of efficient asp systems a difficult task. nevertheless  some efforts have been done in this direction  and a number of asp systems are now available. the two best known asp systems are dlv  eiter et al. 
1   and smodels  niemela：  1; simons  1   but also other systems support asp to some extent  including ccalc  mccain and turner  1   dcs  east and truszczyn＞ski  1   quip  egly et al.  1   and xsb  rao et al.  1 . the core of an asp system is model generation  where a model of the program is produced  which is then subjected to an answer set check. for the generation of models  asp systems employ procedures which are similar to davis-putnam procedures used in sat solvers. as for sat solvers  the heuristic  branching rule  for the selection of the branching literal  i.e.  the criterion determining the literal to be assumed true at a given stage of the computation  is fundamentally important for the efficiency of a model generation procedure  and dramatically affects the overall performance of an asp system. while a lot of work has been done in ai developing new heuristics and comparing alternative heuristics for sat  see  e.g.   hooker and vinay  1; li and anbulagan  1; freeman  1    only little work has been done so far for asp systems. in particular  we are not aware of any previous comparison between heuristics for asp.
　in this paper  we evaluate different heuristics for asp systems. to this end  we first consider a couple of heuristics which have been very successful in sat solvers or in nondisjunctive asp systems  and we extend them to the framework of  disjunctive  asp. we then perform an experimentation activity to compare the different heuristics. in particular  we implement the heuristics in the asp system dlv  and we compare their respective efficiency on a number of benchmark problems taken from various domains. the experiments show interesting results  and evidence a couple of promising heuristic criteria for asp  which sensibly outperform the heuristic of dlv. nevertheless  this paper is not at all a conclusive work on heuristics for asp; rather  it is a first step in this field that will hopefully stimulate further works on the design and evaluation of heuristics for asp  which are strongly needed to build efficient asp solvers.
1	answer set programming language
in this section  we provide a formal definition of the syntax and semantics of the answer set programming language supported by dlv: disjunctive datalog extended with strong negation. for further background  see  gelfond and lifschitz  1; eiter et al.  1 .
asp programs
a  disjunctive  rule	is a formula
where are classical literals  atoms possibly preceded by the symbol   and . the disjunction is the head of   while the conjunction is the body  the positive body  and the negative body of . comparison operators  like   are built-in predicates in asp systems  and may appear in the bodies of rules.
　a disjunctive datalog program  also called asp program in this paper  is a finite set of rules.
　as usual  an object  atom  rule  etc.  is called ground or propositional  if it contains no variables.
answer sets
we describe the semantics of consistent answer sets  which has originally been defined in  gelfond and lifschitz  1 .
　given a program   let the herbrand universe be the set of all constants appearing in and the herbrand base be the set of all possible combinations of predicate symbols appearing in with constants of   possibly preceded by .
a set of literals is said to be consistent if  for every literal   its complementary literal is not contained in .
	given a rule  	denotes the set of rules obtained
by applying all possible substitutions	from the variables in
to elements of . similarly  given a program   the ground instantiation of is the set .
　for every program   we define its answer sets using its ground instantiation in two steps  following lifschitz  1 : first we define the answer sets of positive programs  then we give a reduction of general programs to positive ones and use this reduction to define answer sets of general programs.
　an interpretation is a set of ground literals. a consistent interpretation is closed under  where is a positive program   if  for every   at least one literal in the head is true whenever all literals in the body are true. is an answer set for if it is minimal w.r.t. set inclusion and closed under .
　the reduct or gelfond-lifschitz transform of a general ground program w.r.t. a set is the positive ground program   obtained from by  i  deleting all rules whose negative body is false w.r.t. x and  ii  deleting the negative body from the remaining rules.
　an answer set of a general program is a set such that is an answer set of .
1	answer sets computation
in this section  we describe the main steps of the computational process performed by asp systems. we will refer particularly to the computational engine of the dlv system  which will be used for the experiments  but also other asp systems  like smodels  employ a very similar procedure.
　an answer set program in general contains variables. the first step of a computation of an asp system eliminates these variables  generating a ground instantiation of .1 the hard part of the computation is then performed on this ground asp program generated by the instantiator.
the heart of the computation is performed by the model
generator  which is sketched in figure 1. roughly  the model generator produces some  candidate  answer sets. the stability of each of them is subsequently verified by the function isanswerset i   which verifies whether the given  candidate  is a minimal model of the program obtained by applying the gl-transformation w.r.t. and outputs the model  if so. isanswerset i  returns true if the computation should be stopped and false otherwise.
function modelgenerator i: interpretation : boolean; var inconsistency: boolean; begin i := detcons i ; if i = then return false;  * inconsistency *  if no atom is undefined in i then return isanswerset i ; select an undefined ground atom according to a heuristic; if modelgenerator    then return true;
	else return modelgenerator 	 ;
end;
figure 1: computation of answer sets
　the modelgenerator function is first called with parameter set to the empty interpretation.1 if the program has an answer set  then the function returns true setting to the computed answer set; otherwise it returns false. the model generator is similar to the davis-putnam procedure employed by sat solvers. it first calls a function detcons    which returns the extension of with the literals that can be deterministically inferred  or the set of all literals upon inconsistency . this function is similar to a unit propagation procedure employed by sat solvers  but exploits the peculiarities of asp for making further inferences  e.g.  it exploits the knowledge that every answer set is a minimal model . if detcons does not detect any inconsistency  an atom is selected according to a heuristic criterion and modelgenerator is called on and on . the atom plays the role of a branching variable of a sat solver. and indeed  like for sat solvers  the selection of a  good  atom is crucial for the performance of an asp system. in the next section  we describe a number of heuristic criteria for the selection of such branching atoms.
1	heuristics
throughout this section  we assume that a ground asp program and an interpretation have been fixed. here  we describe the heuristic criteria that will be compared in section 1. we consider  dynamic heuristics   the asp equivalent of up heuristics for sat   that is  branching rules where the heuristic value of a literal depends on the result of taking true and computing its consequences. given a literal  
will denote the intepretation resulting from the application of detcons  see previous section  on ; w.l.o.g.  we assume that is consistent  otherwise is automatically set to false and the heuristic is not evaluated on at all.
heuristic . this is an extension of the branching rule adopted in the system satz  li and anbulagan  1  - one of the most efficient sat solvers - to the framework of asp.
　the length of a rule  w.r.t. an interpretation    is the number of undefined literals occurring in . let denote the number of unsatisfied rules1 of length w.r.t.
  which have a greater length w.r.t. . in other words  is the number of unsatisfied rules whose length shrinks to if the truth of is assumed and propagated in the interpretation . the weight is
thus  the weight function prefers literals introducing a higher number of short unsatisfied rules. intuitively  the introduction of a high number of short unsatisfied rules is preferred because it creates more and stronger constraints on the intepretation so that a contradiction can be found earlier  li and anbulagan  1 . we combine the weight of an atom
with the weight of its complement to favour such that and are roughly equal  to avoid that a possible failure leads to a very bad state. to this end  as in satz  we define the combined weight comb- of an atom as follows:
comb-
given two atoms	and	  heuristic	prefers	over
    iff comb- comb- . once a -maximum atom is selected  heuristic takes if
	 	else.
heuristic . the second heuristic we consider is inspired to the branching rule of smodels - a well known asp system  simons  1 . let denote the number of atoms which are either true or false in a  three-valued  intepretation . then  define
since maximizes the size of the resulting intepretation  it minimizes the atoms which are left undefined. intuitively  this minimizes the size of the remaining search space  which is   where is the number of undefined atoms in    simons  1 . similar to smodels  the heuristic cautiously maximizes the minimum of and .
more precisely  the preference relationship of is defined as follows. given two atoms and   if	;
otherwise 	if
and
　　　　　　　　　　. once a -maximum atom is selected  heuristic takes either or   depending on the same selection principle.
remark. it is worthwhile noting that the heuristic of smodels  while following the above intuition  is more advanced and sophisticated than . unfortunately  it is defined for non-disjunctive programs  and centered around properties of unstratified negation. which is not so important in our framework. we do not see any immediate extension of smodels' heuristic to the framework of disjunctive asp programs. 
heuristic . let us consider now the heuristic used in the dlv system. even if this is more  naive  than the previous heuristics  we will benchmark it in order to evaluate the impact of changing the branching rule on the test system.
　a peculiar property of answer sets is supportedness: for each true atom of an answer set   there exists a rule of the program such that the body of is true w.r.t. and is the only true atom in the head of . since an asp system must eventually converge to a supported interpretation  asp systems try to keep the interpretations  as much supported as possible  during the intermediate steps of the computation. to this end  the dlv system counts the number of unsupportedtrue  ut  atoms  i.e.  atoms which are true in the current interpretation but still miss a supporting rule  further details on uts can be found in  faber et al.  1b  where they are called mbts . for instance  the rule implies that must be true in every answer set of the program; but it does not give a  support  for . thus  in the dlv system is taken true to satisfy the rule  and it is added to the set of unsupportedtrue; it will be removed from this set once a supporting rule for will be found  e.g.  is a supporting rule for in the interpretation  . given a literal   let be the number of ut atoms in . moreover  let and be the number of ut atoms occurring  respectively  in the heads of exactly 1 and 1 unsatisfied rules w.r.t. . the heuristic of dlv considers   and in a prioritized way  to favor atoms yielding interpretations with fewer atoms  which should more likely lead to a supported model . if all ut counters are equal  then the heuristic considers the total number of rules which are satisfied w.r.t. . more precisely  given two atoms and :
1.	if	;
1. otherwise ;ifand1. otherwise ;ifand1. otherwise .ifand　a -maximum atom is selected by the heuristic of dlv. unlike the previous heuristics  considers only atoms  instead of literals   and it does not take into account what happens when the selected atom leads to a failure  i.e.  is not considered in the heuristic .
heuristic . finally  we have considered a simple  balanced version  of the heuristic of dlv  where also the complement of an atom is evaluated for the heuristic. given an atom   let	 
	 	 
and	. the heuristic works precisely as   but considers the primed counters. once the best atom has been selected  it is taken positive or negative  depending on .
1	benchmark programs
to evaluate the different heuristics presented in the previous section  we chose a couple of benchmark problems: 1sat  blocksworld planning  hamiltonian path  and strategic companies.
1sat is one of the best researched problems in ai and generally used for solving many other problems by translating them to 1sat  solving the 1sat problem and transforming the solution back to the original domain:
	let	be a propositional formula in conjunctive normal
form  cnf 	where the are literals over the propositional variables . is satisfiable  iff there exists a consistent conjunction of literals such that  see e.g.  papadimitriou  1  for a complete definition .
　1sat is a classical np-complete problem and can be easily represented in our formalism as follows:
　for every propositional variable      we add the following rule which ensures that we either assume that variable or its complement true:
and for every clause	in	we add the constraint
where     is if is a positive literal   and if is a negative literal .
hamiltonian path  hampath  is another classical npcomplete problem from the area of graph theory:
given an undirected graph   where is the set of vertices of and is the set of edges  and a node of this graph  does there exist a path of starting
	at	and passing through each node in	exactly once 
　suppose that the graph is specified by using two predicates and 1  and the starting node is specified by the predicate  unary  which contains only a single tuple. then  the following program solves the problem
hampath.
% each node has to be reached.
% guess whether to take a path or not.
% at most one incoming/outgoing arc!
blocksworld  bw  is a classic problem from the planning domain  and one of the oldest problems in ai:
given a table and a number of blocks in a  known  initial state and a desired goal state  try to reach that goal state by moving one block at a time such that each block is either on top of another block or the table at any given time step.
　figure 1 shows a simple example that can be solved in three time steps: first we move block c to the table  then block b on top of a  and finally c on top of b.
	initial:	goal:

figure 1: simple bw example
　due to space restrictions we refer to  erdem  1; faber et al.  1a  for a complete encoding.
strategic companies  stratcomp  finally  is a complete problem  which has been first described in  cadoli et al.  1 :
a holding owns companies  each of which produces some goods. some of these companies may jointly control others. this is modelled by means of predicates
	 	is produced by	and	  and
　　　　　　　　　 company	is jointly controlled by	 	and	 .
now  some companies should be sold  under the constraint that all goods can be still produced  and that no company is sold which would still be controlled by the holding afterwards. a company is strategic  if it belongs to a strategic set  which is a minimal set of companies satisfying these constraints.
　the answer sets of the following natural program correspond one to one to the strategic sets. checking whether any given company is strategic is done by brave reasoning:  is there any answer set containing   


 as in  cadoli et al.  1  we assume that each product is produced by at most two companies and each company is jointly controlled by at most three companies to allow for an easier representation.
1	benchmark data
for 1sat  we have randomly generated 1cnf formulas over variables using a tool by selman and kautz  selman and kautz  1 . for each size we generated 1 such instances  where we kept the ratio between the number of clauses and the number of variables at 1  which is near the cross-over point for random 1sat  crawford and auton  1 .
　the instances for hampath were generated using a tool by patrik simons which has been used to compare smodels against sat solvers  cf.  simons  1    and is available at http://tcs.hut.fi/software/smodels/misc/ hamilton.tar.gz. for each problem size we generated 1 instances  always assuming node 1 as the starting node.
　the blocksworld problems p1 and p1 have been employed in  erdem  1  to compare asp systems  and can be solved in 1 and 1 steps  respectively. we augmented these by problems p1 and p1 which require 1 and 1 steps  respectively. for each of these problems  we generated 1 random permutations of the input.
　for stratcomp  finally  we randomly generated 1 instances for each problem size   with companies and products. each company is controlled by one to five companies  where the actual number of companies is uniform randomly chosen. on average there are 1 relations per company.
　the benchmark data are available at http://www.dbai. tuwien.ac.at/proj/dlv/. all experiments were performed on an athlon/1 freebsd 1 machine with 1mb of main memory. the binaries were produced with gcc
1.1.
1	experimental results and conclusion
the results of our experiments are displayed in the graphs of figures 1. in each graph  the horizontal axis reports a parameter representing the size of the instance. on the vertical axis  we report the average running time  expressed in seconds  over the 1 instances of the same size we have run  see previous section .
remark. all heuristics have been implemented in a straightforward way  without optimizations  so the running times reported in the graph are meaningful only for comparing the relative efficiencies of the heuristics. 
　we have allowed a running time of 1 seconds for each problem instance. in the graphs  the line of an heuristic stops whenever some problem instance was not solved in the maximum allowed time. the following table displays  for each heuristic  the maximum instance-size where the heuristic could solve all problem instances in the maximum allowed time.
1sat11hampath11bwp1p1p1p1stratcomp11　as expected  heuristic   the  native  heuristic of the dlv system  which does not combine the heuristic values of complementary atoms  is the worst in most cases. it does not terminate on the instance p1 of bw  it could not solve any of the benchmark instances of stratcomp  it does not appear at all in figure 1   and stopped earlier than the others on 1sat.
　heuristic   the extension of satz heuristic to asp  behaves very well on average. on 1sat  bw  and stratcomp  could solve all benchmark instances we have run. it is the fastest on bw and one of the two fastest on 1sat. it shows a negative behaviour only on hampath. in this problem  considering unsupportedtrue literals seems to be crucial for the efficiency.
　heuristic is surprisingly good compared to . it is a simple  balanced version  of heuristic  the heuristic values of the positive and of the negative literal are combined by sum . this simple extension to dramatically improves the performance. indeed  heuristic solves nearly all instances we ran  only on 1sat it stopped a bit earlier than other heuristics . it is the best heuristic on stratcomp and  importantly  on hampath  where it beats all other heuristics of a relevant factor.
　the behaviour of heuristic   based on the minimization of the undefined atoms  is rather controversial. it behaves very well on 1sat and bw  but it is extremely bad on hampath  where it stops at 1 nodes already and is beaten even by the  naive  heuristic . this confirms that further studies are needed to find a proper extension of the heuristic of smodels to the framework of disjunctive asp.
	concluding  we observe that both heuristic	and heuristic
　  significantly improve the efficiency of the native heuristic of the dlv system. the dramatic improvement obtained by the simple change from to   confirms even more the importance of a careful study of branching rules in asp systems. this paper is only a first step in this field  we hope that further works will follow  for proposing new heuristics for asp and for better understanding the existing ones  in order to improve the efficiency of asp systems.

figure 1: blocksworld problems  average running times

figure 1: hamiltonian path problems  average running times

figure 1: strategic companies  average running times
references
 cadoli et al.  1  m. cadoli  t. eiter  and g. gottlob. default logic as a query language. ieee tkde  1 :1  1.
 crawford and auton  1  james m. crawford and larry d. auton. experimental results on the crossover point in random 1sat. artificial intelligence  1-1 :1  march 1.
 east and truszczyn＞ski  1  d. east and m. truszczyn＞ski. dcs: an implementation of datalog with constraints. nmr'1.
 egly et al.  1  u. egly  t. eiter  h. tompits  and s. woltran. solving advanced reasoning tasks using quantified boolean formulas. in aaai'1  pp. 1. aaai press.
 eiter et al.  1  t. eiter  g. gottlob  and h. mannila. disjunctive datalog. acm transactions on database systems  1 :1- 1  september 1.

figure 1: 1sat problems  average running times
 eiter et al.  1  t. eiter  w. faber  n. leone  and g. pfeifer. declarative problem-solving using the dlv system. logicbased artificial intelligence  pp. 1. kluwer  1.
 erdem  1  e. erdem. applications of logic programming to planning: computational experiments. unpublished draft. http://www.cs.utexas.edu/users/ esra/papers.html  1.
 faber et al.  1a  w. faber  n. leone  c. mateis  and g. pfeifer. using database optimization techniques for nonmonotonic reasoning. ddlp'1  pp. 1.
 faber et al.  1b  w. faber  n. leone  and g. pfeifer. pushing goal derivation in dlp computations. lpnmr'1  pp. 1- 1.
 freeman  1  j.w. freeman. improvements on propositional satisfiability search algorithms. phd thesis  university of pennsylvania  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. classical negation in logic programs and disjunctive databases. new generation computing  1-1  1.
 hooker and vinay  1  j.n. hooker and v. vinay. branching rules for satisfiability. journal of automated reasoning  1- 1  1.
 li and anbulagan  1  c.l. li and anbulagan. heuristics based on unit propagation for satisfiability problems. in ijcai 1  pp. 1.
 lifschitz  1  v. lifschitz. foundations of logic programming. principles of knowledge representation  pp. 1. csli publications  stanford  1.
 lifschitz  1  v. lifschitz. answer set planning. iclp'1  pp. 1. the mit press.
 mccain and turner  1  n. mccain and h. turner. satisfiability planning with causal theories. kr'1  pp. 1. morgan kaufmann publishers  1.
 niemela：  1  i. niemela：. logic programming with stable model semantics as constraint programming paradigm. annals of mathematics and artificial intelligence  1-1 :1  1.
 papadimitriou  1  christos h. papadimitriou. computational complexity. addison-wesley  1.
 rao et al.  1  p. rao  k.f. sagonas  t. swift  d.s. warren  and j. freire. xsb: a system for efficiently computing wellfounded semantics. lpnmr'1  pp. 1.
 selman and kautz  1  b. selman and h. kautz  1. ftp: //ftp.research.att.com/dist/ai/.
 simons  1  p. simons. extending and implementing the stable model semantics. phd thesis  helsinki university of technology  finland  1.
graph theoretical characterization and computation of answer sets
thomas linke
institut fu：r informatik  universit：at potsdam
postfach 1 1  d-1 potsdam  germany  linke cs.uni-potsdam.deabstract
we give a graph theoretical characterization of answer sets of normal logic programs. we show that there is a one-to-one correspondence between answer sets and a special  non-standardgraph coloring of so-called block graphs of logic programs. this leads us to an alternative implementation paradigm to compute answer sets  by computing non-standard graph colorings. our approach is rule-based and not atom-based like most of the currently known methods. we present an implementation for computing answer sets which works on polynomial space.
1	introduction
answer set semantics  gelfond and lifschitz  1  was established as an alternative declarative semantics for logic programs. originally  it was defined for extended logic programs1  gelfond and lifschitz  1  as a generalization of the stable model semantics  gelfond and lifschitz  1 . currently there are various applications of answer set programming  e.g.  dimopoulos et al.  1; liu et al.  1; niemela：  1 . furthermore  there are reasonably efficient implementations available for computing answer sets  e.g. smodels  niemela： and simons  1  and dlv  eiter et al.  1 . systems  like deres  cholewin＞ski et al.  1  and quip  egly et al.  1   are also able to compute answer sets  although they were designed to deal with more general formalisms.
　both systems and most of the theoretical results as well deal with answer sets in terms of atoms  or literals . this paper aims at a different point of view  namely characterizing and computing answer sets in terms of rules. intuitively  the head of a rule	is in some answer set if are in and none of is in . let
 1 
be a logic program and let us call the rules           and   respectively. then has two different answer sets and . it is easy to see that the application of blocks the application of wrt   because if contributes to   then and thus cannot be applied. analogously  blocks wrt answer set . this observation leads us to a strictly blockage-based approach. more precisely  we represent the block relation between rules as a so-called block graph. answer sets then are characterizedas special non-standardgraphcoloringsof block graphs. each node of the block graph  corresponding to some rule  is colored with one of two colors  representing application or non-application of the corresponding rule. the block graph has quadratic size of the corresponding logic program. since the block graph serves as basic data structure for our implementation  it needs polynomial space.
1	background
we deal with normal logic programs which contain the symbol used for negation as failure. a rule    is any expression of the form
 1 
where	 	 	  and	 	  are atoms. a rule is a fact if	  it is called basic if	. for a rule	we define and	.
furthermore  let denote the positive part and the negative part of . definitions of the head  the body  the positive and negative body of a rule are generalized to sets of rules in the usual way. the elements of are referred to as the prerequisites or positive body atoms of . the elements of are referred to as the negative body atoms of . if   then is said to be prerequisite-free. a set of rules of the form  1  is called a  normal  logic program. a program is called prerequisite-free if each of its rules is prerequisite-free.
we denote the set of all facts of program	by	.
	let	be a rule.	then denotes the rule
　　　　. for a logic program let . a set of atoms is closed under a basic program iff for any
　　  whenever . the smallest set of atoms which is closed under a basic program is denoted by cn .
　the reduct    of a program relative to a set of atoms is defined by and we say that a set of atoms is an answer set of a program iff cn . the reduct is often called the gelfond-
lifschitz reduction.	observe  that there are programs  e.g.
             that do not possess an answer set. throughout this paper  we use the term  answer set  instead of  stable model  since it is the more general one.
　a set of rules of the form  1  is grounded iff there exists an enumeration of such that for all we have that
	for a set of rules	and
a set of atoms we define the set of generating rules of wrt as follows
 1 
the following result relates groundedness and generating rules to answer sets.
lemma 1	let	be a logic program and	be a set of atoms.	then	is an answer set of	iff  i 	is grounded and  ii 	cn	.
this lemma characterizes answer sets in terms of generating
rules. observe  that in general	 take and	 .
	assume that for each program	we have
	for each rule	we have	 1 
in section 1  we show how to generalize our approach to both normal logic programs with multiple positive body atoms and also to extended logic programs. therefore  the assumption above is not a real restriction.
　we need some graph theoretical terminology. a directed graph is a pair such that is a finite  nonempty set  vertices  and is a set  arcs . for a directed graph and a vertex   we define the set of all predecessors of as
	. analogously  the set of all successors of	is defined as
	. a path from	to	in
is a finite subset	such that	 
	 	and	for each	. the arcs
of a path	are defined as
	. a path from	to	for some	is called a cycle in
.
　in order to represent more information in a directed graph  we need a special kind of labeled graphs. is a directed graph whose arcs are labeled with zero
 1-arcs  and with one  1-arcs   respectively. for	we distinguish 1-predecessors  1-successors  from 1-predecessors  1successors  denoted by	 	  and	 	  for	  respectively. a path	in	is called 1-path if	.	the length of a cycle in a graph is the total number of 1-arcs occurring in the cycle. additionally  we call a cycle even  odd  if its length is even  odd .
1	block graphs and application colorings
we now go on with a formal definition of the conditions under which a rule blocks another rule.
definition 1 let be a logic program s.t. condition  1  holds  and let maximal grounded. the block graph
	of	is a directed graph with vertices
and two different kinds of arcs defined as follows
and and observe  that there exists a unique maximal grounded set for each program   that is  is well-defined. this definition captures the conditions under which a rule blocks another rule  e.g.  . we also gather all groundedness information in   due to the restriction to rules in the maximal grounded part of logic program . this is important because a block relation between two rules and becomes effective only if is groundable through other rules. e.g. for program the maximal grounded subset of rules is empty and therefore contains no 1-arcs. figure 1 shows the block graph of program  1 .

figure 1: block graph of program  1 .
　we now define so-called application colorings or acolorings for block graphs. a subset of rules is a grounded 1-path for if is a 1-path from some fact to in .
definition 1 let be a logic program s.t. condition  1  holds  let be the corresponding block graph and let be a mapping. then is an a-coloring of iff the following conditions hold for each
iff one of the following conditions holds
a. and for each	we have
b. there is some	s.t.	.
iff both of the following conditions hold
a. or it exists grounded 1-path	s.t.	1
b. for each	we have	.
let be an a-coloring of some block graph . rules are then intuitively applied wrt some answer set of if they are colored . condition specifies that a rule is colored  not applied  if and only if is not  grounded    a  or is blocked by some other rule   b . a rule is colored
　 applied  if and only if it is grounded   a  and it is not blocked by some other rule   b  . this captures the intuition which rules apply wrt to some answer set and which do not  see section 1 .
　let . then program has block graph . by definition 1 there is no a-coloring of . for this reason  we need both conditions and .
lemma 1 let be a logic program s.t. condition  1  holds and let be an a-coloring of . then condition holds iff condition does not hold.
in general  we do not have the equivalence stated in this lemma  because there are examples  see above  where no acoloring exists. lemma 1 states that a-colorings are welldefined in the sense that they assign exactly one color to each node.
we obtain the main result:
theorem 1 let be a logic program s.t. condition  1  holds and let be the block graph of . then has an answer set iff has an a-coloring . furthermore  we have
answer sets can therefore be computed by computing a-
colorings  e.g.	and correspond to answer set	of program  1 .
1	computation of a-colorings
for the following description of our algorithm to compute a-colorings  let be some logic program s.t. condition  1  holds. let be a partial mapping. is represented by a pair of  disjoint  sets s.t.
	and	1. we refer to map-
ping	with the tuple	and vice versa. assume that
is a global parameter of each presented procedure  indicated through index  . let and be sets of nodes s.t. contains the currently uncolored nodes     and contains colored nodes whose color has to be propagated. figure 1 shows the implementation of the non-deterministic procedure color in pseudo code.
procedure color list partial mapping var node;
	if	propagate	fails	then	fail;
;
	if	choose	fails	then
;
　if propagate fails then fail else output ; else
;
;
if	color	succeeds then exit else
                     ; if color	succeeds then exit else fail;
figure 1: definition of procedure color .
　notice that all presented procedures  except choose   return some partial mapping through parameter or fail. choose returns some node or fails.
	when calling color	the first time  we start with
　　　  and . that is  we start with all facts colored . basically  color takes both a partial mapping and a set of uncolored nodes and aims at coloring these nodes. this is done by choosing some uncolored node     with choose and by trying to color it first. in case of failure color tries to color node with . if this also fails color fails. therefore  we say that node is used as a choice point. all different a-colorings are obtained by backtracking over choice points.
　choose selects some uncolored node     s.t. and or the following condition holds: there is some s.t. .
if there is no such then choose fails. this strategy to select choice points ensures that nodes are grounded. observe that  if choose fails and then we have to color all nodes in with   since they cannot be grounded through rules .
　during recursive calls contains the choice point of the former recursion level. the color of nodes has to be propagated with propagate  see figure 1  for two reasons. first  when coloring nodes in with color     it is not checked whether this is allowed  wrt the actual  . this check is done by propagate . this means  color fails only during propagation  see theorem 1 . second  propagating already colored nodes prunes the search space and thus reduces the necessary number of choices. since choice points make up the exponential part of our problem  propagation becomes the essential part of our approach.
　currently  we propagate only in arc direction as it is sufficient for correctness and completeness of the algorithm. therefore  we have to deal with four propagation cases: if a node is colored     then this color has to be propagated over 1- and over 1-arcs. let be nodes s.t. and assume that is already colored.
then we have to propagate this color to node . for example  propagating over 1-arcs gives . for reasons of correctness  we cannot propagate colors without any further tests. we have got the following result.
theorem 1 let be a logic program s.t. condition  1  holds  let be the corresponding block graph and let be a mapping. if is an a-coloring of then
for each	if	then conditions hold:and the following a  for each
ifwe have b  for eachwe haveifand for each
or there is someand c  for eachwe haveifand for each d  for eachwe haveifand for each.according to theorem 1  a rule contributes to some answer set if it is colored . in case of  a  there is no further condition  because a node has to be colored if there is some
1-predecessor of which is colored  take in b definition 1 . in other words  cannot be applied if it is blocked by some other applied rule. intuitively  condition  b  says that has to be applied if all of its 1-predecessors are colored   is not blocked  and one of its 1-predecessors is colored   is a consequence of applied rules or is a fact . condition  c  states that rule is applied if it is  grounded  through one of its 1-predecessors and if it is not blocked by some other rule. the last condition postulates that rule cannot be applied if cannot be derived from other applied rules. theorem 1 implies that a mapping is no a-coloring if propagate fails. figure 1 shows the implementation of propagate . the purpose of propagate is to
procedure	propagatelistpartial mapping  varnode;whiledoselectfrom;ifthen a ifpropafailsthenfail; c  　if elsepropcfailsthenfail; b  ifpropbfailsthenfail; d ifpropdfailsthenfail .figure 1: definition of procedure propagate .
apply the corresponding propagation cases  e.g. if then cases  a  and  c  have to be applied.
　the four procedures used in propagate can be easily implemented. for example  propb is shown in figure 1. first 
procedure propb node partial mapping var node ; set of nodes;
　　　　　　　　condition  b  holds for	; while	do
	select	from	;
if	then	fail; if	then
                 ; propagate	.
figure 1: definition of procedure propb .
it determines the set of all 1-successors of s.t. condition  b  holds. finally  it tests whether all nodes in can be colored . if node is currently uncolored it is colored and its color is propagated. if then propb fails  otherwise is already colored and we go on with the next node from . the procedures for the remaining propagation cases can be implemented analogously. whenever some currently uncolored node is colored during propagation  this color is recursively propagated by calling propagate .
	for partial mapping	we define the set of
corresponding answer sets	as is answer set of	and and
if is undefined for all nodes then contains all answer sets of . if is a total mapping then contains exactly one answer set of  if is an a-coloring . with this notation we formulate the following result:
theorem 1 let be a logic program s.t. condition  1  holds  let and be partial mappings. then for each we have if propagate succeeds and is the actual partial mappingafter executing propagate
then
this theorem states that propagate neither discards nor introduces answer sets corresponding to some partial mapping . hence  it justifies that only nodes used as choice points lead to different answer sets. therefore backtracking is necessary only over choice points  see figure 1 .
	define	is some output of color
               . with this notation  we obtain correctness and completeness of color .
theorem 1 let be a logic program s.t. condition  1  holds  let be its block graph  let be a mapping and let be defined as above. then is an acoloring of iff .
let us demonstrate how color computes the a-colorings of the block graph of program  1   see figure 1 . we invoke color with   and
           . first  propagate is executed. by propagating with case  c  we get and recursively . this gives . after updating uncolored nodes we obtain . now choose   variable  is executed. for choose
there are two possibilities to compute the next choice point s.t. hold  namely and . assume . after updating we have and the first recursive call color is executed where
               . again color of node has to be propagated by executing propagate .
by using propagation case  c  for	we obtain
	.	this color is recursively propagated using
case  a   which gives	.	this leads to
	. since	becomes the empty set 
choose	fails and	is the first output. invoking backtracking means that the last recursive call to color	fails. then and color	is executed. by using case  d  for	we obtain	and thus	with case  b . hence the second solution is
. since there is no other choice
point  we have no further solutions.
1	generalizations
in this section  we discuss generalizations of the presented approach. first  we show how to apply our method to normal logic programs with multiple positive body literals. let be a normal program without restrictions. for each rule we define
 1 
where are new atoms not appearing in . for a program we set . hence  we have defined a local program transformation which has linear size of the original program. each normal program is transformed into some program in which for each rule with
             . that is why we may interpret as some kind of normal form of . the following lemma obviously holds:
lemma 1 let be a normal logic program and let be a set of atoms. then is an answer set of iff there exists an answer set of s.t. and contain exactly the same atoms out of the set of all atoms occurring in .
　it is straightforward to extend the algorithm presented in section 1 to normal programs . let us call all rules
with and-nodes and all other rules ornodes. observe  that on the one hand  we do not have to modify definition 1 of block graphs for programs . it stays as it is. we just distinguish two different kinds of nodes in . on the other hand  definition 1 and procedures presented in section 1 deal only with or-nodes and consequently we have to extend it to and-nodes. for and-node we know by definition that  see  1  . therefore  cannot be blocked and we do not have to consider cases b and b of definition 1. in order to extend definition 1  we require that the following conditions hold for each and-node  corresponding to conditions a and a of definition 1 for or-nodes : iff there is some s.t.
	iff for each	we have	.
according to  1   for and-node	we have
   and will nevercontainany andnode. for this reason  we obtain only two new propagation cases  in which we propagate the color of or-nodes over 1arcs to and-nodes. let be some and-node and let be the actual color of  or-node . then we have the following new cases
 c'  for each for eachwe haveifand d'  for eachwe haveif which can be easily integrated into propagate .
　 gelfond and lifschitz  1  show that logic programs with classical negation are equivalent to normal logic programs when new atoms are introduced. with this technique our approach is also suitable for computing answer sets of general logic programs  with classical negation . additionally  we may apply techniques presented in  janhunen et al.  1  to handle disjunctive logic programs.
1	related work
directed graphs are often associated with logic programs and used in theory and applications. usually  the nodes of these graphs are the atoms of the programs  e.g. dependency graphs or tms networks  doyle  1. these approaches use rules to define graphs on atoms whereas we have used atoms to define graphs on rules.
　other approaches  dimopoulos and torres  1; brignoli et al.  1  are more or less rule-based but have some serious drawbacks: they deal only with prerequisite-free programs  because  wrt to answer set semantics  there is some equivalent prerequisite-free program for each program. since in general equivalentprerequisite-freeprogramshave exponentialsize of the original ones  approaches which rely on this equivalence need exponential space.
　in fact  the block graph is a specialization of graphs defined on rules in  papadimitriou and sideri  1; linke and schaub  1  for default theories. whereas in the case of default logic the aforementioned graphs are abstractions of the essential blocking information  here they contain all information necessary for computing answer sets. although we focus on the practical usage of the block graph it may also be used as a tool for theoretical analysis of logic programs. for example  results presented in  linke and schaub  1  imply that a logic program without odd cycles always has some answer set.
　clearly  color is  like smodels  a davis-putnam like procedure. once again  the main difference is that color determines answer sets in terms of generating rules whereas smodels and dlv construct answer sets in terms of literals. using rules instead of atoms has the advantage that we have complete knowledge about which rule is responsible for some atom belonging to an answer set. atom-based approaches additionally have to detect the responsible rule and ensure groundedness  because in general there may be several rules with the same head. we obtain groundedness of generating rules as a by-product of our strategy to select choice points with procedure choose .
1	conclusion
the main contribution of this paper was the definition of the block graph of a program . as a theoretical tool  the block graph seems to be suitable for investigations of many concepts for logic programs  e.g. answer set semantics  wellfounded semantics or query-answering. as a first result  we have described answer sets as a-colorings  a non-standard kind of graph colorings of  . this led us to an alternative algorithm to compute answer sets by computing a-colorings which needs polynomial space.
　finally  let us give first experimental results to demonstrate the practical usefulness of our algorithm. we have used two np-complete problems proposed in  cholewin＞ski et al.  1 : the problem of finding a hamiltonian path in a graph  ham  and the independent set problem  ind .
　concerning time  our first prolog implementation  development time 1 month  is not comparable with state of the art implementations. however  theorem 1 suggests to compare the number of used choice points  because it reflects how an algorithm deals with the exponential part of a problem. unfortunately only smodels gives informationabout its choice points. for this reason  we have concentrated on comparing our approach with smodels. results are given for finding
smodels11nomore11table 1: number of choice points for ham-problems.
all solutions of different instances of ham and ind. table 1 shows results for some ham-encodings of complete graphs where is the number of nodes1. surprisingly  it turns out that our non-monotonic reasoning system  nomore  performs very well on this problem class. that is  with growing problem size we need less choice points than smodels. this can also be seen in table 1 which shows the corresponding time measurements. for finding all hamiltonian cycles of a we need less time than the actual smodels version. to be fair  for ind-problems of graphs cir 1we need twice

1
 in a complete graph each node is connected to each other node. 1
	a so-called circle graph cir	has	nodes	and arcs
.
the choice points  and much more time  smodels needs  because we have not yet implemented backward-propagation. however  even with the same number of choice points smodels is faster than nomore  because nomore uses general backtracking of prolog  whereas smodels backtracking is highly specialized for computing answer sets. the same applies to dlv.
ham forind for cirsmodels111dlv111nomore111table 1: time measurements in seconds for ham- and indproblems on a sun ultra1 with two 1mhz sparc processors.
　for future work  we may also think of different improvements of our algorithm. first of all  we have to integrate backward propagation  propagating colors against arc direction   since this will definitely improve efficiency by further reducing the number of choice points. second  we may try to pre-color not only facts but also some other nodes  e.g. each node with has to be colored . the block graph may also be used for other improvements. for example  it is possible to replace 1-paths without incoming and outgoing 1-arcs by only one 1-arc. finally  we have to investigate different heuristics for procedure choose to select the next choice point.
　the approach as presented in this paper has been implemented in eclipse-prolog  aggoun et al.  1 . the current prototype is available at http://www.cs.unipotsdam.de/ linke/nomore.
acknowledgements
i would like to thank t. schaub  ph. besnard  k. wang  k. konczak  ch. anger and s.m. model for commenting on previous versions of this paper.
　this work was partially supported by the german science foundation  dfg  within project  nichtmonotone inferenzsysteme zur verarbeitung konfligierender regeln .
references
 aggoun et al.  1  a. aggoun  d. chan  p. dufresne and other. eclipse user manual release 1. eclipse is available at http://www.icparc.ic.ac.uk/eclipse  1.
 brewka  1  g. brewka. nonmonotonic reasoning: logical foundations of commonsense. cambridge university press  cambridge  1.
 brignoli et al.  1  g. brignoli  s. costantini  o. d'antona  and a. provetti. characterizing and computing stable models of logic programs: the non-stratified case. in proc. of conf. on information technology  pp. 1  1.
 cholewin＞ski et al.  1  p. cholewin＞ski  v. marek  a. mikitiuk  and m. truszczyn＞ski. experimentingwith nonmonotonic reasoning. in proc. of the int. conf. on logic programming  pp. 1. mit press  1.
 cholewin＞ski et al.  1  p. cholewin＞ski  v. marek  and m. truszczyn＞ski. default reasoning system deres. in proc. kr-1  pp. 1. morgan kaufmann publishers  1.
 dimopoulos and torres  1  y. dimopoulos and a. torres. graph theoretical structures in logic programs and default theories. theoretical computer science  1- 1  1.
 dimopoulos et al.  1  y. dimopoulos  b. nebel  and j. koehler. encoding planning problems in non-monotonic logic programs. proc. of the 1th european conf. on planing  pp. 1 toulouse  france  1. springer verlag.
 doyle  1  j. doyle. a truth maintenance system. artificial intelligence  1-1  1.
 egly et al.  1  u. egly  th. eiter  h. tompits  and st. woltran. solving advanced reasoning tasks using quantified boolean formulae. proc. aaai-1  pp. 1  1.
 eiter et al.  1  t. eiter  n. leone  c. mateis  g. pfeifer  and f. scarcello. a deductive system for nonmonotonic reasoning. in j. dix  u. furbach  and a. nerode  editors  lpnmr-1  pages 1. springer verlag  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. the stable model semantics for logic programming. in proc. of the int. conf. on logic programming  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. logic programs with classical negation. in proc. of the int. conf. on logic programming  pp. 1  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. classical negation in logic programs and deductive databases. new generation computing  1-1  1.
 janhunen et al.  1  t. janhunen  i. niemela：  p. simons  and j. you. unfolding partiality and disjunctions in stable model semantics. in a.g. cohn  f. guinchiglia  and b.selman  editors  proc. kr-1  pp. 1  1.
 linke and schaub  1  t. linke and t. schaub. alternative foundations for reiter's default logic. artificial intelligence  1-1  1.
 liu et al.  1  x. liu  c. ramakrishnan and s.a. smolka. fully local and efficient evaluation of alternating fixed points. proc. of the 1th int. conf. on tools and algorithms for the construction analysis of systems  pp. 1  lisbon  portugal  1. springer verlag.
 niemela： and simons  1  i. niemela： and p. simons. smodels: an implementation of the stable model and wellfounded semantics for normal logic programs. in j. dix  u. furbach  and a. nerode  editors  proc. lpnmr-1  pp. 1. springer  1.
 niemela：  1  i. niemela：. logic programming with stable model semantics as a constraint programming paradigm. annals of mathematics and artificial intelligence  1 1 :1 1.
 papadimitriou and sideri  1  c. papadimitriou and m. sideri. default theories that always have extensions. artificial intelligence  1-1  1.

logic programming and
theorem proving
logic programming

a framework for declarative update specifications in logic programs
thomas eiter and michael fink and giuliana sabbatini and hans tompits
institut fu：r informationssysteme  abteilung wissensbasierte systeme 1 
technische universita：t wien  favoritenstrasse 1  a-1 vienna  austria
e-mail: eiter michael giuliana tompits  kr.tuwien.ac.at

abstract
recently  several approaches for updating knowledge bases representedas logic programshave been proposed. in this paper  we present a generic framework for declarative specifications of update policies  which is built upon such approaches. it extends the lups language for update specifications and incorporates the notion of events into the framework. an update policy allows an agent to flexibly react upon new information  arriving as an event  and perform suitable changes of its knowledge base. the framework compiles update policies to logic programs by means of generic translations  and can be instantiated in terms of different concrete update approaches. it thus provides a flexible tool for designing adaptive reasoning agents.
1	introduction
updating knowledge bases is an important issue for the realization of intelligent agents  since  in general  an agent is situated in a changingenvironmentandmust adjust its knowledge base when new information is available. while for classical knowledge bases this issue has been well-studied  approaches to update nonmonotonic knowledge bases  like  e.g.  updates of logic programs  alferes et al.  1; eiter et al.  1; zhang and foo  1; inoue and sakama  1  or of default theories  williams and antoniou  1   are more recent.
　the problem of updating logic programs  on which we focus here  deals with the incorporation of an update   given by a rule or a set of rules  into the current knowledge base
   . accordingly  sequences of updates lead to sequences of logic programs  which are given a declarative semantics. to broaden this approach  alferes et al.  1a  have proposed the lups update language  in which updates consist of sets of update commands. such commands permit to specify changes to in terms of adding or removing rules from it. for instance  a typical command is   stating that rule should be added to if is currently true in it. similarly  expresses that must be eliminated from
  without any further condition.
　however  a certain limitation of lups and the above mentioned formalisms is that while they handle ad hoc changes of   they are not conceived for handling a yet unknown update  which will arrive as the environment evolves. in fact  these approaches lack the possibility to specify how an agent should react upon the arrival of such an update. for example  we would like to express that  on arrival of the fact
　　   this should be added to   while bestbuy information about other shops is removed from .
　in this paper  we address this issue and present a declarative framework for specifying update behavior of an agent. the agent receives new information in terms of a set of rules  which is called an event   and adjusts its in accord to a given update policy  consisting of statements in a declarative language. our main contributions are summarized as follows:
　 1  we present a generic framework for specifying update behavior  which can be instantiated with different update approaches to logic programs. this is facilitated by a layered approach: at the top level  the update policy is evaluated  given an event and the agent's current belief set  to single out the update commands which need to be performed on . at the next layer  is compiled to a set of rules to be incorporatedto ; at the bottom level  the updatedknowledge base is represented as a sequence of logic programs  serving as input for the underlying update semantics for logic programs  which determines the new current belief set.
　 1  we define a declarative language for update policies  generalizing lups by various features. most importantly  access to incoming events is facilitated. for example  retract   states that if  is told  then  is removed from the knowledge base. statements like this may involve further conditions on the current belief set  and other commands to be executed  which is not possible in lups . the language thus enables the flexible handling of events  such as simply recording changes in the environment  skipping uninteresting updates  or applying default actions.
　 1  we analyze some properties of the framework  using the update answer set semantics of eiter et al.  as a representative of similar approaches. in particular  useful properties concerning maintenance are explored  and the complexity of the framework is determined. moreover  we describe a possible realization of the framework in the agent system impact  subrahmanian et al.  1   providing evidence that our approach is a viable tool for developing adaptive reasoning agents.
1	preliminaries
we assume the reader familiar with extended logic programs
 elps   gelfond and lifschitz  1 . for a rule   we write and to denote the head and body of   respectively. furthermore  stands for default negationand for strong negation. is the set of all literals over a set of atoms   and is the set of all rules constructible from .
	an update program  p  is a sequence	of
elps  where . we adopt an abstract view of the semantics of elps and update programs given as a mapping   which associates with every sequence p a set p of rules; intuitively  p are the consequences of p. different instantiations of are possible  according to various proposals for update semantics. we only assume that satisfies some elementary properties which any  reasonable  semantics satisfies. in particular  we assume that
p holds  and that the following property is satis-
fied: given p and   then p iff	p .
　we use here the semantics of eiter et al.   which coincides with the semantics of inheritance programs due to buccafurriet al. . the semantics of elps and update sequences p with variables is defined as usual through their ground versions and p over the herbrand universe  respectively. in what follows  let     p  etc. be ground.
　an interpretation is a set which contains no complementary pair of literals. is a  consistent  answer set of an elp iff it is a minimal model of the reduct
     which results from by deleting all rules whose body contains some default literal with   and by removing all default literals in the bodies of the remaining rules  gelfond and lifschitz  1 . by we denote the collection of all answer sets of . the rejection set 
　　　p   of p with respect to the interpretation	is given by	p	p where	p	  and  for	 	p contains every rule
such that	and	  for some
	p with	. then 	is an answer set of
p	iff	is an answer set of	p .
we denote the collection of all answer sets of p by p . since implies p   the semantics extends the answer set semantics.  eiter et al.  1  describes a characterization of the update semantics in terms of single elps.
example 1 let	 
       and	. then 	has the single answer set	with	;	has answer set	with	; and possesses	as unique answer set with	.
　the belief set p is the set of all rules such that is true in each p . we shall drop the subscript     if no ambiguity can arise. with a slight abuse of notation  for a literal   we write p if p .
1	update policies
we first describe our generic framework for event-based updating  and afterwards the language   the language

belief set at step
knowledge state
update policy
executable commands
compilation
update sequence
update semantics

figure 1: from knowledge state to belief set at step .
around   for specifying update policies.
1	basic framework
we start with the formal notions of an event and of the knowledge state of an agent.
definition 1 an event class is a collection of finite sets of rules. the members are called events.
　informally  describes the possible events  i.e.  sets of communicated rules  an agent may witness. for example  the collection of all sets of facts from a subset of atoms may be an event class. in what follows  we assume that an event class has been fixed.
definition 1 a knowledge state consists of an elp	the initial knowledge base  and a sequence	of events	 	. for
      	is the projection of to the first events.
　intuitively  describes the evolution of the agent's knowledge  starting from its initial knowledge base. when a new event occurs  the current knowledge state
changes to	  which requests the agent to incorporate the event into its knowledge base and adapt its belief set.
　the procedure for adapting the belief set on arrival of is illustrated in figure 1. informally  at step of the knowledge evolution  we are given the belief set and the knowledge state
                   together with the new event   and we want to compute in terms of the update policy . first  a set of executable commands is determined from . afterwards  given the previously computed sets   the sequence is compiled by the transformation into the update sequence p
	. then 	is given by	p .

	::=	;
::=
		;
	::=	;
	::=	;
	::=			;
	::=	;
	::=	
	 	;
	::=	 	;
	::=	;
	::=	 	;
	::=	;

	table 1: syntax of an update statement in	.
1	language	: syntax
the language generalizes the update specification language lups  alferes et al.  1a   by allowing update statements to depend on other update statements in the same program  and more complex conditions on both the current belief set and the actual event  note that lups has no provision to support external events . these features make it suitable for implementing rational reactive agents  capable  e.g.  of filtering incoming information.
　the syntax of is given in table 1. in what follows  we use to denote update commands and to refer to rules or rule variables. in general  an statement may have the form
which states conditional assertion or retraction of a rule
	  expressed by	  depending on other commands
　　　　　　　...    and conditioned with the proviso whether belongs to the current belief set and whether is in the actual event. the basic commands are the same as those in lups  for their meaning  cf.  alferes et al.  1a    plus the additional command ignore  which allows to skip unintended updates from the environment  which otherwise would be incorporated into the knowledge base. each conditionin   both of the form and   can be substituted by a list of such conditions. note that in lups no conditions on rules and external events can be explicitly expressed  nor dependencies between update commands. we also extend the language by permitting variables for rules and literals in the update commands  ranging over the universe of the current belief set and of the current event  syntactic safety conditions can be easily checked . by convention  variable names start with capital letters.
definition 1 an update policy is a finite set of statements.
for instance  the	statement
 1 
means that all rules in the event have to be incorporated into the new knowledge base  except if it is explicitly specified that the rule is to be ignored. similarly  the command retract forces a rule to be deactivated. the option event states that an assertion or retraction has only temporary value and is not
supposedto persist by inertia in subsequentsteps. the precise meaning of the different update commands will be made clear in the next section.
example 1 consider a simple agent selecting web shops in search for some specific merchandise. suppose its knowledge base    contains the rules


and a fact	as an initial time stamp. here 
expresses that a shop   which has a sale and whose web site is up  is queried by default  and   serve to detect that no site is queried  which causes ' ' to be true. assume that an event    might be any consistent set of facts or ground rules of the form   stating that shop has a sale on date   such that contains at most one time stamp
.
　an update policy may be defined as follows. assume it contains the incorporate-by-defaultstatement   as well as:
;
;
.
informally  the first statement repeatedly confirms the information about a future sale  which guarantees that it is effective on the given date  while the second statement revokes this. the third one removes information about a previously ended sale assuming the time stamps increase . furthermore  includes also the following statements:
;
;
.
the first statement keeps the time stamp	in
unique  and removes the old value. the other statements simply state that sales information about shop is ignored.
1	language	: semantics
according to the overall structure of the semantics of   as depicted in figure 1  at step   we first determine the executable command given the current knowledge state and its associated belief set
                 p   where p	. to this end  we evaluate the update policy	over the new event	and the belief set	p .
	let	be the groundedversionof	overthe language
underlyingthe givenupdate sequenceand the receivedevents. then  the set of reduced update statements at step is given by
where
and such that
	p	and
	the update statements in	are thus of the form
	.	se-
mantically  we interpret them as ordinary logic program rules .	the
program is the collection of all these rules  given   together with the following constraints  which exclude contradictory commands:


definition 1 let be a knowledge state and an update policy. then  is a set of executable update commands at step iff is an answer set of the grounding of .
　since update statements do not contain strong negation  executable update commands are actually stable models of
　　　 gelfond and lifschitz  1 . furthermore  since programs may in general have more than one answer set  or no answer set at all  and the agent must commit itself to a single set of update commands  we assume a suitable selection function    returning a particular if an answer set exists  or  otherwise  returning   where is a reserved atom. these atoms are used for signaling that the update policy encountered inconsistency. they can easily be filtered out from   if needed  restricting the outcomes of the update to the original language.
　next we compilethe executablecommands into an update sequence   serving as input for the belief function . this is realized by means of a transformation   which is a generic and adapted version of a similar mapping introducedby alferes et al.  1a . in what follows  we assume a suitable naming function for rules in the update sequence  enforcing that each rule is associated with a unique name .
definition 1 let	be a knowledge state and an update policy. then  for   is inductively de-
fined as follows  where are the executable commands according to definition 1:
set
　　　　　　　　　　  where	are new atoms. furthermore  initialize the sets	of persistent commands and	of effective commands to	.  	and	are as follows:
	=	;
=



=










		.
　on the basis of this compilation  we can define the belief set for a knowledge state :
definition 1 let and be as in definition 1  and let be the corresponding executable commands obtained from definition 1. then  the belief set of is given by
example 1 reconsider example 1 and suppose the event
	occurs at	. then 
 
 
the corresponding program	has the single answer set
 
which is compiled  via function	  to 	and
. as easily seen  the belief set
=	contains	and	.
1	properties
in this section  we discuss some properties of for particularupdatepolicies  using the definitionof based on the update answer sets approach of eiter et al.   as explained in section 1. we stress that the properties given below are also satisfied by similar instantiations of   like  e.g.  dynamic logic programming  alferes et al.  1 . first  we note some basic properties:
if  called empty policy   then will never be updated; the belief set is independentof   and thus static. hence    for each
.
if	 called unconditional assert policy   then all rules contained in the received events are directly incorporated into the update sequence. thus 	  for each	.
	if	is empty  then the knowledge is not updated  i.e. 
	. we thus have	.
	similarly  if	  then
.
physical removal of rules
an important issue is the growth of the agent's knowledge base  as the modular construction of the update sequence through transformation causes some rules and facts to be repeatedly inserted. this is addressed next  where we discuss the physical removal of rules from the knowledge base.
lemma 1 let p be an update sequence. for every with   the following holds: if i   or ii and   or iii
	such that no rule	with	exists 
where	  and	  then	p the following property holds:
theorem 1 let	be a knowledge state and
　　p   where p . furthermore  let p result from p after repeatedly removing rules as in lemma 1  and let p	  where
	p	p
p
then 	p .
　thus  we can purge the knowledge base and remove duplicates of rules  as well as all deactivated  retracted  rules.
history contraction
another relevant issue is the possibility  for some special case  to contract the agent's update history  and compute its belief set at step merely based on information at step
　　. let us call a factual assert policy if all assert  event  and always  event  statements in involve only facts. in this case  the compilation for a knowledge state
　　　　　　　　can be simplified thus:  1 	  and  1  the construction of each	involves facts	and instead of	and	  respectively.
for such sequences  the following holds:
lemma 1 let p	be an update sequence such
that contains only facts  for . then  p   where   and
.
　we can thus assert the following proposition for history contraction:
theorem 1 let be a factual assert policy and p be the compiled sequence obtained from by
the simplified method described above. then 
	  where	is as in lemma 1.
　simple examples show that theorem 1 does not hold in general. the investigation of classes of policies for which similar results hold are a subject for further research.
computational complexity
finally  we briefly address the complexity of reasoning about a knowledge state . an update policy is called stratified iff  for all statements   the asso-
ciated rulesform a stratified logic program where	results fromby replacing the	declarationby default negation.	for stratified	  anyhas at most one answer set. thus the selection function	is redundant.	otherwise  thecomplexity cost ofmust be taken into account.	if　　　is unknown  we consider all possible return values  i.e.  all answer sets of   and thus  in a cautious reasoning mode  all possible
from figure 1. clearly  for update answer sets  deciding is in conp; it is polynomial  if is stratified and each     contains only facts.
theorem 1 let be the update answer set semantics  and polynomial-time computable with an np oracle. then  given a ground rule and ground
  the complexity of deciding whether
is as follows entries denote completeness results;
the case of unknown	is given at the right of  /  :
fact. assert & strat.stratifiedgeneralstratified/general/similar results hold  e.g.  for dynamic logic programming.
the results can be intuitively explained as follows. each and as in figure 1 can be computed iteratively  
        where at step polynomially many problems must be solved to construct . from and previous results  is easily computed in polynomial time. since contains less than rules  step is feasible in polynomial time with an np oracle.
thus  p is polynomially computable with an np oracle  and p is decided with another oracle call. updating a stratified such that only sets of facts   may be added preserves polynomial decidability of
; this explains the polynomial de-
cidability result. in all other cases  -hard problems such as computing the lexicographically maximal model of a cnf formula are easily reduced to the problem.
　if is unknown  each possible result of can be nondeterministically guessed and verified in polynomial time. this leads to conp complexity.
1	implementational issues
an elegant and straightforward realization of update policies is possible through impact agent programs. impact  subrahmanian et al.  1  is a platform for developing software agents  which allows to build agents on top of legacy code  i.e.  existing software packages  that operates on arbitrary data structures. thus  in accordance with our approach  we can design a generic implementation of our framework  without committing ourselves to a particular update semantics	.
　since every update policy is semantically reduced to a logic program  the corresponding executable commands can be computed using well-known logic programming engines like     or . hence  we may assume that a software package    for updating and querying a knowledge base is available  and that can be accessed through a function returning the current belief set . moreover  we assume that has a function
         which lists all rules of a current event. then  an update policy can be represented in impact as follows.
　 1  conditions on the belief set and the event can be modeled by impact code call atoms  i.e. atoms  
		  and	  where	is a constant
 or a variable . in impact  is true if constant is in the result returned by ; a variable is bound to all such that is true;      is negation.
　 1  update commands can be easily represented as impact actions. an action is implemented by a body of code in any programming language  e.g.  c ; its effects are specified in terms of add and delete lists  sets of code call atoms . thus  actions like     etc.  where is a parameter  are introduced.
　 1  statements are represented as impact action rules

 where  is the list of the code call atoms for the conditions on the belief set and the event in as described above.
　the semantics of impact agent programs is defined through status sets. a reasonable status set is equivalent to a stable model of a logic program  and prescribes the agent to perform all actions where is in . thus  represents the executable commands of figure 1 in accord with   and the respectiveaction executionaffects the computation of via . for more details  cf.  eiter et al.  1 .
1	related work and conclusion
our approach is similar in spirit to the work in active databases  adbs   where the dynamics of a database is specified through event-condition-action  eca  rules triggered by events. however  adbs have in general no declarative semantics  and only one rule at a time fires  possibly causing successive events. in  baral and lobo  1   a declarative characterization of adbs is given  in terms of a reduction to logic programs  by using situation calculus notation.
　our language for update policies is also related to action languages  which can be compiled to logic programs as well  cf.  e.g.   lifschitz and turner  1  . a change to the knowledge base may be considered as an action  where the execution of actions may depend on other actions and conditions. however  action languages are tailored for planning and reasoning about actions  rather than reactive behavior specification; events would have to be emulated. furthermore  a state is  essentially  a set of literals rather than a belief set as we define it. investigating the relationships of our framework to these languages in detail-in particular concerning embeddings-is an interesting issue for further research.
　a development in the area of action languages  with purposes similar to those of   is the policy description language  lobo et al.  1 . it extends traditional action languages with the notion of event sequences  and serves for specifying actions as reactive behavior in response to events. a policy is a collection of eca rules  interpreted as a function associating with an event sequence a set of actions.
	seems thus to be more expressive than	; possible
embeddings of	into	remain to be explored.
	the	language could be extended with several features:
	 1  special atoms	telling whether	is actually part of
　　 i.e.  activated by    allowing to access the  extensional  part of .
 1  rule terms involving literal constants and variables 
e.g.       where are variables and is a fixed atom  providing access to the structure of rules. combined with  1   commands such as  remove all rules involving   can thus be conveniently expressed.
　 1  more expressive conditions on the knowledge base are conceivable  requesting for more complex reasoning tasks  and possibly taking the temporal evolution into account. e.g.      expressing that was true at the previous stage.
　in concluding  our generic framework  which extends other approaches to logic program updates  represents a convenient platform for declarative update specifications and could also
be fruitfully used in several applications. exploring these issues is part of our ongoing research.
acknowledgements
this work was partially supported by the austrian science fund  fwf  under grants p1-inf and n z1-inf.
references
 alferes et al.  1a  j. alferes  l. pereira  h. przymusinska  and t. przymusinski. lups - a language for updating logic programs. in proc. lpnmr'1  lnai 1  pp. 1. springer  1.
 alferes et al.  1  j. alferes  j. leite  l. pereira  h. przymusinska  and t. przymusinski. dynamic updates of nonmonotonicknowledgebases. j. logic programming  1 :1  1.
 baral and lobo  1  c. baral and j. lobo. formal characterization of active databases. in proc. lid'1  lncs 1  pp. 1. springer  1.
 buccafurri et al.  1  f. buccafurri  w. faber  and n. leone. disjunctive logic programs with inheritance. in proc. iclp'1  pp. 1. mit press  1.
 eiter et al.  1  th. eiter  m. fink  g. sabbatini  and h. tompits. considerations on updates of logic programs. in proc. jelia'1  lnai 1  pp. 1. springer  1.
 eiter et al.  1  th. eiter  m. fink  g. sabbatini  and h. tompits. declarative knowledge updates through agents. in proc. aisb'1 symp. on adaptive agents and multi-agent systems  york  uk  pp. 1. aisb  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. the stable model semantics for logic programming. in proc. icslp'1  pp. 1. mit press  1.
 gelfond and lifschitz  1  m. gelfond and v. lifschitz. classical negation in logic programs and disjunctive databases. new generation computing  1-1  1.
 inoue and sakama  1  k. inoue and c. sakama. updating extended logic programs through abduction. in proc. lpnmr'1  lnai 1  pp. 1. springer  1.
 lifschitz and turner  1  v. lifschitz and h. turner. representing transition systems by logic programs. in proc. lpnmr'1  lnai 1  pp. 1. springer  1.
 lobo et al.  1  j. lobo  r. bhatia  and s. naqvi. a policy description language. in proc. aaai/iaai'1  pp. 1. aaai press / mit press  1.
 marek and truszczyn＞ski  1  w. marek and m. truszczyn＞ski. revision programming. theoretical computer science  1-1  1.
 subrahmanian et al.  1  v.s. subrahmanian  j. dix  th. eiter  p. bonatti  s. kraus  f. ozcan  and r. ross. heterogeneous agent systems. mit press  1.
 williams and antoniou  1  m.-a. williams and g. antoniou. a strategy for revising default theory extensions. in proc. kr'1  pp. 1. morgan kaufmann  1.
 zhang and foo  1  y. zhang and n. foo. updating logic programs. in proc. ecai'1  pp. 1. 1.
abduction in logic programming:
a new definition and an abductive procedure based on rewriting
	fangzhen lin	jia-huai you
	department of computer science	department of computing science
hong kong university of science and technology	university of alberta
	clear water bay  kowloon  hong kong	edmonton  alberta  canada t1g 1abstract
we propose a new definition of abduction in logic programming  and contrast it with that of kakas and mancarella's. we then introduce a rewriting system for answering queries and generating explanations  and show that it is both soundand complete under the partial stable model semantics and sound and complete under the answer set semantics when the underlying program is so-called odd-loop free. we discuss an application of the work to a problem in reasoning about actions and provide some experimental results.
1	abduction in logic programming
in general  given a background theory   and an observation to explain  an abduction of w.r.t. is a theory such that . normally  we want to put some additional conditions on   such as that it is consistent with and contains only those propositions called abducibles. for instance  in propositional logic  given a background theory   a set of assumptions or abducibles  and a proposition   an explanation of is commonly defined  see  reiter and de kleer  1    poole  1   and  konolige  1   to be a minimal set of literals over such that and is consistent.
　in the context of logic programming  abduction has been investigated from both proof-theoretic and model-theoretic perspectives  e.g.  eshghi and kowalski  1; kakas and mancarella  1; satoh and iwayama  1  . one of the most influential definitions of abduction in logic programming is that of kakas and mancarella's generalized stable model semantics . given a logic program   a set of atoms standing for abducibles  and a query   kakas and mancarella define an abductive explanation to be a subset of such that there is an answer set  also called a stable model  of that satisfies .
　one can see the following two differences between this definition and the one that we defined above for propositional logic: in propositional logic  is a set of literals  but in logic programming  it is just a set of atoms; in propositional logic  must be minimal  in terms of the subset ordering relation; but there is no such requirement in the case of logic programming.
　one could argue that these differences are due to the fact that under the answer set semantics  negation is considered to be  negation-as-failure.  if none of the atoms in appear in the head of a rule in the logic program   then adding a set to really means that we are adding the complete literal set    to . this would also explain why there is no minimality conditionin the definition: two complete sets of literals are never comparable in terms of the subset relation.
　however  while this notion of abductive explanations makes sense in theory  it is problematic in practice. for instance  if   and   then there are two abductive explanations for according to kakas and mancarella's definition: and . in general  if has elements  then there are abductive explanations for   a number that is too big to manage.
　since in this case is the explanation that we are looking for  it is tempting here to say that we should prefer minimal abductive explanations like what we did for propositional logic. as we mentioned above  this does not make sense if we take an abductive explanation to be a complete set of literals as implied by the answer set semantics. however  one can still try to minimize the set of atoms  in this case  preferring over	.
　however  this minimization strategy is problematic when a program contains negation. consider a situation in which a boat can be used to cross a river if it is not leaking or  if it is leaking  there is a bucket available to scoop the water out of the boat. this can be axiomatized by the following logic program :
now suppose that we saw someone crossed the river  how do we explain that  clearly  there are two possible explanations: either the boat is not leaking or the person has a bucket with her. in terms of kakas and mancarella's definition  there are three abductive explanations for    
　　　　　　　　  and	  assuming that	is the set of abducibles. but only one of them    is minimal.
　on a closer look  we see that in our first example  when we say that is a preferred explanation over all the others  we do not mean the complete set of literals   is preferred over all the others. while we want to be part of the explanation  we don't necessarily want because we do not want to apply negation as failure on abducibles  which are assumptions one can make one way or the other. what we want is for the set itself to be the best explanation for .
　one way to justify this is that all possible ways of completing this set into a complete set of literals  and
       turn out to correspond to all the abductive explanations of according to kakas and mancarella's definition. the same kind of justification turns out to work for our second example as well: the reason that is not a preferred explanation is that while its completion according to negation-as-failure 	is an explanation  some of its other completions  for example is not an explanation. this
motivates our following definitions.
1	abduction in logic programming revisited
in this paper  we consider  normal  logic programs which are sets of rules of the form
where   and are atoms of the underlying propositional language . here     is the so-called default negation  and defined according to the answer set semantics  gelfond and lifschitz  1 .
　let be a logic program  a set of propositions standing for abducibles  and a proposition. in the following  without loss of generality  we shall assume that none of the abducibles in occur in the head of a rule in .1
　in the following  by a hypothesis we mean a consistent set of literals over   i.e. it is not the case that and are both in for some . we say that a hypothesis is complete if for each atom   either or is in   but not both. notice that a complete hypothesis is really a truth-value assignment over the language . we say that a hypothesis is an extension of another one if   and a complete extension is an extension that is complete.
definition 1 a complete hypothesis	is said to be an explanation of	w.r.t.	and	iff there is an answer set	of such that	contains	and for any	 	  where	is the set of atoms in	.
definition 1 a hypothesis is said to be an explanation of iff every complete extension of it is an explanation. a hypothesis is said to be a minimal explanation if it is an explanation  and there is no other explanation such that .
	consider the logic program	in the previous section about
　　　　. the following are the complete hypotheses that explain	:
now consider . clearly every complete extension of this set is an explanation  so it is an explanation as well. furthermore  it is a minimal explanation as none of its element can be deleted for it continue to be an explanation.
similarly 	is also a minimal explanation.
　if we take a hypothesis to be the conjunction of its elements  then we have that in the propositional logic 
where is the set of all complete hypotheses that are explanations of   the set of all explanations of   and the set of all minimal explanations of . therefore the set of minimal explanations is a succinct representation of the set of all explanations.
　it is clear from our definition that a complete hypothesis is an explanation of iff is an abductive explanation of according to kakas and mancarella's definition. this implies that if none of the abducibles occur in the head of any clauses in   then
where is the set of all abductive explanation of according to kakas and mancarella's definition 
　　　　  and is the set of all minimal explanations of . so in a sense  the set of kakas and mancarella's abductive explanations and that of our minimal explanations are equivalent. however  as we have seen above  the number of abductive explanations can be very large. enumerating them all is impossible even in simple  small domains. in contrast  the number of minimal explanations are much smaller. more importantly  just like explanations in propositional logic  they only include  relevant propositions. 
　but computationally  it may be hard to compute minimal explanations from scratch. it is often easier to compute first a small  cover  of all explanations.
definition 1 a set	of hypotheses is said to be a cover of
w.r.t.	and	iff
where	is the set of minimal explanations of .
proposition 1 if is a cover of   then each must be an explanation of .
so a cover is a set of explanations such that any complete explanationmust be an extensionof one of the explanationsin the cover. once we have a cover  then we can find all minimal explanations by propositional reasoning alone. recall that a conjunction of literals is a prime implicant of a formula
if   and there is no other such that and is a subset of   i.e. is a minimal conjunction of literals that entails .
proposition 1 let be a cover of . then a hypothesis is a minimal explanationof iff it is a prime implicant of .
　in the rest of this paper  we shall propose a rewriting system for generating explanations of a proposition in a logic program. we shall first define it for logic programs without abducibles. we will then extend the system to logic programs with abducibles  and show that for any query  the rewriting system generates an approximation of a cover set in the general case  and exactly a cover set when the given logic program has no so-called  odd loops.  we will then discuss an application of our system to reasoning about actions and present some experimental results.
1	goal rewrite systems
given a  ground  program   the clark completion of   denoted   is the following set of equivalences: for each atom  
	if	does not appear as the head of any rule in	 
.
otherwise 	 with default negations replaced by negative literals   if there are exactly rules with as the head. we write for if is empty.
　the idea of goal rewriting is simple. a completed definition	can be used as a rewrite rule from left to right: is rewritten to   and to . we call these literal rewriting  and the completed definitions program  rewrite  rules.
　a goal is a formula which may involve   and . a goal is also referred to as a goal formula. a goal resulted from a literal rewriting from another goal is called a derived goal. a goal with negation appearing only in front of atoms is said to be signed  a term introduced in  kunen  1  for a similar purpose. for convenience  we assume that all goals are signed  which can be achieved easily by simple transformations using the following rules: for any formulas and  
　like a formula  a goal may be further transformed to a suitable form for literal rewriting without changing its semantics. with a mechanism of loop handling  rewriting of a goal terminates at either meaning that is proved  or meaning that is not proved. hence  a goal rewrite system consists of three types of rewrite rules:  i  programrules from for literal rewriting   ii  simplification rules to transform and simplify goals  and  iii  loop rules for handling loops.
1	simplification rules
the simplificationsubsystem is formulatedwith a mechanism of loop handling in mind  which requires keeping track of literal sequences where each     is in the goal formula resulted from rewriting . two central mechanisms in formalizing goal rewrite systems are rewrite chains and contexts.
rewrite chain: suppose a literal is written by its definition where or . then  each literal in the derived goal is generated in order to prove . this ancestor-descendant relation is denoted . a sequence is then called a rewrite chain  abbreviated as . notice that it is essential here that any goal be in the form of a signed goal  and that when is in   we have that but not .
context: a rewrite chain
records a set of literals for proving . we will write and call a context.
for simplicity  we assume that whenever is generated  it is automatically replaced by   where is the set of literals on the correspondingrewrite chain  and is automatically replaced by .
　note that for every literal in any derived goal  the rewrite chain leading to it from a literal in the given goal is uniquely determined. as an example  suppose the completion of a program is:   . we then have a rewrite sequence . for the three literals in the last goal  we have the following rewrite chains for them from :     and .
simplification rules:

	let	's be any goal formulas 	a context  and	a literal.
 sr1. sr1'. sr1. sr1'.
sr1.	if	is consistentsr1.ifis inconsistentsr1.ifsr1'.if sr1. sr1'.
　the simplification system is a nondeterministic transformation system. the primed version of a rule is its symmetric case. most of the rules are about the logical equivalence between the two sides of a rule. sr1 merges two contexts if they are consistent  otherwise sr1 makes it a failure to prove. sr1 and sr1' prevent generating an inconsistent context before literal is even proved.
　note that the proof-theoretic meaning of a goal formula may not be the same as the logical meaning of the formula. e.g.  the goal formula  a tautology in classic logic  could well lead to an if neither nor can be proved.
　for goal rewriting that does not involve loops  the system described so far is sufficient.
example 1 let	be
	then	is:
the rewrite sequence below is generated by focusing on the left part of a goal.
where
%
1	loop rules
after a literal is rewritten  it is possible that at some later stage either or appears again in a goal on the same rewrite chain. thus  a loop is a rewrite chain such that
　　　  or . a loop analysis involves classifying all the cases of loops  and for each one  determining the outcome of a rewrite according the underlying semantics. for the problem at hand  there are only four cases.
definition 1 letbe a rewrite chain.	if	or  then	is called an odd loop.	if	  then
- is called a positive loop if and are both atoms and each literal on is also an atom;
- is called a negative loop if and are both negative literals and each literal on is also negative;
- otherwise 	is called an even loop.
in all the cases above 	is called a loop literal.
　note that when   and all of     have the same sign  this sign is either positive or negative  though both types of loops are caused by  positive loops  in the given program. these two types of loops must be treated differently according to the semantics.
　it turns out that we only need two rewrite rules to handle all four cases.
loop rules: let	be a rewrite chain.

lr1.
if   for some   is a positive loop or an odd loop.
lr1.
if   for some   is a negative loop or an even loop.
　apparently  a loop literal should always be rewritten by a loop rule.
example 1 . below  is proved and is not.
	. both	and
can be proved.
	. neither	nor	is
proved:
1	goal rewrite systems and their properties
to summarize  a rewrite sequence is a sequence of zero or more rewrite steps  denoted   such that is an initial goal  one involving no context   and
for each	 	is obtained from	by
literal rewriting at a non-loop literal in	; or applying a simplification rule to a subformula in	; or applying a loop rule to a loop literal in	.
　we may call a subsequence a rewrite sequence in the understanding that it is part of some rewrite sequence from an initial goal .
definition 1 a goal rewrite system is a triple
   where is the set of all goals  is a set of rewrite rules which consists of program rules from   the simplification rules  and the loop rules; and is the set of all rewrite sequences.
　goal rewrite systems are like term rewrite systems  see  e.g.   dershowitz and jouannaud  1   everywhere except at terminating steps: a terminating step at a subgoal may depend on the history of rewriting.
　two desirable properties of rewrite systems are the properties of termination and confluence. rewrite systems that possess both of these properties are called canonical systems. a canonical system guarantees that the final result of rewriting from any given goal is unique  independent of any order of rewriting. it therefore allows an implementation to be based on any particular order of rewriting.
　since the simplification system is terminating and literal rewriting only generates non-repeated rewrite chains  it is clear that a goal rewrite system is terminating when the given program is finite. a goal is called a normal form if it cannot be rewritten by any rule.
proposition 1 let	be a goal rewrite system.
if	is finite then every rewrite sequence in	is finite. further  for any rewrite sequence	  if	is a normal form  then either	or	for some	.
definition 1 a goal rewrite system is confluent iff for any rewrite sequences and   there exist and rewrite sequences and .
theorem 1 any goal rewrite system with a finite is confluent.
　the goal rewrite systems described here are sound and complete w.r.t. partial stable models. these results are special cases of those for the rewrite systems for abduction given in the next section.
1	rewrite systems for abduction
let be a program and a set of abducibles. an abducible literals is either an abducible or its negative counterpart .
　the rewriting framework that we defined earlier can be extended for abduction in a straightforward way: the only difference in the extended framework is that we do not apply the clark completion to abducibles. that is  once an abducible appears in a goal  it will remain there unless it is eliminated by the simplification rule or .
　just like a rewrite to is written as   where is the underlying rewrite chain  cf. section 1   a rewrite to an abducible literal will be written as for rewrite chain .
	in the following we shall denote by	the
rewrite system obtained by the logic program and the set of abducibles. we shall show that it is both sound and complete w.r.t. the partial stable models semantics  przymusinski  1   which is a three-valued generalization of answer set semantics. an answer set of is also its partial stable model. but sometimes may not have any answer sets. for instance  if   then has no answer set  because there is no way to assign a truth value to . however  has a partial stable model in which is true and is undefined. the following definition is adopted from  you and yuan  1 .
　let be a  ground  program. for any set of default negations  let	  where is the standard propositional derivation relation with each default negation being treated as a named atom  . a partial stable model of is defined by a maximal fixpoint of the function that applies twice    while satisfying   in the following way: for any atom
 	if	 	if	  and	is
undefined otherwise. notationally  any	such that
and represents that is undefined in . an answer set  also called a stable model  can then be defined as a special case by a fixpoint such that and
.
theorem 1 let be a finite program  a set of propositions  and the goal rewrite system w.r.t. and .
soundness: for any literal	and any rewrite sequence
where each is either an abducible literal or   if is consistent  then there exists a partial stable model of such that .
completeness: for any set of atoms   and any literal in a partial stable model of   there exists a rewrite
sequence
such that	  and	.
　we say that a program has no odd loops if there is no odd loop starting with any literals  programs that have no odd loops are also called call-consistent  dung  1  . it is wellknown that if has no odd loops  then the partial stable models of and the answer sets of coincide. we now show that for any goal  the underlying rewrite system will generate an approximation of a cover for any program   and exactly a cover when has no odd loops.
theorem 1 let	be a goal rewrite system.
suppose	is a proposition and
is a rewrite sequence such that each is either or an abducible literal  and is consistent for each . then  if has no odd loops then
is a cover of . in general  for arbitrary	we have
where	is any cover of .
　consider again the boat example in section 1. the clark completion of is:
since   and are abducibles  so rewriting for terminates in one step  and produces the following cover:
notice that the second explanation is not minimal. to get minimal ones  we have to compute prime implicants of the
disjunction of explanations in the cover  which are and	.
1	related work
traditionally  logic programmingproof procedures have been defined abstractly in terms of derivation and refutation. termination has been considered a separate  implementation issue. on the one hand  this separation is possible since the underlying semantics allows the completeness to be stated without resorting to termination. but completeness is rarely guaranteed in an implementation. on the other hand  the separation is also necessary since these procedures deal with nonground programs for which the problem of loop-checking is undecidable  even for function-free programs  bol et al.  1  . for answer set programming however  loop handling has become a semantic issue: a sound and complete procedure cannot be defined without it. thus  a distinct feature of our work is a mechanism of loop handling both for termination and for the implementation of the underlying semantics.
　completed programs have been used in query answering in abstract  abductive procedures in  console et al.  1; denecker and schreye  1; fung and kowalski  1  for non-ground programs with constraints. these procedures are defined for the three-valued completion semantics under the certainty mode of reasoning - computing bindings for which an  existential  goal is true in all indented models. in our case the reasoning mode is brave - establishing whether a query is true in one of the intended models.
　our work is closely related to another abstract procedure  the eshghi-kowalski procedure  ekp   eshghi and kowalski  1   which is sound and complete for ground programs under finite-failure three-valued stable models  giordano et al.  1 . besides loop handling and termination  to some extent  one can say that our goal rewriting system  grs  simulates ekp in a nontrivial way.
1. grs incurs no backtracking! backtracking is simulated by rewriting disjunctions  e.g.  .
1. loops that go through negation are handled in ekp by nested structures while in grs by a flat structure using rewrite chains.
　these features plus loop handling made it possible to formalize our system as a rewriting system benefiting from the known properties of term rewriting in the literature. this also distinguishes our use of rewrite systems from that by  fung and kowalski  1 . it seems remarkable that a form of nonmonotonic reasoning is just rewriting  two areas of research that had little connection previously.
to illustrate these feature  consider the following program
and the questionwhether we can prove . we may answer this question by the following reasoning: to have we must have
      r1 ; to have we must have either or  r1  which requires having  r1 and r1 . this results in a contradiction. note that in this reasoning we need to remember what was required previously   in this case . this is exactly how the proof is done by grs
however  ekp will go through six nested levels  and do it twice through backtracking  before the same conclusion can be reached.
　rewriting can be applied to function-free programs for proving ground goals. the idea is that if every derived goal is ground  then all the mechanisms given in this paper apply directly. obviously  if for every rule in the given program a variable that appears in the body also appears in the head  then a ground goal will be rewritten to another ground goal. domain restricted programs  niemela：  1  can be instantiated only on domain predicates over variables that do not appear in the head so that the resulting non-ground programs also satisfy this requirement. for example  the program given in the next section is domain restricted.
1	applications and experimental results
we have implemented the writing framework in swi-prolog. in the following  we discuss the performance of our implementation on one particular application of abduction in logic programming  which is the problem of computing successor state axioms from a causal action theory  lin  1 .
　consider a logistics domain in which we have a truck and a package. we know that the truck and the package can each be at only one location at any given time  and that if the package is in the truck  then when the truck moves to a new location  so is the package. suppose that we have the following propositions: ta - the truck is at location initially; pa - the package is at location initially; in - the package is in the truck initially; ta - the truck is at location after the action of moving it from to is performed; pa
- the package is at location after the action of moving the truck from to is performed; and in - the package is in the truck after the action of moving the truck from to is performed. we then have the following logic program:
ta
pa	ta	in ta	ta	taol taol	ta pa	pa	paol paol	pa
	in	in
the first rule is the effect axiom. the second rule is a causal rule which says that if a package is in the truck  then the package should be where the truck is. the rest are frame axioms. for instance  the third one is the frame axiom about ta  with the help of a new predicate taol: if the truck is initially at   and if one cannot prove that it will be elsewhere after the action is performed  then it should still be at .
　as one can see  the above program  when fully instantiated over any given finite set of locations  has no odd loops. so our rewrite system will generate a cover for any query. note that in the program we have omitted domain predicate for each variable in the body of a rule  all the variables in the program refer to locations . thus  the program is domain restricted and needs only to be instantiated for the variable in the fourth and sixth rules over the domain of locations. now let the set of abducibles be the following set:
	in	pa	ta
the	following	table	shows	some	of	the	results	for
:1
queryresulttimetafalse1tatrue1papain1papain	papapa1papain1papain	papapa1papain1painpain	painpain	pa1for instance  the row on pa says that for it to be true  the package must initially be at and cannot be inside the truck  otherwise  it would be moved along with the truck   and the computation took 1 seconds.the row on pa says that for it to be true  either the package was initially at or it was inside the truck. the outputs for larger s are

1
on a piii 1ghz pc with 1mb ram running swi-prolog
1.1.
similar. the performance varies for different queries. for simple queries like ta   their covers can be computed almost in constant time. the hardest one is for pa which took 1 minutes when .
　it is interesting to compare our system with an alternative for computing the cover of a query. as we mentioned in section 1  the set of abductive explanations according to kakas and mancarella is actually a cover. one way of computing these abductive explanations is to add  for each proposition   the following two clauses   satoh and iwayama 
1  : and into the original program  and use the fact that there will be a one to one correspondence between abductive explanations of under the original program and answer sets of the new program that contain . so one can use an answer set generator  for example smodel or dlv  to compute a cover of query by generating all the answer sets in the new program that contain the query. however  the problem here is that there are too many such answer sets in this case. for instance  suppose there are locations  then the number of answer sets that contain any particular query is in the order of   roughly one half of the number of complete hypotheses  even for a very simple query like ta . we do not know at the moment if there is any efficient way of using an answer set generator to compute a cover set of a query.
1	future work
there are several directions for extending this work. one of them is to consider rewriting for non-ground logic programs for some restricted yet decidable classes of non-groundgoals. another important one is to extend this to programs with constraints of the form:
our new definition of abduction can be extended to include these constraints straightforwardly. the challenge is in extending our rewriting system accordingly.
1	acknowledgements
we would like to thank ilkka niemela： for helpful discussions related to the topics of this paper  and ken satoh for comments on earlier version of this paper. the first author's work was supported in part by the research grants council of hong kong under competitive earmarked research grant hkust1e. the work by the second author was carried out mainly during his visits to hong kong university of science and technology and the institute of information science  academia sinica in taiwan.
