 
this paper presents a statistical approach to collaborative filtering and investigates the use of latent class models for predicting individual choices and preferences based on observed preference behavior. two models are discussed and compared: the aspect model  a probabilistic latent space model which models individual preferences as a convex combination of preference factors  and the two-sided clustering model  which simultaneously partitions persons and objects into clusters. we present em algorithms for different variants of the aspect model and derive an approximate em algorithm based on a variational principle for the two-sided clustering model. the benefits of the different models are experimentally investigated on a large movie data set. 
1 	introduction 
the rapid growth of digital data repositories and the overwhelming supply of on-line information provided by today's communication networks bears the risk of constant information overload. information filtering refers to the general problem of separating useful and important information from nuisance data. in order to support individuals with possibly different preferences  opinions  judgments  and taste  in their quest for information  an automated filtering system has to take into account the diversity of preferences and the relativity of information value. one commonly distinguishes between  at least  two major approaches  resnik et a/.  1 :  i  contentbased filtering organizes information based on properties 
of the object of preference or the carrier of information such as a text document  while  ii  collaborative filtering  goldberg et al.  1   or social filtering  aims at exploiting preference behavior and qualities of other persons in speculating about the preferences of a particular individual. 
1 	machine learning 
1 	information filtering 
most information filtering systems have been designed for a particular application domain  and a large fraction of the research in this area deals with problems of system architecture and interface design. in contrast  this paper will take a more abstract viewpoint in order to clarify some of the statistical foundations of collaborative filtering. in particular  the presupposition is made that no external knowledge beyond the observed preference or selection behavior is available  neither about properties of the objects  such as documents  books  messages  cds  movies  etc.  nor about the involved persons  such as computer users  customers  cineasts  etc. . this working hypothesis is not as unrealistic as it may seem on first sight since  for example  many computer systems which interact with humans over the web do not collect much personal data for reasons of privacy or to avoid timeconsuming questionnaires. the same is often true for properties of objects where it is sometimes difficult to explicitly determine those properties that make it relevant to a particular person. moreover  one might integrate information from both sources in a second step  e.g.  by deriving prior probabilities from person/object features and then updating predictions in the light of observed choices and preferences. 
1 	dyadic data 
we thus consider the following formal setting: given are j and a set of objects 
we assume that observations are 
available for person/object pairs  x y   where  and  this setting has been called dyadic data in  hof-
mann et al  1 . in the simplest case  an observation will just be the co-occurrence of x and y  representing events like  person x buys product y  or  person x participates in y . other cases may also provide some additional preference value v with an observation. here  we will only consider the simplest case  where corresponds to either a negative or a positive example of preference  modeling events like  person x likes/dislikes object y . 
　two fundamental learning problems have to be addressed:  i  probabilistic modeling and  ii  structure dis-

covery. as we will argue  different statistical models are suitable for either task. the aspect model presented in section 1 is most appropriate for prediction and recommendation  while the two-sided clustering model introduced in section 1 pursues the goal of identifying meaningful groups or clusters of persons and objects. all discussed models belong to the family of mixture models  i.e.  they can be represented as latent variable models with discrete latent variables. the main motivation 
behind the introduction of latent variables in the context of filtering is to explain the observed preferences by some smaller number of  typical  preference patterns which are assumed to underly the data generation process. in probabilistic modeling  this is mainly an attempt to overcome the omnipresent problem of data sparseness. models with a reduced number of parameters will in general require less data to achieve a given accuracy and are less sensitive to overfitting. in addition  one might also be interested in the structural information captured by the latent variables  for example  about groups of people and clusters of objects. 
1 	the aspect m o d e l 
1 	model specification 
in the aspect model  hofmann et al.  1   a latent class 
variable is associated with each ob-
servation  x  y . the key assumption made is that x and y are independent  conditioned on z. the probability model can thus simply be written as 
where 	are class-conditional multi-
nominal distributions and p z  are the class prior probabilities. notice that the model is perfectly symmetric with respect to the entities x and y. yet  one may also re-parameterize the model in an asymmetric manner  e.g.  by using the identity 
a dual formulation can be obtained by reversing the role of x and y. eq.  1  is intuitively more appealing than 
 1  since it explicitly states that conditional probabilities are modeled as a convex combination of aspects or factors in the case of collaborative filtering  this implies that the preference or selection behavior of a person is modeled by a combination of typical preference patterns  represented by a distribution over ob-
jects. notice that it is neither assumed that persons form 'groups'  nor is stipulated that objects can be partitioned into 'clusters'. this offers a high degree of flexibility in modeling preference behavior: persons may have a multitude of different interests  some of which they might share with some people  some with others  a fact which can be expressed perfectly well in the aspect model. it is also often the case that objects are selected by different people for different reasons. in this case  one might have a number of aspects with high probability p y z  for a particular object y. 
1 	model fitting by em 
the standard procedure for maximum likelihood estimation in latent variable models is the expectation maximization  em  algorithm  dempster et al.  1 . em alternates two steps:  i  an expectation  e  step where posterior probabilities are computed for the latent variables z  based on the current estimates of the parameters   ii  an maximization  m  step  where parameters are updated for given posterior probabilities computed in the previous e-step. 
　for the aspect model in the symmetric parameterization bayes' rule yields the e-step 

by standard calculations one arrives at the following mstep re-estimation equations 

where n x  y  denotes the number of times the pair  x  y  has been observed. alternating  1  with  1  and  1  defines a convergent procedure that approaches a local maximum of the log-likelihood. 
　implicit in the above derivation is a multinomial sampling model  which in particular implies the possibility of multiple observations. this may or may not be appropriate and one might also consider hypergeometric sampling without replacement  although according to statistical wisdom both models are expected to yield very similar results for large populations. 
1 	extension to preference values 
let us now focus on extending the aspect model to capture additional binary preferences  we distinguish two different cases:  i.  situations where the selection of an object is performed by the person  which then announces her or his preference in retrospect   ii  problems where the selection of y is not part of the behavior to be modeled  for instance because it is controlled or triggered by some other external process. 
　　1 the presented models can be further generalized to handle arbitrary preference values  but this requires to specify an appropriate likelihood function based on assumptions on the preference scale. 
	hofmann and puzicha 	1 


figure 1: graphical model representation of the aspect model  a  and its extensions to model preference values  b - d   case i  and  e   f   case ii . 
case i. in the first case  there are three different ways to integrate the additional random variable v into the model  as shown by figure 1  b - d . in  b  v is conditionally independent of x and y given z  which is a very strong assumption. one implication is that aspects are typically employed to either model positive or nega-
tive preferences. in variant  c  and  d   v also depends on either x or y which offers considerably more flexibility  but also requires to estimate more parameters. it is 
straightforward to modify the em equations appropriately. we show the equations for model  c   the other variants require only minor changes. for the e-step one obtains comparing  1  with  1  one observes that i is now replaced by  since y is treated as a fixed  observation-dependent  conditioning variable. note that by conditioning on both  x and y  one gets  which reveals the asym-
metry introduced into the aspect model by replacing one of the class-conditional multinomials with a vector of bernoulli probabilities. the presented version is the 
 collaborative  model. reversing the role of x and y yields the dual counterpart in figure 1  f   where the prediction of v depends directly on x and only indirectly 
on y  through z . again combining both type of dependency structures in a multinet might be worth consider-ing. 
	1 	t h e two-sided clustering m o d e l 
	1 	model specification 
   in the two-sided clustering model the strong assumpwhere n x  y  v  denotes the number of times a particu- tion is made that each person belongs to exactly one lar preference has been observed  typically n x y  v  group of persons and that each object belongs to exactly 
{1} . from p y  v z  one may also derive . and one group of objects. hence we have latent mappings p v y  z   if necessary. the m-step equation for p{x z  
does not change. effectively the state space of y has been enlarged to 
　notice that one might also consider to combine the model variants in figure 1 by making different conditional independence assumptions for different values of z. the resulting combined model corresponds to a bayesian multinet  geiger and heckerman  1 . 
case i i . in the second case  the multinomial sampling model of selecting y or a  y  v  pair conditioned on z is no 
longer adequate. we thus propose a modification of the aspect model starting from  1  and replace multinomials p y z  with bernoulli probabilities p v y  z   assuming that y is always conditioned on  cf. figure 1  e  . this modification results in the e-step which partition x into k groups and y into l groups  respectively. this is very different in spirit from the aspect model  where the leitmotif was to use convex combinations of prototypical factors. while we expect the clustering model to be less flexible in modeling prefer-
ences and less accurate in prediction  a fact which could be verified empirically   it might nevertheless be a valuable model for structure discovery which has applications 
of its own right. to formalize the model  let us introduce the following sets of parameters: p x  and p y  for the marginal.probabilities of persons and objects  p c  and p d  for the prior probabilities of assigning persons/objects to the different clusters  and  most importantly  cluster association parameters between pairs of cluster  c d . now we may define a probabilistic model by 
a factorial prior on the latent class variables 
1 	machine learning 

star trek iv 1 
star trek.ii 1 
star trek vi 1 
star trek iii 1 
the fifth element 1 dr. strangeiove 1 
a clockwork orange 1 
delicatessen 1 
cinema paradiso 1 
brazil 1 pinocchio 1 
the aristocats 1 
snow white and the seven dwarfs 1 
the jungle book 1 
the lion king 1 richard iii 1 
les miserables 1 
the madness of king george 1 
in the name of the father 1 
the visitors  les visiteurs  1 the rock 1 
eraser 1 
independence day  id1  1 
mission: impossible 1 
trainspotting 1 the piano 1 
the remains of the day 1 
in the name of the father 1 
forrest gump 1 
shadowlands 1 ready to wear 1 
what's love got to do with it  1 
circle of friends 1 
dolores claiborne 1 
when a man loves a woman: 1 como agua para chocolate 1 i 
three colors: red: 1 
three colors: blue: 1 
three colors: white: 1 
the piano: 1 figure 1: movie aspects extracted from eachmovie along with the probabilities 

completes the specification of the model. the association parameters  increase or decrease the probability of observing a person/object pair  x y  with associated cluster pair  c  d  relative to the unconditional independence model p x y  = p x p y . in order for  1  to define a proper probabilistic model  we have to ensure a correct global normalization  which constrains the choice of admissible values for the association parameters 
1 	variational em for model fitting 
the main difficulty in the two-sided clustering model is the coupling between the latent mappings c z  and d y  via the cluster association parameters an additional problem is that the admissible range of also depends on c x  and d y . since an exact em algorithm seems to be out of reach  we propose an approximate em procedure. first  since c x  and d y  are random variables we define the admissible range of  to be the set of values for which 

where the expectation is taken w.r.t. the posterior class probabilities where the q distributions are free parameters to be determined. in the  approximate  m-step one has to maximize  hofmann and puzicha  1  
technically  one introduces a la-
grange multiplier to enforce  1  and after some rather are marginals of the posteriors 
and n x   n y  are marginal counts. eq.  1  can be given a very intuitive interpretation by considering the hard clustering case of  where it reduces to the expected mutual information between pairs of classes c and d in either spaces: the numerator in  1  then simply counts the number of times a person x belonging to a particular cluster c has been observed in conjunction with an object y from cluster d  while the denominator reduces to the product of the probabilities to  independently  observe a person from cluster c and an object from d. 
　it remains to perform the variational approximation and to determine values for the q-distributions by choosing q in order to minimize the kl-divergence to the true posterior distribution. details on this method also known as mean-field approximation - can be found in  jordan et a/.  1; hofmann and puzicha  1 . for brevity  we report only the final form of the variational e-step equations: 
notice that these equations form a highly non-linear  coupled system of transcendental equations. a solution is found by a fixed-point iteration which alternates the computation of the latent variables in one space  or more precisely their approximate posterior probabilities  based on the intermediate solution in the other space  and vice versa. however  the alternating computation 
	hofmann and puzicha 	1 

four weddings and a funeral 
home alone 
sleepless in seattle 
dave 
pretty woman 
the piano apollo 1 
batman 
batman forever 
star trek: generations 
stargate 
goldeneye e.t.: the extraterrestrial 
alice in wonderland 
cinderella 
old yeller 
mary poppins 
the fox and the hound m*a*s*h 
full metal jacket 
the bridge on the river kwai 
apocalypse now 
chinatown 
the shining 	kalifornia 	!
short cuts 
smoke 
red rock west 
romeo is bleeding 
crumb figure 1: movie clusters extracted from eachmovie. 

has to be interleaved with a re-computation of the parameters  because certain term cancelations have been exploited in the derivation of  1 . the resulting alternation scheme optimizes a common objective function and always maintains a valid probability distribution. to initialize the model we propose to perform one-sided clustering  either in the x or the y space. 
1 	clustering with preference values 
like the basic aspect model  the two-sided clustering model is based on multinomial sampling  i.e.  it models independently generated occurrences of  x  y  pairs. to model preference values v conditioned on  x  y  pairs  we modify the model by replacing the association parameters with bernoulli parameters 

likelihood under the posterior distribution of the latent mappings c x  and d{y  which yields the formula 

in the hard clustering limit  this simplifies to counts of 
how many persons in cluster c like  or dis-
like  v = -1  objects from cluster d  ignoring missing values . the denominator then corresponds to the total number of votes available between x's belonging to c and y's belonging to d. a factorial approximation of 
　　1  refined models may also consider additional weights to account for individual preference averages. 
1 	machine learning 
k l  aspect co-occ.  a  cluster co-occ.  aspect pref.  d  1  
1  
1  1  
1  1  
1  
1  1 
1 
1 
1  1  
1  1  
1  1  1 
1 
1 
1 
1  
1  1 
1 
1 
1  1  
1  1  
1  1  table 1: perplexity results on eachmovie for different model types  columns  and different model complexities  rows . 
these equations are very similar to the ones derived in  ungar and foster  1 . the clustering model they present is identical to the bernoulli model in  1   but the authors propose gibbs sampling for model fitting  while we have voted for the computationally much faster variational em algorithm.1 
1 	experimental results 
to demonstrate the utility of latent class models for collaborative filtering  we have performed a series of experiments with the eachmovie dataset which consists of data collected on the internet  almost 1 million preference votes on a 1 scale which we have converted to  preferences by thresholding .1 table 1. summa-
1
    for example  on the eachmovie database used in the experiments we were not able to train models with gibbs sampling because of the immense computational complexity. 
1
    	for 	more 	information 	on 	this 	dataset 	see www/research.digital.com/src/eachmovie. 

	data 	1 recommendations data 1 recommendations star trek: the motion picture 
star trek: generations 
star trek ii 
| 	star trek iii 
star trek v 
star trek vi the empire strikes back 
star trek: first contact 
raiders of the lost ark 
	stargate 	j
the terminator 
return of the jedi pulp fiction 
fargo 
smoke 
 	three colors: blue 
four weddings and a funeral 
a1: a space odyssey the silence of the lambs 
toy story 
dead man walking 
batman 
leaving las vegas 
the piano figure 1: two exemplary recommendations computed with an aspect model  k = 1 . 

rizes perplexity results1 obtained with the aspect model and the two-sided clustering model for different number of latent classes. as expected the performance of the aspect model is significantly better than the one obtained with the clustering model. the aspect model achieves a reduction of roughly a factor 1 over the marginal independence model  baseline at k = 1 . by using annealing techniques  cf.  hofmann and puzicha  1   slightly better results can be obtained  numbers in brackets . 
　to give an impression of what the extracted movie aspects and movie clusters look like  we have displayed some aspects of a k = 1 model in figure 1 and clusters of a k = l = 1 solution represented by their members with highest posterior probability in figure 1. notice that some movies appear more than once in the aspects  e.g. 'the piano' . both authors have also been subjected to a test run with the recommendation system. the result - which was perfectly satisfying from our point of view - is shown in figure 1. we hope the reader might also spot one or another valuable recommendation. 
1 	conclusion 
we have systematically discussed two different types of latent class models which can be utilized for collaborative filtering. several variants corresponding to different sampling scenarios and/or different modeling goals have been presented  emphasizing the flexibility and richness of latent class models for both  prediction and structure discovery. future work will address alternative loss functions and will have to deal with a more detailed performance evaluation. 
acknowledgments 
this work has been supported by a daad postdoctoral fellowship  th  and the german research foundation  dfg  under grant bu 1-1  jp . the eachmovie 
　　1 the perplexity v is the log-scale average of the inverse probability 1/p y x  on test data. 
preference data is by courtesy of digital equipment corporation and was generously provided by paul mcjones. 
