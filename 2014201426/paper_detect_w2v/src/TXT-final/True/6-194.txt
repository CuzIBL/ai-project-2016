 
in this paper we propose a generalisation of the k-nearest neighbour  k-nn  retrieval method based on an error function using distance metrics in the solution and problem space. it is an interpolate method which is proposed to be effective for sparse case bases. the method applies equally to nominal  continuous and mixed domains  and does not depend upon an embedding n-dimensional space. in continuous euclidean problem domains  the method is shown to be a generalisation of the shepard's interpolation method. we term the retrieval algorithm the generalised shepurd nearest neighbour  gsnn  method. a novel aspect of gsnn is that it provides a general method for interpolation over nominal solution domains. the performance of the retrieval method is examined with reference to the iris classification problem  and to a simulated sparse nominal value test problem. the introduction of a solution-space metric is shown to out-perform conventional nearest neighbours methods on sparse case bases. 
1 introduction 
we present in this study a case-based reasoning  cbr  retrieval method that utilises a distance metric imposed on solution space. the motivation for such a method is to extend a powerful interpolative method  already proven in the real domain  so that it applies equally in the domain of nominal values. interpolative methods are well studied in the real domain  and can give good results from relatively sparse datasets. however  no general interpolative method exists for nominal  discrete  solution domains. 
1 	an error function 
the gsnn method will be applied to a general class of problem and solution domains. a distance metric is here defined on the problem domain x and on the solution domain y. for the problem 
space  the term in the shepard's method  is generalised to over x. for the solution space is used where is the value of y which minimizes the error function: 

　that this is a minimum follows from the positive definite form:  
　the function depends only upon the euclidean distance over y = r and x = in order to generalise the method completely  we propose the error function: 
a  
　here  the set are the k nearest neighbours in the problem space to the point x. and are distance on domains the retrieved value y is the value which minimizes the error function /. the gsnn algorithm is given as follows: 

1 	illustrative example 
in this example we illustrate in detail how the method works. we choose the iris data set  fisher  1 . the problem space is x and x =  is a point in x. 
the solution space y= {setosa  versicolour  virginica}. for the problem space we define distance according to a 

* the support by the university of greenwich and tunku ab-

weighted sum of attributes. for the y space  we define  by using the distances between cluster centres to 
represent the distance between the classes. these distances are shown in the following matrix: 

dul rahman college  tarc  is acknowledged. 
poster papers 	1 

	setosa versicolour 	virginica 
　setosa 1 .1 .1 versicolour .1 .1 virginica .1 .1 
we take two cases  one from setosa and one from virginica: xj=  1.1  1  1   y = setosa x1 =  1  1  1  1   y1 = virginica we take as target the versicolour iris: x =  1  1  1  1  y =   
taking p=l and k = 1  the function l y  is: 
since i  versicolour  is minimum  we take as the estimated value. this example shows an advantage of the interpolation method in situations where cases are sparse  in that it can correctly predict nominal values not represented in the case base itself. 
1 	test on a simulated case base 
to examine how the gsnn method might work on real case bases  we simulated case bases of varying density and structure  and used the method to estimate simulated target sets. as a basis for the simulation  we adapted the function used by ramos and enright   i.e. 
 to give 1 nominal values  
yi ... y1i. these 1 nominal values inherited a distance metric from the numeric values:  
　test 1 uses regularly spaced cases at various case densities. this might represent a well organised case base. test 1 uses randomly selected cases  and is intended to represent disorganised sparse case bases. cases are constructed in the domain:  over a regular square lattice  with 1 1 points. 
  size methods k=l k=1 k=1 k=1 1 gsnn 1 1 1 1 k-nn 1 1 1 dwnn 1 1 1 1 gsnn 1 1 1 1 k-nn 1 1 1 dwnn 1 1 1 1 gsnn 1 1 1 1 k-nn 1 1 1 dwnn 1 1 1 table 1. errors in estimating a test set of 1 targets  for regular case bases. 
table 1 shows the result of test 1. these results confirm that gsnn with k   1 can out-perform both k-nn and dwnn  mitchell  1  for case bases with regular structure. table 1 shows the results of test 1. the results show that more errors are recorded for random case bases than for regular case bases of equivalent size  whatever the value of k. once again  the results show that gsnn out-performed the other nearest neighbour methods. 
  size methods k=l k=1 k=1 k=1 1 gsnn 1 1 1 1 k-nn 1 1 1 dwnn 1 1 1 1 gsnn 1 1 1 1 k-nn 1 1 1 dwnn 1 1 1 1 gsnn 1 1 1 1 k-nn 1 1 1 dwnn 1 1 1 table 1. errors in estimating a test set of 1 targets  for random case bases. 
1 	conclusion 
in this paper  we have proposed a method for interpolation over nominal values. the method generalises the shepard's interpolation method by expressing it in terms of the minimization of a function i y . this function relies only on distance metrics defined over problem and solution spaces. the method has an advantage for cbr in that it is applicable to case bases with nominal values in the problem and solution domain where no natural ordering exists. the examples studied indicate that gsnn could be useful in cbr with a sparse set of cases  and particularly where the cases can be organised. tests show that gsnn is more efficient as a retrieval engine than other nearest neighbour methods. the inclusion of a solution space metric in the gsnn technique could be useful in two areas of cbr:  i  the selection of an optimum case base   ii  case based model building  from experimental or numerical modeling exercises. investigations using numerical models indicate at gsnn would appear to be a promising approach for the construction of efficient case-based models. 
