 	1. the kbms architecture 

　　we define and discuss an architecture for a knowledge base management system  kbms  to support the efficient manipulation of large and diverse bodies of knowledge. the architecture has been formulated within an amalgamated language-metalanguage framework. all aspects of the system possess a logical semantics. our emphasis is on modularity  the organization of theories and their metatheories within the system theory  and the clean integration of multiple representation languages. the expressivity of the architecture is discussed and an implementation based on an extended prolog reported. its utility is currently being investigated in a number of diverse applications. 
1. introduction 
　　the work reported here began two years ago with the aim of addressing the problems of complexity and efficiency that arise in the management of large and diverse bodies of knowledge. logic programming is our starting point. although prolog is the most mature logic programming language  it has a number of shortcomings that make it unsuitable in its current form as the basis for a kbms. we have attempted to overcome these failings. 
　　what are the attributes of a kbms that we believe are important  it must: 
o possess simple semantics o support and integrate multiple representations o encourage reusability of knowledge o support a logical model of updating o efficiently handle very large amounts of data o provide a general basis for a range of flexible and powerful tools 
o subsume rdbms capabilities 
　　these attributes are further discussed in  sharpe ef a/ 1 . the key to overcoming the problems with prolog is using the horn clause language as its own metalanguage - the strict separation of logic from metalogic expressed in an amalgamated language and metalanguage  bowen & kowalski 1 . however  this amalgamation merely provides a framework within which we must define a kbms architecture that meets our stated objectives. 
　　our architecture incorporates theories as first-class data-objects which provide the basis for the representation  storage  retrieval and updating of knowledge. the kbms provides an organization of theories that controls their access and interactions. 
　　in our view a knowledge base comprises not only the data but also the metadata - in other words knowledge about how to handle that data. all this knowledge is partitioned into theories and organized within a single system. theories are referred to via terms that act as names for those theories. there are two basic interfaces to named theories: proving and updating. the former is used for handling all queries  and the latter for handling all updates. 
　　the representation of knowledge within a named theory is regarded as hidden. the theory is effectively a black box with only its metatheories  interpreters and assimilators  having direct access to the contents of the theory. as a consequence of this structuring  multiple representations of knowledge within a single knowledge base is possible. integration of the separate theories is achieved by allowing variables of a query in a named theory to be bound by the interpreter of that theory to terms within the theory. we therefore require a global convention across the whole knowledge base for the meaning of such terms. 
1 the system 
　　in our architecture  the entire knowledge base is represented by a distinguished theory known as the system. this theory defines the relationship between names of theories  the theories themselves and the names of their metatheories  interpreters  assimilators and attributes . whenever a query is made in a named theory that theory must be retrieved from the system - performed by querying the system. the relationships between names and theories is expressed using the full power of horn clauses  thereby enabling the representation of complex many-to-many relationships. 
　　since the system is itself a theory  it can easily be extended by including extra axioms to define  say  a variety of methods for retrieving named theories and choosing any metatheories required. another effect of this architectural decision is that updates to the system are handled like updates to any other theory. consequently  a new system results from any update. potentially access to all previous versions of the system is possible. 
1 theories 
　　theories are built from bags of horn clauses. unit clauses can readily support a variety of representation languages. of course  if queries are to be handled for a given representation then an interpreter must be defined. every clause in a theory has a unique identifier for possible use in metatheories. am theories in the system have associated with them three sets of metatheories: interpreters  assimilators  and attributes. each of these sets contains one or more members but if there is more than one member then inference within the system is needed to choose between them. all metatheories  
	black and manley 	1 
being theories  similarly possess three associated sets. however  to avoid the apparent infinite regress  recursion in each of the three sets quickly terminates with built-in metatheones. for instance  efficiency considerations mean that more than two levels of interpretation are rarely used in practice. the built-in interpreter is prolog. 
1 interpretation 
　　all theories are queried via a standard predicate interface. this interface takes a theory name and a set of goals: kbms prove system theoryname goals . the system is consulted to find the current version of the named theory and the name of its interpreter. if there is more than one interpreter for the named theory then inference within the system is used to choose between them. the kbms then attempts to prove that the interpreter can solve the goals. this causes a recursion - due to the need to find an interpreter for the interpreter - terminated by the built-in interpreter. 
　　a theory's interpreter may be one of the standard theorem-provers  but  as is well-known  uniform proof procedures are very inefficient  bundy et al 1 . it is desirable to make use of domain-specific control in order to direct the proof especially when large bodies of knowledge are involved. by allowing each theory to have its own interpreters we are able to carry out theory-specific inference control. inference control can make use of both syntactic and semantic constraints  for example type information and the number of sub-goals in a candidate clause  gallaire & lasserre 1  bundy et al 1 . the primary use of attribute theories is to allow particular theory-dependent information to be abstracted out of interpreters and assimilators thereby making them theory-independent and hence reusable. non-standard interpreters have to be user-supplied; they have to be accessible to the querying interface and  understand   supply a semantics for  the representation of the theories for which they act as theorem-provers. interpreters have the full power of the horn clause metalanguage at their disposal and they operate on theories via a number of lower-level  built-in kbms primitives. theory-to-theory linkages are obtained by including goals in one theory that require a proof in another named theory. 
1 assimilation 
　　all assimilation proceeds via a standard interface: kbms update  system .theory name.updates new system . in a similar fashion to interpretation  all assimilators must understand  and translate into  the representation of the theory they are updating. updates are specified as a list of actions all actions are eventually reduced into primitive  declarative add and delete operations on the theory. the simplest form of assimilator is one that merely carries out all additions and deletions without any kind of check. alternatively the assimilator can enforce integrity constraints. there are two basic approaches to updating under constraints: to carry out a 
　　list of actions  checking on the consistency of the final theory; to carry out the whole set of relevant checks after the addition  or deletion  of each clause. the former method has the advantage of ignoring unavoidably inconsistent transitional states of the theory. if an illegal update is attempted  the assimilation fails and all related updates will be undone during backtracking. the checks advocated by kowalski  kowalski 1  can be incorporated into an assimilator  but they prove to be very expensive. a second set of checks might be called domain-dependent  syntactic and semantic integrity constraints. under this heading one might place constraints such as: predicate p must always be binary; the second argument of predicate p must always be less than 1. as with interpreters this type of theory-dependent information can 
1 	architectures and languages 
be abstracted out into the theory's attribute theory thereby making the assimilator applicable to more than one theory. 
　　the update model used is completely declarative. this means that any update  addition or deletion  to a theory results in a new theory. both the previous version and the new version are accessible as long as references to both still exist. this can be achieved with very little time and space overhead. since the system in some sense contains all theories  an update to a theory is also an update to the system. as the system is also a theory  a new system is created whenever any update occurs. again  all previous versions of the system are available as long as they are still referenced. 
　　the system also has an assimilator. a typical task might be to check that when adding a new theory to the system all of its nominated metatheories also belong to that system. 
1. an implementation 
　　we have implemented a kbms  kbmso  adhering to the principles stated earlier. it possesses all the functionality described above and was built from an extended prolog developed to meet our requirements. the extensions provide the theory as a first-class data-object supporting declarative operations. all of these operations are backtrackable leaving the state of the kbms unchanged. in implementing declarative updating  our design decision was to maximize the speed of accessing versions of a theory at the expense of space. however updates to the most recent version of a theory are handled incrementally with the minimum of space overhead as a single theory datastructure contains all such incrementally produced versions. when an update is made to a version which is no longer the current one copying occurs. 
　　the execution speed of querying horn clauses stored in theories  using prolog as their interpreter  runs at about two-thirds the speed of the native prolog system. there is no space overhead  and little time overhead  incurred in querying previous versions of a theory. 
1. expressivity and limitations 
　　there are several techniques that have been reported as being important in the management of large knowledge bases  bowen 1  kauffman & qrumbach 1  furukawa et al 1 . we believe that these are really secondary to the fundamental problem of organizing information into a rigorous logic-metalogic framework. with the right architecture  they all become artefacts of particular representation methodologies or specialized logic programming devices. 
1 inheritance 
　　inheritance has been put forward as a primary structuring concept for the design of modularized knowledge bases. it has also been used to tackle problems of representation  such as in default reasoning . it is undoubtedly useful in certain cases  but it is an imprecise term with many interpretations. in introducing any form of inheritance into our architecture we must not violate our basic design philosophy of named theories behaving as private queriable bodies of knowledge. this precludes any assumptions being made about the representation of such theories. direct inheritance of clauses of one theory by another is therefore not allowed since it would violate this principle for two reasons: 
1  it would create a reliance on understanding the representation of clauses in other inherited theories; 

1  a theory's assimilator would no longer have absolute control over the contents of that theory and would be unable to enforce any associated integrity constraints. 
　　even when two theories have a common representation  semantic inconsistencies can still arise between the theories. the situation is even worse when non-unit clauses are used. trying to overcome these problems by defining schemes for selectively overriding predicates is unsatisfactory: such techniques do not scale up for use in large  diverse knowledge bases. in spite of this other metalanguage-based architectures have placed a strong emphasis on provision of inheritance facilities  kauffmann & grumbach 1  furukawa et ai 1 . a factoring of knowledge can be straightforwardly achieved by modularization into named theories  and the use of the kbms prove predicate to achieve explicitly what inheritance achieves implicitly  at the same time adhering to the ideal of black-box theories. 
　　the system allows the definition of arbitrary relationships between names and theories  bounded only by the expressivity of horn clauses. this leads to a flexible method where an inheritance lattice can be readily described. one form of inheritance is merely theory selection where a goal is proved in one member of a set of theories that are related through the inheritance lattice. backtracking on failing to satisfy the goal in one candidate theory causes attempted satisfaction in the next. 
　　another form of inheritance is naturally incorporated into a theory's interpreter. its principal disadvantage is that it admits the possibility of using semantically inconsistent knowledge. when solving a goal  the interpreter tries first to satisfy that goal in the named theory. if this fails  then instead of backtracking to the previous goal  it queries the system to find the name of a theory from which this theory inherits. it then attempts a proof of the current goal in this alternative theory  which of course proceeds via its own interpreter. this process can continue until the goal is satisfied or until the goal has failed in all the pertinent theories in the inheritance lattice backtracking takes care of multiple inheritance. it differs from the proceeding method in that it allows each individual subgoal to be tried in each theory  instead of just the overall goal. implementing it requires only a minor enhancement to a theory's interpreter. note that such a scheme does not violate any of the principles stated above. 
1 set operations on theories 
　　inheritance can be incorporated into a kbms with regard to the principle of theories being treated as black boxes. the same is not so obviously true of set operations on theories. the union of two theories should not simply be constructed by forming a new theory that is the sum of the two sets of clauses - we might be merging two different representations  or even two semantically inconsistent bodies of knowledge. metaprolog  bowen & weinberg 1   however  encourages this formation of theory unions through the provision of a special operator. if we regarded the union operation as being the addition of one theory to another  then clauses of the second could be added to the first via the firsts assimilator. the obvious faults of this method are that the assimilator might not understand the representation of the clauses it is adding  also that the merged theories might still be semantically inconsistent according to constraints in the second theory's assimilator. the same considerations apply to other set operations such as difference. thus it appears that there are no general solutions to this problem. 
1 proof trees 
　　in some cases efficient control of inference requires being able to reason about global aspects of the proof so far - this requires access to a proof tree. however  like set operations  returning proof trees as a result of querying a named theory violates the principle of regarding that theory as a black box. this is because the nature of the proof tree depends heavily on the representation of the theory and the nature of its interpreter. ideally control information should never be passed in or out of queries. it is the sole responsibility of the interpreter to employ whatever techniques it deems necessary to handle a query. if the formation and use of a proof tree is internal to a particular interpreter  no problems can arise. this is contrary to an approach described in metaprolog  bowen & weinberg 1 . 
1 partial evaluation 
　　partial evaluation is a technique whose generality and applicability has been overstated. when there of static information available which affects control of inference  then partial evaluation can be effectively used to remove redundant run-time computation. this gives an elegant model of the process performed by compilation  kahn 1  kursawe 1 . however the power of an interpreter lies in controlling combinatorial explosion. this normally requires dynamic information and hence cannot be evaluated in advance. an example is goal ordering based on instantiation levels. another problem with partial evaluation is that it is time and space consuming. it can be useful in certain restricted situations such as in handling a frequently used class of queries on a relatively static theory. the obvious place in the architecture for a partial evaluator is in the role of an assimilator. 
　　it is possible to incorporate all these aspects into the architecture described. in some cases strongly held ideals will be violated but this is a matter of user responsibility. what is required is a methodology for their use - this is beginning to come from the extensive and varied uses to which the system is currently being put. 
1. summary 
　　we have described the motivation and principles underlying the design of a kbms architecture. all aspects of the system possess a clear logical semantics. the use of an amalgamated language and metalanguage gives rise to considerable expressivity and flexibility in the representation and manipulation of knowledge. a central feature is the modularization of large bodies of knowledge into theories that can be regarded as black boxes. the use of standard query and update interfaces allows the representation of the knowledge to remain hidden. we have implemented a system  kbmso  on top of an extended prolog  and its utility is currently being investigated in domains such as: protein structure determination  co-operative problem solving  commercial database applications and declarative graphics. 
