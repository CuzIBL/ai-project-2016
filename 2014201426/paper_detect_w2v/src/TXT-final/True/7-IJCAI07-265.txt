
we describe a briefing system that learns to predict the contents of reports generated by users who create periodic  weekly  reports as part of their normal activity. the system observes content-selection choices that users make and builds a predictive model that could  for example  be used to generate an initial draft report. using a feature of the interface the system also collects information about potential user-specific features. the system was evaluated under realistic conditions  by collecting data in a project-based university course where student group leaders were tasked with preparing weekly reports for the benefit of the instructors  using the material from individual student reports.
this paper addresses the question of whether data derived from the implicit supervision provided by end-users is robust enough to support not only model parameter tuning but also a form of feature discovery. results indicate that this is the case: system performance improves based on the feedback from user activity. we find that individual learned models  and features  are user-specific  although not completely idiosyncratic. this may suggest that approacheswhich seek to optimize models globally  say over a large corpus of data  may not in fact produce results acceptable to all individuals.
1 introduction
in this paper we describe a personalized learning-based approach to summarization that minimizes the need for learning-expert time and eliminates the need for expertgenerated evaluation materials such as a  gold standard summary  since each user provides their own standard. while the work we describe is specific to summarization  we believe that the basic techniques are extensible to other learning situations that share the characteristic of repeated execution and the availability of unambiguous user feedback.
　of course this comes at a cost  which is the end-user time needed to teach the system how to produce satisfactory summaries. we would however argue that end-user involvement is more likely to generate quality products that reflect both functional needs and user preferences and is indeed worth the effort. the current paper describes the application of this approach in the context of an engineering project class in which students were expected to produce weekly summaries of their work. for the course we worked with  a reporting requirement was already in place. we minimally modified the process  by augmenting and instrumenting the web-based reporting software already in use  to collect relevant data from users; apart from this their normal activities were not perturbed. the data collected was used to perform off-line learning experiments.
　while each student produces their own logs  their team leader was additionally tasked with reading the logs and selecting log entries  referred as 'items' hereon  suitable for a summary of group activity. learning is based on information about which items are selected by the team leader. we further collect data identifying the  features  deemed important by the leader creating the summary. this is done by asking the team leader to highlight key words/phrases in the selected summary items  the phrases that they believe led them to select that item . this might be thought of as having the users directly select summary content units  nenkova and passonneau  1  or identify rouge-like units  n-grams  word sequences   lin  1 . although in this paper we focus on extractive summarization  we believe that this approach can be extended to prime information for abstractive summarization  one of our long-term goals for this work.
　the plan of this paper is as follows. we describe relevant ideas from the literature. we then describe the domain and the report generation tasks performed by the class and give details of the data collected. the learning system section outlines key details  feature representation  weighting  ranking  modelselection etc. . after describingthe basic learning framework  we describe the experiments and results from the different settings of our system followed by the conclusions.
1 related work
although automatic text summarization has been explored for almost a half-century  luhn  1   most current work
focuses on generic newswire summaries  duc  1 1 1 .  radev and mckeown  1  provide a useful distinction between briefings and the general concept of summaries where they focus on generating multidocument newswire briefings.  mani et al.  1  also focus on generating briefings  however their approach contrasts with ours and is in a different domain. they assume that users are given an outline of the briefing and then try to populate the outline  whereas our system does not provide an initial structure  except for a generic template . on the other hand  mani et al.  1  extend their work to multimedia inputs. from our current perspective we believe that the system should focus on identifying important information  allowing the user to concentrate on its organization into a coherent presentation.
　a few personalized interactive learning systems have also been proposed for summarization.  amini  1  describes a query-relevant text summary system based on interactive learning. learning is in the form of query expansion and sentence scoring by classification.  leuski et al.  1  have explored interactive multi-document summarization  where the interaction with the user was in terms of giving the user control oversummary parameters  supportrapid browsingofdocument set and alternative forms of organizing and displaying summaries. their approach of 'content selection' to identify key concepts in unigrams  bigrams and trigrams based on the likelihood ratio  dunning  1  is different from our statistical analysis and is of some interest.  zhang et al.  1  have proposed a personalized summarization system based on the user's annotation. they have presented a good case of the usefulness of user's annotations in getting personalized summaries. however their system differs from the current one in several respects. their scenario is a single document newswire summary and is very differentfrom a briefing. also  their system is purelystatistical and does not include the concept of a human-in-the-loop that improves performance.
　 elhadad et al.  1  have applied personalized summarization in the medical domain where  given a patient profile and a set of documents from a search query  the system generates a personalized summary relevant to the patient. the active learning community  raghavan et al.  1  has also been moving towards using user feedback for identifying important features. some other interesting domains in which summarization systems have been developed are technical chat  zhou and hovy  1   newsgroup conversations newman and blitzer  1   email threads  rambow et al.  1   spoken dialogues  zechner  1  as well as others.
　 garera and rudnicky  1  describe a summarization system for a recurring weekly report-writing taking place in a research project. they found that knowledge-engineeredfeatures lead to the best performance  although this performance is close to that based on n-gram features. given this  it would be desirable to have a procedure that leverages human knowledge to identify high-performance features but does not require the participation of experts in the process. we describe an approach to this problem below.
1 target domain
we identified a domain that while similar in its reporting structure to the one studied by  garera and rudnicky  1  differed in some significant respects. specifically  the report-

figure 1: system demonstration:acquiring user based features via phrase highlighting
writers were not experienced researchers who were asked to generate weekly reports but were students taking a course that already had a requirement for weekly report generation. the course was project-based and taught in a university engineering school. the students  who were divided into groups working on different projects were required to produce a weekly report of their activities  to be submitted to the instructors. each group had well-defined roles  including that of a leader. students in the class logged the time spent on the different activities related to the course. each time-log entry included the following fields: date  category of activity  time spent and details of the activity. the category was selected from a predefined set that included coding  group meeting  research and others  all previously set by the instructors .
　an example time-log entry is: visited the voyager site. spoke with the marine architects. got a feel for the possibilities and scope of the project.1  the data in this domain is not as structured and grammatical as newswire documents. it is also different from chat-like/conversational corpora in its usage of words although it does contain a few spelling mistakes. one important characteristic of the data is that the content of the time-logs changes over time as the project progresses. so the word-based features that may indicate importance in the earlier weeks may not be useful in the later weeks.
　the task of the team leader is to prepare a weekly report for the benefit of the instructor  using the time-log entries of the individual team members as raw material. as the students were already using an on-line system to create their logs  it was relatively straightforward to augment this application to allow the creation of leader summaries. the augmented application providedan interface that allowed the leader to more easily prepare a report and was also instrumented to collect data about their behavior. instrumentation included mouse and keyboard level events  we do not report any analysis of these data in this paper .
1 data collection process
following  garera and rudnicky  1   the leader selected items from a display of all items from the student reports. figure 1 shows the interface used for the data collection. the leader was instructed to go through the items and select a subset for inclusion in the report. selection was done by highlighting the  important  words/phrases in the items  described to the participant as being those words or phrases that led them to select that particular item for the report . the items with highlighted text automatically become the candidate briefing items.1 the highlighted words and phrases subsequently were designated as custom user features and were used to train a model of the user's selection behavior. examination of the data indicated that users followed these instructions and did not  for example  simply select entire items or just the first word.
1 data collected
we were able to collect a total of complete 1 group-weeks of data. one group-week includes the time logs written by the members of a particular group and the associated extractive summaries. the class consisted of two stages  design and implementation  lasting about 1 and 1 weeks respectively. the groups and roles were reconstituted after the first stage. this meant that after the first stage we had new  authors  and could not combine the first stage data with the second stage into a continuous sequence. there were 1 groups in the first stage with average 1 students per group and 1 groups in the second stage with average 1 students per group  several students dropped the course after the first few weeks . there were on average 1 weekly summaries per group for stage 1 and 1 weekly summaries per group in stage 1. as is evident fromthe averages  not all the groupssubmitted a summary every week. to provide consistent data for development  testing and analysis of our system  we selected those 1 groups from the later stage of the class that produced reports most consistently  these are described further in the evaluation section below .
1 learning system
we modeled our learning process on the one described by  garera and rudnicky  1 ; that is  models were rebuilt on a weekly basis  using all training data available to that point  i.e.  from the previous weeks . this model was then used to predict the user's selections in the current week. for example  a model built on weeks 1 and 1 was tested on week 1. then a model built on weeks  1  1 and 1 was tested on week 1  and so on.
　because the vocabulary varied significantly from week to week  we trained models using only those words  features  that were to be found in the raw data for the target week  since it would not be meaningful to train models on non-observed features.
　the resulting model is used to classify each candidate item as belonging in the summary item or not. the confidence assigned to this classification was used to rank order the raw items and the top 1 items were designated as the  predicted  summary for that week. the following sections explain the learning system in more detail.
1 classifier settings
features
the features used in the classifier are words  or unigrams   stemmed and with stop words removed. we experimented with three different classes of features: a  nefalse: only unigrams b  neclass: unigrams + abstracted named entities using only the neclass label  i.e.  person  organization etc.  eg. 'white house' is treated as a unique token  'location'  representing its class.1 c  neuniq: raw unigrams + each named entity substituted by a unique token eg. 'white house' is treated as a unique token  'location-1' . we used bbn-identifinder  bbn-technologies  1  for extracting the named entities.
feature extraction
we extract the features in two different settings. firstly  we use the entire sentences for extracting the unigram features: fraw. secondly  we combine the entire sentence features with the user-specific features:fuser  fuser  fraw  which is similar to the idea of combining the 'context' and 'annotated keywords' described in  zhang et al.  1 .1 the details of how the final scores were computed are given below.
system description
standard information retrieval  ir  metrics are used to score the features. we did not fix the parameters in the schemes as the characteristics of the data did not particularly recommend any particular setting. the tunable parameters in the schemes and their possible values are: a  term weighing method - tf  term frequency   tf.idf  salton-buckley sb   yates  1 .
b  corpus for measuring idf: for any word  the inverse document frequency can be obtained by considering either the documents in the training set or the test set or both. therefore we have three different ways of calculating idf. c  normalization scheme for the various scoring functions: no normalization  l1 and l1.
　feature scoring in the first setting of extracting unigram features fraw is straightforward using the above mentioned ir parameters  tf  tf.idf or sb . for combining the scores under the second setting with the 'user-specific' features we used the following equation:
	sf = 1+α  sfbase	 1 
week nogroup1group1group1tnianwnisanstnianwnisanstnianwnisans111111.11111.111.1111.111.111.11.111.111.111.111.111.11111.111.111111.111.111.11.111.111.111.11--------
table 1: table showing the week-wise item details for the three selected groups. tni - total number of items in that week  anw - average number of words per item in that week  nis - number of items selected for that week  ans - average numberof words highlighted per selected item for that week where α is the weight contribution for the user-specific features and sfbase is the base score  tf or tf.idf or sb . we empirically fixed α to '1' for the current study.
　we tested the above mentioned variations of feature description  feature extraction and feature scoring using four learning schemes: naive bayes  voted perceptron  support vector and logistic regression. in the event  preliminary testing indicated that support vector and logistic regression were not suited for the problem at hand and so these were eliminated from further consideration. we used the weka
 witten and frank  1  package for developing the system.
1 evaluation
the base performance metric is recall  defined in terms of the items recommended by the system compared to the items ultimately selected by the user.1 we justify this by noting that recall can be directly linked to the expected time savings for the eventual users of a prospective summarization system based on the ideas developed in this study. the objective functions that we used for selecting the system model  built on the basis recall  are:
1. weighted mean recall  wmr : of the system across all weeks. the weeks are given linearly increasing weights  normalized  which captures the intuition that the performance in the later weeks is increasingly more important as they have consecutively more training data.
1. slope of the phase-wise performance curve  slope : we first calculate the three phase-wise recall performance values  normal average of the recall values  and then find out the slope of the curve for these three points.
　note that these metrics are used as a selection criterion only. results in figure 1 are stated in terms of the original recall values averaged over the phase and across the three users. we compare these with the results for the random baseline. the randombaseline is calculatedby randomlyselecting items over a large number  1  of runs of the system and determining the mean performance value. 1
1 experiment and results
we selected for experimentation the three groups that most consistently generated complete weekly datasets . these groups had 1  1 and 1 complete weeks of data. detailed statistics of the data are shown in table 1. since the groups had different number of weeks  we grouped the observations into three phases by combining the results from successive weeks to create a phase. merging the data into three phases is also consistent with our intuition about the task  that is  an activity has an initial starting period  middle activities and then the closing activity. we combined the observations with a sliding window to form the three phases. the window size and the overlap were different for the groups  though the same within a group.
　since we treated modeling technique as a free variable  we ran experiments for all the possible combinations of feature and system dimensions described in the previous section. the overall best performing system based on the jointly optimized metrics of wmr and slope was selected as the final model from which to generate the system predictions. the final model has the following system parameters:- learning scheme: voted perceptron  term weighing scheme: saltonbuckley  document frequency set: training set  normalization scheme: none. the feature set that gave best performance was neuniq.
　figure 1 shows the key results of our experiments with the above-mentionedmodel. the performance is averaged across the three groups.
　figure 1 a  shows that the performance obtained with generic n-gram features is comparable to the performance incorporating user-specific features. while one would have hoped that user-specific features would do better than generic ones  it's clear that users select consistent personal features.
　figure 1 b  shows that user-selected features are at least as good as those selected based on information gain  arguably an  optimal  way to select useful features. table 1 shows the mean number of active features in each phase  that is  how many previously identified features occur in the test set . here we see something interesting  which is that information

tionally used such as the 'first' sentence of a document etc. are not suitable because of a characteristically different corpora.

 a  recall values for the final model for  b  recall comparison between informa-  c  recall comparison between individual individual users comparing the raw n- tion gain selected features and user se- user training and cross-group training gram features with user-combined fea- lected features
tures
figure 1: figure showing the various experiments. 'raw n-grams' represents the entire n-grams in the items. 'user-combined' represents the group-wise single user  selected features combined with raw n-gramfeatures. 'ig-combined'is the information gain selected features combined with raw n-grams. 'cross-group' is the training data pooled across groups and the user'sselections combined with raw n-grams.
gain  ig  selects very few features  while humans select a broader number  not only those that might directly predict item importance  but also features that could in the future be of use. it is also curious that there is little overlap between the ig selections and the human ones. one interpretation is that humans are selecting a broader and potentially more robust set of features. the latter is supported by the observation that the ig-based model fails to select any active feature 1% of the time  while the human model fails only 1% of the time for this reason.
　figure 1 c  shows what happens if we pool user-selected features across any two groups and use these to train a single model for testing performance on the third group. here we see something different: cross-group features do not perform as well as group-specific features. this suggests that the specific features do matter  as they reflect a particular group's activities  as well as perhaps the leader's emphases in his or her reports . table 1 is comparable to table 1 and indicates that  while there is better overlap  pooling data introduces significant error into the model. together these results suggest that important features are both task-specific and user-specific. the latter interpretation is consistent with the results reported by  garera and rudnicky  1 .
　table 1 shows the most frequent active features selected under the conditions of single user selection  information gain and the cross-group pooled data.
1 conclusions
we show that consistent summarization models can be built from relatively sparse user-generateddata and that these mod-
phase noig-selectedsu-selectedoverlap1.1.11.1.1.1111table 1: table showing the phase-wise number of features under the information gain  ig  and single user  su  selected conditions. last column shows the features that are common between the two techniques.
phase nocg-selectedsu-selectedoverlap1.1.1.11111.1.1.1table 1: table showing the phase-wise number of features under the cross-group pooled  cg  and single user  su  training data conditions. last column shows the features that are common between the two techniques.
els are sufficiently consistent to predict  with increasing accuracy  the summarization behavior of these users. we also show that naive end-users are able to consistently select features at least as well as an automatic process. there is moreover evidence that the feature sets selected by humans may turn out to be more robust in the long run than automatic features  since predictive power is spread over a larger number of features . although this in itself is a useful result  it also opens the possibility of understandingand eventually au-
su-selectedig-selectedcg-selectedworkgroupworktestpdaclassclasssensortestboatgpboatsensorcreatsensorscriptgpspdacodeschedulscriptpdaiosystemdatatimelinphasetripinputcodetable 1: table showing the most frequent active features selected under different conditions sorted by frequency: single user  su   cross-group pooled  cg  and information gain  ig  selected conditions.
tomating the apparently sophisticated feature selection process used by humans.
　these results  together with the implicit-feedback paradigm embodied in the report-generation interface suggest that it may be possible to design viable learning systems that can be effectively trained by an end-user. such an approach to customization may be more powerful than ones based on adaptation  where models and features sets may be pre-determined  and that may produce system behavior that more faithfully reflects the needs of individual users.
acknowledgement
we would like to thank daniel p. siewiorek and asim smailagic for allowing us to do data collection in their class and susan finger and team for helping us in instrumentingthe data collection. we would also like to thank eric nyberg  eric riebling and team for the named entity annotations. this work was supported by darpa grant nbchd1. the content of the information in this publication does not necessarily reflect the position or the policy of the us government  and no official endorsement should be inferred.
