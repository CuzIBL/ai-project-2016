
there are vast amounts of free text on the internet that are neither grammatical nor formally structured  such as item descriptions on ebay or internet classifieds like craig's list. these sources of data  called  posts   are full of useful information for agents scouring the semantic web  but they lack the semantic annotation to make them searchable. annotating these posts is difficult since the text generally exhibits little formal grammar and the structure of the posts varies. however  by leveraging collections of known entities and their common attributes  called  reference sets   we can annotate these posts despite their lack of grammar and structure. to use this reference data  we align a post to a member of the reference set  and then exploit this matched member during information extraction. we compare this extraction approach to more traditional information extraction methods that rely on structural and grammatical characteristics  and we show that our approach outperforms traditional methods on this type of data.
1 introduction
the semantic web will revolutionize the use of the internet  but the idea faces some major challenges. first  construction of the semantic web requires a lot of extra markup on documents  but this work should not be forced upon everyday users. second  there is a lot of information that would be more useful if it were annotated for the semantic web  but the nature of the data makes it difficult to do so. examples of this type of data are the text of ebay posts  internet classifieds like craig's list  bulletin boards such as bidding for travel 1 or even the summary text below the hyperlinks returned after querying google. we call each piece of text from these sources a  post.  it would be beneficial to add semantic annotation to such posts  like that shown in figure 1  but the annotation task should carry no burden to human users.
　information extraction  ie  can be used to extract and semantically annotate pieces of some text. however  ie on

figure 1: a post from bidding for travel
posts is especially difficult because the data is neither structured enough to use wrapper technologies such as stalker  muslea et al.  1  nor grammatical enough to exploit natural language processing  nlp  techniques such as those used in whisk  soderland  1 .
　this lack of grammar and structure can be overcome by adding knowledge to ie. this extra knowledge consists of collections of known entities and their common attributes  which we call  reference sets.  a reference set can be an online set of reference documents  such as the cia world fact book. it can also be an online  or offline  database  such as the comics price guide.1 with the semantic web we envision building reference sets from the numerous ontologies that already exist. continuing with our hotel example from figure 1  assume there is an ontology of u.s. hotels  and from it we build a reference set with the following attributes: city  state  star rating  hotel name  area name  etc.
　to use reference sets for semantic annotation we exploit the reference set to determine which  if any  of the attributes appear in the post. to do this  we first determine which member of the reference set best matches the post. we call this the record linkage step. then we exploit the attributes of this reference set member for the information extraction step by identifying and labeling attributes from the post that match those from the matching member of the reference set. we annotate the post in this manner.
　for instance  the circled hotel post in figure 1 matches the reference set member with the hotel name of  holiday inn select  and the hotel area of  university center.  using this match we label the tokens  univ. ctr.  of the post as the  hotel area   since they match the hotel area attribute of the matching reference set record. in this manner we annotate all of the attributes in the post that match those of the reference set. figure 1 illustrates our approach on the example post from figure 1. for purposes of exposition  the reference set shown only has 1 attributes: name and area.

figure 1: annotation algorithm
　in addition to annotating attributes in the post from the reference set  we also annotate attributes that are identifiable  but not easily represented in reference sets. examples of such attributes include prices or dates. this is shown by the price of figure 1. also  we include annotation for the attributes of the matching reference member.  these are called  ref hotel...  in figure 1.  since attribute values differ across posts  these reference member attributes provide a set of normalized values for querying. also  the reference set attributes provide a simple visual validation to the user that the ie step identified things correctly. lastly  by including attributes from the matching reference member  we can provide values for attributes that were not included in the post. in our example post of figure 1  the user did not include a star rating. by including the reference set member's attribute for this star rating  we add useful annotation for information that was not present initially.
　this paper describes our algorithm for semantic annotation using reference sets. section 1 describes the record linkage step and section 1 describes the extraction step. section 1 presents experimental evaluation  and section 1 presents discussion of results. section 1 presents related research  and section 1 describes our conclusions.
1 aligning posts to a reference set
to correctly parse the attributes from the post  we need to first decide what those attributes are. to aid in this process  we match the post to a member of the reference set.
　since it is infeasible to compare the post to all members of the reference set  we construct a set of candidate matches in a process known as  blocking . many blocking methods have been proposed in the record linkage community  see  baxter et al.  1  for a recent survey of some   but the basic idea is to cluster candidates together around a blocking key. in our work  a candidate for a post is any member of the reference set that shares some n-gram with that post. the choice of algorithm here is independent of the overall alignment algorithm.
　next we find the candidate that best matches the post. that is  we must align one data source's record  the post  to a
　record from the other data source  the reference set candidates . this alignment is called record linkage  fellegi and sunter  1 .
　however  our record linkage problem differs and is not well studied. traditional record linkage matches a record from one data source to a record from another data source by relating their respective  decomposed attributes. yet the attributes of the posts are embedded within a single piece of text. we must match this text to the reference set  which is already decomposed into attributes and which does not have the extraneous tokens present in the post. with this type of matching traditional record linkage approaches do not apply.
　instead  we create a vector of scores  vrl  for each candidate. vrl is composed of similarity scores between the post and each attribute of the reference set. we call these scores rl scores. vrl also includes rl scores between the post and all of the attributes concatenated together. in the example reference set from figure 1  the schema has 1 attributes  hotel name  hotel area . if we assume the current candidate is   hyatt    pit airport  . then we define vrl as:
vrl= rl scores post  hyatt    rl scores post  pit airport   
rl scores post  hyatt pit airport   
each rl scores post attribute  is itself a vector  composed of three other vectors:
rl scores post attribute = token scores post attribute   edit scores post attribute   other scores post attribute  
　the three vectors that compose rl scores represent different similarity types. the vector token scores is the set of token level similarity scores between the post and the attribute  including jensen-shannon distance  both with a dirichlet prior and a jelenik-mercer mixture model  and jaccard similarity. the vector edit scores consists of the following edit distance functions: smith-waterman distance  levenstein distance  jaro-winkler similarity and character level jaccard similarity.  all of the scores in token scores and edit scores are defined in  cohen et al.  1 .  lastly  the vector other scores consists of scores that did not fit into either of the other categories  specifically the soundex score between the post and the attribute and the porter stemmer score between the two.
　using rl scores of just the reference attributes themselves gives a notion of the field similarity between the post and the match candidate. the rl scores that uses the concatenation of the reference attributes gives an idea of the record level match. since the post is not yet broken into attributes  this is how we determine these field and record level similarities. we could use only the concatenated scores since all we desire is a record level match. however  it is possible for different records in the reference set to have the same record level score  but different scores for the attributes. if one of these records had a higher score on a more discriminative attribute  we would like to capture that.
after all of the candidates are scored  we then rescore each
vrl. for each element of vrl  the candidates with the maximum value at that index map this element to 1. the rest of the candidates map this element to 1.
　for example  assume we have 1 candidates  with the vectors v1rl and v1rl:
v1rl =  .1.1 ... 1 1 
           v1rl =  .1.1 ... 1 1  after rescoring they become:
v1rl =  1 ... 1 
v1rl =  1 ... 1 
the rescoring helps to determine the best possible candidate match for the post. since there might be a few candidates with similarly close values  and only one of them is a best match  the rescoring separates out the best candidate as much as possible.
　after rescoring  we pass each vrl to a support vector machine  svm   joachims  1  trained to label them as matches or non-matches. when the match for a post is found  the attributes of the matching reference set member are added as annotation to the post.
1 extracting data from posts
to exploit the reference set for extraction we use the attributes of the best match from the reference set as a basis for identifying similar attributes in the post.
　to begin the extraction process  we break the post into tokens. in our example from figure 1  the post becomes the set of tokens  { $1    winning    bid  ...}. each of these tokens is then scored against each attribute of the record from the reference set that was deemed the match. this scoring consists of building a vector of scores which we call vie. although similar to vrl  the vie vector does not include the vector token scores because we are comparing single tokens of the post. instead  we include a vector common scores. the common scores vector includes user defined functions such as regular expressions  which help identify the attributes that are not present in the reference set  such as price or date. so  vie consists of the vector common scores and an ie scores vector between each token and each attribute from the reference set.
our rl scores vector from the record linkage step 
rl scores post attribute = token scores post attribute   edit scores post attribute   other scores post attribute  
becomes 
ie scores token attribute = edit scores token attribute  
other scores token attribute  
　so  using our example from figure 1  we match the post to the reference set member { holiday inn select   university center }. so  if we generate the vie for the post token  univ.  it would look like:
vie =  common scores  univ.    ie scores  univ.   holiday inn select   
ie scores  univ.   university center   
since each vie is not a member of a cluster where the winner takes all  there is no binary rescoring.
　each vie is then passed to a multiclass svm  tsochantaridis et al.  1  trained to give it a class label  such as hotel name  hotel area  or price. intuitively  similar attribute types should have a similar vie. we expect that hotel names will generally have high scores against the reference set attribute of hotel names  and small scores against the other attributes  and the vector will reflect this.
　the svm learns that any vector that does not look like anything else should be labeled as  junk   which can then be ignored. this is an important idea because without the benefits of a reference set this would be an extraordinarily difficult task. if features such as capitalization and token location were used  who is to say  great deal  is not a car name  also  many traditional ie systems that work in this unique domain of ungrammatical  unstructured text  such as addresses and bibliographies  assume that each token of the text must be classified as something  an assumption that cannot be made when users are entering text.
　however  by treating each token in isolation there is a chance that a junk token will be mislabeled. for example  a junk token might have enough letters to be labeled as a hotel area. then when we extract the hotel area from the post  it will have a noisy token. therefore  labeling each token individually gives an approximation of the data to be extracted.
　to improve extraction  we can exploit the power of the reference set by comparing the whole extracted field to its analogue reference set attribute. thus  once all of the tokens from a post are processed and we have whole attributes labeled  we take each attribute and compare it to the corresponding one from the reference set. then we can remove the tokens that introduce noise in the extracted attribute.
　to do this  we first get two baseline scores between the extracted attribute and the reference set attribute. one is a jaccard similarity  which demonstrates token level similarity. however  since there are many misspellings and such  we also need an edit-distance based similarity metric. for this we use the jaro-winkler metric. these baselines give us an idea of how accurately we performed on the approximate extraction. using our post from figure 1  assume the phrase  holiday inn sel. univ. ctr.  was  holiday inn sel. in univ. ctr. . in this case  we might extract  holiday inn sel. in  as the hotel name. in isolation  the token  in  could be the  inn  of a hotel name. comparing this extracted hotel name to the reference attribute   holiday inn select   we get a jaccard similarity of 1 and a jaro-winkler score of 1.
　next  we go through the extracted attribute  removing one token at a time and calculating the new jaccard and jarowinkler similarities. if both of these new scores are higher than the baseline  then that token is a candidate for removal. once all of the tokens are processed in this way  the candidate for removal that has the highest scores is removed  and we repeat the whole process. the process ends when there are no more tokens that yield improved scores when they are removed. in our example  we find that  in  is a removal candidate since it yields a jaccard score of 1 and a jaro-winkler score of 1. since it has the highest scores after the iteration  it is removed. then we see that removing any of the remaining tokens does not provide improved scores  so the process ends.
　aside from increasing the accuracy of extraction  this approach has the added benefit of disambiguation. for instance  a token might be both a hotel name and a hotel area  but not both at the same time  as an example   airport  could be part of a hotel name or a hotel area. in this case  we could label it as both  and the above approach would remove the token from the attribute that it is not. however  our implementation currently assigns only one label per token  so we did not test this disambiguation technique.
　thus  the whole extraction process takes a token of the text  creates the vie and passes this to the svm which generates a label for the token. then each field is cleaned up and we add the annotation to the post. this produces the output shown at the end of figure 1.
1 results
to validate our approach  we implemented our algorithm in a system named phoebus and tested the technique in two domains: hotel postings and comic books.
　in the hotel domain  we attempt to parse the hotel name  hotel area  star rating of the hotel  price and dates booked from the bidding for travel website. this site is a forum where users share successful bids for priceline. we limited our experiment to posts about hotels in sacramento  san diego and pittsburgh. as a reference set  we use the bidding for travel hotel guides. these guides are special posts that list all of the hotels that have ever been posted about in a given area. these posts provide the hotel name  hotel area and the star rating  which are used as the reference set attributes.
　the comic domain uses posts from ebay about comic books for sale searched by keyword  incredible hulk  and  fantastic four . our goal is to parse the title  issue number  price  condition  publisher  publication year and the description from each post.  note: the description is a few word description commonly associated with a comic book  such as 1st appearance the rhino.  as a reference set for this domain  we used the comics price guide1 for lists of all of the incredible hulk and fantastic four comics  as well as a list of all possible comic book conditions. in this case  the reference set included the attributes title  issue number  description  condition and publisher.
　experimentally  we split the posts in each domain into 1 folds  one for training and one for testing  where the training fold was 1% of the total posts  and the testing fold was the remaining 1%. we ran 1 trials and report the average results for these 1 trials.
1 alignment results
our approach hinges on exploiting reference sets  so the alignment step should perform well. we report the results of the alignment step in table 1. according to the usual record linkage statistics we define:
             #correctmatches precision = 
#totalmatchesmade
         #correctmatches recall = 
#possiblematches

　we compare our record linkage approach to whirl  cohen  1 . whirl is record linkage system that does soft joins across tables by computing vector-based cosine similarities between the attributes. all other record linkage systems require matching based on decomposed fields  so whirl served as a benchmark because it does not have this requirement. as input to whirl  one table was the test set of posts  1% of the posts  and the other table was the reference set with the attributes concatenated together. as in our record linkage step  using the concatenation of attributes can best mirror finding a record level match. this was also done because joining across each reference set attribute separately leaves no way to combine the matches for these queries. for example  would we only count the reference set members that score highest for every attribute as matches 
　we ran a similarity join across these tables  which produced a list of matches  ordered by descending similarity score. for each post that had matches from the join  the reference set member with the highest similarity score was called its match. these results are also reported in table 1. since phoebus is able to represent both an attribute level and record level similarity in its score  using more than just token based cosine similarity  it outperformed whirl.
prec.recallf-measurehotelphoebus111whirl111comicphoebus111whirl111table 1: record linkage results
1 extraction results
we also performed experiments to validate our approach to extraction. specifically  we compare our technique to two other ie methods as baselines.
　one baseline is simple tagger from the mallet  mccallum  1  suite of text processing tools. simple tagger is an implementation of conditional random fields  crf . crfs have been effectively used in ie. as an example  one algorithm combines information extraction and coreference resolution using crfs  wellner et al.  1 .
　we also present our results versus amilcare  ciravegna  1   which uses shallow natural language processing for information extraction. it has been empirically shown that amilcare does much better in extraction versus other symbolic systems  ciravegna  1 . it presents a good benchmark versus nlp based systems  which we expect will not do well on our domains. for the tests  we supplied our reference data as gazetteers to amilcare.
　our ie technique includes scores that we deemed  common  scores  which are used to help identify attributes not in the reference set. we make these clear for each domain  since they are the only domain specific scores we include. for the hotel domain  our common scores are matchpriceregex and matchdateregex  which give a positive score if a token matches a price or date regular expression  and 1 otherwise. for the comic domain  we use matchpriceregex and matchyearregex.
　we present our results using precision  recall and fmeasure as defined above. tables 1 and 1 show the results of correctly labeling the tokens within the posts with the correct attribute label for the hotel and comic domains  respectively. attributes in italics are attributes that exist in the reference set. the column freq shows the average number of tokens that have the associated label.
　table 1 shows the results reported for all possible tokens  which is a weighted average  since some attribute types are more frequent than others. also included in table 1 are  field level  summary results. field level results regard a piece of extracted information as correctly labeled only if all of the tokens that should be present are  and there are no extraneous tokens. in this sense  it is a harsh  all or nothing metric  but it is a good measure of how useful a technique would really be.
　we tested the f-measures for statistical significance using a two-tailed paired t-test with α=1. in table 1 the only f-measure difference that was not significant was the star attribute between phoebus and simple tagger. in table 1 the f-measures for price were not significant between phoebus and simple tagger and between phoebus and amilcare. in table 1 all differences in f-measure proved statistically significant.
　phoebus outperforms the other systems on almost all attributes  and for all summary results. there were 1 attributes where phoebus was outperformed  and two of these warrant remarks.  we ignore hotel name since it was so similar.  on comic titles  phoebus had a much lower recall because it was unable to extract parts of titles that were not in the reference set. consider the post   the incredible hulk and wolverine #1 wendigo . in this post  phoebus extracts  the incredible hulk   but the actual title is  the incredible hulk and wolverine.  in this case  the limited reference set hindered phoebus  but this could be corrected by including more reference set data  such as other comic book price guides.
　the comic description is the other attribute where simple tagger outperformed phoebus. simple tagger learned that for the most part  there is an internal structure to descrip-
hotelprec.recallf-measurefreqareaphoebus1111simple tagger111amilcare111datephoebus1111simple tagger111amilcare111namephoebus1111simple tagger111amilcare111pricephoebus1111simple tagger111amilcare111starphoebus1111simple tagger111amilcare111table 1: extraction results: hotel domain
comicprec.recallf-measurefreqconditionphoebus1111simple tagger111amilcare111descript.phoebus1111simple tagger111amilcare111issuephoebus1111simple tagger111amilcare111pricephoebus1111simple tagger111amilcare111publisherphoebus1111simple tagger111amilcare111titlephoebus1111simple tagger111amilcare111yearphoebus1111simple tagger111amilcare111table 1: extraction results: comic domain
tions  such that they are almost never broken up in the middle. for instance  many descriptions go from the token  1st  to a few words after  with nothing interrupting them in the middle. simple tagger  then  would label all of these tokens as a description. however  lots of times it labeled too many. this way  it had a very high recall for description  by labeling so much data  but it suffered in other categories by labeling things such as conditions as descriptions too.
　phoebus had the highest level of precision for comic descriptions  but it had a very low recall because it ignored many of the description tokens. part of this problem stemmed from classifying tokens individually. since many of the description tokens were difficult to classify from a single token perspective  they were ignored as junk.
1 discussion
one drawback when using supervised learning systems is the cost to label training data. however  our entire algorithm gen-
hoteltoken levelfield levelprec.recallf-mes.prec.recallf-mes.phoebus111111simple tagger111111amilcare111111comictoken levelfield levelprec.recallf-mes.prec.recallf-mes.phoebus111111simple tagger111111amilcare111111table 1: summary extraction results
eralizes well and can perform well with little training data. table 1 presents summary  token level results for extraction. here we trained phoebus on 1% of the posts  and then tested on the other 1%. in both domains  the results are similar to those of the experimental set up  which used 1% of the data for training.
prec.recallf-measurehotel  1% 111hotel  1% 111comic  1% 111comic  1% 111table 1: phoebus: trained on 1% of the data
　since we return the reference set attributes as annotation  we need to examine whether or not this annotation is valid. to do this  we need a way to link the results from performing record linkage to extraction results. note we can consider the fact that a correct match during record linkage is the same as correctly identifying those attributes from the reference set in the post  at the field level. in our ongoing example from figure 1  when we match the post to the reference member with a hotel name of  holiday inn select   it is like extracting this hotel name from the post. thus  we can consider our record linkage results to be field level extraction results for the attributes in the reference set.
　using the reference set attributes as annotation has some interesting implications. for instance  the attributes from the reference set provide a normalized platform of values for querying the data. also  returning reference set attributes for types not found in the post provides information that might have been missing previously. as an example  a post might leave out the star rating. yet  now we have one from the reference set record upon which to query. another interesting implication arises as a solution for the cases where extraction is hard. none of the systems extracted the comic description well. however  by including the reference set description  we consider the record linkage results as how effectively we extracted  and thus labeled   a description. this yields an improvement of over 1% for precision  and almost 1% for recall.
　it may seem that using the reference set attributes for annotation solves the problem  but this is not the case. for one thing  we want to see the actual values entered for different attributes. also  there are cases when the extraction results outperform the record linkage results as seen with the hotel name and star rating. this happens because even if a post is matched to an incorrect member of the reference set  that incorrect member is most likely very close to the correct match  and so it can be used to correctly extract much of the information. for example  there might be hotels with the same star rating and hotel name  but with a different area. if this area is not included in the post or included in a convoluted way the record linkage step might not get a correct match. however  the reference set member could still be used to extract the hotel name and star rating.
　lastly  the more discriminative information that we can pass to the svm  the better it will perform. assume that all we wanted to do is extract price and date from hotel posts  and we would get the rest of the annotation from the reference set. we would still want to train the svm to extract the attributes of the reference set  because it would then know that by classifying a certain piece of information as a certain attribute  it is not another piece of information. that is to say  it is just as important to recognize what a token is not. for example  consider a reference set hotel named  the $1 inn . then when post matches this reference set member  and we see the token  $1   we know that it is most likely a hotel name. if we did not train to extract all of the attributes  this would be classified as a price  since the svm has no notion of a hotel name.
　extraction on all of the attributes also helps the system to classify  and ignore  tokens that are  junk . labeling something as junk is much more descriptive if it is labeled junk out of many possible class labels that could share lexical characteristics. this helps to improve the extraction results on items that are not in the reference set  and we see this in our results.
　on the topic of reference sets  it is important to note that the algorithm is not tied to a single reference set. the algorithm extends to include multiple reference sets by iterating the process for each reference set used.
　consider the following two cases. if we want to extract conference names and cities but we only use one reference set  it would have to contain the power set of cities crossed with conference names. however  if we have two reference sets  one for each attribute  we can run the algorithm once with the conference name data  and once with a reference set of cities.
　the next interesting case happens when a post contains more than one of the same attribute. for example  we want to extract two cities from some post. if we use one reference set  then it would include the cross product of all cities. however  we can use a single reference set of city names if we slightly modify the algorithm. we make a first pass with the city reference set. during this pass  the record linkage match will either be one of the cities that matches best  or a tie between them. in the case of a tie  we just choose the first match. using this reference city we then extract the city from the post  and remove it from the post. then we simply run the process again  which will catch the second city  using the same  single reference set. this could be repeated as many times as needed.
1 related work
our work is motivated by the goal that the cost of annotating documents for the semantic web should be free  that is  automatic and invisible to users  hendler  1 . many researchers have followed this path  attempting to automatically mark up documents for the semantic web  as we propose here  vargas-vera et al.  1; handschuh et al.  1; cimiano et al.  1; dingli et al.  1 . however  these systems rely on lexical information  such as part-of-speech tagging or shallow natural language processing to do their extraction/annotation  e.g. amilcare  ciravegna  1  . this is not an option when the data is ungrammatical  like our post data. in a similar vein  there are systems such as adel  lerman et al.  1  which rely on the structure to identify and annotate records in web pages. again  the failure of our data to exhibit structure makes this approach inappropriate. so  while there is a fair amount of work in automatic labeling  there is not much emphasis on techniques that could do this on text that is unstructured and ungrammatical.
　while the idea of record linkage is not new  fellegi and sunter  1  and is well studied even now  bilenko and mooney  1   most of the focus for this work matches one set of records to another set of records based on their decomposed attributes. there is little work on matching data sets where one record is a single string composed of the other data set's attributes to match on  as in our case. the whirl system  cohen  1  allows for record linkage without decomposed attributes  but as shown in section 1 we outperform whirl by exploiting a larger set of features to represent both a field and record level similarity.
　using the reference set's attributes as normalized values is similar to the idea of data cleaning. however  most of the data cleaning algorithms assume that there are tuple-to-tuple transformations  lee et al.  1; chaudhuri et al.  1 . that is  there is some function that maps the attributes of one tuple to the attributes of another. this approach would not work on our data  where all of the attributes are embedded within the post  which maps to a set of attributes from the reference set.
　information extraction can semantically annotate data  which is why we chose to compare our technique to other ie approaches  such as the simple tagger conditional random field  mccallum  1 . other ie approaches  such as datamold  borkar et al.  1  and cram  agichtein and ganti  1   segment whole records  like bibliographies  into attributes. however  both of these systems require that every token of a record receive a label  which is not possible with posts that are filled with irrelevant tokens. cram is also similar in its use of reference sets for extraction. however  they assume that the reference set members already match the data for extraction  while we do this record linkage automatically. another ie approach similar to ours performs named entity recognition using a dictionary component  cohen and sarawagi  1 . however  this technique requires that entire segments have the same class label  while our technique can handle the case where an attribute is broken up in the middle by another attribute  say a hotel name interrupted by a hotel area.
1 conclusion
in this paper we presented an algorithm for semantically annotating text that is ungrammatical and unstructured. this technique provides much more utility to data sources that are full of information  but cannot support structured queries. using this approach  ebay agents could monitor the auctions looking for the best deals  or a user could find the average price of a four star hotel in san diego. this approach to semantic annotation is necessary as we transition into the semantic web  where information needs annotation for software systems to use it  but users are unwilling to provide the required annotation.
　in the future  we we would like to link this technique with a mediator  thakkar et al.  1  framework for automatically acquiring reference sets. this is similar to automatically incorporating secondary sources for record linkage  michalowski et al.  1 . how to automatically formulate a query to retrieve the correct domain reference set is a direction of future research. also  our current implementation only gives one class label per token. ideally we would give a token all possible labels  and then remove the extraneous tokens when we clean up the attributes  as described in section 1.
1 acknowledgements
we would like to thank william cohen  andrew mccallum and fabio ciravegna for allowing public use of their systems and code. also  thanks to kristina lerman and snehal thakkar for their comments.
　this research is based upon work supported in part by the national science foundation under award no. iis1  in part by the defense advanced research projects agency  darpa   through the department of the interior  nbc  acquisition services division  under contract no.
nbchd1  in part by the defense advanced research projects agency  darpa  and air force research laboratory  air force materiel command  usaf  under agreement number f1-1  in part by the air force office of scientific research under grant number fa1-1  and in part by the united states air force under contract number f1-c-1.
　the u.s. government is authorized to reproduce and distribute reports for governmental purposes notwithstanding any copyright annotation thereon. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements  either expressed or implied  of any of the above organizations or any person connected with them.
