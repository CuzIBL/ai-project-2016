 
multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. however  their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. hence  they are well suited to the problem of training feedforward networks. in this paper  we describe a set of experiments performed on data from a sonar image classification problem. these experiments both 1  illustrate the improvements gained by using a genetic algorithm rather than backpropagation and 1  chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it. 
1 	introduction 
neural networks and genetic algorithms are two techniques for optimization and learning  each with its own strengths and weaknesses. the two have generally evolved along seperate paths. however  recently there have been attempts to combine the two technologies. davis  1  showed how any neural network can be rewritten as a type of genetic algorithm called a classifier system and vice versa. whitley  1  attempted unsuccessfully to train feedforward neural networks using genetic algorithms. in this paper we describe a different genetic algonthm for training teedforward networks. it not only succeeds in its task but it outperforms backpropagation  the standard training algonthm. on a difficult example. this success comes from tailoring the genetic algonthm to the domain of training neural networks. we document the evolution and ultimate success of this algonthm with a series of experiments. 
　the paper is structured as follows. sections 1 and 1 give an overview of neural networks and genetic algonthms respectively with a special emphasis on their strengths and weaknesses. section 1 describes the data on which the expenments were run. section 1 details the genetic algorithm we used to perform neural network weight optimization. 
1 	machine learning 
section 1 describes the experiments we ran and analyzes their results. section 1 provides conclusions about our work and suggestions for future work. 
1 	neural networks 
neural networks are algorithms for optimization and learning based loosely on concepts inspired by research into the nature of the brain. they generally consist of five components: 
1. a directed graph known as the network topology whose arcs we refer to as links. 
1. a state variable associated with each node. 
1. a real-valued weight associated with each link. 
1. a real-valued bias associated with each node. 
1. a transfer function for each node which determines the state of a node as a function of a  its bias b  b  the weights  wt of its incoming links  and c  the states  x   of the nodes connected to it by these links. this transfer function usually lakes the form where / is either a sigmoid or a step function. 
a feedforward network is one whoso topology has no closed paths. its input nodes are the ones with no arcs to them  and its output nodes have no arcs away from them. all other nodes are hidden nodes. when the states of all the input nodes are set  all the other nodes in the network can also set their states as values propagate through the network. the operation of a feedforward network consists of calculating outputs given a set of inputs in this manner. a layered feedforward network is one such that any path from an input node to an output node traverses the same number of arcs. the nth layer of such a network consists of all nodes which are n arc traversals from an input node. a hidden layer is one which contains hidden nodes. such a network is fully connected if each node in layer i is connected to all nodes in layer i+l lor all i. 
　layered feedforward networks have become very popular for a few reasons. for one  they have been found in practice to generalize well  i.e. when trained on a relatively sparse set of data points  they will often provide the right output for an input not in the training set. secondly  a training algorithm called backpropagation exists which can often find a good set of weights  and biases  in a reasonable amount of tune  rumelhart 1al. backpropagation is a variation on gradient search. it generally uses a least-squares optimaiity criterion. the key to backpropagation is a method for calculating the gradient of the error with respect to the weights for a given input by propagating error backwards through 
the network. 
　there are some drawbacks to backpropagation. for one  there is the  scaling problem . backpropagation works well on simple training problems. however  as the problem complexity increases  due to increased dimensionality and/or greater complexity of the data   the performance of backpropagation falls off rapidly. this makes it in feasible for many real-world problems including the one described in section 1. the performance degradation appears to stem from the fact that complex spaces have nearly global minima which are sparse among the local minima. gradient search techniques tend to get trapped at local minima. with a high enough gain  or momentum   backpropagation can escape these local minima. however  it leaves them without knowing whether the next one it finds will be better or worse. when the nearly global minima are well hidden among the local minima  backpropagation can end up bouncing between local minima without much overall improvement  thus making for very slow training. 
　a second shortcoming of backpropagation is the following. to compute a gradient requires differentiability. therefore  backpropagation cannot handle discontinuous optimal-
lty criteria or discontinuous node transfer functions. this precludes its use on some common node types and simple optimality criteria. 
　for a more complete description of neural networks  the reader is referred to  rumelhart 1b . 
1 	genetic algorithms 
genetic algorithms are algorithms for optimization and learning based loosely on several features of biological evolution. they require five components: 
1 a way of encoding solutions to the problem on chromosomes. 
1. an evaluation function that returns a rating tor each chromosome given to it. 
1. a way of initializing the population of chromosomes. 
1. operators that may be applied to parents when they reproduce to alter their genetic composition. included might be mutation  crossover  i.e. recombination of genetic material   and domain-specific operators. 
1. parameter settings for the algorithm  the operators  and so forth. 
given these five components  a genetic algorithm operates according to the following steps: 
i the population is initialized  using the procedure in c1. the result of the initialization is a set of chromosomes as determined in c1. 
1. each member of the population is evaluated  using the function in ci. evaluations may be normalized; the important thing is to preserve relative ranking of evaluations. 
1. the population undergoes reproduction until a stopping criterion is met. reproduction consists of a number of iterations of the following three steps: 
 a  one or more parents are chosen to reproduce. selection is stochastic  but the parents with the highest evaluations are favored in the selection the parameters of c1 can influence the selection process. 
 b  the operators of c1 are applied to the parents to produce children. the parameters of c1 help determine which operators to use. 
 c  the children are evaluated and inserted into the population. in some versions of the genetic algorithm  the entire population is replaced in each cycle of reproduction. in others  only subsets of the population are replaced. 
when a genetic algorithm is run using a representation that usefully encodes solutions to a problem and operators that can generate better children from good parents  the algorithm can produce populations of better and better individuals  converging finally on results close to a global optimum. in many cases  such as the example discussed in this paper   the standard operators  mutation and crossover  are sufficient for performing the optimization. in such cases  genetic algorithms can serve as a black-box function optimizer not requiring their creator to input any knowledge about the domain. however  as illustrated in this paper  knowledge of the domain can often be exploited to improve the genetic algorithm s performance through the incorporation of new operators. 
　genebc algonthms should not have the same problem with scaling as backpropagation. one reason for this is that they generally improve the current best candidate monotonically. they do this by keeping the current best individual as part of their population while they search for better candidates. secondly  genetic algonthms are generally not bothered by local minima. the mutation and crossover operators can step from a valley across a hill to an even lower valley with no more difficulty than descending directly into a valley. 
　the field of genetic algonthms was created by john holland. his first book  holland 1 was an early landmark the best introduction for the interested reader is  goldberg 1 . 
1 the data 
the data used in the experiments were from one stage in the processing of passive sonar data from arrays of underwater acoustic receivers. bbn has been building an expert system to detect and reason about interesting signals in the midst of the wide variety of acoustic noise and interference which exist in the ocean. the mam inputs to the expen system  as well as to sonar operators  are lofargrams  which  like spectograms  are intensity-based images depicting the distribution of acoustic energy as a function of frequency and time. narrowband energy appears on the lofargrams as  lines'' which tend to be predominantly vertical. a line has certain characteristics which provide clues to sonar operators as to what is producing it. deriving algonthms which capture all that the operators can see in real lines has never succeeded despite a large amount of work in the area. therefore  we are attempting to use neural networks in the problem of fine characterization. 
to start  we formed a database of approximately 1 
	montana and davis 	1 
　

fixed-size rectangular pieces of lofargrams  each centered on a line of type determined by an expert. around 1% of these were from lines of a type in which we were particularly interested  and around 1% were from lines of a variety of other types. one experiment we ran investigated whether a feedforward network could classify the line pieces as either interesting or not based on four of the parameters which our system presently uses to characterize such pieces. the network had four inputs  one output  and first and second hidden layers of seven and ten nodes respectively for a total of 1 weights. we used a subset of the examples of size 1 as a training set. it is on this network that the comparative runs described in section 1 were made. 
1 	our genetic algorithm 
we now discuss the genetic algorithm we set up to do neural network weight optimization. we start by describing the five components of the algorithm listed in section 1. 
　1  chromosome encoding: the weights  and biases  in the neural network are encoded as a list of real numbers  see figure 1 . 
　1  evaluation function: assign the weights on the chromosome to the links in a network of a given architecture  run the network over the training set of examples  and return the sum of the squares of the errors. 
　1  initialization procedure: the weights of the initial members of the population are chosen at random with a probability distribution given by t l r l . this is different from the initial probability distribution of the weights usually used in backpropagation  which is a uniform distribution between -1 and 1. our probability distribution reflects the empirical observation by researchers that optimal solutions tend to contain weights with small absolute values but can have weights with arbitrarily large absolute values. we therefore seed the initial population with genetic material which allows the genetic algorithm to explore the range of all possible solutions but which tends to favor those solutions which are a priori the most likely. 
　1  operators: we created a large number of different types of genetic operators. the goal of most of the experiments we performed was to find out how different operators perform in different situations and thus to be able to select a good set of operators for the final algorithm. the operators can be 
1 	machine learning 

grouped into three basic categories: mutations  crossovers  and gradients  see figure 1 . a mutation operator takes one parent and randomly changes some of the entries in its chromosome to create a child. a crossover operator takes two parents and creates one or two children containing some of the genetic material of each parent. a gradient operator takes one parent and produces a child by adding to its entries a multiple of the gradient with respect to the evaluation function. we now discuss each of the operators individually  one category at a time. 
　unbiased-mutate-weights: for each entry in the chromosome  this operator will with fixed probability p = 1 replace it with a random value chosen from the initialization probability distribution. 
　bias ed-mutate-weights: for each entry in the chromosome  this operator will with fixed probability p = 1 add to it a random value chosen from the initialization probability distribution. we expect biased mutation to be better than unbiased mutation tor the following reason. right from the start of a run  parents are chosen which tend to be better than average. therefore  the weight settings in these parents tend to be better than random settings. hence  biasing the probability distribution by the present value of the weight should give better results than a probability distribution centered on zero. 
　mutate-nodes: this operator selects n non-input nodes of the network which the parent chromosome represents. for each of the ingoing links to these n nodes  the operator adds to the links weight a random value from the initialization probability distribution. it then encodes this new network on the child chromosome. the intuition here is that the ingoing links to a node form a logical subgroup of all the links in terms of the operation of the network. by confining its changes to a small number of these subgroups  it will make its improvements more likely to result in a good 
evaluation. in our experiments  n = 1. 
mutate-weakest-nodes: the concept of node 
strength is different from the concept of error used in backpropagation. for example  a node can have zero error if ail its output links are set to zero  but such a node is not contributing anything positive to the network and is thus not a strong node. the concept of strength comes from classifier systems and was introduced into neural networks in  davis 
　
1 . we define the strength of a hidden node in a feedforward network as the difference between the evaluation of the network intact and the evaluation of the network with that node lobotomized  i.e. with its output links set to zero . we have devised an efficient way to calculate node strength not discussed here. 
　tlie operator mutate-weakest-nodes takes the network which the parent chromosome represents and calculates the strength of each hidden node. it then selects the m weakest nodes and performs a mutation on each of their ingoing and outgoing links. this mutation is unbiased if the node strength is negative and biased if the node strength is positive. it then encodes this new network on a chromosome as the child. the intuition behind this operator is that there are some nodes which are not only not very useful to the network but may actually be hurting it. performing mutation on these nodes is more likely to yield bigger gains than mutation on a node which is already doing a good job. note that since this operator will not be able to improve nodes which are already doing well it should not be the only source of diversity in the population. in our experiments  m=l. 
　crossover-weights: this operator puts a value into each position of the child's chromosome by randomly selecting one of the two parents and using the value in the same position on that parent's chromosome. 
　crossover-nodes: for each node in the network encoded by the child chromosome  this operator selects one of the two parent's networks and finds the corresponding node in this network. it then puts the weight of each ingoing link to the parent's node into the corresponding link of the child's network. the intuition here is that networks succeed because of the synergism between their various weights  and this synergism is greatest among weights from ingoing links to the same node. therefore  as genetic matenal gets passed around  these logical subgroups should stay together. 
　crossover-features: different nodes in a neural network perform different roles. for a fully connected  layered network  the role which a given node can play depends only on which layer it is in and not on its position in that layer. in fact  we can exchange the role of two nodes a and b in the same layer of a network as follows. loop over all nodes c connected  by either an ingoing or outgoing link  to a  and thus also to b . exchange the weight on the link between c and a with that on the link between c and b. ignoring the internal structure  the new network is identical to the old network  i.e. given the same inputs they will produce the same outputs. 
　the child produced by the previously discussed crossovers is greatly affected by the internal structures of the parents. the crossover-features operator reduces this dependence on internal structure by doing the following. for each node in the first parents' network  it tries to find a node in the second parent's network which is playing the same role by showing a number of inputs to both networks and comparing the responses of different nodes. it then rearranges the second parent's network so that nodes playing the same role are in the same position. at this point  it forms a child in the same way as crossover-nodes. the greatest improvement gained from this operator over the other crossover operators should come at the beginning of a run before all the members of a population start looking alike. 

　hillcl1mb: this operator calculates the gradient for each member of the training set and sums them together to get a total gradient. it then normalizes this gradient by dividing by the magnitude. the child is obtained from the parent by taking a step in the direction determined by the normalized gradient of size step-size  where step-size is a parameter which adapts throughout the run in the following way. if the evaluation of the child is worse than the parent's  step-size is multipled by the parameter step-size-decay=1; if the child is better than the parent  step-size is multiplied by step-size-expand=1. this operator differs from backpropagation in the following ways: 1  weights are adjusted only after calculating the gradient for all members of the training set and 1  the gradient is normalized so that the step size is not proportional to the size of the gradient. 
　1  parameter settings: there are a number of parameters whose values can greatly influence the performance of the algorithm. except where stated otherwise  we kept these constant across runs. we now discuss some of the important parameters individually. 
　parent-scalar: this parameter determines with what probability each individual is chosen as a parent. the second-best individual is parent-scalar times as likely as the best to be chosen  the third-best is parent-scalar times as likely as the second-best  etc. the value was usually linearly interpolated between 1 and 1 over the course of a run. 
　operator-probabilities: this list of parameters determines with what probability each operator in the operator pool is selected. usually  these values were initialized so that the operators all had equal probabilities of selection. an adaptation mechanism changes these probabilities over the course of a run to reflect the performance of the operators  increasing the probability of selection for operators that are doing well and decreasing it for operators performing poorly. this saves the user from having to hand-tune these probabilities. 
　population-size: this self-explanatory parameter was usually set to 1. 
　an example of an iteration of the genetic algorithm is shown in figure 1. 
	montana and davis 	1 
　

1 	experiments 
in this section we discuss a series of experiments which led us to our final version of the genetic algorithm and one more experiment comparing the genetic algorithm to backpropagation. to evaluate the algonthm with a given set of operators and parameter settings  we performed a series of ten independent runs recording the evaluation of the best individual as a function of the number of iterations. afterwards  we averaged the results so as to reduce the variations introduced by the stochastic nature of the algonthm. even after averaging some variations remain  and so it is a judgment call to separate random variations from significant differences between averaged runs. 
　experiment 1: this experiment was designed to compare the performances of three different versions of mutation. unbiased-mutate-weights  biasedmutate- weights and mutate-nodes. to do this we performed three series of runs  each with the same parameter settings and with crossover-weights as one of the operators. the only difference between runs was the version of mutation used. the results are pictured in figure 1. as predicted  there is a clear ordering in terms of performance: 1  mutate-nodes  1  b i a s e d - m u t a t e weights and 1  unbiased-mutate-weights. 
　experiment 1: this experiment was designed to compare the performances of three different versions of crossover: crossover-weights. crossovernodes and crossover-features. to do this we performed three series of runs  each with the same parameter 
settings and with mutate-nodes as one of the operators. the only difference between runs was the version of crossover used. the results are pictured in figure 1. they indicate that there is little performance difference between 
the different types of crossover. 
　experiment 1: this experiment attempted to determine the effectiveness of the special-purpose operator mutateweakest-nodes. to do this  it compared an averaged run with just the two operators mutate-nodes and crossover-features to an averaged run with these two operators plus mutate-weakest-nodes. the results are pictured in figure 1. having mutate-
weakest-nodes improved the performance at the be-
1 	machine learning 

ginning of the runs but diminished the performance in the middle and at the end. this could indicate that we need to change our definition of node strength. 
　experiment 1: this experiment investigated the effectiveness of a hill-climbing mode for use at the end of a genetic algorithm run. the hill-climbing mode has hillclimb as its only operator and has the parameter parentscalar=1  so that the best individual is always chosen as the parent . the experiment compared an averaged run of 1 iterations with mutate-nodes and crossovernodes to an averaged run of 1 iterations with these two operators and 1 iterations in hill-climbing mode. the results are shown in figure 1. note the rapid progress immediately after entering hill-climbing followed quickly by a period of little or no progress. this reflects the speed of hill-climbing mode at climbing local hills and its inability to go anywhere once it hits the peak. it is clear that hillclimbing mode carries with it a big risk of local minima and therefore should only be used when it is relatively certain that a global minimum is near. 
　experiment 1: this experiment compared the performance of standard backpropagation with our genetic algorithm for training our feedforward network. we used the backpropagation algorithm described in  rumelhart 1a  with a learning rate  i.e. gain  of 1. the genetic algorithm 
　
had two operators. mutate-nodes and crossovernodes  and wis thus a variation on a standard genetic algorithm. 
　when comparing them  we considered two iterations of the genetic algorithm to be equivalent to one iteration  i.e. one cycle through all the training data  of backpropagation. to see why  observe that backpropagation consists of looping through all training data doing i  forward propagation and calculation of errors at the outputs and 1  error backward propagation and adjusting of weights. the second step requires more computation in our network and almost all other networks of interest. the evaluation function of the genetic algorithm performs the same calculations as step 1 . the operators mutate-nodes and crossover-nodes do very little computation. hence  one iteration of backpropagation requires more than twice as much computation as one iteration of the genetic algorithm. 
　the runs consisted of 1 iterations of the genetic algorithm and 1 iterations of backpropagation. the results are shown in figure 1. clearly  the genetic algorithm outperformed backpropagation. 
1 	conclusions and future work 
we have accomplished a number of things with our work on using genetic algorithms to train feedforward networks. in the held of genetic algorithms  we have demonstrated a real-world application of a genetic algorithm to a large and complex problem. we have also shown how adding domainspecific knowledge into the genetic algorithm can enhance its performance. in the held of neural networks  we have introduced a new type of training algorithm which on our data outperforms the backpropagation algorithm. our algorithm has the added advantage of being able to work on nodes with discontinuous transfer functions and discontinuous error criteria. 
　the work described here only touches the surface of the potential for using genetic algorithms to train neural networks. in the realm of feedforward networks  there are a host of other operators with which one might experiment. perhaps most promising are ones which include backpropagation as all or part of their operation. another problem is how to modify the genetic algorithm so that it deals with a stream of continually changing training data instead of fixed training data. this requires modifying the genetic algorithm to handle a stochastic evaluation function. finally  as a general-purpose optimization tool  genetic algorithms should be applicable to any type of neural network  and not just feedforward networks whose nodes have smooth transfer functions  for which an evaluation function can be derived. the existence of genetic algorithms for training could aid in the development of other types of neural networks. 
