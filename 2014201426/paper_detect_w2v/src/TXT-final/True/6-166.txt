 
simultaneous localization and mapping  slam  is a fundamental problem in mobile robotics: while a robot navigates in an unknown environment  it must incrementally build a map of its surroundings and  at the same time  localize itself within that map. one popular solution is to treat slam as an estimation problem and apply the kalman filter; this approach is elegant  but it does not scale well: the size of the belief state and the time complexity of the filter update both grow quadratically in the number of landmarks in the map. this paper presents a filtering technique that maintains a tractable approximation of the belief state as a thin junction tree. the junction tree grows under filter updates and is periodically  thinned  via efficient maximum likelihood projections so inference remains tractable. when applied to the slam problem  these thin junction tree filters have a linearspace belief state and a linear-time filtering operation. further approximation yields a filtering operation that is often constant-time. experiments on a suite of slam problems validate the approach. 
1 introduction 
simultaneous localization and mapping  slam -where a 
robot navigating in an unknown environment must incrementally build a map of its surroundings and localize itself within that map-has attracted significant attention because it is required by many applications in mobile robotics  thrun  1 . typically the environment is idealized so that it consists of an unknown number of stationary  landmarks ; for example  in a given slam application these landmarks may be low-level visual features or structural features such as walls and corners  slam can then be viewed as the problem of incrementally estimating the locations of the robot and landmarks from noisy and incomplete observations. 
one popular approach treats slam as a filtering problem 
 smith et al.  1 . the hidden state of the system at time is represented by a random variable which includes xt 
the state of the robot at time and   the locations of the  landmarks observed up to time thus  the size of the state vector is linear in the number of observed landmarks and grows over time. the kalman filter is used to compute the filtered belief state observations to t i m e w h i c h in this case takes the form of a multivariate gaussian distribution  we regard the m e a n a s the estimate of the map and the covariance matrix a measure of confidence. 
　the kalman filter solution is elegant  but it does not scale well to large slam problems. because  explicitly represents correlations between all pairs of variables  the size of the belief state grows as  and because each of these correlations must be updated whenever a landmark is reobscrved  the time complexity of its filter operation is also 
　　　　this quadratic complexity renders the kalman filter inapplicable to large slam problems and gives rise to the need for principled  efficient approximations. 
　unfortunately  the simplest approach-discarding correlations so each variable is estimated independently-presents problems. ignoring correlations between the robot state and the landmarks' states leads to overconfidence and divergence because correlated observations are treated as if they conveyed independent information  hebert et al.  1 . furthermore  correlations between pairs of landmark states are required for quick convergence when the robot closes a loop  i.e.  it reobserves known landmarks after an extended period of mapping unknown territory  see figure 1 . when the robot closes a loop  it reobserves landmarks whose positions are known with relative certainty; this helps the robot localize. the robot-landmark correlations translate this improved localization estimate into improved position estimates for recently-observed landmarks. the inter-landmark correlations translate these improved position estimates into improvements for the remaining landmarks on the tour.1 thus  the correlations give the kalman filter a valuable property normally associated with smoothing algorithms: it can use current observations to improve estimates  from the past . 
　because quadratically many correlations are necessary to close loops  we view the challenge of scalable slam filtering as that of estimating and reasoning with quadratically many correlations without quadratic time or space complexity. in this paper  we present a novel and general approximate filtering method satisfying this criterion. our point of departure 

robotics 	1 


figure 1: the robot is travelling counter-clockwise on a square path. dots represent landmarks; the true position of the robot is shown by a square; the filter belief state is visualized using the 1% confidence ellipses of the variable marginals  bold for the robot . left: accumulated noise and error has led to uncertain and drifted estimates for the robot and landmark positions. right: after closing the loop  all of the position estimates improve and their confidences increase. 
 in section 1  is to view the filtered belief state of the kalman filter as a gaussian graphical model  cowell et al  1  that evolves over time; this allows us to express correlations in terms of direct dependencies  edges  and indirect dependencies  paths . analyzing the evolution of this graphical model reveals that filter updates add edges to the graphical model  making inference more expensive. this motivates an approximation scheme in which weak or redundant edges are periodically removed to improve the complexity of inference. note that edge removal is very different than simply discarding correlations; because other edges are left intact  paths-and thus correlations-persist between each pair of variables. 
　graphical models give us valuable insight into how good approximate filters can be designed  but using them to represent the belief state presents problems. first  variable marginals like the robot's current position would not be immediately available as they are in the kalman filter representation; we would require inference to obtain them. second  while it is possible to remove edges from a gaussian graphical model using the iterative proportional fitting algorithm  speed and kiiveri  1   its application in this context would be prohibitively slow. finally  choosing edges whose removal leaves a distribution for which inference is tractable is itself a complicated process  kjaerulff  1 . 
　our solution to these problems is to use a different representation of the belief state. exact inference in graphical models is often implemented by message passing on a junction tree  cowell et ah  1 . rather than view the junction tree algorithm as an  inference engine   we use the junction tree itself as our representation of the belief state. this representation has many advantages: the belief state has a  builtin  inference algorithm  namely  message passing ; it gives immediate access to the marginal distribution over any variable; and as we demonstrate  it gives us efficient methods of selecting edges to prune and pruning them. 
　to implement such a junction tree filter  we develop methods for updating the junction tree to reflect filtering updates in 
section 1. these updates can cause the width of the junction tree to grow  making inference more expensive; in section 1 we present a novel  thinning  operation over junction trees called variable contraction. we prove that each variable contraction is a maximum likelihood projection that removes a set of edges from the corresponding graphical model. the approximation error introduced by a variable contraction can be computed efficiently  which allows us to choose which edges to remove at each time step so as to minimize the error. 
　in section 1 we apply these techniques to the slam problem and obtain a thin junction tree filter  tjtf  with a o space belief state representation and a o  time filter operation. by delaying the incorporation of recent evidence into the majority of the map  we can improve the filter's time complexity; we present a method of evaluating the significance evidence has on different portions of the map  which can be used to adaptively interpolate between constant and linear-time filter operations. empirically  we find that these adaptive filters choose constant-time updates when mapping new territory  and when closing a loop  they use time linear in the length of the loop. this is perhaps the best time complexity one would hope for in the slam problem  since linearly-many estimates cannot be improved in constant time. section 1 presents the results of simulation experiments that compare tjtf to other slam filters and section 1 concludes. a companion technical report contains proofs of all propositions as well as additional background  analysis  and experiments  paskin  1 . 
1 	related work 
significant interest in the slam complexity problem has led to a number of approaches  thrun  1 . for example  there are several submap approaches that decompose the problem into a set of small mapping problems yielding a blockdiagonal landmark covariance matrix. these techniques can achieve constant time complexity  but converge slowly because information cannot pass between the submaps. 
　recently  the fastslam algorithm  montemcrlo et al.  1 -a rao-blackwellized particle filter-has attracted attention because of its logarithmic time complexity. however  our experiments show fastslam is susceptible to divergence in large  noisy slam problems. we believe this is because the number of particles required for a satisfactory solution can grow exponentially over time; see  paskin  1  for details. 
　sparse extended information filters  seif   thrun et al  1  can be viewed in terms of the graphical model representation described above; at each time-step  edges are removed so that a constant-time filter operation can be guaranteed. to avoid the additional complexity of inference  seif employs approximate inference over this approximate model. thus  the seif paper provided the valuable insight that sparse graphical models can constitute an efficient solution to slam. implementing this insight while avoiding additional approximation was one of the primary motivations of this work. 
　each of these approaches described above uses a sublineartime filter update  and therefore  none can improve all of the landmark estimates in a single update  like the kalman filter . tjtf has the best of both worlds: its update step takes constant time unless the observation is significant enough to warrant a linear-time update. 

1 	robotics 

　outside of the slam literature  there are two works that are especially relevant. kjairulff  investigated edge removal as a means of reducing the complexity of inference in graphical models. our approach is somewhat simpler  as it operates directly on the junction tree without referring to the underlying graphical model. kjaerulff's analysis of the approximation error inspired ours  and several of his results apply directly to our case. 
　thin junction tree filtering is an assumed density filtering  adf  algorithm because it periodically  projects  the filter's belief state to some tractable family of distributions-in this case  the family of gaussian distributions characterized by thin junction trees. this makes other work on adf relevant  especially that of boyen and koller   in which the belief state of a dynamic bayesian network is periodically pro-
jected to a product-of-marginals approximation. in fact  the connection to this work is stronger: boyen and koller  extended their earlier analysis to filters where the belief state is represented by a junction tree whose structure evolves over time; however  no algorithms were presented. to our knowledge  tjtf is the first algorithm to which this analysis applies. here we apply tjtf to a gaussian graphical model  but nothing prevents its application to the discrete variable networks considered by boyen and koller. 
1 a graphical model perspective on slam 
we begin by presenting the slam model and then formulating slam filtering in terms of graphical models. 
1 	the slam model 
we assume a general slam model where in each time step the robot moves  obtains an odometry measurement of its motion  and makes several observations of landmarks. as in the kalman filter context  we assume that the motion and measurement models are known and that they are lineargaussian.1 the robot motion at time / is governed by 
		 1  
and the odometry measurement yt at time t is governed by 
		 1  
yt is typically a noisy measurement of the robot's velocities. 
　landmark measurements are typically assumed to depend only upon the state of the robot and the state of the observed landmark; for example the observation may consist of the range and bearing to the landmark in the robot's coordinate frame. if the zth landmark measurement at time issued from landmark it is governed by 
 1  
for simplicity  we assume the correspondence between each measurement and the landmark from which it issued is known. this question of data association  while critically important in slam  is largely orthogonal to the issues we address here; in particular  the standard technique of choosing the maximum likelihood data association applies without change in our treatment. when a landmark is first observed  its state variable is added to the belief state with a uninformative  infinite variance  zero covariance  prior; the measurement update yields an informed posterior estimate of its state. 
1 	gaussian graphical models 
 under the assumptions outlined above  the filtered belief state  is a multivariate gaussian distribution. the kalman filter represents this distribution using the moment parameters-the mean vector 
then its probability distribution is 
where d is the length of u. in contrast  gaussian graphical models are usually based upon the canonical parameters- the information vector r/ and matrix 
 1  
where 	is the  log  normalization constant. the canonical and moment parameters are related by an advantage of the canonical parameterization is that multiplication/division of gaussians reduces to addition/subtraction of the parameters. 
　let  be a set of random variables indexed by elements of the finite set v. we will call a subset of v a family. for a family  be the associated set of random variables. a potential over a family  is a non-negative function of let f be a set of families and let be a set of potential functions over these families.  f   p  defines a distribution 
 1  
when the normalizer 	is finite. 
has vertex set 
there is an 
edge between 
the primary value of the markov graph representation comes from the hammersley-clifford theorem  which states that s separates from in the markov graph 
iff 	 provided 	in other words  graph 
separation in the markov graph encodes the conditional independence properties of  because conditional independence properties often translate into efficient inference algorithms  e.g.  junction tree   the markov graph gives good intuitions into the design of efficient approximations. 
robotics 	1    we can represent the gaussian  1  by a markov graph  since if we partition the vector 
and  is the normalization constant. thus  all the potentials of a gaussian graphical model are either unary  node 
potentials  or binary  edge potentials . we also have the important interpretation that if  is unity  and therefore superfluous   meaning there is no edge between i and j in the corresponding markov graph. 
1 	filtering in gaussian graphical models 
filtering can be viewed as a three-step procedure: estimation  in which we incorporate the current time step's measurements; prediction  in which we augment the model with the state variables of the next time step; and roll-up  in which we marginalize out the state variables from the past time step. when the measurement and motion models are lineargaussian  the prediction and estimation steps reduce to multiplying small gaussian potentials into the model; these updates are summarized by 
proposition 1 ignoring irrelevant normalization constants  the motion update of equation  1  can be implemented by multiplying the potential 

into the model; the odometry measurement update of equation  1  can be implemented by multiplying in the potential 

and the landmark measurement update of equation  1  can be implemented by multiplying in the potential 

　the final step of filtering is roll-up  or marginalizing out the past state. the standard rule for marginalization in the canonical parameterization is given by  cowell et al.  1  
fact 1. if a = v i and 
 1  
 id 
 1  
the time complexity of computing  1  and  1  is quadratic in the dimension of and cubic in the dimension of 
　the additive updates above can also be viewed as multiplying in a new potential 	into the model. the markov blanket neighbors in the markov graph. because missing edges in the markov graph correspond to zeros in a  we can infer that this is really a potential over  and therefore that marginalizing ui out of the model places a clique of edges over the markov blanket of i. 

figure 1: example evolution of a slam graphical model   a  in the initial belief state  the robot's state  and the landmarks' states and are marginally independent   b  observing each landmark induces a correlation between and resulting in a new edge   c  the prediction update adds the new robot state to the model and joins it to the current robot state .  d  the roll-up phase marginalizes out of the model  adding a clique edges over all of 's 
neighbors. 
1 	filtering the s l a m graphical model 
using these results we can characterize how the structure of the slam belief state evolves over time  see figure 1 . for each observed landmark we multiply a measurement potential into the graphical model; this adds an edge between xt and thus  after the estimation phase  the robot's state will be connected to the states of all landmarks it has observed. the prediction phase then connects and finally  the roll-up phase marginalizes out this places a potential over the markov blanket of which now includes all observed landmarks and now the slam graphical model takes the form of a complete graph-i.e.  the belief state has no conditional independencies. by induction  this will be true after every time step. 
　an intuition for why the graphical model becomes dense over time is valuable. when the robot measures a landmark  the landmark's state becomes directly correlated with that of the robot  and thus indirectly correlated with all covariates of the robot state  e.g.  other landmark states. when the robot's state is eliminated from the model during roll-up  these indirect correlations must be expressed directly via new edges. 
　importantly  these indirect correlations are often much weaker than the direct ones. thus  even though the slam belief state has no true conditional independencies  there are many  approximate  conditional independencies; e.g.  the landmarks observed at the beginning and end of a tour are almost independent given those observed in the middle. by removing  weak  edges from the graphical model we can enforce these approximate conditional independencies so they can be used to speed inference. 
1 	junction tree filtering 
as discussed in the introduction  the graphical model representation is valuable for motivating our approximate filter  but it is not an appropriate representation for its implementation. instead  we represent the belief state of the filter using a junction tree. we begin by briefly summarizing the relevant concepts; see  cowell et al  1  for details. 
robotics 
1 	junction trees 
let p be a distribution of the form  1  with families f and potentials 	 c  e  be an undirected 
graph where each vertex  or cluster  c is a subset of v  t is  junction tree for p if the following three properties hold: 
1. singly connected property: t is a tree. 
1. potential property: for every family f there is some cluster such that 
1. running intersection property: is present in two clusters and of t  it is also present in all clusters on the  unique  path between and 
with each edge e we associate a separator s = let s be the set of t s separators. 
　given a junction tree 1  we can perform inference in the model by passing messages between the clusters of t. we begin by associating with t a set of potential functions  one for each cluster and 
separator. the charge on t is defined to be 
we initialize 	by setting all cluster and separator potentials to unity  multiplying each potential 	into 	for some 
　　c  which is possible by the potential property   and multiplying into an arbitrary then 
	let c and 	be adjacent clusters with separator 	d. 
passing a message from c to d updates the separator potential 
 1  
 1  
importantly  these updates leave the charge  1  invariant  so 
 thus  we can view them as reparameterizing the distribution p. when messages are passed along every edge in both directions  in an appropriate schedule   the cluster and separator potentials are updated so that they are marginals of  over their respective variables. a junction tree in this state is called consistent and it can be used to obtain marginals over any set of variables that reside together in some cluster. 
　when t has no nonmaximal clusters   so the number of messages required for inference is bounded by 1   
 in the case of a gaussian graphical model  the cluster and separator potentials are gaussians; if they are represented by their canonical parameters  the time complexity of passing a message is dominated by the cost of the marginalization in  1  which is implemented via  1  and  1 ; thus  it is at worst cubic in the size of the cluster. in sum  inference is linear in  and cubic in the width of t  traditionally defined as the size of the largest cluster minus one. 
1 	incremental junction tree maintenance 
we adopt consistent junction trees as the belief state representation of our filter; i.e.  the belief state will be represented by the charge  1  of a consistent junction tree. recall from section 1 that the prediction and estimation phases of the filter update can be implemented by multiplying in small  simple potentials to the probability distribution  and that the rollup phase is implemented by marginalizing variables out of the model. in this section we describe how to incrementally maintain a consistent junction tree under these updates. 
　in what follows we will make use of three nonstandard operations to restructure a consistent junction tree. 
  cloning: to clone a cluster we create a copy d  attach d to c with separator and set 
  merging: let c and d be neighboring clusters with separator s. to merge d into c  we:  1  update 	d;  1  update 	 1  swing all edges incident to d over to c; and  1  remove d from c and s from 1. 
  pushing: let c and d be neighboring clusters with separator s such that but to push i from c to  we update and and pass a message from to d to update and by extension we can push from c to a nonadjacent cluster by successive pushes along the unique path from r to  any nonmaximal clusters created by pushing are subsequently merged into their subsuming neighbors.  
it is easy to check that all of these operations preserve the three structural constraints as well as the charge and consistency of a junction tree. 
multiplying in potentials 
assume our belief state is represented by a consistent junction tree t. in order to update the charge of t to reflect the multiplication of a potential into we must find a cluster and m u l t i p l y i n t o to restore consistency  we could pass messages throughout 1  but this is twice the work needed: a simple consequence of the message-passing updates  1  and  1  is that we need only distribute evidence from i.e  we must pass messages along edges in a preorder traversal from c. 
　if there is no cluster that covers the family a of the new potential  then we must first modify the junction tree t to create one. draper  presents several techniques to do this; in the gaussian case the problem is somewhat simpler  since the potentials bind at most two variables. when multiplying in an edge potential requires us to create a cluster covering   we find the closest pair of clusters and such that and d and push / from c to d. we then multiply into and distribute evidence from d. 
　it is worth noting that in several cases  conditional independencies obviate the evidence distribution step. this is a significant optimization  since message passing is by far the most expensive operation. this occurs  for example  when performing the prediction step  because  is an unobserved directed leaf of the graphical model and therefore does not impact the distributions of the other nodes   when observing a landmark for the first time  due to its uninformative prior   and in certain types of odometry updates. 
marginalizing out variables 
robotics 	1 assume again that we have a consistent junction tree t representing  as described in section 1  marginalizing out of p places a potential over the markov blanket of . because the junction tree must have a cluster that covers this new 

figure 1: illustration of variable contraction. clusters are circles and separators are rectangles; is shaded   a  i can be contracted from or c because they are leaves of cannot be contracted from because the running intersection property would be violated   b  contracting i from c removes it from c and s and marginalizes out of and 
potential  marginalizing out is not as simple as marginalizing it out of all the cluster potentials that bind it. 
	let  be the markov blanket of 	because t has the 
potential property  we are guaranteed that 
i.e.  that 's markov blanket is covered by the clusters containing  in fact  in the sequel this containment will often be strict equality.  moreover  because t has the running intersection property  all clusters containing i constitute a subtree of t  which we denote  by successively merging the clusters of into each other  we can obtain a new junction tree where i resides in a single cluster  marginalizing 
ui out of this junction tree is simple: we remove i from c* and marginalize out of .it is simple to check that this operation results in a consistent junction tree for 
1 	thinning the junction tree 
the updates described in the previous section can cause the clusters of the junction tree to grow; in particular  the merging of clusters required by marginalizations can cause the width of the junction tree to increase quickly. the complexity of message passing scales with the width of the junction tree  and therefore our goal is to define a  thinning  operation that reduces the width  see figure 1 : 
definition 1. let i  v appear in more than one cluster of the consistent junction tree t  let c be a leaf of ti  the subtree of t induced by i   and let s be the separator joining c to a variable contraction of i from c removes from c and s and marginalizes ux out of and .  c is merged into its subsuming neighbor if it becomes nonmaximal.  
we now consider some properties of variable contraction. 
proposition 1. variable contractions preserve consistency and the singly connected and running intersection properties. 
thus  the new junction tree is valid for some distribution  although perhaps not p: the potential property may be violated. variable contraction is local and efficient: it requires marginalizing a variable out of one cluster potential and one separator potential  which in the gaussian case can be accomplished in time using  1  and  1 . also  variable contraction is a general method of  thinning  a junction tree: 
proposition 1. in combination with cloning  variable contraction can reduce the width of any junction tree. 
the following proposition relates the original distribution and the distribution resulting from a variable contraction: 
proposition 1. let be the junction tree obtained from the variable contraction of definition 1. then minimizes the kullback-liebler divergence over all distributions in which 
alternatively  the probability distribution represented by has maximum likelihood  under the original junction tree's distribution  over all distributions in which  is conditionally independent of g i v e n t h u s   we can consider each variable contraction to be a maximum likelihood projection that cuts edges between and c - s. 
　to reduce the width of a given junction tree  we should choose the variable contraction that minimizes the approximation error  which we take to be the kullback-liebler divergence from the original to the approximate distribution  
　　　　　　this approximation error can be computed efficiently  as shown by the following result  cf.  kjaerulff  1  theorem 1  : 
proposition 1. let t be the junction tree obtained from the variable contraction of definition i. then 
		 1  
to compute the conditional mutual information  1  we need only the marginal . in a consistent junction tree  this marginal is simply   and therefore the approximation error of a variable contraction can be computed locally. when is a gaussian distribution  the computation is espe-
cially efficient: its cost depends only the dimension of ui. proposition 1. let c index a set of gaussian random vari-
 1  
　　　　are parameters of the potentials so we can simply extract from each the sub-blocks corresponding to ui and compute the difference of their log determinants. 
1 	thin junction tree filters for slam 
we have now assembled most of the machinery required to design a thin junction tree filter for the slam problem. all that remains is the logic to decide into which clusters new potentials are multiplied and also how variable contractions are employed to thin the junction tree. there are many possibilities; the method below presents a nice compromise between simplicity and performance. we then describe a refinement that can reduce the time complexity from linear to constant. 
1 	linear-time approximate filtering 
recall from section 1 that if the width of our junction tree is k  then it will require space and message passing will take time. i n s l a m s o w e 
can obtain a space filter with a t i m e filter operation by periodically thinning the junction tree so that its width remains bounded by a constant 

1 	robotics 

　we start with roll-up. when the robot state is marginalized out  we must merge all of the clusters in which it resides. in the worst case  can reside in all of the clusters  in which case our belief state will collapse to one large cluster. to prevent this  we iteratively contract   choosing the contraction that minimizes the error  1  each time  until it resides in only one cluster c. then  we perform the time update  which consists of multiplying the motion potential marginalizing out of multiplying the odometry potential into and distributing evidence from 
　when multiplying in a landmark measurement potential for a landmark that is currently in the model  we use the method of section 1  i.e.  we push  until it resides in a cluster with  this may increase the sizes of some clusters  but the subsequent contraction of  in the next rollup ensures this increase in cluster size is temporary.  we then multiply i n t o a n d distribute evidence from c. 
　if instead the landmark j has not previously been observed  we must add the new variable to the model. if the smallest cluster that contains  call it c  can admit another variable without violating the width limit we add  to c and multiply into if not  then we clone c to obtain d  con-
tract xt until it resides only in d  and thin d via a sequence of greedy optimal variable contractions. a cluster overlap parameter  governs the size to which d is thinned  and therefore how many variables reside in the separator s that joins it to c  since in this case if is small  d will admit more new landmark variables before another cloning is required; the trade-off is that its separator s will shrink  reducing the amount of information it can transmit. 
1 constant-time approximate filtering the linear time complexity of the filter above arises mainly because we pass messages to every cluster each time we distribute evidence from some cluster c. we can get a constanttime filter operation by employing a lazy message passing scheme  where we distribute evidence only to constantly many nearby clusters; the approximation is that the marginals of the remaining clusters will not be conditioned on the observation. this introduces minimal error when the observation is uninformative about distant variables; this occurs  e.g.  when the robot is mapping new territory. moreover  because we are still updating the charge correctly  this approximation is temporary: at any later time a full round of message passing  taking linear time  will yield the same estimate we would have obtained by passing all messages at every time step. 
　alternatively  we can interpolate between the linear-time update and this constant-time update by employing an adaptive message passing scheme in which messages are propagated only as long as they induce significant changes in the belief state. if we define  significant  sensibly  this scheme will take constant time when mapping new territory; when closing loops  it will take time linear in the length of the loop. 
　we measure the significance of a message  over the separator s by . the kullback-liebler divergence from the new separator marginal to the original separator marginal. in the gaussian case this is 
where importantly  the significance of evidence propagated from a cluster c  to another cluster c*  measured in this way  decreases with the distance between them in the junction tree  kjaerulff  1  theorem 1 . thus  if a message was not significant for a cluster  it need not continue the evidence distribution. 
1 experiments 
here we present a summary of our findings; the technical report contains more detail and further experiments. 
　we compared tjtf  the kalman filter  and fastslam on large-scale slam simulations in which a robot moves around in a square world that is populated with uniformly distributed point-landmarks. its motion and measurement models are all subject to significant noise and are linearized using the unscented transformation. we used two types of trajectories: a square loop  similar to that a robot mapping an indoor environment might travel  and a switchback trajectory  which could be used to map a large open area . noise and controls were determined in advance so the robot followed the desired path and each filter received identical observations. 
　figure 1 shows two examples of our simulations. the filters are evaluated by their computational cost  millions of floating point operations   localization error  the distance from the robot's position to the filter's estimate  and map error  the average distance from each landmark to the filter's estimate . tjtf was run with the width limit k - 1  the cluster overlap h - 1  and adaptive message passing with the significance threshold set at 1 nats; fastslam was run with 1 particles  as recommended in  montemerlo et a/.  1 . 
　we found that the estimation error of tjtf with maximum cluster sizes as small as 1 can be comparable with the kalman filter  and that it gets smaller as k increases. this indicates that the edges removed by tjtf indeed carry little information; it also suggests that the estimation error of tjtf will be at least competitive with that of seif  an approximate form of the kalman filter  and less than that of the submap approaches  which neglect long-distance correlations . 
we also found that tjtf is good at closing loops; in figure 
1 b  we can see the localization and mapping error of tjtf suddenly drop at t - 1  when the robot first reobserves its starting point; also evident is a sudden increase in the computational cost: the filter is choosing to update the entire map in linear time rather than using cheaper constant-time updates. we found that fastslam had difficulty closing large loops 
 notice its divergence in figure 1 b   and that its estimation error in general was larger than that of tjtf. 
　finally  using accurate counts of floating point operations  we found that tjtf can be as fast as fastslam  and that it becomes more efficient than the kalman filter when the map contains a few hundred or more landmarks. 
1 conclusion 
we believe thin junction tree filters are a promising approximation technique for dynamic probabilistic inference. first  they are flexible  in that they allow the practitioner to 

trade computational complexity for approximation accuracy by varying the width of the junction tree and the depth of 
robotics 	1 

figure 1: in  a  and  c : the solid line is the actual robot path; the dashed line is the integrated odometry; the dash-dotted line is the integrated control signal; circles are landmarks; dots are landmark observations  relative to the unknown actual viewpoint ; for clarity  only some of the 1 landmarks are plotted. in  b  and  d  the floating point counts arc time-averaged for clarity. 

evidence propagation. second  the error of each local approximation can be computed exactly  giving an important indication of how trustworthy the approximate estimates aic. finally  the tjtf approximation is context sensitive in that it is not chosen in advance; rather  the approximation is chosen adaptively to minimize the approximation error. 
   when applied to the slam problem  tjtf performs competitively with the exact filter  but with superior asymptoticspace and time complexity. interestingly  the approach presented here has significant connections to both the submap approach and seif. first  like sk1f  tjtf cuts  weak  edges from the graphical model to speed inference; however  in tjtf we can use exact inference over this approximate model  whereas seif must use approximate inference. second  the belief state of a tjtf has a natural interpretation as a coupled set of local maps  just as in the submap approaches. in particular  each cluster of the junction tree can be viewed as a submap. the tjtf formulation gives concrete semantics to the relationships between the maps  including how they must be updated  how consistency is maintained  and how the set of local maps can be determined online to minimize the approximation error subject to a complexity constraint. 
acknowledgements 
i gratefully acknowledge intel corporation for supporting this research via an intel research internship  as well as barbara engelhardt  kevin murphy  stuart russell  and sekhar tatikonda for valuable comments on a draft of this paper. 
