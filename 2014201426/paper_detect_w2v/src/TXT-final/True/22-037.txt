 
one of the major issues confronting case-based reasoning  cbr  is rapid retrieval of similar cases from a large case base. this paper describes three algorithms which address this problem. the first algorithm works with quantitative cases using a graphical paradigm where the hyperspace containing the cases is divided into smaller and smaller hypercubes. the retrieval time for this algorithm is 1 log n    where n is the number of cases. the second algorithm works on qualitative data by efficiently retrieving cases based on every necessary combination of case attributes. its retrieval time varies only with respect to the number of attributes. the third algorithm is a combination of the previous two and allows retrieval of cases consisting of both quantitative and qualitative information. the algorithms described in this paper are the first practical algorithms designed for case based retrieval on very large numbers of cases. the algorithms easily handle case bases containing millions of cases or more. 
1 introduction 
1 	problem statement 
rapid development of expert systems is hindered by the well-known knowledge acquisition bottleneck. case-based reasoning  cbr  overcomes this problem by representing knowledge as cases  where each case consists of a problem and its solution. a problem can be solved by remembering the solution to a similar problem and adjusting it for the current context  kolodner et al.  1j. if a case base contains enough cases  these adjustments can be very simple. however  retrieving the most similar cases from a very large case base can be time-consuming. our research has focused on developing efficient algorithms for case-based retrieval. 
       the fundamental principle of case-based reasoning is that problems are solved based on a memory of a prior similar situation. a similar situation is acquired through reminding of an individual event or an abstraction through long term memory associations  episodes   kolodner and riesbeck  1  schank  1 . 
       many issues are raised when discussing reminding  or retrieval  of prior episodes in a cbr system |owens  1 . 
cbr system developers have many responsibilities related to 
       *this research was supported in part by the defense advanced research projects agency under contract number daah1c-1  and in part by the air force weapons laboratory under contract number f1-1-c-1. 
creation of the case-base and the supporting retrieval functions. a developer must understand the breadth of knowledge which must be provided in each case  including case-specific attribute knowledge and general  or domain-specific  knowledge. a developer must understand the definition  and application of  the similarity-metric for the domain. a similarity-metric is the measuring scheme used to describe how a combination of new situation's attribute values correspond to a prior case  or case's  attribute values. a developer must understand what forms of retrieval can be utilized in the reminding process of the cbr system  and what scheme should be used for indexing or classification. in this paper we respond specifically to the problem of providing efficient retrieval algorithms for case-base reminding. 
       we have identified three types of retrieval  quantitative retrieval  qualitative retrieval  and a combination of the two. in quantitative retrieval  specifically directed at domains containing numeric representations of information   the values of a case's attributes are numeric and the similarity between any two cases is defined as the inverse of the distance between them. if two cases are similar  the distance between them is small. as the difference between their attribute values increases  their distance increases. 
       in qualitative retrieval  a case's attributes are qualitative values  such as colors or names. similarity is defined as the number of exact matches between attribute values  perhaps employing a weighting scheme to emphasize the importance of a particular attribute. it is important to note that in this situation it is generally not possible to simply perform a search using the most important attribute first. the closest case may not match on the most important attributes but on many less important ones  especially if the weights are similar for all attributes. 
       combined quantitative and qualitative retrieval is necessary for cases which have both quantitative and qualitative attributes. 
1 	problem solution 
the algorithm developed for quantitative retrieval makes use of a graphical paradigm. cases are represented as points in a hyperspace  where the dimensions of the space correspond to the attributes of the case. the space is recursively divided into hypercubes  squares for two dimensions  cubes for three  etc. . each cube contains zero or more points up to a small number. retrieval of the closest point to a point of interest is reduced to finding the hypercube that would contain the point of interest. 
       the qualitative retrieval algorithm uses a tree and hashing scheme to efficiently test every possible combination 
of attributes to determine exact matches. 	the combined retrieval algorithm is a simple combination of these two algorithms. 
	stottler  henke and king 	1 

1 	related work 
case-based reasoning can only be productive to a user if prior experiences can be retrieved efficiently to use in the retrieval process  martin  1 . martin goes on to state that the cbr system must continue to perform at an efficient level of retrieval while the case-base grows considerably. cora is described as a system that stores cases in an overlapping fashion which allows for reconstruction. this is in contrast to the general method of storing cases as individual portions of memory. our paper provides a view of retrieval on individualized cases and feature sets. 
        owens  1  presents the problem that case memory is not static and does not enjoy advanced ordering of features. in particular  cbr systems do not always have a static understanding of the measure of similarity. the similarity metric will change over time and growth of the case-base. owens concluded that discrimination trees provide adequate support if the case-base is static. 
        preparata and shamos  1  describe a well-known computational geometry algorithm for retrieving from a set of existing points  the closest point to an arbitrary point. this algorithm is based on the voronoi diagram which divides the space into volumes  one volume for each point. finding the closest point reduces to finding the volume which the arbitrary point falls into. this can be done in 1 log n   time  where n is the number of points. unfortunately  the amount of space required to store the volumes is impracticably large except in the two dimensional case. however  this retrieval algorithm was the starting point for the development of our quantitative retrieval algorithm. 
        corkill et a!.  1  classify the dimensions which define a blackboard space into ordered and enumerated. their ordered dimensions correspond exactly to our quantitative dimensions. their enumerated dimensions correspond with our qualitative dimensions except that our qualitative dimensions allow an infinite number of values while theirs assume a finite set. they also discuss retrieval based on dimensions with concern for efficiency of the operations. 
        samat  1  discusses uses of quadtrees especially as they relate to storage and retrieval of rectangles. our qualitative retrieval algorithm is based on a quadtree representation. 
1 	paper organization 
section 1 describes the three retrieval algorithms in detail. 
section 1 presents an analysis of the algorithms. 	section 1 provides empirical results and section 1 suggests directions 
for future research. 
1 description of the retrieval algorithms 
1 	voronoj-inspired quantitative retrieval algorithm 
the graphical nature of the voronoi retrieval algorithm prompted us to search for a graphical retrieval algorithm practical for more than two dimensions. the resulting algorithm is termed the voronoi-inspired  vi  quantitative retrieval algorithm. the vi algorithm performs a preprocessing step  in which a hypercube containing all the data points or cases is recursively divided into smaller hypercubes until no hypercube contains more than a certain number  m  of points. note that this means that a hypercube may be empty  but one of its neighbors must be nonempty. figure 1 shows an example of hypercube sub-division where the maximum number of points per cube is two  this value would be unrealistically low from an efficiency standpoint . the retrieval of the closest point  most similar case  consists first of finding the hypercube that contains the test point. 
1 	real-time and high performance 

then  points in that hypercube and  in some circumstances  bordering cubes must be examined for proximity. 
       the integrity of the algorithm depends on the definition of distance between points. the distance formulation should treat all dimensions monotonically  symmetrically and identically. the distance between points depends on the differences between their attribute values. treating dimensions monotonically means that as these differences increase  the distance increases. the magnitude of the increase cannot depend on the sign of the difference or on the dimension. both the geometric definition of distance  square root of the sum of the squares  and the sum of the absolute values of the differences satisfy these criteria. 
       in many applications  a weighted sum will be necessary in a definition for distance. however  a weighted sum does not treat all dimensions identically. in order for the vi algorithm to be applicable  all the weights must be equal. this is handled by simply multiplying all the attribute values of a point by their weights before the point space is preprocessed into smaller and smaller hypercubes. 
in one of our applications  surver iii  king et al.  
1   it was clear that differences were not as important as multiplicative factors. for example  a case with a peak blast overpressure of 1 mpa was closer to a case with a peak blast overpressure of 1 mpa than to a case with 1 mpa. the differences are 1 mpa and 1 mpa respectively  but the multiplicative factors are 1 and 1. therefore  surver iii defined distance as the weighted sum of the multiplicative factors. the multiplicative factors are not symmetric  distance increases faster as a dimension goes toward 1  so the vi algorithm was not direcdy applicable. however  by taking the log of the dimension before the preprocessing step  and 
multiplying by the weights   we were able to achieve the desired results. 
1 	tree-hash qualitative retrieval algorithm 
the tree-hash qualitative retrieval algorithm was designed for retrieval of cases whose attributes have qualitative values. retrieval of a qualitative case is very efficient if we know a priori which dimensions or attributes of the current case will match attributes of cases in the case base. however  this information is never available ahead of time. the tree-hash algorithm overcomes this problem by maintaining multiple 

pointers to cases - one for each possible combination of the dimensions. for example  in the two dimensional case with dimensions dl and d1  the possible dimension combinations are  dl  d1  dld1 . the algorithm attempts a retrieval based on each of these combinations and computes a similarity measure. in our example  we first perform a retrieve based on dl  meaning that dl is the only dimension that must produce an exact match. the same is done for d1. retrieval based on dld1 indicates that both dimensions must match the dimensions of a case in the case base. the algorithm is very fast when the number of dimensions is low. retrieval time does not increase with the number of cases if a hashing scheme is used. various versions of the algorithm are described in detail in the following sections. 
1.1 	assumptions and definitions 
       the tree-hash algorithm assumes that there are a large number of cases in the case base and a possibly infinite set of possible attribute values. the number of dimensions is relatively small and finite. best match of dimensions is defined as the weighted number of matches of the qualitative values. in the case of ties for best match  any best match is acceptable. this requirement is necessary because the number of ties for best match can be an extremely large number of cases  especially if all dimensions are weighted equally. in the discussion below  the following definitions hold: 

       to provide efficient lookup  a unique pointer is created for each case and placed in a hash table exp 1jd  - 1 times  once for each possible combination of attributes  ignoring order. for each case  every node in the tree is visited. when a node is visited  the case is hashed using the dimensions represented by the node. the result of this process is a large hash table which encompasses all the cases  organized by every possible ordering of dimensions. retrieval can now simply consist of visiting the nodes of the tree and performing a hash lookup based on the dimensions of the node. 
1.1 	naive retrieval 
       the simplest retrieval scheme involves traversal of the entire tree in search of a case most similar to the case of current interest. at every node  an attempt is made to retrieve a case and a record is kept of its measure of similarity to the current case. by comparing the similarity measures from the different nodes  the best match can be found. 
1.1 	tree traversal 
       instead of visiting every node in the tree  it is more efficient to search the tree in a specified order to eliminate many of the hash retrievals. traversal of the tree begins at the root. at each node  an attempt is made to hash retrieve a case based on the dimensions of that node. if the retrieval is successful  we proceed down the left branch. if unsuccessful  we proceed down the right branch. this process is continued until a leaf node is reached. at this point  we store the similarity measure from the node of the last successful retrieval. we then backtrack to the last successful node and follow its right son and proceed as before. this process is repeated until there are no successful nodes left to backtrack to. we then compare the stored similarity measures and select the best one. using this scheme  a number of unpromising nodes never have to be visited. 
1.1 	smart tree traversal 
       while the tree traversal algorithm given above avoids visiting some of the nodes of the tree  it can be improved. this is accomplished by computing and storing a score for each node of the tree during the preprocessing stage. the highest possible score at any node is the maximum of the possible scores of its subnodes. at a leaf  the score is computed  for instance  as the weighted sum of the number of dimensions. tree traversal proceeds as in the previous algorithm  except that at each visited node  we compare the similarity measure of the retrieved case to the score at that node. if the similarity measure is equal to or better than the score at the current node  then the subtree under that node does not have to be traversed. this further reduces the number of nodes that have to be examined. 
1 	combined quantitative and qualitative retrieval algorithm 
to retrieve similar cases when the case base contains both qualitative and quantitative dimensions  we use an algorithm which is a straight-forward combination of the two previously described algorithms. the dimensions of the case must first be divided into qualitative and quantitative groups. a tree should be generated as described above using only the qualitative dimensions. at each node of the tree  we define pointers corresponding to each possible value of the qualitative dimensions at that node. for example  if the values of dimension dl are {a  b  and the values of dimension d1 are {c  d}  we would define a set of pointers for the combinations 
{ac  ad  be  bd . each of the pointers is directed toward that 
	stottler  henke and king 	1 

segment of the case base which has matching dimension values. each segment is then divided along its quantitative dimensions according to the vi algorithm. 
       to retrieve the most similar case  every node of the tree is visited. at each node  retrieval is first attempted based on the qualitative dimensions represented by the node. when a matching case exists for these dimensions  we perform a vi retrieval on the segment of the case base with corresponding dimension values. we must also consider the situation where none of the qualitative dimensions match. a similarity score is computed for the node and retrieval proceeds to another node. the case with the highest similarity score is the closest match. 
1 analysis of the retrieval algorithms 
1 	analysis of the vi algorithm 
1.1 	time and space requirements 
       a best case analysis of the vi algorithm indicates that the retrieval time can be expressed as: 
kl * log  n/m  + k1 * m  where 
n = the number of cases  m = the maximum points allowed per atomic 
hypercube  kl and k1 are constants 
the first term is due to the search required to identify the proper atomic hypercube and the second term is due to a search through the points in that and nearby cubes. a 
probabilistic analysis using random cases shows that the mean retrieval time is the same expression as that for the best case  but with larger constants. 
       the worst case retrieval time can be made arbitrarily large. this is done by using a very tight cluster of cases with just a few cases very far away so that the largest hypercube is arbitrarily larger than the cluster containing most of the cases. by making this outer cube larger  more divisions are necessary to get a cube that immediately surrounds the cluster. 
       the time required to add a new case to the case base is proportional to the time to retrieve a similar case. the preprocessing time  which consists of adding n points to the case-base  is 1 n*log n/m  . the required space is o n . 
1.1 	advantages and disadvantages 
       the obvious advantage of the vi algorithm is that it is very fast compared to other methods. it is also surprisingly applicable when combined with preprocessing steps which manipulate the values of the cases1 attributes through multiplication by weights  computation of logarithms  etc. the primary disadvantage of the algorithm is that it treats similarity identically throughout the case base. a change in the definition of similarity requires relatively slow preprocessing before rapid retrieval can again be performed. 
1 	analysis of the tree-hash algorithm 
1.1 	time and space analysis 
       the naive retrieval strategy requires a hash retrieval at every node. there are exp 1 d  - 1 nodes in the tree and the hash retrieval takes constant time  resulting in a retrieval time of kl *  exp 1 d  - 1   where d is the number of dimensions and kl is a constant. 
for tree retrieval  the worst case retrieval time is still 
1 exp 1 d     but the best case is o d . computation of the 
1 	real-time and high performance 
average retrieval time requires additional assumptions and a probabilistic analysis. 
       for smart tree retrieval  the worst case retrieval time is still 1 exp 1 d  . the best case retrieval time is a constant. the average retrieval time can be calculated from additional assumptions and probability theory. 
to add a new case requires exp 1 d  - 1 hash inserts. 
therefore  to preprocess all of the n cases  add a new case n times  requires time of 1 n*exp 1 d  . the space required is also 1 n*exp 1 d . 
1.1 	advantages and disadvantages 
       the biggest advantage of the tree-hash algorithm is its speed. the retrieval time does not increase as the number of cases increases. a change to the importance of the dimensions does not require extensive re-preprocessing as long as the same retrieval dimensions are used. only the tree itself must be regenerated  which can be done very rapidly. 
       the tree-hash algorithm has a couple drawbacks. the retrieval time increases exponentially with the number of dimensions used for retrieval. for practical use of the algorithm  the number of dimensions is kept to about ten. the definition of similarity must be based on exact matches of the attribute values. this requirement limits the applicability of the algorithm. however  this can be partially overcome by defining new retrieval dimensions for the case base. for example  in a case base containing information about movies  we may have a dimension called  country of origin . while we recognize that movies from the united states  england and australia are in some sense more similar to each other than to a movie from france  the similarity definition cannot capture this. we  therefore  add a new dimension called  language spoken  and now have a means of correlating movies in this manner. if new dimensions are added to the case base  the preprocessing step must be performed again. 
1 	analysis of combined retrieval algorithm 
the worst case retrieval time for the combined retrieval algorithm is: 
kl * exp 1 d  *  k1 * log n/m  + k1 * m   where 
d = the number of qualitative dimensions  n = the number of cases  
m is the maximum allowable points per hypercube  k l   k1 and k1 are constants 
the first term results from searching through the tree and the second term results from performing the vi algorithm on the segment of the case base at each tree node. note that in most circumstances the algorithm will perform much better than the worst case because every segment pointed to from a tree node will contain only a small fraction of the cases. 
1 empirical results 
1 	quantitative retrieval empirical results 
a uniform random number generator was used to generate points in a hypercube of two dimensions  a square box . 1  1  and 1 points were used for three different sets of experiments. a value of 1 was used for m  the maximum number of points allowed before a box was quartered . this was a deliberate attempt to make the algorithm perform badly by exploiting small variations in the point distribution. 
       the minimum average depth of nesting for all atomic boxes is the log of n/m to the base 1. when m = 1  this produces 1  1  1  and 1 for n = 1  1  1  and 1 

respectively. our experiments suggest that the actual depth of an atomic box is very rarely greater than one more than this minimum number. the best case results hold within a constant factor. the retrieval time is fast enough to allow disk space to be used to store the indexing  thus keeping local memory from being a limitation. a random sampling of the boxes revealed that the boxes are only empty less than 1% of the time  despite the low value of m. 
       from the experiments  it is clear that the vi algorithm will work extremely well  when it is applicable. a question that remains to be answered through development of case based applications  is how often real data approximates random data and how limiting the restrictions placed on the distance definition are. 
       the vi algorithm has been successfully used in a practical instance of case based reasoning involving purely quantitative data. the algorithm was applied after a 
       preprocessing step in which the logs of the attributes were taken and those results were multiplied by selected weights. one attribute of the problem that tended to make the vi algorithm applicable  was that the ultimate end users were novices and would not want to frequently redefine the similarity expression themselves. 
1 	tree-hash algorithm empirical results 
we are currently working on a manufacturing proposal case base consisting of fifty thousand cases. it appears at this early stage that the tree-hash algorithm is applicable. the number of retrieval dimensions is approximately six. consequently  the exponential term in the retrieval time expression is not worrisome. 
1 future directions 
the three retrieval algorithms presented here are part of a 
darpa sponsored case based reasoning shell currently under development. as part of the shell  these algorithms will compete with other retrieval algorithms for use by developers. as applications are implemented in this shell  the applicability and usefulness of these algorithms will be further tested. 
