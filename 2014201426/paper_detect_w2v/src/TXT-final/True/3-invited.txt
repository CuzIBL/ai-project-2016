
if neurons are treated as latent variables  our visual systems are non-linear  densely-connected graphical models containing billions of variables and thousands of billions of parameters. current algorithms would have difficulty learning a graphical model of this scale. starting with an algorithm that has difficulty learning more than a few thousand parameters  i describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. the latest member of this series can learn deep  multi-layer belief nets quite rapidly. it turns a generic network with three hidden layers and 1 million connections into a very good generative model of handwritten digits. after learning  the model gives classification performance that is comparable to the best discriminative methods.
1 introduction
our perceptual systems make sense of the visual input using a neural network that contains about 1 synapses. there has been much debate about whether our perceptual abilities should be attributed to a few million generations of blind evolution or to a few hundred million seconds of visual experience. evolutionary search suffers from an information bottleneck because fitness is a scalar  so my bet is that the main contribution of evolution was to endow us with a learning algorithm that could make use of high-dimensional gradient vectors. these vectors provide millions of bits of information every second thus allowing us to perform a much larger search in one lifetime than evolution could perform in our entire evolutionary history.
¡¡so what is this magic learning algorithm  i have been involved in attempts to answer this question using undirected graphical models  hinton and sejnowski  1   directed graphical models  hinton et al.  1   or no graphical model at all  rumelhart et al.  1 . these attempts have failed as scientific theories of how the brain learns because they simply do not work well enough. they have  however  produced two neat tricks  one for learning undirected models and one for learning directed models. in this paper  i describe some recent work in collaboration with simon osindero and yeewhye teh that combines these two tricks in a surprising way to learn a hybrid generative model that was first proposed by yee-whye teh. in this model  the top two layers form an undirected associative memory. the remaining layers form a directed acyclic graph that converts the representation in the associative memory into observable variables such as the pixels of an image. in addition to working well  this hybrid model has some other nice features:
1. the learning finds a fairly good model quickly even in deep directed networks with millions of parameters and many hidden layers. for optimal performance  however  a slower fine-tuning phase is required.
1. the learning algorithm builds a full generative model of the data which makes it easy to see what the distributed representations in the deeper layers have in mind.
1. the inference required for forming a percept is both fast and accurate.
1. the learning algorithm is unsupervised. for labeled data  it learns a model that generates both the label and the data.
1. the learning is local: adjustments to a synapse strength depend only on the states of the pre-synaptic and postsynaptic neuron.
1. the communication is simple: neurons only need to communicate their stochastic binary states.
¡¡section 1 describes a simple learning algorithm for undirected  densely-connected  networks composed of stochastic binary variables some of which are unobserved. section 1 shows how to make this simple algorithm efficient by restricting the architecture of the network. section 1 introduces the idea of variational approximations for learning directed graphical models in which correct inference is intractable and describes the  wake-sleep  algorithm that makes use of a variational approximation in a multi-layer  directed network of stochastic binary variables. all of these sections can be safely ignored by people already familiar with these ideas.
¡¡section 1 introduces the novel idea of a  complementary  prior. complementary priors seem about as probable as father christmas because  by definition  they exactly cancel the  explaining away  phenomenon that makes inference difficult in directed models. section 1 includes a simple example of a
¡¡complementary prior and shows the equivalence between restricted boltzmann machines and infinite directed networks with tied weights.
¡¡section 1 introduces a fast  greedy learning algorithm for constructing multi-layer directed networks one layer at a time. using a variational bound it shows that as each new layer is added  the overall generative model improves. the greedy algorithm resembles boosting in its repeated use of the same  weak  learner  but instead of re-weighting each datavector to ensure that the next step learns something new  it re-represents it. curiously  the weak learner that is used to construct deep directed nets is itself an undirected graphical model.
¡¡section 1 shows how the weights produced by the efficient greedy algorithm can be fine-tuned using the  up-down  algorithm which is a contrastive version of the wake-sleep algorithm.
¡¡section 1 shows the pattern recognition performance of a network with three hidden layers and about 1 million weights on the standard mnist set of handwritten digits. when no knowledge of geometry is provided and there is no special preprocessing  the generalization performance of the network is 1% errors on the official test set. this beats the 1% achieved by the best back-propagation nets when they are not hand-crafted for this particular application  and it is quite close to the 1% or 1% achieved by the best support vector machines.
¡¡finally  section 1 shows what happens in the mind of the model when it is running without being constrained by visual input. the network has a full generative model  so it is easy to look into its mind - we simply generate an image from its high-level representations.
¡¡throughout the paper  we will consider nets composed of stochastic binary variables but the ideas can be generalized to other models in which the log probability of a variable is an additive function of the states of its directly-connected neighbours.
1 the boltzmann machine learning algorithm
a boltzmann machine  hinton and sejnowski  1  is a network of stochastic binary units with symmetric connections. it is usually divided into a set of  visible  units which can have data-vectors clamped on them  and a set of hidden units that act as latent variables  see figure 1 . each unit  i  adopts its  on  state with a probability that is a logistic function of the inputs it receives from other units  j:
		 1 
where bi is the bias of unit i  and wij is the weight on the symmetric connection between i and j. the weights and biases of a boltzmann machine define an energy function over global configurations  i.e. binary state vectors  of the net. using ¦Á as an index over configurations of the visible units  and ¦Â for configurations of the hidden units:
	e¦Á¦Â =  xbis¦Á¦Âi	  xs¦Á¦Âi s¦Á¦Âj wij	 1 
	i	i j

figure 1: a boltzmann machine composed of stochastic binary units with symmetric connections. when data is clamped on the visible units  a simple stochastic updating rule infers a configuration of states of the hidden units that is a good interpretation of the data. if no data is clamped and the same updating rule is used for all of the units  the network generates visible vectors from its model.
where s¦Á¦Âi is the binary state of unit i in the global binary configuration ¦Á¦Â. if units are chosen at random and their binary states are updated using the stochastic activation rule in eq. 1 the boltzmann machine will eventually converge to a stationary probability distribution in which the probability of finding it in any global state is determined by the energy of that state relative to the energies of all the other global states:
		 1 
if we sum over all configurations of the hidden units  we get the probability  at thermal equilibrium  of finding the visible units in configuration ¦Á
		 1 
¡¡a boltzmann machine can be viewed as a generative model that assigns a probability  via eq. 1  to each possible binary state vector over its visible units. by changing the weights and biases  we can change the probability that the model assigns to each possible visible vector. so we can model a set of training vectors by adjusting the weights and biases to maximize the sum of their log probabilities. a nice feature of boltzmann machines is that the maximum likelihood learning rule for the weights is both simple and local. we can learn a locally optimal set of weights by collecting two sets of statistics:
  in the positive phase we clamp a training vector on the visible units and then repeatedly update the hidden units in random order using eq. 1. once the distribution over hidden configurations has reached  thermal  equilibrium with the clamped data-vector  we sample the hidden state and record which pairs of units are both on. by repeating this for the entire training set  we can compute  sisj +  the average correlation between i and j when data is clamped on the visible units.
  in the negative phase we let the network run freely with the visible units unclamped. their states are updated in just the same way as the hidden units. once the distribution over global configurations has reached equilibrium  we sample the states of all the units and record which pairs are both on. by repeating this many times  we can compute  sisj    the average correlation between i and j when the network is running freely and therefore producing samples from its generative model.
¡¡we can then follow the gradient of the log probability of the data by using the simple rule
		 1 
where  is a learning rate. it is very surprising that the learning rule is so simple because the gradient of the log likelihood with respect to one weight depends in complicated ways on all the other weights. in the back-propagation algorithm  these dependencies are computed explicitly in the backward pass. in the boltzmann machine they show up as the difference between the local correlations in the positive and negative phases.
¡¡unfortunately  the simplicity and generality of the boltzmann machine learning algorithm come at a price. it can take a very long time for the network to settle to thermal equilibrium  especially in the negative phase when it is unconstrained by data but needs to be highly multi-modal. also  the gradient used for learning is very noisy because it is the difference of two noisy expectations. these problems make the general form of the algorithm impractical for large networks with many hidden units.
1 restricted boltzmann machines and contrastive divergence learning
if we are willing to restrict the architecture of a boltzmann machine by not allowing connections between hidden units  the positive phase no longer requires any settling. with a data-vector clamped on the visible units  the hidden units are all conditionally independent  so we can apply the update rule in eq. 1 to all the units at the same time to get an unbiased sample from the posterior distribution over hidden configurations. this makes it easy to measure the first correlation in eq. 1.
¡¡if we also prohibit connections between visible units  we can update all of the visible units in parallel given a hidden configuration. so the second correlation in eq. 1 can be found by alternating gibbs sampling as shown in figure 1. unfortunately we may need to run the alternating gibbs sampling for a long time before the markov chain converges to the equilibrium distribution. fortunately  if we start the markov chain at the data distribution  learning still works well even if we only run the chain for a few steps  hinton  1 . this gives an efficient learning rule:
		 1 

figure 1: this depicts a markov chain that uses alternating gibbs sampling. in one full step of gibbs sampling  the hidden units in the top layer are all updated in parallel by applying eq. 1 to the inputs received from the the current states of the visible units in the bottom layer  then the visible units are all updated in parallel given the current hidden states. the chain is initialized by setting the binary states of the visible units to be the same as a data-vector. the correlations in the activities of a visible and a hidden unit are measured after the first update of the hidden units and again at the end of the chain. the difference of these two correlations provides the learning signal for updating the weight on the connection.
where the superscript 1 indicates that the correlation is measured at the start of the chain with a data-vector clamped on the visible units and n indicates that the correlation is measured after n full steps of gibbs sampling. the angle brackets denote expectations over both the choice of data-vector and the stochastic updating used in the gibbs sampling. this learning rule does not follow the gradient of the log likelihood  but it does closely approximate the gradient of another function  contrastive divergence  which is the difference of two kullback-leibler divergences  hinton  1 . intuitively  it is not necessary to run the chain to equilibrium in order to see how the data distribution is being systematically distorted by the model. if we just run the chain for a few steps and then lower the energy of the data and raise the energy of whichever configuration the chain preferred to the data  we will make the model more likely to generate the data and less likely to generate the alternatives. an empirical investigation of the relationship between the maximum likelihood and the contrastive divergence learning rules can be found in  carreira-perpinan and hinton  1 .
¡¡contrastive divergence learning in a restricted boltzmann machine is efficient enough to be practical  mayraz and hinton  1 . variations that use real-valued units and different sampling schemes are described in  teh et al.  1  and have been quite successful for modeling the formation of topographic maps  welling et al.  1  and for denoising natural images  roth and black  1 . however  it appears that the efficiency has been bought at a high price: it is not possible to have deep  multilayer nets because these take far too long even to reach conditional equilibrium with a clamped data-vector. also  nets with symmetric connections do not give a causal model in which the data is explained in terms of underlying causes.
¡¡the next section describes a simple learning algorithm for an apparently quite different type of network that uses directed connections. this learning algorithm also has deficiencies  but it can be combined with contrastive divergence learning in a surprising way to produce an algorithm that works much better and is significantly more similar to the real brain.
1 variational learning
inference in directed graphical models that use non-linear  distributed representations is difficult because of a phenomenon called  explaining away   pearl  1  which creates dependencies between hidden variables. this is illustrated in figure 1. radford neal  neal  1  showed that it was possible to use gibbs sampling to perform inference correctly in multilayer directed networks composed of the same type of binary stochastic units as are used in boltzmann machines. the communication required is more complicated than in a boltzmann machine because in addition to seeing the binary states of its ancestors and descendants  a unit needs to see the probability that each of its descendants would be turned on by the current states of all that descendant's ancestors. however  once we have a sample from the posterior distribution over configurations of the hidden units  the maximum likelihood learning rule for updating the directed connection from j to i is very simple:
		 1 
where  is a learning rate and s i is the probability that i would be turned on by the current states of all its ancestors. there is no need for a  negative phase  because directed models do not require the awkward normalizing term that shows up in the denominator of eq. 1. radford neal showed that logistic belief nets are somewhat easier to learn than boltzmann machines  but the use of gibbs sampling to get samples from the posterior distribution still makes it very tedious to learn large  deep nets.
¡¡rich zemel and i realised that it might still be possible to learn a belief net that contained a layer of binary stochastic hidden units even when the cost of computing the posterior distribution  or sampling from it  was prohibitive. instead of trying to perform maximum likelihood learning  we adopted a coding perspective and attempted to learn a model that would minimize the description length of the data  zemel and hinton  1 . the idea is that the sender and receiver both have access to the model and instead of communicating a data-vector directly  the sender first communicates a hidden configuration of the model. this costs some bits  but it also gives the receiver a good idea of what data to expect. given these expectations  the data-vector can be communicated more cheaply1. so it appears that the expected cost of communicating a data-vector is:

1
   shannon showed that  using an efficient block coding scheme  the cost of communicating a discrete value to a receiver is asymptotically equal to the negative log probability of that value under a probability distribution that has already been agreed upon by the sender and the receiver.

figure 1: a simple logistic belief net containing two independent  rare causes that become highly anti-correlated when we observe the house jumping. the bias of  1 on the earthquake node means that  in the absence of any observation  this node is e1 times more likely to be off than on. if the earthquake node is on and the truck node is off  the jump node has a total input of 1 which means that it has an even chance of being on. this is a much better explanation of the observation that the house jumped than the odds of e 1 which apply if neither of the hidden causes is active. but it is wasteful to turn on both hidden causes to explain the observation because the odds on them both happening are e 1 ¡Á e 1 = e 1.
where y¦Á is a configuration of the hidden units  q y¦Á|x  is the probability that the sender will choose to use y¦Á in order to communicate data-vector x  logp y¦Á  is the cost of communicating y¦Á to a receiver who already has the model and logp x|y¦Á  is the cost of communicating x to a receiver who has both the model and the hidden configuration y¦Á. rich soon discovered that it was better to minimize a different function and we eventually understood why.
¡¡suppose there are two different hidden configurations that give the same communication cost for a data-vector. the sender can flip a coin to decide which one to use. after receiving the data-vector  the receiver can figure out what choices were available to the sender and he can therefore figure out the value of the random bit produced by the coin. so if there are two equally good hidden configurations  the sender can communicate one additional bit from a random bit stream by his choice of configuration. in general  the number of extra bits is equal to the entropy of the sender's distribution across hidden configurations. all of these extra bits can be used to communicate some other bit string  so we need to subtract them from the communication cost of the data-vector:
c x 	=	 xq y¦Á|x  logp y¦Á  + logp x|y¦Á   ¦Á
	   xq y¦Á|x logq y¦Á|x !	 1 
¦Á
¡¡if the sender picks hidden configurations from their true posterior distribution  the communication cost is minimized and is equal to the negative log probability of the data-vector under the model. but if it is hard to sample from the true posterior  the sender can use any other distribution he chooses. the communication cost goes up  but it is still perfectly welldefined. the sender could  for example  insist on using a factorial distribution in which the states of the hidden units are chosen independently. the communication cost will then be an upper bound on the negative log probability of the data under the model and by minimizing the communication cost we will either push down the negative log probability of the data or we will make the bound tighter. the looseness of the bound is just the kullback-liebler divergence between the distribution used by the sender and the true posterior  p y¦Á|x .

¡¡the use of an approximate posterior distribution to bound logp x   neal and hinton  1  is now a standard way to learn belief nets in which inference is intractable  jordan et al.  1 .
1 the wake-sleep algorithm
a simple way to make use of variational learning in a multilayer logistic belief net is to use a set of  recognition  connections that compute a factorial approximation to the posterior distribution in one layer when given the binary states of the units in the layer below  hinton et al.  1 . these recognition connections will not  in general  have the same values as the corresponding generative connections. given a set of recognition weights  it is easy to update the generative weights to follow the gradient of the description cost in eq. 1. we use a data-vector to set the states of the visible units and then we use the recognition connections to compute a probability for each unit in the first hidden layer. then we use these probabilities to pick independent binary states for all the units in that layer. this is repeated for each hidden layer in turn until we have a sample from the approximate posterior. given this sample the learning rule for the generative  top-down weights is given by eq. 1. this is the  wake  phase of the wake-sleep algorithm.
¡¡the  correct  way to learn the recognition weights is to follow the derivative of the cost in eq. 1. the recognition weights only affect the q terms; they do not affect the p terms. however  the derivatives w.r.t the q terms are messy because q comes outside the log. so we make a further approximation. in the  sleep  phase  we perform an ancestral pass to generate a sample from the generative model. starting at the top layer  we pick binary states for the units from their independent priors. then we pick states for the units in each lower layer using the probabilities computed by applying the generative weights to the states in the layer above. once we have completed an ancestral pass  we have both a visible vector and its true hidden causes. so we can adjust the recognition weights to be better at recovering the hidden causes from the states of the units in the layer below:
		 1 
where s j is the probability that j would be turned on by the current states of all its descendants.
¡¡the wake-sleep algorithm works quite well  but the sleep phase is not exactly following the gradient of the variational bound. as a result  it does the wrong thing when a datavector can be generated by two quite different hidden configurations: instead of picking one of these hidden configurations and sticking with it  it averages the configurations to produce a vague factorial distribution that gives significant probability to many poor configurations.
1 complementary priors
the phenomenon of explaining away makes inference difficult in directed networks. it is comforting that we can still improve the parameters even when inference is done incorrectly  but it would be much better to find a way of eliminating explaining away altogether  even in models whose hidden variables have highly correlated effects on the visible variables. most people who use directed graphical models regard this as impossible.
¡¡in a logistic belief net with one hidden layer  the prior distribution over the hidden variables is factorial because their binary states are chosen independently when the model is used to generate data. the non-independence in the posterior distribution is created by the likelihood term coming from the data. perhaps we could eliminate explaining away in the first hidden layer by using extra hidden layers to create a  complementary  prior that has exactly the opposite correlations to those in the likelihood term. then  when the likelihood term is multiplied by the prior  we will get a posterior that is exactly factorial. this seems pretty implausible  but figure 1 shows a simple example of a logistic belief net with replicated weights in which the priors are complementary at every hidden layer. this net has some interesting properties.
1 an infinite directed model with tied weights
we can generate data from the infinite directed net by starting with a random configuration at an infinitely deep hidden layer and then performing an ancestral pass all the way down to the visible variables. clearly  the distribution that we will get over the visible variables is exactly the same as the distribution produced by the markov chain in figure 1 so the infinite directed net with tied weights is equivalent to a restricted boltzmann machine.1
¡¡we can sample from the true posterior distribution over all of the hidden layers of the infinite directed net by starting with a data vector on the visible units and then using the transposed weight matrices to infer the factorial distributions over each hidden layer in turn. at each hidden layer we sample from the factorial posterior before computing the factorial posterior for the layer above. this is exactly the same process as starting a restricted boltzmann machine at the data and letting it settle to equilibrium. it is also exactly the same as the inference procedure used in the wake-sleep algorithm  but in this net it gives unbiased samples because the complementary prior at each layer ensures that the posterior distribution really is factorial.
¡¡since we can easily sample from the true posterior  it is easy to learn the weights in the infinite directed net. let us start by computing the derivative for a generative weight  wij1  from a unit j in layer h1 to unit i in layer v1  see figure 1 . in a logistic belief net  the maximum likelihood learning rule for a single data-vector  x  is:
		 1 
where   ¡¤   denotes an average over the sampled states and s 1i is the probability that unit i would be turned on if the visible vector was stochastically reconstructed from the sampled hidden states. computing the posterior distribution over the second hidden layer  v1  from the sampled binary states in the first hidden layer  h1  is exactly the same process as reconstructing the data  so s1i is a sample from s 1i and the learning rule becomes:
		 1 
since the weights are replicated  the full derivative for a generative weight is obtained by summing the derivatives of the generative weights between all pairs of layers:

¡¡all of the vertically aligned terms cancel leaving the boltzmann machine learning rule of eq. 1. the same weights are also used for inference and one might expect this to contribute extra derivatives. fortunately  all of these derivatives are zero. the inference is exact so  to first order  small changes in the inferred posteriors make no change in the log probability of the data. the only reason the recognition weights ever change is because they are tied to the generative weights.
1 a greedy learning algorithm for transforming representations
an efficient way to learn a complicated model is to combine a set of simpler models that are learned sequentially. to force each model in the sequence to learn something different from the previous models  the data is modified in some way after each model has been learned. in boosting  freund  1   each model in the sequence is trained on re-weighted data that emphasizes the cases that the preceding models got wrong. in one version of principal components analysis  the variance in a modeled direction is removed thus forcing the next modeled direction to lie in the orthogonal subspace. in projection pursuit  friedman and stuetzle  1   the data is transformed by nonlinearly distorting one direction in the data-space to remove all non-gaussianity in that direction. the idea behind our greedy algorithm is to allow each model in the sequence to receive a different representation of the data. the model

figure 1: an infinite logistic belief net with tied weights.
performs a non-linear transformation on its input vectors and produces as output the vectors that will be used as input for the next model in the sequence.
¡¡figure 1 shows a multilayer generative model in which the top two layers interact via undirected connections and all of the other connections are directed. there are no intra-layer connections and  to simplify the analysis  all layers have the same number of units. it is possible to learn sensible  though not optimal  values for the parameters w1 by assuming that the parameters between higher layers will be used to construct a complimentary prior for w1. this is equivalent to assuming that all of the weight matrices are constrained to be equal. the task of learning w1 under this assumption reduces to the task of learning an rbm and although this is still difficult  good approximate solutions can be found rapidly by minimizing contrastive divergence  hinton  1 . once w1 has been learned  the data can be mapped through to create higherlevel  data  at the first hidden layer.
¡¡if the rbm is a perfect model of the original data  the higher-level  data  will already be modeled perfectly by the higher-level weight matrices. generally  however  the rbm will not be able to model the original data perfectly and we can make the generative model better using the following greedy algorithm:
1. learn w1 assuming all the weight matrices are tied.
1. freeze w1 and commit ourselves to using to infer factorial approximate posterior distributions over the states of the variables in the first hidden layer.
1. keeping all the higher weight matrices tied to each other  but untied from w1  learn an rbm model of the

figure 1: a hybrid network. the top two layers have undirected connections and form an associative memory. the layers below have directed  top-down  generative connections that can be used to map a state of the associative memory to an image. there are also directed  bottom-up  recognition connections that are used to infer a factorial representation in one layer from the binary activities in the layer below. in the greedy initial learning the recognition connections are tied to the generative connections.
higher-level  data  that is produced by using  to transform the original data.
¡¡if this greedy algorithm changes the higher-level weight matrices  it is guaranteed to improve the generative model. the log probability of a single data-vector  x  under the multilayer generative model is bounded by
logp x 	¡Ý xq y¦Á|x  logp y¦Á  + logp x|y¦Á   ¦Á
¡¡¡¡¡¡¡¡¡¡¡¡¡¡ xq y¦Á|x logq y¦Á|x  ¦Á where y¦Á is a binary configuration of the units in the first hidden layer  p y¦Á  is the prior probability of y¦Á under the model defined by the weights above h1 and q ¡¤|x  is any probability distribution over y. the bound becomes an equality if and only if q ¡¤|x  is the true posterior distribution.
¡¡when all of the weight matrices are tied together  the factorial distribution over h1 produced by applying  to a data-vector is the true posterior distribution  so at step 1 of the greedy algorithm logp x  is equal to the bound. step 1 freezes both q ¡¤|x  and p x|y¦Á  and with these terms fixed  the derivative of the bound is the same as the derivative of
x
	q y¦Á|x logp y¦Á 	 1 
¦Á
so maximizing the bound w.r.t. the weights in the higher layers is exactly equivalent to maximizing the log probability of a dataset in which y¦Á occurs with probability q y¦Á|x . if the bound becomes tighter  it is possible for logp x  to fall even though the lower bound on it increases  but logp x  can never fall below its value at step 1 of the greedy algorithm because the bound is tight at this point and the bound always increases.
¡¡the greedy algorithm can clearly be applied recursively  so if we use the full maximum likelihood boltzmann machine learning algorithm to learn each set of tied weights and then we untie the bottom layer of the set from the weights above  we can learn the weights one layer at a time with a guarantee1 that we will never decrease the log probability of the data under the full generative model. in practice  we replace maximum likelihood boltzmann machine learning algorithm by contrastive divergence learning because it works well and is much faster. the use of contrastive divergence voids the guarantee  but it is still reassuring to know that extra layers are guaranteed to improve imperfect models if we learn each layer with sufficient patience.
¡¡to guarantee that the generative model is improved by greedily learning more layers  it is convenient to consider models in which all layers are the same size so that the higherlevel weights can be initialized to the values learned before they are untied from the weights in the layer below. the same greedy algorithm  however  can be applied even when the layers are different sizes.
1 back-fitting with the up-down algorithm
learning the weight matrices one layer at a time is efficient but far from optimal. once the weights in higher layers have been learned  neither the weights nor the simple inference procedure are optimal for the lower layers. the suboptimality produced by greedy learning is relatively innocuous for supervised methods like boosting. labels are often scarce and each label may only provide a few bits of constraint on the parameters  so over-fitting is typically more of a problem than under-fitting. going back and refitting the earlier models may  therefore  cause more harm than good. unsupervised methods  however  can use very large unlabeled datasets and each case may be very high-dimensional thus providing many bits of constraint on a generative model. under-fitting is then a serious problem which can be alleviated by a subsequent stage of back-fitting in which the weights that were learned first are revised to fit in better with the weights that were learned later.
¡¡after greedily learning good initial values for the weights in every layer  we untie the  recognition  weights that are used for inference from the  generative  weights that define the model  but retain the restriction that the posterior in each layer must be approximated by a factorial distribution in which the variables within a layer are conditionally independent given the values of the variables in the layer below. a variant of the wake-sleep algorithm described in section 1
¡¡can then be used to allow the higher-level weights to influence the lower level ones. in the  up-pass   the recognition weights are used in a bottom-up pass that stochastically picks a state for every hidden variable. the generative weights on the directed connections are then adjusted using the maximum likelihood learning rule in eq. 1. because weights are no longer tied to the weights above them  s i must be computed using the states of the variables in the layer above i and the generative weights from these variables to i. the weights on the undirected connections at the top level are learned as before by fitting the top-level rbm to the posterior distribution of the penultimate layer.
¡¡the  down-pass  starts with a state of the top-level associative memory and uses the top-down generative connections to stochastically activate each lower layer in turn. during the down-pass  the top-level undirected connections and the generative directed connections are not changed. only the bottom-up recognition weights are modified. this is equivalent to the sleep phase of the wake-sleep algorithm if the associative memory is allowed to settle to its equilibrium distribution before initiating the down-pass. but if the associative memory is initialized by an up-pass and then only allowed to run for a few iterations of alternating gibbs sampling before initiating the down-pass  this is a  contrastive  form of the wake-sleep algorithm which eliminates the need to sample from the equilibrium distribution of the associative memory. the contrastive form also fixes several other problems of the sleep phase. it ensures that the recognition weights are being learned for representations that resemble those used for real data and it also helps to eliminate the problem of mode averaging. if  given a particular data vector  the current recognition weights always pick a particular mode at the level above and ignore other very different modes that are equally good at generating the data  the learning in the down-pass will not try to alter those recognition weights to recover any of the other modes as it would if the sleep phase used a pure ancestral pass.
¡¡by using a top-level associative memory we also eliminate a problem in the wake phase: independent top-level units seem to be required to allow an ancestral pass  but they mean that the variational approximation is very poor for the top layer of weights.
1 performance on the mnist database
1 training the network
the mnist database of handwritten digits contains 1 training images and 1 test images. results for many different pattern recognition techniques are already published for this database so it is ideal for evaluating new pattern recognition methods. the network1 shown in figure 1 was trained on 1 of the training images that were divided into 1 balanced mini-batches each containing 1 examples of each digit class. the weights were updated after each minibatch.
¡¡in the initial phase of training  the greedy algorithm described in section 1 was used to train each layer of weights

figure 1: the network used to model the joint distribution of digit images and digit labels.
separately  starting at the bottom. each layer was trained for 1 sweeps through the training set  called  epochs  . during training  the units in the  visible  layer of each rbm had real-valued activities between 1 and 1. these were the normalized pixel intensities when learning the the bottom layer of weights. for training higher layers of weights  the realvalued activities of the visible units in the rbm were the activation probabilities of the hidden units in the lower-level rbm. the hidden layer of each rbm used stochastic binary values when that rbm was being trained. the greedy training took a few hours in matlab on a 1ghz xeon processor and when it was done  the error-rate on the test set was 1%  see below for details of how the network is tested .
¡¡when training the top layer of weights  the ones in the associative memory  the labels were provided as part of the input. the labels were represented by turning on one unit in a  softmax  group of 1 units. when the activities in this group were reconstructed from the activities in the layer above  exactly one unit was allowed to be active and the probability of picking unit i was given by:
		 1 
where xi is the total input received by unit i. curiously  the learning rules are unaffected by the competition between units in a softmax group  so the synapses do not need to know which unit is competing with which other unit. the competition affects the probability of a unit turning on  but it is only this probability that affects the learning.
¡¡after the greedy layer-by-layer training  the network was trained  with a different learning rate and weight-decay  for 1 epochs using the up-down algorithm described in section 1. the learning rate  momentum  and weight-decay1 were chosen by training the network several times and observing its performance on a separate validation set of 1 images that were taken from the remainder of the full training set. for the first 1 epochs of the up-down algorithm  the up-pass was followed by three full iterations of alternating gibbs sampling in the associative memory before performing the down-pass. for the second 1 epochs  six iterations were performed  and for the last 1 epochs  ten iterations were performed. each time the number of iterations of gibbs sampling was raised  the error on the validation set decreased noticeably.
¡¡the network that performed best on the validation set was then tested and had an error rate of 1%. this network was then trained on all 1 training images1 until its error-rate on the full training set was as low as its final error-rate had been on the initial training set of 1 images. this took a further 1 epochs making the total learning time about a week. the final network had an error-rate of 1%. the errors made by the network are shown in figure 1. the 1 cases that the network gets correct but for which the second best probability is within 1 of the best probability are shown in figure 1.
¡¡the error-rate of 1% compares very favorably with the error-rates achieved by discriminative  feed-forward neural networks that have one or two hidden layers and are trained by back-propagation. when the detailed connectivity of these networks is not hand-crafted for this particular task  the best reported error-rate for stochastic on-line learning with a separate squared error on each of the 1 output units is 1%. these error-rates can be reduced to 1% by using small

always set quite conservatively to avoid oscillations. it is highly likely that the learning speed could be considerably improved by a more careful choice of learning parameters  though it is possible that this would lead to worse solutions.
1
   the training set has unequal numbers of each class  so images were assigned randomly to each of the 1 mini-batches.
figure 1: the 1 test cases that the network got wrong. each case is labeled by the network's guess. the true classes are arranged in standard scan order.
initial weights   softmax  output units  a cross-entropy error function  and either very gentle learning  john platt  personal communication  or a regularizer that penalizes the squared weights by an amount that is carefully chosen using a validation set. for comparison  nearest neighbor has a reported error rate  google mnist  of 1% if all 1 training cases are used  which is extremely slow  and 1% if 1 are used. this can be reduced to 1% and 1% by using an l1 norm.
¡¡the only standard machine learning algorithm that outperforms the generative model is support vector machines which give error rates of 1% or 1%. but it is hard to see how support vector machines can make use of the domain-specific tricks  like weight-sharing and sub-sampling  which lecun et.al. use to improve the performance of discriminative neural networks from 1% to 1%. there is no obvious reason why weight-sharing and sub-sampling cannot be used to reduce the error-rate of the generative model. further improvements can always be achieved by averaging the opinions of multiple networks or by enhancing the training set with distorted data  but these techniques are available to all methods  though data enhancement can seriously slow-down methods that do not scale sub-linearly with the size of the training set.
figure 1: each row shows 1 samples from the generative model with a particular label clamped on. the top-level associative memory is run for 1 iterations of alternating gibbs sampling between samples.
1 testing the network
one way to test the network is use a stochastic up-pass from the image to fix the binary states of the 1 units in the lower layer of the associative memory. with these states fixed  the label units are given initial real-valued activities of 1 and a few iterations of alternating gibbs sampling are then used to activate the correct label unit. this method of testing gives error rates that are almost 1% higher than the rates reported above.
¡¡a better method is to first fix the binary states of the 1 units in the lower layer of the associative memory and to then turn on each of the label units in turn and compute the exact free energy of the resulting 1 component binary vector. almost all the computation required is independent of which label unit is turned on  teh and hinton  1  and this method computes the exact conditional equilibrium distribution over labels instead of approximating it by gibbs sampling which is what the previous method is doing. this method gives error rates that are about 1% higher than the ones quoted because of the stochastic decisions made in the up-pass. we can remove this noise in two ways. the simplest is to make the up-pass deterministic by using probabilities of activation in place of stochastic binary states. the second is to repeat the stochastic up-pass twenty times and average either the label probabilities or the label log probabilities over the twenty repetitions before picking the best one. the two types of average give almost identical results and these results are also very similar to using a deterministic up-pass  which was the method used for the reported results.
1 looking into the mind of a neural network
to generate samples from the model  we perform alternating gibbs sampling in the top-level associative memory until the markov chain converges to the equilibrium distribution. then figure 1: each row shows 1 samples from the generative model with a particular label clamped on. the top-level associative memory is initialized by an up-pass from a random binary image in which each pixel is on with a probability of 1. the first column shows the results of a down-pass from this initial high-level state. subsequent columns are produced by 1 iterations of alternating gibbs sampling in the associative memory.
we use a sample from this distribution as input to the layers below and generate an image by a single down-pass through the generative connections. if we clamp the label units to a particular class during the gibbs sampling we can see images from the model's class-conditional distributions. figure 1 shows a sequence of images for each class that were generated by allowing 1 iterations of gibbs sampling between samples.
¡¡we can also initialize the state of the top two layers by providing a random binary image as input. figure 1 shows how the class-conditional state of the associative memory then evolves when it is allowed to run freely  but with the label clamped. this internal state is  observed  by performing a down-pass every 1 iterations to see what the associative memory has in mind. this use of the word  mind  is not metaphorical. a mental state is the state of a hypothetical world in which a high-level internal representation would constitute veridical perception. that hypothetical world is what the figure shows.
1 conclusion
the network in figure 1 has about as many parameters as 1 cubic millimeters of mouse cortex. several hundred networks of this complexity would fit within a single voxel of a high resolution fmri scan. learning algorithms are getting better  but they still have a long way to go.
acknowledgments
the ideas presented in this paper came from research collaborations with terry sejnowski  yann lecun  jay mcclelleand  radford neal  rich zemel  peter dayan  michael jordan  brendan frey  zoubin ghahramani  yee-whye teh  max welling  simon osindero and many other researchers. andriy mnih helped to prepare the manuscript. the research was supported by nserc  the gatsby charitable foundation  cfi and oit. geh holds a canada research chair in machine learning.
