 
autonomous robot systems operating in an uncertain environment have to be reactive and adaptive in order to cope with changing environment conditions and task requirements. to achieve this  the hybrid control architecture presented in this paper uses reinforcement learning on top of a discrete event dynamic system  deds  framework to learn to supervise a set of basis controllers in order to achieve a given task. the use of an abstract system model in the automatically derived supervisor reduces the complexity of the learning problem. in addition  safety constraints may be imposed a priori  such that the system learns on-line in a single trial without the need for an outside teacher. to demonstrate the applicability of the approach  the architecture is used to learn a turning gait on a four legged robot platform. 
1 	i n t r o d u c t i o n 
　autonomous robot systems operating in an uncertain environment have to be able to cope with new situations and task requirements. important properties of the control architecture of such systems are thus that it is reactive  allows for flexible responses to novel situations  and that it adapts to longer lasting changes in the environment or the task requirements. 
　although model-based control techniques have been used successfully in a wide variety of tasks  they are very sensitive to imprecisions in the model and are often not robust with respect to unexpected situations. to better address the reactivity requirements of autonomous systems  behavior-based architectures  brooks  1  were developed. in this paradigm  system behavior is constructed on-line from combinations of elemental  reactive behaviors. the often ad hoc character of these 'this work was supported in part by nsf iri-1 
1 	robotics 
behaviors  however  can lead to an extremely complex organization of behavioral elements. in addition  the resulting policy can be brittle with respect to relatively minor perturbations including those introduced by other behaviors or by changes in control context. the control basis approach used here attempts to circumvent this problem by employing carefully designed declarative control primitives to construct overall system behavior  and therefore allows predictions about the outcome of behavioral sequences. this approach has been used successfully in manipulation and locomotion tasks  grupen et al.  1; huber et a/.  1 . 
　while such bottom-up approaches address the issue of reactivity  the used composition is in most cases given by the designer and very specific to the task at hand. in order to render such a system adaptive and allow it to adjust its overall behavior to changing task requirements  learning techniques have to be employed. in the extreme case  this learning has to occur without the direct influence of an outside teacher in order to obtain autonomous behavior. reinforcement learning techniques are well suited to such behavior composition tasks since they can learn sequences of behavior from simple reinforcement signals. in most applications  however  these techniques have been applied at a very low level  thus leading to a very high complexity of the learning task. this complexity rendered these approaches inadequate for on-line learning in complex systems. 
　to address these complexity problems as well as to provide a base reactivity to the system  some work has been done to combine this learning framework with the robustness of behavior based control approaches  maes and brooks  1; mahadevan and connell  1 . here either the problem is decomposed a priori and only subproblems are learned  or previously designed behaviors are used as elemental actions within a reinforcement learning task. while this dramatically reduces the complexity of the state and action spaces for the learning problem  the character of the behaviors often restricts their applicability to a very limited domain  potentially requiring the design of new components whenever the task changes. in addition  most of these approaches do not address safety considerations by allowing the exploration to take random actions  thus permitting the occurrence of catastrophic failures. in autonomous systems  however  this is not permissible since the system can not recover. it is thus necessary that such systems can learn in a single trial without the need for outside supervision. one way to address this is by using a parametric controller which is inherently safe as a basis for the learning task  singh et a/.  1 . the learning component learns thereby only a setting of the parameters while the controller assures a baseline performance. the use of a single controller  however  increases designer effort and limits the scope of the system. 
　the approach presented here addresses the complexity and safety issues by means of a hybrid continuous/discrete control architecture. behavior is constructed on-line from a set of stable and convergent control elements. the stable character of these base controllers is then used in a discrete event dynamic system  deds  framework  sobh et a/.  1  to construct a supervisor which provides the structure for the reinforcement learning component. this dramatically reduces the complexity of the learning problem by reducing the state and action spaces  and supports techniques designed to limit exploration to safe and relevant areas of the behavior space. moreover  the system inherits the reactivity and stability of the underlying control basis. 
　in the following  section 1 briefly introduces the control basis approach before section 1 describes the discrete event architecture used to automatically synthesize admissible control policies  and the reinforcement learning technique for acquiring a policy for a given task. section 1  finally  shows an example for the overall architecture in the walking domain where a turning gait is learned on-line in a single trial. 
1 	the control basis approach 
　the control basis approach constructs behavior online by combining feedback control elements drawn from a set of carefully designed basis controllers. individual control elements are thereby largely task and device independent and represent solutions to generic robot control problems  allowing a small set of these elements to span a large range of tasks on a wide variety of platforms. 
　in this framework  control is derived on-line by associating input resources  sensors or sensor abstractions   and output resources r  actuators  with feedback control laws drawn from the control basis. the resulting controllers can then be activated concurrently according to a task dependent composition policy under the  subject to   constraint. this constraint restricts the control actions of subordinate controllers such that they do not counteract the objectives of higher priority controllers. the resulting concurrent control policy inherits the stability and convergence properties of the elemental control elements. a complete control policy takes then the form of a sequence of concurrent controller activations of the form shown in figure 1. different tasks in this framework are achieved by changing composition policies over the same set of controllers rather than by designing new control elements. 

figure 1: control composition 
　this approach has already been used successfully on a variety of tasks in the manipulation and locomotion domain  grupen et a/.  1; huber et al.  1 . in all these cases  however  composition policies were hand crafted  thus requiring the system designer to anticipate the exact behavior of the controllers. to achieve more autonomous behavior of a robot system  however  the system has to be able to adapt to novel situations and task contingencies without the need for outside supervision. in order to achieve this  the architecture presented in this paper learns the optimal composition policy in an efficient way using the reinforcement learning paradigm. 
1 	composition architecture 
　reinforcement learning  barto et a/.  1  offers a flexible way to acquire control strategies automatically and thus to adapt to new contingencies and task requirements. most reinforcement learning systems  however  operate at a very low level by directly influencing actuator commands  easily leading to an explosion in the complexity of the learning task. this renders such approaches impractical for on-line learning in complex systems. to avoid this problem and to be able to prevent catastrophic failures  the architecture presented here attempts to learn a composition policy for the underlying controllers. to do so it uses the goal directed character of the control modules described in section 1 to build an 
	huber & grupen 	1 

abstract description of the system under the influence of the basis controllers. running controllers to convergence  the possible behavior of the system is modeled as a deds on a symbolic predicate space characterized by the individual goals of the control modules. this abstract system model is then used as the basis for an exploration-based learning system to acquire an optimal control strategy for the given task. the deds formalism encodes safety constraints in the model and thus limits the exploration to the space of admissible control policies. the overall architecture is shown in figure 1. 

figure 1: the control architecture 
　in this approach  all continuous sensor input and actuator output is handled by the elements of the control basis. activation and convergence of these controllers is then interpreted as the events used in the abstract deds model of the system behavior. after imposing safety and domain constraints  the deds supervisor represents the set of all admissible control policies. throughout exploration or in a separate system identification phase  additional transition probabilities for this model can be acquired  thus improving the quality of the largely task independent model. this model allows knowledge of system behavior to be generalized across tasks and supports additional off-line learning on the estimated system model  sutton  1 . in addition to acquiring intrinsic structure in the form of transition probabilities  this hybrid architecture also solves the resource allocation problem by learning to assign resources to controllers optimally given the current reinforcement structure. 
1 	d e d s supervisor 
　the feedback control primitives employed as elemental actions in this approach act as stable attractors and 
1 	robotics 
thus form basins of attraction in the continuous physical space. the system may therefore be described at an abstract level by means of convergence predicates associated with the underlying controllers. the abstraction from continuous state space to discrete predicate space represents a dramatic reduction in complexity and thus forms a good basis for the reinforcement learning task. since activation and convergence determine the progress of the plant in this abstract space  the overall system can be modeled as a hybrid deds  opening the derivation of a possible supervisor to a large body of formal techniques  ramadge and wonham  1; sobh et a/.  1 . in particular  constraints such as safety and the absence of deadlock conditions can be imposed a priori on the control policy. 
predicate space description 
in order to allow for the abstraction step and to provide an efficient way of deriving an abstract model of the possible system behavior  the effects and interactions between control primitives have to be described symbolically by their possible effects on the set of predicates. characterizations for composite controllers can thereby be generated automatically from descriptions of the individual elements of the control basis  thus limiting the work of the designer to these control elements. as opposed to most deds approaches where the designer has to provide a complete system model  stiver et a/.  1; kosecka and bogoni  1   these simple descriptions allow for an automatic generation of a predicate space model of all possible system behavior  for details on the controller characterization and the construction of the system model see  huber and grupen  1  . this model takes the form of a nondeterministic finite state automaton  and forms the basis for the supervisor synthesis and reinforcement learning components of the proposed architecture. 
supervisor synthesis 
in the deds framework  control is performed by enabling and disabling of controllable events in a supervisor. in the case of the architecture presented here  the set of these events corresponds to controller activations while convergence events are not controllable. to allow for safety of a chosen control policy or to ensure that no deadlock occurs  the deds formalism provides methods to automatically impose constraints on the supervisory automaton. doing so  the system model can be pruned a priori to the set of all policies which obey the given set of constraints  ramadge and wonham  1; huber and grupen  1 . in the same fashion  additional domain knowledge and designer preferences can be incorporated into the control system  further reducing the complexity for the reinforcement learning component. throughout learning and system operation  this 

discrete supervisor is then used to limit exploration to admissible parts of the behavior space  and to activate controllers according to the policy selected by the learning system. 
1 	learning component 
　reinforcement learning provides a mechanism for adapting to changing environments and tasks without external supervision  mahadevan and connell  1 . in this exploration-based paradigm  the system learns a control policy which maximizes the amount of reinforcement it receives. a major drawback of this scheme  however  is its inherent complexity and the need for exploration in order to find better policies. in the architecture presented here  these problems are addressed by learning controller activations on top of the predicate space model defined by the deds supervisor  thus reducing the size of the action and state spaces considered in the learning task  and enforcing safety constraints throughout exploration. in addition  this also facilitates the acquisition of transition probabilities between predicate states and thus allows run-time experience to improve the predictive power of the abstract model. 
　the learning component used here employs qlearning  watkins  1   a widely used temporal difference method that learns a value function over state/action pairs in order to represent the quality of a given action. estimates of the future discounted payoff of an action  
are computed  where is the immediate reinforcement obtained at this time step  and is a discount factor. using this function the control policy is given as the action with the highest q-value in the current state. to adapt this to the finite state transition model used as the underlying state space description for learning  this value function can be represented in a distributed fashion as 

where p x a  y  is the probability that controller a in state x will lead to state y and  represents the value of the corresponding transition. throughout learning this estimate is then updated according to 
is the learning rate. at the same time  frequency 
counts can be used to determine the transition probabilities within the nondeterministic supervisor. together this allows the overall architecture to adapt efficiently to changing task requirements by on-line acquiring correct control policies. 
1 locomotion example 
　to demonstrate the applicability of the proposed architecture  it has been implemented on a four legged  twelve degree of freedom walking robot. the objective was to acquire useful policies for turning gaits. it has already been shown that hand crafted control gaits for such tasks can be derived  huber et a/.  1 . the example presented here uses the proposed architecture to learn solutions autonomously with minimal external supervision. to do this  the robot was put in an initial stable configuration onto an even surface and the learning process was initiated without any further input from an outside teacher. 
1 	control basis and deds supervisor 
　the control basis used for these tasks was already used successfully for dextrous manipulation and locomotion tasks  grupen et al.  1; huber et a/.  1  and consists of solutions to three generic robotics problems  
configuration space motion control  
contact configuration control  and 
kinematic conditioning. 
each of these basis control laws can then be bound to subsets of the system resources  legs 1 1 and position and orientation of the center of mass  shown in figure 1. for details on this control basis and the resource bindings see  huber et a/.  1 . 

figure 1: controller and resource notation 
　for the example presented here  the set of possible controllers and controller bindings was limited in order to allow for a concise notation for the predicate space model. the set of controller/resource combinations available to the system consists here of all instances of the contact configuration controller of the form 

	huber & grupen 	1 

used to construct composite controllers  allowing a total of 1 composite controllers or actions in the deds and learning components. in addition  this choice of candidate controllers limits the predicate space to 1 predicates  corresponding to the convergence of a controller/input binding pair in the following way: 

where * is a wildcard and indicates the independence of the predicate evaluation from the output resource. 
   after automatically constructing the graph of all possible system behavior in this space  a safety constraint for quasistatic walking  namely that the platform has to be always stable  can be imposed on the supervisor. in terms of the predicates this implies that at least one stance has to be stable  or in other words has to evaluate true. furthermore  knowledge about the platform can be introduced in the form of domain constraints. for the legged platform employed here  for example  kinematic limitations do not allow the simultaneous stability of two opposing support triangles. adding this knowledge as  further reduces the size of the supervisor to the one shown in figure 1   

figure 1: deds supervisor for rotation task 
where the numbers in the states represent the values of the 1 predicates. it should be noted here  that for the purpose of illustration  the complete supervisor has been built a priori in this example. in general  however  this 
1 	robotics 
could be done incrementally in the course of exploration without the violation of any constraints. 
1 	learning results 
　the supervisor derived from the control basis represents all admissible control policies. it does not  however  express any task objectives or an optimal control policy. in order to learn these for the counterclockwise rotation task  a reinforcement schedule has to be present which rewards the system whenever it performs the task correctly. the reinforcement used in this example is 1 if the control action led to a counterclockwise rotation  -1 if it led to a clockwise rotation  and 1 otherwise. the robot system is then put onto a flat surface and the learning process is started. 
　several experiments of this form were performed in order to investigate the performance of the control and learning components. in all these trials the system rapidly acquired the correct gait pattern while exploration was slowly decreased from 1% to 1%. this minimum level of random actions was maintained to introduce perturbations and thus to learn a more robust policy. figure 1 depicts a typical learning profile for this task. 

control steps 
figure 1: learning curve for counterclockwise task 
　the learning curve shows that a correct turning gait is learned after approximately 1 learning steps. throughout this entire learning process  which on the real robot took approximately 1 minutes  the robot platform never entered an unsafe situations due to the limitations imposed by the deds supervisor. 
　the final control policy is shown in figure 1. this graph shows all possible transitions that can occur while following the learned policy. the core of this policy is the cycle indicated by bold transition arrows which corresponds to a stable turning gait. transition probabilities within this cycle are   1%  making it a stable attractor for this policy. for this core  the corresponding control actions are indicated on the bottom of the figure. the learned control actions in all other states attempt to lead the system onto this stable cycle  making the policy more robust with respect to perturbations. 


figure 1: learned control policy for counterclockwise 
rotation task 
1 	conclusions 
　reactive and adaptive control architectures for autonomous systems pose many challenges due to the complexity of the task and the limited amount of supervision possible. the architecture presented in this paper employs a hybrid control architecture with a reinforcement learning component in order to address these issues. continuous reactive control is thereby derived from a carefully designed control basis while the composition policy is learned on top of an abstract predicate space in a deds framework. this allows the imposition of safety constraints and thus permits new tasks to be learned online in a single trial and without the need for an external teacher. in addition  it dramatically reduces the complexity of the action and state space  making learning feasible even for complex tasks and platforms. 
