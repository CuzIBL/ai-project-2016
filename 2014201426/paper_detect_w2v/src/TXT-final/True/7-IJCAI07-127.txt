
restart strategies are commonly used for minimizing the computational cost of randomized algorithms  but require prior knowledge of the run-time distribution in order to be effective. we propose a portfolio of two strategies  one fixed  with a provable bound on performance  the other based on a model of run-time distribution  updated as the two strategies are run on a sequence of problems. computational resources are allocated probabilistically to the two strategies  based on their performances  using a well-known k-armed bandit problem solver. we present bounds on the performance of the resulting technique  and experiments with a satisfiability problem solver  showing rapid convergence to a near-optimal execution time.
keywords: randomized algorithms  restart strategies  performance modeling  heavy-tailed distributions  algorithm portfolios  satisfiability  lifelonglearning  adversarial multi-armed bandit problem.
1 introduction
when trying to communicate across a lossy communication channel with unbounded delays  how long should we wait before resending an unanswered message  in multimodal function optimization using gradient descent  how many steps should we take before starting from a new random initial point  in randomized search over a tree  how deep should we go before abandoning all hopes of finding the goal  and starting the search anew  in similar situations  it is desirable to minimize the time to solve a given problem instance  with a given algorithm  whose run-time is a random variable with a possibly unknown distribution. a restart strategy can be used  to generate a sequence of  cutoff  times  at which the algorithm is restarted if unsuccessful.
¡¡it has been proved that knowledge of the run-time distribution  rtd  for a given algorithm/problem combination1 can be used to determine an optimal uniform strategy  based on a constant cutoff  luby et al.  1 . in the same paper  the authors argue that such a strategy is of little practical interest  as a model of performance is usually unavailable. they then present a universal restart strategy  whose performance is worse than the optimal strategy by a logarithmic factor.
¡¡these two strategies are based on opposite hypotheses about the availability of the rtd. as a third alternative  a runtime sample for a set of problem instances could be collected  and used to learn a model of the underlying rtd. there are two potential obstacles to this approach. first  gathering a meaningful sample of run-time data can be computationally expensive  especially in the very case where a restart strategy would be most useful  i. e.  when the run-time exhibits huge variations  gomes et al.  1 . second  a given problem set might contain instances with significantly different rtds: the obtained model would capture the overall behavior of the algorithm on the set  but the corresponding restart strategy would be suboptimal for each instance. problems met after the initial training phase could also be captured poorly by the estimated rtd  again leading to a suboptimal strategy.
¡¡both obstacles are not difficult to circumvent. training cost can be reduced by using a small censored sample of run-time values  obtained aborting runs that exceed some cutoff time. this obviously has an impact on the model's precision  but  the requirements in this sense are less strict than one would expect  and a coarse model already allows to evaluate a near-optimal strategy  gagliolo and schmidhuber  1b . the second problem is unavoidable in a worst case setting; but in practical situations we do not expect the rtd of each instance of a problem set to be uncorrelated  and a sub-optimal uniform strategy based on the set rtd can still have a great advantage over the large bound of the universal.
¡¡in this paper we show how luby's universal and uniform restart can be effectively interleaved  to solve a set of problem instances. the uniform strategy is based on a non-parametric model of the rtd on the problem set  which is updated every time a new problem instance is solved  in an online fashion. the resulting exploration vs. exploitation tradeoff  is treated as a bandit problem: a solver from  auer et al.  1  is used to allocate runs among the two strategies  such that  as the model converges  the uniform strategy is favored if the rtd

uniform randomly picked without replacement from the set.of the set is close to the rtd of most instances  and worstcase bounds on performance are preserved otherwise.
¡¡in the following  restart strategies  sect. 1  are briefly introduced  followed by a discussion of an  adversarial bandit  approach to algorithm selection  sect. 1   which will be used in our strategy  sect. 1 . we give references to other related work in sect. 1. sect. 1 presents experiments with a randomized satisfiability problem solver. sect. 1 discusses potential impact  and viable improvements  of the presented approach.
1 optimal restart strategies
a restart strategy consists in executing a sequence of runs of a randomized algorithm  in order to solve a same problem instance  stopping each run r after a time t r  if no solution is found  and restarting the algorithm with a different random seed; it can be operationally defined by a function t : n¡úr+ producing the sequence of thresholds t r  employed. luby et al.  proved that the optimal restart strategy is uniform  i. e.   one in which a constant t r  = t is used to bound each run. they show that in this case the expected value of the total run-time tt can be evaluated as
		 1 
¡¡where f t  is the cumulative distribution function  cdf  of the run-time for an unbounded run of the algorithm  i. e.  a function quantifying the probability that the problem is solved before time t.
¡¡if such distribution is known  an optimal cutofftime t  can be evaluated minimizing  1 . otherwise  the authors suggest a universal non-uniform restart strategy  with cutoff sequence {1 1 1 1 ...} 1 proving that its performance tu is bounded with high probability with respect to the expected run-time e tt   of the optimal uniform strategy  as
	tu  = 1e tt   loge tt   + 1 	 1 
1 algorithm selection as a bandit problem
consider now a sequence b = {b1 ... bm} of problem instances  and a set of algorithms a = {a1 a1 ... ak}  such that each bm is solvable by at least one ak. without loss of generality  we assume that the execution of each ak can be subdivided in sequential steps  and indicate with tk r   r = 1 ...  the duration of the r-th step of the k-th algorithm on the current problem. each ak has its own mechanism for determining tk r : e. g.  for an iterative algorithm  tk r  can be the cost of the r-th loop . in this setting  one generally wants to solve the incoming problem instances as fast as possible. this could mean identifying the single best algorithm
for the whole set  or for each instance  algorithm selection  rice  1  ; or  in the more general setting of algorithm portfolios  huberman et al.  1   the optimal schedule for interleaving the execution of different algorithms  each performing well on different subsets of b.
¡¡in both cases  an obvious trade-off is faced  between exploration of the performance of the various ak  and exploitation of the solver s  estimated to be fastest. a well-known  and well aged  paradigm for addressing such a trade-off is the multi-armed banditproblem. in its most basic form  robbins  1   it is faced by a greedy gambler  playing a sequence of trials against a k-armed slot machine  each trial consisting of a choice of one of k available arms  whose rewards are randomly generated from different stationary distributions. the gambler can then receive the corresponding reward  and regret what he would have gained  if only he had pulled any of the other arms. the aim of the game is to minimize this regret  defined as the difference between the expected cumulative reward of the best arm  and the one cashed in by the gambler. generally speaking  a bandit problem solver  bps  can be described as a mapping from the entire history of observed rewards xk for each arm k to a probability distribution p =  p1 ... pk   from which the choice for the successive trial is picked.
¡¡in recent works  the original restricting assumptions have been progressively relaxed. in the partial information case  only the reward of the pulled arm is revealed to the gambler. in the non-oblivious adversarial setting  the rewards for a given trial can be an arbitrary function of the entire history of the game  and are thus allowed to be deceptive  but have to be set before the current choice is made. based on these pessimistic hypotheses  auer et al.  devised a probabilistic gambling scheme  exp1   whose regret on a finite number of trials is bounded with high probability as o m1 k logk 1   k being the number of arms  m the number of trials  see alg. 1 . the algorithm features a fixed lower bound ¦Ã on the exploration probability  and a parameter ¦Á controlling the amount of exploitation. both can be set based on m to obtain the optimal regret rate above.1

algorithm 1 exp1 m  by auer et al.

set ¦Á =   1k logk /m 1 set ¦Ã = min{1   k logk / 1m  1} initialize sk := 1 for k = 1 ... k;
for each trial do set
pick arm k with probability p k :=  1   ¦Ã pk + ¦Ã/k
observe reward xk ¡Ê  1  update
end for

¡¡the selection of a best algorithm for a set of problems can be naturally described in a k-armed bandit setting 1 where  pick arm k  means  execute the next step of algorithm ak   alg. 1 .

algorithm 1 algorithm selection using bps

initialize bps  p. for each problem do
set tk := 1  rk := 1  k = 1 ... k repeat
pick k ¡«p
execute step rk + 1 of ak update timer rk := rk + 1  tk := tk + tk rk  if problem solved then
¡¡observe reward xk := 1/tk else
observe reward xk := 1
end if
¡¡update p = bps k xk  until problem solved
end for

¡¡note that this simple scheme falls in the partial information  non-obliviousadversarial setting  as we only observe reward for successful algorithms  and this reward depends on previous choices: for example  if ak takes r steps to solve a problem  we will not see any reward for the first r   1 pulls. our adversary is not malevolent  but it can be deceptive  especially if the tk r  vary among algorithms:1 even so  any bounds on performance of bps will hold  and given our definition of reward  the expected performance of  alg. 1  will converge to the one of the best solver for the set of problems.
1 mixing restart strategies
traditional statistical tests assess the goodness of a model measuring the fit of the corresponding probability density function  pdf  along the whole spectrum of observed samples. in our case  the sole purpose of the model f  of the rtd is to set up a restart strategy  in order to gain on future performance  so the only quantity of practical interest is the loss in performance induced by the mismatch of our model. in  gagliolo and schmidhuber  1b  we studied the impact of increasingly censored sampling1 on the performance of an estimated uniform restart strategy. this impact is quite low  due to the fact that  1  depends only on the lower quantiles of the distribution: a rough model of f  obtained from a heavily censored sample from the lower portion of the time scale  suffices to evaluate a near-optimalstrategy  allowing to reduce its training cost.
¡¡the practical implementation of such a technique requires some censoring strategy to limit the cost of solving the training instances. assuming no a priori information about t   an ideal candidate is the universal strategy of luby  sect 1   as it invests equal portions of run-time on a set of exponentially
spaced restart thresholds. note that the restart thresholds of unsuccessful runs can be exploited in the form of censored samples. such a simple scheme would allow to keep the performance bound of the universal strategy during training  and use the gathered data to estimate f   and the corresponding strategy t   to be exploited for future problem instances. to limit the cost of learning  and allow exploitation of the model to start as soon as possible  we will instead adopt an online  or life-long learning approach. we will use the  bandit  algorithm selection scheme described in sect. 1 at an upper level  to control allocation of computational resources. in this case  the arms are the two restart strategies  each generating a sequence of restart thresholds tk rk   to bound a same algorithm s;  pull arm k  simply means  start s on the current problem  with threshold tk rk  . if the problem is not solved  tk rk  = tk rk   and xk := 1; if it is solved in a time ts  = tk rk   tk rk  = ts  and a positive reward is observed.
¡¡our strategy gambler  alg. 1  starts by solving the first problem using the universal strategy alone. after a problem instance is solved  the solution time of the successful run  and all the collected censored samples from the unsuccessfulruns  can be used to update the estimate f   and evaluate a new t   to be used on next problem. to model f  we will use the non-parametric product-limit estimator by kaplan and meier . it guarantees large sample convergence to an arbitrary distribution  and can account for censored samples as well. the non-parametric form is particularly appealing  as it does not require any a-priori assumptions about the rtd  it is fast to update  and the resulting cdf is a step-wise function  which makes the evaluation of the integral in  1  inexpensive. to use exp1 as the bps we only need to normalize the rewards. we can do so by setting a lower and upper bound on ts  tmin ¡Ü ts ¡Ü tmax. in order to limit the impact of this choice on the convergence rate of exp1  we adopt a logarithmic reward scheme: upon solution of a problem during a restart with strategy k  rk :=  logtmax logtk / logtmax  logtmin   tk being the total time spent by strategy k on that problem  including unsuccesful runs. in this way 1 is the maximum reward  that would be obtained by a strategy that solved a problem in a time tmin. note that the use of a logarithm allows to set tmin and tmax  respectively  to a very small and a very large value  such that only the knowledge of a very loose bound on ts is required.1
¡¡let us now analyze how the bound  1  combines with the optimal regretof exp1. in the following  e y  is the expected value of a random variable y  tk represents solution time of strategy k on a single problem  rk the number of restarts performed  pk ¡Ê  1  the probability of picking a restart strategy  and indices t   u  t   will label quantities related to algorithm 1 gambler m  restart strategy for algorithm s

set tmin  tmax initialize exp1 m   pu = 1. for each problem 1 ... m do
set tk := 1  rk := 1  k = 1 repeat
pick k ¡«p
run s with cutoff tk rk + 1  update timer rk := rk+1  tk := tk+min{ts tk rk }
if problem solved then
¡¡observe reward else
observe reward xk := 1
end if update p =exp1 k xk 
until problem solved
end for

the unknown optimal uniform  universal  and estimated optimal uniformstrategies  respectively  while g will identify our novel strategy gambler  interleaving the execution of the latter two. on a given problem  for which the rtd of our algorithm s is f t   a restart strategy with thresholds t r  can be viewed as sequence of independent bernoulli processes  with success probabilities f t r  . the number of restarts
r required to solve the problem is then distributed with pdf
: for a uniform strat-
egy t r  = t  p rt  is geometric  with e rt  = 1/f t . for any deterministic strategy  the expected time to solve a problem is  a monotonic function of e r . for u this implies that the bound  1  can be translated into a bound on ru  with the same high probability. given our simple reward scheme exp1 will keep a constant p =  pu pt   during solution of a single problem. consider now the worst-case setting in which t  is such that f t   = 1  i. e.  the uniform restart t  will never solve the problem. if u spends ru restarts  in the meantime another rt  restarts will have been wasted on t   with e rt   = e ru  1 pu /pu. as exp1 always keeps pu ¡Ý ¦Ã/1  the expected performance of gambler on this problem will then be bounded by e tu + rut  1   ¦Ã /¦Ã  with high probability  and upper bounds on tu and ru will also guarantee an upper bound on tg.
1 originality and related work
restart strategies are particularly effective if the rtd of the controlled algorithm exhibits  heavy  tails    i. e.  is pareto for both very large and very small values of time. such behavior is observed for backtracking search on structured underconstrained problems in  hogg and williams  1; gomes et al.  1   but also in other problem domains  such as computer networks  as in  van moorsel and wolter  1 . an alternative solution is to run multiple copies of the same algorithm in parallel  algorithm portfolios  huberman et al.  1; gomes and selman  1  . in  luby et al.  1   luby presents parallel restart strategies  which represent the natural combination with the portfolio approach: its solution is not adaptive  as it simply consists in restarting each algorithm with the universal strategy u. the literature on metalearning and algorithm selection  giraud-carrier et al.  1; rice  1   algorithm portfolios  huberman et al.  1; gomes and selman  1   anytime algorithms  boddy and dean  1   provides many examples of the application of performance modeling to resource allocation. most works in these field adopt a more traditional offline approach  in which run-time samples are collected during an often impractically long initial training phase  and the obtained model is used without further tuning on subsequently met problems. as discussed in the introduction  this also implies stronger assumptions about the representativeness of the training set  and does not guarantee upper bounds on execution time.
¡¡the max k-armed bandit problem is a variation on the original formulation  in which the estimated reward is only updated for the arm obtaining the maximum reward value  on a set of plays. solvers for this game are used in  cicirello and smith  1; streeter and smith  1  to maximize performance quality  on a single problem instance.
¡¡allocation of resources is usually set before each problem instance solution  and possibly updated afterward. dynamic approaches  in which feedback information from the algorithms is exploited to adapt resource allocation during problem solution  represent a notable recent trend. in  lagoudakis and littman  1; petrik  1   algorithm selection is modeled as a reinforcement learning problem  a more general setting than the bandit problem proposed in  sect. 1 . models of performance conditional on the dynamic state after a fixed execution time are used in  kautz et al.  1; wu  1  to implement dynamic context-sensitive restart policies for sat solvers: also in this case the main difference of our approach is the online learning setting.  gagliolo and schmidhuber  1a  present a heuristic dynamic online learning approach to resource allocation  in which a conditional model of performance is trained and progressively exploited during training.
1 experiments
the experiments were conducted using satz-rand  gomes et al.  1   a version of satz  li and anbulagan  1  in which random noise influences the choice of the branching variable. satz is a modified version of the complete dpll procedure  in which the choice of the variable on which to branch next follows an heuristic ordering based on first and second level unit propagation. satz-rand differs in that  after the list is formed  the next variable to branch on is randomly picked among the top h fraction of the list. we present results with the heuristic starting from the most constrained variables  as suggested in  li and anbulagan  1   and noise parameter set to 1.
¡¡a benchmark from satlib was used  consisting of different sets of  morphed  graph-coloring problems  gent et al.  1 : each graph is composed of the set of common edges among two randomgraphs  plus fractionsp and 1 pof the remaining edges for each graph  chosen as to form regular ring lattices. each of the 1 problem sets contains 1 instances  generated with a logarithmic grid of 1 different values for the parameter p  from 1 to 1  to which we henceforth refer with labels 1 ... 1. satz-rand has an initialization cost which depends only on the size of the problem  which in this case is the same for all sets. this cost was found to be always bigger than 1. we then set tmin = 1  tmax = 1  about 1 minutes on our machine   and  to be fair with the universal strategy  we multiplied and shifted its tu r  as tmin 1 + tu r  . with m = 1  parameters of exp1 are determined as ¦Á = 1  ¦Ã = 1.
¡¡each experiment was repeated 1 times with different random seeds  and a different random order of the instances. for comparison purposes  we repeated the experiments running the original algorithm  without restarts  labeled s  and the universal strategy alone  u . to compare with the ideal performance of t   we evaluated  a posteriori for each run  the t that minimized the cost of the problem set  tl set    and the cost of each instance  tl inst    based on the actual runtime outcomes. note that these can be different for each run  and their performancesare lower bounds on the performances of the optimal t   evaluated from the unknown actual rtd of  respectively  the problem set  and each problem instance.1 the difference of these two bounds is particularly interesting  as it is an indirect measure of the heterogeneity of the rtd of the instances in each set. to show the effect of a more heterogeneous problem set  we also run experiments with all the 1 instances grouped as a single set  labeled a in the graph; m = 1  ¦Á = 1  ¦Ã = 1   again randomly mixed. in figure 1 we present  for each set  the total computation time1 for our mixture  g   and the comparison terms. upper 1% confidence bounds are given  estimated on the 1 runs. plots of the rtd and restart cost for each set can be found in  gagliolo and schmidhuber  1b .
¡¡the results are quite impressive  and further confirm that the estimated restart strategy t  is not very sensitive to the fit of the model f . in a typical run  between 1% and 1% of the sample is censored  as there is only one uncensored runtime for each task; the fit of the model is visibly bad  and it is limited to the lower portion of the time scale  near or below t   but t  itself quickly converges close enough to t   giving a near-optimal strategy.
¡¡problems in 1 are easy. satz-rand solves all instances in a similar time  any larger t is an optimal restart value  i.e.  restarts are never executed  and the performance of a single copy s is the same as the ones of the optimal restarts. also our gambler quickly learns that  while u has to reach this value for every problem  resulting in a five times worst performance. in problems 1 to 1 we see the effect of a heavy-tailed rtd. s solves each set with times between 1 ¡Á 1  1 
 in
both cases. 1
¡¡¡¡as we are also conducting experiments with parallel portfolios of heterogeneous algorithms  and thus we need a common measure of time  we modified the original code of satz-rand adding a counter  that is incremented at every loop in the code. the resulting time measure was consistent with the number of backtracks. all results reported are in these loop cycles: on a 1 ghz machine  1 cycles take about one minute.

figure 1: experiments with satz-rand. time to solve each set of problems  1 ¡Ö 1 min. . g is our mixed strategy gambler  whose performance is initially as u  the universal restart strategy  and quickly converges near l  set   a lower bound on the performance of the unknown optimal restart strategy for the whole set. l  inst  is instead a lower bound on the performance of a distinct optimal restart strategy for each instance. a is the heterogeneous set obtained mixing all sets 1 ... 1. upper 1% confidence bounds estimated on 1 runs.
and 1 ¡Á 1  1 . in practice  only a few  unlucky  runs have very long completion times  but this is enough to penalize its performance dramatically. gambler scores fairly against the lower bounds  and u is between 1 and 1 times worst. from problem 1 on  we see that the heavy tail effect is less marked  but the two lower bounds diverge noticeably: instances in these sets present more heterogeneousrtds  and the instance-optimal t  inst  may vary of more than one order of magnitude. here u is only 1-1 times worse than gambler. the worst performance of gambler compared to l  set  can be seen on problem 1  where tg is about 1 times tl  set . here most problems require a small threshold t  and a few require one about ten times larger: in this situation also threshold t  set  varies visibly among different runs  and t  sometimes overestimates the optimal threshold. results on the whole set of problems a are better than expected: tg/tl  set  is about 1  but the performance compared to tl  inst  is obviously worst  1 . this is natural  as both gambler and t  set  cannot discriminate different restarts for each problem. u completes the set at 1 ¡Á 1  about 1 times worse than gambler.
1 conclusions and future work
we presented a novel restart strategy  gambler   combining the universal and optimal strategies by  luby et al.  1   based on a bandit approach to algorithm selection. gambler takes the best of two worlds: it preserves the upper bounds of the universal strategy in a worst-case setting  and it proved to be effective in a realistic application  with only a small overhead over an ideal lower bound. it can save a few orders of magnitude in computation time for algorithms with a heavy-tailed rtd  and  unlike the universal strategy  it does not worsen the performance otherwise. it has a negligible overhead compared to the controlled algorithm  and does not require the tuning of any parameter  nor any a-prior assumption about the rtd  excluding very loose bounds on execution time  which in the presented experiments differed of 1 orders of magnitude. it does not require an initial training phase.
¡¡the parameters of exp1 are automatically set based on the number of problems to be solved. if this number is not known in advance  exp1 can be used instead.
¡¡the obtained strategy can be applied to any randomized algorithm  including  e. g.  evolutionary algorithms  stochastic local search  gradient descent from a random initial value. its main limitation is that it relies on the assumption that the problems to be solved display a similar rtd  but we saw in practice that this requirement is not strict  and the observed performance was consistently better than the one of the universal strategy alone.
¡¡the strategy has a modular structure: it would allow inclusion of more strategies  with an o k logk  impact on performance  guaranteed by exp1. or more algorithms  each with its own rtd model. we particularly expect improvements from the use of conditional models  based on problem features  as they should allow for a distinction among different sub-classes of problems  sharing a similar rtd. a further step could be taken including dynamic information from the actual execution  as exemplified in  kautz et al.  1; gagliolo and schmidhuber  1a .
¡¡acknowledgments. the authors would like to thank faustino gomez for precious comments on the draft of this paper. this work was supported by snf grant 1-
1.
