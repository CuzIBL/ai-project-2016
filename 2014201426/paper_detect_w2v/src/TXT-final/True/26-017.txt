 
although empirical machine learning has seen many algorithms  one of its most important goals has been neglected. important real-world problems often have just a primitive representation  to which the target concept bears only a remote  obscure relationship. this consideration leads to a class of measures that may be applied to data to estimate difficulty for standard algorithms. as the concept becomes harder  current decision tree and decision list methods give increasingly poor accuracy  though backpropagation does better. a new system for feature construction scales up best. the fundamental limitation of standard algorithms is caused by two problems: greedy search and representational inadequacy. critical analysis and empirical results show that lookahead alleviates the greedy hill-climbing problem at high cost  but even this is insufficient. combining lookahead with feature construction alleviates the  complex global replication  problem with hard concepts. for principled algorithm development and good progress  researchers need to study hard concepts and system behavior using them. 
1 	introduction 
even in non-incremental  attribute-based learning from examples  many algorithms have been created. to improve their design  researchers often rely on established tactics. one is to identify shortcomings of current algorithms  such as the replication problem  pagallo  1 . this sometimes leads to a new algorithm  in this case fringe  which is later found to have limited value  such as applicability to relatively few concepts  yang et al.  1 . another tactic is to alter the representation. replacing decision trees with decision lists  clark and 
niblett  showed that cn1 performed better for certain concepts. these and other tactics amount to a strategy of parallel hill-climbing in a space of designs. 
    this work was supported in part by nsf grant iri-1. 
1 	machine learning 
　bradshaw  argues that science advances more quickly when we guide design-space search using a careful functional analysis and suitable decomposition of the underlying problem. although researchers always use functionality to guide design  algorithms can be evaluated and developed properly only if we decompose the whole problem well. 
　in this paper we address an undeveloped or neglected dimension of algorithm functionality: concept learning from  primitive  attributes  when experts cannot readily specify a favorable abstract representation. we contend that evaluating learning systems using data bases generated from well understood domains is weak because most of the work is already done. problems that are simplified through expert knowledge of appropriate abstract attributes are easy to solve  holte  1; rendell & seshu  1 . in contrast  primitive representation generally degrades accuracy to the point that current algorithms become useless  rendell & seshu . full assessment of learning algorithms and adequate progress on their design require that we address behavior and phenomena using harder problems. 
　section 1 presents a view of concept difficulty based on qualities of real-world representation and typical algorithms. in section 1 we survey measures and adopt a particular one for the assessment of algorithm performance. section 1 uses the difficulty measure to assess typical data bases and several algorithms found in learning. these results show the incompleteness of studies that indicate comparable accuracies using various algorithms  mooney et al.  1; weiss & kapouleas  1 . a new algorithm lfc   ragavan & rendell  1; ragavan et al.  1  this volume  scales up particularly well with harder concepts. section 1 analyzes these results to explain algorithm behavior and to promote research directions. 
1 	representation  algorithms  and difficulty 
in an attribute-based representation  each training example is described by an n-tuple of attribute values and a class value  positive or negative in the two-class case . a concept is an intensional description of the class  for which a learning algorithm constructs a hypothesis. 
numerous extant algorithms perform well on famil-
jar concepts and data found in machine learning work mooney et al.  1; weiss k kapouleas  1 . for many of the irvine data bases  even a very simple algorithm attains accuracies within 1% of the best method  holte  1 . the basic reason for the high accuracies is that the representation for the underlying concept matches the bias of the sbl algorithm. typical systems use the similarity  sbl  bias: neighboring points in instance space are likely in the same class  rendell  1 .1 
　standard learning systems are accurate if the concept is localized in instance space  rendell k cho  1   so they are good at this function of describing single locations. an area of uniform class membership values  especially a peak or valley in the function  is a contiguous  constant-valued region. a region is describable using a few disjuncts  to capture irregular shapes   and is easy to find with simple algorithms that use greedy methods  to view one attribute at a time . although the details depend on the approach  rendell  1   this function could be called region description  rd . concepts learned accurately by an sbl system designed for rd are  easy  for the algorithm. 
　in contrast  hard concepts are  spread out;  their class membership functions have a high degree of variation  devroye k gyorfi  1; rendell k seshu  1 . function variation is correlated with other phenomena  such as entropy  watanabe  1  and attribute interaction  devijver k kittler  1   which we collectively call  concept  scattering. hard concepts are learned very poorly by standard algorithms  even when many data are available  rendell k seshu  1 . 
　concept scattering is not just an academic consideration; it relates directly to representations for real-world domains. easier concepts are associated with good abstract representations  harder concepts with primitive information closer to direct observation. a good representation for checkers involves piece advantage and center control  whereas primitive attributes are the contents of board squares. a high-level representation for symbol recognition includes definitions of lines and circles  while a low-level representation uses pixel gray-levels. no satisfactory abstract representation is yet known for protein structure prediction  but a hard starting point is the primary sequence of amino acids  each position being an attribute . 
　in such examples  abstract representations simplify the relationship  reduce the complexity or variation  between the concept and its attributes  drastal et al.  1; rendell k seshu  1 . for example  the likelihood of winning checkers increases monotonically with piece advantage and center control  samuel  1 . in contrast  the likelihood of winning changes drastically with a small change in the contents of one or two board squares  rendell  1 . primitive representation causes concept scattering. 
good abstract representation is based on knowledge. 
concepts that have had the benefit of expert knowledge to hone good attributes may be learned accurately and quickly  rendell k seshu  1; samuel  1 . but even 
   1 this geometric view is clearest if the attributes are numeric  but it can be extended to other types. 
moderately complex concepts are difficult if the representation is primitive and little knowledge is available  quinlan  1; rendell  1 . the relationship of primitive attributes to the target concept is often obscure  making the problem hard and standard algorithms slow and inaccurate  devijver k kittler  1; rendell k seshu  1 . for some important real-world problems  the best known representation is poor  and existing induction systems perform little better than guessing based on the prior class probability  seshu et al.  1; towell et al.  1 . 
　in addition to the region description function rd when concepts are easy  we need to do something about poor representation. since primitive attributes cause a proliferation of concept regions  another function might be coalescing abstraction  ca : change of representation  feature construction; matheus k rendell  1  to diminish the number of regions  variation reduction; rendell k seshu  1 . ca allows rd to manage its task  drastal et al.  1 . 
　algorithm limitations for different amounts of abstraction are largely unexplored  though theory has shown that as concept complexity increases  more data are required to maintain accuracy  devroye k gyorfi  1; ehrenfeucht et al.  1 . to better understand the relationships between algorithm capability and representation quality  we can analyze hard concepts  characterize concept difficulty  locate associated deficiencies of algorithms  then reorient approaches to system design. 
1 	capturing concept difficulty 
1 	measures of difficulty 
if we quantify poor representation  we can determine the capabilities of algorithms in a more principled way. to approximate concept difficulty  various formalisms have been used. 
　measures can be based on the form of the hypothesis  as in theoretical research. although they provide guiding principles  hypothesis-based measures are impractical for assessing and developing algorithms  since the form of the concept is often unknown. the quantities we examine here are based on the data. 
　one view is that hard concepts have a classmembership function that is either close to monotonic or else singly-peaked  whereas hard concepts  functions  have many peaks and valleys  regions of uniform class membership; rendell  1; rendell k cho  1 . a related view is quinlan's  description of hard problems as having large numbers of disjuncts  more peaks require more distincts to describe . multiple peaks or disjuncts generally cause attribute interaction  which can be simplified in terms of statistical correlation  devi-
jver k kittler  1 . a generalized measure of scattered peaks or disjuncts is function variation  devroye k gyorfi  1; rendell k seshu  1 . more variation for boolean concepts leads to greater complexity  much conjunction and disjunction; ehrenfeucht et al.  1 . finally  concept difficulty also may be described using entropy  saxena  1; watanabe  1   which is commonly used to distinguish attribute relevance in splitting 
	rendell and ragavan 	1 
algorithms  quinlan  1  and transformation schemes  devyver & kittler  ch. 1 . 
　here we advocate an entropy measure called the blurring. the blurring of a concept is the average information in the concept over all relevant attributes  conditioned on each attribute in turn. this estimates scattering and interaction because each term in the conditional expression corresponds to a one-dimensional pro-
jection in instance space. one attribute alone provides little information about the concept class when scattering and interaction are high. for difficult concepts  any one-dimensional projection is a  blur  comprising many highs and lows of concept class-membership  positive and negative examples . consequently  such a projection shows a large degree of uncertainty about the concept. in general  the greater the difficulty of the concept  the more blurred these projections become.  instead of averaging attribute contributions  variants of this definition might be used  as discussed later.  

1 	measurement issues 
a measure to characterize concept difficulty should have certain qualities: it might correspond to a human perspective. the measure should track concept difficulty as experienced by some standard algorithms. to be useful in detailed studies  a measure should apply to a wide variety of domains and capture fine differences in concept difficulty. the measure should be defined for and computable from both the concept  usually for synthetic cases  and its training data  for real-world cases  when the concept is unknown . 
　according to these desiderata  some of the candidate measures seem particularly appropriate  and others less so. the number of peaks in the class-membership function is useful for controlled experiments with synthetic concepts  rendell & cho  1   but is coarse  since peaks vary in extent  height  and shape. moreover  the number of peaks is hard to measure in real data. in contrast  function variation and concept blurring are finegrained  and may be computed indirectly from the concept definition  calculating all values  or estimated directly from available data when the concept is unknown  ignoring missing instances . 
　like some other measures  blurring captures human preference for compact spatial representations and against scattered primitive representations. as detailed in section 1  a also estimates the difficulty experienced 
1 	machine learning 
by many learning algorithms  in terms of their accuracy. blurring is general  applicable to any type of feature   and precise  responsive to small variations in feature quality . 
　whether we choose blurring  variation  or any other datarbased quantity  our current definitions seem to be limited in one way or another. once again we focus on blurring. first  the exact form of the definition is questionable. to measure projection blurring accurately   should use only those attributes relevant for learning 
the concept. averaging may be good if the relevant attributes are known  but more commonly they are unknown  and too many irrelevant attributes artificially raises the blurring estimate. nevertheless  this is not a serious problem as long as most of the variables are relevant  which is true of our early studies . our current goal is to assess existing algorithms  which is somewhat insensitive to this issue. 
　another question is the importance of higher dimensional projections for more complete definitions of blurring. to measure blurring   estimates entropy in onedimensional projections. this simplification may have limited use  because it does not discriminate higher order interactions and therefore compresses estimates of difficulty at the higher end of the scale. however  our experiments suggest that much can be done with the simple definition  because typical  values are relatively low and current algorithms fail before a concept becomes very hard. 
　a final simplification in the present definition of blurring is its omission of instance space dimensionality  others have included dimensionality for different uses than ours   devijver & kittler  1; saxena  1 . we factor dimensionality out of the current experiments  by keeping it nearly constant. we explore the effects of concept scattering on various algorithms  which likely retain their ranking independent of dimensionality. 
　some of the other measures  such as the variation  avoid the above problems  although they have different drawbacks. as our understanding of algorithm behavior becomes more refined  blurring or related measures should be improved. 
1 	using the blurring measure 
1 	blurring examples and interpretation 
like holte   we found that many databases in the 
irvine repository are easy. if we measure the entropy for the best attribute  we often find values close to zero  though many of the irvine databases have values 1.1  likely because less relevant attributes mixed with one important one raise the average.  values of 1 or less often seem to indicate easy sbl learning; for example simple algorithms run on iris versicolor    = 1  give around 1% accuracy. 
　some of the irvine data bases have high values  such as pima diabetes with = 1  and  seven attribute  majority voting with = 1. other domains having high include protein structure and bankruptcy  ragavan et al.  1  this volume; also see rendell  1 . for boolean concepts  the highest blurring is for parity. in 
　

the most extreme feature interaction  no single attribute can individually provide information about the concept  but collectively the attributes provide full information. while parity and other boolean concepts are artificial  they can mimic hard concepts in real-world domains. in particular  degrees of interaction can be simulated using different orders of parity. parity attributes provide a necessary and sufficient  basis  set for boolean concepts  seshu  1 . 
　as defined  blurring is non-linear; higher values of compress intuitive differences. for example = 1 for pima diabetes indicates much greater difficulty than 
 = 1 for iris. beyond some threshold  around 1   increasing the blurring slightly requires many more data and a more complex algorithm to learn  as seen below. 
　although we use to measure concept scattering and attribute interaction  the data can be blurred for two other reasons. one is fundamental representation inadequacy  as opposed to representation unfavorable to the algorithm. fundamental inadequacy can cause one point in the instance space to represent different objects and class values; destroying such a distinction increases entropy. the other cause of blurring  entropy  is noise. attribute and class noise cause false positives and negatives  which increase scattering. 
　the blurring  captures all three factors that exacerbate concept learning  but does not discriminate them. if causes are unknown  the value of  becomes an upper limit for estimating concept scattering. for real-world domains  the exact sources of entropy are often uncertain  although other measures may conceivably disambiguate. in our controlled experiments using synthetic data  the only cause of blurring is scattering. 
　we must consider concept scattering and attribute interaction when developing learning algorithms for primitive representations. if we do not design experiments to assess induction systems on hard concepts  our design efforts will be handicapped. experiments would not diagnose system deficiencies  bradshaw  1 . blurring  or variation  or some characterization of real-world difficulty can facilitate and systematize the assessment of algorithms. 
1 	algorithm performance 
this summary of algorithm accuracy as a function of blurring omits much of the experimental detail found in  ragavan k rendell  1 . we ran several algorithms  including ids  c1  quinlan  1   fringe  greedys 
 pagallo  1   dcfringe  yang et al.  1   backprop 
 rumelhart et al.  1   mars   friedman  1   and lfc  ragavan k rendell; ragavan et al.  1   on four synthetic and four real-world concepts. figure 1 shows predictive accuracies obtained using ten-fold crossvalidation; differences are mostly significant at the 1 level. the curve labeled  best standard  is a composite formed by choosing  row by row  the best accuracy of all greedy algorithms that output logic or tree hypotheses  id1  greedys  dcfringe  etc. . this curve is a mixture of systems  ids or dcfringe often giving the best results. 
　the graph shows that the accuracy of all algorithms degrades with increasing  the nonlinear anomalies are unsurprising; one reason is that training sample sizes differ  ragavan k rendell  1 . otherwise  differences other than blurring have a relatively minor effect  rendell k cho  1 .  explains most of the accuracy differences: hard concepts are learned poorly by standard algorithms  see also rendell k seshu  1 . 
　but the tested algorithms differ markedly. this is contrary to the results of weiss and kapouleas  and mooney et al.   because their studies did not account for concept difficulty. when we consider concept scattering and attribute interaction as represented by the blurring  we see that backprop scales up better. lfc  ragavan k rendell  1; ragavan et al.  1  scales up best. for hard concepts  we find large accuracy differences such as 1% versus 1% and 1% versus 1%. 
　concept scattering and attribute interaction are the essence of difficulty in poorly-understood domains. the ability of algorithms to manage scattered concepts may be their least understood but most important functional difference. in the analysis below  we correspond the behavior of current algorithms with their design elements and with requirements for hard concepts. 
1 	algorithm functionality 
1 	basic sbl techniques 
similarity-based methods are sometimes dichotomized into iterative splitting algorithms  divide-and-conquer  e.g.  cart  breiman et al.  1; pls1  rendell  
1; ids  quinlan  1  and set covering algorithms 
 separate-and-conquer  e.g.  aq  michalski  1; cn1  clark k niblett  1; greedys  pagallo  1 . the former are often associated with decision tree representations  while the latter frequently use decision lists. basic iterative splitters use one attribute at a time to build a decision tree. although a greedy  hill-climbing strategy is efficient  it produces inaccurate trees when the concept is hard-when class membership is scattered and attributes interact  rendell k cho  1 . 
　in hard concepts a number of attributes together determine the class of an example; a single attribute provides little or no information about the class. conserendell and ragavan 1 
　
quently  depending upon statistical coincidences in the data  a decision tree learner can easily make a poor choice of attributes. simple boolean parity illustrates the effects of attribute interaction: if the concept is xor x 1  x1  the probability of an example from a uniform distribution being positive is 1 both before and after consideration of the value of x1 or x1 alone. each attribute by itself is uninformative  but greedy splitting still chooses one of them for creating a decision node  making an  uninformed  split. this produces a tree with three decision nodes  as the two conjunction relations x1 and x1 need to be captured to discriminate all the examples correctly. because every split on average halves the number of data  the likelihood of building the correct tree diminishes  unless the training sample is very large. this problem worsens if the parity  or any attribute interaction  is higher-order. 
　moreover  a greedy algorithm is likely to select irrelevant attributes: their discriminatory power is as good as  or better than  relevant but interacting attributes  statistical anomalies produce a stronger effect than the zero information in a relationship such as parity . the result of greedy splitting is a verbose tree. with limited training data  verbosity perpetuates poor node selection  now farther down the tree because of reduced statistical support there. one instance of this data fragmenting is pagallo's  fringe replication problem. 
　functionally  basic decision tree algorithms perform well on concepts having little concept scattering or attribute interaction  but otherwise these algorithms are highly inaccurate  as illustrated in fig. 1 . one way to attempt improvement of sbl algorithms is to search their design space. 
1 	some extensions and other designs 
some algorithms extend greedy splitters. fringe  pagallo  1   dcfringe  yang et al.  1  and citre  matheus & rendell  1   use the decision tree produced by a greedy splitter to construct new features for improving tree quality. repeated local patterns in the decision tree are coalesced to give new features. the new features are added to the original attributes for competitive splitting  to build a new tree. the process continues iteratively until no new features are found  giving a tree in which repeated patterns are eliminated. fringe conjoins two adjacent nodes near the positive leaves of the tree. dcfringe constructs both conjunctions and disjunctions based on the fringe structure. citre con-
joins adjacent fringe  root  or intermediate nodes. but such greedy feature construction algorithms improve relatively few concepts  yang et al.  1 . 
　fringe-like algorithms are limited by the quality of the original decision tree from which they construct features. if the basis for the construction of the original tree is a greedy splitter  accuracies remain low  fig. 1 . with adequate data in a parity problem  fringe would eventually construct the right features  but this construction is coincidental. over many runs  is just as likely to construct irrelevant features because greedy splitters cannot discriminate properly. with limited data  even this inefficient technique will break down. 
1 	machine learning 
　other approaches than fringe transform decision trees for incremental induction. idl  van de velde  1  switches nodes in the tree based on frequency of attribute occurrence. indurkhya and weiss  discuss a similar attribute swapping technique. but these approaches are also limited by the quality of the algorithm that produces the original decision tree. 
　other design varieties are also possible. instead of extending greedy sbl  one could use a different representation. a decision list is a set of rules involving con-
junctions of attributes  rivest  1 . each conjunction is a specific rule for attribute interaction. however  algorithms such as greedys  pagallo  1  are not designed to find the precise interaction except by assessing the marginal effect of one attribute at a time. as currently designed  these algorithms also suffer greedy limitations. consequently  they perform poorly for harder problems  as shown in figure 1. 
　although search of design space is necessary  fast advancement involves a good decomposition of functionality  bradshaw  1 . decision lists and greedy algorithms are functional: both are designed for economy. but this is just one of many functions. to guide design space search  we need to focus on the critical dimensions of algorithm functionality. 
　in terms of section 1  greedy algorithms are good at the region description function rd as long as the blurring is low and attribute interaction is weak. but for hard concepts  finding the regions of unform class membership is not so simple  because peaks and valleys are blurred in greedy projections. to help find regions  the learning approach needs to discover the specifics of attribute interaction by countering blurring. 
1 	finding interactions using lookahead 
backpropagation and statistical techniques designed to handle attribute interaction have had good results across a variety of domains  rumelhart et al.  1; friedman  1 . another way to manage interaction is to look ahead in a decision tree  effectively using all multidimensional projections of the training sample on the attributes. an sbl splitting algorithm could look ahead to observe the effects of current attribute selection further down in the hypothesis construction. the best sequence is found after evaluating all expansions of a decision tree. this avoids problems of attribute interaction by evaluating combinations of features. norton  found that his exhaustive lookahead algorithm idx gave fairly good improvement. 
　exhaustive lookahead  however  is expensive. we found that the lookahead program within buntine's and caruana's  ind takes hours to run for moderately difficult financial concepts  ragavan et al.  1  this volume . even with the fastest available architectures  dimensionality growth in the training data can swamp the system  because the search space grows doubly exponentially as the lookahead increases. 
　greedy tree building is too limited and exhaustive lookahead is too expensive. so a cure for attribute interaction may lie in dynamic lookahead. for fixed dimensionality instance space  naive lookahead would evaluate the same number of features at every node  independent of concept complexity. instead  ragavan's lookahead feature construction lfc uses a combination of several techniques to guide lookahead search. the algorithm applies both analytic and heuristic methods to decide dynamically which paths to attempt and how far to pursue the search. results showed reasonable speeds. 
　however  even lookahead is not enough to maximize performance. we found that accuracy improves further when feature construction caches the information  ragavan & rendell  1 .  figure 1 summarizes some accuracy results.  feature construction also benefits comprehensibility and abstraction. 
　in terms of section 1  sbl with lookahead performs the region description function rd well  even when concepts are scattered and attributes interact. to implement the coalescing abstraction function ca  an algorithm can create compact hypotheses by finding suitable new features. 
1 	compressing hypotheses using new features 
even with lookahead to specify attribute relationships  an independent problem remains. the replication problem  pagallo  1  is just one type of verbosity. in hard concepts  local  fringe  replications are relatively few  yang et al.  1   yet  complex global replication  seems prevalent. global replication is the disguised occurrence of similar attribute combinations in different parts of the tree. many hard concepts require repeated decision patterns which are distributed throughout the tree and not readily recognizable  rather than being confined to the fringes and clearly demarcated. 
　for example  given all the lookahead it needs  a greedy learner for two-parity  xor  builds a tree having three layers and seven nodes. this verbosity has splits on one attribute occurring on two subtrees of the root. with limited training data  or with fixed training sample size but higher-degree attribute interaction  the data are exponentially decimated as the tree is grown. 
　to alleviate this rapid data fragmenting  separate-andconquer methods have been developed that increase the available data for further hypothesis growth. decision list algorithms such as cn1  clark & niblett  1  and greedys  pagallo  1  construct a new feature at every node  and retain the examples not covered by the feature for further list construction. as features are formed by conjoining data attribute literals to give specialized terms  this technique increases the available data for making further statistical inferences to construct subsequent features. 
　but this approach also creates some problems. one is that specializing on each disjunct of the concept at every step can cause the hypothesis to grow rapidly for concepts having numerous disjuncts scattered over the instance space. this was the case with a bankruptcy concept  where greedys gave an accuracy of 1%  compared with lfc's 1%  ragavan et al.  1 . 
　lfc handles global replication differently. complex concepts typically contain implicit patterns caused by prominent relationships among the original data attributes. the algorithm caches search information by constructing useful new features  e.g.  sequences of decisions down a path of a tree is a conjunction of attributes . in the xor example  lfc constructs new features such as f1 = x1 a x1 and f1 = x1 ax1  to build a concise and accurate tree  f1 and f1 represent abstractions  closer  to the parity concept than x1 or x1- new features describe relationships intermediate between the original attributes and the concept. lfc tackles the feature interaction problem directly  constructing features while building the decision tree. the new features compress the hypothesis. 
　compared with cn1  greedys  and gs  murphy k pazzani  1   lfc differs primarily because it tackles both the attribute interaction problem  using directed lookahead  and the global replication problem  through feature construction . norton's  idx also reacts to the required degree of conjunction  though it uses exhaustive lookahead and does not cache features. the  lookback  techniques of indurkhya and weiss  and van de velde  do not construct new features and so are limited to the expressive power of the original attributes at each node. 
　lfc's combination of dynamic lookahead and feature construction consistently and considerably outperformed all other algorithms we ran  especially on hard concepts  ragavan & rendell  1  because the design addressed both attribute interaction and representation verbosity. these design requirements resulted from an analysis of the nature of hard concepts. when concepts are scattered and their attributes interact  the region description function rd leads to lookahead  or its equivalent . when concepts are scattered and their attributes interact  the coalescing abstraction function ca leads to construction of new features  found during lookahead  that compress the representation by reducing global replication through economic abstraction. 
1 	conclusion 
these results show the importance of careful analysis. 
currently many learning systems perform very poorly  displaying accuracies  fig. 1  tens of percentage points worse than lfc  ragavan and rendell  1; ragavan et al.  1  this volume . this new feature construction system was designed for hard real-world problems characterized by attribute interaction and concept scattering. to continue to improve algorithm design  we need to pay close attention to the nature of real-world problems. concept difficulty measures such as variation and entropy can help pinpoint algorithm behavior and aid system development. 
