
this work presents a new algorithm  called heuristically accelerated minimax-q hammq   that allows the use of heuristics to speed up the wellknown multiagent reinforcement learning algorithm minimax-q. a heuristic function h that influences the choice of the actions characterises the hammq algorithm. this function is associated with a preference policy that indicates that a certain action must be taken instead of another. a set of empirical evaluations were conducted for the proposed algorithm in a simplified simulator for the robot soccer domain  and experimental results show that even very simple heuristics enhances significantly the performance of the multiagent reinforcement learning algorithm.
1 introduction
reinforcement learning  rl  techniques are very attractive in the context of multiagent systems. the reasons frequently cited for such attractiveness are: many rl algorithms guarantee convergence to equilibrium in the limit  szepesvari and littman  1   they are based on sound theoretical foundations  and they provide model-free learning of adequate suboptimal control strategies. besides that  they are also easy to use and have been applied to solve a wide variety of control and planning problems when neither an analytical model nor a sampling model is available a priori.
¡¡in rl  learning is carried out on line  through trial-anderror interactions of the agent with its environment  receiving a reward  or penalty  at each interaction. in a multiagent system  the reward received by one agent depends on the behaviour of other agents  and a multiagent reinforcement learning  mrl  algorithm needs to address non-stationary scenarios in which both the environment and other agents must be represented. unfortunately  convergence of any rl algorithm may only be achieved after extensive exploration of the state-action space  which can be very time consuming.
the existence of multiple agents increases by itself the size of the state-action space  worsening the rl algorithms performance when applied to multiagent problems. despite that  several mrl algorithms have been proposed and successfully applied to some simple problems  such as the minimaxq  littman  1   the friend-or-foe q-learning  littman  1  and the nash q-learning  hu and wellman  1 .
¡¡an alternative way of increasing the convergence rate of an rl algorithm is to use heuristic functions for selecting actions in order to guide the exploration of the state-action space in an useful way. the use of heuristics to speed up reinforcement learning algorithms has been proposed recently by bianchi et al.   and it has been successfully used in policy learning for simulated robotic scenarios.
¡¡this paper investigates the use of heuristic selection of actions in concurrent and on-policy learning in multiagent domains  and proposesa new algorithmthat incorporatesheuristics into the minimax-q algorithm  the heuristically accelerated minimax-q  hammq . a set of empirical evaluation of hammq were carried out in a simplified simulator for the robot soccer domain  littman  1 . based on the results of these simulations  it is possible to show that using even very simple heuristic functions the performance of the learning algorithm can be improved.
¡¡the paper is organised as follows: section 1 briefly reviews the mrl approach and describes the minimax-q algorithm  while section 1 presents some approaches to speed up mrl. section 1 shows how the learning rate can be improved by using heuristics to select actions to be performed during the learning process  in a modified formulation of the minimaxq algorithm. section 1 describes the robotic soccer domain used in the experiments  presents the experiments performed  and shows the results obtained. finally  section 1 provides our conclusions.
1 multiagent reinforcement learning
markov games  mgs  - also known as stochastic games  sgs  - are an extension of markov decision processes  mdps  that uses elements from game theory and allows the modelling of systems where multiple agents compete among themselves to accomplish their tasks.
formally  an mg is defined by  littman  1 :
  s: a finite set of environment states.
  a1 ...ak: a collection of sets ai with the possible actions of each agent i.
  t : s ¡Áa1¡Á...¡Áak ¡ú ¦° s : a state transition function that depends on the current state and on the actions of each agent.
: a set of reward functions
specifying the reward that each agent i receives.
¡¡solving an mg consists in computing the policy ¦Ð : s ¡Á a1¡Á...¡Áak that maximizes the reward received by an agent along time.
¡¡this paper considers a well-studied specialization of mgs in which there are only two players  called agent and opponent  having opposite goals. such specialization  called zerosum markov game  zsmg   allows the definition of only one reward function that the learning agent tries to maximize while the opponent tries to minimize.
¡¡a two player zsmg  littman  1  is defined by the quintuple  where:
  s: a finite set of environment states.
  a: a finite set of actions that the agent can perform.
  o: a finite set of actions that the opponent can perform.
  t : s ¡Á a ¡Á o ¡ú ¦° s : the state transition function  where ¦° s  is a probability distribution over the set of states defines a probability of transition from state  at a time t + 1  when the learning agent executes action a and the opponent performs action o.
: the reward function that specifies the reward received by the agent when it executes action a and its opponent performs action o  in state s.
¡¡the choice of an optimal policy for a zsmg is not trivial because the performance of the agent depends crucially on the choice of the opponent. a solution for this problem can be obtained with the minimax algorithm  russell and norvig  1 . it evaluates the policy of an agent regarding all possible actions of the opponent  and then choosing the policy that maximizes its own payoff.
¡¡to solve a zsmg  littman  proposed the use of a similar strategy to minimax for choosing an action in the qlearning algorithm  the minimax-q algorithm  see table 1 . minimax-q works essentially in the same way as q-learning does. the action-value function of an action a in a state s when the opponent takes an action o is given by:

and the value of a state can be computed using linear programming  strang  1  via the equation:
	 	 1 
where the agent's policy is a probability distribution over actions  ¦Ð ¡Ê ¦° a   and ¦Ða is the probability of taking the action a against the opponent's action o.
¡¡an mg where two players take their actions in consecutive turns is called an alternating markov game  amg . in this initialise q t s a o .
repeat:
visit state s.
select an action a using therule  eq. 1 .
execute a  observe the opponent's action o. receive the reinforcement r s a o  observe the next state s.
update the values of q  s a o  according to:
. .
until some stop criterion is reached.

table 1: the minimax-q algorithm.
case  as the agent knows in advance the action taken by the opponent  the policy becomes deterministic  ¦Ð : s ¡Á a ¡Á o and equation 1 can be simplified:
	v  s  = maxminq s a o .	 1 
a¡Êa o¡Êo
¡¡in this case  the optimal policy is ¦Ð  ¡Ô argmaxa mino q  s a o . a possible action choice rule to be used is the standard:
	min q  s a o 	if q ¡Ü p 
¦Ð s  =o	 1  arandom otherwise 
where q is a random value with uniform probability in  1  and p  1 ¡Ü p ¡Ü 1  is a parameter that defines the exploration/exploitation trade-off: the greater the value of p  the smaller is the probability of a random choice  and arandom is a random action selected among the possible actions in state s. for non-deterministic action policies  a general formulation of minimax-q has been defined elsewhere  littman  1; banerjee et al.  1 .
¡¡finally  the minimax-q algorithm has been extended to cover several domains where mgs are applied  such as robotic soccer  littman  1; bowling and veloso  1  and economy  tesauro  1 .
¡¡one of the problems with the minimax-q algorithm is that  as the agent iteratively estimates q  learning at early stages is basically random exploration. also  update of the q value is made by one state-action pair at a time  for each interaction with the environment. the larger the environment  the longer trial-and-error exploration takes to approximate the function q. to alleviate these problems  several techniques  described in the next section  were proposed.
1 approaches to speed up multiagent reinforcement learning
the minimax-sarsa algorithm  banerjee et al.  1  is a modification of minimax-q that admits the next action to be chosen randomly according to a predefined probability  separating the choice of the actions to be taken from the update of the q values. if a is chosen according to a greedy policy  minimax-sarsa becomes equivalent to minimax-q. but if a is selected randomly according to a predefined probability distribution  the minimax-sarsa algorithm can achieve better performance than mimimax-q  banerjee et al.  1 .
¡¡the same work proposed the minimax-q ¦Ë  algorithm  by combining eligibility traces with minimax-q. eligibility traces  proposed initially in the td ¦Ë  algorithm  sutton  1   are used to speed up the learning process by tracking visited states and adding a portion of the reward received to each state that has been visited in an episode. instead of updating a state-action pair at each iteration  all pairs with eligibilities different from zero are updated  allowing the rewards to be carried over several state-action pairs. banerjee et al.  also proposed minimax-sarsa ¦Ë   a combination of the the two algorithms that would be more efficient than both of them  because it combines their strengths.
¡¡a different approach to speed up the learning process is to use each experience in a more effective way  through temporal  spatial or action generalization. the minimax-qs algorithm  proposed by ribeiro et al.   accomplishes spatial generalization by combining the minimax-q algorithm with spatial spreading in the action-value function. hence  at receiving a reinforcement  other action-value pairs that were not involved in the experience are also updated. this is done by coding the knowledge of domain similarities in a spreading function  which allows a single experience  i.e.  a single loop of the algorithm  to update more than a single cost value. the consequence of taking action at at state st is spread to other pairs  s a  as if the real experience at time t actually was.
1 combining heuristics and multiagent reinforcement learning: the hammq algorithm
the heuristically accelerated minimax q  hammq  algorithm can be defined as a way of solving a zsmg by making explicit use of a heuristic functionto influence the choice of actions during the learning process. h s a o  defines a heuristic that indicates the desirability of performing action a when the agent is in state s and the opponent executes action o.
¡¡the heuristic function is associated with a preference policy that indicates that a certain action must be taken instead of another. therefore  it can be said that the heuristic function defines a  heuristic policy   that is  a tentative policy used to accelerate the learning process. the heuristic function can be derived directly from prior knowledge of the domain or from clues suggested by the learning process itself and is used only during the selection of the action to be performed by the agent  in the action choice rule that defines which action a should be executed when the agent is in state s. the action choice rule used in hammq is a modification of the standard rule that includes the heuristic function:
¦Ð s  =
 1 
whereis the heuristic function. the subscript t indicates that it can be non-stationary and ¦Î is a real variable used to weight the influence of the heuristic  usually 1 .
¡¡as a general rule  the value of ht s a o  used in hammq should be higher than the variation among the q  s a o  values for the same s ¡Ê s  o ¡Ê o  in such a way that it can influence the choice of actions  and it should be as low as possible in order to minimize the error. it can be defined as:
s i o    q  s a o  + ¦Ç if a = ¦Ðh s  
h s a o  =
.
 1 
where ¦Ç is a small real value  usually 1  and ¦Ðh s  is the action suggested by the heuristic policy.
¡¡as the heuristic function is used only in the choice of the action to be taken  the proposed algorithm is different from the original minimax-q in the way exploration is carried out. since the rl algorithm operation is not modified  i.e.  updates of the function q are the same as in minimax-q   our proposal allows that many of the theoretical conclusions obtained for minimax-q remain valid for hammq.
theorem 1 consider a hammq agent learning in a deterministic zsmg  with finite sets of states and actions  bounded rewards   discount factor ¦Ã such that 1 ¡Ü ¦Ã   1 and with values used in the heuristic function bounded by   s a o  hmin ¡Ü h s a o  ¡Ü hmax.
for this agent  q  values will converge to q   with probability one uniformly over all states s ¡Ê s  provided that each stateaction pair is visited infinitely often  obeys the minimax-q infinite visitation condition .
proof: in hammq  the update of the value function approximation does not depend explicitly on the value of the heuristic function. littman and szepesvary  presented a list of conditions for the convergence of minimax-q  and the only condition that hammq could put in jeopardy is the one that depends on the action choice: the necessity of visiting each pair state-action infinitely often. as equation 1 considers an exploration strategy - greedy regardless of the fact that the value function is influenced by the heuristic function  this visitation condition is guaranteed and the algorithm converges. q.e.d.
¡¡the condition that each state-action pair must be visited an infinite number of times can be considered valid in practice - in the same way that it is for minimax-q - also by using other strategies:
  using a boltzmann exploration strategy  kaelbling et al.  1 .
  intercalating steps where the algorithm makes alternate use of the heuristic function and exploration steps.
  using the heuristic function during a period of time shorter than the total learning time.
¡¡the use of a heuristic function made by hammq explores an important characteristic of some rl algorithms: the free choice of training actions. the consequence of this is that a suitable heuristic function speeds up the learning process  otherwise the result is a delay in the learning convergence that does not prevent it from converging to an optimal value.
initialise q t s a o  and ht s a o .
repeat:
visit state s.
select an action a using the modified
execute a  observe the opponent's action o. receive the reinforcement r s a o  observe the next state s.
update the values of ht s a o .
update the values of q  s a o  according to:
.
.
until some stop criterion is reached.

table 1: the hammq algorithm.
¡¡the complete hammq algorithm is presented on table 1. it is worth noticing that the fundamental difference between hammq and the minimax-q algorithm is the action choice rule and the existence of a step for updating the function ht s a o .
1 robotic soccer using hammq
playing a robotic soccer game is a task for a team of multiple fast-moving robots in a dynamic environment. this domain has become of great relevance in artificial intelligence since it possesses several characteristics found in other complex real problems; examples of such problems are: robotic automation systems  that can be seen as a group of robots in an assembly task  and space missions with multiple robots  tambe  1   to mention but a few.
¡¡in this paper  experiments were carried out using a simple robotic soccer domain introduced by littman  and modelled as a zsmg between two agents. in this domain  two players  a and b  compete in a 1 x 1 grid presented in figure 1. each cell can be occupied by one of the players  which can take an action at a turn. the action that are allowed indicate the direction of the agent's move - north  south  east and west - or keep the agent still.
¡¡the ball is always with one of the players  it is represented by a circle around the agent in figure 1 . when a player executes an action that would finish in a cell occupied by the opponent  it looses the ball and stays in the same cell. if an action taken by the agent leads it out the board  the agent stands still.
¡¡when a player with the ball gets into the opponent's goal  the move ends and its team scores one point. at the beginning of each game  the agents are positioned in the initial position  depicted in figure 1  and the possession of the ball is randomly determined  with the player that holds the ball making the first move  in this implementation  the moves are alternated between the two agents .
to solve this problem  two algorithms were used:
  minimax-q  described in section 1.
  hammq  proposed in section 1.

figure 1: the environment proposed by littman . the picture shows the initial position of the agents.
figure 1: the heuristic policy used for the environment of figure 1. arrows indicate actions to be performed.
¡¡the heuristic policy used was defined using a simple rule: if holding the ball  go to the opponent's goal  figure 1 shows the heuristic policy for player a of figure 1 . note that the heuristic policy does not take into account the opponents position  leaving the task of how to deviate from the opponentto the learning process. the values associated with the heuristic function are defined based on the heuristic policy presented in figure 1 and using equation 1.
¡¡the parameters used in the experiments were the same for the two algorithms  minimax-q and hammq. the learning rate is initiated with ¦Á = 1 and has a decay of 1 by each executed action. the exploration/ exploitation rate p = 1 and the discount factor ¦Ã = 1  these parameters are identical to those used by littman  . the value of ¦Ç was set to 1. the reinforcement used was 1 for reaching the goal and -1 for having a goal scored by the opponent. values in the q table were randomly initiated  with 1 ¡Ü q s a o  ¡Ü 1. the experiments were programmed in c++ and executed in a amd k1-ii 1mhz  with 1mb of ram in a linux platform.
¡¡thirty training sessions were run for each algorithm  with each session consisting of 1 matches of 1 games. a game finishes whenever a goal is scored by any of the agents or when 1 moves are completed.
¡¡figure 1 shows the learning curves  average of 1 training sessions  for both algorithms when the agent learns how to play against an opponent moving randomly  and presents the average goal balance scored by the learning agent in each

figure 1: average goal balance for the minimax-q and hammq algorithms against a random opponent for
littman's robotic soccer.

figure 1: average goal balance for the minimax-q and hammq algorithms against an agent using minimax-q for littman's robotic soccer.
match. it is possible to verify that minimax-q has worse performance than hammq at the initial learning phase  and that as the matches proceed  the performance of both algorithms become similar  as expected.
¡¡figure 1 presents the learning curves  average of 1 training sessions  for both algorithms when learning while playing against a learning opponent using minimax-q. in this case  it can be clearly seen that hammq is better at the beginning of the learning process and that after the 1th match the performance of both agents becomes similar  since both convergeto equilibrium.
¡¡student's t-test  spiegel  1  was used to verify the hypothesis that the use of heuristics speeds up the learning process. figure 1 shows this test for the learning of the minimaxq and the hammq algorithms against a random opponent  using data shown in figure 1 . in this figure  it is possible to

figure 1: results from student's t test between minimax-q and hammq algorithms  training against a random opponent.

figure 1: results from student's t test between minimaxq and hammq algorithms  training against an agent using minimax-q.
see that hammq is better than minimax-q until the 1th match  after which the results are comparable  with a level of confidence greater than 1%. figure 1 presents similar results for the learning agent playing against an opponent using minimax-q.
¡¡finally  table 1 shows the cumulative goal balance and table 1 presents the number of games won at the end of 1 matches  average of 1 training sessions . the sum of the matches won by the two agents is different from the total number of matches because some of them ended in a draw. what stands out in table 1 is that  due to a greater number of goals scored by hammq at the beginning of the learning process  this algorithm wins more matches against both the random opponent and the minimax-q learning opponent.
training sectioncumulative goal balanceminimax-q ¡Á random 1 ¡À 1  ¡Á  1 ¡À 1 hammq ¡Á random 1 ¡À 1  ¡Á  1 ¡À 1 minimax-q ¡Á minimax-q 1 ¡À 1  ¡Á  1 ¡À1  hammq ¡Á minimax-q 1 ¡À 1  ¡Á  1 ¡À 1 table 1: cumulative goal balance at the end of 1 matches  average of 1 training sessions .
training sectionmatches wonminimax-q ¡Á random 1 ¡À 1  ¡Á  1 ¡À 1 hammq ¡Á random 1 ¡À 1  ¡Á  1 ¡À 1 minimax-q ¡Á minimax-q 1 ¡À 1  ¡Á  1 ¡À 1 hammq ¡Á minimax-q 1 ¡À 1  ¡Á  1 ¡À 1 table 1: number of matches won at the end of 1 matches  average of 1 training sessions .
1 conclusion
this work presented a new algorithm  called heuristically accelerated minimax-q  hammq   which allows the use of heuristics to speed up the well-known multiagent reinforcement learning algorithm minimax-q.
¡¡the experimental results obtained in the domain of robotic soccer games showed that hammq learned faster than minimax-q when both were trained against a random strategy player. when hammq and minimax-q were trained against each other  hamm-q outperformed minimax-q.
¡¡heuristic functions allow rl algorithms to solve problems where the convergence time is critical  as in many real time applications. this approach can also be incorporated into other well known multiagent rl algorithms  e.g. minimaxsarsa  minimax-q ¦Ë   minimax-qs and nash-q.
¡¡future works include working on obtaining results in more complex domains  such as robocup 1d and 1d simulation and small size league robots  kitano et al.  1 . preliminary results indicate that the use of heuristics makes these problems easier to solve. devising more convenient heuristics for other domains and analyzing ways of obtaining them automatically are also worthwhile research objectives.
