 
most constructive induction researchers focus only on new boolean attributes this paper reports a new constructive induction algorithm  called xofn  that constructs new nominal attributes in the form of x-of-n representations an x-of-n is a bet containing one or more attribute-value pairs for a given instance  its value corresponds to the number of its at tribute-value pairs that are true the promising preliminary experimental results  on both artificial and real-world domains  show that constructing new nominal attributes in the form of x-of-n representations can significantly improve the performance of selective induction in terms of both higher prediction accuracy and lower theory complexity 
1 	introduction 
a well-known elementary limitation of selective indue tion algorithms is that when task-supplied attributes are not adequate for describing hypotheses  their performance in terms of prediction accuracy and/or theory complexity is poor to overcome this limitation  constructive mduction algorithms  michalski  1  transform the original instance space into a more adequate space by creating new attributes by contrast to new attributes  the task-supplied attributes are called primitive attributes 
　on real-world application domains  domain experts use three different types of attributes binary  nominal  and continuous-valued attributes 1 to describe their examples and concepts different types of attributes have different advantages for example  boolean attributes are very simple  nominal and continuous-valued attributes are complex but more powerful for representing concepts note that attributes with more than two ordered discrete values can be specified as either nominal or continuous-valued attributes 
　most selective mduction algorithms can accept attributes of these three kinds however  many existing 
　　some more sophisticated attributes such as structured attributes may be used  but here we talk about only these three most commonly used types of attributes 
1 	learning 
constructive induction algorithms such as fringe  pagallo  1  and citre  matheus and rendell  1  construct new boolean attributes only by using logical operators such as a  --  and v id1-of-1  murphy and pazzani  1l  creates  as new attributes  m-of-n representations statmg whether at least m of n conditions are true m-of-n representations are also boolean attributes 
　a few systems such as bacon  langley et ai} 1  and induce  michalski  1  explore methods to construct new continuous-valued attributes using mathe1matical operators such as multiplication and division systems such as lmdt  brodley and utgoff  1  and swapl  indurkhya and weiss  1  construct linear discriminant functions as new attributes to the best of the author's knowledge  few researchers have developed constructive mduction systems that construct new nominal attributes one exception is that induce  aq1-dci  and aq1-mci construct attribute counting attributes for rule learning  michalski  1  bloedorn et al  1  they have ordered discrete values  but are used more like continuous-valued attributes rather than nominal attributes  see section 1 for the differences from x-of-n  subsetting used by c1 groups discrete values of a smgle primitive nominal attribute to form a new test  quinlan  1  it can be thought as a method of constructing new nominal attributes the author knows of no other decision tree algorithm that constructs new nominal attributes 
　in this paper  we propose a new constructive mduction algorithm that constructs nominal attributes in the form of x-of-n representations our implemented system  called xofn  uses decision trees as its theory description language however  the idea of constructing new nominal attributes in the form of x-of-n representations is not limited to decision tree learning it is not difficult to extend the idea to rule learning 
　the following section describes x-of-n representations section 1 presents the approach to constructing new nominal attribute* in the form of x-of-n representations and the constructive mduction algorithm xofn section 1 reports experimental results of xofn on several artificial and real-world domains section 1 further discusses the x-of-n representations and the xofn algorithm section 1 discusses related work  and finally  the paper concludes with future work 


figure 1 a tree on the monks1 domain 
1 	x-of-n representation 
in short  an x-of-n is a set containing one or more attribute-value pairs for a given instance  the value of an x-of-n representation corresponds to the number of its attribute-value pairs that are true 
definition 1 the x-of-n representation 
　let {ax | 1   t   maxatt  be the set of attributes of a domain  and for each alt {vx1 | 1   ;   maxattvalx  be its value set 1 where maxatt is the number of attributes  and maxattval  is the number of different values of ax an x-of-n representation is a set  denoted as 
x-of-f avk | avt is an attnbute-value pair denoted as 

where n+ u the number of attnbute-value pairs m the x-of-n representation  called the size of the x-of-n rep resentatwn  and n is the number of different attributes that appear in the x-of-n representation the value of an x-of-n can be any number between 1 and n 
　given an instance  its value is x iff x of the avk are true an at tribute- value pair . ' is true for an instance iff attribute .a  of the instance has value vtj 
　now  let us see an example of the x-of-n representation the target concept of the monks1 problem  thrun et al  1  is  exactly two of the six attributes have their first valuen it can be represented using a tree as in figure 1 the test at the root is a nominal x-of-n representation given instances  and  its values are 1 and 1 respectively 
　aa a nominal attribute  the x-of-n representation has one main advantage over conjunction  disjunction  and m-of-n representations stronger expressive power without expanding the search space 
　many constructive induction algorithms such as fringe  citre  and ci  zheng  1  use a and -. as constructive operators they can onl  indirectly represent disjunctive concepts by using the negation of a conjunction this makes it harder to construct new attributes in the form of disjunctions  especially on domains with primitive nominal attributes some algorithms also have v as a constructive operator that makes it easier to create disjunctions however  there are still some other concepts such as parity concepts  atleaat  exactly  at-most m-of-n concepts  and their possible combinations  that cannot be effectively represented id1-of-1 and mon  ting  1  can only create at-least 
m-of-n representations from the definition  we can see that x-of-n can directly represent all the following types of concepts 
a
　　at the moment  the x-of-n representation is defined on binary and nominal attributes continuous-valued attnbutes are transformed into binary or nominal attributes by discretization 

  disjunction with internal disjunction  as x-of-n in 
	{1  	 n}  and 
  possible combinations of the above six types of concepts 
　compared with m-of-n representations  the search space of x-of-n representations is smaller this is because to create an m-of-n representation  we must first search for n+ at tribute-value pairs and then search for a value m when the number n is large  searching for a good value m can be expensive however  to create an x-of-n representation  we need only search for n+ at tribute-value pairs x-of-n representations have as large a search space as conjunction and disjunction representations 
　one disadvantage of x-of-n representations is a  fragmentation  problem when x-of-n representations with large ns are used as tests for decision trees  they quickly split training sets into a large number of small subsets this makes it harder to find good tests for the subtrees however  this situation occurs only when training sets are relatively small and target concepts are very complex m the sense that a few x-of-n representations are not enough if target concepts are not complex or huge data sets are available  it is not a problem 
1 	constructing new nominal attributes for decision trees 
now  let us describe how to create and use x-of-n representations in constructive mduction xofn uses the wellknown decision tree learning algorithm c1  quinlan  1  as its selective induction component it consists of a single process as shown in table 1  xofn builds a decision tree by constructing  at each decision node  one new attribute based on primitive attributes using the local training set if the new attribute constructed at a node is better than all primitive attributes and previously created new attributes  xofn uses it as the test for the node  otherwise discards it and uses the best of the primitive attributes and previously constructed new attributes after growing a tree  xofn apphes the pruning mechanism used by c1  quinlan  1  
　decision trees built by conventional tree learning algorithms such as c1 use a test based on one attribute at each decision node they are called univariate trees  brodley and utgoff  1  by contrast  xofn creates multivariate trees in which tests can refer to multiple attributes we call them x-of-n trees as x-of-n representations are used as multivariate tests at each decision node  besides creating one new x-of-n attribute  xofn considers reusing x-of-n attributes constructed previously for other decision nodes 
　function  construct-xof-n     in xofn constructs the best x-of-n representation using the local training set 
1
this means that x-of-n equals   or n 
zheng 

xofn-tree aprimulve  	aactxw  	dtmming  	c  
input apnrmtive a set of primitive attributes  
aactive a set of primitive and x-of-n attributes for creating a test for the current decision node  initialized as aprimitive 
dtraining a set of training examples represented using active 
c majority class at the parent node of the current node  initialized as the majority class in the whole training set 
output a decision tree  
aactive modified bv adding one new x-of-n attribute constructed at this point 
if
 dtraining is empty  
then return a leaf node labeled with c 
else 
{ c = the majority class in dtraining 
if  all examples in dtraining have the same class c  then return a leaf node labeled with c 
else 

j 
} 
table 1 kernel of the xofn algorithm 
at a decision node it performs a simple greedy search in the instance space defined by primitive attributes the starting point of the search is an empty x-of-n attribute at each search step  it applies one of two operators adding one possible attribute-value pair or deleting one possible attribute-value pair to make the search efficient  if possible 1 the deleting operator is applied first during the search  xofn creates and keeps the beat x-of-n representation for each possible size finally  function  construct-x-of-n     returns the best of the x-of-n representations retained 
　the information gain ratio1 is used as the evaluation function for comparing and belecting new attributes to 
   *lf the size of an x-of-w representation is. less than or equal to two  the deleting operator cannot be applied  because the best x-of-n of 1ize one has already been found 
1  as used by c1 
1 	learning 
avoid creating very complex new attributes that might over-fit the training data  another criterion is added  based on the coding cost of new attributes by complex  we mean that an x-of-n representation has a large number of attribute-value pairs 1 this land of new attribute splits the training set into many subsets  so over-fitting is likely to occur we use a similar coding method described in  quinlan and rivest  1  the coding cost of a new attribute includes two parts one is for coding the new attribute itself the other is for coding the exceptions when applying the new attribute as a classifier to the local training data at the current decision node there is no weight added to either part the newly constructed x-of-n representation  new-x-of-n  will replace the current best x-of-n  best-x-of-n  to become the best 
one only if the following condition is true 

with this condition  the algorithm accepts a new attribute with a higher gain ratio if its codmg cost is not higher the algorithm also accepts a new attribute with the same gain ratio but with a lower coding cost 
　the stopping criterion for searching x-of-n representations is that the maximum possible size of x-of-n representations is reached to reduce the search time  another restriction is applied if no better new attribute has been found in five consecutive search steps 1 the algorithm terminates 
　from the definition of x-of-n representations  it may seem that x-of-n representations can only be directly constructed from binary and nominal attributes to deal with continuous-valued attributes  the xofn algorithm has a preprocessor that discretizes primitive continuousvalued attributes in the current implementation  we use a  ery simple method  although some better  but more complex  methods could be used when there are some primitive continuous-valued attributes  xofn runs c1 on primitive attributes once and gets cut points for continuous-valued attributes then it discretizes the continuous-valued attributes using the cut points the new attribute construction is earned out on the discretized attributes  binary attributes  and nominal attributes 
1 	experiments 
the most commonly used measures of the behavior of learning systems are prediction accuracy of learned theories on test examples  theory complexity  and tune complexity theory complexity  abbreviated to complexity later in this paper  is the size of a learned theory for a rule set  it is the number of all conditions in the rule set for a tree  it is the sum of sizes of all nodes of the tree the size of a leaf is 1 the size of a decision node is 1 for a univariate tree  and is the number of at tribute- value pairs m the test of the node for a multivariate tree such 
1
　　we talk about the situation that most primitive attributes in the x-of-n representation are different 
1 this default setting is arbitrary  but it can be changed 
there is no special reason for selecting  five  


	table 1 	description of real-world domains 
as a tree built by xofn  id1-of-1  or ci time complexity is the execution time needed by a learning algorithm we focus here on the first two measures  accuracy and theory complexity  because time results of other algorithms are not available or different computers and/or programming languages are used as far as the time complexity is concerned  xofn is much slower than the selective tree learning algorithm c1  but is still acceptable for example  on a dec a x p 1 workstation the cpu time for xofn on the cleveland. heart disease domain  1 continuous-valued attributes  1 classes  1 cases  is 
1 seconds  while it is 1 seconds for c1 on the nettalk  phoneme  domain  1 nominal attributes with 1 different values  1 classes  1 cases   it is 1 seconds for xofn and 1 seconds for c1 
   to evaluate experiment all y the effects of constructing new nominal attributes in the form of x-of-n reprewntations  the results of xofn on a set of artificial and real-world domains are given for comparison  e also give the results of some other constructive induction algorithms aq1-hci  aq1-dci  aq1-mci  id1-of-1  and our algorithm ci  zheng  1  ci creates new attributes for decision trees by using conjunctions of conditions from production rules generated by c1rules  quinlan  1  the default option setting of ci is used here to create a new attribute  ci chooses two conditions  which are near the root of a tree  from a rule with this setting  ci constructs new attributes based on two primitive attributes  and identifies relevant attributes  zheng  1  the results of c1 are given for reference 
1 	e x p e r i m e n t a l domains and methods 
   three monks domains  thrun et al  1  are chosen because they are well-studied there are published results for more than 1 different learning algorithms on them they represent three different types of learning tasks with two binary and four nominal attributes to make the monks1 problem harder  especially for simple m-of-n learning methods   bloedorn et al  1  creates 
b there are fifty four english phonemes  but phonemes for 
 word boundary  and  period  do not appear in the dataset 
     b there are six english stresses  but one for  word boundary  does not appear in the dataset 
　　1 the number of all possible phoneme-stress pairs that appear in the dictionary of 1 words is 1 
the  noisy and irrelevant monks1  problem by adding 1% random classification noise  by inverting the classes  in the training set  and adding seven random five-value irrelevant attributes in both the training and teat sets there is no classification noise in the test set on all these four domains  the fixed training set  a subset of the whole universe  and test 1et  the whole universe  are given by the problem designers in our experiments we follow this methodology and run experiments once on the given training set and test set for each domain 
　in addition  ten real-world domains are used  on which m-of-n like concepts are expected to be found  spackman  1  they are five medical domains  cleveland heart disease  hepatitis  liver disorders  pima indians diabetes  and wisconsin breast cancer   one molecular biology domain  promoters   three linguistics domains  nettalk phoneme  nettalk stress   and nettalk letter    and one game domain  tic-tac-toe  all are from the uci repository of machine learning databases  murph  and aha  1  table 1 gives a brief summary of the domains  including the data set size  the number of binary  b   nominal  n   continuous-valued  c   total  t  attributes  and the number of classes 
　phoneme and stress are two basic subproblems of the nettalk domain letter is a combination of them their tasks are mapping a letter in an english word into a phoneme  a stress  and a phone me-stress pair respectively following the method used in  diettench et al  1  we generate data sets by using a window of length 1  but use the 1 most common english words 
　on each real-world domain  a 1-fold cross-validation  bruman et al  1  is conducted in all the experiments reported here    and xofn are run with their default option settings  and are run on the same partition for all the domains no parametertuning is done here pruned trees are used for all the four algorithms 
1 	experimental results 

the monks domains 
table 1 summarizes the accuracv  acc  and complexity 
 com  of and 
xofn on three monks problems 	the results of aq1dci and  are from  thrun et al 1  the 
table shows that only xofn solves all three problems with correct representations as we expected  xofn finds a perfect representation of the target concept on monks1 
　to explore how noise and irrelevant attributes affect the perlormance of learning algorithms on the monks1 domain  we give our results of c1  ci id1-of-1  and xofn  and the results of aq1-dci  aq1-hci  and 
aq1-mci from  bloedorn et al  1  in table 1 only 
xofn learns the correct concept because of noise  the learned concept is not the perfect representation instead  xofn finds two new attributes and 
how-
ever  it is still the most concise representation among those learned by these algorithms except for c1 and 
zheng 


table 1 on the noisy and irrelevant monks1 domain 
ci which return a tree having only one leaf this illustrates that xofn can  to some extent  tolerate irrelevant attributes under conditions of noise  but this matter remains to be explored further 
the real-world domains 
to demonstrate that the xofn algorithm can work well on real-world domains as well as artificial domains  tables 1 and 1 give the performance of xofn on ten real-world domains it uses only c1  ci  and id1-of-1 as references  since very few directly comparable results of other constructive induction algorithms are available from publications each value given is the average of a ten-fold cross-validation it is worth mentioning that lfc  ragavan and rendell  1  achieves higher prediction accuracy  1%  than xofn on pima indians diabetes domain  see section 1  
　to compare accuracies of xofn  id1-of-1  ci  and c1  a two-tailed pairwise t-test is used in table 1  boldface indicates that ci  id1-of-1  or xofn is better than c1 with the significance level of above 1%  while 
* and # denote that xofn is better than id1-of-1 and ci respectively at the 1% significance level 
　on seven out of ten domains  xofn achieves a significant improvement on prediction accuracj over c1 on the other three domains  the differences are not significant the reason why xofn does not work well on these domains might be that there are no x-of-n representations in them  or there are some but xofn cannot find them due to the simple search strategy of the current implementation table 1 also shows that id1-of-1 is significantly better than c1 on four domains  and there is no significant difference on the other six domains ci is significantly better than c1 on six domains  significantly worse on the nettalk letter  domain  and there is no significant difference on the other three domains compared to id1-of-1  xofn is significantly better on four domains and there is no significant difference on the other six domains compared to ci  xofn is significantly better on six domains the differences on the 
1 	learning 

other domains are not significant h on seven out of ten domains  xofn achieves the highest accuracy among the four algorithms as far as the complexity is concerned  on most domains  xofn can learn a more concise tree than c1  especially on those domains where xofn achieves a higher accuracy compared to ed1-of-1  xofn learns smaller trees on seven out of ten domains 
1 	discussion 
it has been found that xofn works quite well on a set of artificial and real-world domains we expect that xofn can be applied to domains containing m-of-n concepts  such as biomedical domains  spademan  1   linguistic domains  and domains containing parity concepts found for example in digital logic circuit design to apply xofn to domains containing complex dnf concepts with long terms  some mechanisms are necessary to overcome the  fragmentation  problem one approach is using subsetting of c1  quuilan  1  two other possible solutions are using subranging and building decision graphs  oliver et ol  1  instead of decision trees subranging is similar to subsetting but also considers the order of the values of x-of-n representations subrangmg reduces the number of outcomes of an x-of-n test by combining some adjacent values of the x-of-n representation which are identical m terms of splitting data sets 
　at the moment  xofn uses the cut points found by c1 to discretize primitive continuous-valued attributes 
   1  on the diabetes domain  the significance level of the difference between ci and xofn is less than 1%  although the significance level of the difference between ci and c1 is above 1% and the average accuracy of c1 is higher than that of xofn the reason is that the accuracy of xofn is higher than that of ci in some folds 

other discretization methods that can be used are multiinterval discretization methods  catlett  1  fayyad and irani  1   supervised/unsupervised methods  van de merckt  1   and an entropy method  ragavan and rendell  1  the current xofn discretizes continuousvalued attributes statically in the sense that discretization occurs before new attribute construction an alternative is dynamic discretization  l e doing discretization while constructing new attributes this method might be able to create good discretizations but with an increased computational complexity 
　up to now  we have concentrated only on treating x-of-n representations as nominal attributes because their values are ordered  however  they can also be treated as continuous-valued attributes since the standard decision tree based learning algorithms transform continuous-valued attributes into binar  tests the continuous-valued x-of-n should work well when the target concept requires x-of n representations with only one rut pomt  m-of-n concepts  however  on domains requiring x-of-n representations with more than one cut point  the contmuous-valued x-ofn has weaker expressive power than the nominal x-of-n for details on this issue please see  zheng  1  
1 	related work 
the closest related work is id1-of-1  murphy and pazzani  1  it constructs nev  binary  attributes m the form of m-of-n representations  while xofn constructs xof-n representations when building a decision tree both id1-of 1 and xofn construct one new attribute for each decision node using the local training set instead of building trees  mon  ting  1  creates m-of-n rules 
the production rule learning algorithms induce 
 michalski  1     dloedorn et al  1  use the counting operator1 #vareq x  to construct new attributes that count the number of attributes in an instance winch take the value x for primitive boolean attributes  a boolean counting operator takes a vector of n boolean attributes  n   1  and counts the number of true values for an instance like x-of-ns  new attributes constructed by these two operators have ordered discrete values however  when used to generate production rules  they are treated more like continuous-valued attributes than nominal attributes 1 the boolean counting attribute is a special case of the #vareq x  attribute  while the #vareq x  attribute is a special case of the xof-n representation 
　most hypothesis-dnven constructive mduction algonthms such as fringe  pagallo  1   citre  matheus and rendell  1   ci  zheng  1   and aq1-hci  wnek and michalski  1  construct and select a set of new attributes based on the entire training set this strategy has a shortcoming new attributes that have high values of the evaluation function for the entire training set might have lower values than other unselected new attributes for a training subset after a 

of a decision tree or a rule set has been created heus and rendell  1  to overcome this  xofn constructs one new attribute using the local training set for each decision node therefore  the new attribute constructed by xofn at each decision node is the best one in xofn's search space in terms of the evaluation function another difference between xofn and these algorithms is that the latter interleave the theory learning phase and the process of building new attributes  and generate new attributes by analyzing the theory learned previously  while xofn has only one iteration and constructs new attributes by analyzing data 
　like id1-of-1 and xofn  lfc  ragavan and rendell  1  is also a data-driven constructive induction algorithm that builds multivariate tree1  but it uses negation and conjunction as constructive operators lfc creates one conjunction for each decision node by using a directed lookahead search it achieved quite high prediction accuracy on a couple of real-world domains such as puna indians diabetes  but the problem is that it has a sensitive parameter  lookahead depth  which needs to be set when applied to a domain another multivariate tree learning algorithm is lmdt  brodlev and utgoff  1  that generates a linear machine at each decision node when building a tree 
1 	conclusions and future work 
selective induction algorithms budd their theories by selecting primitive attributes and are thus limited for hard tasks whose primitive attributes are not sufficient for  or drrectly relevant to  representing the theories to overcome this problem  constructive induction algorithms use various methods to generate powerful new attributes however  most algorithms construct only binary attributes  while domain experts use binary  nominal  and continuous-valued attributes to describe their tasks from this point  we first proposed a nch knowledge representation means  namely x-of-n representations  and then gave a new constructive induction algorithm xofn it generates x-of-n representations as new nominal attributes for decision trees using a data-driven constructive strategy 
　the experiments ulustrate the learning power of the xofn algorithm on some artificial and real-world domains in terms of both higher prediction accuracy and lower theory complexity in addition  it has been shown that xofn can achieve higher accuracy on some domains than other contructive induction algorithms that construct  as new attributes  conjunctions or m-of-n representations 
　however  the current xofn is a preliminary implementation a lot of research is worth doing and some is in progress examples are exploring more appropriate evaluation functions for comparing and selecting x-of-ns  other search methods  other discretization methods for continuous-valued attributes  and implementing subranging and decision graph algorithms in addition  examining xofn's ability to tolerate missing values  noise  and irrelevant attributes should be interesting 
zheng 

acknowledgments 
this research was partially supported by an arc grant  to j r. quinlan  and by a research agreement with digital equipment corporation the author is supported by emss scholarship the author appreciates the advice and suggestions j r quinlan has given many thanks to j r quinlan for providing c1  p m murphy for supplying the code of id1-of-1  also to k m ting  a varsek  n indurkhya  p brazdil  p langley  m cameron-jones  and three anonymous reviewers for comments that improve the ideas and earlier drafts finall   p m murphj and d aha are gratefully acknowledged for creating and managing the uci repository of ml databases 
