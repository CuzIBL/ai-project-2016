 
dynamic bayesian networks  dbns  offer an elegant way to integrate various aspects of language in one model. many existing algorithms developed for learning and inference in dbns are applicable to probabilistic language modeling. to demonstrate the potential of dbns for natural language processing  we employ a dbn in an information extraction task. we show how to assemble wealth of emerging linguistic instruments for shallow parsing  syntactic and semantic tagging  morphological decomposition  named entity recognition etc. in order to incrementally build a robust information extraction system. our method outperforms previously published results on an established benchmark domain. 
1 information extraction 
information extraction  ie  is the task of filling in template information from previously unseen text which belongs to a pre-defined domain. the resulting database is suited for formal queries and filtering. ie systems generally work by detecting patterns in the text that help identify significant information. researchers have shown  freitag and mccallum  1; ray and craven  1  that a probabilistic approach allows the construction of robust and well-performing systems. however  the existing probabilistic systems are generally based on hidden markov models  hmms . due to this relatively impoverished representation  they are unable to take advantage of the wide array of linguistic information used by many non-probabilistic ie systems. in addition  existing hmm-based systems model each target category separately  failing to capture relational information  such as typical target order  or the fact that each element only belongs to a single category. this paper shows how to incorporate a wide array of knowledge into a probabilistic ie system  based on dynamic bayesian networks  dbn -a rich probabilistic representation that generalizes hmms. 
　let us illustrate ie by describing seminar announcements which got established as one of the most popular benchmark domains in the field  califf and mooney  1; freitag and mccallum  1; soderland  1; roth and yih  1; ciravegna  1 . people receive dozens of seminar announcements weekly and need to manually extract information and paste it into personal organizers. the goal of an ie system is to automatically identify target fields such as location and topic of a seminar  date and starting time  ending time and speaker. announcements come in many formats  but usually follow some pattern. we often find a header with a gist in the form  postedby: john host .domain; who: dr. s t e a l s ; when: 1 am;  and so forth. 
also in the body of the message  the speaker usually precedes both location and starting time  which in turn precedes ending time as in: m d r . steals presents in dean h a l l at one a m . ' ' the task is complicated since some fields may be missing or may contain multiple values. 
　this kind of data falls into the so-called semi-structured text category. instances obey certain structure and usually contain information for most of the expected fields in some order. there are two other categories: free text and structured text. in structured text  the positions of the information fields are fixed and values are limited to pre-defined set. consequently  the ie systems focus on specifying the delimiters and order associated with each field. at the opposite end lies the task of extracting information from free text which  although unstructured  is assumed to be grammatical. here ie systems rely more on syntactic  semantic and discourse knowledge in order to assemble relevant information potentially scattered all over a large document. 
information extraction 	1 　ie algorithms face different challenges depending on the extraction targets and the kind of the text they are embedded in. in some cases  the target is uniquely identifiable  singleslot   while in others  the targets are linked together in multislot association frames. for example  a conference schedule has several slots for related speaker  topic and time of the presentation  while a seminar announcement usually refers to a unique event. sometimes it is necessary to identify each word in a target slot  while some benefit may be reaped from partial identification of the target  such as labeling the beginning or end of the slot separately. many applications involve processing of domain-specific jargon like lnternetese-a style of writing prevalent in news groups  e-mail messages  bulletin boards and online chat rooms. such documents do not follow a good grammar  spelling or literary style. often these are more like a stream-of-consciousness ranting in which asciiart and pseudo-graphic sketches are used and emphasis is provided by all-capitals  or using multiple exclamation signs. as we exemplify below  syntactic analysers easily fail on such corpora. 
　other examples of hi application domains include job advertisements  califf and mooney  1   rapier   executive succession  soderland  1   whisk   restaurant guides  muslea et al .  1   stalker   biological publications  ray and craven  1  etc. initial interest in the subject was stimulated by arpa s message understanding conferences  muc  which put forth challenges e.g. parsing newswire articles related to terrorism  sec e.g. mikheev  . below we briefly review various ie systems and approaches which mostly originated from muc competitions. 
　successful ie involves identifying abstract patterns in the way information is presented in text. consequently  all previous work necessarily relies on some set of textual features. the overwhelming majority of existing algorithms operate by building and pruning sets of induction rules defined on these features  srv  rapier  whisk  lp 1  . there are many features that are potentially helpful for extracting specific fields  e.g. there are tokens and delimiters that signal the beginning and end of particular types of information. consider an example in table 1 which shows how the phrase ''doctor steals presents in dean h a l l at one am.  is represented through feature values. for example  the lemma  am  designates the end of a time field  while the semantic feature  title  signals the speaker  and the syntactic category nnp  proper noun  often corresponds to speaker or location. since many researchers use the seminar announcements domain as a testbed  we have chosen this domain in order to have a good basis of comparison. 
　one of the systems we compare to  specifically designed for single-slot problems  is srv  freitag  1 . it is built on three classifiers of text fragments. the first classifier is a simple look-up table containing all correct slot-fillers encountered in the training set. the second one computes the estimated probability of finding the fragment tokens in a correct slot-filler. the last one uses constraints obtained by rule induction over predicates like token identity  word length and capitalization  and simple semantic features. 
　rapier  califf and mooney  1  is fully based on bottom-up rule induction on the target fragment and a few tokens from its neighborhood. the rules are templates specifying a list of surrounding items to be matched and potentially  a maximal number of tokens for each slot. rule generation begins with the most specific rules matching a slot. then rules for identical slots are generalized via pair-wise merging  until no improvement can be made. rules in rapier are formulated as lexical and semantic constraints and may include pos tags. whisk  soderland  1  uses constraints similar to rapier  but its rules are formulated as regular expressions with wild cards for intervening tokens. thus  whisk encodes a relative  rather than absolute position of tokens with respect to the target. this enables modeling long distance dependencies in the text. whisk performs well on both single-slot and multi-slot extraction tasks. 
　ciravcgna  presents yet another rule induction method  lp 1. he considers several candidate features such as lemma  lexical and semantic categories and capitalization to form a set of rules for inserting tags into text. unlike other approaches   lp 1 generates separate rules targeting the beginning and ending of each slot. this allows for more flexibility in subjecting partially correct extractions to several refinement stages  also relying on rule induction to introduce corrections. emphasizing the relational aspect of the domain  roth and yoh  developed a knowledge representation language that enables efficient feature generation. they used the features in a multi-class classifier snow-ie to obtain the desired set of tags. the resulting method  snow-ie  works in two stages: the first filters out the irrelevant parts of text  while the second identifies the relevant slots. 
　freitag & mccallum  use hidden markov models  hmm . a separate hmm is used for each target slot. no preprocessing or features is used except for the token identity. for each hidden state  there is a probability distribution over tokens encountered as slot-fillers in the training data. weakly analogous to templates  hidden state transitions encode regularities in the slot context. in particular prefix and suffix states are used in addition to target and background slots to capture words frequently found in the neighborhood of targets. ray&craven  make one step further by setting hmm hidden states in a product space of syntactic chunks and target tags to model the text structure. the success of the hmm-based approaches demonstrate the viability of probabilistic methods for this domain. however  they do not take advantage of the linguistic information used by the other approaches. furthermore  they are limited by using a separate hmm for each target slot  rather than extracting data in an integrated way. 
　the main contribution of this paper is in demonstrating how to integrate various aspects of language in a single probabilistic model  to incrementally build a robust information extraction system based on a bayesian network. this sys-

tern overcomes the following dilemma. it is tempting to use a lot of linguistic features in order to account for multiple aspects of text structure. however  deterministic rule induction approaches seem vulnerable to the performance of feature extractors in pre-processing steps. this presents a problem since syntactic instruments that have been trained on highlypolished grammatical corpora  are particularly unreliable on weakly grammatical semi-structured text. furthermore incorporating many features complicates the model which often has to be learned from sparse data  which harms performance of classifier-based systems. 
1 	features 
our approach is statistical  which generally speaking means that learning corresponds to inferring frequencies of events. the statistics we collect originates in various sources. some statistics reflect regularities of the language itself  while others correspond to the peculiarities of the domain. with this in mind we design features which reflect both aspects. there is no limitation on the possible set of features. local features like part-of-speech  number of characters in the token  capitalization and membership in syntactic phrase are quite customary in the in. in addition one could obtain such characteristics of the word as imagibility  frequency of use  familiarity  or even predicates on numerical values. since there is no need for features to be local  one might find useful including frequency of a word in the training corpus or number of occurrences in the document. notice that the same set of features would work for many domains. this includes semantic features along with orthographic and syntactic features. 
　before we move on to presenting our system for probabilistic reasoning  let us discuss in some detail notation and methods we used in preliminary data processing and feature extraction. to use the data efficiently  we need to factor the text into  orthogonal  features. rather than working with thousands of listems  generic words1  in the vocabulary  and combining their features  we compress the vocabulary by an order of magnitude by lemmatisation or stemming. orthographic and syntactic information is kept in feature variables with just a few values each. 
tokenization 
tokenization is the first step of textual data processing. a token is a minimal part of text which is treated as a unit in subsequent steps. in our case tokenization mostly involves separating punctuation characters from words. this is particularly non-trivial for separating a period  manning and schutze  1  since it requires identifying sentence boundaries. consider a sentence: speaker: dr. s t e a l s   chief exec. of r i c h . c o m   worth $1 m i l . 
lemmatisation 
we have developed a simple lemmatiser which combines outcome of some standard lematisers and stemmers into a lookup table. combined with lemmatisation is a step of spell 
   'a word is a sequence of alphabetical characters  which has some meaning assigned to it. this would cover words found in general and special vocabulary as well abbreviations  proper names and such. checking to catch misspelled words. this is done by interfacing with the unix ispell utility. 
gazetteer 
our original corpus contains about 1 different listems. this does not take into account tokens consisting of punctuation characters  numbers and such. about 1% are proper nouns. the question of building a vocabulary automatically was previously addressed in ie literature see e.g. riloff  . we use the intersection of two sets. the first set consists of words encountered as part of target fields and in their neighborhood. the second set consists of words frequently seen in the corpus. aside from vocabulary there are two reserved values for out-of-vocabulary  oov  words and not-a-word  naw . for example see blank slots in the lemma row of table 1. the first category encodes rare and unfamiliar words  which are still identified as words according to their part of speech. the second category is for mixed alphanumerical tokens  punctuation and symbolic tokens. 
syntactic categories 
we used ltchunk software from u.of edinburgh nlp group  mikheev et al  1 . it produces 1 pos tags from upenn treebank set  marcus et ai  1 . we have clustered these into 1 categories: cardinal numbers  cd   nouns  nn   proper nouns  nnp   verbs  vb   punctuation  .   preposition/conjunction  in  and other  sym . the choice of clusters seriously influences the performance  while keeping all 1 tags will lead to large cpts and sparse data. 
syntactic chunking 
following ray&craven   we obtain syntactic segments  aka syntactic chunks  by running the sundance system  riloff  1  and flattening the output into four categories corresponding to noun phrase  np   verb phrase  vp   prepositional phrase  pp  and other  n/a . table 1 shows a sample outcome. note that both the part-of-speech tagger and the syntactic chunker easily get confused by non-standard capitalization of a word  presents  as shown by incorrect labels in parenthesis.  steals  is incorrectly identified as a verb  whose subject is  doctor  and object is  presents . remarkably  other state-of-the-art syntactic analysis tools  charniak  1; ratnaparkhi  1  also failed on this problem. 
capitalization and length 
simple features like capitalization and length of word are used by many researchers  e.g. srv  freitag and mccallum  1   case representation process is straightforward except for the choice of number of categories. we found useful introducing an extra category for words which contain both lower and upper case letters  not counting the initial capital letter  which tend to be abbreviations. 
semantic features 
information extraction 	1 there are several semantic features which play important role in a variety of application domains. in particular  it is useful to be able to recognize what could be a person's name  geographic location  various parts of address  etc. for example  we are using a list of secondary location identifiers provided by us postal service  which identifies as such words like hall  wing  floor and auditorium. we also use a list of 1 most popular names from us census bureau; the list is augmented by rank which helps to decide in favor of first or last name for cases like  alexander . in general this task could be helped by using a hypernym feature of wordnet project  fellbaum  1 . the next section presents probabilistic model which makes use of the aforementioned feature variables. 
1 	bien 
we convert the ih problem into a classification problem by assuming that each token in the document belongs to a target class corresponding to ether one of the target tags or the background  compare to freitag  . furthermore  it seems important not to ignore the information about interdependencies of target fields and document segments. to combine advantages of stochastic models with feature-based reasoning  we use a bayesian network. 
　a dynamic bayesian network  dbn  is ideal for representing probabilistic information about these features. just like a bayesian network  it encodes interdependence among various features. in addition  it incorporates the clement of time  like an hmm  so that time-dependent patterns ; uch as common orders of fields can be represented. all this is done in a compact representation that can be learned from data. we refer to a recent dissertation  murphy  1  for a good overview of all aspects of dynamic bayesian networks. 
　each document is considered to be a single stream of tokens. in our dbn  called the bayesian information extraction network  bien   the same structure is repeated for every index. figure 1 presents the structure of bien. this structure contains state variables and feature variables. the most important state variable  for our purposes  is  tag  which corresponds to information we are trying to extract. this variable classifies each token according to its target information field  or has the value  background  if the token does not belong to any field.  last target  is another hidden variable which reflects the order in which target information is found in the document. this variable is our way of implementing a memory in a  memory-less  markov model. its value is deterministically defined by the last non-background value of  tag  variable. another hidden variable   document segment   is introduced to account for differences in patterns between the header and the main body of the document. the former is close to the structured text format  while the latter to the free text.  document segment  influences  tag  and together these two influence the set of observable variables which represent features of the text discussed in section 1. standard inference algorithms for dbns are similar to those for hmms. in a dbn  some of the variables will typically be observed  while others will be hidden. the typical inference task is to determine the probability distribution over the states of a hidden variable over time  given time series data of the observed variables. this is usually accomplished using the forwardbackward algorithm. alternatively  we might want to know the most likely sequence of hidden variables. this is accomplished using the viterbi algorithm. learning the parameters of a dbn from data is accomplished using the em algorithm  see e.g. murphy  . note that in principle  parts of the system could be trained separately on independent corpus 
last target 

figure 1: a schematic representation of bien. 
to improve performance. for example  one could learn independently the conditional vocabulary of email/newsgroup headers  or learn a probability of part-of-speech conditioned on a word  to avoid dependence on external pos taggers. also prior knowledge about the domain and the language could be set in the system this way. the fact that etime almost never precedes stime as well as the fact that speaker is never a verb could be encoded in a conditional probability table  cpt . in large dbns  exact inference algorithms are intractable  and so a variety of approximate methods have been developed. however  the number of hidden state variables in our model is small enough to allow exact algorithms to work. indeed  all hidden nodes in our model are discrete variables which assume just a few values.  documentsegment  is binary in 
{header  body  range;  lasttargct  has as many values as  tag -four per number of target fields plus one for the background. 
1 	results 
several researchers have reported results on the cmu seminar announcements corpus  which we have chosen in order to have a good basis of comparison. the cmu seminar announcements corpus consists of 1 documents. each announcement contains some tags for target slots. on average starting time appears twice per document  while location and speaker 1 times  with up to 1 speaker slots and 1 location slots per document. sometimes multiple instances of the same slot differ  e.g. speaker dr. steals also appears as joe steals 1 . ending time  speaker and location are missing from 1%  1% and 1% of documents correspondingly. in order to demonstrate our method  we have developed a web site which works with arbitrary seminar announcement and reveals some semantic tagging. we also make available a list of errors in the original corpus  along with our new derivative seminar announcement corpus1. 
　　1 obtaining 1% performance on the original corpus is impossible since some tags are misplaced and in general the corpus is not marked uniformly-sometimes secondary occurrences are ignored. 
1
   the corpus and demo for this paper are available from http://www.eecs.harvard.edu/~pesha/papers.html 

table 1: fl performance measure for various ie systems. 
　the performance is calculated in the usual way  by precision 	and recall 
                       combined into f measure geometrical average we report results using 
the same ten-fold cross validation test as other publications concerning this data set  roth and yih  1; ciravegna  1 . the data is split randomly into training and testing set. the reported results are averaged over five runs. table 1 presents a comparison with numerous previous attempts at the cmu seminar corpus. the figures are taken from roth and yih  1 j. bien performs comparably to the best system in each category  while notably outperforming other systems in finding location. this is partly due to the  lasttarget  variable.  lasttargct  variable turns out to be generally useful. here is the learned conditional probability table  cpt  for p targct lasttaryet 1 where the element  /  j  corresponds to the probability to get target tag .1 after target tag / was seen. we learn that initial tag is stime or speaker with 1 likelihood ratio; etime is naturally the most likely follower to stime and in turn forecasts location. 

　other variables turn out to be useless  e.g. the number of characters does not add anything to the performance  and neither does the initially introduced  seentag  variable which kept track of all tags seen up to the current position. table 1 presents performance of bien with various individual features turned off. note that figures for complete bien are 

table 1: fl performance comparison across implementations of bien with disabled features. 
information extraction 

figure 1: a learning curve for precision and recall with growing training sample size. 
slightly better than in table 1 since we pushed the fraction of the training data to the maximum. capitalization helps identify location and speaker  while losing it does not damage performance drastically. although information is reflected in syntactic and semantic features  most names in documents do not identify a speaker. one would hope to capture all relevant information by syntactic and semantic categories  however bien does not fare well without observing  lemma . losing the semantic feature seriously undermines performance in location and speaker categories- - ability to recognize names is rather valuable for many domains. 
　reported figures are based on 1%-1% split of the corpus. increasing the size of training corpus did not dramatically improve the performance in terms of f measure  as further illustrated by figure 1  which presents a learning curve- precision and recall averaged over all fields  as a function of training data fraction. trained on a small sample  bien acts very conservatively rarely picking fields  therefore scoring high precision and poor recall. having seen hundreds of target field instances and tens of thousands of negative samples  bien learns to generalize  which leads to generous tagging i.e. lower precision and higher recall. 
　so far we provide results obtained on the original cmu seminar announcements data  which is not very challenging. most documents contain the header section with all the target fields easily identifiable right after the corresponding key word. we have created a derivative dataset in which documents are stripped of headers and two extra fields are sought: date and topic. indeed this corpus turned out to be more difficult  with our current set of features we obtain only 1% performance on speaker and 1% performance on topic. date does not present a challenge except for cases of regular weekly events or relative dates like  tomorrow . admittedly  the bootstrapping test performance is not a guarantee of systems performance on novel data since preliminary processing  i.e. tokenization and gazetteering  as well as choice for pos tag set  lead to a strong bias towards the training corpus. 
1 

1 	discussion 
we have described how to integrate various aspects of language into a single probabilistic model  and to incrementally build a robust ie system based on a bayesian network. currently  we are working on learning the structure of bien automatically. it seems to subject itself nicely to structural km  friedman  1; murphy  1 . the first step is automatic selection of relevant features. another direction of current work is using approximate inference. we have tried lbp  loopy-belief propagation   murphy et al  1; murphy  1   but for the current structure of bien it seems to give no gain. more challenging applications which require larger  stronger connected networks  will benefit from approximate inference algorithms. it will enable quick on-line inference on the network learned off-line with exact methods  as well as learning for cases where exact inference is infeasible. one such network will result from integrating a pos tagger and other feature extractors into bien. this is a natural extension of bien since various text processing routines are mutually dependent. consider for example pos tagging  sentence boundary detection and named entities recognition. another complex bien structure will result if we try to better reflect complex relational information  califif and mooney  1; roth and yih  1; 1  e.g. to process cases like seminar cancellations and rescheduling; and handle multi-slot extraction  e.g. multiple seminar announcements and conference schedules. 
acknowledgments 
kevin murphy provided bnt  kobi gal helped to handle the corpus  anonymous reviewers gave helpful feedback. 
