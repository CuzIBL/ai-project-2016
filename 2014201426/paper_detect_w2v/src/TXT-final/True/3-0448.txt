
an application of relational instance-based learning to the complex task of expressive music performance is presented. we investigate to what extent a machine can automatically build 'expressive profiles' of famous pianists using only minimal performance information extracted from audio cd recordings by pianists and the printed score of the played music. it turns out that the machine-generated expressive performances on unseen pieces are substantially closer to the real performances of the 'trainer' pianist than those of all others. two other interesting applications of the work are discussed: recognizing pianists from their style of playing  and automatic style replication.
1	introduction
relational instance-based learning is a machine learning paradigm that tries to transfer the successful nearest-neighbor or instance-based learning  ibl  framework to richer firstorder logic  fol  representations  emde and wettschereck  1; tobudic and widmer  1 . as such it is a part of inductive logic programming  ilp   the field of research which - after the euphoria in the nineties - suffered a certain flattening of interest in recent years  the main reason being the difficulties of effectively constraining the extremely large hypothesis spaces. nevertheless  some ilp systems have recently been shown to achieve performance levels competitive to those of human experts in very complex domains  e.g.  king et al.  1  .
　a successful application of relational ibl to a real-world task from classical music has been presented in previous work   tobudic and widmer  1  . a system that predicts expressive timing and dynamics patterns for new pieces by analogy to similar musical passages in a training set has been shown to learn to play piano music 'expressively' with substantial musical quality.
　here we investigate an even more interesting question: can a relational learner learn models that are 'personal' enough to capture some aspects of the playing styles of truly great pianists  a system is presented that builds performance models of six famous pianists using only crude information related to expressive timing and dynamics obtained from the pianist's
gerhard.widmer jku.at
cd recordings and the printed score of the music. experiments show that the system indeed captures some aspect of the pianists' playing style: the machine's performances of unseen pieces are substantially closer to the real performances of the 'training' pianist than those of all other pianists in our data set. an interesting by-product of the pianists' 'expressive models' is demonstrated: the automatic identification of pianists based on their style of playing. and finally  the question of automatic style replication is briefly discussed.
　the rest of the paper is laid out as follows. after a short introduction to the notion of expressive music performance  section 1   section 1 describes the data and its representation in fol. we also discuss how the complex task of learning expressive curves can be decomposed into a well-defined instance-based learning task and shortly recapitulate the details of the relational instance-based learner distall. experimental results are presented in section 1.
1	expressive music performance
expressive music performance is the art of shaping a musical piece by continuously varying important parameters like tempo  loudness  etc. while playing  widmer et al.  1 . human musicians do not play a piece of music mechanically  with constant tempo or loudness  exactly as written in the printed music score. rather  skilled performers speed up at some places  slow down at others  stress certain notes or passages etc. the most important parameter dimensions available to a performer are timing  tempo variations  and dynamics  loudness variations . the way these parameters 'should be' varied is not specified precisely in the printed score; at the same time it is what makes a piece of music come alive  and what distinguishes great artists from each other.
　tempo and loudness variations can be represented as curves that quantify these parameters throughout a piece relative to some reference value  e.g. the average loudness or tempo of the same piece . figure 1 shows a dynamics curve of a small part of mozart's piano sonata k.1  1st movement  as played by five famous pianists. each point gives the relative loudness  relative to the average loudness of the piece by the same performer  at a given point in the piece. a purely mechanical  unexpressive  rendition of the piece would correspond to a flat horizontal line at y = 1. tempo variations can be represented in an analogous way. in the next section we discuss how predictive models of such curves can be auto-

figure 1: dynamics curves of performances of five famous pianists for mozart sonata k.1  1st mvt.  mm. 1. each point represents the relative loudness at the beat level  relative to the average loudness of the piece by the same performer .
matically learned from information extracted from audio cd recordings and the printed score of the music.
1	data and methodology
the data used in this work was obtained from commercial recordings of famous concert pianists. we analyzed the performances of 1 pianists across 1 different sections of piano sonatas by w.a.mozart. the pieces selected for analysis are complex  different in character  and represent different tempi and time signatures. tables 1 and 1 summarize the pieces  pianists and recordings selected for analysis.
　for learning tempo and dynamics 'profiles' of the pianists in our data set we extract time points from the audio recordings that correspond to beat 1 locations. from the  varying  time intervals between these points  the beat-level tempo and its changes can be computed. beat-level dynamics is computed from the audio signal as the overall loudness of the signal at the beat times as a very crude representation of the dynamics applied by the pianists. extracting such information from the cd recordings was an extremely laborious task  even with the help of an intelligent interactive beat tracking system  dixon  1 . from these measurements  computing pianists' dynamics and tempo performance curves as shown in figure 1 - which are the raw material for our experiments - is rather straightforward.
　an examination of the dynamics curves in figure 1 reveals certain trends common for all pianists  e.g. up-down  crescendo-decrescendo tendencies . these trends reflect certain aspects of the underlying structure of the piece: a piece of music commonly consists of phrases - segments that are heard and interpreted as coherent units. phrases are organized hierarchically: smaller phrases are grouped into higher-level phrases  which are in turn grouped together  constituting a musical context at a higher level of abstraction etc. in figure 1  the hierarchical  three-level phrase structure of this passage is indicated by three levels of brackets at the bottom. in this work we aim at learning expressive patterns at different levels of such a phrase structure  which roughly corresponds to various levels of musical abstraction.
table 1: mozart sonata sections selected for analysis. section id should be read as  sonataname  :  movement  :  section . the total numbers of phrases are also shown.
section idtempo descr.#phraseskv1:1fast 11kv1:1fast 11kv1:1fast 11kv1:1fast 11kv1:1slow 11kv1:1slow 11kv1:1fast 11kv1:1fast 11kv1:1slow 11kv1:1slow 11kv1:1fast 11kv1:1fast 11kv1:1fast 11kv1:1fast 11kv1slow 11table 1: pianists and recordings.
idpianist namerecordingdbdaniel barenboimemi classics cdz 1 1  1rbroland batikgramola 1  1ggglenn gouldsony classical sm1k 1  1mpmaria joao 	piresdgg 1-1  1asandras＞ schiffadd  decca  1-1  1mumitsuko uchidaphilips classics 1-1  1.1	phrase representation in fol
phrases and relations between them can be naturally represented in first-order logic. in our collection of pieces  phrases are organized at three hierarchical levels  based on a manual phrase structure analysis. the musical content of each phrase is encoded in the predicate phrcont/1. it has the form phrcont id a1 a1 ...   where id is the phrase identifier and a1 a1 ... are attributes that describe very basic phrase properties. the first seven of these are numeric: the length of a phrase  the relative position of the highest melodic point  the 'apex'   the melodic intervals between starting note and apex  and between apex and ending note  metrical strengths of starting note  apex  and ending note. the next three attributes are discrete: the harmonic progression between start  apex  and end  and two boolean attributes that state whether the phrase ends with a 'cumulative rhythm'  and whether it ends with a cadential chord sequence. the remaining attributes describe - in addition to some simple information about global tempo and the presence of trills - global characteristics of the phrases in statistical terms: mean and variance of the durations of the melody notes within the phrase  as rough indicators of the general 'speed' of events and of durational variability   and mean and variance of the sizes of the melodic intervals between the melody notes  as measures of the 'jumpiness' of the melodic line .
　this propositional representation ignores an essential aspect of the music: its temporal nature. the temporal re-

figure 1: multilevel decomposition of dynamics curve of performance of mozart sonata k.1:1  mm.1: original dynamics curve plus the second-order polynomial shapes giving the best fit at four levels of phrase structure.
lationships between successive phrases can be naturally expressed in fol  as a relational predicate succeeds id1 id1   which simply states that the phrase id1 succeeds the samelevel phrase id1. supplying the same information in a propositional representation would be very difficult.
　what is still needed in order to learn are the training examples  i.e. for each phrase in the training set  we need to know how it was played by a musician. this information is given in the predicate phrshape id coeffs   where coeffs encode information about the way the phrase was played by a pianist. this is computed from the tempo and dynamics curves  as described in the following section.
1	deriving the training instances: multilevel decomposition of performance curves
given a complex tempo or dynamics curve and the underlying phrase structure  see figure 1   we need to calculate the most likely contribution of each phrase to the overall observed expression curve  i.e.  we need to decompose the complex curve into basic expressive phrase 'shapes'. as approximation functions to represent these shapes we decided to use the class of second-degree polynomials  i.e.  functions of the form y = ax1 + bx + c   because there is ample evidence from research in musicology that high-level tempo and dynamics are well characterized by quadratic or parabolic functions  todd  1 . decomposing a given expression curve is an iterative process  where each step deals with a specific level of the phrase structure: for each phrase at a given level  we compute the polynomial that best fits the part of the curve that corresponds to this phrase  and 'subtract' the tempo or dynamics deviations 'explained' by the approximation. the curve that remains after this subtraction is then used in the next level of the process. we start with the highest given level of phrasing and move to the lowest. as tempo and dynamics curves are lists of multiplicative factors  relative to a default tempo   'subtracting' the effects predicted by a fitted curve from an existing curve simply means dividing the y values on the curve by the respective values of the approximation curve.
　figure 1 illustrates the result of the decomposition process on the last part  mm.1  of the mozart sonata k.1  1st movement  1st section. the four-level phrase structure our music analyst assigned to the piece is indicated by the four levels of brackets at the bottom of the plot. the elementary phrase shapes  at four levels of hierarchy  obtained after decomposition are plotted in gray. we end up with a training example for each phrase in the training set - a predicate phrshape id coeff   where coeff = {a b c} are the coefficients of the polynomial fitted to the part of the performance curve associated with the phrase.
　input to the learning algorithm are the  relational  representation of the musical scores plus the training examples  i.e. timing and dynamics polynomials   for each phrase in the training set. given a test piece the learner assigns the shape of the most similar phrase from the training set to each phrase in the test piece. in order to produce final tempo and dynamics curves  the shapes predicted for phrases at different levels must be combined. this is simply the inverse of the curve decomposition problem: given a new piece  the system starts with an initial 'flat' expression curve  i.e.  a list of 1 values  and then successively multiplies the current values by the multi-level phrase predictions.
1	distall  a relational instance-based learner
we approach phrase-shape prediction with a straightforward nearest-neighbour  nn  ibl  method. standard propositional k-nn is not applicable to our data representation discussed in section 1. instead  we use distall  an algorithm that generalizes propositional k-nn to examples described in firstorder logic  tobudic and widmer  1 .
　distall is a representative of the line of research first initiated in  bisson  1   where a clustering algorithm together with its similarity measure was presented. this work was later improved in  emde and wettschereck  1   in the context of the relational instance-based learning algorithm ribl  which in turn can be regarded as distall's predecessor. we skip technical details here  but the main idea behind distall's similarity measure is that the similarity between two objects depends not only on the similarities of their attributes  but also on the similarities of the objects related to them. the similarities of the related objects depend in turn on their attributes and related objects. for our music learning task it means that the 'shaping' of the current  test  phrase depends not only on its attributes  but also on the preceding and succeeding music  through the relation succeeds id1 id1   see section 1   which is - from a musical point of view - a rather intuitive idea. for a more detailed description of distall see  tobudic and widmer  1 .
　experimental results with distall on midi-like  i.e.  very detailed  performance data produced by a local pianist are reported in  tobudic and widmer  1 . the new contribution of the current paper is that we have laboriously measured audio recordings by truly famous artists and can show - for the first time - that distall actually succeeds in capturing something of personal artistic performance style.
1	experiments
1	learning predictive performance models
for each pianist  we conducted a systematic leave-one-pieceout cross-validation experiment: each of 1 pieces was once table 1: results of piecewise cross-validation experiment. the table cells list correlations between learned and real curves  where rows indicate the 'training pianist'  and columns the pianist whose real performance curves are used for comparison. the correlations are averaged over all pieces  weighted by the relative length of the piece. each cell is further divided into two rows corresponding to dynamics and tempo correlations  respectively. the highest correlations in each row are printed in bold.
compared withlearned fromdbrbggmpasmudb.1.1.1.1.1.1.1.1.1.1.1.1rb.1.1.1.1.1.1.1.1.1.1.1.1gg.1.1.1.1.1.1.1.1.1.1.1.1mp.1.1.1.1.1.1.1.1.1.1.1.1as.1.1.1.1.1.1.1.1.1.1.1.1mu.1.1.1.1.1.1.1.1.1.1.1.1left aside as a test piece while the remaining 1 performances  by the same pianist  were used for learning. distall's parameter for the number of nearest neighbors was set to 1  and the parameter for the depth of starting clauses  see  tobudic and widmer  1   to 1  meaning that the distance between two phrases can be influenced by at most 1 preceding and 1 succeeding phrases .
　the expressive shapes for each phrase in a test piece are predicted by distall and then combined into a final tempo and dynamics curve  as described in section 1. the resulting curves are then compared to the real performance curves of all pianists  for the same test piece . if the curve learned from the performances of one pianist is more similar to the real performance curve of the 'teacher' pianist than to those of all other pianists  we could conclude that the learner succeeded in capturing something of the pianist's specific playing style. the described procedure is repeated for all pieces and all pianists in our data set.
　correlation is chosen as a measure of how well the predicted curve 'follows' the real one. the curves are first normalized so that their autocorrelations are identically 1  giving a correlation estimate between curves as a number in the range  -1 . the results of the cross-validation experiment averaged over all pieces  weighted by the relative length of the pieces  are given in table 1.
　interestingly  the system succeeded in learning curves that are substantially closer to the 'trainer' than all others  for all pianists. some of the pianists are better 'predictable' than others  e.g. daniel barenboim and mitsuko uchida  which might indicate that they play mozart in a more 'consistent' way. while at first sight the correlations may not seem impressive  one should keep in mind that artistic performance is far from predictable  and the numbers in table 1 are averages over all pieces  about half an hour of concert-level piano table 1: detailed results  tempo dimension  of the crossvalidation experiment with mitsuko uchida was the 'training' pianist. for each piece  the correlations between predicted curve and actual tempo curves from all pianists are given. the average over all pieces is given in the last row  reproduced from the last row of table 1 
piecedbrbggmpasmukv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1:1.1.1.1.1.1.1kv1.1.1.1.1.1.1total.1.1.1.1.1.1music per artist . moreover  the correlation estimates in table 1 are somewhat unfair  since we compare the performance curve produced by composing the polynomials predicted by the learner  with the curve corresponding to the pianists' actual performances. however  what distall learned from was not the actual performance curves  but an approximation curve which is implied by the three levels of quadratic functions that were used as training examples. correctly predicting these is the best the learner could hope to achieve.
　table 1 shows a more detailed picture of the crossvalidation experiment  where the training pianist was mitsuko uchida and the numbers refer to correlations in the tempo domain. in 1 out of 1 cases the learner produces tempo curves which are closer to uchida's playing than to any other pianist  with correlations of .1 and better  e.g.  for kv1:1 and kv1:1 . the results are even more interesting if we recall that the learner is given a very crude  beat-level representation of the tempo and dynamics applied by the pianist  without any details about e.g. individual voices or timing details below the level of beat. on the other hand  the piecewise results revealed that some of the pianists  e.g. gould or pires  seem to be less 'predictable' with our approach than uchida  not reported here for lack of space .
　figure 1 shows an example of successful performance style learning. we see a passage from a mozart piano sonata as 'played' by the computer after learning from recordings of other pieces by daniel barenboim  top  and mitsuko uchida  bottom   respectively. also shown are the performance curves corresponding to these two pianists' actual performances of the test piece. in this case it is quite clearly visible that the curves predicted by the computer on the test piece are much more similar to the curves by the respective 'teacher' than to those by the other pianist.

figure 1: dynamics and tempo curves produced by distall on test piece  sonata k.1  1rd mvt.  1nd section  mm.1- 1  after learning from daniel barenboim  top  and mitsuko uchida  bottom   compared to the artists' real curves as measured from the recordings.
　admittedly  this is a carefully selected example  one of the clearest cases of style replication we could find in our data. the purpose of this example is more to give an indication of the complexity of the curve prediction task and the difference between different artists' interpretations than to suggest that a machine will always be able to achieve this level of prediction performance.
1	identification of great pianists
the primary goal of our work is learning predictive models of pianists' expressive performances.1 but the models can also be used in a straightforward way for recognizing pianists. the problem of identifying famous pianists from information obtained from audio recordings of their playing has been addressed in the recent literature  saunders et al.  1; stamatatos and widmer  1; widmer and zanon  1 . in  widmer and zanon  1   a number of low-level scalar features related to expressive timing and dynamics are extracted from the audio cd recordings  and various machine learning table 1: confusion matrix of the pianist classification experiment. rows correspond to the test performances of each pianist  1 per row   columns to the classifications made by the system. the rightmost column gives the accuracy achieved for all performances of the respective. the baseline accuracy in this 1-class problem is 1%.
predictionpianistdbrbggmpasmuacc. % db1111rb1111gg1111mp1111as1111mu1111total------1algorithms are applied to these. in  saunders et al.  1   the sequential nature of music is addressed by representing performances as strings and using string kernels in conjunction with kernel partial least squares and support vector machines. the string kernel approach is shown to achieve better performance than the best results obtained in  widmer and zanon  1 . a clear result from both works is that identification of pianists from their recordings is an extremely difficult task.
　the pianists studied in the present paper are identical to those in  widmer and zanon  1  and  saunders et al.  1 ; unfortunately  the sets of recordings differ considerably  because manual phrase structure analyses  which are needed in our approach  were available only for certain pieces   so a direct comparison of the results is impossible. still  to illustrate what can be achieved with a relational representation and learning algorithm  we briefly describe a classification experiment with distall.
　each of the 1 pieces is set aside once. the 1 performances of that piece  one by each pianist  are used as test instances. a model of each pianist is built from his/her performances of the remaining 1 pieces. the result is two predicted curves per pianist for the test piece  for tempo and dynamics   which we call model curves. the final classification of a pianist  represented by his/her tempo and dynamics curves tt and td on the test piece  is then determined as

 1 
　where p is set of all pianists and mpt and mpd are the pianists' model tempo and dynamics curves. in other words  the performance is classified as belonging to the pianist whose model curves exhibit the highest correlation  averaged over tempo and dynamics  with the test curves. for each pianist  distall is tested on the 1 test pieces  which gives a total number of 1 test performances. the baseline accuracy - the success rate of pure guessing - is 1  or 1%. the confusion matrix of the experiment is given in table 1.
　again  it turns out that the artists are identifiable to varying degrees  but the recognition accuracies are all clearly above the baseline. in particular  note that the system correctly identifies performances by uchida in all but one case. obviously  the learner succeeds in reproducing something of the artists' styles in its model curves. while these figures seem to compare very favourably to the accuracies reported in  widmer and zanon  1  and  saunders et al.  1   they cannot be compared directly  because different recordings were used and  more importantly  the level of granularity of the training and test examples are different  movements in  widmer and zanon  1; saunders et al.  1  vs. sections in this paper   which probably makes our learning task easier.
1	replicating great pianists 
looking at figure 1  one might be tempted to consider the possibility of automatic style replication: wouldn't it be interesting to supply the computer with the score of a new piece and have it perform it 'in the style of'  say  vladimir horowitz or arthur rubinstein  this question is invariably asked when we present this kind of research to the public. unfortunately      the answer is: while it might be interesting  it is not currently feasible.
　for one thing  despite the huge effort we invested in measuring expressive timing and dynamics in recordings  the amount of available training data is still vastly insufficient vis-a-vis the enormous complexity of the behaviour to be learned. and secondly  the sort of crude beat-level variations in tempo and general loudness capture only a very small part of what makes an expressive interpretation; essential details like articulation  e.g.  staccato vs. legato   pedalling  the shaping of individual voices  etc. are missing  and will be very hard to measure from audio recordings at all . a computer performance based only on applying these beat-level tempo and loudness changes will not sound anything like a human performance  as can be readily verified experimentally. thus we have to admit that the title we chose for this paper is a bit pretentious: the computer cannot be expected to play like the great pianists - at least not given the current methods and available training data. it can  however  extract aspects of personal style from recordings by great pianists  as has been shown in the our experiments.
1	conclusion
an application of relational instance-based learning to a difficult task from the domain of classical music was presented: we showed how the problem of learning models of expressive piano performance can be reduced to applying simple expressive phrase-patterns by analogy to the most similar phrases in the training set. in particular  it was shown that a relational learner like distall succeeds in learning performance strategies that obviously capture aspects individual artistic style  which was demonstrated in learning and artist classification experiments.
　the ultimate goal of this kind of research is not automatic style replication or the creation of artificial performers  but to use computers to teach us more about the elusive artistic activity of expressive music performance. while it is satisfying to see that the computer is indeed capable of extracting information from performance measurements that seems to capture aspects of individual style  this can only be a first step.
in order to get real insight  we will need learning algorithms that  unlike nearest-neighbor methods  produce interpretable models. that is a natural next step in our future research.
acknowledgments
this research is supported by the austrian fonds zur forderung der wissenschaftlichen forschung  fwf ：	 project no. y1-inf . the austrian research institute for artificial intelligence acknowledges basic financial support by the austrian federal ministry for education  science  and culture  and the federal ministry of transport  innovation  and technology. thanks to werner goebl for performing the harmonic and phrase structure analysis of the mozart sonatas.
