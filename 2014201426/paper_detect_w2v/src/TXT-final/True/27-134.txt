irv: learning to integrate visual information across camera 
movements 
p e t e r n . p r o k o p o w i c z 
artificial intelligence laboratory 
department of computer science 
university of chicago 
1 east 1th street 
	chicago  illinois 	1 p a u l r. c o o p e r 
intelligent perception and action lab 
dept. of elec. eng. and computer science northwestern university 
1 maple avenue 
	evanston  illinois 	1 
¡¡our eyes see well only what is directly in front of them; they must continually scan the faces  words  and ob-
jects around us. perceptual integration is the process of combining the resulting jumpy  nonuniform images  figure 1  into our stable  comprehensive perception of the world. visual robots that scan their environment with moving cameras must also integrate visual information. we describe irv  a visual robot that sees across camera movements  prokopowicz  1 . furthermore  irv learns to integrate from experience  which consists of a series of random movements of a camera mounted on a motorized pan-tilt platform  observing the day-to-day activity in a laboratory. the learning procedure makes minimal assumptions  is robust  and scales. that is  learning proceeds without a prior analytic model  external calibration references  or a contrived environment  and can compensate for arbitrary imaging distortions  including lens aberrations  rotation of the camera about its viewing axis  and spatially-varying or even random sampling patterns. 
prokopowicz and cooper 	1    irv develops an accurate model of its own visualmotor geometry by learning to predict the sampled images that follow each random  but precise  camera movement  figure 1 . the model describes the relationship between any relative camera movement vector and the subsequent apparent motion of each pixel in the image. this relationship between corresponding pairs of visual points and camera movement vectors is stored in a representation we call the visual motor calibration map. the map is filled over time from natural observations during development. such table-based techniques for perceptual-motor development have been used to learn hand-eye coordination  mel  1  and dynamic arm control policies  atkeson  1 . for example  mel's murphy memorized the relationship between the visual position of key points on its arm and the joint angles of the arm in that position. the individual experiences that irv uses to fill its table are the visual shifts of pixels between successive images. this fundamentally ambiguous correspondence problem can not be determined from any single example. irv overcomes this ambiguity by accumulating evidence from every repetition of each possible camera movement. effectively  every apparent pixel correspondence  there are typically hundreds for every pixel  votes for the existence of an actual correspondence under the camera movement that just occurred. 

figure 1: the problem: a sequence of three overlapping views taken by a foveal  or spatially-varying  camera. slight changes in viewpoint emphasize completely different details. any understanding of the whole bcene  lower right  demands integration of information across eye movements  both for human and foveal computer vision. 
eventually  enough votes accumulate to determine the true geometric relationship between pixels in successive images for the entire repertoire of movements  and the predictions become more accurate. 
¡¡the calibration map is implemented as a connectionist visual memory that  during each movement  transforms visual information from the previous fixation into a reference frame centered on the new viewing direction. the visual shift of a single pixel for a particular movement is embodied as a three-way connection between units representing the camera movement  the visual location of a feature before the movement  and the location of the same feature after the movement. these connections are not present initially but develop during early 

figure 1: learning to predict foveal images. far left: foveal image before five degree leftward camera movement. left: image of same scene after movement. right: the ability to predict a post-movement image begins to develop after about 1 repetitions of the same relative movement. far right: prediction improves after 1 examples. resolution in the center of the predicted image is limited by the original peripheral resolution. figure 1: acquired visual-motor model allows calibration for arbitrary distortions. top left: another foveallysampled image. bottom left: the same image with arbitrary scrambling of the pixels. during learning  every input looks like this. right: same pixels  interpreted with calibration based on acquired visual-motor correspondences  after about 1 movements  or three days of learning. 
experience by a hypothesize-and-test process. over repeated practice movements  each visual unit notes any correlation between its inputs after the movement  and those of other units before the movement. the combinatorics of the problem make it impossible to record the frequency of every conceivable correspondence between pixels. instead  only a small random set of possible correspondences is evaluated at any time. this reduces the number of connections needed for learning to 
1 npicelenmovements   which is feasible both for artificial and biological systems  prokopowicz  1 . 
   the learned relationship between corresponding pairs of visual features and relative camera movements defines a motor-baaed metric that irv uses to interpret an arbitrary non-uniform visual representation in terms of known movement angles. this interpretation assigns a true visual angle for each pixel  regardless of optical or sampling distortions. the calibration workb by constraining the angles assigned to pairs of corresponding pixels so that they are separated by the size and direction of the movement angle for which they have been 
1 videos 
found to correspond. as the accuracy of learned visualmotor correspondences improves  so does the accuracy of the constrained assignment of visual angles to pixels  figure 1 . 
   irv can see over a field of view wider than that observable from the camera in a single position  figure 1 . irv learns to visualize internally the location of peripheral image details that can no longer be resolved. the connectionist visual memory continually accumulates visual features near the fovea  and integrates them over time and eye movements by imagining where they would appear from the present viewing direction. we have replicated human psychophysical experiments which show that irv can perceive and make accurate judgements about simple forms too large to fit in a single view. 
   the connectionist computational architecture and the experimental environment approximate the conditions of biological perceptual development; the learning algorithm is neurophysiologically plausible. learning and mature performance both manifest time and space complexities commensurate with human abilities and resources. in other words  the learning algorithm scales completely. the results confirm the practicality of visual robots that learn to perceive the stability of the world despite eye movements  learn to integrate geometric features across fixations  and  in general  develop and calibrate accurate models of their own perceptual-motor systems. 
