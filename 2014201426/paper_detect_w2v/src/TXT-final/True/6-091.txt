 
in this paper we describe an integrated multilevel learning approach to multiagent coalition formation in a real-time environment. in our domain  agents negotiate to form teams to solve joint problems. the agent that initiates a coalition shoulders the responsibility of overseeing and managing the formation process. a coalition formation process consists of two stages. during the initialization stage  the initiating agent identifies the candidates of its coalition  i.e.  known neighbors that could help. the initiating agent negotiates with these candidates during the finalization stage to determine the neighbors that are willing to help. since our domain is dynamic  noisy  and time-constrained  the coalitions are not optimal. however  our approach employs learning mechanisms at several levels to improve the quality of the coalition formation process. at a tactical level  we use reinforcement learning to identify viable candidates based on their potential utility to the coalition  and case-based learning to refine negotiation strategies. at a strategic level  we use distributed  cooperative casebased learning to improve general negotiation strategies. we have implemented the above three learning components and conducted experiments in multisensor target tracking and cpu re-allocation applications. 
1 introduction 
multiagent coalition formation is important for distributed applications ranging from electronic business to mobile and ubiquitous computing where adaptation to changing resources and environments is crucial. it increases the ability of agents to execute tasks and maximize their payoffs. moreover  coalitions can dynamically disband when they are no longer needed or effective. thus the automation of coalition formation will not only save considerable labor time  but also may be more effective at finding beneficial coalitions than human in complex settings  jennings  1 . 
　although considerable research has been conducted either in coalition formation among self-interested agents  e.g.   tohme and sandholm  1    sandholm et al  1    sen and dutta  1    or in coalition formation among cooperative agents  e.g.   shchory et al  1    little work has been done in coalition formation among both selfinterested and cooperative agents. furthermore  there have been no attempts to study coalition formation among such agents in a dynamic  real-time  uncertain  and noisy environment  which is a typical real-world environment and in which a sub-optimal coalition needs to be formed in a realtime manner. 
　we propose an integrated multilevel learning approach to multiagent coalition formation. in our approach  agents are assumed to be cautiously cooperative-they are willing to help only when they think they benefit from it-and honest. however  due to the noisy  uncertain  dynamic and real-time nature of our domain  not every agent can be correct in its perceptions and assumptions. thus  to achieve a coalition  an initiating agent has to negotiate with other agents. through concurrent  multiple 1 -to-1 negotiations  the initiating agent identifies the agents that are willing to help. the formation process is successful if the initiating agent successfully persuades enough agents to join the coalition. 
　note that in this paper  we focus on improving the quality of the coalition formation process  and not on the quality of the coalition after it is formed and executed. 
　note also that our approach is an example of the  good enough  soon enough  design paradigm. in our domain  an agent has incomplete information about the environment  the task execution is time constrained  and the communication between agents is not reliable  so an optimal coalition formed from the deep learning is impractical. thus  a suboptimal yet fast coalition formation process is warranted. 
1 coalition formation 
multiagent systems 	1 in our problem domain  when an agent cannot solve a task execution or resource allocation problem by itself or can get more benefits from collaborating with other agents  it initiates a coalition formation process to form a coalition and solve the problem jointly. figure 1 depicts our coalition formation modules that make up the two stages: initialization and finalization. the feasibility study and the ranking of candidates are the initialization stage whereas the negotiations and their management the finalization stage. this two-stage model  soh and tsatsoulis  1  allows an agent to form an initial coalition hastily and quickly to react an event and to rationalize to arrive at a working final coalition as time progresses  as a result of our previously described domain nature. 

figure 1. an overview of the coalition formation process 
here we briefly describe each module of the design: 
 1  dynamic profiling: every agent dynamically profiles each neighbor as a vector in the agent about the negotiation relationship between them  and profiles each negotiation task as a case in the casebase about the negotiation strategy description and negotiation outcome. 
 1  feasibility study: this module analyzes the problem and computes  a  whether the agent has the resources to do something about it  and  b  if yes  the list of agents that the agent thinks could help. 
 1  ranking of candidates: this module scores and ranks each candidate  and proportionately assigns the requested demand to each candidate  based on its potential utility  section 1 . 
 1  management: this module initiates negotiations with top-ranked candidates. that is  the module manages multiple  concurrent 1 -to-1 negotiations. for each negotiation task  it first finds a negotiation strategy through cbr. then  it spawns a thread to execute that negotiation task. the module oversees the various negotiation threads and modifies the tasks in real-time. for example  the module will terminate all remaining negotiations once it finds out that it no longer can form a viable coalition. the module will reduce its requests or demands once it has secured agreements from successful negotiations. and so on. in effect  this management simulates a 1 -to-many negotiation. 
 1  cbr: given the problem description of a task  the cbr module retrieves the best case from the casebase  and adapts the solution of that best case to the current problem. this is based on the work of  soh and tsatsoulis  1 . 
 1  negotiation: our negotiation protocol is argumentative. the initiating agent provides evidence for its request to persuade the responding agent. the responding agent evaluates these evidence pieces and if they are higher than a dynamic persuasion threshold  then the responding agent will agree to the request. the responding agent also has the ability to counter-offer due to time constraints or poor evidence. this is based on the work of  soh and tsatsoulis  1 . 
 1  acknowledgment: 	once all negotiations are com-
pleted  if a coalition has been formed  the agent confirms the success of the coalition to all agents who have agreed to help. if the agent has failed to form a coalition  it informs the agents who have agreed to help so they can release themselves from the agreements. 
　in the next section  we will discuss the learning mechanisms  a critical part of our coalition formation approach. 
1 	learning 
our learning approach incorporates reinforcement learning and case-based learning at two levels. at a tactical level  we use reinforcement learning to identify viable candidates based on their potential utility to the coalition  and casebased learning to refine specific negotiation strategies. at a strategic level  we use distributed  cooperative case-based learning to improve general negotiation capabilities. 
1 reinforcement learning 
reinforcement learning is evident at the coalition initialization and finalization stages. during initialization  the initiating agent measures the potential utility of a candidate based on a weighted sum of  1  its past cooperation relationship with the initiator such as the candidate's helpfulness  friendliness  and the agent's helpfulness and importance to the candidate   1  its current cooperation relationship with the initialing agent such as whether the two agents have already been negotiating about other problems  and  1  its ability to help towards the current problem. an initiating agent thus will more likely approach the agents that have been helpful before  thus reinforcing the cooperation relationship among them. 
　during finalization  an initiating agent also appeals to a candidate about how helpful the initiating agent has been in the past. a candidate is more easily persuaded if it realizes that a particular agent has been helpful in the past  and thus once again reinforcing their cooperation relationship. for details  please refer to  soh and tsatsoulis  1 . 
1 case-based learning 
we use cbr to retrieve and adapt negotiation strategies for negotiations during coalition finalization. we also equip our cbr module with both individual and cooperative learning capabilities  figure 1 . individual learning refers to learning based on an agent's perceptions and actions  without direct communication with other agents. cooperative learning refers to learning other agents' experience through interaction among agents. when an agent identifies a problematic case in its casebase  it approaches other agents to obtain a possibly better case. 

1 	multiagent systems 


figure 1. the relationship between case learning and cbr as well as negotiation tasks in an agent 
there has been research in distributed and cooperative 
cbr.  prasad and plaza  1  proposed treating corporate memories as distributed case libraries. resource discovery was achieved through  1  negotiated retrieval that dealt with retrieving and assembling case pieces from different resources in a corporate memory to form a good overall case  and  1  federated peer learning that dealt with distributed and collective cbr  plaza et ai  1 .  martin et ah  1  extended the model using the notion of competent agents.  martin and plaza  1  employed an auctionbased mechanism that focused on agent-mediated systems where the best case was selected from the bid cases. 
　our methodology employs a cautious utility-based adaptive mechanism to combine the two learning approaches  an interaction protocol for soliciting and exchanging information  and the idea of a chronological casebase. it emphasizes individual learning and only triggers cooperative learning when necessary. our cooperative learning also differs from collective cbr in that it does not merge case pieces into one as it considers entire cases. in addition  our research focus here is to define a mechanism that combines individual and cooperative learning. 
　note that the communication and coordination overhead of cooperative learning may be too high for cooperative learning to be cost-effective or timely. moreover  since an agent learns from its own experience and its own view of the world  its solution to a problem may not be applicable for another agent facing the same problem. this injection of 
foreign knowledge may also be risky as it may add to the processing cost without improving the solution quality of an agent  marsella et al.% 1 . 
1.1 chronological casebase and case utility 
we have utilized the notion of a chronological casebase in which each case is stamped with a time-of-birth  when it was created  and a time-of-membership  when it joined the casebase . all initial cases are given the same timeof-birth and time-of-membership. in addition  we profile each case's usage history  table 1 . an agent evaluates the utility of a case based on its usage history. if the case has a low utility  it may be replaced  or forgotten . if the case is deemed problematic  then a cooperative learning will be triggered and the case will be replaced. table 1 shows the heuristics we use in tandem with the chronological casebase. when a negotiation completes  if the new case adds to the casebase's diversity  the agent learns it. if the casebase's size has reached a preset limit  then the agent considers replacing one of the existing cases with the new case. for our individual case-based learning  we use heuristics ///  //1  and h1. 

table 1. the usage history that an agent profiles of each ease 

table 1. heuristics that support the chronological casebase 
1.1 cooperative learning 
figure 1 depicts our cooperative learning design. we adhere to a cautious approach to cooperative learning: 
 1  the agent evaluates the case to determine whether it is problematic. to designate a case as problematic  we use heuristics h1 and h1  a  frequently used  case is problematic if it has a low success rate  tsu/tu  and a high incurrence rate  tinc/tu . the profiling module keeps track of the utility of the cases. 
 1  the agent only requests help from a selected agent that it thinks is good at a particular problem. we want to approach neighbors who have initiated successful negotiations with the current agent  with the hope that the agent may be able to learn how those neighbors have been able to be successful. this is determined based on the profile of each neighbor that the agent maintains. the exchange protocol is carried out by the case request and case response modules. 

multiagent systems 	1 

 1  if the foreign case is similar to the problematic case  the agent adapts the foreign case before adopting it into its cascbase. at the same time  the usage history parameters of the new case are reset. 
 1  if a problematic case cannot be fixed after k times  it will be removed  heuristics h1 and h1 . 

figure 1. the cooperative learning design 
1 	experiments and results 
we have implemented a multiagent syrtem with multiple agents that perform multi-sensor target tracking and adaptive cpu reallocation in a noisy environment  simulated by a java-based program called radsim . each agent has the same capabilities  but is located at a unique position. each agent controls a sensor and can activate the sensor to search-and-detect the environment. when an agent detects a moving target  it tries to implement a tracking coalition by cooperating with at least two neighbors. and this is when a 
cpu shortage may arise: the activity may consume more cpu resource. when an agent detects a cpu shortage  it needs to form a cpu coalition to address the crisis. 
　the multiagent system is implemented in c++. in the current design  each agent has 1 + jv threads. the core thread is responsible for making decisions and managing tasks. a communication thread is used to interact with the message passing system of the sensor. an execution thread actuates the physical sensor: calibration  search-and-detect for a target  etc. each agent also has n negotiation threads to conduct multiple  concurrent negotiations. 
　we used two simulations for our experiments. we conducted experiments in a simulation called radsim where communication may be noisy and unreliable  and one or two targets may appear in the environment. we also designed and implemented our own cpu shortage simulation module. each task is designated with a cpu usage amount plus a random factor. when an agent detects a cpu shortage  the tasks that it currently performs slow down. thus  a cpu shortage that goes unresolved will result in failed negotiations since our negotiations are time-constrained. 
1 impacts of learning 
we also conducted experiments with four versions of learning:  1  both case-based reasoning and reinforcement learning  cbrrl    1  only case-based reasoning  norl    1  only reinforcement learning  nocbr   and  1  no learning at all  nocbrrl . figure 1 shows the result in terms of the success rates for negotiations and coalition formations. 

figure 1. success rates of negotiations and coalition formations for different learning mechanisms 
　the agent design with both case-based reasoning/learning and reinforcement learning outperformed others in both negotiation success rate and coalition formation success rate. that means with learning  the agents were able to negotiate more effectively  perhaps more efficiently as well  that led to more coalitions formed. without either learning  but not both   the negotiation success rates remained about the same but the coalition formation rate tended to deteriorate. this indicates that without one of the learning methods  the agents were still able to negotiate effectively  but may be not efficiently  resulting in less processing time for the initiating agent to post-process an agreement . with no learning  the agents fared noticeably poorly. 
1 resource allocation and system coherence 
we conducted experiments in cpu re-allocation to test the coherence of our system. we refer to the cpu allocation as a sustenance resource since in order for an agent to obtain more cpu  it needs to incur cpu usage while negotiating for the resource. by varying the amount of the initial cpu allocation to each agent  we created mildly-constrained  overly-constrained  and unevenly-constrained scenarios. tables 1 and 1 compared the agents' behavior in terms of successes in negotiations and coalition formations. in particular  the coalition success rate is the number of successfully formed coalitions over the number of coalitions initiated  where a coalition is successfully formed when the cpu obtained satisfies the agent's need. we observed the following: 
 1  in all experiments  the reduction in cpu shortage of each agent and the whole system was obvious. gradually  the cpu resource was reallocated more evenly among agents. the possibility of a cpu shortage decreases and each agent's shortage amount decreases. this shows a coherent  cooperative behavior among the agents. 

1 	multiagent systems 

 1  in all experiments  after some period of time  each agent's cpu allocation converged to an average level  1% . after that  each agent fluctuated around that level. 
 1  we also observed that the coalition formation was the most successful for the system as a whole when there were roughly the same number of resourceful and resourcestarved agents  experiment #1  and this type of system also required the least number of negotiations and coalitions to converge. 
table 1. comparison between negotiations in experiments 
1.1 comprehensive experiment a  cea  
we conducted four sets of experiments in cea as shown in table 1. the goal of these experiment sets was to investigate how learning differed given different casebase sizes  and how learning differed given different types of initial casebases  some had cases collected from different agents from an earlier run  some had only their own cases . note that for the following experiments we set the limit on the casebase size as 1 where case replacement started to take place. we used two main parameters to evaluate the casebases: utility and diversity. first  we rank the outcome of each case following the utility values of table 1. 

table 1. experiment sets. for example  in esi  every agent has 1 cases in its casebase; and so on 

table 1. utility of each outcome for a case 
table 1. utility and difference gains for both sub-experiments expl and exp1  after the second stage  for initiating casebases 
looking at all our results  we observed the following: 
 1  cooperative learning results in more utility and diversity per learning occurrence than individual learning  
multiagent systems 	1  1  a small casebase learns more effectively in terms of utility and diversity  but not faster since our learning is problem-driven. a large casebase learns in a similar manner as an average casebase except when it is greater than the preset limit that triggers case replacement. 
 1  the initial casebase affects the effectiveness of learning. both types of learning bring more utility and diversity to an initial casebase previously grown within an agent than one that has been influenced by other agents. 
1.1 comprehensive experiment b  ceb  
the objective of ceb was to see how the learning results changed in different environments  as shown in table 1. 

tabic 1. sub-experiments setup in ceb 
　a tracking coalition is more taxing since it requires at least three agents to be successfully formed. moreover  a tracking task is durational such that it takes time to actually carry out the tracking task. however  a cpu re-allocation task is carried out at a point in time. in addition  a tracking task is highly time-constrained. a coalition has to be formed in time to catch the target before the target moves out of the sensor coverage area. thus  negotiations related to tracking are more difficult to manage. for these three sets of sub-experiments  they had a few things in common:  1  all of them began with the same set of initial case bases  and  1  every sub-experiment ran with the both individual and cooperative learning. we observed the following: 
 1  different environments affect agents' learning behavior. depending on the frequency of a task and its characteristics  an agent may rely more on individual learning or cooperative learning. for example  if a type of tasks  tracking  is time consuming and durational  then increasing its frequency actually weakens the potential benefit of individual learning and encourages more cooperative learning. 
 1  the environments impact the two initiating and responding roles differently  especially for negotiations associated with tough requirements  such as at least three members of a tracking coalition . since an initiating agent has to shoulder the coalition management and decision making  it is able to learn more diverse and useful cases. but  negotiating as a responder  an agent's responsibility is less and thus considers fewer issues; thus the learning is less impressive. 
1 	conclusions 
we have described an integrated multilevel approach to coalition formation  using case-based learning and reinforcement learning to learn better tactics as the agent solves a problem  and distributed  cooperative case-based learning to learn improve the agent's knowledge base strategically. we have conducted several experiments and the results have been promising in proving the feasibility of our approach. with learning  our agents negotiate and form coalitions better. our future work will focus on tying the outcome of an executed coalition  already formed  to the planning stage to improve our strategic learning. 
acknowledgments 
this work is partially supported by a grant from the darpa ants project  subcontracted from the university of kansas  contract number: 1-1 . we also thank juan luo for her programming and experiments. 
