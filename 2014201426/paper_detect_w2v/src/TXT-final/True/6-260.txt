 
we address the problem of optimally controlling stochastic environments that are partially observable. the standard method for tackling such problems is to define and solve a partially observable markov decision process  pomdp . however  it is well known that exactly solving pomdps is very costly computationally. recently  littman  sutton and singh  1  have proposed an alternative representation of partially observable environments  called predictive state representations  psrs . psrs are grounded in the sequence of actions and observations of the agent  and hence relate the state representation directly to the agent's experience. in this paper  we present a policy iteration algorithm for finding policies using psrs. in preliminary experiments  our algorithm produced good solutions. 
1 predictive state representation 
we assume that we are given a system consisting of a discrete  finite set of n states 1  a discrete finite set of actions a  and a discrete finite set of observations o. the interaction with the system takes place at discrete time intervals. the initial state of the system so is drawn from an initial probability distribution over states i. on every time step t  an action at is chosen according to some policy. then the underlying state changes to  and a next observation 1i+1 is generated. the system is markovian  in the sense that for every action  the transition to the next state is generated according to a probability distribution described by an  n x n  transition matrix  similarly  for a given observation o and action a  the next observation is generated according to an  n x n  diagonal observation matrix  is the probability of observation o when action a is selected and state i is reached. since we are interested in optimal control  rather than prediction  we also assume that there exists a set of reward vectors  for each action a  where is the reward for taking action 
a in underlying state i 
　psrs are based on the notion of tests. a test is an ordered sequence of action-observation pairs q =  the 
prediction for test q is the probability of the sequence of observations  being generated  given the sequence of actions a1...ak. the prediction for a test q given prior history //-  denoted  is the probability of seeing the sequence of observations of q after seeing history h and taking the sequence of actions specified by q. for any set of tests q  its prediction vector is: 

a set of tests q is a psr if its prediction vector forms a sufficient statistic for the dynamical system  i.e.  if all tests can be predicted based on p q|h . of particular interest is the case of linear psrs  in which there exists a projection vector mq for any test q such that 

   littman et al. also define an outcome function u mapping tests into n-dimensional vectors defined recursively by:  and u aoq  =  where e repre-
sents a null test and cn is the  1 x n  vector of all is. each component u   q  indicates the probability of the test q when its sequence of actions is applied from state st. a set of tests q =  = 1 ..a;} is called linearly independent if the outcome vectors of its tests  arc linearly independent. using this definition  such a set q can be found by a simple search algorithm in polynomial time  given the pomdp model of the environment. littman  sutton and singh  1  showed that the outcome vectors of the tests in q can be linearly combined to produce the outcome vector for any test. 
1 policy evaluation using psrs 
we assume that we are given a policy and that the initial start state of the system  is drawn according to the staring probability distribution i. if we consider a given horizon t  only a finite number of tests of length t are possible when starting from i. let be this set of possible tests. 
　the value of a memoryless policy with respect to a given start state distribution i is the expected return over all possible tests that can occur when the starting state is drawn form i and then behavior is generated according to policy  
where is the expected return for test q given that the initial state is drawn from / and policy is followed. 

1 	poster papers 

　let u be the matrix formed by concatenating the outcome vectors for all tests in and be its pseudoinverse. the columns of u define the probability of each test in  when applied from each underlying state. consequently  iu represents the probability of the tests in  when starting in /. 
　for each action-observation combination ao  we can define a 
and a projection vector: 

　considering the probability distribution over tests that generates and the expected return of a test q given / and we have: 

this evaluation method requires a large amount of precomputation  but it can be useful if a small horizon suffices to get a good policy. 
1 policy iteration for psrs 
similar to pomdps in psrs we can define action-value functions on the level of tests. the action-value function for taking action a after test q can be computed as: 

　then  the policy of the agent can be improved by choosing an action greedily with respect to this action-value function: 

　the agent must  in other words  select the best policy tree rooted at each decision point and each time step. therefore the total running time of the algorithm is  and the complexity of the algorithm is only 
single-exponential in the horizon time. 
1 experimental results 
we experimented with a standard gridworld navigation task used in the pomdp literature  cassandra  1; parr& russell  1 . the environment is a  1 x 1  grid. the agent has four actions  n  s  e  and w  which change its location deterministically to one of the four neighboring states. there is one goal state  in the lower right corner  which generates a distinct observation and a reward of +1. all the other states are perceptually aliased and generate no reward. the initial probability distribution is uniform over all states except the goal. taking any action in the goal state moves the agent uniformly randomly to one of the other states. 
　for this problem we have 1 possible combinations of action-observation  ao  pairs: 1 -nnothing 1-wnothing 1enothing 1-snothing 1-egoal 1-sgoal. the set of core tests 

figure 1: policy quality vs. number of iterations for the 1 grid world 
q found consists of 1 linearly independent tests: 1  1  1  1  1  1  1  1  1  1  1  1  1  1. we tried this problem for finite horizon case with a discount factor r = 1. figure 1 indicates the performance of the algorithm on this problem. 
1 conclusion and future work 
the key difficulty for our planning algorithm is that the number of possible tests grows exponentially with the horizon length. hence  the algorithm cannot be used for large problems or for the infinite horizon case. however  this is not worse than other existing exact solution methods for solving pomdps. our hope is that good approximations of the optimal solution can be found efficiently. 
