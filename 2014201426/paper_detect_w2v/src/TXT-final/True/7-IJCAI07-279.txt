
most word sense disambiguation  wsd  methods require large quantities of manually annotated training data and/or do not exploit fully the semantic relations of thesauri. we propose a new unsupervised wsd algorithm  which is based on generating spreading activation networks  sans  from the senses of a thesaurus and the relations between them. a new method of assigning weights to the networks' links is also proposed. experiments show that the algorithm outperforms previous unsupervised approaches to wsd.
1 introduction
word sense disambiguation  wsd  aims to assign to every word of a document the most appropriate meaning  sense  among those offered by a lexicon or a thesaurus. wsd is important in natural language processing and text mining tasks  such as machine translation  speech processing  information retrieval  and document classification. a wide range of wsd algorithms and techniques has been developed  utilizing machine readable dictionaries  statistical and machine learning methods  even parallel corpora. in  ide and veronis  1  several approaches are surveyed; they address wsd either in a supervised manner  utilizing existing manually-tagged corpora  or with unsupervised methods  which sidestep the tedious stage of constructing manually-tagged corpora.
　the expansion of existing  and the development of new word thesauri has offered powerful knowledge to many text processing tasks. for example  exploiting is-a relations  e.g. hypernyms/hyponyms  between word senses leads to improved performance both in text classification  mavroeidis et al.  1  and retrieval  voorhees  1 . semantic links between senses  as derived from a thesaurus  are thus important  and they have been utilized in many previous wsd approaches  to be discussed below. previous wsd approaches  however  have not considered the full range of semantic links between senses in a thesaurus. in  banerjee and pedersen 

　　  partially funded by the pened 1 programme of the eu and the greek general secretariat for research and technology.
  supported by the marie curie intra-european fellowship.1  a larger subset of semantic relations compared to previous approaches was used  but antonymy  domain/domain terms and all inter-pos relations were not considered.
　in this paper we propose a new wsd algorithm  which does not require any training  in order to explore the aforementioned potential. a word thesaurus is used to construct spreading activation networks  sans   initially introduced in wsd by quillian . in our experiments we used wordnet  fellbaum  1   though other thesauri can also be employed. the innovative points of this new wsd algorithm are:  a  it explores all types of semantic links  as provided by the thesaurus  even links that cross parts of speech  unlike previous knowledge-based approaches  which made use of mainly the  is-a  and  has-part  relations;  b  it introduces a new method for constructing sans for the wsd task; and  c  it introduces an innovative weighting scheme for the networks' edges  taking into account the importance of each edge type with respect to the whole network. we show experimentally that our method achieves the best reported accuracy taking into account all parts of speech on a standard benchmark wsd data set  senseval 1  palmer et al.  1 .
　the rest of the paper is organizedas follows. section 1 provides preliminary information concerning the use of semantic relations and sans in the wsd task. section 1 discusses our san construction method and edge weighting scheme. section 1 presents our full wsd method  and section 1 experimental results and analysis. section 1 discusses related wsd approaches  and section 1 concludes.
1 background
1	semantic relations in wordnet
wordnet is a lexical database containing english nouns  verbs  adjectives and adverbs  organized in synonym sets  synsets . hereafter  the terms senses and synsets are used interchangeably. synsets are connected with various edges  representing semantic relations among them  and the latest wordnet versions offer a rich set of such links: hypernymy/hyponymy  meronymy/holonymy  synonymy/antonymy  entailment/causality  troponymy  domain/domain terms  derivationally related forms  coordinate terms  attributes  and stem adjectives. several relations cross parts of speech  like the domain terms relation  which connects senses pertaining to the same domain  e.g. light  as a noun meaning electromagnetic radiation producing a visual sensation  belongs to the domain of physics   and the attributes relation  which connects a word with its possible values  e.g. light  as a noun  can be bright or dull . our method utilizes all of the aforementioned semantic relations.
1	wsd methods that exploit semantic relations
several wsd approaches capitalize on the fact that thesauri like wordnet offer important vertical   is-a    has-part   and horizontal  synonym  antonym  coordinate terms  semantic relations. some examples of these approaches are  sussna  1; rigau et al.  1; leacock et al.  1; mihalcea et al.  1; mavroeidis et al.  1; montoyo et al.  1 . most of the existing wsd approaches  however  exploit only a few of the semantic relations that thesauri provide  mostly synonyms and/or hypernyms/hyponyms. for example patwardhan et al.  and banerjee and pedersen  focus mostly on the  is-a  hierarchy of nouns  thus ignoring other intra- and all inter-pos relations. in contrast  our method takes into account all semantic relations and requires no training data other than the thesaurus.
1	previous use of sans in wsd
spreading activation networks  sans  have already been used in information retrieval  crestani  1  and in text structure analysis  kozima and furugori  1 . since their first use in wsd by quillian   several others  cotrell and small  1; bookman  1  have also used them in wsd  but those approaches required rather ad hoc handencoded sets of semantic features to compute semantic similarities. the most recent attempt to use sans in wsd that we are aware of  overcoming the aforementioned drawback  is  veronis and ide  1 .
　figure 1 illustrates how sans were applied to wsd by veronis and ide. let w1 and w1 be two words that co-occur  e.g. in a sentence or text  and that we want to disambiguate. they constitute the network nodes  word nodes  depicted in the initial phase of figure 1; more generally  there would be n word nodes  corresponding to the n words of the text fragment. next  all relevant senses of w1 and w1 are retrieved from a machine readable dictionary  mrd   and are added as nodes  sense nodes  to the network. each word is connected to all of its senses via edges with positive weights  activatory edges . the senses of the same word  e.g. s1 and s1  are connected to each other with edges of negative weight  inhibitory edges . this is depicted as phase 1 in figure 1. next  the senses' glosses are retrieved  tokenized  and reduced to their lemmas  base forms of words . stop-words are removed. each gloss word  gw  is added as a node to the network  and is connected via activatory links to all sense nodes that contain it in their glosses  phase 1 . the possible senses of the gloss words are retrieved from the mrd and added to the network. the network continues to grow in the same manner  until nodes that correspond to a large part or the whole of the thesaurus have been added. note that each edge is bi-directional  and each direction can have a different weight.
　once the network is constructed  the initial word nodes are activated  and their activation is spread through the net-

figure 1: a previous method to generate sans for wsd.
work according to a spreading strategy  ensuring that eventually only one sense node per initial word node will have a positive activation value  which is taken to be the sense the algorithm assigns to the corresponding word. note that this approach assumes that all occurrencesof the same word in the text fragment we apply the algorithm to have the same sense  which is usually reasonable  at least for short fragments like sentences or paragraphs.
　we use a different method to construct the network  explained below. note also that veronis and ide do not use direct sense-to-sense relations; in contrast  we use all such relations  as in section 1  that exist in the thesaurus. moreover they weigh all the activatory and inhibitory edges with 1 and  1  whereas we propose a new weighting scheme  taking into account the importance of each edge type in the network.
1 san creation
in this section we propose a new method to construct sans for the wsd task  along with a new weighting scheme for the edges. our algorithm disambiguates a given text sentence by sentence. we only consider the words of each sentence that are present in the thesaurus  in our case wordnet. we also assume that the words of the text have been tagged with their parts of speech  pos . for each sentence  a san is constructed as shown in figure 1. for simplicity  in this example we kept only the nouns of the input sentence  though the method disambiguates all parts of speech. the sentence is from the d1 file in senseval 1 data set:
 if both copies of a certain gene were knocked out  benign polyps would develop. 
　to construct the san  initially the word nodes  in our case the nouns copies  gene and polyps  along with their senses are added to the network  as shown in the initial phase of figure 1. the activatory and inhibitory links are then added  as in the previous section  but after this point the san grows in a very different manner compared to veronis and ide. first  all the senses of the thesaurus that are directly linked to the existing senses of the san via any semantic relation are added to the san  along with the corresponding links  as shown in expansion round 1 of figure 1. every edge is bi-directional  since

figure 1: our method to construct sans.
the semantic relations  at least in wordnet  are bi-directional  e.g. if s1 is a hypernym of s1  s1 is a hyponym of s1 . in the next expansion round  the same process continues for the newly added sense nodes of the previous round. the network ceases growing when there is a path between every pair of the initial word nodes. then the network is considered as connected. if there are no more senses to be expanded and the respective san is not connected  we cannot disambiguate the words of that sentence  losing in coverage. note that when adding synsets  we use breadth-first search with a closed set  which guarantees we do not get trapped into cycles.
1	the spreading activation strategy
the procedureaboveleads to networkswith tens of thousands of nodes  and almost twice as many edges. since each word is eventually assigned its most active sense  great care must be taken in such large networks  so that the activation is efficiently constrained  instead of spreading all over the network. our spreading activation strategy consists of iterations. the nodes initially have an activation level 1  except for the input word nodes  whose activation is 1. in each iteration  every node propagates its activation to its neighbors  as a function of its current activation value and the weights of the edges that connect it with its neighbors. we adopt the activation strategy introduced by berger et al.   modifying it by inserting a new scheme to weigh the edges  which is discussed in section 1. more specifically  at each iteration p every network node j has an activation level aj p  and an output oj p   which is a function of its activation level  as shown in equation 1.
	oj p  = f aj p  	 1 
the output of each node affects the next-iteration activation level of any node k towards which node j has a directed edge. thus  the activation level of each network node k at iteration p is a function of the output  at iteration p   1  of every neighboring node j having a directed edge ejk  as well as a function of the edge weight wjk  as shown in equation 1. although this process is similar to the activation spreading of feed-forward neural networks  the reader should keep in mind that the edges of sans are bi-directional for each edge  there exists a reciprocal edge . a further difference is that no training is involved in the case of sans.
		 1 
unless a function for the output o is chosen carefully  after a number of iterations the activation floods the network nodes.
we use the function of equation 1  which incorporates fan-out and distance factors to constrain the activation spreading; τ is a threshold value.
  if aj p    τ
　　　　　　　　　 1    otherwise
equation 1 prohibits the nodes with low activation levels from influencing their neighboring nodes. the factor p+1 diminishes the influence of a node to its neighbors as the iterations progress  intuitively  as  pulses  travel further . function fj is a fan-out factor  defined in equation 1. it reduces the influence of nodes that connect to many neighbors.
		 1 
ct is the total number of nodes  and cj is the number of nodes directly connected to j via directed edges from j.
1	assigning weights to edges
in information retrieval  a common way to measure a token's importance in a document is to multiply its term frequency in the document  tf  with the inverse  or log-inverse  of its document frequency  idf   i.e. with the number of documents the token occurs in. to apply the same principle to the weighting of san edges  we consider each node of a san as corresponding to a document  and each type of edge  each kind of semantic relation  as corresponding to a token.
　initially each edge of the san is assigned a weight of  1 if it is inhibitory  edges representing antonymy and competing senses of the same word   or 1 if it is activatory  all other edges . once the network is constructed  we multiply the initial weight wkj of every edge ekj with the following quantity:
	etf ekj  ， inf ekj 	 1 
etf  defined in equation 1  is the edge type frequency  the equivalent of tf. it represents the percentage of the outgoing edges of k that are of the same type as ekj. when computing the edge weights  edges corresponding to hypernym and hyponym links are considered of the same type  since they are symmetric. the intuition behind etf is to promote edges whose type is frequent among the outgoing edges of node k  because nodes with many edges of the same type are more likely to be hubs for the semantic relation that corresponds to that type.
	etf	 1 
the second factor in equation 1  defined in equation 1  is the inverse node frequency  inf   inspired by idf. it is the frequency of ekj's type in the entire san.
		 1 
n is the total number of nodes in the san  and ntype ekj  is the number of nodes that have outgoing edges of the same type as ekj. as in idf  the intuition behind inf is that we want to promote edges of types that are rare in the san.
1 the wsd algorithm
our wsd algorithm consists of four steps. given a postagged text  a designated set of parts of speech to be disambiguated  and a word thesaurus:
step 1: fragment the text into sentences  and select the words

having a part of speech from the designated set. for each sentence repeat steps 1 to 1.
step 1: build a san  accordingto section 1. if the san is not

connected  even after expanding all available synsets  abort the disambiguation of the sentence.
step 1: spread the activation iteratively until all nodes are in-

active.1 for every word node  store the last active sense node with the highest activation.1
step 1: assign to each word the sense corresponding to the

sense node stored in the previous step.
1 experimental evaluation
we evaluated our algorithm on a major benchmark wsd data set  namely senseval 1 in the  english all words  task. the data set is annotated with senses of wordnet 1. we experimented with all parts of speech  to be compatible with all published results of senseval 1  palmer et al.  1 . table 1 shows the numberof occurrencesof polysemousand monosemous words of wordnet 1 in the data set we used  per pos  as well as the average polysemy.
	nouns	verbs	adj.	adv.	total
monosemous111polysemous111av. polysemy111table 1: occurrences of polysemous and monosemous words of wordnet 1 in senseval 1.
1	methods compared
in order to compare our wsd method to that of  veronis and ide  1   we implemented the latter and evaluated it on senseval 1  again using wordnet 1. we also included in the comparison the baseline for unsupervised wsd methods  i.e. the assignment of a random sense to each word. for the baseline  the mean average of 1 executions is reported. moreover  in order to evaluate the possibility of including glosses in our method  instead of only synset-to-synset relations  we implemented a hybrid method which utilizes both  by adding to our sans the gloss words of the synsets along with their senses  similarly to the method of veronis and ide. for the purposes of this implementation  as well as for the implementation of the original method of veronis and ide  we used the extended wordnet  moldovanand rus  1   which providesthe pos tags and lemmas of all wordnet 1 synset glosses. in the comparison  we also include the results presented in  mihalcea et al.  1 . there  another unsupervised knowledge-based method is proposed and is evaluated on senseval 1; it uses thesauri-generated semantic networks  along with pagerank for their processing. we also report the accuracy of the best reported unsupervised method that participated in the senseval 1  english all words  task  presented in  litkowski  1 .
1	performance of the methods
table 1 presents the accuracy of the six wsd methods  on the three files of senseval 1. the presented accuracy corresponds to full coverage  and hence recall and precision are both equal to accuracy. the results in table 1 suggest that our method outperforms that of veronis and ide  the hybrid method  and the random baseline. moreover  our method achieved higher accuracy than the best unsupervised method that participated in senseval 1  and overall slightly lower accuracy than the reported results of  mihalcea et al.  1 .

 baseline	synsets+glosses	 veronis & ide	 synsetsfigure 1: accuracy on polysemous words and the respective 1 confidence intervals.
　figure 1 shows the corresponding overall results for the four methods we implemented  when accuracy is computed only on polysemous words  i.e. excluding trivial cases  along with the corresponding 1 confidence intervals. there is clearly a statistically significant advantage of our method  synsets  over both the baseline and the method of veronis and ide. adding wordnet's glosses to our method
 synsets+glosses  does not lead to statistically significant difference  overlapping confidence intervals   and hence our method without glosses is better  since it is simpler and requires lower computational cost  as shown in section 1. the decrease in performance when adding glosses is justified by the fact that many of the glosses' words are not relevant to the senses the glosses express  and thus the use of glosses introduces irrelevant links to the sans.
wordssan
synsetssan glosses veronis and idesan
synsets+glossesbaselinebest unsup. senseval 1pagerank
mihalceamonopolyfile 1  d1 11111unavailable1file 1  d1 11111unavailable1file 1  d1 11111unavailable1overall1111111table 1: overall and per file accuracy on the senseval 1 data set.　figure 1 does not show the corresponding results of mihalcea et al.'s method  due to the lack of correspondingpublished results; the same applies to the best unsupervised method of senseval 1. we note that in the results presented by mihalcea et al.  there is no allusion to the variance in the accuracy of their method  which occurs by random assignment of senses to words that could not be disambiguated  nor to the number of these words. thus no direct and clear statement can be made regarding their reported accuracy. in figure 1 we compare the accuracy of our method against mihalcea et al.'s on each senseval 1 file. in this case we included all words  monosemousand polysemous  because we do not have results for mihalcea et al.'s method on polysemous words only; the reader should keep in mind that these results are less informative than the ones of figure 1  because they do not exclude monosemous words. there is an overlap between the two

 synsets file 1	synsets file 1	synsets file 1 mihalcea et al. file 1	mihalcea et al. file 1	mihalcea et al. file 1figure 1: accuracy on all words and the respective 1 confidence intervals.
confidence intervals for 1 out of 1 files  and thus the difference is not always statistically significant.
　regarding the best unsupervised method that participated in senseval 1  we do not have any further information apart from its overall accuracy  and therefore we rest on our advantage in accuracy reported in table 1. finally  we note that to evaluate the significance of our weighting  we also executed experiments without taking it into account in the wsd process. the accuracy in this case drops by almost 1%  and the difference in accuracy between the resulting version of our method and the method of veronis and ide is no longer statistically significant  which illustrates the importance of our weighting. we have also conducted experiments in senseval 1  where similar results with statistically significant differences were obtained: our method achieved an overall accuracy of 1% while ide and veronis achieved 1%. space does not allow further discussion of our senseval 1 experiments.
1	complexity and actual computational cost
let k be the maximum branching factor  maximum number of edges per node  in a word thesaurus  l the maximum path length  following any type of semantic link  between any two nodes  and n the number of words to be disambiguated. since we use breadth-first search  the computational complexity of constructing each san  network  is o n ， kl+1 . furthermore  considering the analysis of constrained spreading activation in  rocha et al.  1   the computational complexity of spreading the activation is o n1 ， k1l+1 . the same computational complexity figures apply to the method of veronis and ide  as well as to the hybrid one  although k and l differ across the three methods. these figures  however  are worst case estimates  and in practice we measured much lower computational cost. in order to make the comparison of these three methods more concrete with respect to their actual computational cost  table 1 shows the average numbers of nodes  edges  and iterations per network  sentence  for each method. moreover  the average cpu time per network is shown  in seconds   which includes both network construction and activation spreading. the average time for the san synsets method to disambiguate a word was 1 seconds. table 1
sansan glossessansynsetsveronis and idesynsets+glossesnodes/net.1.1 11.1edges/net.1.1 11.1pulses/net.111sec./net.111table 1: average actual computational cost.
shows that our method requires less cpu time than the hybrid method  with which there is no statistically significant difference in accuracy; hence  adding glosses to our method clearly has no advantage. the method of veronis and ide has lower computational cost  but this comes at the expense of a statistically significant deterioration in performance  as discussed in section 1. mihalcea et al. provide no comparable measurements  and thus we cannot compare against them; the same applies to the best unsupervised method of senseval 1.
1 related work
the majority of the wsd approaches proposed in the past deal only with nouns  ignoring other parts of speech. some of those approaches  yarowsky  1; leacock et al.  1; rigau et al.  1  concentrate on a set of a few pre-selected words  and in many cases perform supervised learning. in contrast  our algorithm requires no training  nor hand-tagged or parallel corpora  and it can disambiguate all the words in a text  sentence by sentence. though wordnet was used in our experiments  the method can also be applied using ldoce or roget's thesaurus. kozima and furugori  provide a straightforward way of extracting and using the semantic relations in ldoce  while morris and hirst  present a method to extract semantic relations between words from roget's thesaurus.
1 conclusions
we have presented a new unsupervised wsd algorithm  which utilizes all types of semantic relations in a word thesaurus. the algorithm uses spreading activation networks  sans   but unlike previous wsd work it creates sans taking into account all sense-to-sense relations  rather than relations between senses and glosses  and it employs a novel edge-weighting scheme. the algorithm was evaluated on senseval 1 data  using wordnet as the thesaurus  though it is general enough to exploit other word thesauri as well. it outperformed:  i  the most recent san-based wsd method  which overcame the problems older approaches faced  and  ii  the best unsupervised wsd method that participated in senseval 1. it also matched the best wsd results that have been reported on the same data.
