 
this paper describes improvements to the temporal difference learning method. the standard form of the method has the problem that two control parameters  learning rate and temporal discount  need to be chosen appropriately. these parameters can have a ma-
jor effect on performance  particularly the learning rate parameter  which affects the stability of the process as well as the number of observations required. our extension to the algorithm automatically sets and subsequently adjusts these parameters. the learning rate adjustment is based on a new concept we call temporal coherence  tc . the experiments reported here compare the extended  algorithm performance with human-chosen parameters and with an earlier method for learning rate adjustment  in a complex game domain. the learning task was that of learning the relative values of pieces  without any initial domain-specific knowledge  and from self-play only. the results show that the improved method leads to better learning  i.e. faster and less subject to the effects of noise   than the selection of human-chosen values for the control parameters  and a comparison method. 
1 introduction 
two major parameters that control the behaviour of 
sutton's  temporal difference algorithm td a.  are the learning rate  or step-size    and the temporal discount parameter   
　the choice of these parameters can have a major effect on the efficacy of the learning algorithm  and in practical problems they are often determined somewhat arbitrarily  or else by trying a number of values and 'seeing what works'  e.g. tesauro  . another widely used method is to use a learning rate that decreases over time  but such systems still require the selection of a suitable schedule. 
　sutton and singh  describe systems for setting both  and  within the framework of markov-chain 
1 	computer game playing 
models. however these methods assume relatively small numbers of distinct states  and acylic graphs  and so are not directly applicable to more complex real-world problems. jacobs  presented the 1delta-bar-delta' algorithm for adjusting  during the learning process. we compared the performance of delta-bar-delta with our algorithm on our sample domain. more recently  almeida  and schraudolph  have presented other methods for  adaptation for stochastic domains and neural networks respectively. 
   we describe a new system which automatically adjusts  and  this system does not require any a priori knowledge about suitable values for learning rate or temporal discount parameters for a given domain. it adjusts these parameters according to the learning experiences themselves. we present results that show that this method is effective  and in our sample domain yielded better learning performance than our best attempt to find optimum choices of fixed and  and better learning performance than delta-bar-delta. 
1. 	temporal difference learning 
temporal difference learning methods are a class of incremental learning procedures for learning predictions in multi-step prediction problems. whereas earlier prediction learning procedures were driven by the difference between the predicted and actual outcome  td methods are driven by the difference between temporally successive predictions. 
   sutton's  algorithm can be summarised by the following formula. given a vector of adjustable weights  w  and a scries of successive predictions  p  weight adjustments are determined at each timestep according to: 
	with respect to 	and 
   is the prediction at timestep t. the temporal discount parameter   provides an exponentially decaying weight for more distant predictions. 
　　the formula shows that is parameterised by  the learning rate  and the temporal discount factor. 

both parameters  and especially can have a major effect on the speed with which the weights approach an optimum. in sutton's paper  learning behaviour for different and values in sample domains is presented  but no method for determining suitable values a priori is known. learning rates too high can cause failure to reach stable values and learning rates too low can lead to orders of magnitude more observations being necessary. methods of choosing suitable and values before or during the learning are therefore advantageous. there have been several algorithms proposed for adjusting in supervised and td learning: ours is based on a new principle that we call temporal coherence. 
1. 	temporal coherence: adjusting  
our system of self-adjusting learning rates is based on the concept that the learning rate should be higher when there is significant learning taking place  and lower when changes to the weights are primarily due to noise. random noise will tend to produce adjustments that cancel out as they accumulate. adjustments making useful adaptations to the observed predictions will tend to reinforce as they accumulate. as weight values approach their optimum  prediction errors will become mainly random noise. 
　motivated by these considerations  our temporal coherence  tc  method estimates the significance of the weight movements by the relative strength of reinforcing adjustments to total adjustments. the learning rate is set according to the proportion of reinforcing adjustments as a fraction of all adjustments. this method has the desirable property that the learning rate reduces as optimum values are approached  tending towards zero. it has the equally desirable property of allowing the learning rate to increase if random adjustments are subsequently followed by a consistent trend. 
   separate learning rates are maintained for each weight  so that weights that have become close to optimum do not fluctuate unnecessarily  and thereby add to the noise affecting predictions. the use of a separate learning rate for each weight allows for the possibility that different weights might become stable at different times during the learning process. for example  if weight a has become fairly stable after 1 updates  but weight b is still consistently rising  then it is desirable for the learning rate for weight b to be higher than that for weight a. an additional potential advantage of separate learning rates is that individual weights can be independent when new weights are added to the learning process. if new terms or nodes are added to an existing predictor  independent rates make it possible for the new weights to adjust quickly  whilst existing weights only increase their learning rates in response to perceived need. 
　the tc learning rates are determined by the history of recommended adjustments to each weight. we use the term 'recommended change' to mean the temporal difference adjustment prior to multiplication by the learning rate. this detachment of the learning rate enables the tc algorithm to respond to the underlying adjustment impulses  unaffected by its own recent choice of learning rate. it has the additional advantage that if the learning rate should reach zero  future learning rates are still free to be non-zero  and the learning does not halt. 
　the recommended change for weight  at timestep t is defined as: 
after game is: 
where is the individual learning rate for weight and c is a learning rate for the whole process. 
   for each weight we are interested in two numbers: the accumulated net change  the sum of the individual recommended changes   and the accumulated absolute change  the sum of the absolute individual recommended changes . the ratio of net change  n  to absolute change  a  allows us to measure whether the adjustments to a given weight are mainly in the same 'direction'. we take reinforcing adjustments as indicating an underlying trend  and cancelling adjustments as indicating noise from the stochastic nature of the domain  or limitations of the domain model that contains the weights . the individual learning rate  for each weight  is set to be the ratio of net recommended change to absolute recommended change: 

with the following definitions and update rules: 
 at prediction t is the final outcome 
the operational order is that changes to are made first  using the previous values of  a  and   then and  arc updated. the parameter c has to be chosen  but this does not demand a choice between fast learning and eventual stability  since it can be set high initially  and the  then provide automatic adjustment during the learning process. all the  are initialised to 1 at the start of the learning process. 
　the foregoing formulae describe updating the weights and learning rates at the end of each sequence. the method may be easily amended to update more frequently  e.g. after each prediction   or less frequently  e.g. after a batch of sequences . for the experiments reported in this paper  update at the end of each game sequence is natural and convenient to implement. 
	beal and smith 	1 

1. 	prediction decay: determining  
 we determine a value for the temporal discount parameter   by computing a quantity we call prediction decay. prediction decay is a function of observed prediction values  indexed by temporal distance between them  described in more detail in appendix a. an exponential curve is fitted to the observed data  and the exponential constant  from the fitted curve is the prediction decay. we set initially  and thereafter. 
	the use of 	has the desirable characteristics that 
 i  a perfect predictor will result in  and td 1  is an appropriate value for the limiting case as predictions approach perfection   ii  as the prediction reliability increases   increases  and it is reasonable to choose higher values of  for td learning as the prediction reliability improves. we make no claim that setting is optimum. our experience is that it typically performs better than human-guessed choice of a fixed  a priori} 
　the advantage of using prediction decay is that it enables  to be applied effectively to domains without prior domain knowledge  and without prior experiments to determine an effective  when combined with our method for adjusting learning rates  the resulting algorithm performs better than the comparison method  and better than using fixed rates  in both test domains. 
　　prediction decay is the average deterioration in prediction quality per timestep. a prediction quality function measures the correspondence between a prediction and a later prediction  or end-of-sequence outcome . the observed prediction qualities for each temporal distance are averaged. an exponential curve is then fitted to the average prediction qualities against distance  figure 1 shows an example   and the exponential constant of that fitted curve is the prediction decay  we set the td discount parameter  to 1 initially  and thereafter. in the experiments reported   and hence were updated at the end of each sequence. 
　the prediction quality measure   we used is defined below. it is constructed as a piece-wise linear function with the following properties: 
i. 	when the two predictions p and  are identical  
　　= 1.  the maximum  is 1  ii. as the discrepancy between p and  increases   decreases  
iii. when one prediction is 1 and the other is 1  then  
　　　= - 1 .  the minimum iv. for any given py the average value of for all possible values of such that equals 1. 
 thus random guessing yields a score of zero.  this property is achieved by the quadratic equations in the definition below. 
we achieve all these properties by defining: 

p is the current prediction  p' is an earlier prediction  and d refers to the temporal distance between p and p'. 
predictions lie in the range  1  1 . 
　it is assumed that the learning occurs over the course of many multi-step sequences  in which a prediction is made at each step; and that the sequences are independent. to form a prediction pair  both predictions must lie within the same sequence.  is the average prediction quality over all prediction pairs separated by distance d observed so far. for this purpose  the terminal outcome at the end of the sequence is treated as a prediction. at every prediction  the  are incrementally updated. 
　an example graph from our experimental results is given in figure 1. this example is typical of the fit to the observed data in the test domain. the exponential curve is fitted to the average prediction quality by minimising the mean squared error between the exponential curve and the observed  values.  was fairly stable in the 
range 1 - 1 during the test runs. 

figure 1: fit of the prediction quality temporal decay to observed data from the game domain  after 1 games. 
　to prevent rarely occurring distances from carrying undue weight in the overall error  the error term for each distance is weighted by the number of observed prediction pairs. thus we seek a value of which minimises: 


1  by expending sufficient computation time to repeatedly rerun the experiments we found somewhat better values for  
1 	computer game playing 
where is the average prediction quality for distance dy and is the number of prediction pairs separated by that distance  and / is the length of the longest sequence in the observations so far. 

　in the experiments reported here the value for was obtained by simple iterative means  making small incremental changes to its value until a minimum was identified. values for   and hence  were updated at the end of each sequence. 
1. 	delta-bar-delta 
the delta-bar-delta algorithm  dbd  for adapting learning rates is described by jacobs  1   sutton  later introduced incremental. dbd for linear tasks. the original dbd was directly applied to non-linear tasks  and hence more easily adapted to our test domain. in common with our temporal coherence method  it maintains a separate learning rate for each weight. if the current derivative of a weight and the exponential average of the weight's previous derivatives possess the same sign  then dbd increases the learning rate for that weight by a constant   if they possess opposite signs  then the learning rate for that weight is decremented by a proportion   of its current value. the exponential average of past derivatives is calculated with  as the base and time as the exponent. the learning rates are initialised to a suitable value   and are then set automatically  although the meta-parameters and  must be supplied. to adapt dbd to td domains  we compute a weight adjustment term  and a learning rate adjustment at each timestep  after each prediction  but we only apply the weight and learning rate adjustments at the end of each td sequence. dbd is very sensitive to its metaparametcrs and prior to our experiments we performed many test runs  exploring a large range of metaparameter values and combinations. we used the best we found for the comparison between dbd and tc reported here. both algorithms update the weights  and the internal meta-parameters  at the end of each sequence. 
1. learning in a complex domain 
we tested our methods in a complex game domain. the chosen task was the learning of the values of chess pieces by a minimax search program  in the absence of any chess-related initial knowledge other than the rules of the game. 
　we attempted to learn suitable values for five adjustable weights  pawn  knight  bishop  rook and queen   via a series of randomised self-play games. learning from self-play has the important advantage that no existing expertise  human or machine  is assumed  and thus the method is transferable to domains where no existing expertise is available. beal and smith 1; 1j show that  using this method  it is possible to learn relative values of the pieces that perform at least as well as those quoted in elementary chess books. the learning performance of the temporal coherence scheme was compared with the learning performance using fixed learning rates  and with delta-bar-delta. 
　the td learning process is driven by the differences between successive predictions of the probability of winning during the course of playing a series of games. in this domain each temporal sequence is a set of predictions for all the positions reached in one game  each game corresponding to one sequence in the learning process. the predictions vary from 1  loss  to 1  win   and are determined by a search engine that uses the adjustable piece weights to evaluate game positions. the weights are updated after each game. 
　at the start of the experiments all piece weights were initialised to one  and a series of games were played using a 1-ply search. to avoid the same games from being repeated  the move lists were randomised. this had the effect of selecting at random from all tactically equal moves  and the added benefit of ensuring a wide range of different types of position were encountered. 
1 	evaluation predictions 
in order to make use of temporal differences  the values of positions were converted from the evaluation provided by the chess program into estimations of the probability of winning. this was done using a standard sigmoid squashing function. thus the prediction of probability of winning for a giver  position is determined by: 

where v is the 'evaluation value'  and/  is the piece count differential for piece type i at the given position. 
　this sigmoid function has the advantage that it has a simple derivative: 

1 	results 
to visualise the results obtained from the various methods for determining learning rates  we present graphs produced by plotting weights for each of the five piece values over the course of runs consisting of 1 game sequences each. beal and smith  1  show that this method is capable of learning relative piece values that compare favourably with the widely quoted elementary values of pawn = 1  knight and bishop = 1  rook = 1 and queen = 1. the number of sequences in each run is large enough that the values reach a quasi-stable state of random noise around a learnt value. to confirm that the apparent stability is not an artefact  each experiment was repeated 1 times  using different random number seeds. 
　figure 1 shows the average weights achieved using fixed settings of a = 1 and x = 1 over a series of 1 runs. these settings offered a good combination of learning rate and stability from the many fixed settings that we tried. a lower learning rate produced more stable values  but at the cost of further increasing the number of sequences needed to establish an accurate set of relative values. raising the learning rate makes the weights increasingly unstable. 
　figure 1 shows the average weights produced by the delta-bar-delta algorithm over 1 runs. for this domain 
	beal and smith 	1 



we used meta-parametcrs of k = 1  = 1  = 
1 and  = 1  guided by data presented by jacobs 
 and preliminary experiments in this domain. for  which dbd does not set  we used  derived from our experience with the fixed rate runs. we tried a number of other meta-parameter settings  none of which performed better than the chosen set. it is possible that a comprehensive search for a better set of meta-parameters might have improved the performance of the delta-bardelta algorithm  but given the computational cost of a 
single run of 1 sequences  we were unable to attempt a systematic search of all the meta-parameter values. 
　figure 1 shows the average weights obtained using temporal coherence. it can be seen from the figure that all traces have approached their final values after about 1 sequences  some weights much sooner . comparing with figures 1 and 1 it can be seen that the tc algorithm is faster to approach final values  and more stable once they are reached. in addition  the traces in figure 1 are smoother than in figures 1 and 1  representing less variation due to noise in the individual runs. 

figure 1: progress over an average of 1 runs 
　figure 1 shows the average piece values over 1 runs for the various methods  combined into a single term measuring progress toward the values achieved at the ends of the runs. from this figure we can see that deltabar-delta does not improve much on a carefully-chosen fixed learning rate  and that temporal coherence clearly produces faster learning. the tc and fixed-a final weights were not significantly different. 
　to confirm that the learning process had produced satisfactory values  a match was played pitting the learnt values against the values widely quoted in elementary chess books q=1  r=1  n/b=1  p=l. one program used those values  the other used the weights learnt using temporal coherence  as a check that the learnt values were at least as good as the standard ones. in a match of 1 games the tc values achieved a score of 1% against the standard values  won= 1 lost=1 drawn=1 . 
1. 	conclusions 
we have described two new extensions  temporal coherence  and prediction decay  to the temporal difference learning method that set and adjust the major control parameters  learning rate and temporal discount  automatically as learning proceeds. the resulting td algorithm has been tested in depth on a complex domain. 
　the results demonstrated both faster learning and more stable final values than a previous algorithm and the best of the fixed learning rates. the test domain was one in which values were learnt without supplying any domainspecific knowledge. we also tested the tc algorithm in a bounded walk domain  sutton  1j  and found similar advantages. 
　in our comparisons with the delta-bar-delta algorithm  we tried to find good parameter sets for dbd  which requires four meta parameters instead of the one control parameter   we tried several different  meta-  parameter sets in each domain  but were unable to find a set of parameters that improved performance over the results presented in sections 1 and 1. it is possible that a systematic search for better set of meta-parametcrs in each of the domains might improve performance. however  it is a major drawback for the method that it requires its meta-parameters to be tuned to the domain it is operating in. the methods presented here do not require a search for good parameter values. 
　the experimental results demonstrated that the temporal coherence plus prediction decay algorithm achieved three benefits:  1  automatic setting and adjustment of parameters  1  faster learning and  1  more stable final values. 
