 
　　　recent research demonstrates the use of goal regression as an analytic technique for learning search heuristics. this paper critically examines this research and identifies essential applicability conditions for the technique. the conditions that operators be invertible and that the domain be closed with respect to the inverse operators severely limit the use of analytic goal regression. in those restricted domains which satisfy the applicability conditions  analytic goal regression only discovers required preconditions for operator application. discovering pragmatic preconditions is beyond the capability of the technique. an alternative  called experimental goal regression  is defined which approximates the results of analytic goal regression without suffering from these limitations. 
i. introduction 
     goal regression was first used in ai by waldinger  wald1  as a technique for detecting and analyzing goal interactions during planning. given a goal state g and an operator op  a goal regression product is a description of a sub-goal state 1  such that op applied to s achieves g. the goal regression product corresponds to dijkstra's notion of weakest pre-condition  dijk1 . according to dijkstra  wp op g  is the weakest constraint on a state s which guarantees that the application of op to s yields a state satisfying g.*** 
     recent research in machine learning demonstrates the use of goal regression to improve concept acquisition. in particular  this research focusses on techniques for learning problem solving heuristics. given an operator sequence for a problem solution  goal regression serves to back-propagate constraints through the sequence to form 
heuristics for the individual operators. this technique involves formal reasoning with operator semantics and will be called analytic goal regression. 
     this paper is a critical review of the research on analytic goal regression and suggests a change in direction. 
section 1 briefly reviews this research. section 1 discusses 
　　* support for this research was provided by the army research office  under great number aro daag1-k-1. 
   ** partial support for this research was provided by hughes ai research center  calabasas  ca. 
*** in computer programs op is a sequence of program statements. the applicability conditions for successful application of analytic goal regression to machine learning. these conditions appear to restrict the use of analytic goal regression to a relatively small class of problem domains. section 1 introduces experimental goal regression as an alternative to the analytic technique. experimental goal regression uses induction from examples to approximate the  correct  result. this removes the constraining applicability conditions and permits goal regression in a wide class of 
problem domains. section 1 compares analytic and experimental goal regression. 
ii. review of research 
     utgoff  utg1  demonstrates the use of goal regression to adjust the bias inherent in the concept hierarchy trees used in lex  m1tc1 . partial state descriptions are regressed through a solution path to form a composite constraint on initial states in the path. motivating this work is the observation that the state description vocabulary should be rich enough to describe composite constraints. typically  this vocabulary is a priori domain knowledge. the significance of utgoff's use of goal regression is that the vocabulary can be dynamically enriched during the learning process. 
     porter and kibler  port1a  use an empirical variant of goal regression to improve the rate of learning problem solving heuristics. their method of episodic learning discovers useful operator sequences  kibl1b . the learning is incremental and heuristics which recommend operators applied near the goal state are learned first. these heuristics are regressed through the episode to learn additional heuristic rules. the significance of this use of goal regression is that the rate of episodic learning can be dramatically improved by broadcasting the refinement of one heurbtic through an episode to enable the refinement of other heuristics. 
     minton  mint1  demonstrates the power and potential of goal regression by showing effective learning from a single training instance. with a technique called constraint based generalization  state descriptions are generalized by deducing why the training instance is classified as positive. the technique is applied to learning generalized state descriptions for forced wins in two-person games. given a chain of actions resulting in a forced win  goal re-
　　　
1 	b. porter and d. kibler 
gression is used to back-propagate the important descriptors of the forced win state. the goal regression product is a description of the set of states for which the chain of actions achieves the forced win. the significance of this use of goal regression is that  a limited form of   oneshot  learning is possible by reasoning with explicit goal descriptions. 
iii. applicability conditions for analytic goal regression 
　　　this section defines two applicability conditions for effective use of analytic goal regression. 
a. invertible 	operators 
　　　the first applicability condition requires that the domain operators be invertible. given a strips-like declarative operator definition  inverses are easily computed by reversing the roles of the delete-list and add-list. however  procedural representations are more flexible and powerful for defining operator transformations   hewi1  mcde1  . unfortunately  analytically inverting a procedurally defined operator appears impossible in general. a similar problem occurs if the goal description is defined procedurally. 
　　　lex-ii  mitc1  partially circumvents this problem by providing the learning element with operator inverses. both the domain operators and their inverses are represented procedurally. as utgoff discovered  utg1   the chief shortcoming of this approach is the inability to reason with some operator definitions. for example  consider the substitution operator in symbolic integration which replaces a sub-expression of an integral with a variable. in lex  this operator is defined as: 

where poly f x   stands for a polynomial in x. the problem discovered by utgoff is that analytic goal regression with this operator definition fails. the critical constraint that whatever matches /' be the derivative of whatever matches / is not explicit in this operator representation. this constraint is embedded in an opaque representation of the operator. procedural representations can conceal essential operator constraints and prevent the analytic computation of goal regression products. 
b. relative closure of representation language 
　　　the second applicability condition for analytic goal regression requires that the representation language adequately express goal regression products. as we demonstrate  analytic goal regression can produce state descriptions which are unrepresentable in the language used for forward reasoning. in particular this constraint is not satisfied by merely assuming that one has a strips-like representation. 
　　　there is no problem with computing a goal regression product if the goal and the operator are fully instantiated. however  regressing expressions through partially instantiated operator definitions can be troublesome  nils1  pp 1 . consider the operator unstack x y  defined with the following strips rule: 
pre and delete conditions: handempty  clear x  on x  y  
add conditions: holding  x   clear  y  
the regression of the partial state description clear  c   for some constant c  yields the disjunction  y = c   /clear  c . external disjunction is often precluded from concept description languages  mich1  but commonly occurs in analytic goal regression products. 
　　　while a disjunctive clause might be split into separate clauses  thereby eliminating the disjunction  negated clauses can also be troublesome. for example  the regression of clear c  through unstack x  b  yields c  a clear c . these examples demonstrate that the concept description languages must support disjunction and negation if analytic goal regression is used. 
　　　the problem of representation language closure arises in minton's research  mint1 . as described in section 1  minton applies goal regression to learn forced move board positions in two person games. in particular  consider minton's example in the game gomoku. gomoku is similar to tic-tac-toe except the board is a 1 grid and the object is to get 1 x's or o's in a row. an open fourposition for x is four x's in a row with an adjacent blank position. an open three-position for x is three x's in a row with two blank positions on one side and one blank position on the other. 
　　　we believe that the description language cannot represent all goal regression products in this domain. the description language for board positions is a conjunct of predicates. minton computes the goal regression product of an open four-position as an open three-position. assuming the natural gomoku move operator definition: 
pre-condition and delete-condition: empty  square  add-condition: on  square  x  
the open three-position is only a subset of the correctly computed goal regression product. an open four can also result from applying the move operator to a blank - x - blank - x - x - blank 
position. therefore  the correct goal regression product is a disjunctive clause which is outside the expressive power of the description language. this problem might be addressed by extending the description language to include terms which correspond to disjunctive expressions in the original language. but  discovering appropriate shifts in representation language may be more difficult than the problem being addressed. 
　　　we believe that these applicability conditions restrict the use of analytic goal regression to a relatively uninteresting set of problem domains. in addition to these necessary limitations  there is an additional pragmatic concern. 
the technique forms totally precise rules which may not capture the data at hand. analytic goal regression could be used in the total absence of experience and might be called learning by reasoning or  no-shot  learning. 
　　　the next section proposes an alternative which approximates the results of analytic goal regression but relies on experience  rather than reasoning. 
iv. experimental goal regression 
　　　experimental goal regression is an alternative to analytic goal regression in which the applicable conditions are eliminated. the technique yields an approximation of the result of analytic goal regression. this approximation is incrementally refined using standard machine learning techniques. 
　　　experimental goal regression uses induction to approximate a goal regression product. consider states s1  s 1   . . .   sn in the domain search space which are mapped by operator op into a state satisfying the goal g. experimental goal regression applies standard induction techniques to s1  s1 ...  sn to form a partial description of the regression of g through op. 
　　　experimental goal regression avoides the limitations of analytic goal regression discussed in the previous section. first  the technique does not preclude procedural operator representations since op is not inverted. second  goals can be represented procedurally since experimental goal regression uses them only as boolean predicates on state descriptions. third  experimental goal regression does not require the language to be complete with respect to goal regression products. only a useful partial characterization of the inverse is generated instead of a completely accurate one as in the case of analytic goal regression. 
　　　the practical success of experimental goal regression relies on an efficient construction of multiple examples of goal regression products. one approach to example generation is perturbation  kibl1a   which relies on inherent regularity and continuity in the search space  lena1 . given a single example  perturbation automatically generates and classifies neighboring examples. the selection of the most promising neighbors can be guided with some knowledge of the transformation performed by the operator. relational operator models  port1b  are one technique for approximating operator definitions. 
v. an example 
　　　the following example compares analytic and experimental goal regression for learning search heuristics. the task is the algebraic simplification of a pair of simultaneous linear equations labelled a and 1. the operator sub a b  replaces equation 1 with the result of subtracting equation a from equation 1. similarly  sub b  a  replaces equation a with the result of subtracting equation b from equation a. for simplicity we assume that the sub operators are only 
b. porter and d. kibler 1 
applicable when the equations have equal x-coefficients. 
these operators can be defined as: 
	operator 	preconditions and 	add conditions 
delete conditions 

　　　as with other systems for learning search heuristics  lang1  ohls1   the learning element uses a static evaluation function for credit assignment. since the overall problem solving goal is to simplify the equations  the function counts the number of non-zero terms in the equations. 
　　　let us examine how analytic goal regression would learn heuristics for the sub operator. consider the following positive training instance for the operator sub a b : 

the static evaluation function reveals that sub a  1  is not effective in reducing the number of nonzero terms in the example. 
　　　experimental goal regression induces a goal regression product from positive examples. the representation of both the sub operators and the static evaluation function are irrelevant to the success of experimental goal regression. examples of effective applications of sub at b  are generated  perhaps by perturbation of a given example  and classified as positive or negative by the static evaluation function. induction over the set of positive examples using the climbing hierarchy tree generalization operator yields the heuristic rule: 
a : 	nonzero1x 	+ integer1y = integer1 
-  sub a  b  
b : 	nonzero1x 	+ nonzero1y = nonzero1 
1 	b. porter and d. kibler 
where nonzero and integer are typed variables. 
　　　a goal regression product  derived experimentally  incorporates the pragmatic preconditions for effective use of each operator. in the example above  pragmatic preconditions for sub a  b  require that both the y-coefficient and the constant term in equation b be non-zero. pragmatic preconditions are enforced by constraints external to the operator definition. therefore  they cannot be discovered by analytic goal regression. 
　　　we believe that combinations of analytic and experimental goal regression might be more promising than either technique alone. for example  a combination of techniques might mitigate the following problems: 
  analytic goal regression relies on extending the representation language to precisely describe goal regression products  section iiib . 
  experimental goal regression relies on efficiently generating a set of state descriptions  section iv . 
　　　using a combination of the two techniques  the goal regression product computed by analytic goal regression might be used to constrain the generation of state descriptions used in experimental goal regression. an analytic goal regression product that can be approximated in the representation language can be used as a  seed  for state description generation. the combination of analytic and experimental goal regression is a topic of future research. 
vi. conclusions 
　　　goal regression promises to be a powerful technique in learning about actions. predominantly  analytic goal regression has been explored. analytic goal regression is a powerful reasoning technique for  no-shot  learning. that is  learning could proceed given only the definition of the goals and operators. unfortunately  the necessity to invert operator definitions and to stay within the representation language severely limits the applicability of analytic goal regression. experimental goal regression is an alternative to the analytic technique which relies on proven machine learning algorithms to approximate the results of analytic goal regression. coupled with automatic example generation  experimental goal regression is an effective machine learning technique that does not suffer from the limitations of analytic goal regression. 
