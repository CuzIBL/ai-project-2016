 
keeping track of multiple objects over time is a problem that arises in many real-world domains. the problem is often complicated by noisy sensors and unpredictable dynamics. previous work by huang and russell  drawing on the data association literature  provided a probabilistic analysis and a threshold-based approximation algorithm for the case of multiple objects detected by two spatially separated sensors. this paper analyses the case in which large numbers of sensors are involved. we show that the approach taken by huang and russell  who used pairwise sensor-based appearance probabilities as the elementary probabilis-
tic model  does not scale. when more than two observations are made  the objects' intrinsic properties must be estimated. these provide the necessary conditional independencies to allow a spatial decomposition of the global probability model. we also replace huang and russell's threshold algorithm for object identification with a polynomial-time approximation scheme based on markov chain monte carlo simulation. using sensor data from a freeway traffic simulation  we show that this allows accurate estimation of long-range origin/destination information even when the individual links in the sensor chain are highly unreliable. 
1 	introduction 
the problem of tracking multiple objects over time has long been studied in the literature on data association  bar-shalorn and fortmann  1; bar-shalom  1 . the problem is defined as that of associating a set of current observations with a set of existing  tracks  or object trajectories  creating new tracks as needed. radar tracking of multiple aircraft is the canonical application. 
in ai  the problem of object identification is essentially 

   'permanent address: statistics dept.  hebrew university of jerusalem. 
1 	robotics and perception 
the same: deciding if some newly observed object is the same as some previously observed object. solving this problem is essential for any intelligent agent that reasons about individual objects. huang and russell  1; 1  provide a fairly general formulation of the problem and describe an application to traffic surveillance. other possible applications range from removing  duplicate  entries from databases to re-recognizing locations during exploratory map-building. 
　the object identification problem is difficult because sensors are noisy  objects look similar  and object behaviors are unpredictable. this leads to a large number of possible assigrunents specifying identities among observed objects. for example  in the traffic surveillance application studied by huang and russell  the sensors are cameras at various locations on a freeway network and the objects are vehicles. many thousands of vehicles pass each camera  and the system must decide whether each vehicle is a new vehicle or the same as one previously observed at a different location. over time  these decisions give rise to hypothetical vehicle trajectories. deriving a set of trajectories is the first 
step of many traffic surveillance applications  such as the average link travel time between locations or origindestination counts along different routes through the system. moreover  sudden changes in these quantities can be indicators of highway incidents  such as accidents or breakdowns. 
   adopting the notation used by huang and russell  let denote an assignment placing pairs  or  more gen-
erally  sets  of observed objects into equivalence classes  where each class represents an existing object  and let o denote all observations made to date. then the posterior probability that two objects and  are the same is given by 
 1  
　　　　　　we will make use can be assumed uniform. 

this is because  the probability of an assignment in the absence of observations linking the objects  must be invariant under renaming of the objects. this it the exchangeability assumption of huang and russell. 
　other quantities can also be calculated by summing over  for example  in freeway surveillance  specifies the correspondence between vehicles observed at upstream and downstream sensor locations. for a given the average link travel time  between the two locations can be calculated directly if the observations include the arrival time at each location. then the posterior expectation of the link travel time is 

this paper addresses the two principal difficulties that arise in putting such equations into practice. 
　section 1 deals with the computation of the terms-in particular their decomposition into tractable local models that can be estimated from data. we show that the decomposition proposed by huang and russell using appearance probabilities  while adequate for the case of two sensor locations  does not scale up to handle the decomposition of a global model for many sensors. in fact  this appears to require the estimation of intrinsic parameters of the observed objects  which render successive observations conditionally independent. 
　　section 1 deals with the intractability of the summation in eq.  1   which includes an exponential number of terms. whereas huang and russell describe a heuristic scheme that seems to work well in practice  we apply the markov chain monte carlo  mcmc  method  a general-purpose approximation algorithm for probabilistic inference that can be shown to converge in polynomial time for the specific inference problem involved in object identification. furthermore  the algorithm can be adapted easily to incorporate online updating of the probability models required for computing  the overall scheme is in fact an online em algorithm with mcmc as an approximate e-step.1 
　　section 1 describes an application of the new approach to data extracted from a freeway simulation. we show that the estimation of intrinsic parameters  as described in section 1  successfully handles some multi-camera scenarios for which the appearance probability models of huang and russell are not applicable. we also show that the mcmc method allows accurate estimation of long-range origin/destination information even when the individual links in the sensor chain are highly unreliable. 
1 	scaling up to multiple sensors 
as mentioned in the section 1  calculation of assignment probabilities  is crucial for object identification. the calculation will be done using probability models that  in some way  capture the properties of the sensors 

figure 1: schematic diagram showing three consecutive camera sites  a  b  and c  and three vehicle trajectories. 
and the behavior of the objects being tracked. we hope to find models that allow decomposition of the global assignment probability in much the same way that local causal models allow decomposition of joint probabilities in bayesian networks. 
　we begin by describing the models used by huang and russell  explaining how they work only for two sensor locations. we then describe an alternative approach based on estimation of intrinsic properties of objects. 
1 	problems with appearance probabilities 
we begin with the case of two consecutive cameras a and b as considered by huang and russell. let the observations at each be  =  and let wab be an assignment pairing up objects observed at a with objects observed at  then 
		 1  
where a is  again  a normalizing constant. the term  is dropped by exchangeability- 
conditioning on the initial observations provides no information about matching with the subsequent objects. the approximate equality in the last line arises from the assumption of approximate independence among vehicle 
trajectories.1 
　in eq.  1   the terms  are called appearance probabilities since they describe  how an object can be expected to appear at subsequent observations given its current appearance   huang and russell  1 . the appearance probability models for freeway vehicles are composed of factors such as the arrival time at b given the arrival time at a  the measured colour at b given the measured colour at a  and so on. huang and russell show how these models can be estimated online from 

matched vehicles in a very straightforward way  avoiding the need for camera calibration. 
　let us now extend this approach to three cameras  using the scenario of  figure 1  as an example. an assignment  now specifies sequences of three observations that belong to a single object  and can be decomposed into two pairwise assignments  and 
as in eq.  1   we can apply bayes' rule and eliminate 
                       then we can apply the chain rule: is replaced in the last line b y b e c a u s e assignments of vehicles at c carry no information about a and b. now the last term on the rhs of eq.  1  can be written as the product of appearance probabilities between a and b  as in eq.  1 . however  the first term cannot be simplified to give the appearance probabilities between b and that is  
to see why  consider the extreme case in which cameras a and c can read the license plate of each vehicle  but camera b is broken. then the posterior distribution for assignments should have all its mass on assignments wabc that correctly match up vehicles at a and c; whereas both the pairwise models will be uninformative and hence will fail to propagate information from a to c. 
　　two possible fixes are 1  use multicarnera models  e.g.  and 1  estimate models for 
the first fix 
enlarges the model dimension and scales exponentially with the number of cameras. the second fix requires a quadratic number of models; moreover  it is unclear how to combine the predictions of these models. in summary  it seems that  despite their many advantages  appearance probability models apply only to the two-camera case. 
1 	spatial decomposition via intrinsic properties 
the example of the broken camera at b raises the following problem: we wish to propagate information between nonadjacent sensors  yet we do not want to have to employ nonlocal probability models  since such models result in a combinatorial explosion in the number of parameters to be estimated. the solution is  essentially  to let the objects themselves carry the necessary information. as with kalman filters and hidden markov models  hidden state variables can render current observations conditionally independent of previous observations. this provides a decomposition of the global model. 
   let  represent the hidden state of the objects observed at location /  and let range over possible values of  notice that  given the hidden state  the observations at a camera are independent of all the other observations. more formally  
robotics and perception 
figure 1: graphical model for object identification inference  showing two objects at two sensor locations. 
for all  and  now  we can introduce hidden state into 
eq.  1  by summing over  and  and simplifying using conditional independence  to yield a nested sum exactly as in the derivation of the forward equations for hmms. the only significant  difference is that the relationship between successive hidden state variables is only meaningful if we condition on the assignment so we know which vehicle is which: 
  1  
thus  the introduction of hidden state solves the problem of section 1. unlike eq.  1   eq.  1  calls only for two-camera models. moreover  these models need be 
estimated only for neighbouring camera pairs. 
　　as with appearance probabilities  we can decompose the expressions in eq.  1  into models for individual vehicles by making the appropriate independence assumptions  again  assuming the models depend on some global context variables . the models we obtain are the transition models such as and the sensor models such as as with hmms  these models can be learned online using em  dempster et al.  1   as we show below. the process is complicated by the fact that we must simultaneously estimate both the hidden state variables of the observed objects and the global assignment saying which object is which. 
　for traffic surveillance and many other applications  some aspects of the hidden state do not change over time. 

we call these intrinsic variables; for traffic surveillance  these include colour  length  width  and so on. intrinsic variables have no transition model but often have very noisy sensor models  specific to each sensor location. dynamic variables such as lane  speed  and arrival time must be tracked as the vehicle progresses through the freeway network. often they have relatively noiseless sensor models. thus  the hidden variables  can be divided into the intrinsic variables   and the dynamic variables  similarly  the observed variables o can be divided into observations of a  and d  observations of figure 1 illustrates all the independence assumptions we have made. it is relatively easy to augment this model to include dependencies between  for example  vehicle size and lane. 
1 approximate inference and online model updating 
as mentioned in section 1  the expressions for the probability of identity  eq. 1  and other quantities involve exponentially many terms. it can be shown that the inference problem is equivalent to computing the permanent of a matrix and hence is  it is possible to compute the most probable assignment for  vehicles in  time using the hungarian algorithm  cox and hin-
gorani  1 ; as huang and russell point out  however  this assignment may be of little interest if the individual matches therein are highly unreliable. they developed a heuristic  leave-one-out  algorithm with runtime that alleviates this problem and works well in practice. the algorithm identifies individual matches that exceed a reliability threshold and then treats those matches as if true. the matches are used to compute link travel time and to update the appearance probability models. a major drawback of this approach is that the if the fraction of reliably matched vehicles on each link is significantly below 1%  as often happens  the number of vehicles that can be tracked across a multi-link freeway network is vanishingly small  huang and russell  1 . 
　in this section  we describe an alternative approach to the inference problem based on markov chain monte carlo  mcmc . roughly speaking  mcmc approxi-
mates sums of probabilities such as eq.  1  by sampling a small number of high-probability terms; thus  for ob-
ject identification  it considers a small number of likely assignments and likely values for the hidden state variables. given any particular assignment and set of values for the hidden variables  estimating the transition and sensor models is trivial because the data is complete. this suggests an online em scheme for learning the models  as shown in figure 1. 
1 	introduction to mcmc inference 
m c m c inference is a general-purpose method for approximating  the expected value of the function  when its argument x is drawn from a probability distribution  typically  a posterior 
distribution over x given evidence 
for each newly detected vehicle 
augment markov chain state to include the vehicle 
repeat until models converge 
	e: run mcmc  sampling from 	and 
m: update models from sampled values 
figure 1: the overall inference scheme: online em using m c m c for the e-step and updating the models directly from the states sampled by mcmc. 
　　ideally  can be approximated simply by sampling from if each sample can be drawn in constant time  then from chernoft bounds we will have a polynomial-time approximation method. for general we know this cannot exist; sampling from an arbitrary distribution in constant  time is not always possible. 
　mcmc inference  metropolis et al.  1; gilks et al.  1  provides a general method to generate sam-
ples from by defining a markov chain whose states are the objects and whose stationary distribution is samples are produced by simulating the markov chain and selecting states from among those visited. 
in the metropolis-hastings method  transitions in the 
markov chain are constructed in two steps; 
  given the current state  a candidate next  state is sampled from the proposal distribution which may be  more or less  arbitrary. 
  the transition to  is not automatic  but  occurs with an acceptance probability defined by 

notice that to use this rule we need only be able to compute ratios which conveniently 
avoids the need to normalize 
provided  is defined in such a way that the chain is ergodie  this transition mechanism defines a markov chain whose stationary distribution is  and hence the average value of over the sampled states will converge to the desired value 
   the complexity of the original inference problem is reflected in the mixing rate of the markov chain  which determines the speed at  which the sample average converges. jerrum and sinclair  have shown that a markov chain defined on assignments  as described below  yields a fully randomized approximation scheme for estimating expectations over the probability distribution on assignments. this means that  if we want to approximate some such function to within ratio in a world with n objects  the chain will run in time polynomial in n and  to approximate  with probability the probability may be boosted to by running the chain times and taking the median value. of course  this assumes that proposals and acceptance probabilities can be computed in time constant  in n  which we show below. therefore  m c m c provides an efficient approximation scheme for object identification. 
	pasula  russell  ostland  and ritov 	1 

1 	application to traffic surveillance 
as we have seen in section 1  traffic surveillance  as defined here  involves taking the expected value of a quantity over all assignments  given the observations the calculation of this expected value  can be approximated by sampling from an ergodic markov chain with state space  and stationary distribution  the quantities to be estimated  which include ltts and origin destination counts  are very simple to derive for any given assignment  an assignment specifies trajectories through the sensor network with known times at each sensor location  enabling us to read off the desired quantity directly. 
　　the transitions in the markov chain may be set up in many ways  as long as ergodicity is ensured. in our approach  each transition is simply a swap between two assignment pairs across one pair of sensors. for example  in figure 1 observed object at sensor is originally matched with observed object at sensor while object at is matched with at a transition leads to being matched with and with the 

figure 1: figure demonstrating a single transition from one assignment with four trajectories to another. 
observation pairs to be switched are suggested in a very simple mariner. the algorithm cycles through all pairs of adjacent sensors  and all matched pairs currently at each sensor. for each such pair  a plausible second pair to swap with is then chosen uniformly at random. such a chain is provably ergodic. 
　once a pair is chosen  it is accepted or rejected based on the acceptance probability as eq.  1 . this involves the computation of the ratio 

because of the simple form of the swapping proposals  the  values are trivial to compute. we can also derive a general expression for which will permit us to 
robotics and perception 
simplify the ratio and make its calculation more efficient. let us demonstrate this using the transition in figure 1. 
generalizing eq.  1  for the observation sequence 

as was noted in section 1  the hidden variables can be divided into dynamic variables which change over time  and intrinsic variables which do not. the observed variables are analogously divided into d and  here  we assume that all the dynamics can be observed reliably. thus  assuming that and 
are mutually independent  	becomes 
	  and 	be-
comes now  we can make use of the assumption that the intrinsic variables remain constant across all sensors to discard the  terms. 
similarly  since we assume that dynamic variables are observed with perfect reliability  we can replace  with ds. the  terms can then be dropped. so  given our assumptions  eq.  1  simplifies as follows: 
finally  note that the only hidden variables we are summing over now are the intrinsics t. moreover  t is the same across all cameras by assumption  so we can combine all the summations. the  terms are independent of  and so can be moved outside the summation 
 1  
we can now use trajectory independence to factor this equation into separate terms  one for each trajectory. let the variable represent a trajectory within a specified and let represent the hidden intrinsic variables for the 

putative object that follows trajectory define and dt analogously. 
　eq.  1  may now be factored into a product of probabilities for individual trajectories: 
i 
where 
 we can now evaluate the ratio in a simplified form. as shown in figure 1  each markov chain transition affects only two trajectories  those that include  clearly  the probabilities along the remaining trajectories remain unchanged. thus   will share many common factors. all these factors  including the normalization constant a  cancel out. if we let xy signify the trajectory which the ratio simplifies to 
this ratio can be computed in time proportional to the longest network trajectory  which is constant for any given network. note that the bottleneck of the computation is the evaluation of the  integrals over products of intrinsic noise models. currently  all of these models are gaussian  and so the products are gaussian and integration is not. difficult. however  some care is required when introducing other models  to ensure that the. computation is still efficient. similar problems may be introduced if the assumption of noise-free dynamics is abandoned. without this assumption  we will need to integrate over the  as well as over the 
this can still be efficient  as long a.s the models are chosen well. 
1 	model updating 
the dynamic variable models at each link and the intrinsic variable models at each sensor are all learnt using em. a model update is performed whenever a new object  is observed at a sensor  and the mcmc process has been given time to converge. each em iteration proceeds as follows. 
　the e-step requires computing joint  expectations for the hidden variables  and  with mcmc  this is approximated by sampling w from the markov chain and extending each sample by calculating again  the fact that all our intrinsic models are gaussian yields a relatively straightforward calculation that requires time proportional to trajectory length. for example  consider a sequence of vehicle length measurements with a uniform prior and a sequence of gaussian noise models with a common variance. in this situation  the expected true vehicle length is just the mean of all the observations. if other  non-gaussian  models are used  the computation may become much more complicated-in which case  sampling over / as well as over  may be the best approach. 
　　the m step is exact. it uses the  and / values of each sample as if they were observed variables to perform conventional parameter learning. 
   running em to its convergence requires a markov chain run for each em iteration  and we run em whenever a new observation is made  resulting in many markov chain runs. in practice  the chain converges very quickly  as the models change only slowly with each incoming observation. we are currently investigating the use of online em  where only a single iteration is performed for each new available data point. nowlan  l1l  has proved that this approach should lead to locally maximum likelihood estimates in the limit  and our preliminary experimental results are encouraging. 
1 	experimental results 
we have performed two experiments comparing our approach to that of huang and russell. our data sets were created using a freeway simulator  forbes et al.  1  that allowed us to control road configurations  camera characteristics  and complex vehicle behaviour. each set included the observations generated by approximately one hundred cars  and the models used in the experiments had been learnt by our em-mcmc algorithm using data generated with the same parameter settings. 
1 	o v e r c o m i n g f a u l t y sensors 
in this experiment  the algorithms were applied to a three-camera network such as that in figure 1  set up in a manner analogous to the license-plate example of section 1. the only intrinsic attribute used was colour  and every car had a unique colour. the outer two cameras  a and c  measured colour exactly. gaussian noise was added to the measured colour at b. thus  the data set contained enough data for a complete matching between cameras a and o  but a pair wise appearancebased model could be expected to lose accuracy due to its hasty independence assumptions. 
　the results of the experiment  averaged over three runs  can be seen in figure 1. the y axis requires a little explanation. in the case of the huang and russell algorithm  which results in discrete matches  it shows simply the percentage of correct a to c matches: in the ease of the mcmc  it shows the average percentage of correct matches across all the samples drawn. the advantage of mcmc is clear: hidden feature estimation helps to maintain an almost constant degree of accuracy  whereas the pairwise approach is highly sensitive to sensor noise. 
1 	e s t i m a t i n g o r i g i n / d e s t i n a t i o n 	c o u n t s 
here  we compare the algorithms in the slightly more realistic setting shown in figure 1. the aim is to estimate the origin-destination counts between the two entry points and the three exit points. this requires tracking each object across the entire network. in this task  the 
	pasula  russell  ostland  and ritov 	1 


figure 1: the vehicle matching accuracy of two algorithms as a function of the variance in the gaussian noise at the central sensor. 
greater number of sensors should be a liability to the pairwise algorithm  as each pair of neighbouring sensors can make its own mistakes independently of the others. our algorithm  on the other hand  should benefit from the ability to observe each object several times to estimate intrinsics more reliably. 
　the colour variance was manipulated as before  but at all nine cameras simultaneously. figure 1 shows the percentage accuracy of the origin-destination counts as a function of the colour noise variance. as expected  the mcmc algorithm substantially outperforms the huangrussell algorithm  holding up well even for levels of noise that essentially wipe out the colour altogether. on the other hand  figure 1 shows that both methods are unable to find exact matches accurately for high levels of noise. the ability of the mcmc algorithm to recover reasonable counts despite the failure of individual matches suggests that its samples contain a reasonable amount of information about the ensemble behaviour of the vehicles. 
1 	summary and future work 
we have described an improved approach to object identification based on the estimation of the intrinsic properties of objects and the use of markov chain monte carlo to approximate the posterior probabilities efficiently. we have shown that this approach works on computer-generated data  and that its computational requirements are not prohibitive. we believe that this approach should be applicable in many data association applications. 
　we are currently working towards applying our approach in the real world. to this end  we have extended the approach to handle realistic problems such as missing observations. in the near future  we will receive data on vehicle observations made by cameras placed above the 1 freeway as part of the berkeley road watch project. 
robotics and perception 

figure 1: schematic diagram of simulated freeway network with nine cameras. 

figure 1: the origin-destination count accuracy of the two algorithms as a function of the colour noise variance. 

figure 1: the origin-destination vehicle matching accuracy of the two algorithms as a function of the colour noise variance. 

　solving the tracking and data association problems for individual vehicles is only the first part of the solution for large  complex applications such as traffic surveillance. the next step is to connect these low-level computations to high-level  aggregated models  which will permit prediction and control for large networks containing hundreds of thousands of vehicles. 
