 
we present an architecture for rule induction that emphasizes compact  reduced-complexity rules. a new heuristic technique for finding a covering rule set of sample data is described. this technique refines a set of production rules by iteratively replacing a component of a rule with its single best replacement. a method for rule induction has been developed that combines this covering and refinement scheme with other techniques known to help reduce the complexity of rule sets  such as weakest-link pruning  resampling  and the judicious use of linear discriminants. published results on several real-world datasets are reviewed where decision trees have performed relatively poorly. it is shown that far simpler decision rules can be found with predictive performance that exceeds those previously reported for various learning models  including neural nets and decision trees. 
1 	introduction 
although many different models of induction  such as decision trees  neural nets  and linear discriminants  have been used for classification  their common goal is accuracy of prediction. a central issue in the design of most classifiers is the tradeoff of goodness of fit versus model complexity. while an increasingly complex classifier can usually be made to cover training samples better  its accuracy of predictions for new cases may be inferior to a simpler  less complex classifier. for example  a fully expanded decision tree may cover training samples completely  but a smaller tree  with a larger apparent error on the training cases  may be more accurate in its predictions for new cases. 
　classifier complexity and prediction accuracy are highly related. learning from sample data can be described as the estimation of parameters for a model. finding the appropriate complexity fit of a model is a determination of the number of parameters that can be accurately estimated from the samples. from this characterization of learning it follows that given two classifiers that cover the sample data equally well  the simpler one is usually preferred because fewer parameters are estimated and therefore  predictions are likely to be more accurate. the theoretical foundation of this form of complexity fit analysis is presented in  wallace and freeman  1 . the related concept of minimum description length as a measure of the best complexity fit is found in  rissanen  1  rissanen  1 . 
　in practice  designers of learning systems have implicitly recognized these principles  and many techniques for model simplification can be characterized as finding relatively compact solutions with an appropriate complexity fit. examples from decision trees are quite numerous including the use of heuristic tree splitting functions to reduce 
1 	learning and knowledge acquisition 
expected tree size  breiman  friedman  olshen  and stone  
1  quinlan  1   tree pruning  breiman  friedman  olshen  and stone  1  quinlan  1a   and the onestandard-error heuristic for selecting among pruned trees  breiman  friedman  olshen  and stone  1 . for parametric statistical linear discriminants  heuristic variable selection methods have been developed to reduce the number of features in the discriminant  james  1 . for rule induction  simplification has been achieved by pruning the rules implicit in decision trees  quinlan  1a . for single hidden layer back-propagation neural nets  the number of hidden units can be used as measure of complexity fit  and the apparent and true error rates will follow the classical statistical pattern  weiss and kapouleas  1 . 
　thus  there are strong theoretical reasons for developing learning methods that cover samples in the most efficient and compact manner. in this paper  we discuss a new approach to generating reduced complexity solutions for rule induction models. in a related work  weiss  galen  and tadepalli  1   it was shown that short  disjunctive normal form expressions sometimes offer superior solutions. while that method empirically supports the reduced complexity approach  it is applicable only to problems with few attributes and classes. in this paper  we describe a new iterative rule method for inducing reduced complexity solutions in larger dimensions. we then describe a unified approach to finding the appropriate complexity fit among several competing rule sets. 
1 methods 
1 	the rule-based classification model 
we are given a set of sample cases  s  where each case is composed of observed features and the correct classification. the problem is to find the best rule set rsbest where the error rate on new cases  errtrue rsbest   is minimum. 
we examine solutions posed in disjunctive normal form 
 dnf   where each class is classified by a set of disjunctive productions  terms . each term is a conjunction of tests  pi  where pi is a proposition formed by evaluating the truth of a binary-valued feature or by thresholding any of the values a numerical feature assumes in the samples. one such model is the decision tree  where all the implicit productions are mutually exclusive. however  a general dnf model does not require mutual exclusivity of rules. with productions that are not mutually exclusive  rules for two classes can potentially be satisfied simultaneously. such conflicts can be resolved by assigning a class  or rule  priority ordering  with the last class considered a default class. for example  the rule set in figure 1 is a solution induced for the heart disease data discussed in section 1. 
　

figure 1: example rule set 
　while tree induction remains the most widely applied rule-based learning system  other learning techniques have been developed with some success for non-mutually exclusive dnf rule induction. these systems include c1 
  quinlan  1b  quinlan  1a  and the cn1  clark and niblett  1  and greedy1  pagallo and haussler  1  variants of the aq family of rule induction systems  michalski  mozetic  hong  and lavrac  1 . 
　although the aforementioned rule induction methods may appear quite different  only a few key variations emerge. these rule induction methods can be characterized in terms of their covering schemes and their rule refinement techniques. the covering rule-set is induced by  a  decision tree; or  b  a single best rule is found  the cases covered by that rule are removed from the training sample  and the next rule is induced until no cases remain. the set of rules can be refined by pruning or by applying some statistical test. 
1 	a swap-1 covering procedure 
both the tree covering and the single-best-rule covering methods that have been mentioned  look ahead one attribute and try to specialize the tree or rule. to this end  a heuristic mathematical function is used  such as an entropy or gini function  breiman  friedman  olshen  and stone  1 . for the tree covering solutions  these heuristics tend to work well on many problems  and the combinatorics of finding an optimal solution make alternative search procedures impractical. 
　like the tree induction methods  current single-best-rule methods expand only a single rule at a time and add tests one by one until a rule has 1% predictive value  i.e. makes no errors on the training cases.1 although any single rule is relatively short  these single-best-rule procedures never look back  only look ahead for a single test. 
　in this section  we present a procedure  called swap-1 that constantly looks back to see whether any improvement can be made before adding a new test. the following steps are taken to form the single best rule:  a  make the single best swap for any rule component including deleting the component;  b  if no swap is found  add the single best component to the rule. figure 1 formally describes the swap-1 procedure. as in  weiss  galen  and tadepalli  1    best  is evaluated as predictive value  i.e. percentage correct decisions by the rule. for equal predictive values  maximum case coverage is a secondary criterion. swapping and component addition terminate when 1% predictive value is reached. 
　the process of generating the single best rule can be seen in figure 1  where an example rule is generated in 1 steps. swap-1 tries to maximize predictive value. the initial rule is randomly assigned p1  which gets swapped out in favor of the single best test  p1. then in step 1  pi is the single best component that can be add to the rule. however  in step 1  p1 is swapped out for p1  which is found by refining previously selected rule components. in the final step  we see that p1  which was swapped out in the first step  gets 
input: s a set of training cases initialize  = empty set  
repeat 
create a rule b with a randomly chosen attribute as its lhs while  b is not 1% predictive  do 
make the single best swap for any component of b  
including deleting the component  using cases in ck 
if no swap is found  add the single best component to b 
endwhile 
pk := rule b that is now 1% predictive 
	  	:= cases in ck that satisfy the single-best-rule pk 

until  ck is empty  find rule r in rk that can be deleted without affecting 
performance on cases in s 
while  r can be found  

endwhile output rk and halt 
figure 1: the swap-1 procedure 
step predictive value 	rule 	1 1% p1 1 1% p1 1 1% p1&pl 1 1% p1&pl 1 1% p1 & pi & p1 1 1% p1 & p1 & p1 & p1 1 1% p1 & pi & p1 & p1 figure 1: example of swapping rule components 
swapped in again. thus  it can be seen that if a test is swapped out  it does not necessarily stay out  but can be added back later on if doing so improves the predictive accuracy of the current rule. the completed rule is selected as the single best rule  and the method proceeds as usual with the removal of the covered cases  and the reapplication of the single-best-rule construction procedure to the remaining cases. 
　as a pure covering procedure  swap-1 has been compared to other rule-based methods for covering randomly generated expressions from uniformly distributed attributes  indurkhya and weiss  1  indurkhya and weiss  1 . its performance exceeds that of other rule induction methods and matches the performance of fringe  pagallo  1   an iterative tree induction technique. when applied to real-world data with numerical attributes  the swap-1 procedure can lead to fragmentation of the data by covering with too many short rules. there may be longer rules that are still 1% predictive  but cover more cases. therefore  once a 1% predictive rule  ri  is obtained  a longer rule r: is induced by swapping on ri for minimum errors  not predictive value  to obtain rk and then 
weiss and indurkhya 
　
re-initializing r1 in the swap-1 procedure with rk. ri is then compared for coverage with the longer rule rj. 
　finding the optimal combination attributes and values for even a single fixed-size rule is a complex task. however  there are other optimization problems  such as the traveling salesman problem  lin and kernighan  1   where local swapping finds excellent solutions. 
1 	finding the right complexity rule set 
there have been some efforts to find the best rule set by devising measures of minimum description length which can give reasonable answers because complexity and predictive performance are highly related  quinlan and rivest  1 . however  the central objective remains to find a rule set that minimizes the true error rate  and this can be more directly measured by determining the true error rates of varying complexity rule sets. 
given a set of samples s  the objective is to select rule set 
rsbest from {rs1 ...rsi ...rsn}  a collection of varying complexity rule sets  such that rsbest will make the fewest errors on new cases t. in practice  the optimal solution can usually not be found because of incomplete samples and limitations on search time. typically  there are insufficient cases to both train and accurately estimate the error rate of a rule set  err rsi . moreover  it is not possible to search over all possible rule sets of complexity cx rsi   where cx is some appropriate complexity fit measure  such as the number of components in the rule set. 
　several thousand independent test cases are sufficient to give highly accurate estimates of the error rate of a 
　classifier  highleyman  1 . when fewer cases are available  resampling gives the most accurate estimates of the error rate. cross-validation  stone  1  is generally the procedure of choice  breiman  friedman  olshen  and stone  1   and 1-fold cross-validation1 is usually quite accurate when the cases number in the hundreds. because cross-validation techniques  such as 1-fold or leaving-oneout  average the estimates for many classifiers that are trained on approximately the same number of cases as the full sample  learning techniques have been developed that can train on all sample cases. 
　if the set {rs1 ...rsi ...rsn} is ordered by some complexity measure cx rsi   then the best one is selected by min  err rsi  .1 thus to solve this problem in practice  a method must induce and order {rsi} by cx rsi  and estimate each errcrsj . such methods have been developed for decision trees. studies have shown that the minimization of a single complexity parameter  cx  in addition to the error rate estimator  err  adds a little bias to the estimates when used with resampling  breiman  friedman  olshen  and stone  1 . 
　a variation of weakest-link pruning  also known as costcomplexity pruning  breiman  friedman  olshen  and stone  1   can be used to prune a rule set and form {rsj}. let the rule set rsj be the covering rule set. each subsequent rsi+1 can be found by pruning rsi at it weakest link. a rule set's weakest link can be defined as in equation 1  
1
   the avenge result! of 1 runs using 1% training and 1% letting cases  with 1 mutually exclusive test partitions. 
1
   or as in  breiman  friedman  olshen  and stone  1   the min{cx rs  } that is close  within one standard error  to min err rs. . 
learning and knowledge acquisition 
where errors rsi  is the number of errors rsi makes on the training cases and size rsi  is the number of components in rsi the weakest link is the point at which the fewest new errors per deleted components are introduced. as in  quinlan  1a   a rule set can be be pruned by deleting single rules or single components  where the weakest link is measured by wl i . repeated pruning of the least significant single component or rule to rsi sequentially forms {rsk} and the global minimum is wl i . the rule set at that point becomes the next rsi+1. the process is repeated until the final rsn is generated  where rsn is single component rule of selecting the largest class. 
		 1  
　the application of weakest-link pruning results in an ordered series of decreasing complexity rule sets  {rs i . the complexity of rsi can be measured in terms of wli or alternatively size rsi . with a large set of independent test cases and weakest-link pruning based on apparent error rate estimates  the true error rate of each rsi can be estimated by errtest rsi   the error rate of rsi on the test cases. with smaller samples  where thousands of test cases are not available  resampling is preferable and more accurate. as in 
 bneiman  	friedman  	oishen  	and 	stone  	1   
{rs1 ...rsi ...rsn} are determined by weakest-link pruning on the complete training set. an n-fold cross-validation  typically 1-fold  is performed. in each fold k  an auxiliary rule set is induced using the training set of that fold. a new rsk of complexity  approximately  equal to size rs|  is found by weakest link pruning. test error-rates arc obtained using the test cases corresponding to that fold. the average of the error rates over all the folds for each rule-set of size rsi   errcv rs1   is the cross-validation estimate of the true error rate of rsi. 
　consistent with the minimum length description approach  each rule covers the original cases with only the weakest rules and components removed. as given  pruning a rule set is less stable and accurate than tree pruning because coverage of the pruned set is highly variable. while pruning a subtree retains full coverage of the data set  pruning rules can leave gaps in the coverage. moreover  for rsi of complexity size rsi   there may be a better rsi of size rsi . unlike the decision tree induction where the nodes are fixed  rsi can be refined by the swapping single components to minimize the apparent training error 
err  rsi . 	the process of refining any rule set rsi into 
rsi 	can be described as modifying rsi such that 
 the 
rules are iteratively checked for the best component deletion  rule deletion  or component swap. here the definition of  best  is minimum errors. thus a rule set can be refined so that the refined rule set is smaller or equal in size to the original rule set and makes fewer or an equal number of errors than the original. 
　the net result of this process is an error rate estimate for varying complexity rule sets. a typical result is illustrated in figure 1  where the results of resampling for the rheumatic disease application in section 1 are summarized. for each rule set rsi  figure 1 lists the number of rules  the number 

figure 1: example of summary table 

of rule components  the apparent error rate on the training cases  the test error rate  by cross-validation   the standard error of the test error  the average number of components over all cross-validated test sets  and the complexity measured by weakest-link pruning. plotting the error rates for increasingly complex rsi as in figure 1  illustrates the classical pattern of behavior for the apparent error rate on training cases versus the estimated true error on test cases. as model complexity increases  the apparent error rate decreases  but the true error rate flattens out and eventually increases. 
1 	mixed models for reduced complexity 
while complexity within a given model can be measured in some common units  complexity measures for different models are not readily comparable. for example  we have adopted the number of rules components as the unit of complexity measure  where components are simple logical propositions. while a tree can be directly decomposed into a set of rules  where a path to a terminal node is a rule  these rules would share common components  and a complexity unit of tree nodes seems more appropriate. complexity units measured in terms of weights are appropriate for linear 
discriminants and neural nets. 
　our approach has been to minimize the complexity of rule sets. although difficult to express in terms of single units  the true complexity of a solution could be further reduced by allowing for mixed models. for example  it is well-known that in  two dimensional  geometric terms  rulebased models find solutions that are parallel to the axes and cannot efficiently fit even simple linear functions between features such as x y. this has led to a number of hybrid methods that incrementally embed alternative models such as piecewise linear discriminants  breiman  friedman  olshen  and stone  1  or perceptrons  utgoff  1  within decision trees. 
　in contrast to those nonparametric  incremental methods  we have used the standard parametric linear discriminant1 where the solution is posed as in equation 1  and for each class i  fi c   a linear function of the set of attributes e  is derived. an unknown pattern is classified by applying the functions and choosing the class whose linear magnitude is greatest. in addition  heuristic stepwise feature selection can be used to find a reduced complexity linear solution by reducing the original set of features {ek} to {e j } where j k  i.e a subset of the given features  reducing tne number of weights that are estimated  james  1 . 
		 1  
　the parametric stepwise linear discriminant is highly developed and has a strong tendency to produce comparable results on both the training and test cases. in our design  the discriminant function is completely derived prior to rule induction  and the results  on the training cases  are used to create artificial features. one binary higher-order feature per class is specified  where each feature is simply whether or not that class is selected during training by the linear discriminant. to the rule induction system  each of these higher-order features is no different than any of the original features. unlike previous approaches where the numerical result of applying a single linear function is used  here the encoding is simplified to a binary feature that preserves the classification result of the discriminant. this results in an interesting interplay between the linear discriminant and an induced rule set. for example in figure 1  class 1 is chosen when both ld1 and rbps 1 are true  where ld1 is the selection of class 1 by the linear discriminant  and rbps is a test result. 

figure 1: heart disease example of mixed model rules 
1 results 
　to evaluate the efficacy of reduced complexity learning  four applications were considered. these applications are real-world problems with significant published results. following the original publications  studies were performed and published by other researchers comparing different learning models  usually back-propagation neural nets compared with decision trees. of particular note  decision trees performed relatively poorly in these published comparisons. figure 1 summarizes the characteristics of the data sets for each application  and the train-and-test variations  such as leaving-one-out  lv-1   used in the original studies to estimate the true error rate. 
　figure 1 summarizes the reported results of previous studies and our new results for a reduced complexity approach. cx  the complexity measure  is measured in units of weights for neural nets or linear discriminants  propositions  rule components  for rules  and nodes for trees.1 we review the results for each application. 
text-to-speech recognition 
　the nettalk lext-to-speech application was originally reported in  sejnowski and rosenberg  1  and a rigorous comparative study of id1 decision trees and back-
propagation neural nets was reported in  diettrich  hild  and bakri  1 . while the larger goal is the recognition of spoken words  in the previous analyses  the problem is formulated as the recognition of  letters  or classes of phoneme-stress pairs. although the problem fits the traditional classification mold of mutually exclusive classes  previous studies have used a relatively complex nonmutually exclusive encoding for the classes. this was done based on knowledge of the domain and its suitability for a neural net representation.1 in our analysis  we return to the traditional classification approach. there are 1 phoneme classes and 1 stress classes but only 1 phoneme-stress pairs with more than 1 case in the training set. despite over 1 training cases  relatively few cases appear for many classes. the rule sets {rsi} are induced from the training set  and given the large number of test cases  the appropriate complexity fit  cx rsi   can be determined by the test cases.1 the classes were ordered by decreasing predictive values of their rule sets.1 
　the previously reported best result was for a neural net with an error rate of .1 and over 1 weights. a reduced complexity rule-based solution was found with an error rate of .1 and 1 rule components.1 the previously reported error rate for a tree-based solution was a relatively weak .1. however  with the 1-class representation  the error rate of an induced tree is the same as the swap-1 rule-based solution although with far greater complexity. 
heart disease classification 
an application to heart disease was originally reported in 
  detrano et al.  1  and a comparative study of  id1  decision trees and  back-propagation  neural nets was reported in  shavlik mooney and towell  1 . in the comparative study  the average test results for ten 1% training and 1% testing sets were used. there  the best result was for a neural net with an error rate of .1 and 1 weights. the result of a 1-fold  or a 1-fold  crossvalidation for swap-1 yields a simple 1-rule  1-component solution with an estimated error rate of .1  as compared with the reported .1 for the decision tree. unlike the results in  shavlik mooney and towell  1   results were improved by pruning the rule sets and not using binary encodings of numerical variables. when the linear discriminant is added with the original features  the rule set rsbest is simply the linear discriminant which has an estimated error rate of .1 with 1 weights.1 
dna pattern analysis 
　in  towell  shavlik  and noordeweir  1  several learning methods are compared on a dna pattern recognition task. in addition  a human expert's  theory   described in terms of a grammar  is reformulated as a neural network and refined - yielding somewhat better results than pure empirical learning systems.1 error rates are estimated by leaving-one-out  and the best reported pure learning result is for a neural net with an error rate of .1 and over 1 weights. the tree solution does relatively poorly with an error rate of .1  and a reduced complexity 1-rule solution with 1 components is better with an error rate of .1. still the rule-based solution is not fully competitive. including the results of the linear discriminant in the feature set  results in an induced rule set that is pruned back to the linear discriminant alone. this discriminant function has only 1 weights and an error rate estimate of .1. the reduced complexity of this linear discriminant is due to the success of stepwise feature selection.1 with 1 features and only 1 cases  there are far too many weights to be estimated if all the features are used  and a reduced complexity discriminant is desirable. 
rheumatic disease diagnosis 
　a very preliminary version of a rule-based expert system for diagnosing rheumatic diseases was described and evaluated in  lindberg.et al.  1   the knowledge base and data were later used in heuristic refinement systems  ginsberg  weiss  and politakis  1  that could modify a theory  i.e. an expert-specified rule base  to improve its performance. these systems were restricted to minor refinements that would assure the preservation of the expert's knowledge base close to its original form. a more radical  theory revision  approach uses the theory  i.e the expert knowledge base  as a starting point and allows massive changes to the rule set. such an approach was taken in  ginsberg  1  where these same data were used to evaluate a theory revision method  rtls . there  it was reported that that the leaving-one-out estimate of the error rate for the five class problem is zero. because this new rule set is radically different from the expert's knowledge base  it is worthwhile considering how well a pure empirical learning system might do. 

figure 1: comparison of best reported  decision tree and swap-1 results 　from a purely empirical learning perspective  the previous  rtls  solution is far too complex with 1 rules to provide a good complexity fit to the data.1 the true error rate of rtls is also higher than cited because error rates were measured in a non-traditional manner. some ties were scored as correct  and answers were marked correct when they agreed with the output of a second refinement program  seek1. however  the output of seek1 had a leaving-one-out error rate estimate of .1  so the true error rate of rtls is  .1. 
　most empirical learning systems cannot handle this dataset because almost half the feature values are missing. using cart to induce a decision tree  with its surrogate strategy for handling missing values  yields a high error rate of .1. yet with swap-1  a 1-rule  1-component solution can be found that has a leaving-one out estimate of .1. even with the missing values  the covering of the 1 cases is quite compact. with so many missing values  the advantage of the rule-based solution over the decision tree can be traced to the non-mutual exclusivity of the rules.1 
1 discussion 
we have presented better solutions than those previously reported for four significant classification applications. these solutions are superior in terms of estimated error rates  but equally important are far less complex than the previously reported solutions. we were particularly interested in these applications because decision trees performed relatively poorly compared to competitive learning models. the reduced complexity rule induction approach was readily able to to exceed the performance of the decision trees. however  in two instances  the pure rulebased solution was still not competitive with alternative methods. by combining the results of the stepwise parametric linear discriminant with the original feature set  we were able to exceed previous results  at the expense of a mixed model solution. interestingly  in two applications the method indicated to use solely the linear discriminant in other applications  the rules can exhibit much interplay between the discriminant functions and the original features. we showed how a traditional classifier model can be incorporated into a rule-based solution  and a method was presented that arbitrates between the two models. there are many applications were pure rule-based solutions are a priori preferred by humans  and some applications where linear classifiers do not yield good results  weiss and kapouleas  1 . 
　the central theme of the learning methods we have described is the reduction of rule complexity. we have presented an architecture for minimizing complexity of rules by reducing the number of rules and components and maximizing rule coverage of cases by iteratively swapping rule components. the goal of minimizing rule complexity is entirely consistent with the goal of maximizing predictive accuracy. to accomplish this task we have also borrowed many techniques  including weakest link pruning and resampling  that have proven effective in other applied learning systems. 
　while our approach is directed towards a rule-based solution  the notion of minimizing complexity is not restricted to any particular model. neural net solutions that are compact and minimize the number of weights are also likely to increase predictive accuracy. for the applications we cited  similar simplifications to other models may yield improved results. for example  the previously used nettalk neural net model might benefit from a reduced number of weights. the limiting factor is not only the specific model  but also the effectiveness of the learning technique and computational time. while the swap-1 approach may appear computationally overwhelming  it is highly bounded for instance  when the number of errors for a candidate swap exceeds those for the current best swap  
weiss and indurkhya 
that candidate can be immediately dropped. for all applications except nettalk  a 1-fold cross-validation required only several minutes of sparcstation-1 time. the nettalk application required on the order of 1 days. 
still  with steadily increasing available compuiationai power  we can look forward to more computationally intensive attempts to extract the maximum amount of information from sample data. 
acknowledgments 
this research was supported in part by nih grant p1-rr1. we thank kevin kern for his programming support and our colleagues for supplying the data. 
