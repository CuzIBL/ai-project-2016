 
most implementations of and-parallelism tackle the shared variable problem by running literals in parallel only if they have no variables in common and thus are independent from each other. but even in its independent form  and-parallel clause execution remains an intricate matter with numerous problems to be solved. first to mention  a carefully worked out synchronization mechanism is required to control process activities like dependency checking  literal ordering  message passing and intelligent backtracking. due to the complex nature of these problems  implementation of and-parallelism often results in a rather opaque interplay of processes. therefore  based on a new kind of process  this paper presents a sequential - prolog like - view of and-parallelism for making it more understandable on one hand and easier to implement on the other. processes of the new type will be called partial and-processes. 
1 	introduction 
in process based systems and-parallelism is usually realized by introducing producer/consumer relationships among clause literals and their corresponding or-processes. therewith independent literals act as parallel producers of variable bindings  which will be consumed by dependent literals in subsequent steps. dividing clause literals into producers and consumers shows the advantage of avoiding binding conflicts since shared variables can have only one producer at a time. realizing the producer/consumer approach to and-parallelism  however  requires a sophisticated synchronization scheme to guarantee correct process interaction. especially in the backward phase of clause evaluation  chang  1  conery  1   a lot of activities have to be controlled  ranging over message passing  reordering literals  cancellation and creation of processes. moreover  to complicate things  the whole synchronization job is settled on a single and-process in most systems. but loading a process with so much work may seriously affect its message passing capacity and thus turn it into a communication bottleneck. with all these drawbacks in mind one might question  whether producer/consumer parallelism can actually be realized in a better arranged manner. obviuosly it would help a system designer if he could view parallel clause execution in terms of a well known and easy to understand sequential system. offering such a view through partial and-processes  a new kind of process  will be the subject of this paper. 
   the paper starts by introducing some notational framework. after that  the principles of process based program execution will be illustrated by the hand of and/or-trees. two fundamental search strategies for such trees will be discussed then  before the special strategy giving and-parallelism a sequential touch is explained in section four. finally we pick up the task of producing output substitutions through intelligent backtracking in order to demonstrate how partial and-processes solve a typical implementation problem of and-parallelism. 
1 	notation 
for the rest of the paper the reader is assumed to be familiar with the basic terminology of logic programming as introduced by lloyd   for example. some supplementary notation used subsequently is given below. 
   variables will be named by letters x  y  z  u  and v. letters a  b  c  and d denote constants  and f  g  h function symbols. an expression can be a term or literal. if e represents an expression  var e  is the set of all variables occuring in e. two expressions e1 and e1 are called dependent if var e1  n var e1  = 1. substitutions are named by greek letters like 1  a  and y; a substitution 1= {x1/t1 ... xn/tn}  n o  binds variable x to 
	schend 	1 


1 	parallel and distributed processing 


	schend 	1 


1 	parallel and distributed processing 

at this moment 	pand-son of p by sending all its failure literals. 


figure 1 backtracking situation between pand-processes 
　process ps x  reports its failure to  which immediately reacts by requesting a new input substitution from its pand-father  based on the nested loop model  the father generates the next output tuple  1  and sends the corresponding substitution  to the requesting pand-son. however  
will obviously lead to a failure  too  because the value of variable x remaines unchanged by the new substitution. so  our pand-process generated just another failure tuple. 
definition 1 
let p 	be a pand-process with output tuple t and 
corresponding output substitution  tuple t is called a failure tuple if  is unsolvable for some active literal of the pand-son of p. else it will be called a success tuple. 
   to avoid repeated generation of failure tuples - like  1  and  1  in the example above - a pand-process must incorporate some kind of intelligent backtracking  conery  1  woo and choe  1  or equivalently  use some form of intelligent tuple generation. this should  for instance  allow process  of figure 1 to generate tuple  1  directly without regarding  1 . tuple  1  and its 
corresponding output substitution  =  bind x to a different value  with which clause 
execution succeeds finally. 
   the question now is  how tuple generation can be made intelligent. to achieve this aim  a pand-process p obviously needs some kind of information about what was wrong with its actual output substitution. within the parallel execution scheme presented here  this information will be supplied by the definition 1 
let p be a pand-process named p r|n . an active literal let is called a failure literal if its corresponding or-process fails to compute any answer substitution. the set of all failure literals of p is denoted by fl t . for an and-process p  we set fl p = t by definition. 
   our example process  figure 1  for instance  would report s x  as a failure literal to its pand-father:  now the father process can conclude that it must produce a new binding for variable x  for changing the value of y would definitely not repair the failure and thus lead to another backtracking. as a new value for x can only be achieved by changing the first component of the actual tuple  1   we automatically know  1  to be another failure tuple. this is confirmed by looking at its corresponding output substitution   which changes the value of y  but not that of x. 
　　in order to determine all its failure literals a pand-process proceeds as follows: after descendant or-processes have been created for every active literal  it enters a kind of test state. there the process waits until all or-sons have acknowledged by sending a success or failure message. the literal of a failing or-process will be recorded as a failure literal thereby. note  a pand-process will wait forever if one of its or-sons is caught in an infinite loop. by the way  waiting for all or-processes to acknowledge allows us to handle so called multiple failures  conery  1  woo and choe  1  in a rather simple manner. further  collecting all failure literals before backtracking results in a more intelligent version since information about failures gets more comprehensive. this information will be used to repair the failure by generating a new output tuple  the corresponding substitution of which binds at least one variable in each failure literal to another value. such tuples and substitutions will be called failure relevant from now on. 
   in terms of our nested loop model the whole problem of generating tuples intelligently may now be stated as: what loop variable  or equivalently  what tuple component should be changed in order to make its corresponding output substitution relevant to the failure   to answer this question we first of all introduce a successor relation on and- and pand-processes. definition of the relation is based on the process tree of figure 

	schend 	1 

let 
s x y } 
                                                                                              = max{1} = 1. using the 1 parallel and distributed processing 

　　over against this  if a pand-proccss should fail to find an increasable tuple component  no more output tuples can be generated and tuple generation is finfished. a pand-process  which is unable to produce new output substitutions  initiates backtracking to its own predecessor. in this manner backtracking eventually arrives at the and-process above  telling it that clause execution is done. 
　　at the end of this section some aspects of our backtracking scheme should be discussed: 
　　the nested loop model for tuple generation shows the advantage of being easy to implement. but its simplicity must be paid for in some sense as tuple generation may become incomplete if or-processes compute infinite answer sets. one might expect this a serious drawback since many tuples will never be considered. yet  generating all of infinitely many tuples is impossible  anyway. the only disadvantage is that successful output substitutions are probably not produced if a pand-process is stuck in an infinite answer set. fortunately loops of this kind may be stopped through intelligent backtracking. 
　　requesting a new output/input substitution through backtracking is generally not restricted to pand-processes  but also used by an and-process to obtain all solutions for its clause. this is simply done by backtracking to its own predecessor immediately after having received an input substitution from it. the head literal of the clause will be reported as the failure literal thereby. 
1 	conclusions 
implementing and-parallelism in the producer/consumer style requires a sophisticated synchronization scheme for controlling or-processes  which work on body literals independently. especially handling failures through intelligent backtracking makes clause execution a rather complicated problem  the solution of which often results in a confusing interplay of processes in practice. to make and-parallel clause execution more perspicuous and therefore easier to implement  this paper conveyed a sequential view of and-parallelism. based on a heterogeneous execution strategy  which is sequential and parallel in nature  independent literals are grouped together and treated as a single module. literal groups are solved sequentially by partial and-processes  which in their turn start descendant or-processes for executing group literals in parallel. the main advantage resulting from the sequential part of the execution strategy is that clause execution may be tackled in a simple prolog like manner. as an example  the backtracking scheme presented here bears some similarity to the naive right to left method of prolog  except that it is made intelligent on the basis of producer/consumer relations. multiple failures of or-processes can be handled efficiently by partial and-processes  all the more since information about failing literals is gathered to achieve a more intelligent backtracking version. apart from this  the load on and-processes will be eased significantly  allowing them to react to incoming messages promptly. 
references 
 chang  1  j.h. chang. high performance of prolog programs based on a static data dependency analysis. ph.d. thesis  1. 
 conery  1  john s. conery. parallel execution of logic programs. kluwer academic publishers  1. 
 degroot  1  doug degroot. restricted and-parallelism. 
in proceedings of the international conference on fith generation computer systems  tokyo  1  pp. 1. 
ikowalski  1  robert a. kowalski. logic for problem solving  	artificial 	intelligence 	series  	vol.1  elsevier-north holland  new york  1.  lloyd  	1  	j.w. 	lloyd. 	foundations 	of 	logic 
programming. springer-verlag  berlin  1. 
 woo and choe  1  n.s. woo and k.-m. choe. selecting the backtrack literal in the and/or-process model. in ieee 1 symposium on logic programming  salt lake city  utah  pp. 1. 
	schend 	1 

unsupervised learning by backward inhibition 
tomas hrycej 
pcs computer systeme gmbh 
pfaelzer-wald-str. 1  d-1oo1   muenchen 1  west germany 

abstract 
backward inhibition in a two-layer connectionist network can be used as an alternative to  or an enhancement of  the competitive model for unsupervised learning. two feature discovery algorithms based on backward inhibition are presented. it is shown that they are superior to the competitive feature discovery algorithm in feature independence and controllable grain. moreover  the representation in the feature layer is distributed  and a certain 
 classification hierarchy  is defined by the features discovered. 
1 	introduction 
connectionist  or neural network  models  i.e.  models consisting of networks of simple processing nodes  have been shown to possess cognitive capabilities  in particular  the capabilities of shape recognition  hinton and lang  1  fukushima  1  or von der malsburg and bienenstock  1   development of concepts  dalenoort  1   learning 
 grossberg  1  1  ackley et al.. 1  rumelhart et al.  1  le cun  1   storing of patterns  hopfield  1   generalization  anderson  1   etc.. 
　connectionist models provide some important advantages over symbolic models: 
  automatic development of representation  
  correction of noisy input  
  graceful degradation if some cells are erroneous  
  generalization and 
  inherent parallelism. 
　connectionist models are typically less transparent than symbolic ones. so the preferable  or even the only possible  form of knowledge input is learning by examples. 
　the most straightforward form of learning is supervised learning  i.e.  associating inputs with some known outputs. the best-known algorithms are backpropagation algorithm of rumelhart et al.  and boltzman machine learning algorithm of ackley et 
1 	parallel and distributed processing 
al. .  analogical mathematical concepts can be found in discriminant and regression analysis  kohonen  1 . for symbolic supervised learning see  e.g.  winston   quinlan  or kodratoff and ganascia .  however  in some situations  the data available are not sufficient for supervised learning: 
1   correct  outputs  or responses  are not always known. in some cases  merely an overall criterion of response quality is given  e.g.  the survival criterion for living organisms  or reaching a positive biological or emotional state for humans . this has lead to a modification of supervised learning - instead of a set of correct responses  only a reinforcement criterion is given. 
1  even with such a  weaker  reinforcement  direct learning of correct responses may fail because of a high complexity of input. this can be illustrated on the backpropagation model  rumehart et al.  1 . it has been shown that only a limited class of input-output encodings can be materialized in a two-layer connectionist system. so more complex  multiple-layer systems and corresponding learning schemes have been developed. however  multiple-layer systems scale poorly - systems with five or more layers seem to be computationally intractable. so there are obvious limitations of the supervised learning: more complex inputs require more layers  but too many layers are  in turn  intractable. 
a proposal for solution of the latter problem 
 ballard  1  is to partition the network in some modular way. a part of the network would find  e.g.  by  autoassociation   a form of unsupervised learning  in ballard's model  a distributed and highly invariant representation of input and supervised learning would be then applied to this discovered representation  instead of the original input. 
　more general learning models capable of performing unsupervised classification by discovering characteristic features or regularities of input data have been presented by rumelhart and zipser  and grossberg . they perform essentially some clustering algorithm in a network form  for a symbolic analogy  see  e.g.  conceptual clustering of stepp and michalski  or the  hybrid  model of 

lebowitz  . however  those models suffer from several deficiencies. as argued in section 1  the model of rumelhart and zipser suffers from 1  random and uncontrollable grain of feature discovery  1  constructing redundant and local feature representation  and 1  difficulties in building feature hierarchies  for more details  see section 1 . 
   this paper presents a novel type of feature discovery model  a backward inhibition model  that does not suffer from the above deficiencies. mathematically  it is related rather to variance analysis than to cluster analysis. 
1 	existing models 
in recent past  several models for feature discovery have been presented. most of them can be classified as  competitive models . a well-known representant of this class is critically examined in section 1. 
   to introduce some mathematical concepts necessary for explanation of backward inhibition concept  a rudimentary one-feature model is presented in section 1. 
1 	simple correlation model 
the basis for all models treated in this paper is a simple noncompetitive two-layer correlation model  further referred to as  basic adaptive model . all nodes of the input layer are connected to a single node of the feature layer  see figure 1 . a feature y is represented by the vector w of weights assigned to connections to a feature node. activation level of a y is given by w ' x   x being input vector  activation rule . this model learns by a widely used correlation learning rule  whose continuous form is: dw = a*y*x*dt  with a     l a constant. 

   this rule is a special case of adaptation law studied by kohonen  1  page 1  case 1 . as shown by kohonen  the weight vector converges to the dominant eigenvector  i.e.  the eigenvector corresponding to the largest  dominant  eigenvalue  of the input correlation matrix e x*x . 
   since this is equally true for all weight vectors  all feature units discover the same feature  the dominant eigenvector. so this model is no serious candidate for feature discovery. however  it can be modified in several ways to perform more satisfactorily. 
1 	competitive correlation model 
one of the possible modifications is the competitive model proposed by rumelhart and zipser   for a more general model  see grossberg  . the feature layer of this model consists of multiple feature nodes. instead of activating each feature proportionally to the weighted input into the feature cell  only the feature unit with maximal activation  i.e.  with maximal similarity  = inner vector product  of its weight vector with the input vector  fires  and only its own weights are modified. 
   an obvious interpretation of the learning process in this model is that  for a given input vector  the feature with the maximal value of similarity measure  inner vector product  between the input and its currents weights learns by moving its weights toward the input vector. so competition causes each input vector to  attract  the  nearest  feature  which results in partitioning input into exclusive  clusters . since  because of competition between features  very different inputs will probably fire different features while similar ones will fire a single common one  the system will converge to a stable state in which each feature corresponds to a cluster of input stimuli. the similarity between members within a single cluster will be significantly higher than the similarity between those of different clusters. a more formal description of this learning process is given in  rumelhart and zipser  1 . 
   a shortcoming of this model is that some feature cells can remain inactive  i.e.  they never fire because of being always outperformed by others. 
   a direct consequence of this behaviour is that typically very coarse features are found. the only way to refine the grain of discovered features is by increasing number of feature cells. but the more feature cells there are  the higher the probability that many of them remain inactive. 
   there are two proposals for remedy in  rumelhart and zipser  1   but our analysis of them has shown that  in general case  they do not exhibit better performance than the basic competitive model  hrycej  1 . 
   there are some additional deficiencies of the competitive model: 
1  features found by the competitive model are disjoint. if their number were substantially higher than two  features would be  almost  independent. but satisfying this condition cannot 
	hrycej 	1 
always be guaranteed - on the contrary  finding only two features is very frequent  see above and section 1 . but then  there is  in fact  only one independent feature - the features are simply logical complements of each other. 
1  the representation of input by the feature layer is local since each input activates a single feature node. so the representation found does not possess advantageous properties  error correction  efficient coding  automatic generalization  hinton et al.  1   of distributed representations. 
1  none of the features discovered by the competitive model represents a finer partitioning of others. this results directly from their disjointness. so all features are of the same hierarchical level. a hierarchical modification of such a system would be possible if additional layers were added so that clusters discovered by lower levels would be further partitioned by on higher levels. this arrangement requires an a priori structuring of the network  a tree structure . for above reasons  such a structure would be very inefficient: successor nodes of an inactive feature node would always remain inactive  too. 
1  an additional drawback of this algorithm is that maximum finding is no parallel operation.  however  there are some parallel alternatives to maximization  reggia  1  chun et al.  
1 .  
　note: the above criticism concerns only feature-discovery properties of the competitive model. there are further valuable properties of competition  e.g.  noise reduction or contour enhancement  not covered by the alternative backward-inhibition model below. 
1 	backward inhibition 
the drawbacks of the above competitive model result from its lack of capability to discover finer features simultaneously with coarse ones. this seems to be the very nature of competition. this section presents a concept of  backward inhibition  which copes with this problem. 
1 	backward inhibition concept 
the basis for my further reflections is the idea that more subtle features can be discovered only if strong features overshadowing them are suppressed in some way. 
the implementation of this idea is very simple. 
after a strong feature y  has been successfully learned  it is suppressed in the input for some time. the optimal way to suppress a feature in the next input vector is to isolate its component orthogonal to the suppressed feature  i.e.  to subtract the orthogonal projection of input vector x on the feature weight vector wj. since the feature to be suppressed is represented by wj with |wj| = 1  we get 
1 	parallel and distributed processing 

with x - original input  xx - corrected input  y  -
feature unit. the suppressed feature unit will not be further activated  since 

   the suppression can be simulated by propagating inhibitory signal  in the size of feature activation  backwards to input units. e.g.  if feature node a has been activated to activation level 1 via connections of strengths 1 and 1 to input nodes b and c  respectively  it inhibits nodes b and c by amounts - 1 * 1 . 1 = - l and -1.1.1  respectively. 
   these properties of backward inhibition will be used in the next two sections for constructing feature discovery algorithms. 
1 	eigenvectors as features 
the adaptive rule of section 1 discovers only the dominant eigenvector of input correlation matrix. we can use backward inhibition principle of the previous section for furhter features to converge to further eigenvectors. so all eigenvectors of input correlation matrix can be found by successively applying backward inhibition. 
   eigenvectors of a correlation matrix have some properties which make them intuitively good candidates for meaningful features: 
a they extract highly correlated input lines. 
b  if all input lines are mutually independent  eigenvectors are unit vectors  while the dominant eigenvector corresponds to the input line with largest variance  or  the most important input feature . 
c  if all input lines are completely correlated  the dominant eigenvector corresponds to the vector of input variances  or to the correlation matrix diagonal . 
d  if the correlation matrix can be partitioned to several diagonal submatrices corresponding to mutually independent groups of highly correlated input lines  there are eigenvectors corresponding to each of these groups. 
e  eigenvectors of a correlation matrix  which is always a symmetric matrix  are orthogonal and thus independent. 
f  they define a linear transformation which recodes the input as completely as possible  for a given dimensionality. in other words  they explain the variability of the input in the most compact way  in the sense of main components . 
   so the set of all eigenvectors seems to be an appropriate feature system. 
1 	latency time algorithm 
as stated in the previous section  all eigenvectors of input correlation matrix can be found by successively applying backward inhibition. the simplest method to do this would be to use a sequential algorithm. first  it learns the dominant eigenvector  i.e.  it lets the weight vector of the first feature node converge to the dominant eigenvector. then  it inhibits this dominant feature in the input  via orthogonal projection  see above  so that the eigenvector with the second largest eigenvalue becomes the dominant eigenvector of such a modified input  and can be in turn discovered by some other feature node  and so on until all eigenvectors have been discovered. it is  in fact  a gram-schmidt process for evaluation of orthogonal projections  kohonen  1  page 1 . the ordering of features corresponds to descending ordering of eigenvectors by the absolute value of corresponding eigenvalues. 
for all f from 1 to number of features: 
learn f-th feature by the basic adaptive rule while backward-inhibiting the input by all features ff f. 
   an obvious drawback  and cognitive inadequacy  of this algorithm is its sequentiality. this is  to a certain degree  an intrinsic characteristics of the problem since our adaptation rule always learns only the dominant feature. in other words  small features have to be enabled to come out of the shadow of large features by filtering large features out. this can occur only sequentially. however  control can be made parallel  or better to say local in each feature node  by the following extended activation rule applied simultaneously to all features: 
step 1: activate feature nodes by the basic adaptive model activation rule. 
step 1: if the weights of a certain feature node have reached a  temporarily  stable state  this feature node performs backward inhibition of input nodes for a certain number of cycles  the latency time. 
　the learning rule remains identical to that of the basic adaptive model  learning of a latent feature node is suspended during its latency time . 
   for this algorithm  a criterion of weight stability is needed. a simple test of weight increment 1w for being zero  or very small  is not appropriate since input is inherently varying and thus weights never become completely stable. so an average over recent periods has to be used. i have used an exponentially weighted average of weight increment 1vv over past cycles edwi edwabs  a ratio of average increment and average absolute value of increment  with edw = p*edw +  1-p *1w and edwabs = p* edwabs + 
 v-p *|1w|. this ratio converges to zero if positive increments are periodically traded-off by negative increments while their absolute values may be relatively large  depending on the a-coefficient of the adaptive rule . 
   this feature discovery model is superior to the competitive one in several aspects: 
1  eigenvectors of a correlation matrix are orthogonal  since a correlation matrix is always symmetric  and thus independent. 
1  the grain of feature discovery can be controlled by latency time parameter. if latency time is short  strong features are rapidly learned while there is not enough time for weak features 
to be discovered. if the latency time is long  week features are discovered  too. 
1  the representation of input by the feature layer is distributed  see the following example . 
1  the eigenvectors being ordered by their eigenvalues  they represent a certain implicit hierarchy  as illustrated by the following example. 
example 1: let us have 1 input patterns  see figure 
1  represented by 1 binary input lines.  for symmetry  -1 are substituted for zeros.  although theoretically 1 features could be found  we will be satisfied with 1 features. features discovered by backward inhibition algorithm are presented in table 1 as they are contained in the patterns  the activation value of each feature multiplied by 1 is given in each feature column . for easier orientation  an abridged code is given in the middle column. symbol meanings are: + for feature activation   1  - for feature activation   1  o otherwise. 

   a  classification hierarchy   formed by descending order of eigenvalues is given in figure 1. as we can see  dominant feature 1 partitions the paterns to  diagonally oriented   +  and  margin-oriented   - . those diagonally oriented are partitioned by feature 1 to  bottom right - top left   +  and  bottom left - top right   - . features 1 and 1 measure  balancedness  of diagonal patterns. 
this hierarchy is not explicit in the feature vector. 
however  an implicit encoding is all that is needed for input into a further associative network. some associations can depend only on strong features like  diagonal  vers   margin-oriented   encoded by a single feature cell f1 while weaker ones like diagonal 
	hrycej 	1 

orientation are encoded by f1 and f1 together  yet more special features by adding a further feature cell  etc.  so that the actual encoding corresponds to the  path  in the hierarchy tree. 

　the feature system can be used to classify novel patterns  as illustrated by the following example. 
　example 1: four novel patterns have been experimentally classified by the system  see figure 1 and the table 1 . note that pattern 1 has been classified as  bottom right - top left  and  balanced  but undecided if  margin-oriented  or  diagonal . pattern 1 is  margin-oriented   pattern 1  diagonal  and  balanced  with undecided direction. 
　note: with short latency times  only approximations of eigenvectors are found. since the  residue  after suppressing  strong  eigenvectors may then substantially differ from the exact one  features corresponding to  weak  eigenvectors may substantially differ from exact eigenvectors  too. however  in all cases tested they showed very high orthogonality  so that good properties of features discovered were preserved. 
1 	parallel and distributed processing 
1 	competitive algorithm 
backward inhibition can also be used to improve grain control of the competitive model of section 1. after a feature unit y  has won the competition and its weights have incrementally learned  the feature represented by its weight vector wj is  partially  suppressed for some time: xx = x - b*yi*v/i with 1   b   1. parameter b controls the extent of suppression and thus the grain of feature discovery. because of competition features do not correspond to eigenvectors. 
　although this model is difficult to treat mathematically  it can be supposed  and has been verified experimentally  see example 1  that even weak features get their chance to attract a weight vector so that the grain of feature discovery will be refined. 
　example 1: to illustrate grain control  a set of five input vectors has been taken  1 1 1    1 1 1    1 1 1    1 1 1   and  1 1 1 . their correlation matrix is 
　the 1-unit case showed poor convergence for b 1  so corresponding results are not presented. it can be seen that basic competitive algorithm partitioned the patterns typically into three groups  no matter if there were five or ten feature units. the backward-inhibition-based algorithm has found  by contrast  three to five groups  five being the maximum since there are only five different input vectors   depending on the backward inhibition strength b. 
1 	conclusion 
backward inhibition in a two-layer connectionist network can be used as an alternative to  or an enhancement of  the competitive feature discovery model. 
　the noncompetitive backward inhibition algorithm discovers features in the form of input correlation matrix eigenvectors and thus satisfies the requirements of independence  controllable grain  and distributed representation. since the eigenvectors can be ordered by the absolute value of the corresponding eigenvalues  they form a certain  feature hierarchy . 
　the competitive backward inhibition algorithm operates analogically to basic competitive algorithm  but provides means for controlling the grain of feature discovery by inhibition parameter. 
　the advantage of backward inhibition over traditional numeric methods of cluster and variance analysis is their parallel implementation in a neural network and immediate construction of distributed representation which can be used for further processing  e.g.  in a supervised-learning model. 
