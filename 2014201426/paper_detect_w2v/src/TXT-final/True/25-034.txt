 
on the basis of its optimal asymptotic time complexity  ac-1 is often considered the best algorithm for establishing arc consistency in constraint satisfaction problems  csps . in the present work  ac1 was found to be much more efficient than ac-1  for csps with a variety of features.  variable pairs were in lexical order  and in ac-1 they were added to the end of the list of pairs.  this is supported by arguments for the superiority of ac-1 over most of the range of constraint satisfiabilities and for the unlikelihood of conditions leading to worst-case performance. the efficiency of ac-1 is affected by the order of variable testing in 
phase 1  'setting up' phase ; performance in this phase can thus be enhanced  and this establishes initial conditions for phase 1 that improve its performance. but  since ac-1 is improved by the same orderings  it still outperforms ac-1 in most cases. 
1 	introduction 
　local consistency techniques can be used with search to improve the efficiency of algorithms that solve constraint satisfaction problems  csps . they have also proven useful in standalone implementations  because they can yield solution sets or highly reduced problems  deville and van hentenryck  1 . the most common techniques establish arc consistency  i.e.  pairwise consistency for all variables in the problem or for a selected subset. this paper is concerned with algorithms that establish arc consistency for the entire problem. 
ac-1 was introduced by mackworth  as a 
generalization and simplification of the earlier  filtering  algorithm of ullman  and waltz   now known as ac-1. mackworth and freuder  showed that this algorithm has a worst case complexity bound that is significantly better than that of the simple strategy of scanning all pairs until no inconsistencies are found. subsequently  mohr and henderson  introduced ac-1 and showed that it has an even better worst case complexity  that is optimal in the general case. 
　this historical development has led to the impression that at each stage an algorithm was derived that was generally superior to previous algorithms in terms of efficiency  e.g.  perlin  1 . but since the main results are in terms of worst case asymptotic behavior  it is not clear that ac-1  the best algorithm in this sense  is actually better for a 
this material is based on work supported by the national science foundation under grant no. iri-1. 
given problem or even better on average. it is possible that  for classes of problems  either ac-1 or ac-1 is superior. this is suggested by van hentenryck's  remark that ac-1 is better for some problems  ac-1 for others. 
　a related problem concerns the order in which variable domains are checked for consistency. it has been found that certain ordering heuristics can yield marked improvement in the performance of ac-1  wallace and freuder  1 . for some problems  this might tip the scales in favor of this algorithm. on the other hand  it has been claimed  in personid communications  that ac-1 is  order-independent . if this were true  and if ac-1 was better than ac-1 for a given problem  it would not be necessary in this case to consider such ordering at all. here it is shown that both ac1 and ac-1 provide opportunities for ordering to be effective  so these heuristics are important regardless of the algorithm chosen. 
section 1 describes the algorithms ac-1 and ac-1. 
section 1 discusses average case performance under certain probabilistic assumptions. section 1 gives results of tests of these algorithms on problems that differ with respect to key problem parameters  including random problems and reduced queens problems. large problems with parameter values that yield very difficult problems were also tested. section 1 considers problems that result in worst case performance by ac-1 from a theoretical and empirical standpoint. section 1 discusses effects of ordering the phase 1 list of variable pairs in ac-1  and presents empirical results for the problems of section 1. section 1 discusses problem classes in which the worst case complexity of arc consistency is less than o ed1 . section 1 presents conclusions. 
1 description of the algorithms 
　a binary constraint satisfaction problem involves a set of n variables  vi  each with a domain of values  di  that it can assume. in addition  the problem is subject to some number of binary constraints  cij. each a subset of the cartesian product of two domains  di x dj.. a binary constraint specifies which pairs of values can be simultaneously assumed by the pair of variables. a csp is associated with a constraint graph  where nodes represent variables and arcs or edges represent constraints. 
　ac-1 involves a series of tests between pairs of constrained variables  vi and v.. specifically  values in di are checked against the constraint between vi and vj to see if they are consistent with at least one value in dj; unsupported values are deleted. the ac-1 algorithm is shown in figure 1. all ordered pairs of constrained variables 
wallace 
　
a re first put in listofpairs. each pair   vi  vj   is removed and di is tested against dj when values are deleted  it may be necessary to add pairs back to listofpairs to determine if these deletions lead to further deletions. 
initialize listofpairs to { vi  vj  there is a constraint between vi and v. . 
while listofpairs is not empty select and remove  vi  vj  from listofpairs. 
test vi against v.-. 
if any values are removed from di  add to listofpairs any pairs  vk  vi   k』j  such that there is a constraint between vk and vi and  vk  vi  is not already present in listofpairs. 
figure 1. the ac-1 algorithm. 
　ac-1 also begins with a sequence of tests between all pairs of constrained variables  figure 1: phase 1 . but in this case the purpose of the tests is to determine amount of support  i.e.  how many values in the domain of vj support value a in the domain of vi and which values these are. 
this information is kept in special data structures for use in 
phase 1 of the algorithm. if in the course of phase 1  a value is found to have no support  this is recorded  in array mark  and the value removed from the domain  as in ac-1. 
　alter one pass through the set of variable pairs  ac-1 constructs a list of unsupported values  listofbadvalues  in the form of variable-value pairs. then  for each element on the list   vk  c   the list of values supported by that value 
 collected in phase 1  is examined. for each of the latter values  e.g.  value d in the domain of vl  the associated counter is decremented  here  counter   vk  vl  d  . if a counter is decremented to zero and the value was previously supported  it is put on the list of unsupported values.  note. the present ac-1  figure 1  is identical to mohr and 
henderson  except that in phase 1 array mark is checked before a counter is decremented rather than after.  
1 average performance of ac-1 and ac-1 
　the following facts have been established regarding the time complexity of these algorithms. the worst case complexity of ac-1 is bounded above by q ed1   where e is the number of constraints  edges  and d the maximum domain size  mackworth and freuder  1 . the time complexity of ac-1 is always o ed1   mohr and 
henderson  1 . since the lower bound on the complexity of arc consistency is 1 ed1   ac-1 is always optimal in terms of time complexity. 
　another factor suggests that ac-1 would sometimes be more efficient than ac-1. in ac-1  each value in di is tested in terms of a yes/no query: is it supported by any value in dj or not  in ac-1 on the other hand  value testing involves questions of magnitude: how many values in dj 
1 	constraint satisfaction problems 
　
　to give some concrete examples  suppose in this case ac-1 performs 1 constraint checks for each value tested against this domain. if p = .1  the expected number of constraint checks performed by ac-1 for each value is 1; if p = 1  the expected number is 1  if p = .1  the expected number is 1  and if p = .1  the expected number is 1. if also has 1 values  the expected numbers of constraint checks for probabilities just given are 1  1  1 and 1  respectively  in contrast to 1 constraint checks for ac-1. thus  across a wide range of inclusion probabilities  ac-1 performs less than half the number of constraint checks that ac-1 requires. in these cases  if ac-1 performs twice as many variable-pair tests as ac-1 does in phase 1 it will still be more efficient.  note also that on successive tests of the same pair  the number of values checked will be smaller.  for random problems  especially when variablepair tests are ordered by a good heuristic  the number of variable pairs that must be retested is usually much less than this   wallace and freuder  1  and see below . 
1 experimental results 
　evidence on the frequency with which ac-1 outperforms ac-1 and the average difference in performance was obtained from empirical studies. three kinds of csp were examined: 
　 i  random problems in which values for number of constraints  added to a spanning tree based on randomly selected constraints   domain size and satisfiability were chosen from the range of possible values  and then elements were chosen  at random  from the set of possible elements to equal the parameter value. in the problems used here  n = 1  so 1 constraints could be added to a spannning tree. thus  the number of added constraints could range between 1 and 1. similarly  the maximum domain size = 1  and the value for a domain size was chosen between 1 and 1 inclusive. and for two domains with  say  1 values  a value for the satisfiability of a constraint between them was chosen between 1 and 1 inclusive. these are called random parameter value problems. 
　 ii  random problems in which each element in the set of possible  additional  constraints  or possible domain values  or possible constraint pairs  was chosen with a probability that was constant for that panuneter. for example  in one set of ten-variable problems the probability for inclusion of a constraint was 1  for inclusion of a domain value the probability was 1  maximum domain size = 1  and for inclusion of a constraint pair the probability was 1. this yields problems with  on average  about 1 constraints  five values per domain and a relative satisfiability of 1 per constraint.  null domains and constraints were not allowed.  these are called random probability of inclusion problems. 
　 iii  k-queens problems with one domain  row  restricted to one or a few elements. this yields queens-like problems that can be reduced by arc consistency methods. unlike the random problems  constraint graphs are always complete. 
　random parameter value problems are heterogeneous internally  but average values for domain size and satisfiability are near the middle of the range. they are subject to considerable reduction by arc consistency methods. probability of inclusion problems are more homogeneous with respect to their parameter values  with average values close to those predicted from original probabilities. the amount of reduction due to violations of arc consistency is usually less than for random parameter value problems. five-queens problems with a domain restricted to one value show considerable domain reduction; with 1-queens the reduction is much less. 
　ten twelve-variable random parameter value problems were generated in each of two constraint-ranges: 1 and 1. five problems in each range with no solutions were also collected; lack of solution was detected by arc consistency algorithms in all cases  by domain 'wipeout'. tests of these problems are referred to as experiment 1 in the discussion of results. 
　ten-variable probability of inclusion problems were generated with the following probability values: 1  1 and 1 for constraint inclusion  1 and 1 for domain value inclusion  maximum d = 1   and 1 and 1 for constraint pair inclusion. these factors were fully crossed  to produce twelve sets of problems. five problems with solutions were generated in each category. tests of these problems are referred to as experiment 1. 

ii - 1 	1 - 1 	1 - 1 - 1 no wipeout 	wipeout 
number of constraints 
figure 1 performance of arc consistency algorithms on 1variable random parameter value problems. number of constraint checks to produce consistency or wipeout 
　five- and ten-queens problems were tested  with the one domain restricted to one value or to either two  1-queens  or four  1-queens  values. the restricted domain was associated with either the first variable  first row   a middle variable  third or fifth row for 1- and 1-queens problems  respectively   or the last variable. all possible values were tested in the one-value cases  while five combinations of values were tested in each multi-valued case. these tests are referred to as experiment 1. 
the relative performance of ac-1 and ac-1 in 
experiments 1 was quite consistent  figures 1 . ac-1 outperformed ac-1 on every problem tested  and the number of constraint checks was 1 or better in most cases  even with comparisons limited to phase 1 of ac-1.  phase 1 does not use constraint checks.  ac-1 was  therefore  superior to ac-1 on problems differing in degree of difficulty  amount of domain reduction  ranging from no reduction to reduction to a single solution or to domain wipeout   key parameter values  and systematic versus random patterns of dependency 
wallace 
　
 in the queens and random problems  respectively . 
　figures 1 and 1 show the expected effect on ac-1 of increasing the total number of values that must be checked  either by adding constraints or by increasing average domain size. at the same time  the effect of these changes on ac-1 is much smaller. for probability of inclusion problems  increasing constraint satisfiability  decreasing the tightness  also led to more constraint checks by ac-1. this was because there were fewer values deleted in the course of checking. but  consistent with the analysis in section 1  decreasing the tightness led to fewer constraint checks by ac-1. this overcame the effect of fewer deletions  which is also a factor in ac-1's performance. 

figure 1. performance of ac-1 and ac-1 on problems with different probabilities of inclusion of constraints  pc } domain elements  pd  and constraint pairs  pp . 
similar results were found with queens problems  figure 
1 . since the list was initially in lexical order  these problems were progressively harder as the restricted domain was further back in the list. this was true for both algorithms  but the proportional increase was greater for 
ac-1.  the ratio of highest to lowest means was 1 and 
1 for ac-1  for the 1- and 1-queens problems  respectively  and 1 and 1 for ac-1.  it seems that as a general rule ac-1 is less affected by problem features that produce more work for arc consistency algorithms. 
　in each experiment  differences in mean values were evaluated statistically by repeated measures analyses of variance  with algorithms as the repeated factor.  separate analyses were done for problems with and without solutions in experiment 1 and for 1- and 1-queens problems in experiment 1.  for each test except that for the random parameter problems without solutions  the algorithms factor was statistically significant at the 1 level. in the latter case  this factor was significant at the 1 level. in experiment 1  the interaction of the algorithms factor with each of the three problem parameters that were varied was statistically significant at the 1 level  reflecting the greater effect on ac-1. the factors based on probability of constraint inclusion and domain element inclusion were themselves significant at the 1 level  while the factor based on constraint value-pair inclusion was not statistically 
1 	constraint satisfaction problems 
significant. in addition  the interaction between the former two probability factors and the third-order interaction of these factors with the algorithms factor were significant at the 1 level. in experiment 1  the factor based on location of domain restriction was significant at the 1 level in analyses based on the 1- or 1-queens problem  and the interaction between this factor and the algorithm factor was significant at the 1 level in both cases. 

	domain 	restricted 
figure 1. performance of arc consistency algorithms on queens problems where one domain is restricted to one value. 
　for these problems. phase 1 of ac-1 was done very quickly. in experiment 1  the average number of badlist accesses was about 1  including 1 list additions  and there were about 1 sjc  support list  accesses. in experiment 1  there were few operations of any kind except for problems with small domains  p = 1  and small satisfiabilities  p = 1 ; even here  means for badlist accesses only varied from 1 to 1  with a mean of 1 to 1 additions  and means for sjc accesses varied between 1 and 1. for problems with the largest domains and satisfiabilities  there was no domain reduction  so no work was required in phase 1. for queens problems with singleton domains  there were 1 or 1 badlist accesses for 1-queens and 1 for 1-queens  while the average number of sjc accesses was 1 and 1  respectively. for 1- and 1-valued domains  the number of operations of each type was reduced by a factor of five for 1queens problems  while no work was required for 1-queens problems since there was no domain reduction. 
　two further experiments were run to determine whether ac-1 would maintain its superiority with larger problems. 
in the first experiment  ten random parameter value problems were generated with 1  1 or 1 variables. there was no restriction on number of constraints  and in all cases the maximum  d  was 1. after each problem was generated  a solution was added by selecting a value from each domain at random and  if necessary  replacing a value pair in a constraint with the pair that was consistent with this solution. arc consistency reduced most of these problems to the added solution  with some exceptions among 1-variable problems. in the second experiment probability of inclusion methods were used to generate 1variable csps. the domain size was always four and the probability of value pair inclusion was 1  so they resembled four-color problems with respect to their parameter values. a range of densities was chosen that 
　
included a peak in the performance curve  based on forward checking with dynamic ordering. this peak was near the  critical connectivity value   williams and hogg  1 . it wits  therefore  possible to compare ac-1 and ac-1 on inherently difficult its well as easy problems generated with the same methods. in general  these problems did not admit much domain reduction. for this reason  similar problems with relative satisfiability equal to 1 were tested  in which peak difficulty was associated with a density of 1. these admitted more reduction  but the results for arc consistency were similar to the results for the first set of problems. 

　figure 1. performance of arc consistency algorithms on random parameter value problems of varying size. 
　results for the first experiment on larger problems are shown in figure 1. rather than decreasing as problems get larger  if anything  the degree of improvement with ac-1 increases. it should also be mentioned that  in testing ac-1  four 1-variable problems had to be run on a larger machine because of space requirements. in addition  as problem size increased  the amount of work required in phase 1 became appreciable  ranging from a me;in of 1 badlist accesses and 1 sjc accesses for 1-variable problems to 1 badlist accesses and 1 sjc accesses for 1-variable problems. but performance was still dominated by phase 1. 

problem density 
figure 1. performance of arc consistency algorithms and forward checking on large random problems over a range of densities that includes the critical connectivity associated with very hard problems. 
　for 1-variable problems  ac-1 was markedly better than ac-1 at all problem densities  including those associated with hard problems. this suggests that the inherent difficulty of a problem bears no relation to worst case behavior of ac-1. and it confirms the conclusion of the previous experiment that ac-1 retains its superiority to ac-1 in the average case its problem size is 'scaled up'. 
1 worst case conditions for ac-1 
　mack worth and freuder  discuss the theoretical worst case condition for ac-1  in which each domain  di   is tested and reduced 1 d  times  requiring the degreei-1 implicated pairs of variables to be put back on the list on each occasion. they show that the total number of list additions in this case is d 1e - n   which  when multiplied by d1  gives an upper bound for worst case complexity. their argument can be extended: if the total reduction is 
o d  in each of o n  domains  and if  for each of these domains  a reduction of 1  occurs at the end of every set of o degreei  tests against its constraining variables  then 
ac-1 will exhibit worst case behavior. 
　problems that result in worst case performance can be described in terms of sequential dependencies holding between values in different domains. for example  if value a in domain di of vi is supported by b in d;  and b; depends in turn on c and d in dk  then there is a sequential dependency from a to {c d} via b. this can be shown by a restriction diagram  wallace and freuder  1 : 

in a full diagram each column represents all values that can be eliminated at the beginning of testing or after all possible values have been eliminated in the previous column. there can  therefore  be up to 1e entries in a column. 
　sequential dependencies can take many forms. at one extreme  if any reduction is possible   there may be no indirect dependencies  and the restriction diagram has one column. the other extreme is a single dependency chain of length nd-1  whose restriction diagram has nd columns. 
　obviously  if sequential dependencies allow enough domains to be reduced by 1 d  in one stage  one column of the restriction diagram   then worst case behavior cannot occur. this can happen if one value supports 1 d  values in another domain or if there is an effective branching factor in successive columns of the restriction diagram  e.g.  value a in di is the sole support of values in two domains  each of which is the sole support of values in two domains  etc. conversely  worst case conditions can only occur if the dependency relations are essentially 1 and the total length of the chains is o nd . 
　if two dependency chains overlap with respect to the variables involved in a particular segment  there will be an ordering that allows deletions along both chains at the same time. if a sufficient number of chains are concurrent  then worst case conditions will be order- dependent: otherwise worst case conditions are order-independent. 
wallace 
　
　for problems whose constraint graphs are trees  dependency chains cannot be longer than n-1  since a chain must end if a leaf-node is encountered. for  if the value deleted from the domain of that node were to support a value in the domain of a neighboring node  the latter value would have prevented deletion in the first place. this means that  for worst case tree-structured problems  the dependency chains must overlap; hence  worst case conditions for such problems are always order-dependent. there is  in fact  an ordering for ac-1 that guarantees optimal performance: for any node considered as the root of the tree  begin with the leaves and move toward the root  in each case testing parent against child and never testing a node farther from the root after testing one nearer. after the root has been checked against its children  proceed toward the leaves testing children against parents. with this ordering  if values are deleted when  is tested against  other pairs 
will still be on the list. hence  the complexity is o ed1 .  this is essentially the double directed arc consistency procedure described by dechter and pearl .  
　if the constraint graph has cycles  worst ease conditions can be order-independent. but even in these cases the performance of ac-1 may not be markedly worse than ac-1 unless domains are very large. this is because the proportionality factor associated with performance for ac-1 is always while for ac-1 the proportionality factor associated with o ed1  performance can equal the maximum value of 1. two independent factors improve ac-1's performance:  i  in worst case conditions 
domain sizes diminish with every set of tests  so the term in the expression for worst case performance  must continually decrease   ii  for a constraint between and values in  will be tested against all values in only if there is no support or the sole support is the last value tested  using a standard test order . but this is incompatible with worst case conditions  since more than 1  1  values in 
note. proportionality factor is ac-1 ccks for list additions divided by 
　order-independent worst case csps were constructed with a single cycle of nodes and one long dependency chain 
1 	constraint satisfaction problems 
 beginning with the last voidable pair in the lexical order  with elements. table 1 gives results for different e and ac-1 is linear in e  and for these problems constraint checks were 1 % greater with ac-1 for each cycle length. appreciable differences in favor of ac-1 were only found when domains had 1 or 1 elements initially. for all problems the proportionality factor for ac-1 was 1 
- 1. interestingly  when = 1  changing one constraint pair at random reduced the effort required by ac-1 to about that for ac-1  and with two such changes ac-1 was better. when  one or two random changes reduced the number of constraints made by ac-1 to about 1. 
1 effect of ordering list of variable pairs 
　in the experiments discussed in section 1  the list of variable pairs was ordered lexically  and in ac-1 the list was thereafter treated as a queue. this appears to be the common procedure  mackworth  1; nadel  1 . however  it has been found that  for ac-1  ordering the list in terms of basic parameter values can have a marked effect on efficiency  wallace and freuder  1 . in this work it was shown that maintaining the list in order  by increasing size of the domain checked for support  is a very effective heuristic for reducing the number of constraint checks needed to achieve arc consistency. this heuristic is readily combined with an o n  pigeonhole sort  leading to an overall efficiency that is superior to lexical/queue ordering. it has been asserted  in personal communications  that ac-1 is  order-independent . but  since phase 1 of ac-1 is similar to the initial pass through the list of pairs in ac-1  it seemed likely that ordering heuristics found effective with the latter algorithm would also work with the former. in particular  order should matter because the same domain is often tested more than once; if values can be deleted sooner from this domain there will be a savings in the number of constraint checks. this is especially important in view of the fact that  in the experiments above  most of the work was done in phase 1. 

figure 1. effect of ordering in phase 1 of ac-1 for random parameter value problems with solutions. ac-1 with domjup ordering is also shown. 
　phase 1  is less affected by ordering than phase 1 because in the second phase there are no irrelevant tests. the only important exception is domain wipeout; early detection can obviously improve efficiency. otherwise  the number of badlist and support list accesses is always the same for an initial set of badlist values  support lists and counters.  unlike mohr and henderson   the present implementation allows effects on counter decrements  
　
because these are preceded by tests of array mark; in limited tests such effects were very small.  
　however  the number of operations can be significantly reduced in phase 1 if the tallies are smaller at the beginning of this phase. this will occur if values are eliminated early in phase 1  when they have not contributed very much to the tallies. included in the first three experiments  therefore  were tests with an ordering heuristic based on increasing size of the supporting domain. this heuristic is called domjup  reflecting the convention that the domain of vi is tested against the domain of v.. 
　in experiments 1 and 1  use of the domjup heuristic produced a marked decrease in the number of constraint checks required in phase 1  figure 1. similar results were obtained for random parameter value problems with no solutions and queens problems.  however  this did not make acm as good as ac-1  since the hitter's performance is also improved by the heuristic  for random parameter value and 1-queens problems  domjup produced a greater proportional decrease in constraint checks for ac-1 than ac-1.  in both experiments  the heuristics factor in the analysis of variance was significant at the 1 level. in the queens problems  the interaction of the heuristics factor and the factor related to the domain restricted was also significant at the 1 level. this is because domjup put domains most likely to be reduced at the front of the list in all cases. 
　in experiment 1  where degree of domain reduction was much less  the improvement in constraint checks due to domjup was slight  although it was still better on average even when only a few values could be deleted. but in this case the heuristics factor was not statistically significant. 
figure 1 also shows that improved performance in phase 
1 of ac-1 led to improvement in phase 1  measured by sjc list accesses. as indicated above  this is because values that 
are deleted quickly are removed before they can be included in support lists for other values. thus  even when order of testing bad values has little or no effect on the efficiency of phase 1  this phase can be strongly affected by a 'ripple effect' from the ordering used in phase 1. 
1 problems with reduced complexity 
recently it has been shown that there are certain classes of 
csps for which worst case complexity of arc consistency algorithms is o ed . in some cases this is because domain support can be determined in one operation  for example  if the constraints are functional relations  actually bisections   deville and van hentenryck  1 . in other cases this is because   i  the values in a domain can be partitioned into equivalence classes  so relations can be factored into a set of relations based on these classes   ii  there are bounds on the number of links between these classes fperlin  1 . for bijectional constraints  the factors that make ac-1 less efficient in general are not present; in particular  the argument in section 1 is not relevant. in this case  optimality considerations indicate that ac-1 should be used. on the other hand  ordering the list during phase 1 can still improve efficiency. for factorable relations  condition  i  above   differences in efficiency should be reduced but not eliminated  so ac-1 may still be superior. if condition  ii  also holds  optimality considerations may again be the most important factor  although the relative superiority of the two algorithms remains an open question. 
1 conclusions 
　although there is a small set of problems for which ac-1 is basically less efficient than ac-1  in practice the fonner algorithm is almost always more efficient for csps based on generalized relations. worst case conditions aside  conditions which make arc consistency algorithms less efficient  greater number of constraints  larger domains  have a proportionally greater effect on ac-1 than on ac-1. when constraints have greater satisfiability  ac-1 is also less efficient while ac-1 is actually improved. if  in special cases  ac-1 is the preferred algorithm  then ordering the sequence of variable pair tests in phase 1 can still improve the efficiency of this procedure  as it does for ac-1. 
