 
the blackbox planning system unifies the planning as satisfiability framework  kautz and selman 1  1  with the plan graph approach to strips planning  blum and furst 1 . we show that strips problems can be directly translated into sat and efficiently solved using new randomized systematic solvers. for certain computationally challenging benchmark problems this unified approach outperforms both satplan and graphplan alone. we also demonstrate that polynomialtime sat simplification algorithms applied to the encoded problem instances are a powerful complement to the  mutex  propagation algorithm that works directly on the plan graph. 
1 	introduction 
it has often been observed that the classical ai planning problem  that is  planning with complete and certain information  is a form of logical deduction. because early attempts to use general theorem provers to solve planning problems proved impractical  research became focused on specialized planning algorithms. sometimes the relationship to inference was explicitly acknowledged: for example  the strips system  fikes and nilsson 1  was originally described as a way to make theorem-proving practical. in other work the relationship to deduction was developed after the fact. for example  chapman's  1  work on tweak clarified the logic behind one variety of non-linear planning. 
　the belief that planning required specialized algorithms was challenged by the work on planning as propositional satisfiability testing of kautz and selman  1  1 . satplan showed that a general propositional theorem prover could indeed be competitive with some of the best specialized planning systems. the success of satplan can be attributed to two factors: 
  the use of a logical representation that has good computational properties. both the fact that satplan uses propositional logic instead of first-order logic  and the particular conventions suggested for representing time and actions  are significant. differently declarative representations that are semantically equivalent can still have quite distinct computational profiles. 
challenge papers 
  the use of powerful new general reasoning algorithms such as walksat  selman  kautz  and cohen 1 . many researchers in different areas of computer science are creating faster sat engines every year. furthermore  these researchers have settled on common representations that allow algorithms and code to be freely shared and fine-tuned. as a result  at any point in time the best general sat engines tend to be faster  in terms of raw inferences per second  than the best specialized planning engines. in principle  of course  these same improvements could be applied to the specialized engines; but by the time that is done  there will be a new crop of sat solvers. 
　an approach that shares a number of features with with the satplan strategy is the graphplan system  developed independently by blum and furst  1 . graphplan broke previous records in terms of raw planning speed  and has become a popular planning framework. comparisons to satplan show that neither approach is strictly superior. for example  satplan is faster on a complex logistics domain  they are comparable on the blocks world  and on several other domains graphplan is faster. for excellent reviews and discussions of the two systems  see kambhampati  1  and weld  1 . 
　a practical difference between satplan and graphplan is that the former takes as input a set of axiom schemas  while the input for the latter is a set of strips-style operators. however  they bear deep similarities. both systems work in two phases  first creating a propositional structure  in graphplan  a plan graph  in satplan  a cnf wff  and then performing a search  over assignments to variables  that is constrained by that structure. the propositional structure corresponds to a fixed plan length  and the search reveals whether a plan of that length exists. kautz and selman  1  noted that the plan graph has a direct translation to cnf  and that the form of the resulting formula is very close to the original conventions for satplan. in the unifying framework of kambhampati  1   both are examples of disjunctive planners. the initial creation of the propositional structure is a case of plan refinement without splitting  while the search through the structure is a case of plan extraction. we hypothesize that the differences in performance of the two system can be explained by the fact that graphplan uses a better algorithm for instantiating  refining  the propositional struc-

ture  while satplan uses more powerful search  extraction  algorithms. 
　satplan fully instantiates a complete problem instance before passing it to a general logic simplifier  a limited inference algorithm that runs to completion in polynomial time  and a solver  a complete or incomplete model-finding program . by contrast  graphplan interleaves plan graph instantiation and simplification. the simplification algorithm used in graphplan is based on mutex computation  an algorithm for determining that pairs of actions or pairs of facts are mutually exclusive. mutex computation can be viewed as rule of limited inference that is specialized to take particular advantage of the structure of planning problems  kambhampati et al 1 . specifically  mutex computation is a limited application of negative binary propagation: 

each application of the rule allows the deduction of a negative binary clause  a mutex . the mutex algorithm used by graphplan is incomplete  not all mutexes that logically follow can be inferred  and terminates in polynomial time. note that this algorithm is different from the simplification rule of unit propagation employed by the original satplan: more powerful in propagating negative clauses  but somewhat less powerful in propagating positive information. the set of mutexes is used in two ways by graphplan  both to prune nodes from the graph during instantiation and to prune branches of the search tree that involve mutually exclusive actions. 
　these observations have led us to create a new system that combines the best features of graphplan and satplan. this system  called blackbox 1 works in a series of phases: 
1. a planning problem  specified in a standard strips notation  is converted to a plan graph of length k  and mutexes are computed as described above; 
1. the plan graph is converted to a cnf wff; 
1. the wff is simplified by a general cnf simplification algorithm; 
1. the wff is solved by any of a variety of fast sat engines; 
1. if a model of the wff is found  then the model is converted to the corresponding plan; otherwise  k is incremented and the process repeats. 
note that specialized limited inference is used in mutex computation  while general limited inference is used in cnf simplification. we will return to the complementary nature of these two processes in section 1 below. the input to the final general sat engine can be considered to be the combinatorial core of the problem. the basic translation from a plan graph to sat is described in kautz  mcallester  and selman  1 ; in section 1 we will also describe a variation used in some of our experiments. baioletti et al  1  also propose a similar scheme for translating plan graphs augmented with temporally-qualified goals into sat. 
　the wff generated from the plan graph can be considerably smaller than one generated by translating strips operators 
   source 	code 	and 	benchmarks 	available 	from http://www.research.att.comtkautz/blackboxy. 

figure 1: relationship of the cutoff value  measured in backtracks until a restart is performed  on expected solution time. data is for a randomized backtracking algorithm  satz-rand  for the satplan encoding of a logistics planning problem  log.d . the y-axis specifies the expected number of backtracks performed until a solution is found  counting the previous failed restarts. 
to axioms in the most direct way  as was done by the earlier medic system of ernst  millstein  and weld  1 . furthermore  the fact that the plan graph's mutex relationships are directly translated into negative binary clauses makes the formula easier to solve by many kinds of sat engines. 
　blackbox currently includes the local-search sat solver walksat  and two systematic sat solvers  satz  li and anbulagan 1  and rel sat  bayardo and schrag 1   in addition to the original graphplan engine  that searches the plan graph instead of the cnf form . the two systematic solvers are comparable in power although quite different in approach: satz is based on forward-checking  while relsat employs dependency-directed backtracking. in order to have robust coverage over a variety of domains  the system can employ a schedule of different solvers. for example  it can run graphplan for 1 seconds  then walksat for 1 minutes  and if still no solution is found  satz for 1 minutes. 
　the blackbox system actually introduces new sat technology as well  namely the use of randomized complete search methods. as shown in gomes  selman  and kautz  1   systematic solvers in combinatorial domains often exhibit a  heavy tail  behavior  whereby they get  stuck  on particular instances. adding a small amount of randomization to the search heuristic and rapidly restarting the algorithm after a fixed number of backtracks can dramatically decrease the average solution time. figure 1 illustrates the effect of adding randomized restarts to a deterministic backtracking search algorithm. we see from the figure that as the cutoff is increased from its lowest setting the mean solution time first rapidly decreases. then  as the cutoff continues to increase  the mean solution time increases in a near-linear fashion. this increase in expected time is due to the fact that for this problem a nonnegligible portion of the runs take arbitrarily long to complete. in this example  at the optimal cutoff value  about 1 out of 1 runs succeeds in finding a solution. 
　we applied this randomization/restart technique to the version of satz used by blackbox. the variable-choice heuristic for satz chooses to split on a variable that maximizes a particular function of the number of unit propagations that would 
	kautz and selman 	1 

be performed if that variable were chosen  see li and anbulagan  1  for details . our version  satz-rand  randomly selects among the set of variables whose scores are within 1% of the best score.  the value 1% was selected empirically.  the cutoff value is specified in the solver schedule. no one value cutoff value is ideal for all domains. indeed  predicting good cutoff values is a deep theoretical question  the known asymptotic results  luby et al. 1  are often not practical . if you need to solve a number of similar problems  it is feasible to carefully tune the cutoff value on a few of the instances  and then use that value for the rest of the set. however  this cannot be done when a unique problem is encountered  or if you do not specify the parallel plan length in advance and the system must search through a series of different size problems. 
　a simple and effective practical solution is to provide the solver with a schedule of different cutoff values. the schedule specifies some number of trials with a very low cutoff value  and if those all failed  then so many at a higher value  and so forth until either a solution is found or all trials end in failure. this can be viewed as a version of the well-known search strategy of iterative deepening. like iterative deepening  the time spent on trials with a low cutoff value is negligible compared to that spent on trials with a higher value  so that even when the early part of the schedule fails to find a solution little overall effort is wasted. 
1 empirical results 
in order to test the effectiveness of the blackbox approach we selected benchmark problems that have the following characteristics: 
  domains are computationally challenging: there is no simple polynomial time algorithm for optimal planning. 
  solution time is dominated by search: plan graph generation is relatively fast  but solution extraction is hard. 
  problem instances are critically constrained: finding optimal plans is much harder than finding sub-optimal ones. 
  both both parallel and sequential planning problems are included. 
  instances push the limits of the approach in order to demonstrate how it scales up. 
in this paper we present results on a set of such problems from logistics planning  velosa 1   a highly parallel planning domain  and from the classic blocks world  a purely sequential domain. the test machine was an sgi 1 mhz r1 challenge server  where each process ran on a dedicated processor. the largest amount of ram allocated by blackbox during solution of the largest logistics problem  log.d  was 1 mb  and during the solution of the largest blocks world problem  bw.c  was 1 mb. most of the memory was allocated during the phase of constructing the initial plan graph. 
　table 1 compares the performance of blackbox  version 1   satplan  graphplan  and ipp  version 1  on our logistics benchmark problems  where the optimal plan length is provided as input. graphplan is the original code developed by blum and furst  1  that also is incorporated in the 
challenge papers 
front-end of blackbox. ipp  koehler et al. 1  is a new implementation of the graphplan algorithm including many improvements and extensions. we tried the solvers satz  satzrand  and walksat for both blackbox and satplan. input to blackbox  graphplan  and ipp was in pddl strips format  mcdermott 1 ; input to satplan was a set of hand-crafted axiom schemas using the  state-based  encodings described in kautz and selman  1 . blackbox simplified wffs before passing them on to a solver using the failed literal rule  while satplan simplified using unit propagation. 
　let us first make some general observations. we see that the scaling of blackbox using any of the sat solvers is better than graphplan. it is important to note that up to the point at which the wff is generated from the plan graph  the code running in blackbox and graphplan is identical. this indicates that the cost of performing the sat translation is small compared to the savings gained by using any of the sat engines instead of the  relatively  simple backward-chaining search performed by graphplan. although ipp generally improves upon graphplan  in this case it only provides faster solution times on the two smallest instances  and is always slower than blackbox. this is due to the fact that most of the improvements in ipp over graphplan are not invoked in this test: they only come into play when the plan length is not given  or when the initial state description contains facts that are irrelevant to the solution. this test also does not allow us to take advantage of the  rifo  heuristic graph pruning option in ipp  because doing so prevents ipp from finding the minimum parallel length solutions at all. 
　in short: for critically-constrained planning problems where plan extraction is the computational bottleneck for graphplan-type solvers  translating the plan graph into sat and applying a general sat solver can boost performance. 
　a second general observation is that the scaling of the best solution times for blackbox  using satz-rand  is close to the scaling of the best solver-only times for satplan  using walksat . this is quite remarkable  given the fact that the blackbox encodings were generated automatically  while the satplan axioms were carefully hand-tuned in order to provide the best possible performance for walksat. the satplan encodings even included explicit state invariants 
 such as the fact that  a truck is only at one location at a time   that are known to boost the performance of problem solvers  kautz and selman 1 . even more striking is the fact that when the time to generate the satplan encodings is also taken into account  the overall blackbox times are consistently better than the satplan times. for example  blackboxtakes 1 seconds to generate and solve log.d  while satplan takes 1 minutes  1 minutes to generate and 1 seconds to solve . 
　in short: advances in sat solvers have made planning using sat encodings automatically generated from strips operators competitive with planning using handcrafted sat encodings. 
　these results contrast with earlier experiments on solving automatically-generated sat encodings of planning problems. kautz and selman  1  reported that both walksat 


table 1: results on critically constrained logistics benchmark planning problems running on a 1 mhz sgi challenge server. optimal parallel plan length was provided as an input. blackbox options for column  satz  are  compact -1 -then satz . 
blackbox options for column  satz-rand  are:  compact -1 -then satz -cutoff 1 -restart 1 -then satz -cutoff 1 -restart 
1 . satplan solver options for column  satz-rand  are:  satz -cutoff 1 -restart 1 . walksat options for both blackbox and satplan are:  -best -noise 1 -cutoff 1 . timings are real wall-clock times including all input and output; differences less than 1 second between different programs are not significant due to implementation details. timings for the randomized methods are averages over 1 trials. timings for satplan separate time used to generate wff  create  and time used for each of the solvers. long dash  -  indicates solution not found after 1 hours. 

and ntab  a determinisitc backtracking solver  less complex than satz or rel-sat  had difficulty solving plan graph generated sat encodings of the larger logistics problems  getting as far as log.b before the running time exploded. the medic system  ernst  millstein  and weld 1  used the same solvers but generated the sat encodings directly from the strips axioms without taking advantage of an intermediate plan graph representation  by using the conventions described in kautz  mcallester  and selman  1 . they reported a solution time of 1 hours using walksat on log.a. one should note that there is no significant overhead in using a plan graph for generation; in fact  the generation phase in blackbox takes only a few seconds for each of problems described above  versus several minutes for generation by satplan or medic. 
　a longer version of this paper will contain a detailed comparison with medic. however  our preliminary experiments indicate that wffs generated from a plan graph  as in blackbox  have significantly different computational properties from ones generated directly from strips  as in medic   despite the fact that they are logically equivalent 
 kautz  mcallester  and selman 1 . in particular  the plan graph-based wffs contain fewer variables  more clauses  and are easier to solve. for example  the encoding of log.a generated by blackbox contained 1 variables and 1 clauses  while the encoding generated by medic  using the regular operator representation with explanatory frame axioms  contained 1 variables and 1 clauses. as shown above  the blackbox wff can be solved by satz-rand in 1 seconds  but we have not yet been able to find a setting for the parameters for satz-rand that will let it solve the medic wff in less than 1 hours. 
　the differences between the two kinds of wffs can be explained by the fact that the plan graph algorithm prunes many unreachable nodes  thus reducing the number of variables in the corresponding encoding  while propagating mutexes between nodes  thus increasing the number of  negative binary  clauses. the added binary clauses increase propagation at each branch of the backtracking search and thus speed the solution time. an interesting open question that we are currently investigating is whether a sat solver that uses dependency-directed backtracking  e.g. rel sat  can actually  learn  the added clauses while running on a medic-type encoding. 
　in short: use of an intermediate plan graph representation appears to improve the quality of automatic sat encodings of strips problems. 
　next  let us consider the differences in performance caused by different sat solvers for blackbox and satplan. first  we see that while walksat performs very well on the smaller blackbox instances  it does poorly on the two largest  log.c and log.d. by contrast  local search works well for even the largest satplan encodings.  this suggests some specific connection between local search and statebased encodings  a topic that has received relatively little attention since the original satplan work.  the deterministic version of satz shows more consistent behavior across the blackbox instances  although it stumbles on rocket.b and log.d. satz stumbles even more dramatically on log.a and log.d for the satplan encodings. 
　what is happening in each of these  stumbles  is that the satz variable choice heuristic  which is usually very good  has made a wrong choice early on in the search tree  and so the algorithm spends much time exploring a large area of the search space that is devoid of solutions. as discussed in gomes  selman  and kautz  1   one can observe this occurring for backtrack search for either a deterministic algorithm on a large distribution of problem instances  or for a randomized backtrack algorithm repeatedly solving a single instance. the latter case is the easiest to observe and has formed the basis of most experimental work on the subject  since one can simply do many runs of the algorithm  where the variable choice heuristic randomly breaks  ties  . in the experiments discussed here we have  by contrast  a deterministic algorithm running on small of different instances. in a set of examples this small it is not surprising that the phenomena 
	kautz and selman 	1 


table 1: results for blackbox finding optimal solutions to benchmark planning problems where system must search for the minimum parallel time length. the  timeout/satz-rand  solver options are  -maxsec 1 graphplan -then satz -cutoff 1 -restart 1 -then satz -cutoff 1 -restart 1 . the  prove optimal/satz ' solver options are  -maxsec 1 graphplan -then satz -cutoff 1 -restart 1 -then satz . the  prove optimal/rel-sat  solver options are  -maxsec 1 graphplan -then relsat . long dash  -  indicates solution not found after 1 hours. in every case the same quality solutions were ultimately found. 
only occurred for 1 of the 1 trials. 
　we next reran the experiments using the randomized/restart version of satz described earlier. the blackbox schedule for satz-rand used a cutoff of 1 backtracks for up to 1 restarts  followed by a cutoff of 1 backtracks restarting until a solution was found. the satplan schedule for satzrand was a cutoff of 1 backtracks restarting until a solution was found. these schedules were only very roughly tuned by hand after observing a few trials and are not necessarily optimal. however  in each case the observed solution time was significantly reduced. for blackbox the times for rocket.b and log.d were cut in half  while even more significant savings were seen for satplan  where the solution time for log.d decreased from 1 hours to 1 minutes. 
　in short: randomized restarts boost the performance of systematic sat algorithms on encodings of planning problems. 
　table 1 shows the results of running blackbox and the same logistics instances where the parallel solution length is not specified in advance. the times for running graphplan or ipp in this mode on these instances are only marginally higher than when the plan length is given as input: most of the work the graphplan-typc engines perform occurs when the plan length reaches the optimal bound. we ran blackbox with satz-rand in two modes: in the first  timeout  mode  if a solution is not found after a few restarts  1 restarts at cutoff 1  1 restart at cutoff 1   the plan length is incremented. in the second mode  blackbox is made complete by making the final part of the solver schedule run satz without any cutoff. thus  only the second mode actually proves optimally. by comparing two modes  we see that the first  timeout  is much faster than the second  even though the same quality solutions are ultimately found. this is because in the second mode most of blackbox's effort goes into proving the non-existence of a solution of length one step less than optimal. or  in other words  the  co-np  part of the sat translation was empirically harder than the  np  part for satz-
challenge papers 
rand. finally  the table presents some data from our initial experiments using the dependency-directed backtracking sat solver rel-sat. this is also a complete method that guarantees optimality  but now we see that it's timings are comparable with using satz-rand in its incomplete mode.  when the plan length is given as input  our preliminary experiments indicate that satz-rand usually has a edge over relsat.  
　in short: performance of blackbox for plan length search tasks can be acceptable  even though information from failed too-short trials is not reused  as it is in graphplan. 
　a problem with the sat approach in some domains is that the problem encodings become very large. a notable case is the classic single-armed blocks world. because no parallel actions are permitted  the plan graph must contain as many layers as there are actions in the solution. if there are n blocks  then there are 1 n1  actions and 1 n1  mutexes per layer. thus the translation of a 1-step  1 block problem  large.c  contains about 1 million clauses  most of which are negative binary clauses  mutexes . 
　we therefore developed a modification to the translation scheme that can reduce the number of clauses in such cases. note that it is not logically necessary to translate a particular mutex if that negative binary clause is implied by other parts of the translation. if particular  if we add clauses that state that an action implies its effects as well as its preconditions  the latter are part of the default translation   then mutexes do not need to be explicitly translated for actions with conflicting effects or conflicting preconditions: only mutexes where the effect of an action conflicts with the precondition of another are needed. table 1 shows the results of performing this  compressed  translation. the encodings are about 1% smaller in the number of clauses and considerably easier to solve by satz-rand  which has no difficulty in chaining through the horn clauses that entail the  missing  mutexes . for comparison the final column provides solution times for the graphplan search engine working on the plan graphs that explicitly include all the inferred mutex relations. performance of blackbox and graphplan is comparable  although neither is currently state of the art.  however  blackbox's ability to find optimal solutions to a 1-step blocks world problems would have been considered state of the art performance as little as two years ago.  
　in short: sat encodings become problematically large in sequential domains with many operators  although refinements to the encoding scheme can delay the onset of the combinatorial explosion. 
　in summary  our experiments show that blackbox provides an effective approach for  search bound  problem instances in certain highly parallel planning domains such as logistics planning. the approach runs into difficulties in domains such as the blocks world where both the intermediate plan graph and the sat translation become very large  although the technique of compressed encodings provide significant relief. 
　a longer version of this paper  in preparation  will include results on an expanded set of benchmark problems  including the instances from the aips-1 planning competition. although the performance of blackbox in the aips competi-


table 1: comparing the default and  compressed  sat translations produced by blackbox  for blocks world problems where the optimal plan length is input  no parallel actions possible . solver used by blackbox is  -compact -1 -then satz -cutoff 1 -restart 1 -then satz -cutoff 1 . star  *  indicates solver failed due to memory size  and long dash  -  that no solution found 

after 1 hours. 
tion was respectable  no competitor dominated it in on all categories in round 1  and only ipp did so in round 1   we must note that the competition problem instances did not provide a way of distinguishing planning systems that employ plan graphs on the basis of their search strategies. nearly all of the instances were  too easy  in the sense that once a planning graph was generated any search strategy could extract a solution  or  too hard  in the sense that the planning graph grew intolerably large before conversion to cnf. for example  blackbox's difficulty in dealing with the  gripper  domain were due to explosion of the initial plan graph  even though the domain is inherently non-combinatorial  a linear time domain specific optimal planning algorithm exists . differences in performance between the various systems was largely due to various graph-pruning strategies each employed  such as the rifo strategy of ipp  nebel et al. 1 . many of these strategies can be incorporated into blackbox by simply replacing it's graphplan front-end with e.g. ipp. 
　the memory required for the sat encoding can be an issue for running blackbox on small-memory machines  as noted above  ones with less than the 1 mb required for log.d   particularly because the current code does not optimize memory use  e.g.  several copies of the wff are kept in core  and memory is not reused when wffs are sequentially generated with larger bounds . even so  the falling prices for ram  currently about $1 for 1mb  support the argument that the approach will only grow more practical with time. a more serious technical challenge comes from recent work on structure sharing techniques for compactly representing large plan graphs  as will appear in the next versions of ipp and stan  fox and long 1  . how can one translate such representations into sat without multiplying out all the shared structure  instead of compiling into pure sat  one might try to compile the plan graph into a smaller set of axiom schemas  that is  a  lifted  form of cnf. the axiom schemas could be passed on to a general lifted sat solver or further compiled into rules in a constraint logic programming system. the latter alternative appears particularly attractive in the light of good results recently obtained in using constraint logic programming to solve planning problems  van beek and chen 1 . 
1 	the role of limited inference 
the plan graph approach to strips planning gains much of its power through its use of mutex computations  as we briefly 

table 1: the number of variables in the encoding of a series of planning problems before simplification  and the percentage determined by simplification by unit propagation  uprop   the failed literal rule  flit   and by the binary failed literal rule  blit . the first set of problems are blocks world and the second set are logistics. 
described above. during construction of the plan graph  graphplan marks a pair of instantiated actions as mutually exclusive if one deletes a precondition or add-effect of the other. it further determines that a pair of facts  predicates fully instantiated at a particular time-instant  are mutually exclusive if all the actions that add one are exclusive of all actions that add the other. additional mutexes are added between actions if a precondition of one is mutex with a precondition of the other. if one takes the number of preconditions or effects of an operator to be constant then mutex computation can be performed in 1 n1  time  where n is the number of instantiated actions  where an instantiated action specifies all its parameters as well as a particular time step . 
　thus mutex computation is simply a specialized form of constraint propagation  i.e.  limited deduction. some nodes can be determined to be inconsistent during instantiation and immediately eliminated from the plan graph. the remaining mutex relations are used to constrain the search over the either the graph or its sat translation. it is natural to wonder if other forms of limited inference are useful for planning problems. blum  personal communication  observes that computing higher-order mutexes  between triples of actions  etc.  is not very useful. do the binary mutex computations extract all important  local  information from the problem instances  
	kautz and selman 	1 

we decided to test this hypothesis by experimenting with a series of different limited inference algorithms that work on the the sat encodings of the problems. we used the program  compact  developed by james crawford  and considered the following options: 
unit propagation apply unit resolution. 	requires 1 n  time. 
failed literal for each literal  try adding the literal to the formula and applying unit resolution. if inconsistency is determined then then literal can be set to false. requires 1 n1  time. 
binary failed literal for each pair of literals  try adding the pair of literals to the formula and applying unit resolution. if inconsistency is determined then the binary clause consisting of the negations of the literals can be added to the formula  and the single failed literal rule applied again. requires  time. 
　table 1 shows the result of applying each of these kinds of simplifications to a series of encodings of blocks world and logistics planning problems. for each problem we show the number of variables in the instance and the percentage of those variables whose values are determined by local computation. the results for unit propagation  uprop  seem to confirm the intuition that there is little local information left in these problems. for the blocks world problems only between 1% and 1% of the variables are determined by unit propagation  and for the logistics problems no more than 1% are determined. however  the story changes dramatically for the failed literal rule  flit . in the blocks world from 1% to 1%  i.e.  the problem is completely solved!  of the variables are determined. in the logistics domain over 1% of the variables are eliminated. the binary failed literal rule  blit  is even more powerful. all of the blocks world problems were either solved completely or made trivial to solve  less than 1 variables  by this rule. the logistics problems were also further reduced in size  although they remained non-trivial to solve. 
　these results led us to select the failed literal rule as the default simplification procedure for blackbox. it runs quickly and greatly decreases the size and hardness of the problem instance. so far the higher overhead for 
the binary failed literal rule makes it impractical for the domains we have considered: it takes about as long to simplify the problem with the binary rule as to solve it using the unary simplifier and satz-rand. still  these results suggest that an improved implementation of the binary rule could be of dramatic help in certain domains. 
　thus we see that general limited inference computations on the sat-encoding of planning problems provide a powerful complement to the kind of specialized mutex computations performed by the graphplan front-end to blackbox. there is a role both for planning-specific and domain-independent simplification procedures. in future work we plan to see if we can find other polytimc simplification algorithms for sat that take particular advantage of the structure of sat encodings of planning problems. 
1 	challenge papers 
1 	conclusions 
we have provided an overview of the blackbox planning system  and described how it unifies the plan graph approach to strips planning with the planning as satisfiability framework. it provides a concrete step toward the ijca1 challenge for unifying planning frameworks  kambhampati 1 . we discussed empirical results that suggest that new randomized systematic sat algorithms are particularly suited to solving sat encodings of planning problems. finally  we examined the role of limited inference algorithms in the creation and solution of problem encodings. 
　there is strong evidence that the best current general sat engines are more powerful than the search  plan extraction  engines used by graphplan and its descendents. although it is possible to incorporate the heuristics used by these general solvers back into a specialized planner  see rintanen  1  for such an approach   given the rapid development of new sat engines such a tactic may be premature. as an alternative  giunchiglia et al.  1  present evidence that it possible to dramatically boost the performance of a general sat engine by feeding it a tiny amount of information about the structure of the encoding  in particular  identification of the action variables . there is also much work on improving the plan graph generation phase  e.g.  kambhampati et al.  1   nebel et al.  1   which could be directly incorporated in blackbox by replacing its front-end. 
　blackbox is an evolving system. our general goal is to unify many different threads of research in planning and inference by using propositional satisfiability as a common foundation. an important direction that we have not touched on in this paper is the use of domain specific control knowledge in planning  bacchus and kabanza 1; kautz and selman 1; gerevini and schubert 1; fox and long 1 ; see cheng  selman and kautz  1  for work on adding control knowledge to blackbox. 
