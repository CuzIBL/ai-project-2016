 
     this paper describes an approach to object location based on matching model regions to surface image regions. these matches  together with object models  provide hypotheses about the 1d location of the objects containing the recognized surfaces. when a hypothesis is complete  the program describes the matched data and model surface boundaries in terms of the raw model. these descriptions are used used to verify the physical consistency of the hypotheses and to explain extra or hypothesize missing features caused by obscured components. 
1. introduction 
     the most important recent work in high-level vision  acronym   has demonstrated the importance of embedding object models and an understanding of the scene-to-image transformation in an intelligent vision program. with these components  a program can predict how an object will appear given its current beliefs about the scene  rather than having to rely on image models  or  more likely  a feature space representation of these views . concurrently  much low-level vision research has been attempting to provide a surface-based description of images. 
     this paper describes a program  imagine  which matches surface regions to object models to recognize and locate projections of three dimensional objects in two dimensional images. the approach is data driven  with three major stages. the first stage matches image regions to model surfaces  with the goal of estimating the 1d orientation parameters for the image region. this information is used to hypothesize specific object surfaces. the second stage relates the hypotheses according to the structural relationships embodied in the object models. the third stage verifies that the hypothesized objects are consistent with real world constraints  such as boundary adjacency and surface ordering. this process requires calculating descriptions of the predicted surfaces and associated data regions. recognition is considered successful if a set of data is found that adequately accounts for all features of a model. in particular  this requires the program to reason about object back surfaces  tangent surfaces  small features  obscured features and non-rigidly attached subcomponents. 
this work was supported by a post-graduate studentship from the university of edinburgh. thanks also go to j.a.m. howe and r.j. beattle. 
in short  the program has four goals: 
- to locate instances of specific 1d objects in 1d images  
- to locate images features corresponding to all features of the model  or explain why the image features are not present. 
- to verify that the features are consistent with the geometrical and topological predictions made by the model  and 
- to extract the parameters needed to fully characterize an object's scene position  including any associated with flexibly connected objects. 
     coping with obscured features of objects is an important point of this work. this has required work in two areas; first  in deciding what are relevant aspects of obscured objects  ie. what features disappear and what new ones appear   and second  in extracting information about those features. this second point has required work on extracting 1d positional information from surface shapes and descriptions of surface and image regions. 
     this paper discusses the model and the reasoning rules used to analyze the main example image that of a robot upper and lower arm assembly. 
1. what does 'recognize* mean in this context 
     the meaning of  recognition  is largely dependent on the goals of the recognizer  that is. what type of output is desired for what type of input. if the only brown objects in the environment are tables  then a program could find tables by looking for brown regions. to some extent  all recognition programs are just discrimination programs  of which the table recognizer is a minimal such program. on the other hand  even the most sophisticated programs  e.g. acronym   are fundamentally the same  they attempt to find a set of evidence that is consistent with their model of the object and for which there is no contradictory evidence. the distinction is largely one of sophistication - richer models  greater embedded knowledge about object structure relationships and more detailed matchings reduce the possibilities of false recognitions. 
     in general  recognition consists of four subactions: 
- location of interesting data. 
- selection of a model. 
- instantiation and verification of model  and 
- determination of object's location and orientation. 
1 r. fisher 
in particular  this program: 
- considers all data interesting. this is mainly because the input has been previously segmented into connected surface regions. 
- considers all models this is mainly because model invocation is a research problem not addressed in this work. 
- attempts to fully instantiate the model. a model has slots for a set of structurally related surfaces and subobjects the program either fills the slots or finds evidence to explain why they cannot be filled.  this is an important requirement for an intelligent vision program.  surface slots are matched to image regions and substructure slots are matched to previously recognized objects. fully instantiated hypotheses must pass a verification process. 
- locates and orients the objects in three dimensions. the parameters come from estimating the transformation that maps a given model surface to a given image region based on the shape of the image region  and then inverting the transformation from surface to object. 
1. surfaces 
     tne project has concentrated on using surface regions as the primary data primitive. though there are no programs yet that reliably and completely give a surface segmentation  it doesn't seem like a too distant possibility. pentland's surface shape algorithm  1. various stereo algorithms  eg grimson  . optical flow  range finaer systems and structured light systems all provide the information needed to construct a sur-
ace map. 	 surface depth  shape and orientation are 
all largely equivalent representations.  
     there are many reasons for using surfaces as the primitive input data element at this level of analysis. first  irrespective of what interpretation is made  what is seen is the surface of objects. second  as surface regions are matched to model surface regions  the semantics of the relationship is simple: in unobscured situations  a six parameter geometric transformation plus a projection onto the image plane. third  the segmentation of a surface image into regions of uniform surface properties may be simpler. to some extent  any region of homogeneous image properties is likely to correspond with some subset of a uniform surface.  the reverse is not always true  as a shadow across a flat surface would create two distinct regions.  there are many open problems on this topic  such as merging split regions  scale of surface descriptions  etc fourth  surface regions lead to more accurate transformation parameter estimates  mainly because their size gives more accurate measurements . point to model matching and transformation inversion algorithms  e.g. roberts    while elegant  are sensitive to image measurement errors. worse  the loss of information from using just points makes it difficult to determine the image to model correspondences correctly. lastly  once surfaces are chosen as the desired primitive. 
other image boundary information is more useful: 
- reflectance boundaries: surface detail 
- obscuration: object limits  surface depth order 
- shadow: surface location in three dimensions 
- shape: surface relative orientation 
     the input used by the program is that of a surface image segmented into regions of either planer or simply curved orientation. hence  the data is simplified in that we know all boundaries correspond to surface or shape discontinuities. further  the boundaries form closed regions. the program could have used the surface depth or orientation information as part of its inputs  but it was decided to base the work just on the region shape. 
1 . object models 
     this work uses structural three dimensional models of the objects it expects to find the reasons for this are discussed by binford  1: a capable vision system should know about objects  and how objects are seen in images  rather than what types of images an object is likely to produce. one difference between this work and that of binford and brooks  acronym   has been the choice of primitives used to construct the models they use solid object models  generalized cones  as the fundamental building block  and then deduce what image boundaries are produced. one problem with this is that what is seen is surfaces  hence it seems reasonable that the preferential representation for a vision system would make surface-based information explicit.  this argument is largely one of conceptual simplicity  in that it should be possible to automatically deduce a surface representation from a solid representation  though not necessarily easily.  another difficulty with the generalized cone is that it seems to be a conceptually simple representation for only some object classes  though any one representation is probably not sufficient for all tasks. 
     surface primitives are constructed by specifying the boundary of the region  with the assumption that the surface is either planer or has a single axis of curvature and fills the region inside the boundary. 
     objects are described in a subcomponent hierarchy  with objects being composed of either connected surfaces or recursively defined sub-objects. the connections are specified using a six parameter attachment  x.y.z translations  rotation  slant and tilt . these attachments can be rigid  all parameters specified in the model  or may be flexible. flexible attachments are handled by assigning the parameter a symbolic and which is bound value when the corresponding subcomponent is added to the object hypothesis. symbolic parameters are only used with attachments. 
     figure 1 shows some of the model used for the example in section 1. the full model consists of the upper and lower main arms of a unimatlon puma robot. the figure here just shows some of the model for the upper arm and external linkages. figure 1 shows the part models and the coordinate frame locations. the variable  jntn  is the symbolic parameter for the robot's nth joint angle. in the example  the face primitives define the surfaces by listing key boundary points  1  and whether they are joined by a line or simple oriented curve. the assembly blocks define named structures  in terms of either other named structures or surfaces. the  at   ... . ..   portion describes the xyz translation  rotation  slant  tilt afflxment relationship between the main and sub-
     

figure 1 - part of model used for the robot arm recognition example 
1. matching model surfaces to image regions 
     this process produces the main input used by the object recognition and construction process described in section 1. its inputs are the boundary of the image region and a description of the surface as given by the model. it has the goal of estimating the transformation parameters that relates the image to the model 
     the visual motivation for this process is: the shape of an image region gives strong clues to the orientation of the surface. in particular  the program compares the image region shape to the model surface shape  using the boundaries  and attempts to extract measurements that lead simply to the desired parameters. the parameters are rotation  slant  tilt axis orientation  distance and x-y translation. the choice of rotation  slant and tilt as the orientation parameters was motivated by what information was easily found in an image of a transformed surface. the initial parameter estimation process is based on the following observations: 
- rotation and xyz translation of a surface do not alter its relative cross sectional dimensions  and that slant does not alter them drastically  over the range of estimated slants  less than 1 radians . 
- the tilt axis lies approximately at the orientation of greatest slant distortion. 
- the maximum distortion is an indication of the slant relative to the line of sight. 
- the relative scaling of features between the model and image indicates the distance. 
     the heuristics used for the parameter estimation are: 
rotation - correlation of data and model cross 
r. fisher 1 
sections. the peaks are potential rotation offsets. figure 1 shows typical model and data shapes and their corresponding cross section versus angie functions. note that the data function is largely just the translated model function. 

figure 1 - model and data cross sections 
slant and tilt - given an estimated rotation  create a ratio of maximum data to maximum model cross section widths as a function of cross section path angle.  use of the maximum is a heuristic to relate approximately corresponding cross sections.  slant  tilt and distance estimates come from the orientation of the minimum ratio  the ratio itself and the maximum ratio  figure 1 . 

figure 1 - data to model cross section ratio 
distance - given rotation  slant and tilt  compute a 
ratio of data to model cross sections along the data region's major and minor axes. two distance estimates are made from the average and most common ratios. the final value is the average of these two and the previous estimates. the major motivation for the use of three estimators is various sensitivities to obscuration or parameter mis-estimation. 
x.y translation - align data and mode centers of mass transformed by the estimated parameters 
     an optimization phase is then entered. the evaluation function is the weighted distance of the predicted model boundary from the observed image boundary. because of the expected accuracy of the various parameter estimates  optimization preference is given to the worst estimate in order of tilt  distance  slant and rotation  with the x-y translation always adjusted to best fit at each stage. 
     note that the transformation relating a modeled surface to an image region is not always unique  especially when model symmetries  spatial quantization and inaccuracies of fitting numerical data are considered. consequently  the parameter estimation process may produce more than one hypothesis. 
     
1 r. fisher 
rule 1:  object hypothesis   -  surface hypothesis  
     
     these heuristics gave reasonable parameter estimates in 1 of 1 test cases recently. given the tolerances in the matching process  the estimates are acceptable in the successful cases. unfortunately calculating these parameters is largely numerical and computationally expensive - mainly because of image parameter measurement and model-to-data boundary comparisons during the optimization phase. 
1. some rules for recognizing objects 
	here  recognition is a construction process. 	at 
each stage  a hypothesized structure is under consideration. this hypothesis may have several empty substructure slots  in which case the program will attempt to fill the slots in various ways or. the slots may all be full  in which case the hypothesis will be subjected to a verification process. finally  the structure may be used to hypothesize another structure 
     currently  the processing of the hypotheses is exhaustive  and is controlled by a first-in. first-out queue. the selection of which procedure to apply to a hypothesis is determined by a set of rules compiled from the set of meta-rules discussed below. 
some of the rules have binary right-hand-sides. 
when the matcher considers applying such a rule to a hypothesis  a prediction routine is called. using the object models  this routine predicts where in the image the second hypothesis needed for the matching might be found  c.f. freuder's advice process  . then  all hypotheses of the appropriate type from this location is paired with the current hypothesis node and the analysis routine is invoked for each pairing. 
     the general processing philosophy is for the rules to produce all hypotheses consistent with their definition. with the assumption that incorrect hypotheses will not be capable of full consistent instantiation. 
     about two dozen rules exist at the current time. the point of having different rules  rather than a general mechanism  is that each can carry out the specific type of reasoning needed to solve a particular problem. the rules described below are some of those used in the example given in section 1: 
rule 1:  surface hypothesls   -  lmage reglon  
     this rule selects  among all model surfaces  those that meet various pre-screening conditions relative to the particular image region  mainly minimum size relationships . then  the parameter estimation process  section 1  is applied for each surface candidate. a surface hypothesis is generated for each suitable estimate. 
rule 	1: 	 oblect 	hypothesls  	 - 	 ob|ect 	hypothesls  
 surface hypothesis  
     this rule adds a new surface to an existing structure hypothesis. for the rule to apply  the global location of the new hypothesis predicted by the new surface has to be consistent with the previous hypothesis. the resulting hypothesis has all the previous information transferred. including variable bindings. 
     this rule produces a structure hypothesis for each modeled object that has the particular face as a rigidly connected subcomponent. the location of the hypothesis is calculated by taking the location of the face and inverting the attachment relationship to the main structure  figure 1 . 

figure 1 - surface hypothesis to object hypothesis rule 
rule 1:  object hypothesis   -   object hypothesis  
     this rule fills slots corresponding to surfaces invisible because they lie on the back sides of the object. this is done by using the object model to predict the surface orientation  given the hypothesis's location and orientation. 
rule 1:  object hypothesls   -  object hypothesis  
     this rule fills slots that correspond to near tangential surfaces on the front side of an object. because of the severe slant distortion of such surfaces  parameter estimation is unreliable. hence  the slots are filled with references to image regions that lie approximately in the right location and have approximately the correct size. 
rule 1:  verified object   -  object hypothesis  
this rule is discussed in detail in section 1. 
rule 	1: 	 object 	hypothesis  	 - 	 object 	hypothesis  
 bound verified subobjeci  
     this rule adds a new rigidly attached subassembly to the current hypothesis. the location of the main assembly predicted by the new subassembly must be consistent with the previous hypothesis. 
rule 1:  object hypothesis   -  object hypothesis  
 bound verified subobject  
     this rule adds a new flexibly attached subassembly to the current hypothesis. the location of the new subassembly  with the previous hypothesis's loca-
tion and the model attachments  allows the calculation of the attachment relation that must exist between the structure and subassembly. if this is consistent with the explicitly modeled parameters  then the match is acceptable. any remaining symbolic parameters are then bound to the corresponding calculated values  in the context of the resulting hypothesis. 
1. verification and description 
when an object hypothesis is fully instantiated. 
     
the verification rule is applied. this is necessary because the hypothesis construction rules are tolerant in their checking  e.g. location and orientation parameter consistency  and allow a few unlikely fully instantiated hypotheses to be formed. the major discrepancies noted were: use of one image region to support several distinct surface hypotheses and the use of widely separated image regions to support adjacent surface hypotheses. 
     part of the verification phase needs a description of the surface-to-image match. two descriptions are created  that of the surface hypothesis as supported by the image region data  and that of the image region as accounted for by the projected surface hypothesis. the descriptions are given by identifying segments of the boundary of either the image or projected model surface region in correspondence with the description of the surface database model  example below . this correspondence is found by: 
1. taking the boundary of the image region and calculating a similar boundary for the projected surface region  with segments identified by the model description labels   
1. identifying corresponding segments in the two boundaries by overlaying them and marking unlabeled segments  and 
1. creating a description of the correspondence by recording segment identifiers and segment endpoint locations  with some minor editing . 
     in the robot image example  section 1 . there is a partially obscured surface in the robot lower arm assembly. in figure 1. we see how the model and image data are used to give the two descriptions. 

figure 1 - description of image and matched surface boundaries 
     these descriptions are used partly in the general verification process  and partly for the handling of obscuration  section 1 . 
     the total verification process can then be summarized as: 
1. check for ail slots filled 
1. ensure that all surfaces declared as back facing still are  necessary because of parameter adjustments  
r. fisher 1 
1. ensure against duplicate use of image regions 
1. re-optlmlze parameter estimates  given better averages estimates as the initial inputs  
1. describe visible surfaces  as above  
1. for all visible surfaces predicted to have shared boundaries  by the model   ensure that the portions of the surface descriptions corresponding to the shared boundary are compatible  i.e. overlap  
1. ensure that the object's final predicted boundaries coincide well with the observed boundaries. this includes the obscuration analysis discussed in section 1. 
1. coping with partially obscured objects 
     any reasonable three dimensional scene analysis program has to cope with obscuration: objects always have backsides that the model predicts  but are never seen. there are two other forms of obscuration: the object may have self-obscured front surfaces  from either rigidly or flexibly attached subcomponents being closer to the viewer  and the object may be obscured by external  unrelated objects in the scene. 
     in the discussion below  only surfaces are considered. this is because the analysis has focussed on the use of surface descriptions  and because the objects can be described  directly  or recursively  in terms of their surfaces. self-obscured surfaces are those obscured by other surfaces of the same object. external surfaces are those on unrelated objects. 
     knowledge of the models  surfaces and the image formation process is the ultimate source of the information needed to solve obscuration problems. here  the initial use of this knowledge is during .surface description. major deviations of the image boundary from the predicted model boundary are hypothesized to result from obscuring boundaries. the second usage is during object construction. if surfaces are only slightly obscured  then the model instantiation process proceeds normally. if they are greatly obscured  then the parameter estimation techniques will not be successful. then  the presence of a likely image region is considered adequate at this point. 
     the backside surface case is easily handled by examining the predicted surface normal. 
     completely obscured surfaces require evidence of closer surfaces  to support their hypothesized existence. for self-obscured surfaces  the evidence comes from the prediction of other object surfaces. evidence for external surfaces must come from elsewhere. this case has not been implemented yet. possible evidence may come from: other verified object hypotheses or low level feature cues  e.g. as in binford  . 
     partially obscured surfaces get the greatest analysis. because they are partially visible  they fill slots in the object hypothesis. as mentioned above  to do this  the hypothesis formation constraints are weakened. unfortunately  weakening the constraints leads to the possibility of incorrect hypotheses being formed. the major burden of coping with this and obscuration in general is in the verification process 
 rule 1 . 	in addition to the verifications described in 
     
1 r. fisher 
section 1. the routine has four obscuration-specific tasks: 
- to ensure that points where the data description becomes unlabeled are points where three  or more  regions meet  figure 1 . these regions are: object surface  obscuring surface and background surface. this is a variation of the tee test for obscuring surfaces  except that it has advantages in cases of coincidental alignments  in that it is based on surface ordering properties  rather than boundary topology. 

figure 1  1.three surfaces at unlabeled segments 
to ensure that unlabeled model boundaries are consistent with predictions of obscuring surfaces  figure 1 . in particular  this means that the predicted missing boundary  from the model description  must lie completely outside the associated image region the hypothesis can be strengthened in the self-obscured case by showing that the missing segment lies within a region corresponding to a closer surface. 

figure 1 unlabeled model boundaries 
to verify that unlabeled data description boundaries are consistent with obscuring surfaces  figure 1 . that is  the unlabeled data boundary must lie completely within the region predicted for the surface. again  the hypothesis can be strengthened in the self-obscured case by showing that the unlabeled boundary lies on the boundary of a closer object surface. 

figure 1 unlabeled data boundaries 
- to verify that boundaries of undescribed regions coincide with predicted model boundaries 

figure 1 - segmented surface test 
this type of reasoning complements that proposed by binford   in that this uses the depth ordering properties of surfaces to provide clues for hypothesis verification. his approach uses similar reasoning for 
hypothesis formation. 
1 example 
     figure 1 shows a picture of a model of the upper and lower arm assembly of a unimation puma robot  with the boundaries of the hand-segmented surface regions. this image was used as the basis for the results discussed in this section. the location of the model's external coordinate frame origin and the value of the joint angle parameter are given below. the global coordinate system is centered at the focal point  with x horizontal. y vertical and z away from the viewer. all distances are in centimeters and angles are in radians 

figure 1 hand segmented region boundaries on raw image 
     this image was hand segmented to give the major surface regions in the field of view. the segmented image  the model shown in figure 1 and the meta-rules  e.g section 1  were then used as the external inputs into the program. the output of the program is a database of the hypotheses at various stages of instantiation. the only fully instantiated robot hypothesis was the correct one. for that solution  the global coordinate and the corresponding joint angle parameters were: 
     
x:-1 rotation:1 jntl:1 y:1 slant:1 jnt1.1 z:1 tilt:1 jnt1.1      it is felt that the numerical solutions given by the program are reasonable.  the large tilt angle estimate discrepancy occurs because of the essentially zero slant estimate.  it is not apparent  but the results at various intermediate stages are often a lot worse. the accuracy results from averaging estimates from the six visible surfaces of the robot. the predicted surface boundaries of the final robot hypothesis node superposed over the raw image are shown in figure 1  the obscured edge is not removed due to lack of a front-surface hidden line remover.  the picture shows that the numerical estimates are approximately correct. the fitting of model surfaces to data regions aligned boundaries well  but when the parameters estimates from the various surfaces are combined  the boundary fit deteriorates  which shows in this figure. 

figure 1 - edges of located robot on raw image 
1. conclusions and criticisms 
     the results from section 1 show that the program successfully identified the robot assembly and made reasonable estimates of its location  orientation and joint angle parameters. this can be attributed partly to the right choice of input data  i.e. surface regions and 1d object models   and partly to the types of reasoning done at the various stages of analysis. 
the program can be criticized on several points. 
first. its image surfaces must be largely planer for the current parameter estimation techniques to succeed. second  the modeling of surfaces doesn't account for the surface shape internal to the region boundary. third  the program doesn't take advantage of the fact that surface segmentations will probably provide two or three parameter estimates directly  slant.tilt.distance . fourth  its models are non-generic  unlike acronym   and there is no simple mechanism  as yet  available for providing them. fifth  its current dependence on hand-segmented images  while necessary at present  makes the program difficult to adequately evaluate. sixth  the model invocation process currently considers matching all surface regions to all reasonable surface models. because any reasonable model base and image will have many surfaces  a 
r. fisher 1 
more powerful technique is needed. seven. its reasoning is much more oriented to structural  than to visual understanding. the two major uses of image data were in the parameter extraction and the structure verification processes  with the rest of reasoning devoted to matching according to the object models. eight  it's reasoning is largely undirected and thus leads to having a heavy computational cost. finally  because it uses an exact parameter matching technique  with tolerance  threshold  based matching  rather than convergence of parameter ranges  c.f. acronym   it is subject to complete failures when parameter estimates are inaccurate. however  even though generous tolerances were needed  the process was selective enough to only allow a few hypotheses to get close to the verification stage. 
     on the other hand  because it uses a more reasonable semantic primitive  surfaces  and because it uses constructive  task-specific reasoning  it can more completely recognize complex objects. in particular  the program successfully: 
- located and identified a 1d object in a 1d image 
- used surface regions and 1d object models to guide the process. 
- extracted the 1d positional information. including the joint angles of a flexible assembly. 
- fully instantiated and explained the model of the robot assembly  and 
- handled some cases of self-obscuration correctly. 
it is felt that the work described in the paper 
describes advances int the following areas: 1d parameter estimation from surface image region to model region matching. 
- successful recognition of articulated objects  and 
- better understanding of handling obscured surfaces. 
1. 