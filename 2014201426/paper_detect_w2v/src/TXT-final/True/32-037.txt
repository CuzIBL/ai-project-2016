 
one of the most intuitive ideas for enhancing the effectiveness of an information retrieval system is to include the use of a thesaurus. wordnet  as a hand-crafted and general-purpose thesaurus  intuitively should also work fine in information retrieval  but unfortunately  experimental results by many researchers have not been promising. thereby in this paper we investigate why the use of wordnet in information retrieval has not been successful. based on this analysis we propose a method to combine wordnet with predicateargument-based and co-occurrence-based automatically constructed thesauri. experiments using large test collection shows that our method results in a significant improvement of information retrieval performance. 
1 	introduction 
the task of the information retrieval system is to match the query against the document collection and return relevant documents to the user. retrieval performance is usually express in terms of recall  the proportion of relevant document retrieved  and precision  the proportion of retrieved document that is relevant. whereas a perfect retrieval run will have a value of 1 for both recall and precision  in practice precision and recall are inversely related. 
　a critical problem in information retrieval is the case of different words being used to describe the same thing  either in queries or in documents. this require some kind of knowledge base which can equate or relate the terms used in language  i.e.  a thesaurus. a thesaurus is a data structure which groups synonymous terms and relates them as either broader or narrower. a thesaurus can be used to expand a query to include all synonymous or related terms. this method has been known as query expansion method  schutze and pederson  1 . 
　wordnet is currently the most large  hand-crafted  general-purpose  machine-readable  and publically available thesaurus. it is the product of a research project at princeton university which has attempted to model the 
1 	natural language processing 
lexical knowledge of english  miller  1 . wordnet has been used in numerous natural language processing  such as semantic tagging  segond et al.  1   word sense disambiguation  resnik  1a   text categorization  gomez-hidalgo and rodriguez  1   information extraction  chai and biermann  1   and so on with considerable success. however  the use of wordnet in information retrieval have not been very successful. 
　two sets of experiments using the trec collection were performed to investigate the effectiveness of using wordnet for query expansion by voorhees . the first set used handpicked synsets and the second set extends the expansion strategy to include automatically selecting the starting synsets. when the concepts were chosen manually  her method could improve the retrieval effectiveness for short queries  but failed to improve the retrieval effectiveness for long queries. when the concepts were chosen automatically  none of the expansion methods produced significant improvement as compared with an unexpanded run. she further tried to use wordnet as a tool for word sense disambiguation  voorhees  1  and applied it to text retrieval  but the performance of retrieval was degraded. 
　stairmand  used wordnet to investigate the computational analysis of lexical cohesion in text using lexical chain method  morris and hirst  1 . because lexical chains are associated with topics  he suggested that information retrieval  where the notion of topic is very pertinent  is a suitable application domain. he concluded that his method only succeed in small-scale evaluation  but a hybrid approach is required to scale-up to real-word information retrieval scenarios. 
　smeaton and berrut  tried to expand the queries of the trec-1 collection with various strategies of weighting expansion terms  along with manual and automatic word sense disambiguation techniques. unfortunately all strategies degraded the retrieval performance. 
　instead of matching terms in queries and documents  richardson  used wordnet to compute the semantic distance between concepts or words and then used this term distance to compute the similarity between a query and a document. although he proposed two methods to compute semantic distances  neither of them increased the retrieval performance. 

1 	limitations of wordnet 
in this section we analyze why wordnet has failed to improve information retrieval performance. we ran exactmatch retrieval against 1 small standard test collections  fox  1  in order to observe this phenomenon. an information retrieval test collection consists of a collection of documents along with a set of test queries. the set of relevant documents for each test query is also given  so that the performance of the information retrieval system can be measured. we expand queries using a combination of synonyms  hypernyms  and hyponyms in wordnet. the results are shown in table 1. 
in table 1 we show the name of the test collection 
 collection   the total number of documents  and queries and all relevant documents for all queries in that collection. for each document collection  we indicate the total number of relevant documents retrieved  rel-ret   the recall  the total number of documents retrieved  ret-docs   and the precision  for each of no expansion  base   expansion with synonyms  exp. i   expansion with synonyms and hypernyms  exp. ii   expansion with synonyms and hyponyms  exp. ill   and expansion with synonyms  hypernyms  and hyponyms  exp. iv . 
　from the results in table 1  we can conclude that query expansion can increase recall performance but unfortunately degrades precision performance. we thus turned to investigation of why all the relevant documents could not be retrieved with the query expansion method above. some of the reasons are stated below : 
  two terms that seem to be interrelated have differ-ent parts of speech in wordnet. this is the case between stochastic  adjective  and statistic  noun . since words in wordnet are grouped on the basis of part of speech in wordnet  it is not possible to find a relationship between terms with different parts of speech. 
  most of relationships between two terms are not found in wordnet. for example how do we know that sumitomo bank is a japanese company   
  some terms are-not included in wordnet  proper name  etc . 
　to overcome all the above problems  we propose a method to enrich wordnet with an automatically constructed thesaurus. the idea underlying this method is that an automatically constructed thesaurus could complement the drawbacks of wordnet. for example  as we stated earlier  proper names and their interrelations among them are not found in wordnet  but if proper names and other terms have some strong relationship  they often co-occur in the document  so that their relationship may be modeled by an automatically constructed thesaurus. 
　polysemous words degrade the precision of information retrieval since all senses of the original query term are considered for expansion. to overcome the problem of polysemous words  we apply a restriction in that queries are expanded by adding those terms that are most similar to the entirety of query terms  rather than selecting terms that are similar to a single term in the query. 
1 	method 
in this section  we first describe the construction method for each type of thesaurus utilized in this research  and then describe a term weighting method using similarity measure based on these thesauri. 
1 	w o r d n e t 
in wordnet  words are organized into taxonomies where each node is a set of synonyms  a synset  representing a single sense. there are 1 different taxonomies based on parts of speech and also there are many relationships defined within it  fellbaum  1 . in this experiment we use only noun taxonomy with hyponymy/hypernymy  or the is-a relation   which relates more general and more specific senses. 
　the similarity between word w1 and w1 is defined as a shortest path from each sense of w1 to each sense of w1  as below  resnik  1b : 
where np is the number of nodes in path to w1 and d is the maximum depth of the taxonomy. 
1 	co-occurrence-based thesaurus 
the general idea underlying the use of term cooccurrence data for thesaurus construction is that words that tend to occur together in documents are likely to have similar  or related  meanings  qiu and frei  1 . co-occurrence data thus provides a statistical method for automatically identifying semantic relationships that are normally contained in a hand-made thesaurus. suppose two words  a and b  occur and times  respectively  and co-occur  times  then the similarity between a and b can be calculated using a similarity coefficient such as the tanimoto coefficient : 

1 	predicate-argument-based thesaurus 
in contrast with the previous section  this method attempts to construct a thesaurus according to predicateargument structures  hindle  1; grafenstette  1; ruge  1 . the use of this method for thesaurus construction is based on the idea that there are restrictions on what words can appear in certain environments  and in particular  what words can be arguments of a certain predicate. for example  a dog may walk  bite  but can not fly. each noun may therefore be characterized according to the verbs or adjectives that it occurs with. 
	mandala  tokunaga  and tanaka 	1 

table 1: term expansion experiment results using wordnet 
| collection #doc #query #rel base exp. i exp. 1 exp. i l l ! exp. iv adi 1 1 ! 1 rel-ret 1 1 1 1 1 recall 1 1 1 1 1 j ret-docs 1 1 1 1 ! 	1 precision 1 1 1 1 1 1 cacm ! 	1 1 1 rel-ret 1 1 1 1 '   1 recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1jcisi 1 1 1 rel-ret 1 1 1 1 1 1 recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1jcran 1 1 1 rel-ret 1 1 1 1 1 recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1j inspec 1 1 1 rel-ret 1 1 1 1 1 recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1j| lisa 1 1 1 rel-ret 1 1 1 1 1 recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1jmed 1 1 1 rel-ret 1 1 1 1 1  recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1jnpl 1 1 1 rel-ret 1 1 1 1 1 recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1 time 1 1 1 rel-ret 1 1 1 1 1recall 1 1 1 1 1 ret-docs 1 1 1 1 1 precision 1 1 1 1 1 
nouns may then be grouped according to the extent to which they appear in similar constructions. 
   first  all the documents are parsed using the apple pie parser  which is a probabilistic chart parser developed by sekine and grishman . then the following syntactic structures are extracted : 
  subject-verb 
  verb-object 
  adjective-noun 
   each noun has a set of verbs and adjective that it occurs with  and for each such relationship  a tanimoto coefficient value is calculated. 
curring as the subject of verb is the frequency of the noun rij occurring as subject of any verb  and is the frequency of the verb  
1 	natural language processing 
oc- 
curring as the object of verb quency of the noun nj occurring as object of any verb  and f vi  is the frequency of the verb  
where is the frequency of noun occurring as argument of adjective is the frequency of the noun occurring as argument of any adjective  and is the frequency of the adjective 

   we define the similarity of two nouns with respect to one predicate as the minimum of each tanimoto coefficient with respect to that predicate  i.e.  


　finally the overall similarity between two nouns is defined as the average of all the similarities between those two nouns for all predicate-argument structures. 
1 	expansion t e r m weighting m e t h o d 
a query q is represented by a vector  where the  are the weights of the search terms ti contained in query q. 
　the similarity between a query q and a term  can be defined as belows  qiu and prei  1 :  

　where the value of sim{ti tj  can be defined as the average of the similarity values in the three types of thesaurus. 
　with respect to the query g  all the terms in the collection can now be ranked according to their simqt. expansion terms are terms tj with high  
　the weight q tj  of an expansion term tj is defined as a function of simqt q tj : 
where 1 
　an expansion term gets a weight of 1 if its similarity to all the terms in the query is 1. expansion terms with similarity 1 to all the terms in the query get a weight of 1. the weight of an expansion term depends both on the entire retrieval query and on the similarity between the terms. the weight of an expansion term can be interpreted mathematically as the weighted mean of the similarities between the term tj and all the query terms. the weight of the original query terms are the weighting factors of those similarities. 
　therefore the query q is expanded by adding the following query : 
where a j is equal to weight 	belongs to the top r ranked terms. otherwise a j is equal to 1. the resulting expanded query is : 

where the o is defined as the concatenation operator. 
　the method above can accommodate the polysemous word problem  because an expansion term which is taken from a different sense to the original query term is given very low weight. 
1 	experimental results 
in order to evaluate the effectiveness of the proposed method in the previous section we conducted experiments using the trec-1 information retrieval test collection  voorhees and harman  to appear 1 . trec-1 documents consists of the financial times  ft   federal register  fr1   foreign broadcast information service  fbis  and the la times. table 1 gives its document statistics  table 1 give topic statistics  and table 1 is one example out of 1 topics. as a baseline we used smart  salton  1  without expansion. smart is an information retrieval engine based on the vector space model in which term weights are calculated based on term frequency  inverse document frequency and document length normalization. the results are shown in table 1. this table shows the average of non-interpolated recall-precision for each of baseline  expansion using only wordnet  expansion using only predicate-argument-based thesaurus  expansion using only co-occurrence-based thesaurus  and expansion using all of them. for each method we give the percentage of improvement over the baseline. it is shown that the performance using the combined thesauri for query expansion is better than both smart and using just one type of thesaurus. 
table 1: trec-1 document statistics 
source size  mb  # docs  median # words/doc mean # 	1
words/doc disk 1 ft 1 1 1 1 fr1 1 1 1 1 disk 1 fbis 1 1 1 1 la times 1 1 1 1 table 1: trec-1 topic length statistics 
topic section min max mean title 1 1 1 description 1 1 1 narrative 1 1 1 all 1 1 1 | title : journalist risks 
description : 
identify instances where a journalist has been put at risk  e.g.  killed  arrested or taken hostage  in the performance of his work. 
narrative : 
any document identifying an instance where a journalist or correspondent has been killed  arrested or taken hostage in the performance of his work is relevant. 
figure 1: topic example 
	mandala  tokunaga  and tanaka 	1 

table 1: average non-interpolated precision for expansion using combined thesaurus and expansion using only one type of thesaurus. 
topic type base expanded w i t h wordnet only pred-arg only co-occur only combined title 1 　1  +1%  　1  +1%  　1  +1%     1  +1%    !desc 1 　1  +1%  　1  +1%  　1  +1%  　1  +1%  all 1 　1  +1%  　1  +1%  　1  +1%  	1 	|
 +1%  1 	discussion 
in this section we discuss why our method of using wordnet is able to improve the performance of information retrieval. the important points of our method are : 
  the coverage of wordnet is broadened 
  weighting method 
   the three types of thesaurus we used have different characteristics. automatically constructed thesauri add not only new terms but also new relationships not found in wordnet. if two terms often co-occur together in a document then those two terms are likely bear some relationship. why not only use the automatically constructed thesauri   the answer to this is that some relationships may be missing in the automatically constructed thesauri  grafenstette  1 . for example  consider the words tumor and tumour. these words certainly share the same context  but would never appear in the same document  at least not with a frequency recognized by a co-occurrence-based method. in general  different words used to describe similar concepts may never be used in the same document  and are thus missed by the co-occurrence methods. however their relationship may be found in the wordnet thesaurus. 
　the second point is our weighting method. as already mentioned before  most attempts at automatically expanding queries by means of wordnet have failed to improve retrieval effectiveness. the opposite has often been true: expanded queries were less effective than the original queries. beside the  incomplete  nature of wordnet  we believe that a further problem  the weighting of expansion terms  has not been solved. all weighting methods described in the past researches of query expansion using wordnet have been based on  trial and error  or ad-hoc methods. that is  they have no underlying justification. 
the advantages of our weighting method are: 
  the weight of each expansion term considers the similarity of that term with all terms in the original query  rather than to just one or some query terms. 
  the weight of the expansion term accommodates the polysemous word problem. 
this method can accommodate the polysemous word problem  because an expansion term taken from a different sense to the original query term sense is given 
1 	natural language processing 
very low weight. the reason for this is that  the weighting method depends on all query terms and all of the thesauri. for example  the word bank has many senses in wordnet. two such senses are the financial institution and the river edge senses. in a document collection relating to financial banks  the river sense of bank will generally not be found in the co-occurrence-based thesaurus because of a lack of articles talking about rivers. even though  with small possibility  there may be some documents in the collection talking about rivers  if the query contained the finance sense of bank then the other terms in the query would also concerned with finance and not rivers. thus rivers would only have a relationship with the bank term and there would be no relationships with other terms in the original query  resulting in a low weight. since our weighting method depends on both query in its entirety and similarity in the three thesauri  the wrong sense expansion terms are given very low weight. 
　we also experimented this method using other similarity coefficient method and roget thesaurus  and found significant improvement of retrieval performance although the contribution of roget thesaurus is very limited  mandala et al.  to appear 1 . 
1 	conclusion 
this paper analyzed why the use of wordnet  a large and hand-made publically available thesaurus was not so succesful in improving the retrieval effectiveness in information retrieval applications. we found that the main reason is that most relationships between terms are not found in wordnet  and some terms  such as proper names  are not included in wordnet. to overcome this problem we proposed a method to enrich the wordnet with automatically constructed thesauri. 
　another problem in query expansion is that of polysemous words. instead of using a word sense disambiguation method to select the appropriate sense of each word  we overcame this problem with a weighting method. experiments proved that our method of using wordnet in query expansion could improve information retrieval effectiveness. 
　in the future  we will use anaphora resolution to accurately determine the nature of relationships involving proper names. we will also investigate the effect of using different similarity coefficient method to build thesauri to the retrieval performance. 
1 	acknowledgements 
the authors would like to thank mr. timothy baldwin 
 tit  japan  and three anonymous reviewers for useful comments on the earlier version of this paper. we also thank to dr. chris buckley  sablr research  for the smart support  and dr. satoshi sekine  new york university  for providing the apple pie parser program. 
this research is partially supported by the jsps project number jsps-rftf1. 

