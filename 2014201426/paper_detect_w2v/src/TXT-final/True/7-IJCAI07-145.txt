
support vector machines  svm  have been highly successful in many machine learning problems. recently  it is also used for multi-instance  mi  learning by employing a kernel that is defined directly on the bags. as only the bags  but not the instances  have known labels  this mi kernel implicitly assumes all instances in the bag to be equally important. however  a fundamental property of mi learning is that not all instances in a positive bag necessarily belong to the positive class  and thus different instances in the same bag should have different contributions to the kernel. in this paper  we address this instance label ambiguity by using the method of marginalized kernels. it first assumes that all the instance labels are available and defines a label-dependent kernel on the instances. by integrating out the unknown instance labels  a marginalized kernel defined on the bags can then be obtained. a desirable property is that this kernel weights the instance pairs by the consistencies of their probabilistic instance labels. experiments on both classification and regression data sets show that this marginalized mi kernel  when used in a standard svm  performs consistently better than the original mi kernel. it also outperforms a number of traditional mi learning methods.
1 introduction
in supervised learning  each training pattern has a known class label. however  many applications only have weak label information and thus cannot be formulated as supervised learning problems. a classic example as discussed by  dietterich et al.  1  is drug activity prediction  where the task is to predict whether a drug molecule can bind to the targets  enzymesor cell-surfacereceptors . a moleculeis considered useful as a drug if one of its conformations can bind to the targets. however  biochemical data can only tell the binding capability of a molecule  but not a particular conformation. in other words  we only have class labels associated with sets of patterns  bags   instead of the individual patterns  instances . dietterich et al.called this multi-instance  mi  learning. another well-known mi application is content-based image retrieval  chen and wang  1   where each image is a bag and each local image patch an instance.
¡¡following dietterich et al.'s seminal work  a number of new mi learning methods  such as the diverse density  dd  algorithm  maron and lozano-perez  1   have emerged. here  we will focus on methods based on the support vector machines  svm   which have been highly successful in many machine learning problems. there are two main approaches to extend the standard svm for mi data. one is to modify the svm formulation  as in  andrews et al.  1; cheung and kwok  1 . however  unlike the standard svm  they lead to non-convex optimization problems which suffer from local minima. another approach is to design kernels directly on the bags  ga¡§rtner et al.  1 . these socalled mi kernels can then be used in a standard svm. as the instance labels are unavailable  the mi kernel implicitly assumes all instances in the bag to be equally important. however  a central assumption in mi learning is that not all the instances in a positive bag are necessarily positive. thus  many of these instances should have small contributions and the assumption made by the mi kernel is very crude.
¡¡recall that the major difference between mi learning and traditional single-instance learning lies in the ambiguity of the instances labels. if the instance labels were available  then the mi problem could be solved much more easily. a related idea is explored in the em-dd algorithm  zhang and goldman  1 . in each bag  the instance that is most consistent with the current hypothesis is selected as the representative of the whole bag. this converts the multiple-instance data to single-instance data and leads to improved speed and accuracy over the dd algorithm. however  it implicitly assumes that there is only one positive instance in each positive bag. in practice  both dd and em-dd are often inferior to kernel-based mi algorithms  andrews et al.  1; cheung and kwok  1 .
¡¡in this paper  we address the ambiguity of instance labels in the design of mi kernels by using the method of marginalized kernels  tsuda et al.  1 . similar to the expectationmaximization  em  algorithm  it transforms an incomplete data problem  with only the observed data  to a complete data problem  with both the observed and hidden data   which is then often easier to solve. this method has been successfully used to define marginalized kernels for strings   tsuda et al.  1   trees and graphs  mahe¡ä et al.  1 .
¡¡the rest of this paper is organized as follows. section 1 first gives a briefintroductionto the marginalizedkernel. section 1 then describes the proposed marginalized kernel for mi classification  which is followed by an extension to mi regression in section 1. experimental results on a number of classification and regression data sets are presented in section 1  and the last section gives some concluding remarks.
1 marginalized kernels
like the em algorithm  the data are assumed to be generated by a latent variable model  with observed variable x and hidden variable ¦È. the task is to define a  marginalized  kernel between two observed variables x1 and x1. with the help of the hidden variables  one can first define a joint kernel kz z1 z1  over the pairs z1 =  x1 ¦È1  and z1 =  x1 ¦È1 . as the hidden information is indeed unobserved  the posterior distribution of ¦È1 ad ¦È1 are obtained by some probabilistic model p ¦È|x   which in turn is estimated from the data. the marginalized kernel is then obtained by taking expectation of the joint kernel w.r.t. the hidden variables  as: k x1 x1  = x1 p ¦È1|x1 p ¦È1|x1 kz z1 z1 d¦È1 d¦È1   1 
¦È1 ¦È ¡Ê¦¨
 where ¦¨ is the domain of the hidden variables. when the hidden variable is continuous  the summation is replaced by an integration. note that computing  1  may be intractable when ¦¨ is large  and so this is an important issue in designing marginalized kernels.
1 marginalized kernels for mi classification
in mi classification  we are given a set of training bags
{ b1 y1  ...  bm ym }  wherex bi = {xi1 ... xini} is the ith bag containing instances ij's  and yi ¡Ê {1}. for each bag bi  the observed information is then the associated xij's while the hidden information is the unknown instance labels   where cij ¡Ê {1}.
1 the joint kernel
the joint kernel is defined on two combined variables z1 =
 b1 c1  and z1 =  b1 c1 . it can thus utilize information on both the input  xij's  and instance labels  ci . note that while traditional kernels  such as the polynomial and gaussian kernels  are defined on the input part only  recent results show that the use of label information can lead to better kernels  e.g   cristianini et al.  1  .
in this paper  we define the joint kernel as:
	 	 1 
where kx ¡¤ ¡¤  and kc ¡¤ ¡¤  are kernels defined on the input and label parts of the instances  respectively. a simple and reasonable definition of kc is1
             kc c1i c1j  = i c1i = c1j    1  where i ¡¤  returns 1 when the predicate is true  and 1 otherwise. in this case   1  is also equal to the alignment between kernel kx and the instance labels  cristianini et al.  1 . a high alignment thus implies a high kernel value  or similarity  between b1 and b1.
1 marginalizing the joint kernel
to obtain the marginalized kernel  we take expectation of the joint kernel in  1  w.r.t. the hidden variables c1 and c1  as: |	|
c1 c1
computation of the conditional probability p ci|bic  will be postponed to section 1. note that even when p  i|bi  is known  a direct computation of  1  is still computationally infeasible  and takes ¦¸ 1+n1  time  as ci ¡Ê {1}ni.
¡¡with the joint kernel defined in  1   k b1 b1  in  1  can be simplified as:

c1 c1	i=1 j=1 n1 n1
= x=1x=1 kx x1i x1j  1 {kc c1i c1j 
	i	j	c i c j
¡¤ x1p c1i c1   c1i|b1 cx1 p c1j c1   c1j|b1 } 
	c1 c i	 c j
 where ci   cij =  ci1 ... ci j 1 c. using  and the conditional
independence assumption that p cij|bi  = p cij|xij   k b1 b1  can be reduced to
n1 n1
x=1x=1x1 kx x1i x1j kc c1i c1j p c1i|x1i p c1j|x1j .	 1 
i	j	c i c j
 as will be shown in section 1  this can now be computed in polynomial time. in particular  on using  1  as the instance label kernel  k b1 b1  is simply
 
which is very intuitive. finally  to avoid undesirable scaling problems  we normalizethe kernel as in  ga¡§rtneret al.  1 :
.
note that  1  reduces to the mi kernel k bi bj  =
  ga¡§rtner et al.  1  if kc ¡¤ ¡¤ 
is constant. thus  while  ga¡§rtner et al.  1  assumes that all the instance pairs between bi and bj are equally important  kc in  1  weights them differently according to the consistency of their probabilistic labels. moreover  compared to em-dd which chooses only one representative instance from each bag during inference  here we perform marginalization over all possible instance labels and is thus more consistent with the bayesian framework.
1 defining the conditional probabilities
in this section  we consider how to obtain the conditional probability p ci|bi . as mentioned in  tsuda et al.  1   an advantage of the marginalized kernel approach is that the definitions of the joint kernel and probabilistic model are completely separated. thus  one has a lot of freedom in picking a good probabilistic model.
using the probabilistic model for diverse density
motivated by the success of the dd algorithm in mi learning  maron and lozano-perez  1   we first consider using its probabilistic model for estimating p cij|xij . the dd algorithm finds a hypothesis h over the whole instance space1 x by maximizing the following dd function:
here  bi+ and bi  index all the positive and negative bags in the training set  respectively. intuitively  h has a high dd value if all positive bags have instances close to h  while negative instances from all the negative bags are far away from h. under this model  we can then define
	p h|d  = dd h /z 	 1 
where z is a normalizing factor such that 
	|	 1 
and p cij = 1|xij  = 1   p cij = 1|xij .
¡¡however  the dd function is highly nonlinear with many local minima. hence  instead of using only one h obtained from the optimization process  it is often advantageous to use multiple hypotheses  chen and wang  1 . we then have1
		 1 
where the summation is over the set of hypotheses obtained. note that  1  automatically weights each hypothesis by its likelihood. on the contrary  the dd-svm in  chen and wang  1  has no such weighting and has to rely on additional heuristics to filter away the less important hypotheses. substituting all these equations into  1   we finally have
n1 n1
k b1 b1 =x=1x=1 x1 1 kx x1i x1j kc c1i c1j 
	i	j	h  h c i c j
p h1|d p h1|d p c1i|x1i h1 p c1j|x1j h1 .  1 
the complete algorithm is shown in algorithm 1.
¡¡very recently   rahmani and goldman  1  proposed a graph-based mi semi-supervised learning method  where the edge weight between bags b1 and b1 is roughly equal to1
	.	 1 
 this is quite similar to  1 . however  in general   1  is not positive semi-definite and so not a valid kernel function.
algorithm 1 marginalized mi kernel for classification.

input: training set d; a pair of bags b1 and b1
output: k b1 b1 
1: run the dd algorithm with different initializations and store the hypotheses obtained in the array h.
1: for all h ¡Ê h do
compute p h|d  using  1  or  1  and store the value in array q.
 end for 1: for all xij ¡Ê b1 ¡Èxb1 do 1: compute p cij| ij  using  1  and  1 .
1: end for
1: compute kx x1i x1j  for all instance pairs in b1 ¡Á b1.
1: compute k b1 b1  as in  1   using the pre-computed values in the array q  kx and p cij|xij .
using the training accuracy to improve p h|d 
as mentioned earlier  the dd algorithm has been very successful. note that one only has to use the hypotheses  but not their dd values  on classification. recall that the obtained hypotheses are local minima of the dd function. while many of them may have comparable classification accuracies  as will be demonstrated in section 1  a few hypotheses can have dd values that are much higher than the others. this is not surprising as dd value may drop significantly even if the hypothesis is close to only one negative instance. consequently  the summation in  1  is often dominated by a few hypotheses.
¡¡to alleviate this problem while still retaining the merits of dd  we will instead define p h|d  of each dd hypothesis to be proportional to its training accuracy. let predh bi  be the label predicted by h on bi  which is equal to 1 if   and 1 otherwise. then 
		 1 
where m is the number of training bags and z is again for normalization. its superiority over the definition in  1  will be experimentally verified in section 1.
¡¡because of the modular design of the marginalized kernel  we can easily plug in other good estimators of p cij|xij . for example  an em-based approach may also be used.
1 time complexity
let nh be the number of dd hypotheses. in algorithm 1  computing all the p cij|xij 's at step 1 takes o  n1 + n1 nhd  time  assuming that p h|d 's can be obtained in constant time or have been pre-computed  where d is the data dimensionality. assuming that each evaluation of the kernel kx takes o d  time  then computing all the kx ¡¤ ¡¤ 's in step 1 takes o n1d  time. the summation involved in k b1 b1  of step 1 takes o nh1n1  time  as cij only takes 1 or 1 . thus  the total time complexity for computing k b1 b1  is o  nh1 + d n1 +  n1 + n1 nhd   which is polynomial.
1 marginalized kernels for mi regression
in regression  we assume that each bag output yi is normalized to the range  1 . subsequently  each  hidden  instance label cij is also in  1 . in this real-valued setting  we follow the extended dd model in  amar et al.  1   and have:

where z xij  ensures that. it can be easily shown that.
¡¡proceeding as in section 1  we employ the same joint kernel in  1   but with summations of c1i and c1j replaced by integrations over  1   in the marginalized kernel k b1 b1  of  1 . again  this can then avoid integrating over an  n1 +n1 dimensional space.
¡¡the probability that two instances share the same realvalued label is zero  and so  1  is a poor measure of instance label similarity. intuitively  the larger the difference between c1i and c1j  the smaller is kc c1i c1j . noting that |c1i   c1j| ¡Ê  1   we have two natural choices for kc  figure 1:
	 linear  kc c1i c1j  =  1   |c1i   c1j|  	 1 
 gaussian  

figure 1: two possible definitions of the label kernel kc c1i c1j  in the regression setting.
¡¡putting all these together  and after long but straightforward calculations  it can be shown that the marginalized mi kernel between bags b1 and b1 is given by:
 
where g u v  is equal to
1
 z  1   |c1i   c1j|  1   |c1i   u|  1   |c1j   v| dc1i dc1j   1 
1
when  1  is used; and is equal to

when  1  is used. closed form expressions for these two integrals can be found in the appendix. moreover  as in classification  both definitions of k b1 b1  can be computed in polynomial time.
¡¡when multiple hypothesesare used  k b1 b1  can be similarly generalized to:
.
 as for the probability p h|d   again one can follow the extended dd model and obtain:

alternatively  as in  1   we can also use the training accuracy of each hypothesis. following  dooly et al.  1   we use  as h's predicted output of bag bi. the error of h is then eh =
1
  and we can define
		 1 
1 experiments
in this section  we compare the performance of the proposed marginalized kernels with the mi kernel in  ga¡§rtner et al.  1  on a number of mi data sets. as for the kernel defined on the input parts  we use the gaussian kernel kx x1i x1j  =   where ¦Ã is the width parameter.
1 mi classification
for simplicity  the marginalized kernels defined in  1  and  1  are denoted by mg-ddv and mg-acc  respectively.
drug activity prediction
the first experiment is performed on the popular musk1 and musk1 data sets1. the task is to predict if a drug is musky. as mentioned in section 1  each drug is considered a bag  and each of its low-energy conformations an instance. we use 1-fold cross-validation. in each fold  the dd algorithm is applied only on the training data and so the dd hypotheses cannot capture any information in the test set. for comparison  the dd-svm  chen and wang  1  is also run.
¡¡as can be seen from table 1  the mg-acc kernel always performs better than the mi kernel. it also outperforms the dd-svm  which is also using multiple dd hypotheses that are assumed to be equally important. this thus demonstrates the importance of weighting instance pairs based on the consistency of their underlying labels  section 1 .
table 1: testing accuracies  %  on the musk data sets.
musk1musk1dd-svm11mi kernel11mg-ddv kernel11mg-acc kernel11¡¡moreover the mg-acc kernel is better than the mg-ddv kernel. figure 1 a  compares their corresponding p h|d  values   1  for mg-ddv and  1  for mg-acc  for all the dd hypotheses obtained from a typical fold of the musk1 data. as can be seen  for the mg-ddv kernel  a small subset of p h|d  values are very high  and so the summation in  1  is dominated by these few hypotheses. on the other hand  for the mg-acc kernel  the p h|d  values vary much gradually  figure 1 b    and hence more hypotheses can be utilized in computing p cij|xij . moreover  recall that p h|d  in  1  is proportional to h's dd value  while the one in  1  is proportional to h's training accuracy. hence  the almost linear trend in figure 1 b  shows that the higher the dd value  the higher is the training accuracy of the hypothesis. it thus provides another evidence for the success of the dd algorithm.

1 
	1	1
dd hypothesis
 a  p h|d  values for the mg-ddv and mg-acc kernels.1	1	1
dd hypothesis
 b  a closer look on the p h|d  values for the mgacc kernel.figure 1: p h|d  values obtained from a typical fold of the musk1 data. the x-axis shows the different hypotheses obtained by the dd algorithm  sorted in increasing p h|d  values as obtained for the mg-acc kernel.
image categorization
the second experiment is on an image categorization task using the data set1 in  chen and wang  1 . there are 1 classes  beach  flowers  horses  etc.   with each class containing 1 images. each image is regarded as a bag  and each segment an instance. we follow exactly the same setup as  chen and wang  1 . the data is randomly divided into a training and test set  each containing 1 images of each category. since this is a multi-class classification problem  we employ the standard one-vs-rest approach to convert it to a number of binary classification problems. the experiment is repeated 1 times  and the average testing accuracy reported.
¡¡table 1 shows the results  along with those of the ddsvm  hist-svm  chapelle et al.  1  and mi-svm as reported in  chen and wang  1 . as can be seen  the mgacc kernel is again superior to the mi kernel and others. to ensure that the improvement over the mi kernel is statistically significant  we repeat the experiment 1 times  instead of only 1 . the difference is confirmed to be significant at the 1 level of significance by using the paired t-test.
1 mi regression: synthetic musk molecules
we perform mi regression on the data set used in  dooly et al.  1   where the goal is to predict the binding energies of the musk molecules. we use three data sets  lj-1.1  lj-1.1 and lj-1.1  downloaded from the author's table 1: testing accuracies  %  on the image data set.
1 repetitions1 repetitionsdd-svm1 ¡À 1-hist-svm1 ¡À 1-mi-svm1 ¡À 1-mi kernel1 ¡À 11 ¡À 1mg-ddv kernel1 ¡À 1-mg-acc kernel1 ¡À 11 ¡À 1website1  and three more1  lj-1.1  lj-1.1 and lj1.1  used in a recent study  cheung and kwok  1 . the latter were obtained by adding irrelevant features to the former data sets  while keeping its real-valued outputs intact.
¡¡as demonstrated in section 1  the mg-acc kernel is superior than the mg-ddv kernel. hence  we will only experiment with the mg-acc kernel defined with  1  here. as in  dooly et al.  1   we report both the percentage error  %err 1 and mean squared error  mse .
¡¡table 1 shows the results  along with those of dd  emdd and citation-knn  wang and zucker  1  as reported in  cheung and kwok  1 . as can be seen  the proposed mg-acc kernel with the gaussian instance label kernel kc in  1  consistently outperforms the others. its superiority over the mg-acc kernel  with a linearly decaying instance label kernel  indicates that a much smaller weight should be assigned to instance pairs whose labels are quite different  figure 1 . this also explains the relatively inferior performance of the mi kernel  which weights all instance pairs equally.
1 conclusion
in this paper  we show how to design marginalized kernels in multiple-instance learning. first  we pretend that all the hidden instance labels are known  and define a joint kernel using both the instance inputs and labels. by integrating out the hidden data  the marginalized kernel is obtained. this kernel differs from the existing mi kernel in that different instance pairs are weighted by the consistency of their probabilistic labels. experimentally  this marginalized mi kernel always performs better than the mi kernel and also often outperforms other traditional mi learning methods.
¡¡note that the proposed kernel can be straightforwardly used in the regularization framework recently proposed in  cheung and kwok  1 . moreover  because of the modularity of the marginalized kernel  we will also explore even better definitions of the joint kernel and probabilistic model.
acknowledgments
this research has been partially supported by the research grants council of the hong kong special administrative region.
table 1: performance of mi regression on the synthetic affinity data sets.
svmsvm  mg-acc kernel ddem-ddcitation-knn mi kernel linear  1 gaussian  1 data set%errmse%errmse%errmse%errmse%errmse%errmselj-1.1.1.1.1.1.1.1.1.1.1.1.1.1lj-1.1 not available 1111111111lj-1.1.1.1.1.1.1.1.1.1.1.1.1.1lj-1.1--1111111111lj-1.1--1111111111lj-1.1--1111111111a	appendix
using mathematica  it can be shown that  1  
g u v =	1¦Â1  1+g1 u v +g1 u v +g1 v u 
     +g1 u v +g1 v u +g1 u v +g1 u v   ¡Ì¦Ð
+1¦Â1  g1 u v +g1 v u +g1 u v +g1 v u  +g1 u v +g1 u v   
where
g1 u v =1 ¦Â u   v 1 + 1 exp  ¦Â u   v 1   g1 u v = ¦Â 1 u  1u 1v  1 exp  ¦Â 1   u 1   g1 u v = ¦Âu 1v   1u   1    1 exp  ¦Âu1   g1 u v = ¦Â 1u + 1v   1uv + 1  + 1 exp  ¦Â   g1 u v =1¦Â u + v   1uv   1  
 erf 

erf 
 
and erfis the so-called error function. similarly   1  can be shown to be
g u v 	=	 1 |u   v|1 + 1 u   v 1 + 1uv u1 + v1 
	1	1
 1 u + v 1   1 u   v 1 + 1 u + v  + 1.
	1	1
