 
chunking  an experience based-learning mechanism  improves soar's performance a great deal when viewed in terms of the number of subproblems required and the number of steps within a subproblem. this high-level view of the impact of chunking on performance is based on an deal computational model  which says that the time per step is constant. however  if the chunks created by chunking are expensive  then they consume a large amount of processing in the match  i.e  indexing the knowledge-base  distorting soar*s constant time-per-stcp model. in these situations  the gain in number of steps does not reflect an improvement in performance; in fact there may be degradation in the total run time of the system. such chunks form a major problem for the system  since absolutely 1 guarantees can be given about its behavior. 
i  his article presents a solution to the problem of expensive chunks. the solution is based on the notion of restricting the expressiveness of soar's representational language to guarantee that chunks formed will require only a limited amount of matching effort. we analyze the tradeoffs involved in restricting expressiveness and present some empirical evidence to support our analysis. 
1. introduction 
   the goal of the soar project is to build a system capable of general intelligent behavior and autonomous existence . soar is based on formulating all symbolic goal-oriented behavior as search in problem spaces. the primitive acts of the system  called decisions  are those required to pursue this search: the selection of problem spaces  states  and operators  plus the application of operators to states to generate new states. the information necessary for the performance of these primitive acts can be provided in one of two ways: by the firing of productions  or by the recursive use of problem space search in subgoals. both can result in adding new objects to the system's working memory and in adding new information about existing objects  encoded as attributes with values . soar learns by converting subgoal-based search into productions that generate comparable results under similar conditions. the actions of the new productions are based on the 
r
esults of the subgoals. the conditions are based on those working 
memory elements  wmes  in parent goals upon which the results iepended. this chunking process is a form of explanation-based learning  ebl  . 
   chunking improves soar's performance a great deal when viewed in terms of the number of subproblems required and the number of steps within a subproblem f 1 . however  despite the gain in lumber of steps  there may be degradation in the total run time of the system. in   we showed that this anomaly arises due to expensive chunks  i.e.  learned productions that consume a large amount of processing in the match. at its worst  chunking can 

paul roscnbloom 
information sciences institute 
university of southern california 
1 admiralty way 
marina del rey  ca 1 rosenbloom  isi.edu 
convert a subexponential problem space search into an exponential match problem - as shown in |1   matching expensive chunks is np-hard. 
   a concern about degradation in performance due to learning has appeared widely in the ebl literature  1  1  1  1  1 . various approaches have been used to deal with this degradation  most focusing on some form of cost-benefit analysis of the learned material. in this article we present a solution to the problem of expensive chunks that is based on restricting the expressiveness of the system's representation language so as to guarantee that all chunks will be cheap. in terms of a cost-benefit approach to learning  this amounts to guaranteeing that the cost of adding a production will always be close to zero. thus  if the benefits are never less than zero  an explicit on-the-fly cost-benefit analysis can be avoided. 
   this article is organized as follows: section 1 describes the problem of expensive chunks in more detail. section 1 presents our solution to the expensive chunks problem. section 1 takes an in depth look at the solution for a prototypical expensive chunks task: the grid task. section 1 provides further experimental results bearing on our solution. section 1 presents a counter-point  a task in which our solution is at its worst: the tree task. finally  section 1 summarizes the results and presents some issues for future work. 
1. the problem of expensive chunks 
   intelligent systems  such as soar  are based on symbolic architectures  mat partition the complete system into two independent domains. above the architecture is the cognitive domain of flexible symbol processing. below the architecture is the implementation domain of fixed computational processes. in soar  problem space search occurs in the cognitive domain. it is a symbolic process that can itself be controlled by further symbol processing. the production match  on the other hand  is part of the implementation domain. it is a fixed process mat runs to completion unaffected by the knowledge in the cognitive domain. 
   in concert with this distinction  it is possible to describe the affects of learning on task performance via two effects. the cognitive effect is the change in the number of cognitive operations required to perform the task. the computational effect is the change in the amount of time required to perform the individual cognitive steps. for soar  the cognitive effect of chunking is the change in the number of production actions that are executed  while the computational effect is the change in the time required per action. table 1  modified from   shows the cognitive effect  the computational effect  computed as time-per-action before learning /time-pcr-action after learning   the total speedup  this is speedup in total match time  ignoring the other processing performed by the architecture  and the number of chunks learned for eight tasks implemented in soar. these measurements were done on soar/psme   a system that uses a highly optimized implementation of the rete production matcher. 
   as expected  chunking provides a large cognitive effect in all of the tasks. learning causes the number of actions required to drop by a factor of between 1 and 1. for four of the tasks  syllogisms  
	tambe and rosenbloom 	1 
   
monkey  waterjug  and farmer  this cognitive effect is followed by a concomitant speedup in the total match time. however  for the other four tasks  1-puzzle  n-queens  grid and magic-square  the speedup in terms of total match time is less than 1 - the match time has actually increased after chunking. this anomaly occurs because of the computational effect; the time per action for these four tasks increases by as much as a factor of 1  for the magic-square . this computational effect does not come about because of the increased number of chunks - only five chunks are acquired in the magicsquare - but instead from the presence of individually expensive chunks. 
   expensive chunks are a problem for a number of reasons. firstly  they cause the system to slow down with learning  a clearly undesirable effect. secondly  they introduce np-hard search into the implementation domain  the matcher  where it cannot be controlled by additional knowledge. thirdly  they lead to gross violations of soar's ideal computational model  that the time per step in problem solving should be constant. 
   to understand the origins of expensive chunks  it is useful to have a means of analyzing the production match that is independent of many of the variations in match algorithms and implementations. the k-search model  is one such approach that covers match algorithms that find all possible solutions  without the aid of heuristics. this includes widely used match algorithms such as rete  and treat . the k-search model is based on the notion of tokens  i.e.  partial instantiations of productions. consider the production length-1 shown in figure 1-a. it contains three conditions and one action. in the figure  the use of a indicates an attribute  and the use of o indicates a variable. figure 1-b shows the working memory of the production system  which describes the graph shown in figure 1-c. on the creation of the working memory element  current-position b   the production length-1 will match the working memory  generating tokens  such as:  1; x = b  z = c    1; x = b  z = d  etc. the first number in the token indicates the number of conditions matched and the other elements indicate the bindings for the variables. thus  tokens indicate what conditions have matched and under what variable bindings. 
   the tokens generated in the match can be represented in the form of a match tree  as shown in figure 1-d  at each stage  only the additional variable bindings are shown . this tree represents the search conducted by the matcher  using tokens  to match the production. since this search is done in the production system  i.e.  in the knowledge base  it is called k-search  to distinguish it from problem space search. measurements on soar/psm-e indicate that the time spent per token is approximately constant therefore  for soar productions  the number of tokens in the k-search tree is a reasonable estimate of the work done in performing match. 
   for a chunk with a k-search tree of depth h and a constant branching factor of f  the match cost will be  tokens. 
this is exponential in the depth of the tree  h   and if f and h are 
	1 	machine learning 
large  the chunk will clearly be expensive. let's look at both of these factors in a little more detail.  this analysis is a summary of the one presented in .  the depth of a production's k-search tree  h  is equivalent to the number of conditions in the production. for a chunk  this corresponds to the size of the footprint; that is  the number of wmes in the supergoals examined by the problem-solver in producing results of the corresponding subgoal. the branching factor of a production's k-search tree is a function of the number of wmes that match each condition  and the amount of constraint provided by cross-condiuon variable tests. in soar  cross-condition variable tests are used extensively to implement a linked-access restriction. this restriction says that before an object can be tested in a production  a path from a goal to that object  via attributes  must have already been tested. given this restriction  there remain only two sources of branching factor in k-search trees: bad condition orderings  which can reduce the constraint provided by cross condition variable tests; and attributes that have multiple values  multi-attributes .1 multi-attributes are used in representing open  unstructured sets in working memory. for example  in figure 1 -b  connected-to is a multi-attribute of the object b - points c  d  and e exist as an unstructured set of points connected to point b. figure 1-d illustrates how the k-search tree branches out in matching points c  d and e connected to b. 
1. eliminating expensive chunks 
   eliminating expensive chunks means that all of the chunks used by the system in problem solving are cheap chunks. an ideal solution would impose a fixed upper bound on the cost of each chunk  allowing the system to achieve a constant time per step. even if a fixed bound is not possible  it would be preferable to let the cost of the chunks be a small polynomial function rather than the existing exponential function. such chunks will require only a few additional tokens to match them and hence will only minimally distort the constant time-per-step model. 
two techniques that initially look like potential solutions can be 

   
immediately ruled out by the inherently exponential nature of the production match. one technique is the use of smarter match algorithms. this could involve either better automated condition orderings  or other techniques such as selective backtracking . a better condition ordering can definitely reduce the amount of ksearch  see   for example   but cannot completely eliminate it  or even make it non-exponential in general. the same is also unfortunately true for the other smart match algorithms. the other technique that looks like a potential solution is the use of massive parallelism. given any amount of parallelism it is always possible to have an exponential match that will exceed the capacity of the machine. a third technique  that of hand rewriting of expensive chunks  is ruled out by the need for the system to run autonomously. this is one key way in which the problem of expensive chunks differs from the more general problem of expensive productions. when these three techniques are eliminated  four alternative strategies remain: 
1.selective learning/for getting: a cost-benefit analysis is performed before  or after  adding a learned rule into the system's knowledge base. the rule is added to the knowledge base only if the analysis proves to be positive  1  1  1  1 . 
1. selective matching: the matcher reasons about the expense of the learned rules and decides the best rule to be matched at any point in time. thus the compiled rule always remains in the knowledge base. 
1. modifying learned rules: after learning a rule  its lefthand side is simplified to reduce its match cost. this may be accomplished by processes such as removing applicability conditions . 
1. restricting expressiveness: the system gives up some expressiveness of its language to guarantee that all learned rules are cheap. this tradeoff is similar to the tradeoff in the expressiveness of a representational language and its computational tractability . 
   in this article  we investigate the strategy of restricting expressiveness for eliminating expensive chunks. there are two obvious candidates for this restriction: restricting the size of the footprint  and restricting the use of multi-attributes. a restriction on the size of the footprint will bound the depth of the k-search tree. this will make the cost of the chunk be polynomial in the breadth of the k-search tree. however  if the bound is large the chunks will still not be cheap  and if the bound is small  extensive modifications will be required to the production system and perhaps also to the chunking mechanism. the other alternative - restricting the use of multi-attributes - is more promising. multi-attributes control the branching factor of the k-search tree. if multi-attributes are eliminated completely  the branching factor of the k-search tree will be reduced to one. this will then limit the number of tokens in the k-search tree to the size of the footprint. thus  the cost of a chunk will be linear in the number of conditions. this conforms to our definition of cheap chunks. 
   what are the implications of such a restriction  recall that multiattributes are used for representing open  unstructured sets in working memory. this allows accessing  or searching  all elements of the set by matching a single production; that is  by k-search rather than by problem space search. for instance  in figure 1-d  all the points connected to b are obtained via k-search. but k-search in the presence of multi-attributes is combinatoric. when combinatorics occur in the k-search  then they cannot be controlled  potentially causing exponential slowdowns. 
   thus  the restriction on multi-attributes implies that open  unstructured sets cannot be represented directly in working memory. all sets in working memory have to be structured  trees  lists or some other task-specific structures . we refer to the new restricted attributes as unique-attributes. the principle impact of going with unique-attributes is the removal of the combinatorial k-search from the matcher - all combinatorics now occur as search in problem spaces. there are some fixed overheads associated with the problem space search of the unique-attributes  selection of states  operators etc. . these overheads can cause the system to slow down by a constant factor. but  the gain in carrying out the search in the problem space lies in the ability to use control knowledge to terminate or control it. moreover  chunking will gradually reduce and ultimately eliminate this search. thus  the constant overheads of problem space search will also be eliminated from the uniqueattributes. 
   a secondary impact of unique-attributes is that the chunks that are learned may be less general. this occurs because a multi-attribute condition can match any element of an open set  while a uniqueattribute condition can match only one possible element. this issue is examined in detail in the following sections. 
   table 1 presents the total run time  before and after chunking  with unique-attributes and multi-attributes  for the four expensive chunks tasks in table 1.  due to the differences in implementations  these results are not direcdy comparable to those in table 1.  the parethesized numbers are the number of chunks learned. when the unique-attribute chunks were less general than the multi-attribute ones  additional trials were run on the same task until enough chunks had been learned to cover the same scope as the multi-attribute chunks. we see that the time to complete the task without chunking in both representations is comparable. however  three out of the four tasks with multi-attributes require more time after learning than before it. with the unique-attribute representation  all four tasks speed up  by factors between 1 and 1. this is the case even when significantly more unique-attribute chunks are learned. 

	tambe and rosenbloom 	1 
adjacent to a point x on the grid is represented as:  point x connected y . thus  this is an unstructured set of connections between a point and all its immediate neighbors. the problem space has only one operator: move. if the current position is at point x  then for each point y connected to point x  the operator move will be instantiated. the problem solver will solve the problem using some heuristics  or outside guidance  generating a k-search tree of tokens as shown in figure 1-b. this generates 1 tokens  four tokens for each step  even if the heuristics don't work well  the system will generate only four tokens per step . 
the chunk formed in solving the task is shown in figure 1-a. 
the chunk says that if the goal is to reach a point  d   and if the current position is point  x   and if there is a path of length four between them  then prefer the instantiated move operator along that path. the chunk does not consider the points along which the path goes or the direction the path takes. the chunk will therefore transfer to all pairs of points with a path length of four between them. figure 1-b shows the k-search tree formed in matching the chunk. each condition multiplies the number of tokens by four  which is the number of points connected to any given point. since there are four conditions in the chunk  for a path of length four   the total number of tokens is 1  = 1 + 1 + 1 + 1  tokens. comparing this with the four tokens per step in the original problem solving  we see mat the chunk is expensive. the 1 tokens correspond to all possible paths originating from point a that have a length of four. 

   in the unique-attribute version  each location points to its four adjacent locations using specific unique-attributes: up  down  left and right. instead of one operator move  there are four distinct operators  move-up  move-left  move-right and move-down. again  the problem solver moves from a to b using heuristics or outside guidance  generating the tree of tokens shown in figure 1-b. the chunk formed with this representation is shown in figure 1-c. it says that if the goal is to move to a point  d  from the point  x  and if the connection between the two points is through the specific relation described  up-right-up-right   then choose the appropriate operator: move-up. the k-search tree formed is shown in figure 1-d. there are only four tokens formed in this case - much cheaper than the chunk in the multi-attribute case. however  the chunk will transfer only if the two points are connected in a specific manner: up-right-up-right in this case  as opposed to any arbitrary connection 
	1 	machine learning 
of length four in the earlier case. 
   table 1 summarizes the cost and generality of the two representations. the generality is measured in terms of the number of transfers in an nxn grid. the length of the path traversed in the grid is assumed to be/ . comparing the multi-attributes and uniqueattributes  we see that a single multi-attribute chunk has a level of generality that is  p+1  1 times more than the generality achieved by a single unique-attribute chunk. thus to achieve the same performance  the unique-attribute system has to learn  p+1 chunks. however  even after learning all those chunks  the cost of matching all of the productions is only a polynomial number of tokens in the unique-attribute system. it is exponential in the multi-attribute system; i.e.  the multi-attributes have produced an expensive chunk. 

   to understand why the unique-attribute version has an advantage  consider a problem with a path length p = 1. there are only a polynomial number of destinations on the grid that can be reached from a source:  p+1 - 1. however  there are an exponential number of paths from the source to these destinations: 1p = 1. when given the goal of reaching one particular position  the expensive chunk tries to find all possible paths of length four. it discovers all 1 paths to all of the 1 positions; an excessive amount of k-search  since only one path to one position is required. this k-search of all paths to each of the positions gives rise to the exponential factor. on the other hand  the chunks acquired by the unique-attribute version learn only one path to each of those positions. this avoids the useless computation of finding all paths to each position. even after learning all the chunks in this representation  the total amount of k-search done is proportional to the number of destinations  1   and not to the total number of paths to each of the destinations  1 . thus  the multi-attribute representation uses k-search to gain generality. however  in the grid example  much of the generality is essentially superfluous. this excessive generality is a typical characteristic of expensive chunks. a unique-attribute system is able to avoid such excessive k-search  which ultimately delivers only superfluous generality. 
   the restriction on multi-attributes does not imply that a soar system has lost all of its sources of generality. other sources of generality  which are independent of the amount of k-search  can still be exploited. for example: 
1. implicit generalization: chunks are based only on those aspects of the situation that were referenced during roblem solving in the subgoal to produce results . decomposition: if a task is decomposed into smaller subtaslcs  then chunking the smaller subtasks independently provides anotner source of generality. 
1. experimental analysis 
   using two tasks from table 1 - grid and 1-puzzle - this section experimentally demonstrates how restricting multi-attributes eliminates expensive chunks and excessive k-search. to this end  each task was run independently with multi-attributes and with unique-attributes. for each representation and task  the system was first run without chunking. the system was then allowed to chunk on the problem  after which it was then run on the same problem; that is  after having chunked on the problem. a sequence of such experiments was performed with the unique-attributes representation to accumulate a set of chunks yielding the same level of generality as achieved with the multi-attribute representation. 
   
   figure 1 shows the computational effect - the change in time per action - with the addition of chunks for the two tasks  the details of the representations used can be found in  . the first graph in figure 1 is from the grid task. the multi-attribute representation learns 1 chunks  the large number of chunks - 1 - is caused by the variety of subgoals that are being chunked  and causes a computational effect of about .1; that is the time per action has gone up by about a factor of 1. the unique-attribute version requires learning on more problems to reach the same level of generality  each star  *  represents one problem . in the process  it accumulates 1 chunks  but the computational effect is much more limited  about .1 . the graph for the 1-puzzle can be interpreted in a similar manner. 

   from the two graphs  we see that though the unique-attribute version may require more chunks to gain the same generality  it posits a big gain by avoiding excessive k-search  graphs for tokens per action present a similar picture . the two graphs also show quite clearly the ability of the unique-attribute version to stay relatively close to the ideal computational model of constant time per step  and the inability of the multi-attribute version to do so. the reason that the unique-attributes version isn't completely consonant with the ideal computational model is that  though the chunks are individually cheap  each one does add something to the total match cost. we refer to this as the average growth effect  the distortion in soar's computational model due to the addition of a large number of chunks. one potential solution to this problem is parallelism. recent work has shown that chunking increases the concurrency  or available parallelism  in the system . we expect that with future research in parallel production systems  1  1   it will be possible to convert the increase in concurrency into real parallelism  allowing soar to preserve its constant time per step model  if the number of chunks remains bounded  see section 1  . 
   table 1 showed how the unique-attribute version is more efficient after reaching the same level of generality as the multiattribute version  but it didn't show the finer-grained behavior of what happens during the learning process. specifically  it didn't address the issue of the extra time spent by the unique-attributes version in acquiring the extra chunks. to understand this issue  a set of randomly-generated problems in the 1-puzzle domain were run under both versions. both versions started off with zero chunks  and solved the set of problems with chunking turned on  i.e.  chunking continuously across the set of problems. thus  the systems used the chunks learned in one problem to solve the subsequent problems; simultaneously learning more chunks in situations where the earlier chunks did not apply. 
   the first graph in figure 1 shows the number of decisions required by the two versions over the sequence of problems. decisions arc typically used in soar to measure the amount of search carried out by the system. the curves show that initially both systems take a large number of decision cycles to reach the goal. however  this pattern changes quickly as the increased generality provided by the multi-attribute chunks reduces by a greater amount the number of decisions required for the problems. as more problems are solved  and more unique-attribute chunks are acquired  the difference in number of decisions gets increasingly small. the second graph in figure 1 compares the amount of total time required by the two versions of the 1-puzzle task. in contrast to the picture presented for decision cycles  here the performance of the unique-attribute version completely dominates the performance of the multi-attribute version. 
   the first graph in figure 1 shows the time per decision for the two systems. the point corresponding to the zeroth problem corresponds to the time per decision prior to learning. the main message of this graph is that time per decision remains fairly constant in the unique-attribute version  while it takes some fairly large jumps in the multi-attribute version. the second graph in figure 1 shows the cumulative times for the two systems on the 1 problems. the unique attribute system coasistently outperforms the multi-attribute system. very similar results were obtained for the grid task  see .  
1. the tree task: a counterpoint 
   in the grid task there was a trade-off between expressiveness and efficiency  but the amount of efficiency gained far outweighed the amount of expressiveness lost. however  this is not always the case. in the worst case there is a one-one trade-off between the two factors. a good example of this is the tree task. the tree task is just like the grid task except that the structure to be searched is a tree  and the path to be found is always from the root to one of the leaves. in this task  a single multi-attribute chunk  learned for one branch of the tree  can transfer to all other branches of the tree. a unique-attribute chunk  on the other hand  remains specific to a 
   particular branch of the tree. table 1 presents the cost and the generality of the chunks learned in this task  for a tree of depth three and a branching factor of two. 
   one observation that can be made from this table is that the cost of matching all eight unique-attribute chunks  one chunk per path  is equal to the cost of matching the one multi-attribute chunk. thus  
	tambe and rosenbloom 	1 
   

there is no excessive k-search involved in the multi-attribute chunk. a second observation is that the lower generality of the uniqueattribute chunks demands an exponential number of chunks to cover the level of generality of one multi-attribute chunk. the obvious question that this raises is if the unique-attribute version is going to have to acquire  an exponential match anyway  to match the exponential number of productions   why not acquire it all at once via the multi-attribute chunk. the answer to this question lies in the issue of protection. the multi-attribute chunk can add an arbitrarily large exponential cost in a single learning trial. in contrast  the unique-attribute version learns about the individual branches as they are experienced. the match cost always increases gradually  at worst   and remains bounded by the number of branches that have been experienced. at worst the number of branches that have been experienced is equal to the number in the tree  but in many domaias only a small portion of the entire exponential space is ever experienced. 
   a related point is that the system is also protected from learning an exponential number of chunks by its finite lifetime. if the chunking rate is approximately constant over time  then there is a 
	1 	machine learning 

finite number of chunks that the system will ever be able to acquire. under these circumstances the system can work in arbitrarily large exponential domains  but it will never have enough time to learn everything about the domain  as opposed to learning everything about the domain quickly  but never having enough time to use it . 
   how often is this worst case likely to arise  observe that the large  exponential in the depth  tree structure must be present in working memory for performing this task. an exponential amount of time must be spent in generating such an exponential structure. it is unlikely that the problem solver will be able to generate very large structures in its lifetime. on the contrary  in the earlier grid 
   
example  the size of the structure is limited  but the matching of the structure requires exponential time  due to the connectivity . this provides some evidence that the worst case shown here is unlikely to occur; instead  it is more likely for the grid world situations to occur. in fact  none of the tasks from table 1 require a one-one expressiveness-efficiency tradeoff. 
   thus  in the unlikely worst case  the tree search   an expressiveness-efficiency tradeoff exists. however  in the general case  multi-attribute chunks generate excessive k-search. we cannot get rid of this excessive k-search with multi-attributes  since the match problem is inherently intractable. there is no tradeoff involved in this excessive search  the unique-attribute approach simply gets rid of it. 
1. summary and future work 
   expensive chunks are caused by three factors: multi-attributes  big footprints and condition ordering. multi-attributes allow combinatorial searches to occur in the match. these searches can result in exponential slowdowns. by eliminating multi-attributes  i.e.  by restricting the expressiveness of the production system  we were able to limit the cost of a chunk to be linear in the number of its conditions. the principal impact of the unique-attribute representation is the removal of combinatories from the matcher  when combined with the linked-access restriction . the combinatorics now occur in the problem space  where they can be controlled using control knowledge. we provided some empirical evidence to show that our solution not only guarantees cheap chunks  but also gets rid of excessive  and expensive  generality. from a cost-benefit point of view  our solution may be construed as guaranteeing a close to zero cost per chunk. 
   the restriction on expressiveness need not translate into a difficulty in performing tasks in soar. though set operations cannot be performed easily at the production-system-language level  it is possible to perform the tasks at the problem-solving level  using states and operators. we have implemented a set of operators which can perform most of the common set operations  and expect these to effectively replace the functionality provided by multi-attributes. 
   the approach presented in this article is related to the operationality-generality tradeoff discussed in the ebl literature - some expressiveness is sacrificed to form cheap chunks. however  the system is also able to avoid excessive k-search  which docs not involve a tradeoff. in addition  other sources of generality available to the system arc not involved in the tradeoff. 
   recently  chalasani and altmann   1   provided some more evidence in favor of unique-attributes. they pointed out that the knowledge representation adopted by theo  a frame-based architecture for problem-solving and learning  1j  corresponds to the unique-attribute representation. theo s knowledge access language is restricted so that it works within the unique-attributes framework. multi-sets in the knowledge-base are represented in theo in the form of linked-lists. such lists are then processed by user-written lisp routines  rather than by constructs in the query language . 
   a number of issues remain to be addressed. an important issue is the usability and utility of unique-attributes in more complex task domains. toward this end  we have recently converted 
rl-soar   part of an expert system for computer configuration  into the unique-attribute representation. rl-soar uses only four different multi-attribute sets. hence  the conversion turned out to be relatively easy  requiring only two man days. furthermore  rl-soar does not form expensive chunks; thus  there was only about 1% change in decisions and run time due to the conversion. additional large-scale tasks will also need to be converted for a full evaluation. a second issue is whether the unique-attributes restriction is the most appropriate one. though it clearly removes expensive chunks  there are several related restrictions  some more restrictive and some less  which are also worth investigating. we hope that this will allow us to gain a better understanding of the interaction between learning  representation and efficiency. 
acknowledgements 
we thank john laird  allen newell  steve minton  david steier  
don cohen  prasad chalasani  bob macgregor and members of the soar group for many interesting discussions on expensive chunks. we also thank kathy swedlow for technical editing. 