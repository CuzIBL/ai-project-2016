 
it has been shown that hill-climbing constraint satisfaction methods like min-conflicts  minton et a/.  1  and gsat  selman et a/.  1  can outperform complete systematic search methods like backtracking and backjumping on many large classes of problems. in this paper we investigate how preprocessing improves gsat. in particular  we will focus on the effect of enforcing local consistency on the performance of gsat. we will show that enforcing local consistency on uniform random problems has very little effect on the performance of gsat. however  when the problem has hierarchical structure  local consistency can significantly improve gsat. it has been shown  konolige  1  that there are certain structured problems that are very hard for gsat while being very easy for the davis-putnam procedure. we will show that they become very easy for gsat once a certain level of local consistency is enforced. 
1 	introduction 
local search algorithms like min-conflicts  minton et a/.  1  and gsat  selman et a/.  1  have been successfully applied to different classes of constraint satisfaction problems like sat  graph coloring  binary csps and scheduling. the popularity of local search algorithms can be attributed to their efficiency - they can outperform complete systematic search methods like backtracking and backjumping on large classes of problems. the question that arises is whether local search methods are always better  or at least not worse  than complete systematic search methods. this question is somewhat ambiguous since there is no one single version of gsat - most of them employ clever heuristics that significantly improve their performance over the basic version of gsat reported in  selman et a/.  1 . moreover  several problems that once seemed very hard have been 
　*this work was partially supported by nsf grant iri1  by the electrical power research institute  epri   and by grants from toshiba of america  xerox northrop and rockwell. 
1 	constraint satisfaction 
successfully solved once certain heuristics  like clause weighting and random walk  were added to gsat. 
　in this paper we will investigate whether or not gsat can be improved by applying preprocessing. preprocessing algorithms are run on the problem in advance  before the search algorithm is tried  and change the problem representation into a different  but equivalent one. preprocessing algorithms include a variety of consistencyenforcement algorithms. these algorithms make the problem more explicit by adding new constraints that are induced by the existing constraints. enforcing local consistency can improve the performance of complete systematic search methods like backtracking and backjumping by eliminating dead ends  thus reducing the search space. we will apply the idea of enforcing local consistency to gsat with the hope that its performance can also be improved by making the problem more explicit. in particular  we will focus on different forms of partial path consistency since full path consistency is not cost effective for large problem instances. 
　we will focus on two different classes of problems - random uniform problems that do not have any structure and random structured problems. as we will show  local consistency has a very different effect on the performance of gsat on these two classes of problems. on uniform random problems  enforcing local consistency can help gsat solve more problems but the overhead associated with enforcing local consistency and the added complexity of induced constraints eliminates any net gain. however  on a class of structured cluster problems  local consistency dramatically improved the performance of gsat. 
　in a recent paper  konolige  1  it was shown that there are certain classes of structured problems that are very hard for gsat  even if the best currently known heuristics like clause weighting and random walk are used  while being very easy for the davis-putnam procedure. in this paper we will examine a similar class of structured 1sat problems. these problems have a cluster structure - they contain clusters of variables each of which is a small group of variables tightly linked with constraints  1-sat clauses . clusters themselves are linked together by a different set of constraints  1-sat clauses . it turns out that these kinds of hierarchical problems can be extremely hard for gsat using the best currently known heuristics. but surprisingly  these 

problems will become trivial for gsat once a certain amount of local consistency is enforced  in this case a restricted form of bound-1 resolution. the effect of enforcing this kind of local consistency is that it makes constraints more explicit by adding new induced constraints. this will change the gsat search space by eliminating many near solutions 1 so that they will become assignments whose cost is high. 
1 	gsat 
local search algorithms like gsat work by first choosing an initial assignment and then incrementally improving it by flipping the value of a variable so that the new value leads to the largest increase in the number of satisfied constraints. this is done until all constraints are satisfied  or a predetermined number of flips  max-flips  is reached  or gsat reaches a local minimum. 
the following is a standard gsat procedure: 
procedure gsat  csp problem p  max-tries  
　　　max.flips  for i=l to max-tries let a be a random initial assignment forj=l to max-flips 
if a satisfies p  return true else let f be the set of variable-value pairs that  when flipped to  give a maximum increase in the number of satisfied constraints; pick one f e f and let new a be 
current a with f flipped 
end 
end 
return false end 
　this algorithm is almost never used in practice as it is here because its performance can be improved significantly by adding a number of heuristics. our version of gsat uses several heuristics that include  we believe  the best known heuristics today. 
　the basic gsat is non-deterministic because it does not specify how to break ties between two or more variables having an equally good flip  or between two or more values that would give the same increase.  gent and walsh  1  suggest using historic information instead of breaking ties randomly. they propose that in the event of a tie  a variable that was flipped the longest ago be chosen. 
we also use clause weighting as proposed by the 
breakout method of p. morris  morris  1  and in a different form in  selman and kautz  1 . this method proposes a method of escaping local mini mums by reweighting constraints. in addition  we use a version of random walk called random noise strategy in  selman et a/.  1 . this method suggests picking  with probability p  a variable that appears in an unsatisfied constraint and flipping its value. unlike  selman et ai  1   we flip not one  but three variables at a time and the probability p is not 1%  but 1%. this gives 
　　1  near solutions are assignments that have a cost of almost zero. 
a little improvement over the original method because if only one variable is flipped  most of the time gsat will flip it right back. 
　the third heuristic we use is similar to the one proposed in  yugami et a/.  1 . their method proposes a way of escaping local minimums by using value propagation over unsatisfied constraints. we pick an unsatisfied constraint and check to see if it contains any variables whose value has not yet been flipped. if there is at least one  we will flip one of them so that the constraint becomes satisfied. there are two differences in what we do -  yugami et ai  1  computes a closure under value propagation  whereas we do only a fixed number of steps. second  in  yugami et ai  1  this is done every time a local minimum is reached. we do it only at the end of every try as a way of generating a new initial assignment for the next try. 
last  there is always the problem of choosing 
max tries and max-flips. we solve this problem by using a heuristic that determines the length of every try  ie. max flips  automatically during every try  hampson  1 . the idea is that we search as long as we are making progress  and if we haven't made progress for a while we give up and start a new try. progress is measured as finding an assignment that satisfies more constraints than satisfied by any other assignment found by gsat during that particular try. every time we find such an assignment  we give gsat time equal to the amount of time it has spent up until that point from the beginning of the try. if during this time no better assignment was found  we give up and start a new try. using this strategy  we need to give only one parameter  
maxflips  that bounds the total maximum number of flips that gsat will spend on a problem. 
1 	problem format 
the first class of 1sat problems we experimented with is a set of cluster structures. these problems are characterized by the following parameters: 
1. n - the number of variables per cluster. 
1. c - the number of clauses per cluster. 
1. cn - the number of clusters. 
1. cc - the number of clauses between clusters. 
　every cluster structure is generated by first generating cn clusters  each n variables and c clauses  and then generating cc clauses such that all 1 variables in a clause come from different clusters. 
　we also used binary constraint satisfaction problems such that all variables had the same domain of size a' of natural numbers {1 ...  a'}. all binary csp problems are characterized by the following parameters: 
1. n - the number of variables. 
1. k - the number of values. 
1. c - the number of constraints. for binary constraints cmax = n- n- l /1. 
1. t - the tightness of the constraint  as a fraction of the maximum a'1 pairs of values that are nogoods. 
　every binary csp problem is generated by first uniformly randomly choosing c pairs of variables and then 
	kask and dechter 	1 

figure 1: a cluster structure. 
creating a constraint for every pair. to create a constraint  we randomly pick t x k1 tuples and mark them as pairs of values that are allowed. 
　when evaluating the performance of an algorithm  it is always important to know how many of the problems that were tried were actually solvable. this introduces an additional problem for gsat since it is an incomplete algorithm. normally in this situation  every problem is first solved using the davis-putnam procedure or any other complete algorithm. unfortunately  this allows us to solve only small problems. real life problems are likely to be large and intractable for any complete algorithm known today. we can get around this problem if we can generate problems for which we know in advance that they are solvable. the straightforward way of getting solvable 1sat formulas is to first generate a solution and then generate random clauses and keep only those that are satisfied by the solution  in other words  at least one literal in the clause matches the solution . unfortunately  these kind of problems are very easy. however  it turns out that if we throw away not only those clauses that have 1 literals satisfied by the solution  but also those that have exactly 1 literals satisfied by the solution we get random 1sat problems that are on the average at least as hard  for gsat  as the class of all solvable problems. all 1sat cluster structures tested in this paper were generated this way. 
　another property of solvable 1sat formulas generated this way is that when the formulas are uniformly randomly generated  the hardest problems for gsat are in the area where the ratio of the number of variables to the number of clauses is between 1 and 1. when this ratio is smaller or larger  problems are easy for gsat. for example  when the ratio is 1  problems have only 1 solution 1 and on the average  finding this only solution 
   1  we know there is only one solution because when we compute the closure under bound-1 resolution we almost always 
1 	constraint satisfaction 
is much easier for gsat than solving a problem from the 1 - 1 range. 
1 	random cluster structures 
on the class of cluster problems described in the previous section gsat performs very poorly. in table 1. we have the results of experiments with cluster structures of cn = 1 clusters  cc = 1 clauses between clusters  n = 1 variables per cluster and c = 1 - 1 clauses per cluster. each cluster by itself is very easy for gsat because it is strongly over-constrained  the ratio of the number of clauses over the number of variables varies from 1 to 1  and as a result each cluster has only very few solutions. but taken together they appear extremely difficult for gsat. for example  when the number of clauses per cluster grows from 1 to 1 the number of problems gsat is able to solve drops from 1 to 1  and it takes on the average 1 flips to find a solution when it can find one  the upper bound maxflips is 1k . when we increase the number of clauses per cluster to 1  gsat fails to solve any problems  remember that all problems are solvable . for comparison  we have also included the running time of the davis-putnam procedure on the same problems. 
　this is surprising since 1 variable uniform random 1sat formulas are fairly easy for this gsat program. our hypothesis is that this phenomena can be attributed to the structure of the problem. when the number of clauses per cluster increases from 1 to 1  the number of solutions each cluster has  when taken separately  decreases from a few to one. but the number of near solutions 1 remains large. when we start gsat on an initial assignment  it always quickly converges to an assignment that is a near solution. the hardest part for any gsat algorithm is to improve a near solution so that it becomes a real solution. on the cluster problems  gsat quickly finds an assignment that for many clusters is a near solution. but it seems to be unable to improve this assignment. in order to improve it many changes in different clusters need to be made. but the structure of the problem - tight clusters with loose constraints between them - does not provide good guidance for gsat. 
　however  we can enforce local consistency that will change the structure of the problem by adding new  induced constraints. we ran gsat on the same problems after a preprocessing algorithm rbr-1 was run on them. rbr-1 computes a restricted form of bound-1 resolution  dechter and rish  1  by resolving only pairs of original clauses and keeps only those resolvents that have no more that 1 literals. the results of these experiments are 
get a total of cmax = 1  n/1  clauses  which is the maximum number of clauses a solvable 1sat formula can have. also  when we have cmax clauses  the formula has only one solution. finally  notice that new clauses added by bound-1 resolution do not remove any solutions of the original formula. 
   1  a near solution is an assignment that satisfies almost all clauses  and therefore  for which the value of the cost function is almost zero. 

in table 1. we see that after rbr-1 was run  problems became almost trivial. gsat can solve all problems and on the average it needs only 1 flips. almost all of the time was used by rbr-1. 
　when we look at where the new clauses are added we see that almost all of them are local clauses  namely all three literals are from the same cluster. in fact  the total number of clauses per cluster roughly doubles and is close to the maximum possible number of clauses cmax = 1   
     n/1   that a solvable 1sat problem can have. this has the effect of eliminating many near solutions. many assignments that previously satisfied all  except very few  clauses violate many of the new induced clauses. 
1 random uniform problems 
what bound-1 resolution did to cluster structures was that it added new induced constraints that in effect changed the search space by eliminating many near solutions. it would be natural to ask what would be the effect of local consistency enforcement on problems that do not have any special structure to begin with. can they benefit the same way cluster structures did  
　in this section we will focus on uniform random problems - the constraint graph of which does not have any special structure. we ran a series of experiments with both binary csp problems and 1sat formulas. given a random problem  first we ran a local consistency enforcement algorithm and then ran gsat. on 1sat formulas  the local consistency algorithm computes a closure under bound-1 resolution. on binary csps  the local consistency algorithm computes partial path consistency. we now take  in section 1  a small detour to discuss different versions of path-consistency algorithms. 
1 	partial path consistency 
a problem is path consistent  or 1-consistent  iff any instantiation of any two variables that is locally consistent can be extended to a consistent assignment of any third variable  montanari  1 . enforcing path consistency makes the problem more explicit: constraints that are induced by other constraints are added to the problem and thus become explicit. 
　it was shown that path consistency can potentially improve the performance of any backtracking search algorithm  mackworth  1   since it frequently eliminates all dead ends from the search space  dechter and meiri  1 . it would be interesting to know if path consistency will also improve any local search algorithm like gsat. 
　unfortunately the complexity of enforcing path consistency is 1 n1  which is too large for big problems. it was shown in  dechter and meiri  1  that for many problems the overhead of path consistency is not cost effective. therefore instead of computing path consistency exactly we will approximate it by using a restricted form of path consistency called partial path consistency. 
　the idea is the following. we want partial path consistency  ppc  to be as close to path consistency  pc  as possible. this means that whenever pc removes a tuple from a constraint  we would like ppc to do the same. in the extreme case  all tuples will be removed from the constraint and it becomes empty  which means that the problem is inconsistent. in the following experiments we use that as a criteria to measure the quality of our ppc algorithm - whenever pc generates an empty constraint  we want ppc to also generate an empty constraint. 
　the amount of changes made by path consistency depends on the amount of local consistency present in the problem in the beginning  van beek  1    van beek and dechter  1 . intuitively  the tighter the constraints and the denser the constraint graph  the more changes pc will make. it turns out that partial path consistency based on the following heuristic is almost as good as full path consistency - we choose a subset of the variables that have the highest degree and are tightly grouped together  and perform path consistency on the subproblem induced by these variables: 
	kask and dechter 	1 


1 	constraint satisfaction 

we can see  the effectiveness of path consistency depends on the tightness of the constraints. for 1 problems  both full and partial path consistency discover almost all inconsistent problems. for 1 problems  full path consistency is not able to discover inconsistent problems in the 1% area and will become effective only when the density of the constraint graph grows. we also see that partial path consistency is only slightly less effective than full path consistency  especially for problems with tight constraints. 
　we also tested arc consistency and found that while it was very fast  it very seldom changed the problem  except for problems that were very overconstrained. this shows that although arc consistency is computationally attractive  having the worst case complexity of 1 n1   it is not nearly as powerful as path consistency  and will be useful only when constraints are very tight. 
1 local consistency on random uniform problems 
in table 1. we have the results of running gsat on two sets of binary csp problems  one with n = 1 variables  k = 1 values and tightness t = 1  sparse constraint graphs  and the other with n - 1 variables  k = 1 values and t = 1 tightness1  dense constraint graphs . we ran two experiments on the same set of problems  first with just gsat and then partial path consistency  ppc  followed by gsat. for comparison we have included the average running time per problem of a backjumping algorithm with dynamic variable ordering of  frost and dechter  1 . 
　as we can see  enforcing partial path consistency does help gsat solve slightly more problems given the same upper bound maxflips. but if we include time in our 
　1 this tightness was chosen because problems with this tightness are not path consistent  van beek  1 . consideration we see that this strategy is not very useful since the total time it takes to solve a problem gets much worse. there are two reasons for this. first  enforcing  partial  path consistency is computationally expensive  although the complexity of partial path consistency grows very slowly. second  and more importantly  if the constraint graph was not complete  it will be complete after path consistency  or very close in case of partial path consistency . this will add additional complexity to the gsat search algorithm since it has to consider additional constraints at every step. notice that for cluster structures adding constraints was cost-effective while here it is not. 
　in table 1. we have the results of experiments of running gsat with and without a preprocessing algorithm br-1 on uniform random 1sat formulas with n = 1 variables and c - 1 constraints  note that this class of 1sat formulas contains both consistent and inconsistent problems . unlike rbr-1 which resolves only original clauses  rb-1 computes a closure under bound1 resolution. in this case gsat with br-1 was even slightly worse than just gsat in terms of the number of problems solved  although gsat with br-1 used fewer flips and was slightly faster. we also tried the davisputnam procedure  but it took far too long. 
1 	conclusions 
in this paper we focused on the problem of how preprocessing improves the performance of gsat. in particular  we have investigated the effect of enforcing a certain degree of local consistency  as a preprocessing step  on two different classes of problems - random uniform problems that do not have any structure and random structured problems. the effect of local consistency is sharply different on these two classes of problems. 
　our experiments show that when problems do not have any special structure  local consistency does not 
	kask and dechter 	1 

have a significant effect on the performance of gsat. even disregarding the cost of preprocessing  gsat was frequently less effective on the preprocessed problem. however  on certain classes of structured problems  local consistency can significantly improve the performance of gsat. 
　we have shown that there are structured problems that are extremely hard for gsat while easy for the davis-putnam procedure. these problems are so hard that even heuristics like clause weighting which was originally designed to help escape local minimums caused by the special structure of the problem do not seem to help much. the characteristic feature of these problems is the presence of tightly connected clusters of variables which  in turn  are loosely connected by another set of global constraints. 
　this class of problems was first discovered by konolige  konolige  1 . in this paper we have shown how to deal with these kinds of problems. our experiments show that enforcing local consistency  like bounded resolution  can make these problems almost trivial for gsat. the overhead associated with enforcing this kind of local consistency is much less than the computation needed to solve the problem without it. 
acknowledgments 
we would like to thank irina rish for running experiments with the davis-putnam procedure  and dan frost for letting us use his backjumping algorithm with dynamic variable ordering. we would also like to thank steve hampson for interesting and stimulating discussions on gsat. 
