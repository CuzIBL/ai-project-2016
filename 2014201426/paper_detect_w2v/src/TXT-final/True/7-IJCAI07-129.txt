
we investigate the problem of mining closed sets in multi-relational databases. previous work introduced different semantics and associated algorithms for mining closed sets in multi-relational databases. however  insight into the implications of semantic choices and the relationships among them was still lacking. our investigation shows that the semantic choices are important because they imply different properties  which in turn affect the range of algorithms that can mine for such sets. of particular interest is the question whether the seminal lcm algorithm by uno et al. can be upgraded towards multi-relational problems. lcm is attractive since its run time is linear in the number of closed sets and it does not need to store outputs in order to avoid duplicates. we provide a positive answer to this question for some of the semantic choices  and report on experiments that evaluate the scalability and applicability of the upgraded algorithm on benchmark problems.
1 introduction
the problem of mining frequent itemsets has been a topic of intensive research  see e.g.  agrawal et al.  1; goethals and zaki  1; han et al.  1  . since the number of such sets is huge  it is common and more efficient to restrict the search to closed item-sets  bastide et al.  1; zaki  1   where a set is closed if all its supersets have strictly lower frequency in the database. the collection of frequent closed sets contains the same information as the overall collection of frequent item-sets  but is much smaller. there is also a growing interest in mining structured data  such as graphs  and more generally multi-relational databases  and the notion of closed sets has also been imported to this richer setup  de raedt and ramon  1; yan and han  1 . however  there is no general agreement on an appropriate semantics for closed multi-relational patterns as

모모  partly supported by mecd  spanish goverment  through grant ap1  and by an eu ihp marie curie fellowship.   partly supported by nsf grant iis-1  and by a research semester fellowship award from tufts university.
 
partly supported by the eu ist fet project iq. now at the
katholieke universiteit leuven.there exist various possible settings. some approaches employ 붿-subsumption  subgraph homomorphism in the graph setting  to define the frequency of patterns  e.g.  dehaspe and toivonen  1  but others employ object identity subsumption  subgraph isomorphism   malerba and lisi  1; nijssen and kok  1 . another variation exist between mining in a single interpretation  graph   or across multiple interpretations. finally  some authors  e.g.  de raedt and ramon  1; de raedt and dehaspe  1   restrict the implication relation used in defining closures to range-restricted clauses only. in addition to these differences  the notion of a closed set can be coupled with a closure operator that takes a set and calculates its closure and there is more than one way to define such closures. the literature gives the impression that these different choices are unimportant and that algorithmic issues can be studied independentlyof the semantics. our investigation shows that this impression is false and that semantics do matter.
모our investigation follows a seminal paper by uno et al.  who utilized semantic properties in the itemset case to provide a very effective enumeration algorithm for closed sets  named lcm  linear time closed pattern miner . in this paper  we systematically explore the different semantics for mining closed sets in multi-relational databases  and study their effect on the applicability of lcm. our results identify three types of behaviors that can arise as follows.
모first  some definitions specify when a set is closed but make it hard to specify a closure operator. this is the case for the definitions used in closegraph  yan and han  1  and farmer  nijssen and kok  1 . as a result this setting is limited to methods that search by applying refinements  in bfs or dfs mode  until a closed set is found.
모secondly  stronger definitions can specify a closure operator yielding unique closures. in this case one can enumerate closed sets by iteratively extending other closed sets  their  parent    so that the search is more focused. this can be done in the setting of warmr  de raedt and ramon  1; dehaspe and toivonen  1  and jimi  maloberti and suzuki  1   which follow normal first order logic semantics. for this setting  we contribute such an enumeration algorithm generalizing lcm. however  just like the simple algorithms  this algorithm must store previously discovered sets to avoid duplicates in the output inducing a serious memory requirement.
모finally  some definitions provide unique closures such that each closed pattern has a unique  parent  closed pattern. this is the case for the claudien  de raedt and dehaspe  1  setting  whose semantics is specified using substitutions to variables in a clause. for this case  we provide a full generalization of the lcm algorithm of uno et al.  that does not need to store previous sets.
모due to space constraints most proofs and some details are omitted from the paper.
1 problem definition and basic insights
the problems we consider assume that we have a given database db  a language of patterns l and a notion of  frequency  measuring how often a pattern occurs in the database. our databases and pattern are expressed in logical notation. we assume some familiarity with basic notions of logic  lloyd  1  but the following introduces some notation and terminology.
모an atom is an expression of the form p t1 ... tn  where p is a relation of arity n and each ti is a term  i.e. a constant or a variable. this paper employs conjunctions a1 뫇 몫몫몫 뫇 an as data  where terms in the atoms are concrete objects  which we will refer to as interpretations. conjunctions also serve as patterns  where terms can be variables  which are sometimes referred to as queries. notice that when all relations are of arity 1  conjunctions correspond to item-sets. notice also that if we only have one binary predicate  we can consider it as capturing the edge relation of a graph  so graphs and graph patterns form another special case of relational patterns.
모we shall represent conjunctions in list notation  i.e. as  a1 ... an . for a conjunction c and atom p  by  c p  we refer to the conjunction that results from adding p after the last element of c. for a conjunction c and index i  c i  is the prefix of c  i.e.  a1 ... ai . a substitution 붿 = {v1/t1 ... vn/tn} maps variables to terms. the result of applying a substitution 붿 to an expression c is the expression c붿 where all variables vi have been simultaneously replaced by their corresponding term ti in 붿.
1 semantics and frequent sets
to relate the data to a query  we employ subsumption. within inductive logic programming  two notions are popular:
붿-subsumption   plotkin  1  : c1 붿-subsumes c1 if and only if there exists a substitution such that c1붿   c1.
oi-subsumption   malerba and lisi  1  : c1 oisubsumes c1 if and only if there exists a substitution 붿 = {v1/c1 ... vn/cn} such that c1붿   c1 and the ci are different terms not occurring in c1.
we will refer to both cases as subsumption  denoted as
. when we want to identify the substitution witnessing the subsumption we say that via substitution 붿. applied to graphs  the formernotion correspondsto subgraph homomorphism; the latter one to subgraph isomorphism.
모two forms of databases will be considered. in the learning from  multiple  interpretations  li  setting  the database contains a set of interpretations. in this case the natural notion of coverage is

where each interpretation contributes one element to the cover set.
모in the single interpretation  si  setting  the database is a single conjunction and the natural notion of coverage is
 via substitution 붿}
모notice that here the cover set includes substitutions and not interpretations. in both definitions  can be either of the notions of subsumption given above.
모we now have four different semantics. the li-붿 setting learns from interpretations using 붿-subsumption. this closely corresponds to the setting employed by warmr  dehaspe and toivonen  1  and jimi  maloberti and suzuki  1 . the li-oi employs oi-subsumption instead and is employed by closegraph  yan and han  1  and farmer  nijssen and kok  1 . finally  the si-붿 and si-oi settings learn from a single interpretation only. as we shall see  si-붿 closely corresponds to claudien's setting  de raedt and dehaspe  1 .
모the problem of finding frequent queries is that of finding all queries c 뫍 l whose frequency freq c db  뫟 t for some fixed threshold t. the natural notion for frequency is freq c db  = |covers c db |. it is easy to see that frequency is anti-monotonic for the li setting  i.e. adding an atom to a conjunction can only decrease the frequency. this important property is used by all frequent set miners.
remark 1 freq c db  is not anti-monotonic for the si setting. to see this  consider the query  q x   which subsumes  q x  p x z  . for interpretation  q a  p a b  p a c     q x   has one substitution but  q x  p x z   has two substitutions.
therefore  for the si setting  we use the notion of relative frequency defined as relfreq c db  = freq c db /|d|v  where d is the domain  i.e. the set of constants  and v the number of variables appearing in c. relative frequency is intuitively appealing and it is anti-monotonic for the si setting.
remark 1 note that if we have data from the li setting we can modify it slightly and interpret it in the si setting. this is a standard transformation in inductive logic programming. for example consider a dataset with two interpretations i1 =  p a  q a b  q a c   and i1 =  p d  q e f  . we add an identifier argument to each predicate and collapse the data into one interpretation: db =  p i1 a  q i1 a b  q i1 a c  p i1 d  q i1 e f  . it is easy to see that freqli c db  = |coversli c db | is antimonotonic in this case. therefore in this case we have a choice of frequency criterion to use.
1 closures
finding frequent item-sets is a special case of frequent pattern mining in the li case. when mining frequent itemsets many patterns are redundant  and therefore one is typically interested in closed patterns. indeed  consider the itemsets  a d e   a c d   b c  and  b d . then freq  a  db  = freq  a d  db  = 1. the item a is said to imply d  w.r.t. the database . therefore  we define:
definition 1 a conjunction c implies an atom p in the database db  denoted db |= c 뫸  c p   if and only if covers c db  = covers  c p  db .
the rules of the form c 뫸  c p  denote relational association rules that hold with 1 per cent confidence. they are to be understoodas implications that are satisfied by all conjunctions in the database. these expressions correspond to the relational association rules  also called query extensions  introduced in warmr  dehaspe and toivonen  1 . in case the database consists of a single interpretation only  it does not make sense to allow for atoms p that contain variables that do not appear in c. the reason is that the resulting substitutions are not comparable  cf. also remark 1. therefore  in this case one imposes the range-restriction requirement  de raedt and ramon  1   which states that p only contains variables and constants appearing in c. the resulting expression can then be simplified to a  range-restricted  clause c 뫸 p and corresponds to the pattern type induced by claudien  de raedt and dehaspe  1 .
모in the itemset case the closure of a set of items i is defined as the set of items whose existence in a tuple is implied by the existence of i. continuing our illustration  one can verify that the closure of  a  w.r.t. the specified tuples is  a d . an alternative characterization of closed item-sets states that the closure of an item-set i corresponds to the intersection of all tuples in the database subsumed by the item-set i  zaki  1 . note that intersecting item-sets corresponds to computing the minimally general generalizations of two patterns. this will also play an important role in our work.
모the item-set case motivates the iterative closure procedure  icp  defined below. note that this procedure employs a refinement operator 뷈 which computes atoms p to be added to the conjunction. notice also that the condition can be interpreted according to the four dif-
ferent semantics. closure input: pattern c = q1 ... qn;뷈: ref. operator 

1 repeat
1 find an atom
1	
1 until no such atom is found
1 output c
모we first consider a general refinement operator 뷈g that imposes no restrictions on p other than that. some of our theorems below using the normal form require the following syntactic version for the input and output of the refinement operator. we will assume that c uses variables x1 ... xm for some m and that new variables introduced form a contiguous sequences starting with xm+1. this does not change the semantics and does not restrict the form of refinements so we still refer to this case as 뷈g. until section 1 we assume that icp uses 뷈g. the following notion is natural:
definition 1  closed conjunctions of atoms  a	conjunction c is closed if closure c  = c.
the icp algorithm defines closure in a non-deterministic way that may depend on the order of atom additions. depending on the semantics  the properties of icp may vary. in particular  the result may or may not be unique  or the algorithm may not always terminate  in which case the result would not be well defined.
example 1 consider using 붿-subsumption and the dataset  r1 1   r1 1  . then the pattern r1 x1 x1  implies
 r1 x1 x1   r1 x1 x1   as well as  r1 x1 x1   r1 x1 x1   r1 x1 x1   and so on. so we can add a chain of any size  the closure is not finite in size  and the procedure may not terminate.
therefore  under 붿-subsumption  it is necessary to restrict the atoms that can be added to the initial conjunction. section 1 gives a solution that gets around this problem by abstracting the properties of the item-set case from a different perspective. section 1 gives a different solution for the si setting.
모before presenting these  we briefly consider the li-oi setting used in closegraph  yan and han  1  and farmer  nijssen and kok  1 . here the closure procedure is guaranteed to terminate but the results need not be unique:
example 1 consider the two conjunctions:
 e 1 a  e 1 b  e 1 d  e 1 a  e 1 c  
 e 1 a  e 1 b  e 1 c  
this database represents two edge-labeled graphs  and we consider calculating the closure of c = e n1 n1 a . then icp may add e n1 n1 b  to get the closed set  e n1 n1 a   e n1 n1 b  . on the other hand it can add e n1 n1 c  to get the closed set  e n1 n1 a   e n1 n1 c  .
1 normal form
most relational frequent pattern mining algorithms employ a normal-formbased on an orderingof patterns to avoid investigating the same pattern more than once. we use the following order similar to nijssen and kok . we assume that the predicates and constants are ordered  e.g. alphabetically . in addition  we employ a special set of variables z1 z1 z1 ...  ordered accordingto their index  and impose that all constants are smaller than these variables. an order over atoms is then induced by the order over lists composed by the predicate name as first element and its list of arguments following that. an order over conjunctions is induced by the order over lists where the atoms in the conjunction are listed in order.
definition 1  normal form  the normal form nf c  of a conjunction c over variables x1 ... xu is c붿 where 붿 is  1  a one to one substitution from x1 ... xu to new variables z1 ... zu and  1  c붿 is minimal in the order of conjunctions for substitutions of type  1 .
clearly the normal form of every conjunction is unique. the normal form also satisfies the following useful properties.
lemma 1 let c =  q1 q1 ... qn  be a conjunction in normal form.  i  for any i 뫞 n   q1 ... qi  is in normal form.  ii  for any i   n and any subset of indices i   j1   j1   ...   jk 뫞 n   q1 ... qi  is a prefix of the normal form of
 q1 ... qi qj1 ... qjk .
1 li-붿: the lgg closure
one way to define closures in the item-set case is by intersecting the covered tuples. the equivalent in the li-붿 setting is to apply the well-known lgg operator introduced by plotkin . the lgg of s1 s1 is a conjunction of atoms s that 붿-subsumes both s1 and   and  and in addition for any other conjunction-subsumes both s1 s1 it holds that. plotkin  proved that the lgg is well defined and provided an efficient algorithm for calculating the lgg of two conjunctions. note that we do not reduce the lgg. for sets of more than two conjunctionswe simply calculate the lgg by iteratively adding one conjunction at a time. the result of this process is unique up to variable renaming regardless of the order of adding the conjunctions. the downside here is that  since the size of the lgg may grow as the product of the sizes of the input conjunctions  the total size may grow exponentially in the number of conjunctions.
definition 1  lgg closure  let c be a conjunction and db a database  then closurelgg c  = lgg coversli c db  .
we note that a generalization of lcm using the idea of lgg is reported in  arimura and uno  1 . their setting however is specialized to ordered tree patterns with a matching relation that corresponds to oi subsumption. interestingly in this setting the size of the lgg is always small.
모we now employ formal concept analysis  fca   ganter and wille  1  to show that the lgg closure produces a galois connection. we consider the formal context  1db l  where l is the set of possible conjunctions  with the partial orders   on 1db  and . we define 뷍 c  = coversli c db  and 뷋 s  = lgg s  where c 뫍 l and s   db.
theorem 1 the mappings  form a galois connection. more specifically   1 ;
; and  1 
theorem 1 the compositionsand 붟 = 뷋 몫 뷍 are closure operators. that is  they satisfy monotonicity
  extensivity     and
idempotency  붟 붟 c   = 붟 c  .
모using this result  closed conjunctions can be formalized as those coinciding with their closure  that is  붟 c  = c. the result also establishes that for any set of interpretations s of the database  there is a unique conjunction  up to subsumption equivalence  that is the most specific conjunction satisfied in s. moreover  the number of closed queries is finite  as is the number of interpretations in our database db.
모we now develop an algorithm rellcm1 that upgrades the first algorithmin uno et al. to the li-붿 case. we need a notion of closure extension that allows us to go directly from one closed conjunction to another.
definition 1  closure extension  a conjunction c is an extension of a conjunction = closurelgg  c p   for p 뫍 뷈g c .
rellcm1 input: closed pattern c 
1 if c is not frequent then return
1 if c is already in closed pattern table then return
1 store c in closed pattern table
1 output c
1 for all refinements p 뫍 뷈g c 
1 if coversli  c p   =  
1 or coversli  c p   = coversli c 
1 then skip refinement
1 else calculate	= nf closurelgg  c p   
1 rellcm1 
1 return
모rellcm1 stores all previously discovered closed sets in a table and in this way avoids calling the procedure twice on the same set. it uses depth first search. each closed conjunction is refined in all possible ways and closed. the algorithm checks whether the resulting closed set was already discovered  and if not it is output and further refined to identify more closed conjunctions. the algorithm is started by calling rellcm1 closure    . clearly every conjunction output by the algorithm is closed. the following theorem guarantees completeness  that is  that every closed conjunction is output by the algorithm.
 theorem 1 if is a closed conjunction in normal form  then there is a closed conjunction c and a literal  such that c = nf closure.
모clearly  in any run of the algorithm the number of calls to rellcm1 is exactly the number of closed sets. while the number of calls is linear we do discover some patterns more than once. the maximum number of patterns discovered is bounded by the branching factor of the refinement operator times the number of closed sets.
1 si: range restricted closures
we now consider the si setting  both under 붿- and oi subsumption . as argued in section 1 we need to impose range-restriction when defining the closures. thus the icp algorithm employs the operator 뷈rr c  that can only generate atoms containing terms already occurring in c.
definition 1  range restricted closure  closurerr c  = closure c 뷈rr .
the next lemma shows that closurerr c  is well behaved:
lemma 1 for any conjunction c and atoms p q  if db |= c 뫸 p then db |=  c q  뫸 p.
to get a complete generalization of lcm we need to ensure that each conjunction has a unique parent. then the algorithm does not need to store the enumerated conjunctions in order to avoid duplicates as in rellcm1. therefore  we need a more refined notion of closure extension. the following generalizes the corresponding ideas from  uno et al.  1 .
definition 1  core prefix  let c be a conjunction in normal form. the core prefix core c  of c is the least prefix pr of nf c  such that covers pr db  = covers nf c  db .
notice that since we are working in the si setting  if the covers are the same then pr and c must have the same variables.
definition 1  prefix preserving closure extension  let c =  q1 ... qn  be a closed conjunction in normal form. a conjunction c is a ppc-extension of c if:
p1: = nf closurerr  c p    with p 뫍 뷈g c   i.e. c is obtained by adding the atom p to c  taking the closure of  c p  and normalizing.
p1: the atom p satisfies p   q  for all q 뫍 core c . note that because c is in normal form this only requires to check that p is larger than the last atom q in core c . p1: the normal form operation defining c does not change any of the variables in c and p. that is  the normal form may reorder atoms but may not rename atoms in  c p .
p1: let c j  = q1 ... qj be the prefix of c up to qj  where qj is the largest atom in c s.t. qj   p. then   c j  p  is a prefix of c. this means that the core prefix is preserved and that no new atom can appear between p and qj.
the following statements show that closures and cores are well-behaved  and that ppc-extensions define unique parents.
lemma 1 consider any conjunction c in normal form  any subset  core c   and any atom p 뫍/ closurerr c .  1  closurerr c    closurerr  c p  .  1  closurerr  c p   = closurerr  core c  p   = closure.
모it is worth noting that part  1  of the lemma above is violated by the lgg closure: in some cases coversli  core c  p   and the closures are different.
theorem 1  1  let c be a closed conjunction and c a ppcextension of c obtained as in condition  p1  in definition 1.
then core  where c j  is given by condition
 p1  in definition 1 .  1  let c be a closed conjunction in normal form. let t = core   that is  l is a conjunction and p is the last atom in t . let c1 = closurerr l  and let c = nf c1 . then c is a ppc-extension of c and c is the only conjunction for which this holds.
모there is one further caveat to consider: the set of closed patterns can be infinite since adding a variable changes the semantics of the conjunction. therefore one should employ a  depth bound  on the patterns searched for. the algorithm below is started by calling rellcm1 closure    .
rellcm1 input: closed pattern c = q1 ... qn 
1 if c violates depth bound then return
1 if c is not frequent then return
1 output c
1 for all refinements  c p  with p 뫍 뷈g c 
1 and s.t. p is greater than all atoms in core c 
1 if coverssi  c p   =  
1 then skip refinement
1 else calculate
1 let c j  =  q1 ... qj   where
1  qj is largest atom in c with qj   p 
1 if  c j  p  is a prefix of c
1 then rellcm1 
1 return
모as in the previous section the number of calls to rellcm1 is exactly the number of closed sets. notice that the ppcextension restriction reduces the branching factor as well. fi-
minrellcm1rellcm1-oifreq.# closed# freqcf%# closed# freqcf%11  1 s 1  1 s 11  1 s 1  1 s 111  1 s 1  1 s 11  1 s 1  1 s 111  1 s 1  1 s 11  1 s 1  1 s 111  1 s 1  1 s 11  1 s 1  1 s 111  1 s --1  1 s 1  1 s 111  1 s --1  1 s 1  1 s 1table 1: effect of decreasing the minimum frequency threshold for nctrer  with up to 1 variables in patterns and a maximum depth bound of 1 .
nally we note that the results of this section hold for both the si-oi and si-붿 settings.
1 experimental evaluation
we now present an empirical evaluation of the rellcm1 algorithm on two datasets.1 all experiments were run in a pc pentium iv em1t 1mhz under linux 1.
모the first dataset is the nctr estrogen receptor binding database  nctrer   containing 1 molecules  including bond structures.1 since each molecule is a separate structure this is natural for the li setting so the data can be viewed as db = {d1 ... dn}. however as discussed in remark 1 we can embed it as data for the si setting suitable for rellcm1. when doing this we can use |coversli c db | as our notion of frequency but still use the proper coverssi c 뫋idi  for implication and closure calculation.
모we first investigate the effect of increasing the frequency threshold for the nctrer dataset where patterns may contain at most 1 variables and depth bound of 1. table 1 summarizes the results for rellcm1 and relclm1 under object identity  rellcm1-oi . the table reports the number of patterns in each case and the associated run time in parentheses. the table also gives results for enumerating all frequent queries  this was done using a level-wise style algorithm . the first observation is that the oi restriction alone substantially decreases the number of frequent and closed sets. this partly explains the speed of systems using this restriction reported in the literature. to see the effect of using closures we calculate the compression factor as cf% =
. one can see that in both semantic settings closures further decrease the number of patterns and runtime.
모we next investigate the effect of increasing the number of variables for the nctrer dataset for a fixed threshold of 1 and a maximum depth bound of 1. results are summarized in table 1. one can see that rellcm1-oi scales much better than rellcm1 when increasing the number of variables and that the use of closures significantly reduces the number of patterns and runtime. we also observe that the cf% increases when more variables are considered. as an example  one of the closed structures enumerated which captures active molecules is:  dsstoxids x  active x  atom x y 'c'  atom x z 'o'   bond x z y single  bond x w y double  
rellcm1rellcm1-oivars# closed# freqcf%# closed# freqcf%1  1 s 1  1 s 11  1 s 1  1 s 11  1 s 1  1 s 11  1 s 1  1 s 11  1 s 1  1 s 11  1 s 1  1 s 11  1 s 1  1 s 11  1 s 1  1 s 11---1   1 s --1---1   1 s 	--table 1: effect of increasing the number of variables for the nctrer dataset  with frequency threshold 1 and maximum depth bound of 1 .
rellcm1vars# closed# freqcf%1  1 s  1  1 s  11  1 s  1  1 s 11  1 s 1  1 s 1table 1: effect of increasing the number of variables on cora with a relative frequency threshold of 1.
모the second data set is drawn from cora  a database of computer science research papers  mccallum et al.  1 . the resulting collection contains 1 objects and 1 links. here we investigate the number of generated closures when increasing the number of variables. since this is the true si setting we use relative frequency. the results for rellcm1 with threshold 1 are reported in table 1  the numbers for the oi setting are the same for this small number of variables . we see that in this domain closures are effective both in terms of compression and runtime but the effect is less pronounced than in nctrer. among the patterns discovered by rellcm1  there are closed relational sets such as:  neural netw x  probmethods y  link to p x y    saying that a good proportion of researchers publish papers in both areas.
1 conclusions
the paper investigated different semantic settings for closed clauses in multi-relational datasets. we have demonstrated that variant definitions from the literature have significant implications for properties of closed sets  and algorithms for discovering them. the paper developed relational variants of the lcm algorithm that are a promising alternative for mining closed conjunctions in datasets. the experiments demonstrated that closed sets significantly compress the number of frequent sets and that the new algorithms scale well and can be used to mine large patterns.
