
discretization  defined as a set of cuts over domains of attributes  represents an important preprocessing task for numeric data analysis. some machine learning algorithms require a discrete feature space but in real-world applications continuous attributes must be handled. to deal with this problem many supervised discretization methods have been proposed but little has been done to synthesize unsupervised discretization methods to be used in domains where no class information is available. furthermore  existing methods such as  equal-width or equal-frequency  binning  are not well-principled  raising therefore the need for more sophisticated methods for the unsupervised discretization of continuous features. this paper presents a novel unsupervised discretization method that uses non-parametric density estimators to automatically adapt sub-interval dimensions to the data. the proposed algorithm searches for the next two sub-intervals to produce  evaluating the best cut-point on the basis of the density induced in the sub-intervals by the current cut and the density given by a kernel density estimator for each sub-interval. it uses cross-validated log-likelihood to select the maximal number of intervals. the new proposed method is compared to equal-width and equal-frequency discretization methods through experiments on well known benchmarking data. 
1 introduction 
data format is an important issue in machine learning  ml  because different types of data make relevant difference in learning tasks. while there can be infinitely many values for a continuous attribute  the number of discrete values is often small or finite. when learning  e.g.  classification trees/rules  the data type has an important impact on the decision tree induction. as reported in  dougherty et al. 1   discretization makes learning more accurate and faster. in general  the decision trees and rules learned using discrete features are more compact and more accurate than those induced using continuous ones. in addition to the advantages of discrete values over continuous ones  the point is that many learning algorithms can only handle discrete attributes  thus good discretization methods are a key issue for them since they can significantly affect the learning outcome. there are different axes by which discretization methods can be classified  according to the different directions followed by the implementation of discretization techniques due to different needs: global vs. local  splitting  topdown  vs. merging  bottom-up   direct vs. incremental and supervised vs. unsupervised. 
　local methods  as exemplified by c1  discretize in a localized region of the instance space.  i.e. a subset of instances . on the other side  global methods use the entire instance space  chmielevski and grzymala-busse  1 .  
　splitting methods start with an empty list of cutpoints and  while splitting the intervals in a top-down fashion  produce progressively the cut-points that make up the discretization. on the contrary  merging methods start with all the possible cutpoints and  at each step of the discretization refinement  eliminate cut-points by merging intervals. 
　direct methods divide the initial interval in n subintervals simultaneously  i.e.  equal-width and equalfrequency   thus they need as a further input from the user the number of intervals to produce. incremental methods  cerquides and mantaras  1  start with a simple discretization step and progressively improve the discretization  hence needing an additional criterion to stop the process. 
　supervised discretization considers class information while unsupervised discretization does not. equal-width and equal-frequency binning are simple techniques that perform unsupervised discretization without exploiting any class information. in these methods  continuous intervals are split into sub-intervals and it is up to the user specifying the width  range of values to include in a sub-interval  or frequency  number of instances in each sub-interval . these simple methods may not lead to good results when the continuous values are not compliant with the uniform distribution. additionally  since outliers are not handled  they can produce results with low accuracy in the presence of skew data. usually  to deal with these problems  class information has been used in supervised methods  but when no such information is available the only option is exploiting unsupervised methods. while there exist many supervised methods in literature  not much work has been done for synthesizing unsupervised methods. this could be due to the fact that discretization has been commonly associated with the classification task. therefore  work on supervised methods is strongly motivated in those learning tasks where no class information is available. in particular  in many domains  learning algorithms deal only with discrete values. among these learning settings  in many cases no class information can be exploited and unsupervised discretization methods such as simple binning are used.  
　the work presented in this paper proposes a top-down  global  direct and unsupervised method for discretization. it exploits density estimation methods to select the cut-points during the discretization process. the number of cutpoints is computed by cross-validating the log-likelihood. we consider as candidate cutpoints those that fall between two instances of the attribute to be discretized. the space of all the possible cut-points to evaluate could grow for large datasets that have continuous attributes with many instances with different values among them. for this reason we developed and implemented an efficient algorithm of complexity nlog n  where n is number of instances. 
　the paper is organized as follows. in section 1 we describe non-parametric density estimators  a special case of which is the kernel density estimator. in section 1 we present the discretization algorithm  while in section 1 we report experiments carried out on classical datasets of the uci repository. section 1 concludes the paper and outlines future work. 
1 non-parametric density estimation 
since data may be available under various distributions  it is not always straightforward to construct density functions from some given data. in parametric density estimation  an important assumption is made: available data has a density function that belongs to a known family of distributions  such as the normal distribution or the gaussian one  having their own parameters for mean and variance. what a parametric method does is finding the values of these parameters that best fit the data. however  data may be complex and assumptions about the distributions that are forced upon the data may lead to models that do not fit well the data. in these cases  where making assumptions is difficult  nonparametric density functions are preferred.  
　simple binning  histograms  is one of the most wellknown non-parametric density methods. it consists in assigning the same value of the density function f to every instance that falls in the interval  c - h/1  c + h/1   where c is the origin of the bin and h is the binwidth. the value of such a function is defined as follows  symbol # stands for 'number of' :  
1
f =#{instances  that fall in c } n h
once fixed the origin c of a bin  for every instance that falls in the interval centered in c and of width h  a block of size 1 by the bin width is placed over the interval  figure 1 . here  it is important to note that  if one wants to get the density value in x  every other point in the same bin  contributes equally to the density in x  no matter how close or far away from x these points are. 

figure 1. simple binning places a block in every sub-interval for every instance x that falls in it 
this is rather restricting because it does not give a real mirror of the data. in principle  points closer to x should be weighted more than other points that are far from it. the first step in doing this is eliminating the dependence on bin origins fixed a-priori and place the bin origins centered at every point x. thus the following pseudo-formula: 
1
         #{instances that fall in a bin containing x} n binwidth	
should be transformed in the following one:  
1
         #{instances that fall in a bin around x} n binwidth
the subtle but important difference in constructing binning density with the second formula  permits to place the bin around x and the calculation of the density is performed not in a bin containing x and depending from the origin c  but in a bin whose center is upon x. the bin center on x  allows successively to assign different weights to the other points in the same bin in terms of impact upon the density in x depending on the distance from x. if we consider intervals of width h centered on x  then the density function in x is given by the formula: 
1
	f =    	 #{instances that fallin x h x h }
1hn
in this case  when constructing the density function  a box of width h is placed for every point that falls in the interval centered in x. these boxes  the dashed ones in figure 1  are then added up  yielding the density function of figure 1.  this provides a way for giving a more accurate view of what the density of the data is  called box kernel density estimate. however  the weights of the points that fall in the same bin as x have not been changed yet. 

figure 1. placing a box for every instance in the interval around x and adding them up. 
in order to do this  the kernel density function is introduced:                 p = 
where k is a weighting function. what this function does is providing a smart way of estimating the density in x  by counting the frequency of other points xi in the same bin as x and weighting them differently depending on their distance from x. contributions to the density value of f in x from points xi vary  since those that are closer to x are weighted more than points that are further away. this property is fulfilled by many functions  that are called kernel functions. a kernel function k is usually a probability density functions that integrates to 1 and takes positive values in its domain. what is important for the density estimation does not reside in the kernel function itself  gaussian  epanechnikov or quadratic could be used  but in the bandwidth selection  silverman 1 . we will motivate our choice for the bandwidth  the value h in the case of kernel functions  selection problem in the next section where we introduce the problem of cutting intervals based on the density induced by the cut and the density given by the above  kernel density estimation. 
1 where and what to cut 
the aim of discretization is always to produce sub-intervals whose induced density over the instances best fits the available data. the first problem to be solved is where to cut. while most supervised top-down discretization method cut exactly at the points in the main interval to discretize that represent instances of the data  we decided to cut in the middle points between instance values. the advantage is that this cutting strategy avoids the need of deciding whether the point at which the cut is performed is to be included in the left or in the right sub-interval. 
　the second question is which  sub- interval should be cut/split next among those produced at a given step of the discretization process. such a choice must be driven by the objective of capturing the significant changes of density in different separated bins. our proposal is to evaluate all the possible cut-points in all the sub-intervals  by assigning to each of them a score according to a method whose meaning is as follows. given a single interval to split  any of its cutpoints produces two bins and thus induces upon the initial interval two densities  computed using the simple binning density estimation formula. such a formula  as shown in the previous section  assigns the same density value of the function f to every instance in the bin and ignores the distance from x of the other instances of the bin when computing the density in x. every sub-interval produced has an averaged binned density  the binned density in each point  that is different from the density estimated with the kernel function. the less this difference is  the more the sub-interval fits the data well  i.e. the better this binning is  and hence there is no reason to split it. on the contrary  the idea underlying our discretization algorithm is that  when splitting  one must search for the next two worst sub-intervals to produce  where  worst  means that the density shown by each of the sub-intervals is much different than it would be if the distances among points in the intervals and a weighting function were considered. the identified worst sub-intervals are just those to be split to produce other intervals  because they do not fit the data well. in this way intervals whose density differs much from the real data situation are eliminated  and replaced by other sub-intervals. in order to achieve the density computed by the kernel density function we should reproduce a splitting of the main interval such as that in figure 1.  
　an obvious question that arises is: when a given subinterval is not to be cut anymore  indeed  searching for the worst sub-intervals  there are always good candidates to be split. this is true  but on the other hand at each step of the algorithms we can split only one sub-intervals in other two. thus if there are more than one sub-interval  this is the case after the first split  to be split  the scoring function of the cut-points allows to choose the sub-interval to split. 
1 the scoring function for the cutpoints 
at each step of the discretization process  we must choose from different sub-intervals to split. in every sub-interval we identify as candidate cut-points all the middle points between the instances. for each of the candidate cut-points t we compute a score as follows: 
	k	n
score t   =   p xi   f  xi    p xi   f  xi  
	i 1	i k 1
where i= 1 .. k refers to the instances that fall into the left sub-interval and i= k +1 .. n to the instances that fall into the right bin. the density functions p and f are respectively the kernel density function and the simple binning density function. these functions are computed as follows: 
m
 	        f xi  =
w n
where m is the number of instances that fall in the  left or right  bin  w is the binwidth and n is the number of instances in the interval that is being split. the kernel density estimator is given by the formula: 
	p xi  =	
where h is the bandwidth and k is a kernel function. in this framework for discretization  it still remains to be clarified how the bandwidth of the kernel density estimator is chosen. although there are several ways to do it  as reported in  silverman 1   in fact in this context we are not interested in the density computed by a classic kernel density estimator that considers globally the entire set of available instances. the classic way a kernel density estimation works considers n as the total number of instances in the initial interval and chooses h as the smoothing parameter. the choice of h is not easy and various techniques have been investigated to find an optimal h. our proposal  in this context  is to adapt the classic kernel density estimator by taking h equal to the binwidth w  specified as follows. indeed  as can be seen from the formula of p xi   instances that are more distant than h from xi  contribute with weight equal to zero to the density of xi. hence  if a sub-interval  bin  under consideration has binwidth h  only the instances that fall in it will contribute  depending on their distance from xi  to the density in xi. as we are interested in knowing how the current binned density  induced by the candidate cut-point and computed by f with binwidth w  differs from the density in the same bin but computed weighting the contributions of xj to the density in xi on the basis of the distance xi - xj  it is useless to consider  for the function p  a bandwidth greater than w.
1 the discretization algorithm 
once a scoring function has been synthesized  we explain how the discretization algorithm works. figure 1 shows the algorithm in pseudo language. it starts with an empty list of cut-points  that can be implemented as a priority queue in order to maintain  at each step  the cut-points ordered after their value according to the scoring function  and another priority queue that contains the sub-intervals generated thus far. let us see it through an example. suppose the initial interval to be discretized is the one in figure 1  frequencies of the instances are not shown . 
discretize interval  
begin 
potentialcutpoints = computecutpoints interval ; 
	 	priorityqueueintervals.add interval ; 
while stopping criteria is not met do 
 	 	if priorityqueuecps is empty  
　　foreach cutpoint cp in potentialcutpoints do  	 	scorecp = computescoringfunction cp interval ; 
	 	 	priorityqueuecps.add cp scorecp ;  	 	 	 
end for 
 	 	else  
        bestcp = priorityqueue.getbest  ; 
        currentinterval = priorityqueueintervals.getbest  ; 
 	    newintervals = split currentinterval bestcp ; 
 	 	leftinterval = newintervals.getleftinterval  ; 
  rightinterval = newintervals.getrightinterval  ;   potentialleftcps  = computecutpoints leftinterval ; 
potentialrightcps =computecutpoints rightinterval ;  	 	foreach cutpoint cp in potentialleftcps scorecp = computescoringfunction cp leftinterval ; priorityqueuecps.add cp scorecp ; 
priorityqueueintervals.add leftinterval scorecp ; 
	 	 	end for  
 	 	 	    // the same foreach cycle for potentialrightcps  
	end while  	 
end
figure 1. the discretization algorithm in pseudo language 

figure 1. the first cut 
the candidate cut-points are placed in the middle of adjacent instances: 1  1  1  1; the sub-intervals produced by cut-point 1 are  1   1  and  1   1   and similarly for all the other cut-points. now  suppose that  computing the scoring function for each cut-point  the greatest value  indicating the cut-point that produces the next two worst sub-intervals  is reached by the cut-point 1. then the sub-intervals are:  1   1  and  1   1  and the list of candidate cut-points becomes  1  1  1  1  1 . suppose the scoring function evaluates as follows: score 1  = 1  score 1  = 1  score 1  = 1  score 1  = 1  score 1  = 1. the algorithm selects 1 as the best cut-point and splits the corresponding interval as shown in figure 1. 

figure 1. the second cut 
this second cut produces two new sub-intervals and hence the current discretization is made up of three sub-intervals:  1   1    1   1    1   1   with candidate cutpoints  1  1  1  1 . suppose values of the scoring function are as follows: score 1  = 1  score 1  = 1  score 1  = 1  score 1  = 1. the best cut-point 1 suggests the third cut and the discretization becomes  1   1    1   1    1   1    1   1 . thus  the algorithm refines those sub-intervals that show worst fit to the data. a note is worth: in some cases it might happen that a split is performed even if one of the two sub-intervals  which could be the left or the right one  it produces shows such a good fit  compared to the other sub-intervals  that it is not split in the future. this is not strange  since the scoring function evaluates the overall fit of the two sub-intervals. this is the case of the first cut in the present example: the cut-point 1 has been chosen  where the left sub-interval  1   1  shows good fit to the data in terms of density while the right one  1   1  shows bad fit. in this case the interval  1   1  will not be cut before the interval  1   1  and perhaps will remain untouched till the end of the discretization algorithm. the algorithm will stop cutting when the stopping criterion  the maximal number of cut-points  computed by a procedure explained in the next paragraph  is met. 
1 stopping criteria and complexity 
the definition of a stopping criterion is fundamental  to prevent the algorithm from continuing to cut until each bin contains a single instance. even without reaching such an extreme situation  the risk of running into overfitting the model is real  because  as usual in the literature  we use loglikelihood to evaluate the density estimators  the simple binning and the kernel density estimate. as a solution  instead of requiring a specific number of intervals  that could be too rigid and not based on valid assumptions   we propose the use of cross-validation to provide an unbiased estimation of how the model fits the real distribution. for the experiments performed the 1-fold cross-validation was used. for each fold the algorithm computes the stopping criterion as follows: supposing there are n - 1 candidate cut-points  for each of them the cross-validated loglikelihood is computed. in order to optimize performance  at each step a structure maintains the sub-intervals in the current discretization and the corresponding splitting values  so that only the new values for the interval to be split have to be computed at each step. thus the algorithm that computes the log-likelihood for the n - 1 cut-points is performed 1 times overall. the number of cut-points that shows the maximum value of the averaged log-likelihood on the test folds is chosen as the best. the log-likelihood on the test data is given by the following formula: 
	n	nj train
 	 	 	log-likelihood  =  	j test log
	j 1	w n
where nj-train is the number of training instances in bin j  nj-test is the number of test instances that fall in bin j  n is the total number of instances and w is the width of bin j.
　as regards the kernel density estimator complexity  from the formula of p  it can be deduced that the complexity for evaluating the kernel density in n points is n1. for univariate data  the complexity problem has been solved by the algorithms proposed in  greengard and strain  1  and   yang et al 1  which compute the kernel density estimate in o n+n  instead of o n1 . in our context we deal only with univariate data because only single continuous attributes have to processed  and thus for n instances  the theoretical complexity of the algorithm is o nlogn .
1 experiments 
in order to assess the validity and performance of the proposed discretization algorithm  we have performed  experiments on several datasets taken from the uci repository and classically used in the literature to evaluate discretization algorithms in the past. specifically  the dataset used are: autos  bupa  wine  ionosphere  ecoli  sonar  glass  heart  hepatitis  arrhythmia  anneal  cylinder  and auto-mpg. these datasets contain a large set of numeric attributes of various types  from which 1 continuous attributes were extracted at random and used to test the discretization algorithm. 
　in order to evaluate the discretization carried out by the proposed algorithm with respect to other algorithms in the literature  we compared it to three other methods: equalwidth with fixed number of bins  we use 1 for the experiments   equal-frequency with fixed number of bins  we use 1 for the experiments   equal-width cross-validated for the number of bins. the comparison was made along the loglikelihood on the test data using a 1-fold cross-validation methodology. the results on the test folds were compared through a paired t-test as regards cross-validated loglikelihood. table 1 presents the results of the t-test based on cross-validated log-likelihood with a risk level  = 1. it shows the number of continuous attributes whose discretization through our method was significantly better  equal or significantly worst compared to the other methods. 
 our method 
significanlty more accurate      
equalour method 
significanlty less accurate equalwidth 1 bins 1  1%  1 1   1%  equalfreq 1 bins 1  1%  1 1   1%  equalwidth 
crossvalidated1  1%  1 1   1%  table 1. results of paired t-test based on cross-validated log-likelihood on 1 folds. 
　it is clear that  even if in the majority of cases the new algorithm shows no difference in performance with respect to the others  there is an outstanding percentage of cases  at least 1%  in which it behaves better  while the opposite holds only in very rare cases. among the datasets there can be found many cases of continuous attributes whose interval of values contain many occurrences of the same value. this characteristic had an impact on the results of the equal frequency method that often  in such cases  was not able to produce a valid model that could fit the data. this is natural  since this method creates the bins based on the number of instances that fall in it. for example if the total number of instances is 1 and the bins to generate are 1  then the number of instances that must fall in a bin is 1. thus  if among the instances there is one that has 1 occurrences  then the equal frequency method is not able to build a good model because it cannot compute the density of the bin that contains only the occurrences of the single instance. this would be even more problematic in case of cross-validation  which is the reason why no comparison with the equal frequency cross-validation method was carried out. 
　an important note can be made concerning  very  discontinuous data  on which our method performs better than the others. this is due to the ability of the proposed algorithm to catch the changes in density in separated bins. thus very high densities in the intervals  for example large number of instances in a small region  are  isolated  in bins different from those which  host  low densities.  although it is not straightforward to handle very discontinuous distributions  the method we have proposed achieves good results when trying to produce bins that can fit these kind of distributions.  
1 conclusions and future work  
discretization represents an important preprocessing task for numeric data analysis. so far many supervised discretization methods have been proposed but little has been done to synthesize unsupervised methods. this paper presents a novel unsupervised discretization method that exploits a kernel density estimator for choosing the intervals to be split and cross-validated log-likelihood to select the maximal number of intervals. the new proposed method is compared to equal-width and equal-frequency discretization methods through experiments on well known benchmarking data. preliminary results are promising and show that kernel density estimation methods are good for developing sophisticated discretization methods. further work and experiments are needed to fine-tune the discretization method to deal with those cases where the other methods show better accuracy.
　as future application we plan to use the proposed discretization algorithm in a learning task that requires discretization and where no class information is always available. one such context could be inductive logic programming  where objects whose class is not known  are often described by continuous attributes. this investigation will aim at assessing the quality of the learning task and how this is affected by the discretizaton of the continuous attributes. 
