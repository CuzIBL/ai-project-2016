
in behavioural sciences  the problem that a sequence of stimuli is followed by a sequence of rewards r t  is considered. the subject is to learn the full sequence of rewards from the stimuli  where the prediction is modelled by the sutton-barto rule. in a sequenceof n trials  this prediction rule is learned iteratively by temporaldifferencelearning. we presenta closed formula of the prediction of rewards at trial time t within trial n. from that formula  we show directly that for n ¡ú ¡Þ the predictions converge to the real rewards. in this approach  a new quality of correlation type toeplitz matrices is proven. we give learning rates which optimally speed up the learning process.
1 temporal difference learning
we consider here a mathematical treatment of a problem in behavioural biology. this problem has been described e.g. in  dayan and abbott 1  as learning to predict a reward. it is pavlovian in the sense that classical conditioning is adressed  however reward is not immediate  but after a series of stimuli there is a latency time  followed by a series of rewards. using a simple linear rule  we will show that the subject is able to predict the remaining rewards by that rule  repeating the same stimuli-reward pattern over a series of trials. a review of learning theory related to these problems can be found in  sutton and barto 1 .
¡¡there are recent experimental biological correlates with this mathematical model  e.g. in the activity of primate dopamine cells during appetitive conditioning tasks  together with the psychological and pharmacological rationale for studying these cells. a review was given in  schultz 1   the connection to temporal difference learning can be found in  montague et al. 1 .
¡¡in this paper  the focus is on two mathematical issues: a  to provide a direct constructive proof of convergence by giving an explicit dependenceof the prediction error over trials  b  to minimize the learning time by giving a formula for optimal setting of the learning rate. hence  the paper contributes as well to temporal difference learning as a purely mathematical issue  which may be valuable also without reference to behavioural biology and reinforcement learning. it can also be understood in a dynamic programming sense  see  watkins 1 and later work  e.g.  gordon 1 and  szepesvari and smart 1 . we adopt the following notation in the course of this paper  following  dayan and abbott 1 :
  stimulus u t 
  future reward r t 
  sum of future rewards r t 
  weights w k 
  predicted reward v t 
we want to compute  for all trials n of duration t  and for any time of trial t  the predicted reward vn t . the  extended  stimulus u t  is given at times tu min ...tu max   the  extended  reward r t  is presented at times tr min ...tr max.
stimulus and reward do not overlap  i.e. tr min   tu max.
   the subject is to learn the total remaining reward at time t   only after stimulus onset! 
¡¡the brackets  refer  in general  to stochastic values of r t   if not exactly the same rewards are given at each trial  but when there are fluctuations.
¡¡all previous stimuli u t  are weighted to give a linear prediction model  sutton and barto 1 :
		 1 
an update rule for the sutton-barto-formulacan easily be derived from the mean square error  dayan and abbott 1 :

using the partial derivative with respect to any weight w ¦Á  

updates are made in discrete steps in the direction of the negative gradient  providing the following rule:
 1  obviously  the total future reward r t  is not known to the subject at time t. we can use r t  = r t  +r t+ 1 . in this formulation  again r t + 1  is not known. the subject can approximate the value by his prediction v t + 1 . this will
provide the so-called
temporal difference rule at time t:
¦¤w ¦Ó  = ¦Å¦Ä t u t   ¦Ó     ¦Ó = 1...t ¦Ä t  = r t  + v t + 1    v t   1 
the updates can be made sequentially  after each time step t  or in parallel  sampling the weight updates for all times in a given trial  and then updating at the end of the trial. the two alternatives do not substantially differ in the final result after many trials. for computational reasons  we use the parallel version of the update rule. other learning schedules may be adopted  however we use eq.  1  since this is widely accepted in the community  following  dayan and abbott 1 . let us denote the trial number by superscripts n.
1 dynamics of temporal difference learning
the parallel temporal difference rule eq.  1  was obtained  using an approximation to gradient descent. therefore  convergence is not guaranteed and shall be proven in the course of this paper  where a compact notation will be introduced. we proceed as follows:
  incorporatethe rule into the prediction formula  yielding a recursive formula for all predictions vn t  at trial n.
  make this a closed formula vn t .
  show convergence vn t  ¡ú r t  for large n.
  choose an optimal learning rate ¦Å for maximally fast convergence.
we obtain for the predictions vn+1 t  at trial n + 1 the following recursive formula. summation limits explicitely state situations where stimuli and reward phases may overlap  we will restrict to non-overlapping cases later.

 1 
where ¦Än t  = r t  + vn t + 1    vn t  and

since we are interested whether the subject will make proper predictions after all stimuli have been given  we will restrict our analysis to times t   tu max. then g t y  will become independent of t  giving

this can be written in matrix notation as follows  with e the unit matrix:
vn+1 tu max  vn+1 tu max + 1   
	    ...	      =  e   ¦Åga  ¡Á
 vn+1 tr max 
	vn tu max 	1
	     vn tu max + 1      	        ...	r r minmax        
	¡Á	...	+ ¦Åg	r t	 
	  n	r max	  	...
	  v  t	 	r t	 
where we shall now use the total time t = tr max tu max+1:

with any real values  and t ¡Á tmatrices
	g1	g1	¡¤¡¤¡¤	gk	1
.
	g1	g1	g1	¡¤¡¤¡¤	.. gk  
	g =    ...	...	...	...	...         and
	    gk	¡¤¡¤¡¤k	g1	g1	g1
.
1 .. g	¡¤¡¤¡¤	g	g1
1  1
	1	 1	 
a 1 
¡¡¡¡1    1  1
where the  1  in g refers to the full remaining upper and lower triangle  respectively  and in a all entries except for the two diagonals are 1. we can further write this in compact notation  vectors have component index t :
vn+1 =  e   ¦Åga  vn + ¦Ågr
with v1 = 1 one has
n
vga n gr
n=1
= ¦Å ¦Åga  1 e    e   ¦Åga  n+1  gr

only if   ga  is a hurwitz  stability  matrix  all eigenvalues have negative real part   a suitable ¦Å can be chosen such that  e   ¦Åg a  n+1  will vanish with large n. a similar approach  relying on hurwitz matrices  was followed in  sutton 1   however there the hurwitz property was not shown immediately on the matrix structure of eq.  1 . our aim here is to provide an explicit proof  which will establish as an interesting result in its own right a general property of toeplitz matrices with correlation entries.
¡¡we will show the hurwitz property later  and we will give values for ¦Å for which convergenceis assured and for which it is optimal. continuing with the large n-behaviour we obtain vn+1 n ¡ú¡Þ¡ú a 1 r . this holds no matter what the stimulus!
with
a
we obtain	
which is the desired result. this proves convergence in full generality. we need to show that   ga  is a hurwitz matrix  i.e. that all eigenvalues of that matrix have negative real part. further  we want to give values for ¦Å which provide maximum speed of convergence.
1 proof of convergence
in previous approaches in the literature  temporal difference  td  learning was extended to td ¦Ë  learning where the parameter ¦Ë refers to an exponential weighting with recency. learning and convergence issues were considered e.g. in  dayan 1  and  dayan and sejnowski 1 . in the td ¦Ë  framework  what we consider here is td 1 -learning where in contrast to the mentioned literature we concentrate on a direct proof of convergence by establishing a general property of toeplitz matrices with correlation entries.
¡¡the proof proceeds as follows: we will show first that g is positive definite. then  we will show that all eigenvalues of  ga  have positive real part. hence  we will have established the hurwitz property required for convergence.
¡¡in order to see that g is positive definite  we consider any nonzero real vector x and show that xtgx is always positive. we first extractfrom g the diagonalmatrix gd and the upper triangular matrix g+. then we have
xtgx = xtgdx + xtg+x + xtg+tx
= xtgdx + xtg+x +  xtg
= xtgdx + 1xtg+x

where in order to keep summation limits easy it is understood that xi = 1 for i   1 and i   n   zero padding  . there are 1k+1 toeplitz bands  k being the number of stimuli. shifting the time index in eq.  1  from tu min to 1  g j  arises from the k stimuli for

hence xtgx can be written as

rearranging the inner sums with
		 1 
and using vectors u of dimension  k + 1  with components ut = u t  leads to
	xtgxutx  i u	 1 
where the matrices x  i  are  k + 1  ¡Á  k + 1  band matrices with entries x  m mi  +n = xixi+n for all m and n ¡Ý 1  and 1 otherwise. symmetrizing the matrix products  we can write
utx u
 1  the matrix elements of the second term are  for  k ¡Ü n ¡Ü 1:

where the summation limits in the last result were taken  for convenience  larger than necessary for covering all cases of n for which the summand is nonzero  note that n is nonpositive   the extra terms being zero due to the zero padding convention for the xi. as a result  the matrix elements of the first and the second term in eq.  1  have identical format  for nonnegative and nonpositive n  respectively.
¡¡making use of eq.  1  and eq.  1   and incorporating the first sum in eq.  1  leads to
	xtgxu	 1 
where the matrices x  i  are band matrices with entries
x  m mi 	+n = xixi+n for positive and negative n.
¡¡we can now write the sum of matrices in eq.  1  in a different way. denote vectors x  i  of dimension  k + 1  with components x  mi  = xi+m  m = 1...k  where  as before  it is understood that xj = 1 for j   1 and j   n. hence  these vectors will have nonzero components only for i =  k...n.
¡¡notice that the following covariancematrices are generated by these vectors:
		 1 
then we have for  m = 1...k n =  m...k   m :
		 1 
and

which establishes that
		 1 
then we can rewrite eq.  1   using eq.  1 :
xtgxutx 
this sum is certainly nonnegative. we will now show that it cannot become zero.
¡¡the proof is by contradiction. suppose the sum is zero. then all individual terms must be zero. start with the first term. we have from the definition
	utx 	 1 
this becomes zero  since  eq.  1    only for x1 = 1. now proceed with the second term:
utx 
 1 
where the previous result x1 = 1 was used. this becomes zero  since  eq.  1    only for x1 = 1. continuing in this vein  after n steps  we end with the product utx  j=1
= u1xn k + ... + uk 1xn 1 + ukxn = ukxn
where all the previous results were used. hence xn = 1  which in total means that x = 1. this is a contradiction  since x must be a nonzero vector.
¡¡with this result  we have established that xtgx   1  hence we have established:
¡¡symmetric toeplitz matrices of the structure of g after eq.  1   with correlation entries gj after eq.  1   are positive definite.
¡¡we now turn our attention to showing that all eigenvalues of  ga  have positive real part. we will look at any real a and any g which is real  symmetric and positive definite.  we will not require the stronger condition that the structure of g is after eq.  1   and we say nothing about the entries gj. 
¡¡under these premises  the proof will first give a sufficient condition for the sign of the real part of the eigenvalues of  ga . then we will show that this sign is indeed positive for the a at hand.
¡¡let y be any eigenvector of  ga  and ¦Ë be the corresponding eigenvalue. since  ga  is real  y and ¦Ë are either both real or both complex. in the complex case  there exists a further eigenvector y  and corresponding eigenvalue ¦Ë   where     denotes complex conjugated and transposed.
let us denote z = ay and write
	z gz = y a gay = ¦Ëy a y	 1 
also  with g real and symmetric  we have g = g  and
	z gz =  z gz   = ¦Ë y ay	 1 
the summation of eqns. 1 and 1 gives
	1z gz = y   ¦Ë a + ¦Ëa  y	 1 
with g real  symmetric and positive definite  we have for any complex z that z gz is real and positive. this can be used as follows.
case 1: y and ¦Ë are both real.
then we have  with real a  from eq.  1  immediately
	1   re 1zy	 1 
hence

case 1: y and ¦Ë are both complex.
let us write y = v+iw and ¦Ë = g+ih. then from eq.  1   with real a  we obtain
	1	 	re 1z	 1 
v
	1	=	im 1zv	 1 
v combining eqns. 1 and 1 gives

from which follows with sign re ¦Ë   = sign g :

the results for both cases  eqns. 1 and 1  allow the following sufficient condition:
let g be a real  symmetric and positive definite matrix  and a be a real square matrix. if the matrix  is positive definite  then the real parts of the eigenvalues of  ga  are positive.
¡¡let us now turn our attention to the specific matrix a given in eq.  1 . we have
   1    1
at + a =       1
1
... 1
...
 1...
1
 1 
 1       1 it can be shown that the matrix q = at +a is positive definite  by using the previous result of this paper: matrices of the structure of g are positive definite. noting that q has the structure of g  using eq.  1  with k = 1 and u =  1 -1   immediately gives the desired result. this completes our proof that   ga  is a hurwitz matrix.
1 learning rate ¦Å for fast convergence
the convergence behaviour of the temporal difference rule is known from eq.  1 . for large number of trials  convergence speed will be dominated by the eigenvalue of  e   ¦Åg a  with the largest modulus  for which convergence speed is slowest. we have for the eigenvalues  ev :
 ev e   ¦Åga  = 1   ¦Å ev ga  = 1   ¦Å  g + ih   1  where the same notation for the eigenvalues of ga as in the previous section  ¦Ë = g + ih  was used.
¡¡hence  the first task to do is to compute the eigenvalues of ga which are only dependent on the given stimuli u k  and on the total time t. note that ga defines the special structure of an unsymmetric toeplitz matrix with 1k + 1 bands. it is well known that a closed solution for the eigenvalues of toeplitz matrices exists only for not more than three bands  they are given for symmetric and asymmetric cases e.g. in  beam and warming 1 . hence only for k = 1 can a closed solution been given  which we do in sec. 1. for k   1 numerical values must be obtained  special methods for the solution  and for the asymptotic structure of the eigenvalues for large t  are given in  beam and warming 1 .
the square modulus m of the eigenvalues is given by
	m ¦Å  = | ev e   ¦Åga  |1 =  1   ¦Å g 1 + ¦Å1 h1	 1 
we will first give a choice for ¦Å for which convergence is always assured and which also serves as a good rough value for setting ¦Å without much computational effort.
¡¡note first that m ¦Å = 1  = 1 for all eigenvalues. further  since   g a  is hurwitz  g   1 for all eigenvalues  and for small ¦Å   1  m ¦Å    1 for all eigenvalues due to eq.  1 . lastly  for large ¦Å  m ¦Å  will increase again beyond all bounds for all eigenvalues  since m ¦Å  ¡Ø ¦Å1 for large and all eigenvalues.
¡¡after this sketch of the general behaviour of m ¦Å   it is clear that for each eigenvalue k there is a for which m ¦Å k   = 1 and m ¦Å k   is rising with ¦Å k . computing
these values from eq.  1  for eigenvalues ¦Ëk = gk + ihk gives
1gk
	¦Å k  = gk1 + h1k	 1 
hence  m ¦Å    1 for 1   ¦Å   mink ¦Å k   and a good choice ¦Å for convergent behaviour is therefore the mean value of the bounds 
		 1 
note again that whenever   there is an additional eigenvalue gk   ihk for which the minimum in eq.  1  is identical and needs not be computed.
¡¡we now turn to finding the optimal value ¦Å  for fastest convergence. this is given by
	¦Å  = argmin¦Å maxk m ¦Å k  	 1 
this optimization problem has no closed solution. however  a simple line search algorithm can be given to find the solution in finitely many  less than t  steps.
¡¡the general idea is to start at ¦Å = 1 and increase ¦Å until we reach ¦Å . in all of this process  we select a k and go along curves m ¦Å k    ensuring the condition that our chosen k satisfies maxk m ¦Å k  .
¡¡at ¦Å = 1  all m ¦Å k   = 1 and decreasing with ¦Å. the slope is given by  1gk   1. we start by choosing the k  for which k  = argmink gk  which ensures that the condition maxk m ¦Å k   is satisfied by our choice of k . our current position is ¦Åc = 1. then we apply the following procedure:  start 
continuing on curve k   we would reach the minimum for
		 1 
if everywhere between our current position ¦Åc and the proposed position the maximum condition maxk m ¦Å k   is satisfied for k . in order to check this  we compute the intersections of our curve under inspection k  to all other curves k.
¡¡after eq.  1   these intersections are given at values ¦Åk according to
	 1   ¦Åk gk 1 + ¦Å1k h1k =  1   ¦Åk gk  1 + ¦Å1k h1k 	 1 
or
		 1 
¡¡now we inspect the intersection which is closest to our current position of ¦Åc  i.e. for which k  = argmink{ ¦Åk  ¦Åk   ¦Åc}.
¡¡if ¦Åk    ¦Å k   then ¦Å k  after eq.  1  is indeed the solution  free minimum .  stop 
¡¡else  there is an intersection prior to reaching. if the curve that is intersecting is rising at ¦Å k    this means that we are done  since we actually have reached the condition of eq.  1 . the solution is
 bounded minimum .  stop 
¡¡else  if the curve that is intersecting is falling at ¦Å k    we must continue on that new curve which now satisfies the maximum condition in eq.  1 . to this end  we set ¦Åc = ¦Å k   and
k  = k  and continue  change in falling curves with maximum modulus .  go to start 
¡¡fig. 1 illustrates the behaviour with free minimum  fig. 1 with boundedminimum. both figures have a changein falling curves with maximum modulus.
¡¡this algorithm terminates after finitely many steps  either at the minimum of the curve k  or at an intersection of a falling with a rising curve. it is clear that one of the two possibilities exist since all curves eventually rise with large ¦Å.
1 one stimulus
we look at the special case where only one stimulus u is present  hence k = 1. then  eq.  1  takes a particularly simple form  where  e   a  is the unit shift matrix:
avr
=1
 1 

figure 1: free minimum: starting at ¦Å = 1  the solid curve has maximum modulus  k  . at the circle  another falling curve  dashed  is intersecting and takes the maximum role of k . the global minimum of that curve  diamond  is as well the total minimum after eq.  1   with m = 1. the next intersection with the rising  dotted  curve  square  is of no importancesince it occurs at a higher ¦Å. the vertical line indicates the approximate value ¦Å after eq.  1  with non-optimal m ¦Å   = 1.

figure 1: bounded minimum: as in fig. 1  starting at ¦Å = 1  first the solid curve and then the dashed curve have maximum modulus  k  . the next intersection  square  occurs with the rising  dotted  curve  hence this intersection is the total minimum after eq.  1   with m = 1. the global minimum of the dashed curve  diamond  is of no importance since it occurs at a higher ¦Å. the vertical line indicates the approximate value ¦Å after eq.  1  with non-optimal m ¦Å   = 1.
¡¡after eq.  1   any 1   ¦Å   1u 1 will ensure convergence with exponential error decay over time  except for the case when choosing the optimal learning rate ¦Åu1 = 1. in this case we obtain for n ¡Ü t:
	vr	 1 
and for n   t we have vn+1 = a 1r. hence  after finitely many  t  steps  convergence is reached when choosing the optimal learning rate. this is due to the fact that  ga  has only one degenerate eigenvalue  and t generalized eigenvectors of algebraic multiplicity 1...t.
1 conclusion
we have presented a closed formula of the prediction of rewards at trial time t within trial n and shown its convergence  where two general formulae for toeplitz matrices and eigenvalues of products of matrices were derived and utilized. we have given learning rates which optimally speed up the learning process.
