ion  or  isa   links and the thin  black arrows represent decomposition links. preconditions  equality constraints  and temporal constraints are not shown in the graphical representation but are also part of the knowledge we must represent. as well  we must represent under what conditions one plan is better than another plan. it is these constraints that are checked in the plan critiquing phase. 
   as an example of determining whether the ambiguity matters to the task of formulating a cooperative response  consider the exchanges between student and a student-advisor system shown in  1  below. given the student's query  1a   procedure response-generation in fig. 1 shows the steps in formulating a response. as a first step  the plan recognition procedure is called to determine the set of possible plans of the user. given the plan library in fig. 1  the underlying plan of the student is ambiguous. the two possible plans are as shown in  1 . procedure critique is called to critique the plans. one precondition of the action switch-section$ fromsection  tosection  is that there is space available in the destination section. 
1a. u: can i switch to the other section of my numerical analysis course  
	possible 	plans: 
j. avoid uninteresting prof by switching sections of course. 
1. resolve scheduling conflict by switching sections of course. 
b. s: yes. 
c. s: no  there is no space available. 
   for the response shown in  1b   suppose that there is room for the student and thus the precondition is satisfied. in this example  the result of plan critiquing is that both plans are labeled with the annotation  faultless.   procedure ambiguity-matters is called to deter-
natural language 
mine whether the ambiguity regarding the plan of the user matters to the task of formulating a response. it is found that the ambiguity does not matter as both plans are faultless  case la of the algorithm . finally  the critiqued plans are used in formulating a response to the original query. 
   for the response shown in  1c   suppose that there is no room for the student and thus the precondition is not satisfied. the result of plan critiquing is that both plans are labeled with the annotation  failure of preconditions: space-available  tosection .  procedure ambiguity .matters is called to determine whether the ambiguity regarding the plan of the user matters to the task of formulating a response. it is found that the ambiguity does not matter as both plans are annotated with the same fault  case lb of the algorithm . finally  the critiqued plans are used in formulating a response to the original query. 
   in general  in  case la  a response generation procedure can just give a direct answer to the user's query  and in  case 1 b  can give a direct answer plus any warranted additional information  such as telling the user about the fault. 
   in the above examples it was found that the ambiguity did not matter as there was enough information to generate a cooperative response. if instead it were found that the ambiguity did matter  case 1 of the algorithm  we propose that we enter into a clarification dialogue with the user to resolve the ambiguity to the point where it no longer does matter  i.e.  until we are in  case 1 . the remaining plans would then be used in formulating a response. 
1 	clarification dialogues 
what should we ask the user when a clarification is necessary  clearly  we do not want to simply list the set of possible plans and ask which is being pursued. the procedure to determine what to say {clarify  is shown in fig. 1. our proposal for clarification dialogues is tied to a hierarchical plan library. the input to the algorithm is a set of possible plans that relate the user's action to the top-level or end event. each plan in the set is annotated with a critique. the key idea is to ask about the highest level possible  check whether the ambiguity still needs to be further resolved  and if so  ask at the next level down  iteratively  through the hierarchy of events. the algorithm groups the plans according to the type of fault the plan is annotated with and chooses the group with the fewest events in it and asks first about that. 
   as an example of where the ambiguity matters and we ask a question to clarify  consider the exchanges between student and a student-advisor system shown in  1 . given the plan library in fig. 1  the underlying plan of the student is ambiguous. the three possible plans and the results of plan critiquing are as shown. 
   it is found that the ambiguity matters as the annotations are different for some of the plans  case 1b of procedure ambiguity-matters . as a result  procedure clarify is called to initiate a clarification dialogue. at the start of the procedure  current -level is initialized to 

procedure response  generation  query  begin 

   generate response based on query and plan s  in s end 
procedure critique s  begin 
for each plan in s do 
critique and annotate plan 
return  1  
end 
procedure ambiguity.matters s  begin 
we are in one of the following two cases: 
case 1. the ambiguity does not matter. 
the critiques are the same for all the plans. i.e.  a. every plan is faultless  or 
b. every plan is annotated with the same fault. 
return  no   
case 1. the ambiguity does matter. 
the critiques are different for some or all of the plans. i.e.  
a. some  but not all  of the plans are faultless  or 
b. every plan is annotated with a fault and the 
faults are not all the same. 
return  yes   
end 
procedure clarify s  begin first disjunctive branch point from 
the top in s. 
partition s into the set of sets {f 1  . ..  f k }  assigning two plans to the same fi if and only if they have the same fault annotation. while ambiguity.matters =  yes  do begin 
fi in s with fewest distinct events one level below current level 
list distinct events in f m i n one level below current level and ask user whether one of 
the events is being pursued 
if user's answer =  yes  then  

currentjevel next branch point in s 
end return 1  
end 
figure 1: response generation algorithm 
be end-event.  we have only given english descriptions of the possible plans in this example  but see fig. 1 for the and/or graph that represents the possible plans that  arise in a cooking example.  the set of possible plans is partitioned into {f1  f1} where f  contains plan  1a. 1  and f1 contains plan  1a.1  and plan  1a.1 . next  f m i n is found to be f  as it has the smallest number of distinct events one level below currentjevel. asking about the event one level below currentjevel leads to the question shown in  1b . depending on whether the answer is no  1c  or yes  1e   the set of possible plans  s  is updated appropriately. in both cases it is found that the ambiguity no longer matters. finally  the remaining set of critiqued plans are used in formulating a response to the original query  1d or 1f . in this example  by carefully choosing the question to ask  we are able to resolve the ambiguity by asking the user a single question about the single event  avoid-failing.  
1a. u: can i drop numerical analysis  
possible plans with annotations: 
1. avoid failing by dropping course. 
failure of preconditions: not failing  student . 
1. avoid uninteresting prof by dropping course. there exists a better plan: 
switch-sections  fromsection  to section . 
1. resolve scheduling conflict by dropping course. there exists a better plan: 
switch-sections  fromsection  tosection . 
b. s: are you trying to avoid failing the course  
c. u: no. 
d. s: you can drop the course  but if you are trying to avoid an uninteresting professor or trying to resolve a scheduling conflict  a better way is to switch to another section. 
e. u: yes. 
f. s: you can drop the course  but you will still fail the course since your mark will be recorded as withdrawal while failing. 
　as a second example  recall our story in the introduction about cooking for our vegetarian guest. given the plan library in fig. 1  the underlying plan of the cook is ambiguous. the three possible plans are as shown in  1  and  in more detail  in fig. 1. 
1a. u: i'm making marinara sauce. is a red wine a good choice  
partial description of possible plans with annotations  see fig. 1 : 
1. make pasta dish by making fettucini marinara. faultless. 
1. make pasta dish by making spaghetti marinara. faultless. 
1. make meat dish by making chicken marinara. failure of preconditions: not vegetarian guest . 
b. s: are you making a meat dish  
　using procedure clarify  the three plans are critiqued and it is found that the ambiguity matters. the two plans involving a pasta dish are found to be faultless but the plan involving a meat dish is found to be faulty as a precondition is false. using procedure clarify  the question asked to resolve the ambiguity would be  are you making a meat dish  perhaps with justification of why we are asking    after either answer of yes or no 
van beek and cohen 
figure 1: plan library for cooking examples   kautz  
1 ; modified  
we know enough that the ambiguity no longer matters. note that if we just ask the more general  what are you making   this allows such uninformative responses as 
 dinner  or just  you'll see.  
　as the examples show  when asking a question we propose to ask about as high a level of event as possible that still helps to resolve the ambiguity  and to work top down. starting with the top most events and working down may sometimes give as many or more questions as bottom up approaches. however  others have noted that bottom up dialogues are more complex to understand  cohen  1  p. 1  and more liable to misinterpretation  carberry  1  p. 1 . therefore  we believe that a top down approach is preferable1. 
　if more than one question is necessary to resolve the ambiguity  we want the questions to not appear to be dis-
jointed. this is achieved in our proposal by asking about as high a level of event as possible that still helps to resolve the ambiguity and moving systematically downward through the hierarchy of possible plans and thus having the  focus  of the questions change gradually. 
　the design of the algorithm takes into consideration that it is important to attempt to minimize both the length of clarification dialogues and the length of questions. fortunately  because plans are hierarchical  we can often ask about a group of plans by asking about a single event. for example  with reference to fig. 1 
1
　　note that  independent of the order that questions are asked  some questions can be eliminated using plans known from a previous discourse or background knowledge about the user. for example  in a student-ad vising domain  we would not want to ask a user what degree they are pursuing every time they used the system. 
natural language 
figure 1: possible plans for cooking example 
and procedure clarify  if some t  contained the plans: make spaghetti pesto  make fettucini marinara  and make spaghetti marinara  the algorithm would ask about this set of plans by asking the user   are you making a pasta dish   in addition  the questions asked all result in yes/no answers  which removes the task of disambiguating the user's response. an open problem for future research is developing tools for comparing criteria for generating questions. for instance  we may want to choose the group of plans to ask about as a combination of how large a question it will give and how probable or likely the plans in the group actually are. 
1 	discussion 
in this section we summarize our proposals and defend our position that this straightforward way of doing things is a good way. with reference to fig. 1  we discuss the design of boxes 1  1  and 1 and the tradeoffs involved between boxes 1 and 1. 
box 1: resolve the ambiguity with heuristics. 
as mentioned earlier  many researchers have proposed heuristics to prefer one plan over another  allen  1; carberry  1; mckeown tt a/.  1; goldman and chamiak  1; neufeld  1; kautz  1 . some of these heuristics can be incompatible with cooperative response generation. for example  allen's  l1j preference heuristics are generally incompatible with recognizing and responding to faulty plans. because we are using plan recognition for response generation  this should affect the design of box 1 and therefore what gets passed to box 1. 

figure 1: major modules of query-answering system 　box 1: resolve the ambiguity with the user. previous work in response generation makes the assumption that what gets passed to the rg component is a single plan the pr component proposes the user is pursuing. we argue that  unless we are willing to sometimes arbitrarily commit to one plan instead of another  there will be times when one plan cannot be chosen over another and therefore there will be ambiguity about which plan the user is pursuing. result: we need other methods  in addition to heuristics  to resolve the ambiguity. in plan recognition in a discourse setting  as opposed to key-hole recognition   the plan the user is pursuing is knowable simply by asking the user. but we do not want to always just ask if it is not necessary so we need to know when to start a clarification dialogue and what to say. to this end  box 1 contains a procedure that estimates by plan critiquing whether the ambiguity matters to the task of formulating a response. if the ambiguity does not matter the result is passed to box 1. if the ambiguity does matter  a procedure is called that starts a clarification dialogue  responding to the user's question with questions that iteratively differentiate between the possibilities. 
　box 1 vs. box 1: the tradeoffs. much previous work in plan recognition makes the assumption that we want the pr component to commit to and return a single plan. carberry  and mckeown   for example  use a strong heuristic to commit to a single plan. however  committing to a single plan means the system will at times commit to the wrong plan and thus will require the ability to handle natural language debugging dialogues with the user to correct itself. carberry  1  p. 1  argues that a system will appear  unintelligent  obtuse  and uncooperative  if it engages in lengthy clarification dialogues. however  a procedure to perform a debugging dialogue is not specified and is  we speculate  a difficult problem. we argue for not committing early. our hypothesis is that a clarification dialogue is better than a debugging dialogue. the questions in the clarification dialogues are simple to answer  whereas determining that the system has misunderstood your plan requires users to engage in plan recognition. that is  users must recognize the plan the rg component is using from its responses and note that it differs from their plans. moreover  the user may not recognize the system is wrong and be misled. finally  we argue that  if the questions are carefully chosen  the clarification dialogues need not be lengthy or too frequent. 
　preference heuristics can still be used in our approach. these would best be applied when too many top level events give an unwieldy clarification question. there may be tradeoffs between overcommitting in the plan recognition process and engaging in lengthy clarification dialogue  particularly with a large set of complex candidate plans. this may suggest applying pruning heuristics more actively in the plan recognition process  box 1  to reduce the number of questions asked in the clarification dialogue  box 1 . for future work  these tradeoffs will be examined more closely as we test the algorithms more extensively. 
　box 1: generate the response. once box 1 has estimated that any remaining ambiguity does not matter to generating a cooperative response  the disjunction of possible plans is passed to box 1. generating a response is handled by a procedure described in  van beek  1 . there are two cases. 
1. every plan is faultless  so we just give a direct answer to the user's query and ignore the underlying plan until further queries help clarify which plan the user is pursuing. 
1. every plan has the same fault  so we give a direct answer plus some additional information that warns the user about the deficiency and perhaps suggests some alternatives  see  joshi ct al.  1; van beek  1  . 
1 	future work and conclusion 
this paper makes contributions to the areas of plan recognition and response generation  as follows. for plan recognition  we have shown that there are cases where it is possible to retain a disjunction of possible plans and avoid the work incurred in the application of certain pruning heuristics which propose a single plan  when the plan recognition is being done for the purpose of generating a cooperative response. this also demonstrates that it is possible to use kautz-style plan recognition for cooperative response generation. our solution requires initially determining whether it is important to resolve plan ambiguity. and this involves critiquing the full set of possible plans  rather than the one plan that would be proposed if full pruning heuristics were applied. we argue that the process of critiquing  as described in this paper  is not difficult to implement  see  van beek  1  for a description of the implementation  and incurs little overhead and would thus argue that there is a cost saving overall. 
van beek and cohen 
　the paper also contributes to response generation by providing clear criteria for the initiation of clarification dialogues. furthermore  we provide an initial proposal for what questions to ask during clarification  and argue for the advantages of our top-down approach. we believe that natural language generation systems should be designed to involve the user more directly and are clearly demonstrating that this is plausible. 
   we acknowledge that determining the ideal set of questions to ask is still open to future research. this could be empirically tested by observing human advice givers or by gauging reactions to certain clarification dialogues from human audiences. our preference is to first defend our design decision on theoretical grounds  arguing that if the questions we supply are not  optimal  in terms of work spent to generate and need for further clarification  they are only marginally less  optimal  than other options which could be followed. 
acknowledgements. we thank fei song and bruce spencer for comments on an earlier version of this paper and for many discussions about plan recognition. financial assistance was received from the central research fund of the university of alberta and the natural sciences and engineering research council of canada. 
