six topics in search of a parser: .  
an overview of ai language research 
eugene charniak 
department of computer science brown university 
providence. rhode island 1 

1 introduction 
     my purpose in this paper is to give an overview of natural language understanding work within artificial intelligence  ai . 1 will concentrate on the problem of parsing - going from natural language input to a semantic representation. naturally  the form of semantic representation is a factor in such discussions  so it will receive some attention as well. furthermore. 1 doubt that parsing can be completely isolated from text processing issues  and hence i will touch upon such seemingly non-parsing issues as script application. nevertheless  the topic is parsing 
     unfortunately  to present al parsing work with any sort of historical accuracy would be to produce a bewildering forest of names  both of people and programs  rather. i will rather try to extract from the historical record a group of ideas which 1 believe can be molded into a coherent framework naturally  even within these limitations my remarks will be sketchy - this is an article  not a book 
ii syntax 
     parsing takes us from natural language in this paper english  to a semantic representation while we will discuss semantic representation later  for the moment we need simply note that whatever form this representation takes  it must at the very least clearly and unambiguously indicate the logical relations in the sentence. that is  in a sentence like  jack pushed the button.  it must clearly distinguish the pusher from the pushee in english this is typically indicated by word order  but other factors can enter in  as in the case of the passive sentence  the button was pushed by jack.  1 bring this up. of course  because it is typically our knowledge of the syntax of english which gives us a first cut at this information 
a. rules of syntax 
     given that we need to use syntax  the next thing we must decide is if we are to have explicit rules of syntax actually  if interpreted in the narrowest possible fashion  there is a general consensus on this point after all  if we do not have such explicit rules  then the only alternative is to have the rules implicitly buried in the code of the program  or possibly buried in rules for other things  given the general popularity of  rule based systems  these days  it seems reasonable to assume that we will make our knowledge of syntax  rule based  as well 
'many of the opinions expressed in this paper were forged during conversations with graeme hirst and the selection of topics would have been radically different were it not lor his input this research was supported in part by the office of naval 
research under contract n1m-1-c-1. und in part by the notional science foundation under contract is -wo 1. apologies to pirandello thus we will have some rule which will catch the distinction between 
jack wants to go to the store. 
　　　　　　jack wants bill to go to the store. and state that when the internal clause is missing it's subject then the subject is interpreted as the subject of the higher clause. as with other rule based systems  our rules will consist of two parts  a test  which tells us if the rule is applicable  and an action to be performed if the test allows us to proceed. so  for example  in transformational grammar this fact is captured by a rule called equi np deletion. since rules in transformational grammar start with  deep structure  and derive surface structure  the rule would look something like: 

there are two conditions on this rule first npl must equal np1  that is. we must have equal noun phrases . second  the verb v must allow for equi np deletion.  as opposed to  say   believe . we cannot say  jack believes to go to the store   when we want to say something like  jack believes that he went to the store . 
     in parsing  of course  we want to infer the deep structure from the input string. thus our rule  which we might call equi np insertion  will look much different. nevertheless  it will still consist of a test  see if there is reason to believe that the subject of a1 has been deleted  and an action  assume that the subject of s1 is npl 
b. when syntax  when semantics 
     now up to this point my analysis has not been controversial i have not claimed  for example  that these syntactic rules are applied before semantic rules. nor have i claimed that there is a separate syntactic parser. it is claims such as these which get the arguments flowing in the classical linguistic view  of course  syntactic rules are applied to the sentence  producing a complete syntactic tree before any semantic analysis is done. needless to say  such a view is controversial. indeed  very few within ai hold such a position at the very least it is assumed as in winograd's shrdlu program  that syntax continually calls upon semantics to judge the progress of the parse to date  and hence uses it to guide the parsing process so. for example  in a sentence like jack told the boy about the plot. 
the phrase  the boy about the plot  could be interpreted as a single noun phrase  as in  the block on the table  that is. syntax cannot rule out this interpretation. 

1 

however  semantics could presumably note that  about the plot  docs not make much sense as a modifier of 
 the boy   the only possible hold out here might be woods in  he points out that the time needed to do semantic processing in his system is longer than that needed to follow alternative syntactic parses. he may have changed his view on this  but i have not seen it in print however  few people have been convinced by this argument  since most assume that it says more about our lack of knowledge about semantics than anything else.  
but relegating semantics to a subroutine of syntax 
 let us call this the  semantic subroutine  position  is quite controversial. schank and his students  1  have argued for a much tighter connection - one in which there is but a single system with both syntactic and semantic rules we will call this the  combined semantics  position. while there have been a variety of arguments for such a position  ranging from psychological experiments to efficiency considerations  to me the most cogent is that people understand ungrammatical sentences. the argument goes like this: if we had a syntactic parser in charge of everything then it would be responsible for calling the semantic routines. so  in a sentence like  the boys is hungry.  the syntax would have failed to parse it  hence producing nothing which could be handed off to semantics  and hence no understanding 
     while i know of no written response to this argument  several alternatives seem to come up in discussions. one is to use a  blackboard  model as in hearsay  in this scheme the syntax and semantic components work in parallel  leaving their conclusions on a common  blackboard  the idea is that even if the syntactic component fails  the semantic component will still succeed  at least if the sentence is comprehensible 
     that there are troubles with this model is suggested by the fact that hearsay itself did not really use it while technically the two systems worked in parallel  in actual design the semantics would only work on the output of the syntactic component  making the model isomorphic to the semantic subroutine model proposed earlier nor is it hard to see why this should be the case as we have already noted  syntax is the most obvious way to get a first cut at the logical structure of the sentence in a real blackboard model the semantics would be forced to do without the logical structure established by the syntactic component. this would mean that it would have to re-establish the same information by other means if it could do this  then why bother with syntax at all1 in other words  a blackboard model will ultimately degenerate into either a  no syntax  model  or a combined semantics model. 
     the other alternative to the combined semantics model is one.in which we have a syntactic parser  but the grammar it uses is not the  correct  grammar for english  but rather a much more permissive grammar the idoa is to make the permissive grammar such that if a sentence is ungrammatical according to these permissive standards  then it will be incomprehensible as well. but there are still problems here we have already noted that people understand sentences in which subject verb agreement is not obeyed  eg  jack have the ball . yet some sentences can only be completely understood if we use this rule for example 
the fish is hungry. 
the fish are hungry. 
if our permissive syntax does not check for agreement then we will not have extracted the full measure of meaning from such examples. 
     perhaps one final point here we have become so accustomed to the fact that we are concerned with the extraction of meaning from sentences  and not accounting for grammatically judgements  that it is  at times  remarkably easy to forget the fact that people do have grammatically judgements if we are at all interested in psychologically plausible models of language comprehension then we must account for this fact permissive syntax simply will not do this for us. short of including two sets of rules  one permissive  one strict - a rather ad hoc solution at best 
     the way out of this bind is to take a slightly different approach rather than assuming that the grammar of the language should account for our ability to understand ungrammatical sentences  it seems more plausible to me to assume that one has a standard  correct  grammar for the language  and one tries to apply it to the sentence. if the sentence is ungrammatical  then somehow it should be the parsing mechanism itself which allows for the ungrammaticality. that is  it should note the ungrammaticality  but go on anyway  doing the best it cr*n 
     however  the most popular of the syntactic parsers  the  augmented transition network  parser  atn   cannot do this the need to reject ungrammatical sentences is built deep into the workings of atns. ultimately we can trace the problem back to the method atns use for handling situations where there is more than one possible interpretation for the next sentence constituent. for example; 
jack gave the ball to the boy 
jack gave the boy the ball 
after parsing the  gave   the next noun phrase may be either the direct object  the ball   in the first sentence   or the indirect object  the boy   in the second sentence  suppose for the first of the above cases it guesses that the noun phrase is the indirect object after parsing the noun phrase it will then be immediately expecting a second noun phrase  the direct object  as in the second sentence of course  it will not be there  a prepositional phrase is there instead the point is that upon finding that there is no way to proceed  an atn cannot assume the sentence is ungrammatical at that point and try to fix things up. in the case at hand the sentence is not ungrammatical. rather  the inability to proceed stems from an incorrect guess made at an earlier point hence the atn will backtrack to the last decision point and try an alternate guess. 
c. parsifal and paragram 
     the solution to this problem is deterministic parsing if we have a parser which need not backtrack then failure at a given point in the sentence urill indicate ungrammaticality. and hence wc have the possibility of dealing with the ungrammatical sentence marcus' parsifal  is such a parser  although it does not handle ungrammatical sentences. 
     the basic idea behind parsifal is that we have a buffer which allows us to have a limited look-ahead in the sentence. in parsifal the buffer is of size three  so at any point we may have three constituents  such as individual words  but they may be larger units as well  which have yet to be attached to the overall syntactic tree once wc have decided how a constituent is to be attached  however  wc cannot change it so  for example  the parsifal rule for equi np insertion would look something like this: 

1 

hands 	this 	off 	to 	semantics 
b. montague grammar 
one possible source for formalizing ideas would be 

a. woods procedural semantics 
     a while ago  when i was trying to figure out what to teach my students in my undergraduate al course  1 became rather dissatisfied with the state of semantics in al idealy what i wanted was some rule based system for turning the output of syntax into a semantic representation. the trouble is. that none of the recent work in semantics is sufficiently formalized to allow for this 1 found mvself forced to turn back the clock about ten years and return to woods procedural semantics  1j 
     wood's system was a classic rule based system  as we can see by examining one of his rules first  we have several general patterns for syntactic trees 
linguistics. upon turing in that direction however  we find that the major trend in linguistic semantics over the last few years has been montague grammar  which  as we shall sec  is a rather odd companion for al language work 
     montague grammar is exceedingly complex  and there is no hope that 1 could explain it here. for the reader who wants to learn more about montague grammar  the most accessible introduction is the textbook by dowty et al.   1 . as in transformational grammar  a set of grammatical rules analyze the structure of a sentence as opposed to transformational grammar  however  each syntactic rule is associated with a semantic rule which turns the sentonce into a formula of a rather complicated formal logic  the combination of syntactic rule plus translation into logic is essentially just another test/action rule.  the logical formula  in turn  is 
1 

assigned a meaning according to certain rules which define the semantics of the logic. again  the details here need not concern us. all we really need note is that the logic deals with so called  possible worlds   and many of the expressions in the logic have  as their meanings  quantifications over all possible worlds so a certain expression would be true if in all possible worlds in which 
x is true  y is true as well  or some such 
     now from an ai point of view there are a lot of problems with montague grammar the actual syntax montague used was rather trivial  but this is being attacked by the linguists attracted to the framework more troublesome are the issues we run into if we try to take seriously the idea of using the semantics of the formal logic as a way of inferring facts about the world quantifying over all possible worlds is  to quote wilks   no joke  for this and other reasons wilks rejects montague grammar as relevant to ai. 
     by and large  wilks' view has been the majority one thus  while friedman  has implemented a montague grammar parser  it was clearly intended as a toy system. suitable only for learning about montague grammar it does not seem intended as a useful al system hobbs  has also considered the use of montague grammar. but never implemented a program. it would seem that he too had trouble making quantification over possible worlds compatible with al 
     while i basically agree with the above criticism. 1 have recently become more sympathetic to montague grammar  due to two rather nice features the first is the way in which montague grammar handles quantification at first glance  quantification in montague grammar looks a mess so rather than tackle it directly. 
let us start with a simpler example 


now the point of this exercise is that this last formula is exactly what woods would have come up with for this sentence. once we see this  we can then see that the rather complicated expression montague uses for  every  is really just the same as what wood's had the only difference is that while woods had to depart from his rule based system in order to state in english what his sys tern did with all of this. montague  through the use of the lambda calculus  is able to express everything within his rule formalism. in other words  for this example  anyway  the montague formalism is the better one if one wants a truly rule based system. 
     there are other nice features of montague as well one is his handling of the verb  to be  typically al systems handle  to be  as ambiguous between  is of idrntity  and the  is of predication  we have identity m 
mark twain is sam clemens 
this is normally represented by something like 
　　　　　　　　 = mark-twain sam-elemens  the  is of predications  is seen in 
                     mark train is a man this gets a representation like 
 man' mark-twain  
montague is able to have a single sense for the verb  to be  yet end up with the two distinct representations shown above. the differences stem from the difterences between his representations for  a m a n   and  sam clemens . this is not  in itself  an earth shaking development  but it is a nice touch 
1      because of these nice touchs  1 have become interested in montague grammar as a useful tool for al nevertheless. 1 should il make it clear that in most respects my earlier opinions have not changed ll is still the case that quantification over possible worlds is out of the question l o r this an other reasons  the actual semantics of montague grammar does not interest nw nevertheless  montague's method of developing the 
semantic representation using only the lambda calculus and functional application looks very cute to rne. and i would like to see if it can be adopted to more reasonable al systems the example from woods suggests that this is not an idle possibility 
c. word sense ambiguity 
     but for all of my recent infatuation with montague grammar  it should be clear that the linguists are not really going to help us very much the major problem in semantics is not. as discussed above  how to combine predicates into the semantics representation  but rather the disambiguation of predicates   river bank  vs.  commercial bank   and the determination of noun phrase referents   jack put the money in the jar and then put it on the self   since virtually all linguists ignore these problems we are on our own 
needless to say. al has not been standing still since 
wood's paper ten years ago however  the work has not been easily systematrzable to give you some feel for the sorts of things which have been done  and the problems which remain if we are to incorporate them into some easily comprehensible rule based system  let us just look at some of the work which has been done on word sense ambiguity 
     probably the most direct attack on word sense disambiguation is that of rieger and small  rieger and small are particularly interested in cases where a single word has a rather large number of uses so  to take a reasonably typical case  the word  call  can. to give just a few  be used in the following ways 

nor is this complete by any means 
     confronted with such examples we have two basic problems  the more difficult is deciding upon the factors which are relevant to distinguishing between the senses so  we might note that some of the uses in the above examples take a direct object  call frv.d on the phone   where others use a prepositional phrase  called to fred to come upstairs . we can and should use such information  but we must be very careful so  we can also say 
jack called fred to come upstairs 
furthermore  in some cases the local information within the sentence is not sufficient so   jack called fred  could be phoning  shouting  or poker  to name just a few 
     but even having decided upon the factors which enter into the decision  we have yet another problem how and when do we apply these facts for example  if  for any given word we have a small number of sense  say. 1 or 1  then it would seems reasonable to just go through each possibility in turn  and perform the various tests which would rule the possibility in or out however  for words such as  call  the number is certainly larger than 1 


from the viewpoint of rule based systems  there are several problems with this firstly  given we have not specified what sorts of tests appear at the nodes  there are no real constraints on such networks secondly  in such a system we cannot extend our knowledge by simply adding another rule a child  or a programmer  who wanted to add a few new senses for a word would have to modify a rather complicated discrimination net on the other hand  it may well be feasible to automate the construction of such nets  given basic knowledge of how individual senses arc used  in which case the force of such objections would be substantially diminished. 
　　this is. by no means  the only suggestion which has been made concerning word sense disambiguation in the last several years the other idea which i want to discuss  however  is based upon the script idea of story comprehension  so we must break for a discussion of knowledge representation and text comprehension. 
iv knowledge representation issues 
　　the last several years have seen a fair amount of work on various knowledge representations. roughly speaking we can distinguish two trends on one hand there have been various predicate calculus based representations  while on the other we have the script /frame based systems. 
     in fact. 1 believe that these two traditions can  and should  be merged  and my current knowledge representation language. frail   uses both the predicate calculus and frames however  since most of the al work related to parsing has assume one or another of the frame/script approaches.   shall concentrate on this formalism here 
a. comprehension based representations 
     even within the frame /script approaches one can distinguish two trends. on one hand we have those systems which take as their starting point the need to provide a knowledge representation to underlie the comprehension of text certainly the yale script representations  1  fall in this category  as docs my penultimate frame representation  a typical example of such a representation might go as follows: 
1 

  
i assume that with the english glosses  the above should be reasonably self explanatory. basically  a frame such as this is said to be  active  if the activity it describes comes up in a text at that point the program will try matching further actions against those predicted by the script section of the frame if it succeeds  it has understood the action in terms of a higher level goal 
b. general frame representation 
     the other major trend in frame representations have been those which arc less directly tied to language comprehension  and have some pretensions to generality. under this heading 1 would fit krl  l   netl   kl-one   and frail . by and large these languages are characterized by 
1  more attention to the  semantics  of the representation 
1  greater use of the isa hierarchy to eflect a reduction of space by placing required knowledge at the greatest possible level of generality 
1  lack of any explicit script section to the frame  as seen above  with most of the descriptive burden falling on the slot section of the frame 
     again  the details need not concern us what is of concern is the degree to which the two frame groups have gone off in incompatible directions fortunately this seems to be minor. of the above differences  the only important one is the third however  this can be circumvented by the simple expedient of making the events which are part of the script secion of the frame into slots themselves. thus  in such a scheme our cigarette-lighting frame would become 

possible to introduce some syntactic sugar to make the representation look more like the script format shown earlier. frail  1  1  does exactly this 
v interactions between parsing and everything else 
     with the discussion of text comprehension out of the way. we can now look at approaches to parsing issues which are dependent on either the form of the representation  the use to which it is made during comprehension  or both 
a. contextual disambiguation 
     rieger and small's discrimination nets attacked the problem of word sense disambiguation in a brute force way. a rather different approach to the problem depends heavily on the use of scripts during language comprehension in this approach rather than listing all possible senses in a large dictionary  or lexicon  we would have a  default  lexicon which would give only the most standard definitions for  call  it might just be restricted to shouting and telephoning however  we would then have associated with each script/frame a lexicon  which gives for each notion used in the script the word associated with it and vice versa  1  thus  in the poker script we would have an entry for  call  pointing directly into the part of the script which deals with 
 calling a bet  

thus  in looking for a word sense  we would first look to see if the word appeared in the lexicon of an active script  in which case wc would automatically use the con textually suggested meaning 
     the advantage of such an approach is that it suggests a way to avoid the problem encountered earlier in organizing large numbers of senses for a particular word in the script/frame approach we will only have a few to consider  and we would naturally consider the contextually relevant ones first on the other hand  this places some rather sever constraints on our scripts/frames if there is only a small number of such frames active then 
this is fine  but if  as 1 tend to believe  the number might be quite large  they were playing poker with pornographic playing cards on the train on the way to a birthday party while one player was eating a hamburger and another was talking about yesterdays baseball game  then wc have simply replaced a long search through word senses by a long search through scriptal lexicons 
     because of such problems 1 have grave doubts about this approach on the other hand. i have some doubts about the discrimination net approach as well  and it is fair to say that 1 know of no approach to word sense disambiguation which 1 find encouraging  much less satisfactory 1 suppose that my objections to the discrimination net approach are less severe  but it is a rather dull approach the scriptal lexicon approach certainly has more flair 
1 

b. framcs and case 
     one area which seems particularly interesting to me  and which seems comparatively unexplored is the interaction between the processes of language comprehension and the form of semantic representation we use to some degree the scriptal lexicon idea has this flavor  but more interesting to me are examples where properties of language interact with features of the representation which are needed for independent reasons. let me give one such example  concerning case theory in linguistics   1 . 
     for those unfamiliar with case theory  the basic idea is that there arc certain cases associated with each verb these cases relate the arguments of the verb to the verb itself  and do so in a meaningful manner for example aa-b1 files to chicago 
in this sentence the noun phrase  chicago  is said to be in the goal  or destination case of the verb  flies . there are many reasons which have been put forward for wanting such cases one such is the tendency for noun phrases in the same case to take the same prepositions. so the verbs  travel    go    walk    fly   etc   all take  to  with their destinations. 
     several people have noticed the similarity between such cases and the slots of frames in al frame representations this has lead some  for example winston  to assert that cases of a verb are in fact the same thing as slots on a frame. while this comparison seems reasonable  it has not aroused much interest  presumably because nothing seems to follow from it so what if slots and cases are the same  
     interestingly enough  there are some consequences to take but one  see |b| for others   if slots and cases are the same  then if two things can fill the same case  it would seem to imply that they fill the same slot however  this can get us into trouble for example 
jack painted the wall with the brush 
jack painted the wall with the yellow paint 
here both  the brush  and  the yellow paint  are in the instrument case  yet we would normally assume that they should fill two different slots in the painting frame  one for the tool used  and one for the liquid used however  one of the aforementioned frame representations  kl-one  has a feature built into it which will handle this problem 
     in kl-one  not only is there an isa hierarchy of frames  but there is also a hierarchy of slots in particular  one can speak of one slot differentiating another slot so  if wr had a mammal which had a head-of slot  then it would be likely that head-of would differentiate the part-of slot in the phys-obj  physical object  frame naturally  this is useful because wc would automatically know that whatever is true of parts of objects is true of heads of mammals now  returning to the example at hand  wc might have the following representation for painting 

here the painting frame has two slots  tool and liquid so there is no problem in distinguishing what goes on the wall  and what is used to put it there at the same time  since both are differentiations of the instrument slot  we still have the normal case relations 
c. frame specific slot filling 
     in the examples we looked at in the last section we were able to fill certain slots because of syntactic clues in the input sentence in many cases  however  this is not possible. typically in such examples the slot to be filled is quite specific to the frame at hand  so it would be unreasonable to expect the english language to make allowances for it in the syntax. for example  schank and birnbaum  discuss the problem proposed by examples like: 
a plane carrying 1 people ... 
presumably in reading this sentence we establish some sort of air-travel frame/script. this frame will have several slots which may be filled  one of which is the passenger slot the problem here is to realize that  1 people  fills that slot  given that there is nothing directly in the sentence  at least when read literally  which sug-
gests this. 
　　after rejecting several alternatives  schank and birnbaum decide upon the following. within the airtravel frame   or perhaps within some frame higher on the isa hierarchy  such as machine-aided-travel  or some such  we will note that the purpose of this activity is to move passengers  and cargo  to some location that is  
we will have something like: 
 move  passengers  destination  
we then further assume that part of the meaning of  to carry  is that the direct object  in case theory this is often called the patient case  of the carry is moved. hence we have: 
　　　　　　　 move  patient  destination  in this example this becomes. 
 move 1-people  destination  
if we then do a match between this line and the corresponding line in the air-travel frame  we get as a side result that 1-people should be in the passenger slot 
     what is really nice about this is that it does not require any additional machinery beyond what we have already hypothesized for text comprehension while schank and birnbaum do not comment on this fact  the mechanism proposed here is our good old script application mechanism that is  the fact that the passengers of an air-travel are moved from place to place will be part of the script describing the actions taken during airtravel if we do normal script application we will find the match described above  and in the course of this match we will be required to fill the passenger slot as indicated 
　　a similar proposal is put forward by hayes . hayes is concerned with examples like: 
jack got in the car and touched the steering wheel 
the problem here is to recognize that the referent of  the steering wheel  refers to the steering wheel of the car just mentioned or  to put this somewhat differently  assuming that we have some sort of automobile frame  the referent of  the steering wheel  will fill a certain slot in the instance of the automobile frame which wc created to account for the automobile mentioned earlier in the sentence presumably  the automobile frame will look something like: 
1 

 frame automobile isa: traveling-instrument 
slots;  door~of  a door   
 wheel-of  a steering-wheel   
  
in the example of an airplane carrying people  we made the connection based upon the actions which take place during airplane-travel. in this last example there is no action  rather we want to make the connection on the basis of the match between the description of the newly mentioned object  namely steering-wheel  and the restriction on the wheel-of slot in the automobile frame  namely that it be a steering-wheel. 
     while it is not unreasonable to assume that we have two slightly different mechanisms  one for making such associations on the basis of scnptal actions  and one on the basis of slot restrictions  there are reasons to assume that these are really the same thing in particular  when 1 talked about how frail negotiates the differences between script based and general purpose frame representations  i noted that in frail a scnptal action was simply a slot on the frame that is. what typically is represented as 
 frame: airplane-travel is a machine-aided-travel 
slots  passengers  a person   
script  move  passengers  destination  
　　　　　　　-  would  in frail be expressed exactly the same way  but would  internally be translated into 
 frame airplane-travel is a machine-aided-travel 
slots   passengers-of  a person   
 move-step  move  passengers-of  destination    
thus  in both cases  filling a slot in a frame is the result of a match bet-ween the incoming statement and the restriction on a slot in an active frame while there are a lot of problems with the details of all of this  i find it encouraging that various strands of work are slowing comming together 
vi conclusion 
     1 have presented a medley of topics and ideas here  and to conclude let me try to draw them together and sketch what an overall parsing mechanism which included them might look like 
     roughly speaking  the parser would come in three parts the first part would be a reasonably standard syntactic parser the only change discussed here from those parsers already on the market is that 1 would like to see the parser press ahead in the face of ungrammatically syntax would  from time to time  hand off its results to the second part of the parser  the semantics at the moment it seems reasonable to me that this would occur at those points in the syntactic parsing process 
when we wish to attach a new constituent to the syntactic tree 
     one advantage to calling semantics when we are about to attach a constituent to the tree is that this fits nicely with the idea that semantics should be responsible for guiding the path of the syntactic parser hence one possible action that semantics could take is the rejection of the proposed attachment however  the reader should note that 1 have ass dously avoided the issue of how this judgemental aspect fits in with the other processing semantics is doing there is an interesting system by bobrow and webber  which has some ideas on this problem  but as yet i am not happy with any of the proposals i have seen this is an area which requires a lot of work 
     besides deciding on the acceptability of the attachment  semantics must do two other things  translate the english words into semantic symbols  and arrange these symbols into the proper format. one part of the translation process is the disambiguation of word senses presumably we will use one or more of the methods mentioned earlier if we go with the discrimination net approach  which seems the safest  i assume that the nets will be  compiled  from a more declarative format 
     the other part of the translation process requires doing something about referent determination  another topic which i have left out from this paper. there has been a good deal of work on referent determination.  1.1  1  but how  or even if  this work is to be naturally integrated into the framework i am proposing is still an open question 
     lastly  semantics will take the translated symbols  and. using the clues provided by the syntax  establish the functional structure for the constituent  and indicate this structure by putting the symbols together in the appropriate semantic format for this process  the use of the lambda calculus ala montague grammar looks reasonable 
     however  the output of semantics is not really the end of the parsing process. we might call the output a  surface semantics  in so far as it will get some of the slot assignments  but by no means all as wc have already noted  many of the assignments of slots in frames are best done by processes which most naturally fall under the heading of text comprehension thus  the last part of the parsing process will be coextensive with the various recognition processes which go under the heading of text processing it is here that we will decide that the people in the airplane fill the passenger slot it may also be at this point that much of the referent determination takes place we should also note that given our decision to have semantics be a subroutine of syntax  it is an interesting question whether we should have semantics immediately call our text processing component that is. do we wait until the end of the sentence  or at least the end of an s node  before doing text processing  or  is this too intermixed with everything else my guess is the latter  but this question is yet another to be added to the list of problems which still require solutions 
