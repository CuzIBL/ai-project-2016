
we describe an approach to statistically verifying complex controllers. this approach is based on deriving practical vapnik-chervonenkis-style  vc  generalization bounds for binary classifiers with weighted loss. an important case is deriving bounds on the probability of false positive. we show how existing methods to derive bounds on classification error can be extended to derive similar bounds on the probability of false positive  as well as bounds in a decision-theoretic setting that allows tradeoffs between false negatives and false positives. we describe experiments using these bounds in statistically verifying computational properties of an iterative controller for an organic air vehicle  oav .
1 introduction

the computational requirements for high-performance complex control algorithms can vary considerably. the variation arises because the computation depends on a number of factors such as the sensed/estimated state of the system under control  environmental disturbances  and the operational mode of the system. furthermore  the variation in general can not be determined analytically. in hard real-time systems where the computation must complete in time for a command to be issued to the actuator at the next sample instant  these uncertainties pose a significant challenge. this is the reason that pid  proportional-integral-derivative  controllers  with their deterministic execution time  are still the preferred choice in many applications despite their lesser performance.
﹛in order to bring practical acceptance to high-performance complex control algorithms  we propose a compromise that makes use of both types of controller algorithms. highperformance algorithms will be used within a safe operational envelope  soe  where they are guaranteed to complete within the allocated time. outside of the soe  lower performance and computationally simpler algorithms will be used. the soe is determined based on simulation data  and hence is only safe with some statistical guarantees.
﹛the problem of identifying the soe is a binary classification problem where a false negative merely means a conservative use of a low performance controller and a false positive may have drastic consequences such as loss of the vehicle. thus we would like to obtain an soe that has some statistical guarantee that it is indeed safe  i.e.  a classifier with provably small probability of false positive. this however is not the only criterion  for otherwise the trivial classifier that classifies everything as negative  namely the current state of the art  would be the top candidate. instead  the goal is to push the boundaries of the soe as far as possible while keeping a cap on the probability of false positive.
﹛as an example  let us consider the computational property of a high-performance controller for an oav  figure 1 . the oav has a ducted fan propulsion unit  with control provided by movable vanes in the propwash. the vanes are situated in the propulsion airflow and consequently the interactions between the propulsion and the control surfaces are highly non-linear. the trim calculation for the oav is an iterative algorithm whose computational time depends on several factors  elgersma and morton  1 . we are interested in conditions under which this calculation can be reliably used.
﹛our approach is based on statistical learning theory  slt . specifically  we derive statistical guarantees for soes using vapnik-chervonenkis-style generalization bounds for classification problems with weighted loss  of which the classification problem with false positive loss is a special case. we are interested practical bounds-bounds that are asymptotically competitive and have small pre-constants. while the slt literature contains a vast collection of vc-style bounds  the majority of these bounds are stated and proved only for the probability of misclassification and only a few are directly applicable to our problem. furthermore  slt bounds are often derived with little emphasis on obtaining optimal pre-constants. in addition to the emphasis on finding small pre-constants  our analysis has two unique aspects. first  we assume that it is possible to achieve small empirical loss  which is true for the case of false positive loss . second  our analysis is upper-tail-oriented  as we are only interested in deriving upper bounds for the expected loss.
1 preliminaries
notations: we use the symbols p  e  and v to denote the probability  expectation  and variance  respectively. 考 =  denotes a rademacher sequence-a sequence of in-
dependent  symmetric  1-valued random variables.
﹛let x and y be non-empty sets and z = x ℅ y . a pair of  x y  ﹋ x ℅ y is denoted as z. let  z 米  be a fixed probability measure. a training set is a finite sample  drawn independently
according to 米. the probability and expectation with respect to sn are written as pn and en. a hypothesis space is a
set h of functions from x to y . a loss function is a function l : y ℅ y ↙ r. the loss of a hypothesis h ﹋ h on z is l h z  = l h x  y . the expected loss of h is l h  = el h z . the empirical loss of h on the training set
. we assume that
all loss functions have range  1 . when y is finite  we have a classification problem. when |y | = 1  the classification is binary  and let y = { 1}. here we only deal with binary classification.
﹛if we assume that correct classifications incur zero loss  i.e. l  1  1  = l 1  = 1  then what emerges is a loss function that we refer to as weighted classification error  defined as:
 false negative 
 false positive .
here 老 is a number between 1 and 1. the idea is that false positives  bad errors  are more costly than false negatives  good errors . when 老 = 1  i.e. when no difference is made between the two types of errors  the loss is called the misclassification error. when 老 = 1  l 1  h  is the probability of the classifier h making a false positive error.
﹛given a training set sn  a hypothesis h ﹋ h  and a loss function l  how can we bound the expected loss l h   statistical learning theory  slt  provides a probabilistic answer to this question. a typical slt result of the form pn l h         汛 or  succinctly  l h   汛   provides an upper bound   on l h  with confidence at least 1   汛  where   and 汛 are positive  reasonably small numbers. the upper bound   typically depends on 汛  the sample size n  the empirical loss ln h   and a complexity measure of the hypothesis space h. the most important complexity measure in slt is vc dimension. let c   1x be a set of subsets of x. note that h  as a set of binary classifiers on x  is an example of such sets. for any a   x  define c ﹎ a = {c ﹎ a : c ﹋ c}. the growth function of c is defined as
 k c  = max{|c ﹎ a| : a   x |a| = k}.
clearly   k c  ≒ 1k. the vc dimension of c is defined as d c  = sup{k ﹋ z :  k c  = 1k}.
let d = d h . we assume that d   ﹢.
theorem 1.  vapnik  1  equations 1 
	.	 1 
﹛the bound  1  is obtained by applying a very general result of vapnik to the special setting of learning binary classifiers with a specific loss function l 老 . it is thus natural to ask if this bound can be improved. while many slt results have been obtained that address this issue  the majority of are formulated and proved for l = l 1  only. one of the goals of this paper is to examine if these results can be extended to the loss functions l 老  老 ﹋  1 . it turns out that these results fall into two categories: those that are applicable for all 老 ﹋  1   and those that are applicable for 老 = 1 only.
﹛we conclude the preliminaries with the statement of the often-used sauer's lemma.
sauer's lemma.  sauer  1   n h  ≒  en/d d .
1 vc dimension-based bounds
suppose that h is a hypothesis with  small  empirical loss: ln h  ≒  1. we would like to bound the probability that l h  is  large : l h      for some      1. this amounts to bounding pn q  where q is defined as
q = {sn :  h : ln h  ≒  1 l h     }.
there are two major approaches to do this: the classical approach of vapnik and chervonenkis   and the approach based on abstract concentration inequalities developed by talagrand and others.
1 the classical approach
the classical vc analysis begins with the observation that pn q  ≒ p r /p r|q  for any r that satisfies p r|q    1. we then define r based on sn and an additional independent sample whose size may or may not be equal to n. in this analysis we take the former approach: let be an independent sample of size n  commonly referred to as the ghost sample and let ln∩  h  = l sn∩  h   the empirical loss on the ghost sample. denote. let 1 ≒  1    1 ≒  . define r as
	r = {s1n :  h : ln h  ≒  1 ln∩  h     1}.	 1 
upper bounding pn q  now reduces to upper bounding p1n r   the covering step  and lower bounding p1n r|q   the symmetrization step .
﹛the covering step: upper bounding p1n r . intuitively  p1n r  is small because if h has small empirical loss on a sample  its empirical loss on a ghost sample should also be small. we can change the definition of r in  1  as
 where.
the next step uses the so-called permutation technique.
 .
next  we fix s1n and bound the inner expectation. let
	h1n = {h1n =  h x1  ... h x1n   h ﹋ h}   { 	}1n
the mapping h 1↙ h1n is many-to-one. denote l h zi  1 ≒ i ≒ 1n. the inner expectation can be written as !
which  by the union bound  is bounded by

the cardinality of h1n is at most  1n h   which is at most
 1en/d d by sauer's lemma. for a fix h1n ﹋ h1n  the summand in  1  can be written as !
which  by hoeffding's right-tail inequality  is bounded by“
 .
thus we have arrived at the following result.
lemma 1.  vapnik and chervonenkis  1 
.
﹛when  1    1  it is possible to improve upon lemma 1. the idea is  instead of bounding the probability that the absolute discrepancy ln∩  h  ln h  is large  we bound the probability that the relative discrepancy is large. we
 weaken  the definition of r in  1  as
.
now  proceed identically as before  except that 灰 is now replaced with 灰∩  we arrive at the following results. lemma 1.  vapnik and chervonenkis  1 
 .
corollary 1.  vapnik and chervonenkis  1 
.
﹛by considering the relative discrepancy  we have managed to insert the term  1 +  1  resulting in a tighter bound in lemma  1 . further tightening is possible when  1 = 1. instead of using hoeffding's inequality  blumer et al.  use a combinatorial argument that leads to the following improvement of corollary 1.
lemma 1.  blumer et al.  1 
.
﹛the symmetrization step: lower bounding p1n r|q . to lower bound p1n r|q   we can fix sn  ignore the condition ln h  ≒  1  and bound the following conditional probability:
 
or  equivalently 
let h be a hypothesis such that l h     . it suffices to lower bound pn ln h     1 . intuitively  this quantity is large because the empirical loss should be large     1  wherever the expected loss is large      . since  1      we have

by bernstein's left-tail inequality. coupled this with lemma 1  we obtain the following result.
corollary 1.
 .
﹛in this inequality  the parameter  1 is unspecified  and we can minimize the bound over  1 ﹋   1   .
﹛when 老 = 1  the loss function l 老  is binary  and nln h  is a binomial random variable with parameters n and l h . we can thus use several lower bounds on the right-tails of the binomial to obtain
n    1   pn  {ln h     /1}| h : l h         1	 1  n    1   pn  {ln h     /1}| h : l h         1	 1 
for the case  1   1  we can set  1 =   and combine  1  with lemma 1 to obtain the following result.
corollary 1.  vapnik and chervonenkis  1  for l = l 老  老 = 1 
	 .	 1 
﹛for the case  1 = 1  we can set  1 =  /1 and combine  1  with lemma 1 to obtain the following result.
corollary 1.  blumer et al.  1  for l = l 老  老 = 1 

﹛shawe-taylor et al.  further improve  1   using an argument that uses a second sample sk∩ of size k  and  1 = r  
where r = 1   p1/  k  k = n ern/d   1 .
theorem 1.  shawe-taylor et al.  1  for l = l 老  老 =
1 

﹛compared with  1   the sample complexity derived from  1  is smaller by a factor offor typical values    say    1 .
﹛we point out that corollary 1  1  and theorem 1 were previously stated and proved for the loss function l = l 1  only. our analysis extends them to the case l = l 1   and shows that they do not hold when 1   老   1 . the covering argument remains the same  while the symmetrization argument uses the simple observation that nln 老  h  is a binomial random variable with parameters n and l h   regardless of whether 老 = 1 or 老 = 1.
1 talagrand's method
observe that l h  ≒ ln h  + suph﹋h l h    ln h  . the supremum is a random function of sn  where changing a single element zi results in a change of at most 1/n  and thus is  汛-bounded by by mcdiarmid's inequality  mcdiarmid  1 . the next quantity to bound is the expectation of the supremum. this is accomplished using the concept of rademacher average. let g be a class of functions from z to the reals r. the rademacher average of g is defined as rng = n
	.	in this analy-
sis  the role of g is played by the loss class associated with
h: g = lh = {z 1↙ l h z  : h ﹋ h}. the symmetrization inequality  e.g.  bartlett et al.  1   states that
en  suph﹋h l h    ln h    ≒ 1ernlh. thus it remains to bound the rademacher average. the technique is wellestablished and based on the concepts of covering number. denote by n u lh l1 米n   the u-covering number of lh with respect to the metric l1 米n . lemma 1.  dudley  1 
		 1 
﹛the last piece of the puzzle reveals a bound on n u lh l1 米n     defined based on the vc dimension d.
lemma 1.  haussler  1  for all sn:
	.	 1 
﹛combining  1  and  1   with some algebra we obtain the following result.
theorem 1.  dudley-haussler 
	.	 1 
the bound  1  is based on an analysis of the absolute

discrepancy l h    ln h . its deviation term is o pd/n   which should not come as a surprise. it is natural to ask if this approach can be used to analyze some form of relative discrepancy between the expected loss l h  and the empirical loss ln h . consider
	 .	 1 
simple algebra shows that for all h ﹋ h  l h  ≒  1 +
l n ln h +l 1n. consequently  an upper bound on l n can be translated into an upper bound on l h . unlike the previous analysis  it appears that we can not use mcdiarmid's bounded

renders the  difference  unbounded. the solution to thisp difference inequality  as the introduction of the term l h 
problem originates from the work of talagrand on abstract concentration inequalities and their applications to bounding the suprema of empirical processes  talagrand  1  1 . talagrand's inequalities were later improved using the socalled entropy method. the following version provides the best known bound.
lemma 1.  bousquet  1  let f be a countable set of functions from z to r. let b = supf﹋f  sup e f    f    v = supf﹋f v f  and bn = b sn  = supf﹋f e f    . then for any 汐   1 
.

	we now apply lemma 1 with f z  = l h z  /	l h  
b = v = 1  to obtain	p
.
we then proceed to bound en l n  with a technique from massart  that is referred to as peeling and the concept of sub-root functions. a function 肉 :  1 ﹢  ↙﹟ 1 ﹢  is called sub-root if 肉 is non-decreasing and 肉 r / r is nonincreasing. any sub-root function 肉 r  is known to have a unique fix-point r   i.e. 肉 r   = r    bartlett et al.  1 . now  suppose that 肉 is a sub-root function with fixed point r  such that
en  sup{|l h    ln h | : l h  ≒ r}  ≒ 肉 r   r   1.
then we can show that  汐   1  l n is  汛-bounded by
.
the final step is to bound r  using d. this can be done  koltchinskii and panchenko  1  by setting 肉 r  to be dudley's entropy integral. what we end up with is a  汛 bound on l h  that is o dlnn/n   asymptotically comparable to those obtained using the classical approach such as theorem 1  albeit with worse constants.
1 summary of vc dimension-based bounds
given a sample sn of size n and a hypothesis h that has empirical loss ln h  on sn  what can we say about the expected loss l h  of h  the results in this section provide several answers to this question. they all have the general form of  if ln h  is small  then with high probability  l h  is small . the common assumption is that h is selected from a hypothesis space h with finite vc dimension d. in the general case when l is only assumed to have range  1   corollary 1 seems most useful. when l = l 老  老 = 1  we can exploit several binomial tail inequalities to obtain much simpler bounds. when ln h    1  corollary 1 should be used. when ln h  = 1  theorem 1 provides the best-known bound. these results are all based on the idea of uniform convergence of relative discrepancies  ucrd . even corollary 1 and theorem 1 can be viewed as based on degenerate cases of ucrd  with special-purpose combinatorial arguments replacing the general-purpose hoeffding's bound. results that are based on uniform convergence of absolute discrepancies such as lemmas 1 and theorem 1 are not as useful for our purpose  as they have to cover situations that vapnik and chervonenkis  refer to as pessimistic cases. simply put  pessimistic bounds are loose since they need to account for hypotheses with expected losses close to 1. in contrast  we only need to concern ourselves with hypotheses with zero or small empirical losses.
﹛the statistical/computational learning theory literature contains a vast collection of generalization bounds in the general case of function learning and in the special case of learning binary classifiers. to our knowledge no work has explicitly derived generalization bounds for binary classifiers with weighted error penalties as defined in this paper. the bounds in section 1 originate from the seminal work of vapnik and chervonenkis . our contribution here is the extensions to the case 老 = 1  and corollary 1.
﹛talagrand's approach provides a completely different way to arrive at generalization bounds for all loss function l 老  老 ﹋  1  that are asymptotically equivalent with classical bounds. this approach analyzes the mean  or median  panchenko  1   of the supremum of the  sometimes weighted  discrepancies between the expected and empirical losses using talagrand's various concentration inequalities  completed invariably with the symmetrization inequality  dudley's entropy integral bound  and haussler's packing bound. the resulting bounds often have much larger constants and are not as useful for our non-asymptotic purpose.
1 experiments
we now describe the applications of the bounds derived in section 1 to our oav experiment as described in the introduction. we identify four factors that affect the computational time of the iterative algorithm  and choose 1-dimensional axis-parallel hyper-rectangles as our hypothesis space. the vc dimension of this hypothesis space is 1  as it is known that the vc dimension of axis-parallel hyper-rectangles in rm is 1m. thus for 汛 = .1   = .1  we need 1 samples using theorem 1. but using theorem 1  we need only 1 samples. with 1 samples  if we set 汛 = .1    can be as small as 1. the improvement in generalization bound     is about 1-fold and in samples complexity  n  is about
1-fold.
﹛the search for the best hyper-rectangle in this experiment is rather simple. for each sampled input  we determine if the iterative algorithm converges. it turns out that in 1 instances  roughly 1%   the algorithm converges. despite this high  empirical  rate of success  in current practice it still loses out to a pid-like controller with fixed deterministic computation time. next  we randomly choose an axis-parallel 1-dim hyper-rectangle in the input ranges as a hypothesis. if the hyper-rectangle contains a sampled point for which the algorithm does not converge  an unsafe point   then we eliminate that hyper-rectangle  since the guarantees are based on theorem 1 and require zero false positives . otherwise  we count the number of safe points that lie outside the hyperrectangle  false negatives   and choose the hyper-rectangle that has the fewest number of false negatives. after looking at 1 random hyper-rectangles  we are able to come up with one that contains 1 safe points and no unsafe points. this hypothesis thus has 1 - 1 = 1 false negatives. note that the number of false negatives is still quite large. this is because we use hyper-rectangles which constitute a simple hypothesis space that does not approximate the decision surface very well  this soe is nevertheless a big improvement over the trivial  empty soe that is the current state of the art . the advantage to this is that the vc dimension is low  and thus only a small number of samples are required to obtain the statistical guarantee  which reads  the found hyper-rectangle has probability of false positive bounded by 1  and we have at least 1% confidence in this statement. 
﹛there are a number of alternatives to the above procedure. for example  we can use corollary 1 instead of theorem 1  if the requirement of zero empirical loss is too restrictive. in the oav example with 1 samples  this leads to a hypothesis with 1 false positives but only 1 false negatives  a reduction of 1%!  while still maintaining 1% confidence that the probability of false positive is less than 1. furthermore  we can replace the criterion  as few false negatives as possible  with other criteria  for example one that prefers hypotheses with large volumes. finally  if we are willing to make a decision-theoretic tradeoff between false negatives and false positives  e.g. one false positive is as costly as one thousand false negatives   we can set 老 = .1 and apply corollary 1.
1 summary and related work
it has been said that the divide between slt and practice is of grand canyon proportions  perhaps because vc bounds are often too loose to be useful in practice. this paper offers a counterargument in the form of an slt-based approach to verifying complex controllers. we demonstrated this approach on a problem of significant industrial and military interest: deriving a safe operating envelope for a complex control algorithm. this approach offers control engineers a principled way to increasingly replace low-performance  simple control algorithms with high-performance  complex ones while still maintaining a statistically high confidence in safety. a key to making this offer attractive lies in deriving practical vc-style generalization bounds for weighted binary classification  a problem that hitherto has not been given much attention . our vc analysis  which builds upon standard vc analysis of unweighted binary classification  shows that such bounds are indeed possible. they are significantly better than a general bound by vapnik. our analysis precisely pointed to the place where the false negative penalty had an effect  namely the symmetrization argument. we have successfully applied this verification framework to several other control applications  to be reported in an extended version of this paper. we expect these results to have applications outside of the controller verification problem.
﹛from a practical point of view  slt-based generalization bounds have been used mostly in the model selection problem  see e.g.  bartlett et al.  1  . aside from this  they have also been used in several control systems applications  for example  in deriving randomized algorithms for robust control problems whose exact solution is np-hard  and in the context of system identification  vidyasagar  1  chapter 1 . also  machine learning researchers have long recognized the importance of learning classifiers with general loss function. the work in this area is generally referred to as cost-sensitive learning  turney  1 .
﹛the bounds derived in section 1 consist of the empirical loss and a vc confidence term that is independent of the probability measure 米 and the particular sample sn. they are thus necessarily  loose  since they need to hold for  bad  distributions and  bad  samples sn. recent research has focused on data-based measures of hypothesis space complexity such as rademacher averages  koltchinskii  1; bartlett et al.  1 . this direction relies on talagrand's approach as described in section 1. although this approach yields vc dimension-based bounds that have worse constants compared to bounds using the classical approach such as  1   it can be used to derive bounds based entirely on data  i.e. sn  without a priori information about the hypothesis space h  such as its vc dimension d . the following result is an example of such data-dependent bounds.
theorem 1.  bartlett et al.  1  corollary 1  let l = l 老  老 = 1. let the random function 肉 n be defined on

 1 p1  as
.
then 肉 n is sub-root with fixed point r    and for any 汐   1 
.
﹛compared to similar result in section 1  the present one is completely data-dependent: in the definition of 肉 n r   the bound 1r/汕1 is on the empirical  as opposed to the expected  loss ln h   and the empirical rademacher average e考rn  as opposed to ern  is used. thus in theory we can obtain a bound without a priori knowledge of the complexity  such as the vc dimension  of the hypothesis space h or of the underlying probability measure 米. however  in practice  computing or estimating 肉 n and its fixed point r  is far from easy  although bartlett et al.  1  section 1  had made some initial progress in this direction.
acknowledgements
this work was supported in part by the u.s. defense advanced research projects agency  darpa  and the u.s. air force research laboratory under contract no. f1c-1. we thank michael elgersma for help with the oav experiment  and xuanlong nguyen for many useful comments.
