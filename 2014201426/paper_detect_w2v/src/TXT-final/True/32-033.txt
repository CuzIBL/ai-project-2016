 
successfully managing information means being able to find relevant new information and to correctly integrate it with pre-existing knowledge. much information is nowadays stored as multilingual textual data; therefore advanced classification systems are currently considered as strategic components for effective knowledge management. we describe an experience integrating different innovative ai technologies such as hierarchical pattern matching and information extraction to provide flexible multilingual classification adaptable to user needs. pattern matching produces fairly accurate and fast categorisation over a large number of classes  while information extraction provides fine-grained classification for a reduced number of classes. the resulting system was adopted by the main italian financial news agency providing a pay-to-view service. 
1 introduction 
knowledge is nowadays the key source for competitive advantage. the success or failure of a company can depend on the ability to find the right information at the right time. the www explosion  and the increasing usage of internet technologies as a core channel for communication  multiplies the sources of information and increases by orders of magnitude the amount of information available. however  while raising the opportunities for gaining competitive advantages  this also increases the information glut. the main value is not in the information itself  but in the capability of managing it successfully to derive knowledge that is critical to an organisation's objectives. successfully managing information means being able to correctly integrate it 
1  this work was partially funded by the european union in the framework of the language engineering sector  project facile  le 1 . ofai is supported by the austrian 
bmwv. 
1 
luca gilardoni 
silvia mazza 
massimo ferraro 
quinary spa 
via fara 1 
1 milan italy william j. black 
fabio rinaldi 
david mowatt 
umist-pobox1 
manchester m1qd united kingdom with existing structured information  to facilitate communication and knowledge sharing and to support knowledge-based organisations. 
the role of natural language processing and artificial intelligence is fundamental in this respect as: i  the vast majority of this information is textual and available in different languages; ii  the development of new tools for structuring textual data starting from its content represents one of the fundamental steps in successfully managing information. 
this is particularly evident in the business arena  where on-line textual information from news providers has long since been available and heavily used. recent reports from gartner group  bair  1  explicitly mention advanced classification systems characterised by semantic technologies as the most strategically relevant element to support effective knowledge management. however  technology available on the market still mainly resorts to information retrieval-derived systems and techniques  ir . this technology does not provide adequate accuracy when coping with rich and complex classification structures. this is because ir systems do not take into account linguistic features. more linguistically oriented text classification can be achieved by the use of pattern matching  pm . pm can produce a fairly accurate and fast categorisation over a large number of classes  hayes and weinstein  1; jacobs and rau  1 . resource development does not require linguistic expertise and can be done by trained users. but pm is still weak on the analysis of linguistic structures and cannot be used for fine-grained categorisation. information extraction  ie  techniques can be also used for text classification  e.g.  the text filtering subtask of the st task in  muc1  . ie systems perform very well in detecting texts relevant for a single class  e.g.  management succession  with results ranging between 1 for both precision and recall. but ie cannot be performed on a large number of classes: it is an expensive technology as it requires a large amount of time of linguistically aware personnel  grishman  1 . attempts at separating linguistic knowledge  e.g.  syntactic knowledge  from domain dependent knowledge  e.g.  the domain patterns   grishman  1; hobbs et al  

1  simplified the task  but did not solve the general problem. moreover ie systems are still not efficient enough to cope with large amount of texts. this paper proposes the integration of pm and ie as a method for providing classification highly adaptable to different user needs. pattern-based  shallow  classification performs broad-band text filtering  comparable to that done by a human quickly skimming texts  i.e. recognising main topics without careful reading. ie is used to provide more fine-grained classification  comparable to that produced by a careful human reading. ie is used to cope with only some of the classes  i.e. those on which shallow classification results need to be refined. classification provided in this way is very adaptable to user needs as it provides flexibility in different directions: broad/fine-grained classification  large/restricted number of classes  high/low efficiency in the process. it is also flexible in terms of application development. pm rules take less time to develop than ie resources: a pmbased application can be provided as a first  although not complete  answer  waiting for a more complete integrated pm+ie application to be developed. 
the facile system  fast and accurate categorization of information by language engineering  is based on this two-tiered approach. it is able to categorise in detail business texts such as agency news  newspaper articles and analysis reports from different sources and written in different languages  currently english  german  italian  and - to some extent - spanish . the system has been developed as an applied research project  bringing together researchers and end-user organisations. it was delivered as a pre-engineered prototype and evaluated by three users participating in the project 
 an italian rating agency  a spanish savings bank  and a german information broker . the system has been adopted by the main italian financial news agency for production use. it currently provides classified news both for internal work organisation and as an external pay-to-view service. 
in this paper we describe the system  focusing on architectural and evaluation issues. we believe that our approach is interesting because  although it does not propose completely new technologies  it demonstrates that innovative technologies can be profitably integrated to build applications providing high added-value services. 
1 	system architecture 
the system architecture has been driven by the idea of providing classification highly adaptable to user needs. the first need addressed is that of providing a refinable classification strategy  resorting to information extraction only when needed. in general classification is obtained via a multi-step process  where each step adds information to the input. a control module activates the different modules in order to assign the appropriate level of classification needed by the user for each text. to do that the control module uses the information progressively augmented by the different modules to each text. it is this module that decides whether or not ie is to be performed on a specific text. the other modules in the architecture are the preprocessor  the shallow analyser  the deep analyser and a group of applicationspecific modules. from the point of view of flexible classification the preprocessor implements tasks that are to a large degree application independent  such as segmentation  normalisation of numbers  dates and abbreviations  as well as morphological analysis  part of speech tagging  preliminary unknown word guessing and named entity recognition. it does not require major adaptation for new applications. the shallow analyser performs a preliminary classification by exploiting techniques that can be easily tailored to new applications by trained personnel; it is based on pattern matching and assigns a set of classes to each text. the deep analyser performs more fine-grained classification via ie. it produces filled templates pertaining to the categories selected by the shallow analyser. it can be ported to new classes by linguistically-trained personnel only. application layers are constituted by an additional group of modules that permits the adaptation of the system to different user environments. in the rest of this section we will analyse each module separately. 

figure 1: genera! architecture 
1 	preprocessor 
the preprocessor transforms a stream of characters into analysed tokens. it provides morpho-syntactic information and a disambiguated syntactic tag for each token in the input  treating proper names as single tokens and assigning them a  disambiguated  semantic tag. the key steps are the same for all the languages considered  currently english  italian  german and spanish  and use the same machinery with language-specific resources. the steps are:  1  tokenisation  splitting of input text  assignment of orthographic features ;  1  morphological analysis  stemming and assignment of morpho-syntactic information ;  1  part of speech tagging  disambiguation of multiple morpho-syntactic categories ;  1  database lookup  assignment of semantic fea-
	ciravegna et al 	1 

tures ;  1  named entity analysis  nea . steps 1 and 1 integrate generic resources and machinery provided by inxight inc. linguisticx. database lookup adds semantic features  e.g. person  organisation  location etc.  but also auxiliary categories  such as verb-of-speech  used as clues in nea  for  possibly multi-word  tokens in the database. the information in the database is to be regarded as provisional and necessarily incomplete. for example  while  pearl  may be in the database as a first name   pearl harbor  is not a person name. therefore more informed methods for correctly identifying named entities are needed. nea uses non-deterministic chart parsing. the approach used has been partially suggested by  coates-stevens  1 . nea recognises complete names as well as numbers or time expressions. unlike other approaches  e.g.   mikheev et al  1    a highlevel language is used for the definition of the patterns. nea uses language-specific context-sensitive rules that can take into account any information placed in the chart by preceding modules. nea employs a weighting mechanism that enables it to either return all competing analyses or to select the most plausible one  black et al  1 . the preprocessor input is a zoned text  i.e. with parts to be analysed identified and marked up. its output is a text where each  group of  word s  is associated with orthographic and morphological features  normalised form and semantic labels  e.g.  john smith  is a  proper name  with orthographic class  capitalised   feature  masculine  and semantic tag  person  . 
1 	shallow classifier module 
the shallow classifier  scm  is based on pattern matching. it operates in two stages: first it recognises domain relevant concepts mentioned in texts  and then assigns categories to the text. the first step is performed by the shallow analyser. it is based on a refined version of the model presented in  gilardoni et al.  
1   in turn derived from seminal work by  hayes and weinstein  1 ; the shallow analyser models both domain objects  e.g.  shares  market sectors  balance sheet items  and domain events  e.g.  joint ventures  public offers  financial transactions . its pattern matching techniques are integrated with a knowledge representation system. concepts are associated with a set of weighted linguistic patterns. the pattern language is similar in spirit to those implemented by search engines  e.g.  use of boolean operators  proximity  optionality   but is augmented to enable testing of lexical variants  exploiting information from the preprocessing stage. it is also tightly coupled with the knowledge representation language  allowing references to concepts in patterns  exploiting inheritance and relations to reduce complexity. the result of shallow analysis is a set of matches of text portions with concepts with scores  expressing confidence factors. matches maintain reference to the part of text matched enabling them to be used for annotation of texts. the second step is performed by a rule-based categorizer. the text's main topic is deter-
1 
mined using application-specific heuristics  based on both concept matches and context   as well as domain knowledge. the shallow analysis and categorizer use similar but distinct concept lattices  implemented in the same kr formalism. the categorizer' s lattice directly reflects the end-user's perception of the applicationspecific conceptual space. both are domain-dependent  but application specificity is kept confined in the category lattice  resulting in a domain model largely sharable by different applications coping with different classification structures. the patterns that characterise concepts are obviously language dependent. but they are kept associated to concepts  i.e. they are domain motivated and not linguistically motivated. developing a multilingual application they can be handled in an homogeneous way. 
the whole machinery is supported by modular declarative resources to describe concepts  patterns  categories and classification rules. the modularization of these resources allows easy development  maintenance and portability across both languages and applications. it also diminishes the major drawback of a knowledgebased approach  i.e.  the complexity in building the knowledge base   while retaining the major benefits  i.e.  attainable accuracy and consistency . 
on the other side  alternative classification criteria or a different category lattice imply modification to the categorizer's resources  rules  but not necessarily to the domain knowledge encoded in the concepts network or in the associated patterns. 
1 	deep analyser module 
the deep analyser module  dam  performs information extraction to achieve fine-grained classification. it classifies texts by their content  for example by allowing the user to express a query such as  send me any texts concerning bonds issued by european-based financial institutions whose amount exceeds 1 million euro . dam's task is similar to the st task in muc conferences   muc  1    but the kind of templates needed for classification are generally flatter than those used in muc  see the section on evaluation for an example . dam is based on cascades of finite state transducers  fsts . the global architecture is depicted in figure 1. dam receives as input the pre-processed and classified text and produces filled templates. the whole architecture uses the same formalism  and the same set of primitives for all fsts. each fst operates on string of tokens. tokens are abstract representations of lexical elements that have three types of realisations accessible at any time during analysis: lexical  strings  nes  etc.   syntactic  parse trees  typed feature structures  and semantic  quasi logical forms  qlfs   ciravegna and lavelli  1 . the first module applied is the parser. it assumes that there is one and only one possible correct parse tree for each sentence. its goal is to produce a sufficient approximation of such a parse tree  i.e. a tree that captures all the relations use-

fill for information extraction  while leaving the others implicit. parsing is performed in three steps: identification of nps  vgs and pps  chunking   a-structure recognition  i.e.  recognition of subcategorization frames of verbs  some nps  etc.   and modifier attachment. the output of the parser is a parse tree with the relevant modifiers attached  a typed feature structure representing relations among constituents in the parse tree and a qlf. the accuracy in producing the correct a-structure reached p=1%  r=1% in a blind text on 1 texts  ciravegna and lavelli  1 . following parsing  default reasoning is applied to introduce in the qlf additional information not explicitly contained in the text  but needed for template filling  e.g.   if a person working for company x is hired by company y  s/he is no longer employee of x  . 

figure 1: dam architecture 
after 	this  	discourse 	processing 	is 	performed: 
 pro nominal references are resolved for both objects 
 people  organisations  physical objects  and events 
 e.g.  hiring/firing ; implicit relations are also captured  e.g.   the bank of japan decided .... the president said ....  . high accuracy is reached in discourse processing as the amount of available information  i.e.  parse tree with associated information in the typed feature structures and in the qlf  allows reliable choices among candidates. templates are finally filled by using the final qlf  as produced by an additional default reasoning step . template merging and recovery actions cope with missing information. portability was a main requirement for dam. the module can be ported to new domains by just modifying declarative resources: the lexicon  the knowledge base and the fst grammars. the grammars are sequences of cascades of fsts. all grammars use the same formalism and primitives  1 in all . this uniformity is an important advantage  allowing new applications to be developed by a single person. the distinction between domain dependent and domain independent resources simplifies the porting; currently there are 1 fst cascades: 1 are domain independent  1 are domain dependent and one is user dependent  i.e.  the template presentation module . porting to new applications required one to three man months  depending on the application. 
1 around the kernel: application layer 
one of the main user-needs is the integration pf the system directly into the user environment* for this reason the system architecture is divided in two main blocks: the classification kernel  composed by the four modules mentioned above  and the application layer. the classification kernel is currently deployed as a standalone unix module  configurable within a full development environment or stripped down to a runtime executable  accessible by application layers using different communication protocols  including sockets and http. an nt version is currently in an alpha release. the classification kernel is concerned only with text analysis  expecting as input a normalised  tagged text and providing as output an annotated text. input and output tagging is currently based  due to historical reasons  on html extended to carry structural information  with work on xml support ongoing. both formats are dynamically configurable to support different text sources . the application layer is responsible for interaction with the real world  i.e. of feeding the categorizer and of handling categorisation results to make them available to final users. application can be extensively customised or modified to properly integrate within different information technology infrastructures. four applications have been so far implemented with the system  spanning four languages and with different scopes over the classification domains. all of them are different in terms of it integration  but all share a common model with a source and annotated text repository  based on oracle  fulcrum and in two cases on flat files in a shared file system  residing on either unix or nt systems  simple feeders alimenting the kernel running on connected sparc  and different intra/internet interfaces towards annotated text repository. 
1 	evaluation 
the system was evaluated on different corpora for different languages in different application environments. it was subject to extensive albeit informal tests during development using corpora composed by  in at least one case  several thousands texts. the three analysis modules were formally evaluated against user tagged corpora by using automatic tools either acquired  such as the muc scorer  or internally developed. in the following we will focus on the formal evaluation first  discussing each module separately. results are not independent  as the modules operate in a chain: e.g. results of the shallow classifier discount the errors introduced by the preprocessor. however  as soon as former modules start to deliver reasonable results  e.g. in line with those reported below   chain effects become negligible and  in some case  could be recovered at a later stage. the advantage given by the integration of pattern matching and ie techniques is moreover not measurable in terms of percentage gains. being able  for example to discriminate bond issue story depending on amount 
	ciravegna et al 	1 

threshold is currently completely out of reach without ie techniques - while on the other side not being able to properly drive ie has generally dramatic effect as well on both precision and recall  
1 	preprocessor 
formal evaluation of the preprocessor was done for named entity recognition in three languages  english  italian  german . two corpora of newswire messages were selected for each language  one used for building and enhancing the ne rules  the other one used as the corpus for a blind test. for italian and german the corpus has been selected from sources in normal use at user's sites. for english  the test corpus chosen was the muc-1 dry-run corpus  which gave better comparability with the procedure adopted with the other languages than the formal run  in which domain differed between training and testing. annotation of these corpora was performed manually in the form of sgml markup using appropriate tools. preprocessor results were delivered by the system in sgml-annotated form; these results were compared to the pre-annotated version using the muc scoring scheme. see table 1 for 
evaluation results. 
#words #ne prec rec training english 1 1 .1 .1 german 1 1 .1 .1 italian 1 1 .1 .1 j ! test english 1 1 .1 .1 german 1 1 .1 .1 italian 1 1 .1 .1 table 1: preprocessor evaluation 
1 	shallow categorizer module 
the shallow categorizer has been evaluated formally in a blind test for italian and german  on two specific applications. for both languages a test set was selected by end users from usual text sources the system was expected to analyse  agency news and financial newspaper ; the blind test was performed on 1 texts for italian and 1 for german. an english version of the 
italian application was also evaluated with comparable results  albeit in a less formal way using material from reuters corpus  http://www.research.att.com/~lewis/  re-annotated with respect to the category structure handled by the system. the italian application was required to classify texts against a set of about 1 categories arranged in a hierarchy. the classification ranged over several financial domain topics. top-level elements were company news  especially company results  capital and stake operations  ratings  bond issues  management changes and meetings   sector news  refined by specific market sectors and behaviours   and stock market related news. the german application classified over a hierarchy describing country economic indicators 
 inflation  employment  etc.  also discriminating by country and by whether the indicator figures were mentioned as measured data or as forecast. a first measure for precision and recall was taken  considering each category in isolation  i.e. independently of its position in the hierarchy. on this basis  whenever a superordinate category is assigned by the system  this is marked as an error for both the most specific ones  lowering the recall  and for the more general assigned  lowering the precision . the table below summarises the results obtained; please note that english figures were obtained with largely underdeveloped resources: 
# texts ass. corr. a notc cnota prec rec german 1 1 1 	1 	1 .1 .1 italian 1 1 1 	1 1 .1 .1 english 1 1 1 	1 1 .1 .1 j table 1: shallow classification evaluation 
as categories are defined hierarchically they do not induce disjoint partitions. this means that whenever the system 'takes risks' in assigning more specific categories  e.g. wrongly assigning 'annual results' whenever the correct category should have been the more general 'economic results' this is not an error with respect the more general one. analogously  if the application 'stays conservative' and misses a more specific category  the assignment of an ancestor should count as evidence for the more specific category anyway. taking this into account  as in  gilardoni and rocca  1   higher precision and recall than above are obtained for intermediate categories. although less significant from the point of view of system comparison  it is however far more significant from the user point of view  as whenever the hierarchy is  as in our case  structured and deep  the user would likely exploit the structure querying at intermediate levels. 
1 	deep analyser module 
for italian  one template filling was fully developed  bond-issue  and two others have reached the level of demonstration  management succession in companies and company annual results . for english a demonstrator for the field of economic indicators was developed. here we will focus on bond issues in italian. the information to be extracted for each bond issue included: issuer  kind of bond  amount  currency  placement date  interest date  maturity  average duration  global rate and first rate. the corpus  used both for training and test  included 1 texts  agency news and newspaper articles  globally 1 words . number of slot fills per text: 1. the test was performed as follows: first the directives for the annotator were developed and the texts tagged by a person; then dam was instructed to operate in the domain  by just modifying declarative resources  and run on the corpus. finally dam results were compared against the user defined answers by using the muc scorer  douthat  1 . the development cycle was organised as follows: dam resources were developed by carefully inspecting the first 1 texts of the 

1 

corpus. then the system was compared against the whole corpus  1 texts : results were recall=.1  precision=.1  f l =.1. then the resources were tuned on the whole corpus  focusing on the texts that did not reach sufficient results in terms of r&p. the final results on the whole corpus were p=.1  r=.1  f l =.1. 
1 user validation 
formal evaluation enables system comparison and also provides useful information for technical development. nevertheless  real evaluation comes by user validation 
 is this the right system   and by real usage. end users from financial institutions were involved at each stage in development  with feedback incorporated in prototypes released. the adoption of the system for production use by a major italian news agency  not involved in development and moreover putting the system at work already when only a preliminary and largely incomplete prototype was available  is a clear proof of the validity and usefulness of the approach. 
1 	conclusion 
we have described the design and evaluation of an advanced system for fine-grained text categorisation. the system is highly modular  both in its processing filters and in its declarative resources. this has permitted it to be applied to a range of application domains in four european languages. the architecture brings together pm and ie. the system has been a considerable success story: it has been adopted by the main italian financial news agency for providing a pay-to-view internet service. the system integrated many heterogeneous software components. it has been running continuously in the user environment since january 1. applications at user sites involved in the consortium are also available. we contend that this shows that state-of-theart ai techniques are mature enough to provide real world applications. the architecture is designed in order to separate knowledge intensive resources  dam's   from less knowledge intensive modules  scm's   from domain independent modules  preprocessor's . this approach simplifies resource development both in terms of required expertise and in terms of task complexity. in the future we will continue working in the direction of the time-to-market reduction  in order to considerably enlarge the market for fine grained classification systems. in particular we will investigate the use of machine learning techniques for resource development. 
acknowledgements 
participants to the facile project were: quinary spa  itc-irst  italrating spa  it   ofai  sema sae  es   
caja de segovia  es   sis  de   umist-ccl. the authors would like to thank all the project team  in particular p. prunotto  v. fiorentino and k. schirmer. 
