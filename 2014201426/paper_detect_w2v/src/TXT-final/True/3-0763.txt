
we present andstudy the contribution-selectionalgorithm  csa   a novel algorithm for feature selection. the algorithm is based on the multiperturbation shapley analysis  a framework which relies on game theory to estimate usefulness. the algorithm iteratively estimates the usefulness of features and selects them accordingly  using either forward selection or backward elimination. empirical comparison with several other existing feature selection methods shows that the backward elimination variant of csa leads to the most accurate classification results on an array of datasets.
1 introduction
feature selection refers to the problem of selecting input variables  otherwise called features  that are relevant to predicting a target value for each instance in a dataset. feature selection has several potential benefits: defyingthe curse of dimensionality to enhance the prediction performance  reducing measurement and storage requirements and reducing training and prediction times. this paper focuses on the first issue  namely selecting input variables in an attempt to maximizethe performance of a classifier on previously unseen data.
　in this paper  we suggest to recast the problem of feature selection in the context of coalitional games  a notion from game theory. this perspective yields an iterative algorithm for feature selection  the contribution-selection algorithm  csa   intent on optimizing the performance of the classifier on unseen data. the algorithm combines both the filter and wrapper approaches. however  unlike filter methods  features are reranked on each step by using the classifier as a black box. the ranking is based on the shapley value  shapley  1   a well known concept from game theory  to estimate the importance of each feature for the task at hand  specifically taking into account interactions between features.
　throughout the paper we use the following notations. the distribution fromwhich the dataset instances are drawn is represented by two variables   where
represents the input variables  vector of features   and represents a discrete target value  class  for . three sets containing i.i.d. sampled instances of of the form are available: train  validation and test representing the training set  validation set and test set respectively.
given an induction algorithm and a set	 
stands for a classifier constructed from the training set using the induction algorithm  after its input variables were narrowed down to the ones in s  namely labels each instance of the form     with a value in the domain of . the task of feature selection is to choose a subset of the input variables  that would maximize the performance of the classifier on the test set. in what follows we shall focus on optimizing accuracy of the classifier  although we could as easily optimize other performance measure such as the area under the roc curve  balanced error rate etc.
　the rest of this paper is organized as follows: section 1 introduces the necessary background from game theory with a detailed description of the csa algorithm; section 1 provides an empirical comparison of csa with several other feature selection methods  accompanied by an analysis of the results; section 1 discusses the empirical results and provides further insights to the success of the backward elimination version of the csa algorithm.
1 classification as a coalitional game
cooperative game theory introduces the concept of  coalitional games   in which a set of players is associated with a payoff  a real function that denotes the benefit achieved by different sub-coalitions in a game. formally  a coalitional gameis defined by a pair where is the set of all playersand   for every   is a real number associating a worth with the coalition . game theory further pursues the question of representing the contribution of each player to the game by constructing a value function  which assigns a real-value to each player. the values correspond to the contribution of the players in achieving a high payoff.
　the contribution value calculation is based on the shapley value  shapley  1 . an intuitive example of the potential use of the shapley value can be provided in an academic setting. assume that you are a professor runninga lab  and  once and for all  you have decided to distribute the yearly bonus to your students in a fair manner  that reflects the actual contribution of each student to the academic success of the lab. during the year  the students form spontaneous  coalitions  of groups of students  each such group works and publishes a paper summarizing its work  these coalitions may also be assembled by the professor . every paper gets a rank   e.g.  its impact factor   composing its  payoff function . based on this annual data of the students' coalitions and their associated payoffs  the shapley value provides a fair and efficient way to distribute the bonus to each individual student according to his contribution over the year.
　the shapley value is defined as follows. let the marginal importanceof player to a coalition   with   be
 1 
then  the shapley value is defined by the payoff
		 1 
　where is the set of permutations over   and is the set of players appearing before the th player in permutation . the shapley value of a player is a weighted mean of its marginal value  averaged over all possible subsets of players.
　transforming these game theory concepts into the arena of feature selection  in which one attempts at estimating the contribution of each feature in generating a classifier  the players are mapped to the features of a dataset and the payoff is represented by a real-valued function   which measures the performance of a classifier generated using the set of features . finally  the usage of the shapley value for feature selection may be justified by its axiomatic qualities:
axiom 1  normalization or pareto optimality  for any game it holds that
in the context of feature selection  this axiom implies that the performance on the dataset is divided fully between the different features.
axiom 1  permutation invariance or symmetry  for any and permutation on it holds that
this axiom implies that the value is not altered by arbitrarily renaming or reordering the features.
axiom 1  preservation of carrier or dummy-property  for any game such that for every it holds that
this axiom implies that a dummy feature that does not influence the classifier's performance indeed receives a contribution value 1.
axiom 1  additivity or aggregation  for any two games and it holds that
where
this axiom applies to a combination of two different payoffs based on the same set of features. for a classification task these may be  for example  accuracy and area under the roc curve or false positive rate and false negative rate. in such case  the shapley value of a feature  that measures its contribution to the combined performance measure  is just the sum of the corresponding shapley values. the linearity of the shapley value is a consequence of this property. namely  if the payoff function is multiplied by a real number then all shapley values are scaled by namely .
in other words  multiplying the performance measure by a constant does not change the ranking of the features  a vital property for any scheme that ranks features by their 'importance'.
1 estimating features contribution using the msa
the calculation of the shapley value requires summing over all possible subsets of players  which is impractical in our case.  keinan et al.  1  have presented an unbiased estimator for the shapley value by uniformly sampling permutations from . still  the estimator considers both large and small features sets to calculate the contribution values. in our feature selection algorithm  we use the shapley value heuristically to estimate the contribution value of a feature for the task of feature selection. since in most realistic cases we assume that the size of significant interactions between features is much smaller than the number of features    we will limit ourselves to calculating the contribution value from permutations sampled from the whole set of players  with being a bound on the permutation size. notice that most filter methods are equivalent to using where no interactions are taken into account. feature selection using ran-

dom forests  breiman  1  is equivalent to . the bounded estimated contribution value becomes

　where is the set of sampled permutations on subsets of size . the usage of bounded sets coupled with the method for the shapley value estimation  yields an efficient and robust way to estimate the contribution of a feature to the task of classification. for a detailed discussion of the msa framework and its theoreticalbackgroundsee  keinan et al.  1 .
1 the contribution-selection algorithm
the contribution-selection algorithm  csa   described in detail in figure 1  is iterative in nature  and can either adopt a forward selection or backward elimination approach. using the subroutine contribution  it ranks each feature according to its contribution value  and then selects features with the highest contribution values with forward selection  using the sub-routine selection 1  or eliminates features with the lowest contribution values with backward elimination  using elimination . it repeats the phases of calculating the contribution values of the remaining features given those already selected  or eliminated   and selecting  or eliminating  new features  until the contribution values of all candidate features contribution-selection-algorithm 	;	 
1. :=
1. for each
1. := contribution   	;	 
1. if
1. :=	selection 	;  	 
1. goto 1
else
1. return selected
figure 1: thecontribution-selectionalgorithminitsforwardselectionversion. is the input set of features  is a contribution value threshold  is the maximal permutation size for calculating the contribution values  is the number of features selected in each phase. the contribution routine calculates the contribution value of feature with the pay off function described in this section. the selection routine selects at most features with highest contribution values that exceed . in the backward elimination version  the selectionsub-routine is replaced with an eliminationsub-routine which eliminates features in each phase and the halting criterion is changed accordingly.
exceed a contribution threshold with forward selection  or fall below a contribution threshold with backward elimination .
　the algorithm  without further specification of the contributionsub-routine is a generalizationof filter methods. however  the main idea of the algorithm is that the contribution sub-routine  unlike common filter methods  returns a contribution value for each feature according to its assistance in improvingthe classifier's performance which is generatedusing a specific induction algorithm  and in conjunction with other features. using the notation in section 1 and assuming one optimizes the accuracy level of the classifier  the contribution sub-routine for forward selection calculates the contribution values using the following payoff function :
1. :=
1. generate a classifier	from the training set  train
1. evaluate	for all examples of the validation set 
validation
1. return	the	accuracy	level 	defined	as

　the case is an end case which is handled by returning the number of instances in the largest class divided by the total number of instances  a classifier which always selects the most frequent class . backward elimination is quite similar  and the payoff is calculated by sampling permutations from the set of features left after each phase of elimination. the maximal permutation size has an important role in deciding the contribution values of the different features  and should be selected in a way that ensures that different combinations of features that interact together are inspected. its impact is demonstrated in section 1.
　the number of selected features for the selection subroutine controls the redundancies of the selected features; the
nameclassesfeaturestrain sizetest sizereuters111reuters111arrhythmia11internet ads11dexter11arcene11i111table 1: description of datasets used.
higher is  the more likely that features with redundant contribution will be selected. although minimizes the redundancy dependencies of the features  increasing accelerates the algorithm's convergence. the algorithm's halting criterion depends on   which designates a trade-off between the number of selected features  and the performance of the classifier on the validation set. with the forward selection version  choosing means that csa selects features as long as there exists a feature that is likely to improve the classifier's performance  and selects smaller sets of features as is increased. increasing has the opposite effect on the size of the final set of features. the more intuitive halting criterion  to stop when no further performance gain is achieved  is too restrictive  while csa's halting criterion enables the selection of features proved useful at later stages  as verified empirically over several datasets.
1 results
1 the data and benchmark algorithms
to test csa empirically we ran a number of experiments on seven real-world datasets with number of features ranging from 1 to 1  table 1 : the reuters1 dataset and the reuters1 dataset both constructed following  koller and sahami  1  using the reuters-1 document collection; the arrhythmia database from the uci repository  perkins et al.  1  ; the internet advertisements database from the uci repository  blake and merz  1  which was collected for the research of identifying advertisements in web pages  the dexter text categorization dataset and the arcene cancer dataset  both from the nips 1 workshop on feature selection  guyon  1  and the i1 microarray colon cancer dataset  alon et al.  1 .
in principle  csa can work with any induction algorithm
 . however  due to computational constraints we focused on fast induction algorithms or algorithms that may be efficiently combined into csa. we experimented with naive bayes  c1 and 1nn. for each of the datasets  we measured the training set accuracyof each classifier using ten-foldcross validation on the whole set features. for each dataset  all subsequent work used the induction algorithm   that gave the highest cross validation accuracy  as detailed in table 1.
　eight different feature selection schemes were then compared on the datasets described above:
the induction algorithm without performing feature selection to serve as a baseline.
regularized linear svm using the package  joachims  1 . datasets that had more than two
datasetl fwd.  bwd. reuters1nb11reuters1nb11arrhythmiac111internet ads1nn11dexterc111arcenec111i1c111table 1: theparametersandtheclassifierusedwiththecsaalgorithmforeachdataset. is in the induction algorithm used with csa  nb being naive bayes   is the number of features selected in forward selection in each phase  is the number of features eliminated in backward elimination in each phase  is the permutation size and is the number of permutations sampled to estimate the contribution values. for an explanation how hyperparameters are chosen  see text.
classes were split into few binary classification problems.
filtering using mutual information and classification using . we binned continuous domains to estimate the mutual information.
filtering using the pearson correlation coefficient and classification using .
random forests feature selection  breiman  1  and classification using .
feature selection using forwardselection wrapper. since simple wrapper greedily selects a feature that most improves the classifier's validation performance  it is equivalent to forward selection csa with .
classification using after performing feature selection with forward selection csa and parameters as described in table 1. the parameters and were chosen such that the expected number of times that each feature is sampled is higher than 1. the contribution value threshold for stopping selection was . termination of feature selection was fixed by choosing a contributionvalue threshold . no hyperparameter selection was performed on either   or .
classification using after performing feature selection with backward elimination csa and parameters as described in table 1. the parameters and were chosen such that the expected number of times that each feature is sampled is higher than 1. the contribution value threshold for stopping elimination was . no hyperparameter selection was performed on either   or
　to avoid overfitting on the validation set used for calculating the payoff with csa  we used m-fold cross validation instead of a single set.
1 feature selection and classification results
table 1 summarizes the classifiers' performance on the test set and the number of features selected in each of the experiments. the accuracy levels are the fraction of correctly classified test set instances:
the reuters1 dataset. feature selection using random forests did best  yielding accuracy level of 1% with 1 features. not too far behind is the csa in its backward elimination version  1% with 1 features .  koller and sahami  1   for example  report that the markov blanket algorithm yields approximately 1 selected features with accuracy levels of 1% to 1% on this dataset.
thereuters1dataset. csa with backward elimination did best  yielding accuracy level of 1% with 1 features. for comparison   koller and sahami  1  report that the markov blanket algorithm yields approximately 1 selected features with accuracy levels of 1% to 1% on this dataset.
thearrhythmiadataset. this dataset is considered to be a difficult one. csa with backward elimination did best  yielding an accuracy level of 1% with 1 features. forward selection with higher depth value     did better than wrapper  implying that one should consider manyfeatures concomitantlyto performgoodfeatureselection for this dataset. for comparison  the grafting algorithm  perkins et al.  1  yields an accuracy level of approximately 1% on this dataset. theinternetadsdataset. all the algorithms did approximately the same  leading to accuracy levels between 1% and 1% with csa slightly outperforming the others. interestingly enough  the wrapper algorithm did not select any feature; in the first phase  the 1nn algorithm had neighbors from both classes with the same distance for each feature checked  leading to arbitrary selection of one of the classes  and the classifier's performance was constant through all the phase  yielding zero contribution values. however  when selecting the higher depth levels  the simple 1nn algorithm was boosted up to outperform classifiers such as svm.
thedexterdataset. for the dexter dataset  we used algorithm  c1 decision trees  only for the process of feature selection  and linear svm to perform the actual prediction on the features selected. this was done because c1did not give satisfying accuracy levels for any of the feature selection algorithms  and it is impractical to use svm with csa for large datasets. to overcome the difference between the classifiers performing feature selection and the classifier used for the actual classification  we added an optimization phase for the forward selection algorithm after it stopped. in this phase  a ten-fold cross-validation is performed on the dataset in a similar way to the one used to optimize filter methods. the simple mutual information feature selection performed best  followed closely by the contribution-selection algorithm in its backward elimination version and by random forests. this implies that in dexter the contributionof single features significantly outweigh the contribution of feature combinations for the task of classification. the forward selection algorithm did as well as linearsvmwithout feature selection  but with a significantly lower number of features. thearcenedataset. here  just as in the case of dexter  we usec1for the process of feature selection  and linearsvmto perform the actual prediction on the features
datasetwrapperfwd.bwd.reuters1.1  1 1  1 1  1 reuters1.1  1 1  1 1  1 arrhythmia1  1 1  1 1  1 internet ads-1  1 1  1 dexter1  1 1  1 1  1 arcene1  1 1  1 1  1 i1.1  1 1  1 1  1 no fssvmcorr.mirf111  1 1  1 1  1 111  1 1  1 1  1 11.1  1 1  1 1  1 111  1 1  1 1  1 111  1 1  1 1  1 11%  1 1  1 1  1 111  1 1  1 1  1 table 1: comparisonofaccuracylevelsandnumberoffeaturesselectedinthedifferentdatasets. upper table: wrapper and fwd/bwd  csa with forward selection/backward elimination with parameters from table 1 . bottom table: no fs  no feature selection   svm  linear svm without feature selection   corr  feature selection using pearson correlation   mi  feature selection using mutual information   rf  feature selection using random forests . accuracy levels are calculated by counting the number of misclassified instances and given in percentages. the number of features selected is given in brackets.
selected. the csa with backward elimination obtained better performance than the rest of the algorithms. thei1dataset. csa with backward elimination and feature selection using mutual information yielded the best results. the poor performanceof csa with forward selection can be explained by the poverty of data comparing to the number of features; the algorithm selected in the first phases features which explain well the training data by coincidence  and avoided from selecting features that truly contribution to the task of classification. this phenomenon is explained in portrait in section 1.
　in summary  in 1 out of the 1 datasets  csa with backward elimination achieved the best results. in all other cases  csa achieved the second best result.
1 a closer inspection of the results
the msa  intent on capturing correctly the contribution of elements to a task  enables us to examine the distribution of the contribution values of the features. figure 1 depicts a log-log plot of the distribution of the contribution values in the first phase for arrhythmia and dexter  prior to making any feature selection. this distribution follows a scale-free power law  implying that large contribution values  in absolute value  are very rare  while small ones are quite common  justifying quantitatively the need of feature selection. the other datasets were also observed to possess similar power law characteristic.
　the behavior of the algorithm through the process of feature selection/elimination is displayed in figure 1; after the forward selection algorithm identifies the significant features in the first few phases  there is a sharp decrease in the contribution values of the features selected in the following phases 

figure 1: power-lawdistributionofcontributionvalues. this loglog plot of the distribution of the contribution values  absolute value  in the first phase for arrhythmia and dexter  prior to making any feature selection  demonstrates a power law behavior. the corresponding plots for the other datasets show identical power-law characteristics  though with different slopes   and were eliminated for the sake of clarity.
while with backwardelimination  there is a gradual and rather stable increase in the contribution values of the eliminated features. the peaks in the graph of the contribution values in figure 1a demonstrate that the contribution values do change as the csa iterates . in this case  the selection of a single feature considerably increased the contribution value of another feature  pointing at intricate dependencies between features.
　figures 1 and 1 also assist in explaining why backward elimination usually outperforms several feature selection methods  including forward selection; due to the high dimensionality of the datasets  a feature that assists in prediction merely by coincidence  may be selected  on the account of other truly informative features. forward selection is penalized severely in such case: among the few significant features  some will not be chosen. however  backward elimination always maintains the significant features in the non eliminated set; a feature that truly enhances the classifier's generalization will do so for the validation set as well  and will not be eliminated. this leads to a more stable generalization behavior for backward elimination on the test set through the algorithm's progress  figure 1 .
1 final notes
the contribution-selection algorithm presented in this paper views the task of feature selection in the context of coalitional games. it uses a wrapper-like technique combined with a novel ranking method which is based on the shapley contribution values of the features to the classification accuracy. the csa works in an iterative manner  each time selecting new features  or eliminating them  while taking into account the features that were selected  or eliminated  so far.
　csa  similarly to wrapper algorithms  is restricted in the selection of the induction algorithm used for evaluating features sets  due to time limitations. this problem can be reduced by parallelizing  an advantage not shared by other

	1	1	1	1
 b 	number of eliminated features
figure 1: predictionaccuracyandfeaturecontributionduringforwardselection a andbackwardelimination b forthearrhythmia dataset. both figures show how the performance of the c1 classifier improves on the validation set as the algorithm selects  eliminates  new features  while the contribution values of the selected features decrease  increase . the backward elimination generalizes better on the test set through the algorithm's progress. the behavior for the other datasets is similar.
wrapper algorithms which use search methods such as hill climbing; at each phase the permutations can be computed in parallel and upon completion combined to obtain an estimate of contribution values. furthermore  as the algorithm progresses  the number of candidate features for either selection  forward selection  or elimination  backward elimination  decreases. consequently  the number of permutations sampled may be reduced  speeding up the algorithm significantly. the restriction in selecting the learning algorithm for csa does not apply to the prediction once the features are selected. after a set of features is found by the csa  it may be used by any induction algorithm as demonstrated in section 1 with the dexterand arcenedatasets.
　we verified that the feature sets selected by csa are significantly different than those selected by filter methods  and random forests. it turns out that the first  strong  features are selected by most methods. but within few iterations  filters and csa select entirely different features due to the fact that the contributionvalues of the candidate features are modified  sometimes drastically  according to the already selected features.
　the csa was tested on number of datasets  and the results show that the algorithm can improve the performance of the classifier  and successfully compete with an existing array of feature selection methods  especially in cases where the features interact with each other; in such cases performing feature selection with a permutation size higher than one  namely not using the common greedy wrapper approach  can enhance the classifier's performance significantly.
　the results successfully demonstrate the value of applying game theory concepts to feature selection. while the forward selection version of the algorithm is competitive with other feature selection methods  our experiments show that overall  the backward elimination version is superior to them  and produces features sets which can be used to generate a highly performing classifier.
