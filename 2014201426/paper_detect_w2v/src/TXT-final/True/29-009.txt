 
windowing has been proposed as a procedure for efficient memory use in the id1 decision tree learning algorithm. however  it was shown that it may often lead to a decrease in performance  in particular in noisy domains. following up on previous work  where we have demonstrated that the ability of rule learning algorithms to learn rules independently can be exploited for more efficient windowing procedures  we demonstrate in this paper how this property can be exploited to achieve noisetolerance in windowing. 
1 	introduction 
windowing is a general technique that aims at improving the efficiency of inductive classification learners. the gain in efficiency is obtained by identifying an appropriate subset of the given training examples  from which a theory of sufficient quality can be induced. such procedures are also known as subsampling. windowing has been proposed in  quinlan  1  as a supplement to the inductive decision tree learner id1 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. 
　despite first successful experiments in the krkn chess endgame domain  quinlan  1   windowing has not played a major role in machine learning research. one reason for this certainly is the rapid development of computer hardware  which made the motivation for windowing seem less compelling. however  recent work in the areas of knowledge discovery in databases  kivinen and mannila  1; toivonen  1  and intelligent information retrieval  lewis and catlett  1; yang  1  has recognized the importance of subsampling procedures for reducing both  learning time and memory requirements. 
　a good deal of this lack of interest can be attributed to an empirical study  wirth and catlett  1  which showed that windowing is unlikely to gain any efficiency. the authors studied windowing with id1 in various domains and concluded that it cannot be recommended as a procedure for improving efficiency. the best results were achieved in noisefree domains  such as the mushroom domain  where it was 
1 	learning 
able to perform on the same level as id1 without windowing  while its performance in noisy domains was considerably worse. in  fiirnkranz  1a   we have demonstrated that rule learning algorithms are better suited for windowing in noisefree domains  because they learn each rule independently. in this paper  we will show how this property can be exploited in order to achieve noise-tolerance. 
1 the i-rip algorithm 
we have conducted our study in the framework of separateand-conquer rule learning algorithms that has recently gained in popularity  fiirnkranz  1b . the basic learning algorithm we use  i-rip  is based on i-rep  fiirnkranz and widmer  1  and its successor ripper  cohen  1 . however  the algorithms presented in this paper do not depend on this choice; any other effective noise-tolerant rule learning algorithm could be used in i-rip's place. 
　i-rep achieves noise-tolerance by first learning a single  consistent rule on two thirds of the training data and then pruning this rule on the remaining third. the resulting rule is added to the theory  and all examples that it covers are removed from the training set. the remaining training examples are used for learning another rule until no more meaningful rules can be discovered. in  cohen  1  it was shown that some of the parameters of the 1-rep algorithm  like the pruning and stopping criteria  were not chosen optimally. we have implemented the i-rep algorithm as described in  fiirnkranz and widmer  1   but used ripper's rule-value-metric pruning criterion and its 1-rule-accuracy stopping criterion. we have not implemented ripper's rule optimization heuristics. thus our i-rip algorithm is half-way between i-rep and ripper. as such  it is quite similar to i-rep*  which is also described in  cohen  1   but it differs from it in that its implementation is closer to the original i-rep. for example  i-rip considers every condition in a rule for pruning  while i-rep* only considers to delete a final sequence of conditions. on the other hand  i-rep* is able to handle numerical variables  missing values  and multiple classes  which our implementation of i-rip currently does not support. however  these are no principle limitations to the algorithm  and standard enhancements for dealing with these problems could easily be added to all algorithms described in this paper. 

1 	windowing and noise 
the windowing algorithm described in  quinlan  1  starts by picking a random sample of a user-settable size initsize from the total set of examples and uses it for inducing a classifier with a given learning algorithm  in our case the i-rip algorithm briefly described in the last section. this theory is then tested on the remaining examples and the examples that it misclassifies are moved from the test set to the window. another parameter  maxincsize  aims at keeping the window size small. if this number is exceeded  no further examples are tested and the next iteration starts with the new window. to ensure that all examples are tested in the first few iterations  our implementation takes care that those examples which remain untested in one iteration will be tested first in the subsequent iteration. we have named our implementation of a windowed version of i-rip win-rip. 
　an efficient adaptation of this windowing technique to noisy domains is a non-trivial endeavor. in particular  it cannot be expected that the use of a noise-tolerant learning algorithm like i-rip inside the windowing loop will lead to performance gains in noisy domains. the contrary is true: the main problem with windowing in noisy domains lies in the fact that it will eventually incorporate all noisy examples into the learning window  because they will be misclassified by a good theory. on the other hand  the window will typically only contain a subset of the original learning examples. thus  after a few iterations  the proportion of noisy examples in the learning window can be much higher than the noise level in the entire data set  which will make learning considerably harder. 
　assume for example that wln-rip has learned a correct theory from 1 examples in a 1  xx  examples domain  where 1% of the examples are misclassified due to noise. in the next iteration  about 1 noisy examples will be misclassified by the correct theory and will be added to the window  thus doubling its size. assuming that the original window also contained about 1% noise  more than half of the examples in the new window are now erroneous  so that the classification of the examples in the new window is in fact mostly random. it can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. this hypothesis is consistent with the results of  wirth and catlett  1  and icatlett  1    where it was shown that windowing is highly sensitive to noise. 
1 	a noise-tolerant version of windowing 
the windowing algorithm described in  furnkranz  1a   which is only applicable to noise-free domains  is based on the observation that rule learning algorithms will re-discover good rules again and again in subsequent iterations of the windowing procedure. such consistent rules do not add examples to the current window  but they nevertheless have to be re-discovered in subsequent iterations. if these rules could be detected early on  they could be saved and the examples they cover could be removed from the window  thus gaining computational efficiency. the algorithm discussed in  fturnkranz  1a  achieves this by separating the examples that are covered by rules that have been consistent for a larger 

figure 1: a noise-tolerant version of windowing. 
number of examples  so that subsequent iterations only have to learn rules for the yet uncovered parts of the search space. 
　the 1-win algorithm shown in figure 1 is based on the same idea. at the beginning the algorithm proceeds just like win-rip: it selects a random subset of the examples  learns a theory from these examples  and tests it on the remaining examples. however  contrary to win-rip  it does not merely add examples that have been incorrectly classified to the window for the next iteration  but it also removes all examples from this window that are covered by good rules. to determine good rules  win-rip tests the individual rules that have been learned from the current window on the entire data set and computes some quality measure from this information  procedure significant in figure 1 . 
　in principle  this quality measure is a parameter of the windowing algorithm. for example  one could use a measure as simple as  consistency with the negative examples  in order to get a windowing algorithm that is suitable for learning from noise-free data sets. however  in noisy domains  noisetolerant learning algorithms will typically produce rules that are not consistent with the training data. thus  a more elaborate criterion must be used. we have experimented with a variety of criteria known from the literature  but found that they are insufficient for our purposes. for example  it turned out that  at higher training set sizes  cn1's likelihood ratio significance test  clark and niblett  1  will deem almost any rule learned by i-rip as significant  even if the distribution of covered positive and negative examples deviates only slightly from their distribution in the entire training set. 
　eventually  we have settled for the following criterion: for each rule r learned from the current window we compute two accuracy estimates  accwin r  which is determined using only examples from the current window and acctot r  
	furnkranz 	1 

also be considered as such candidates. i-win randomly selects maxincsize of these candidate examples and adds them to the window. by sampling from all examples covered byinsignificant rules  not only negative examples as in regular windowing   we hope to avoid part of the problem outlined in the previous section. however  we stick to adding uncovered 
positive examples only  because after more and more rules have been discovered  the proportion of positive examples in the remaining training set will considerably decrease  so that the chances of picking one of them by random sampling will also decrease. adding only positive uncovered examples may lead to over-general rules  but these will be discovered by the second part of our criterion and appropriate counter-examples will eventually be added to the window. 
　the actual implementation of our algorithm makes use of several optimizations that minimize the amount of testing that 
has to be performed in the algorithm. an important addition considers the case when the underlying learning algorithm is unable to learn any rules from the current window. then  the algorithm in figure 1 will add maxincsize uncovered positive examples to the current window. our implementation of the algorithm deals with these cases by doubling the window size and re-initializing it with a new random sample of the new size. we think that this may lead to faster convergence in some cases  but have not yet systematically tested this hypothesis. furthermore  all algorithms discussed in this paper attempt to remove semantically redundant rules in a postprocessing phase. such rules only cover training examples that are also covered by other rules. we refer to  furnkranz  1al for more details. 
	1 	experimental evaluation 
in each of the experiments described in this section  we report the average results of 1 different subsets of the specified training set size  selected from the entire set of preclassified examples. all algorithms were run on identical data sets  but some random variation may have resulted from the fact that i-rip uses internal random splits of the training data. for each experiment we measured the accuracy of the learned theory on the entire example set and the total run-time of the algorithm.1 all experiments shown below were conducted with a setting of initsize = 1 and maxincsize = 1. these settings have been found to perform well on noise-free domains  fiirnkranz  1a . we have not yet made an attempt to evaluate their appropriateness for noisy domains. 
first we have tested the algorithms on the 1 example 
mushroom database. although this database is known to be noise-free  it forms an interesting test-bed for our algorithms  
because it allows a rough comparison to previous results. for example  windowing with the decision tree learner id1 could not achieve significant run-time gains over pure id1  wirth and catlett  1   while the slightly modified version of windowing used in c1 is able to achieve a run-time improvement of only about 1%  p. 1 of  quinlan  1  . 
　the left column of figure 1 shows the accuracy and runtime results for i-rip  wln-rip  and three versions of i-

win  each one using a different setting of its parameter. in terms of run-time  both regular windowing  and our improved version are quite effective in this domain  at least for higher    1  training set sizes. the three versions of i-wln are clearly the fastest. in terms of accuracy  no significant differences can be observed between 1-rip  win-rip  and i-wln  1   although the latter is able to compensate some of the weakness of i-rip at low example set sizes that is due to its internal split of the data  furnkranz and widmer  1 . 1wln with  - 1 and  = 1 has a significantly worse performance  because these versions are often content with slightly over-general rules  which is detrimental in this noisefree domain. however  we have shown that our windowing algorithm is in fact able to achieve significant gains in runtime without losing accuracy  thus confirming our previous results  furnkranz  1a . 
　for testing the algorithms' noise-handling capabilities we have performed a series of experiments in a propositional version of the well-known krk classification task  which is commonly used as a benchmark for relational learning algorithms. the goal is to learn rules for recognizing illegal white-to-move chess positions with only the white king  the white rook  and the black king on the board. the propositional version of this domain consists of 1 binary attributes that encode the validity or invalidity of relations like adjacent     and = between the coordinates of three pieces on a chess board. we have generated 1 noise-free examples in this domain  which were always used for testing the accuracies of the learned theories. the training sets were generated by subsampling from the 1 example set. artificial noise was generated by replacing the classification of n% of the training examples with a randomly selected classification  chosen with a fair coin . mushroom domain  and are not shown here. 
　the middle column of figure 1 shows the results in the krk domain at a very moderate noise level  1% . regular windowing cannot achieve any performance gains. on the contrary  it is almost twice as expensive as i-rip. i-wln with a noise-free setting of is even more expensive: it needs more than 1 sees  for a 1 example training set  which is six times as much as i-rip. the noise-tolerant versions of our algorithms outperform the other algorithms in terms of run-time. in terms of accuracy  a setting of  seems to heavily over-generalize. performs reasonably well  although it is still a little behind in accuracy. the size of good values for  seems to have some correlation with the noise level in the data. we have performed experiments with various levels of noise and confirmed that higher values of a will 
	furnkranz 	1 

produce better results with increasing levels of noise.1 
　in this domain  we also performed a series of experiments with the aim of analyzing the behavior of i-rip and i-win over varying levels of artificial noise.1 the results in terms of accuracy were very inconclusive with both algorithms having their ups and downs. in terms of run-time  we found that iwin outperforms i-rip at lower noise levels  but the converse is true for higher noise levels. the more random the data are  the less likely it is that the rules learned by i-rip from a window of small size will bear any significance. thus i-win has to successively increase its window size without being able to remove any examples that are covered by rules learned in previous iterations. consequently  it has much larger run-times than i-rip  which learns only once from the entire data set. however  for reasonable noise levels  which can be expected to occur in most real-world applications  say   1%   i-win significantly outperforms i-rip. for example  1-rep's runtime of 1 secs  for learning from the 1 example set with 1% noise is about 1 times higher than that of i-win with a setting of  = 1. this advantage decreases with increasing noise-level: at a noise-level of 1%  i-win is still about 1% faster  but at 1% i-rip is already about five times faster than i-win. the highest noise-level for which i-rip is faster than i-wln increases with training set size  1% for 1 examples  1% for 1  1% for 1 . we take this as evidence that the chances of i-win outperforming i-rip increase with increasing training set sizes or with increasing redundancy in the data. 
　currently  the implementation of our algorithms is limited to binary symbolic domains. the algorithms are not able to handle continuous attributes  missing values  or multiple classes  although nothing in the algorithms prevents the use of standard techniques for dealing with these problems  like the use of thresholds  turning multi-class tasks into a sequence of binary tasks  etc. unfortunately  we were not able to detect a natural domain of a reasonable size in the uci data repository which meets the constraints of our implementation. so we decided to try our algorithms on a discretized  1-class version of quinlan's 1 example thyroid diseases database.1 in this simplified domain  c1 without any pruning  the unpruned tree obtained with -m 1  achieves an accuracy of 1%  estimated by a 1-fold cross-validation  while the pruned tree obtained with default settings has an accuracy of 1 %. the respective tree sizes are 1 vs. 1. we take this as evidence that the data set contains at least a moderate amount of noise. consequently  c1's windowing procedure is quite inefficient and takes more than twice as long    1 cpu secs.  for growing a single tree from the entire data set  parameter -t 1  than c1 with default parameters    1 cpu secs. . 
　the right-most column of figure 1 shows the results in this domain. i-wln with  = 1 significantly outperforms i rip at both measures  run-time and accuracy. only when the entire data set is used for both training and testing  irip maintains an accuracy advantage. this  however  only raises the suspicion that i-rip overfits the data in this domain  while the significance test used in i-wln is able to correct this to some extent by evaluating the predictive performance of the simpler rules learned at low window sizes on 
the entire training set. 
1 	further research 
i-win contains several parameters. in all experiments in this paper we have set the initial window size to 1  and the maximum window increment to 1. we have found these parameters to perform well on noise-free domains  furnkranz  1a   but in some experiments we have encountered evidence that larger values of these parameters could be more suitable for noisy domains. another crucial parameter is the a parameter used in the significance test we have employed. we have seen that in noise-free domains   = 1 will produce good results  while in noisy domains higher values   ~ must be used. we have also seen that the setting of this parameter is very sensitive: too low a setting may lead to exploding costs  while too high a setting may lead to overgeneralization. efficient methods for automating this choice would be highly desirable. 
　another important question is how an extension of i-wln that handles numeric data with thresholding will affect the performance of the algorithm. we expect that the fact that fewer thresholds have to be considered at lower example set sizes will have a positive effect on the run-time performance of windowing  but may have a negative effect on the accuracy of the learned rules. this hypothesis has been stated before  catlett  1   but has never been empirically verified. in fact  we would not be surprised  if a lower set of potential thresholds  like the ones contained in the current window  gave the algorithm less chance for overfitting and could thus even increase predictive accuracy. 
　it lies in the nature of windowing that it can only work successfully  if there is some redundancy in the domain  i.e. that at least some of the rules of good theory can be learned from a subset of the given training examples. in  ftirnkranz  1a  we present an example for a noise-free dataset  where this assumption does not hold  and consequently windowing is not effective. techniques for estimating the redundancy of a domain would be another valuable point for further research. 
1 related work 
there have been several approaches that use subsampling algorithms that differ from windowing. for decision tree algorithms it has been proposed to use dynamic subsampling at each node in order to determine the optimal test. this idea 

has been originally proposed  but not evaluated in  breiman et al  1 . this approach was further explored in catlett 's work on peepholing  catlett  1   which is a sophisticated procedure for using subsampling to eliminate unpromising attributes and thresholds from consideration. 
　most closely related to windowing is uncertainty sampling  lewis and catlett  1 . here the new window is not selected on the basis of misclassified examples  but on the basis of the learner's confidence in the learned theory. the examples that are classified with the least confidence will be added to the training set in the next iteration. 
　a different approach that successively increases the current learning window is presented in  john and langley  1 . here examples are added until an extrapolation of the learning curve does no longer promise significant gains. however  the authors note that this technique can in general only gain efficiency for incremental learning algorithms. 
　work on partitioning  i.e. splitting the example space into segments of equal size and combining the rules learned on each partition  has also produced promising results in noisy domains  but has substantially decreased learning accuracy in non-noisy domains  domingos  1 . besides  the technique seems to be tailored to a specific learning algorithm and not generally applicable. 
1 	summary 
we have presented a noise-tolerant version of windowing that is based on a separate-and-conquer strategy. good rules that have been found at smaller sizes of the training window will be kept in the final theory  and all examples they cover will be removed from the training set  thus reducing the size of the window in the next iteration. examples are added to the window by sampling from examples that are covered by insignificant rules or positive examples that are not covered by any rule of the previous iteration. although we have used a fixed noise-tolerant rule learning algorithm throughout the paper  the presented windowing technique could use any noise-tolerant rule learner as its basic algorithm. 
acknowledgements 
this research is sponsored by the austrian funds zur forderung der wissenschaftlichen forschung  fwf . financial support for the austrian research institute for artificial intelligence is provided by the austrian federal ministry of science and transport. i would like to thank ray mooney for making his common lisp ml library publicly available  which has been used for the implementation of our programs; gerhard widmer for his comments on an earlier version of this paper; the maintainers of and contributors to the uci machine learning repository; and the three anonymous reviewers for valuable suggestions and pointers to relevant literature. 
