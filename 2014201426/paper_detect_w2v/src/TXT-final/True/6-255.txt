 
this poster shows an artificial neural network capable of learning a temporal sequence. directly inspired from a hippocampus model  banquet et al  1   this architecture allows an autonomous robot to learn how to imitate a sequence of movements with the correct timing. 
1 introduction 
this article considers the problem of learning to predict events  i.e. to forecast the future behavior of a system using past experience. this problem has often been viewed and formalized in neural network theory as so-called temporal sequence learning. studying such sequences is a topic of research in several domains such as robotic trajectory planning  speech or vision processing. for most of these  neural networks provide two distinct mechanisms: one for spatial information and the other for temporal information. the main mechanism stores the sequence events regardless of the temporal dependences between them. in parallel  or later  the second mechanism  the so-called short time memory  stm   extracts and learns the temporal relationships between the events. moreover  we have shown in previous works  gaussier and moga  1; andry et al  1  that the capability of learning a temporal sequence is one of most important features of a learning by imitation system. as a capability of learning by observation  imitation is a strong learning paradigm for autonomous systems. imitation can improve and accelerate the learning of sensory/motor associations. in our work  oriented to the design of a neural network architecture allowing learning by imitation  we are involved in the first level of imitation  whiten and ham  1 . this  proto-imitation  level plays a key role in understanding the principles of the perception/action mechanisms necessary to perform higher order behaviors and it is likely that the protoimitation is triggered by a perception ambiguity. in our approach  the starting point for an ''imitating behavior  implementation is the capability of learning temporal sequences of movements. 
1 temporal sequence learning model 
almost all neural models of sequence learning use a discrete temporal dimension by sampling the continuous time at regular intervals. in these models  time proceeds by intervals of at  and the interval between 1 items of a sequence is considered as a few units of at  usually less than 1 . in this section  we introduce the model1 for timing sequence learning with a variable and a long range time interval and we evaluate it. our model  fig. 1 -left  is based on the idea that a prediction  p-type  neuron learns the timing between 1 items  or  to be more precise  learns to predict the end of this time interval. this time interval starts with the firing of a derivation  d-type  neuron and ends with the firing of an input  e-type  neuron. the p-type neuron learns this interval using the activity of the granular  g-type  group of neurons. 

figure 1: left : the overview of the neural model allowing time prediction. the gi are time base neurons  p is the prediction neuron  d and e are formal neurons. right: detailed activity of the p-type neuron. the p neuron firing predict the learned time interval. 
   let us consider a simple example: we present the first item and  one second later  we present the second item. the length of the interval to be learned is t1 = l.s. the first item forces the d neuron to fire. the firing of the d neuron resets all the activity of the g neurons. starting at this instant  the g neuron's activity is expressed by eq. 1 . one second later  the second item forces the e neuron to fire. the firing of the e neuron enables the update of the weight between the p and the g neurons i.e. enables the learning of the to interval. finally  when the first item is presented again the d neuron fires again. 1 milliseconds later  i.e. 1 milliseconds before 
     1 the model is inspired by the functions of two brain structures involved in memory and time learning: the cerebellum and the hippocampus  see  banquet et al. 1  for further neurobioiogical rcferences  

poster papers 	1 

to  the p neuron fires  fig. 1-right  and predicts an imminent firing of the e neuron. 
		 1  
i - position of the neuron in the group  the ith cell of the battery ; and - time constant and the standard deviation associated with 
the ith neuron; 	- instant of the last reset of the battery. 
1 	sequence learning architecture 
timing learning models are currently used by neurobiologist modelers for conditioning simulation ibullock et al  1; grossberg and merrill  1 . alternatively  the proposed model permits the temporal sequence of events to be learned and predicted. in our context  a simple sequence is defined as an enumeration of events with the associated timing  eg.  a b c  and a cyclic sequence as periodic simple sequence  eg.  a b c a b c a ...   with the associated timing. the main idea is to use several batteries of g neurons for learning the timing between two consecutive events and a group of p type neurons for learning the eveni sequencing. the global architecture is shown in fig. 1. the input group  cc  can be viewed as the input interface. any neuron of this group represents a sequence event and it is on while the corresponding input event is present; otherwise it is off. each cc neuron is one-to-one linked with ec group of neurons. the ec group is made up of d-type neurons. each ec neuron is linked with unconditional links to all neurons of a battery in the dg group. in the same way  an ec neuron is connected with unconditional links to all neurons of the corresponding column of the ca1 group. the dg group integrates several batteries 

figure 1: detailed connectivity of the event prediction network. the circle size in dg is associated with the time constants  mi  of the g type neurons. 
of g-type neurons. a dg battery is equivalent to a gi group of neurons shown in section 1. each dg neuron is connected via conditional links to all neurons of the corresponding row in the ca1 group. the size of the cc group  respectively ec  is constrained by the maximum length of sequences. alternatively  the size of a dg battery is a function of the prediction precision of the time interval between two events of the sequence. this architecture can learn all event combinations  in other words  all possibles sequences. consequently  the size of the ca1 group is the square of the input group  cc . the output group  ro  is a winner take all neurons group. a neuron of the ro group has the same signification as a cc neuron. the ro outputs are connected to the ec inputs via one-to-one secondary unconditional links. 
¡¡the architecture allows the learning of all kind of simple or cyclic sequences. the size of learned sequences is limited only by the system memory capacity. 
1 	conclusion 
the timing learning model and the associated neural networks were successfully utilized in  gaussier and moga  1; moga  1  to teach an autonomous robot different  dances . the proposed model allows the correct timing to be predicted and it concords with weber's law. even if weber's law concordance is not a prerequisite  it corresponds to a strong constraint of neurobiological inspired models of perception and learning: if the prediction precision is constant then it is not possible to have reinforcement due to repetitive experiments. in addition  this model was successfully employed  andry et ai  1  to build a learning model based on the prediction of rhythms as a reward signal and for spatiotemporal transition learning in autonomous robot navigation. these results prove that the proposed neural network architecture can serve both as a starting point for understanding imitation mechanisms  and as an effective learning algorithm for autonomous robotics. 
