ions
xiaofeng zhang and william k. cheung
department of computer science
hong kong baptist university
kowloon tong  hong kong
{xfzhang william} comp.hkbu.edu.hk

abstract
due to the increasing demand of massive and distributed data analysis  achieving highly accurate global data analysis results with local data privacy preserved becomes an increasingly important research issue. in this paper  we propose to adopt a model-based method  gaussian mixture model  for local data abstraction and aggregate the local model parameters for learning global models. to support global model learning based on solely local gmm parameters instead of virtual data generated from the aggregated local model  a novel em-like algorithm is derived. experiments have been performed using synthetic datasets and the proposed method was demonstrated to be able to achieve the global model accuracy comparable to that of using the data regeneration approach at a much lower computational cost.
1 introduction
most machine learning and data mining algorithms work with a rather basic assumption that all the data can first be pooled together in a centralized data repository. recently  there exist a growing number of cases that the data have to be physically distributed due to some practical constraints  such as privacy concerns. the need to tackle them calls for the recent research interest in distributed data mining.
¡¡a common methodology for distributed data mining is to first perform local data analysis and then combine the results to form a global one  kargupta et al.  1 . one major limitation is that local analysis may result in loss of information for global pattern discovery. in  zhang et al.  1   a model parameter exchange approach was proposed to improve the global model accuracy  which however is not a generic methodology which can easily be generalized to different types of global models. an alternative is to adopt a
¡¡flexible enough model for local data abstraction and have the subsequent analysis based on the local model parameters  which is what we adopt in this paper.
¡¡as inspired by  merugu and ghosh  1   gaussian mixture model  gmm   bishop  1  is chosen for the local data abstraction. via different parameter settings  gmm can interpolate a set of data with more detailed information as well as a rough ensemble illustration. thus  by sharing only local model parameters for global analysis  the degree of data privacy preservation and the bandwidth requirement in a distributed environment can readily be controlled. in this paper  an em-like algorithm is proposed for learning a global gmm directly from the aggregated version of the local abstractions.
1 problem formulation
1 local data abstraction
let ti ¡Ê  d denote an observed data instance  ¦Èl denote the parameters of the lth local model  gmm  as the abstraction of the lth source. the probability density function of the lth local model plocal ti|¦Èl  with kl components is given as 
plocal ti|¦Èl  = pkj=1l ¦Ájlpj ti|¦Èlj  pkj=1l ¦Ájl = 1
pj ti|¦Èlj  =  1¦Ð  d1 |¦²lj| 1 exp{ 1 ti   ¦Ìlj t¦²lj 1 ti   ¦Ìlj }.
¡¡l sets of local gmms' parameters {¦È1 ¦È1 ... ¦Èl} obtained by performing local model-based density estimation  e.g.  the em algorithm  are sent to a global server to learn a global data model.
1 learning global model parameters
we start the derivation of our learning algorithm by considering first the conventional setting of the expectation maximization  em  algorithm. let rik denote the estimated indicator for the kth component of the global model generating the ith data instance.
¡¡assume rlk is now a new indicator for the lth local component1 with its underlying data to be generated by the kth

global component and rik denotes the average estimate of the probability that data instances from the lth local component are generated by the kth global component. by approximating rik as
pi¡Êlthsource rik
	rik ¡Ö rik =	 1 
nl
where nl denotes the number of data from the lth source  the new m-steps become
¡¡¡¡pll rlk¦Ìl ¦Ìk = p=1ll=1l rlk ¦Ák =tl 1 pll=1 rlk  1 
	¦²k = pl=1 rlkl  ¦²l+¦Ìl¦Ìl	  ¦Ìk¦Ìtk .
pl=1 rlk
kl-divergence is adopted to compute rlk  e-step   resulting in
	rlk =	mexp{ d pglobal t|¦Õk ||plocal t|¦Èl  }	 1 
pi=1exp{ d pglobal t|¦Õi ||plocal t|¦Èl  }
where d p||q  denotes the kl-divergence between the two probabilistic models p and q  which can be derived as ln |¦²l| +  trace ¦² l 1¦²k  +  ¦Ìk   ¦Ìl t¦² l 1 ¦Ìl   ¦Ìk    1 .
|¦²k|
1 experiments
experiments have been performed using synthetic data sets for performance evaluation. four data sets  as ground truth  were generated using known mixture models with 1  1  1 and 1 components  respectively. the data instances were then randomly partitioned as four sources and local gmms were learned via the conventional em algorithm for each source. two global models were trained  one using the proposed method and one using the conventional data regeneration approach. as shown in the table 1  the proposed method can achieve a speedup factor ranging from 1 to 1 with highly comparable model accuracy.
table 1: comparing the global models learned using the proposed method and the data regeneration method  gx denotes the true data model with x components .
time/speeduplog-like./diff.mean/diff.klg1: 1:1s/1-1/+1.1/-111s/1-1/-11/-11g1:
1:
1:1s/1-1/-1.1/-111s/1-1/-1.1/+111s/1-1/+1.1/+11¡¡the proposed approach has also been applied to a more sizeable dataset generated using a gmm with 1 components. global models with different number of components were learned and their corresponding kl-divergence values when compared with the true model are shown in figure 1  a . the divergence value decreased rapidly when the number of components increased from 1 to 1 and then started to saturate. one could determine the optimal number of global components by setting a threshold on the kl-divergence value as cut-off. figure 1  b  shows the result with 1 global components identified.
1 conclusion and future plan
in this paper  we proposed a novel em-like algorithm for learning a global model directly from the aggregated local model parameters with promising experimental results on synthetic datasets. currently  we are extending the obtained results to learn more sophisticated global models. furthermore  we are investigating how active negotiation between

 a  the kl-divergence of the model learned using the proposed method with different numbers of components of the global model.

 b  gmm with 1 components learnedbased on 1 local gaussian components for the aggregated local model.
figure 1: experimental results with a sizeable dataset.
the global mediator and the local data sources for compromising individual local data sources' privacy levels can be adopted to improve the global model accuracy.
acknowledgement
this work has been partially supported by rgc central allocation group research grant  hkbu 1/c .
