 
 we address the conflict between identification and control or alternatively  the conflict between exploration and exploitation  within the framework of reinforcement learning. qlearning has recently become a popular offpolicy reinforcement learning method. the conflict between exploration and exploitation slows down q-learning algorithms; their performance does not scale up and degrades rapidly as the number of states and actions increases. one reason for this slowness is that exploration lacks the ability to extrapolate and interpolate from learning and to a large extent has to  reinvent the wheel . moreover  not all reinforcement problems one encounters are finite state and action systems. our approach to solving continuous state and action problems is to approximate the continuous state and action spaces with finite sets of states and actions and then to apply a finite state and action learning method. this approach provides the means for solving continuous state and action problems but does not yet address the performance problem associated with scaling up states and actions. we address the scaling problem using functional approximation methods. towards that end  this paper introduces two new reinforcement algorithms  qlvq and quad-q-learning  respectively  and shows their successful application for cart centering and fractal compression. 
1 introduction 
reinforcement learning is  the on-line learning of a mapping from situations to actions so as to maximize a 
scalar reward or reinforcement signal. the learner is not told which action to take  but instead must discover which actions yield the highest reward by trying them. in most interesting and challenging cases  actions may affect not only the immediate reward  but also the next situation  and through that all subsequent rewards. 
these two characteristics - trial-and-error search and delayed reward - are the two most important distinguishing features of reinforcement learning   sutton  1 . 
in what is called on-policy learning  sutton and 
barto  1   the agent learns to predict the long-term reward from a fixed policy. with on-policy learning  the learning agent does not change its policy function during learning. the agent only learns to predict the long term expected reward for each state of the environment given that it maintains the same policy and it learns the value function  with off-policy learning  on the other hand  the learning agent changes its policy during learning so as to improve long term expected reward. the objective of off-policy learning is to find an optimal policy function  i.e.  to find the policy that always leads to the best possible long term expected reward. off-policy learning is accomplished through a process of trial and error. the learning agent has to probe the environment in order to determine the directional information needed to modify its behavior. this probing action slows the operation of the learning agent because the changes in behavior required to explore the environment are generally in conflict with how behavior should be changed to exploit the gradient information determined by exploration. this problem is known as the conflict between identification and control or alternatively  the conflict between exploration and exploitation. 
1 	q-learning 
q-learning derives the optimal policy function incrementally as it interacts with the environment. it assumes no apriori knowledge of rewards or transition probabilities. the idea of q-learning is to learn a q-function that maps the current state s and action a to a utility value q s  a  that predicts the total future discounted reward that will be received from current action a and from all subsequent actions  assuming the optimal policy  is followed in subsequent actions. the optimal policy is determined 

1 	uncertainty and probabilistic reasoning 

from the q function as = argmax q s  a . the 

objective of q-iearaing is to learn the q function directly without ever explicitly learning transition probabilities and expected values of rewards. suppose that we know the q function and that currently the environment is characterized by the state an agent then chooses an action a so as to maximize choosing action a results in an environmental state transition from state to state  the agent then chooses the next action b so as to maximize 
 given that the optimal policy is followed after action a is taken  which is the estimate of utility of an action a taken from state is just the immediate reward of taking action a from state plus the maximum utility possible from the next state discounted by the discount factor therefore q satisfies 
where is the immediate reward received when action a is taken from state and is the probability of transitioning to state given that the current state is i. o-learning employs a one step back-up using the estimated q function value at the state of the next time step  watkins and dayan  1 . 
1 	learning and functional approximation 
the basic q-learning algorithm is very slow to converge. one reason for this slowness is that the q function must be learned for each state and action pair independently; having a good idea of the value of q i  a  provides no information about the value of q j  b  for nearby state j and action b. algorithms may converge to local optima or may not converge at all. with onpolicy learning  the convergence of the value function   is all that is of concern because the policy remains fixed. however  with off-policy learning  the objective is to find an optimal policy by incrementally changing it to improve system performance. hence with offpolicy learning  the exploration aspect is important and convergence to the optimal policy is of primary concern  although convergence of the value function is also important. 
¡¡for finite state and action systems  both on-policy and off-policy algorithms exist that can be proven to converge  sutton and barto  1 . the problem that arises with these algorithms is that although they converge  they do not scale up well; that is their performance degrades rapidly as the number of states and actions increases. moreover  not all reinforcement problems one encounters are finite state and action systems. our approach to solving continuous state and action problems is to approximate   discretize   the continuous state and action spaces with finite sets of states and actions and then to apply a finite state and action learning method. this approach provides the means for solving continuous state and action problems but does not address the performance problem associated with scaling up states and actions. we address the scaling problem using functional approximation methods. sect. 1 and 1 describe two new reinforcement algorithms  qlvq and quad-q-learning  respectively  that address the discretization and scaling up problems. 
1 	qlvq 
q-learning performance deteriorates as the dimension of the state space increases. for the case when the policy function is a piecewise constant function with relatively smooth transition regions  we describe in this section qlvq  a novel reinforcement algorithm  that tessellates the state space into regions with piecewise smooth boundaries using labeled vector quantization  lvq   kohonen  1 . the motivation behind integrating lvq and q-learning comes from the fact that lvq  i  discretize the phase space for control problems and significantly reduces the state space requirements  and  ii  improves the overall accuracy as it estimates  'interpolates'  between neighboring cells. 
¡¡the new q - learning and learning vector quantization  qlvq  algorithm is shown in fig. 1. steps 1 through 1 are similar to those employed by q-learning. first one positions k  random  action prototypes for each of the 
c possible actions 	on the state x 
space. the action ai consists of prototypes whose labels correspond to their positioning on the state space x  a total of kc prototypes are used by the lvq component of qlvq. if prototype is nearest to state 
x then the action selected is while ties are arbitrarily broken. the estimate for the utility  is then updated as it was the case with the conventional qlearning by combining the immediate reward  with a discounted utility estimate from the next state y. let dq be the change in the value of q after step 1. the prototypes are then repositioned  at iteration t   see step 1 . 
initialize the action-value function q and the learning rates and  
1. while stopping condition is false 
1. randomly generate state  1. select an action a to execute:  
1. execute action a  and let y be the next  state and  be the reward received. 
1.update  
1. let dq be the change measured after q update  step 1  

figure 1. qlvq algorithm 
	claussen  gutta  and wechsler 	1 

1 	cart centering using qlvq 
control problems represent a good test bed to assess reinforcement algorithms due to their need to determine decision control boundaries expressed as policy  stimulus -action  functions  mappings . the cartcentering problem  discussed in this section  was chosen because its analytical solution is readily available and comparative performance is thus feasible. cart centering is modeled using a phase space representation and the control aspect is handled using qlvq reinforcement learning. 
¡¡phase space for a dynamical system refers to the entire set of allowable states. for a non-autonomous system  such as cart centering  an external control variable  in addition to the initial state and a specified control function  determine the future trajectory of the system. for discrete control problems such as cart centering   bang-bang   control  the optimum policy or decision function is piecewise constant on disjoint regions of the phase space. these regions are separated by hypersurfaces called  switching boundaries . determination of optimum control then reduces to the computation of these boundaries. the table look up approach of q-learning  however  does not exploit the property of the policy function being piecewise constant. the lvq algorithm exploits this property in the determination of the optimal switching boundaries and this motivates the hybrid qlvq reinforcement learning scheme. 
¡¡the cart-centering problem involves a cart that can move either to the left or to the right on a frictionless one-dimensional track  bryson and ho  1; koza  1 . the problem is to center the cart in minimal time  by applying a force of fixed magnitude. the following state information is available: the current position of the cart along the track  x  and the current velocity of the cart  v . at each time step  t   the controller must decide in which direction the force should be applied to the cart so as to bring the cart to a target state of rest  velocity: 1; position: 1 . the cartcentering problem can be described using the following set of equations: 

x: position of the cart  v: velocity of the cart  t: time  a: acceleration  r. size of the time step  normally set to 1 sec   m: mass of the cart  here 1 kg  and f: magnitude of force  here in . 
¡¡the exact  analytical  time-optimal solution specifies that for any given current position x t  and current velocity v t   the applied fixed force f would accelerate the cart in the positive direction  if the inequality shown below holds true: 

the applied fixed force f accelerates the cart in the negative direction when the above inequality does not hold. fig. 1 depicts the computed time-optimal solution map for the cart-centering problem. the boundary between the shaded and unshaded portions of the graph is the switching boundary for the problem. 

figure 1. time-optimal solution map for the cartcentering problem. 
¡¡standard q-learning for cart-centering is run on two  x  v  grid sizes   1  1  and  1  1   respectively  to assess how it scales up. q-learning computes the overall change in the policy q function from one cycle to the next and should stop as soon as the change becomes less than a given threshold  using such stopping criteria the q-learning algorithm failed to otop for both grid sizes. when the phase space dimension is  1  1   the q learning algorithm is terminated after  arbitrary  1 cycles. note that even though a state is randomly selected  the chance of a state  x  v  not being considered during several cycles is very small as q-learning is executed for hundreds of cycles  each cycle consisting of x * v iterations. fig. 1a below shows the q  phase state  - map for q- learning algorithm after 1 cycles  the number of cycles required by qlvq  see fig. 1b  to stop  while fig. 1c shows the q - map after q-learning has been aborted. similar behavior for q-learning is observed when the grid size is doubled to  1  1 . q-learning is aborted after 1 cycles  while qlvq requires only 1 cycles before it stops. 

figure 1a. q - map for 
cart-centering using a phase space  1  
grid after 1 cycles figure 1b. qlvq - map for 
cart-centering using a 
phase space  1  1  grid 
after 1 cycles 
1 	uncertainty and probabilistic reasoning 


figure 1c. q - map for cart-centering using a phase space  1  1  grid after 1 cycles 
¡¡we also assessed whether there exists a solution leading to the state of  rest   i.e.  the state  x  v  =  1  starting from every point in the phase space and within a specified time limit  e  arbitrarily set to 1  is used to compensate for the grid being discrete rather than continuous. the time limit for each point in the phase space is a slight increase  two  on the number of steps required by the analytical solution to reach the rest state. 
¡¡if the grid size is of dimension  1  1   then the number of points in the phase space trying to reach the state of rest neighborhood is 1 * 1 - 1  'rest state'  = 1 while for a grid size dimension of  1  1  the number of points is 1 experimental data shows that for a grid size dimension of  1  1   
1  1 or 1% of the points in the phase space  using q - learning  do not reach the state of rest neighborhood within the time limit  while for qlvq  only 1% of the points in the phase space do not reach the rest state neighborhood. for the case when the grid size is increased to  1  1   1 or 1% of the points in the phase space  using the q - learning solution  fail to reach the state of rest neighborhood within the time limit  while for the qlvq solution 1 or only 1% of the points in the phase space  fail this test. 
1 	quad-q-learning 
quad-q-learning is an outgrowth of q-learning. quadq-learning  requires a different notion of state than that of q-iearning. in quad-q-learning there are two types of actions: those that create new states  called split type actions  and those that do not  called no split type actions. when a split type action is taken  four new environments arise  each with their own state. when a no split type action is taken  the corresponding environment's state does not change and no further actions are taken with respect to that environment. to avoid confusion  we will use the term block attribute  or just attribute  with respect to quad-q-learning  instead of state. quad-q-learning is best understood in the context of a quad tree partitioned image  where an attribute vector such as size and variance represents each image block in the quad tree partition. 
¡¡the objective of quad-q-learning is to learn a policy function  denoted as  that maps local attributes of a range block to a decision on whether or not to split the block and  if the decision is not to split the block  whether to use the affine or bath transform to code it in terms of the pool of available domain blocks. this decision is made in a way that will lead to a decompressed image of the desired quality while maintaining good compression rates. the policy function is learned by generalizing from a few image blocks to make decisions about an entire image. 
¡¡with respect to fractal compression  it is more convenient to work with costs instead of rewards. costs can be thought of as negative reward  but it will be convenient to use a version of quad-q-learning that can handle costs directly. we are actually interested in two costs  one corresponding to compressed image quality  and one corresponding to image compression rate. instead of learning directly  we learn a function that maps * block attribute vector and related action to an expected cost. the block attribute consists of two components  block size and block variance. for each block size and each action the value of the functions is assumed to be a linear function of range variance. block size is a discrete value because of the nature of quad tree partitioning  while variance  on the other hand  is a continuous attribute. hence the local block attribute of a block can be denoted as  i  x   where i is an index of block size  and x is the continuous valued block variance. there are four possible actions:  1  code block using affine transformation   1  code block using bath transformation   1  split as an affine block  and  1  split as a bath block. hence the expected cost of an action a on a block with attribute  i  x  is denoted as q the policy function n is related to q and learned using 
 we assume that the q func-
a 
tion can be expressed as 
¡¡¡¡we learn the q function by learning estimates of and  at each time stepduring learning one obtains estimates which are used to estimate at each time step of the learning procedure  a potential range image block with attribute vector  i  x  is considered. a no split decision corresponds to a decision to code the block using the affine transform or the br th transform. a split type decision corresponds to a cecision to split the block and use only the affine or only the bath transform to code its sub blocks. 
for simplicity  let 	where 
is an index representing the kth block of size considered for action 'v  code using the affine or bath transform  and n is the number of blocks visited  be an instance of the cost of no split action  a  for a block 
	claussen  gutta  and wechsler 	1 

with attribute and 
then one way to estimate mi.a and b is as follows: 
 a  
 b  
where the denominator of  a  is assumed to increase infinitely as n increases without bound. equations  a  and  b  are just the equations for calculating regression coefficients where are the independent variables and  are the dependent variables. in other words we are estimating the parameters of a linear function relating the block variance and the cost of coding. the split action estimation procedure is similar. in the no split case  we are simply trying to estimate the dependent variable cost  as a linear fimction of block variance. the split case is the same  except that the dependent variable is estimated from estimates of the children blocks. that is  we are bootstrapping our estimates. 
1 	fractal compression using quadq-learning 
the image compression process involves image presmoothing  lean domain pool construction  block classification  quad-q-learning  fractal compression  and parameter quantization  coding and storage. quad-q learns a q function which is used to partition an image and to make decisions about which transform  affine or bath  to use for regions in an image based upon local block attributes. the fractal compression phase exploits the q function to encode an image. 
¡¡quad-q-learning learns a q function that captures the relationship between cost  local block attributes and a decision on how to encode a block. a decision determines whether or not a block will be split and what transform method  either affine or bath  will be used to encode a range block. we will actually use two different costs. the first cost estimates the quality of the reconstructed image measured as the root mean square  rms  distance between the original image and the decoded image. the second cost estimates the compression rate measured as the number of bits that it takes to encode a block. these two costs will be used for learning two different q functions  one corresponding to image quality and the other corresponding to image compression. image quality  is estimated using the rms distance between the transformed domain block and the range block  barnsley  1 . 
¡¡quad-q-learning proceeds in two stages. the first stage learns the q function for no split type decisions  while die second stage learns the q function for split type decisions. in general a block at level m of a quadtree is of size  in stage 1 no costs are calculated; rather the costs calculated in stage 1 and the q function estimates from the previous level of stage 1 are passed up in the quad-tree  similar to boot strapping  sutton and barto  1 . that is the cost of splitting a block of size  is an estimate of the sum of the minimum q function estimates of each quadrant block. 
fractal compression uses the q functions as follows. 
first one calculates the variance for the largest range block. one then uses the variance  block size  and the q function to determine the estimated image quality cost  rms  of directly coding the block using each of the affine and bath transforms. if both transforms are acceptable  we use the affine transform since it requires fewer bits. if neither bath nor affine transforms will achieve the desired quality by direct coding  then we use recursive block splitting. the q function is used to determine the expected rms from block splitting. if both transforms achieve the desired rms through block splitting  as determined by the q function  then the transform type yielding the minimum product of expected rms and expected bit rate is chosen. otherwise  the transform with the best expected rms is used. once the transform for the block of largest size is chosen  only that transform is used to code sub-blocks of that block. for smaller blocks  if the block is of smallest size and therefore cannot be split  the block is coded directly using the current transform type. if the block is not of smallest size  then a split or no split decision is made by first calculating the variance for the block and then using the q function to determine the estimated image quality cost of coding the block directly. if the cost of direct coding is acceptable  the block is coded directly; otherwise  the block is split. 
¡¡our novel scheme for fractal compression is evaluated by comparing between jpeg  nelson  1   fisher's fractal compression  fisher  1  as implemented in fracomp  kassler  1   and fractal compression with quad-q-learning  denoted as learnfrac in the graphics below. both rate-distortion curves and compression-time curves are calculated and depicted for each of the aforementioned methods for several images. fig. 1 shows comparative results for the aforementioned compression methods on the lena image. 

1 	uncertainty and probabilistic reasoning 


figure 1. comparison of compression methods 
¡¡bit rate  compression  is measured as the ratio of the file size of the original image measured in bytes to the file size of the compressed image measured also in bytes. distortion  image quality  is measured as the peak signal to noise ratio  psnr . the compression rate vs time to compression curves are important to ensure that improvements in the rate-distortion curves are not merely the result of increased compression time. in general  fractal compression with quad-qlearning requires approximately 1 seconds for learning time  independent of compression. the time advantage gained from learning at high psnr more that offsets the cost of learning and hence fractal compression with quad-q-learning performs comparatively better than fractal compression without learning at low compression and high psnr. for practical locations on the bit rate distortion curve  including the point of the crossover image  fractal compression with quad-qlearning generally performs better than fractal compression alone. the quad-q-learning method and the associated new fractal techniques taken together form a new fractal compression algorithm that frequently out performs other fractal compression algorithms with respect to the trade-off between bit rate  distortion  and compression time. at compression rates above 1  fractal compression with quad-q-learning will produce better quality compressed images than jpeg and outperform it with respect to psnr for high compression rates. jpeg will compress faster  but our proposed method still is well under 1 seconds for a 1 x 1 pixel image on a typical pc. in addition  the proposed method is faster than other fractal compression algorithms for the same compression rate and compressed image quality. another advantage of our proposed method comes from the possibility of choosing the transform type. the bath transform  because of its linear functional part  should be the choice for ranges with significant slope characteristics. 
1 	conclusions 
we addressed in this paper the conflict between identification and control or alternatively  the conflict between exploration and exploitation  within the framework of reinforcement learning. our approach to solving continuous state and action problems has been to approximate the continuous state and action spaces with finite sets of states and actions and then to apply a finite state and action learning method. this approach provides the means for solving continuous state and action problems but does not yet address the performance problem associated with scaling up states and actions. we addressed the scaling problem through generalization using functional approximation methods. towards that end  this paper introduced two new reinforcement algorithms  qlvq and quad-qlearning  respectively  and showed their successful use of functional approximation for generalization purposes with application for cart centering and fractal image compression. 
