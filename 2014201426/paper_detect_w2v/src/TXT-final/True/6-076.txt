 
often the most expensive and time-consuming task in building a pattern recognition system is collecting and accurately labeling training and testing data. in this paper  we explore the use of inexpensive noisy testing data for evaluating a classifier's performance. we assume 1  the  human  labeler provides category labels with a known mislabeling rate and 1  the trained classifier and the labeler are statistically independent. we then derive the number of  noisy  test samples that arc  on average  equivalent to a single perfectly labeled test sample for the task of evaluating the classifier's performance. for practical and realistic error and mislabeling rates  this number of equivalent test patterns can be surprisingly low. we also derive an upper and lower bound for the true error rate when the labeler and the classifier are not independent. 
1 	introduction 
the overall construction of a modern classification system can be divided into four broad tasks:  1  specifying the classifier type   1  collecting data   1  training the classifier  i.e.  learning   and  1  evaluating the classifier  i.e.  testing   duda et al  1 . the second stage  data collection  can further be divided into two tasks: gathering samples and labeling them. recently  the machine learning community has realized that in many practical cases the most expensive part of the whole design process is the labeling of such samples. for example  there is an enormous number of text documents on the internet that can be obtained at very low cost; however  relatively few of these have been labeled - e.g.  according to content topic  language  or style - in a consistent way that would facilitate training a classifier. likewise  there are large databases of recorded speech  handwritten digits  and printed characters but these databases  too  are either not labeled accurately or not labeled at all  stork  1 . to reduce the labeling expense  many researchers have sought ways to modify training algorithms so as to utilize both labeled and unlabeled data  blum and mitchell  1; nigam et al  1 . this approach has shown surprisingly encouraging results  in some cases reducing the number of 
david g. stork 
ricoh innovations  inc. 
1 sand hill road  suite 1 menlo park  ca 1 stork rii.ricoh.com 
labeled samples by a few orders of magnitude  nigam et al  1 . 
¡¡in order to build up and extend this success in reducing the labeling cost  we turn to the problem of reducing the need for accurately labeled data in the classifier evaluation stage. in fact  most of the experiments for learning with labeled and unlabeled data use much more labels for testing than training  nigam et al  1 . thus we now need to address the labeling cost for classifier evaluation. 
¡¡as with many areas of commerce  the general economics of labeling is such that the higher the quality  accuracy  of labeling  the greater the associated cost. this greater cost may be due to greater expertise of the labeler  or the need for multiple passes of cross-checking  or both. there is thus an additional cost to  clean  or  truth  those data and labels. in some situations  such as marking a text corpus  the labeling task is complicated enough that even experts need several passes to reduce labeling errors  eskin  1 . furthermore  in some application domains  obtaining accurate labels is simply too cost prohibitive. for example  for some medical diagnostics  the true disease can only be known with expensive or invasive techniques. similarly  in remote sensing  one must send measuring instruments to the ground location to obtain the  ground truth   and the transportation cost can be astronomical.  it is quite literal for remote sensing of other planets  smyth  1 .  for both situations in practice  one must rely on the imperfect judgements of experts  smyth  1 . 
learning 	1 ¡¡we propose to lower the labeling cost in classifier evaluation by using cheaper  noisy labels. this paper examines methodologies of estimating the error rate and classifier confusion matrix using test data with noisy labels. we shall see that even a slight labeling inaccuracy  say  1%  can have a significant effect on the error rate estimate when the classifier performs well. in addition  when data sets used to be small and expensive to collect  it made sense to spend each additional labeling effort to increase label accuracy on that small data set. however  when data sets are large and cheap to collect  it is no longer obvious how one should spend each additional labeling effort. should one spend it labeling the unlabeled data  or should one spend it increasing the accuracy of already labeled data  we present a preliminary analysis to this question. 
1 	preliminaries and notation 
our formulation assumes an object x possessing a true label 
y €    where 
of nature  e.g.  category membership  for the object. the ob-
ject is presented to a labeler  who marks it with a label 
as his guess of y. the situation that is call a labeling error  or a mislabeling . the classifier system  on the other hand  is presented with the feature vector x that represents certain aspects of the object  and the classifier outputs a label y x  g  as its guess of y. for notational convenience  we will call the classifier output y  and its dependence on the feature vector x is implicit. the situation that is call a classification error  or a misclassification . 
¡¡the probability of the labeler making mistakes  is called the mislabeling rate. the probability of the classifier's label being different from the labeler's label  is called the apparent error rate. our goal is to estimate pr y =y .   which is called the true error rate. note that it is possible to have a high apparent error rate even with a perfect classifier  with a true error rate of zero  simply because of a high mislabeling rate. that is  the classifier can classify all test data perfectly  but will often disagree with the test labels because those labels are incorrect. on the other hand  it is also possible to have a zero apparent error rate even with a high true error rate if the classifier and the labeler make the same kind of mistakes. 
the confusion matrix for the human labeler is defined as 

¡¡for many two-class cases where one class has a much higher prior probability  the actual error rate is not a good measure of classifer usefulness. for example  in detecting email spams or network intrusions  the undesirable events are so rare that one can easily get an error rate less than 1% by classifying all events as  desirable.  in those situations  then  one may want to compute the entire confusion matrix or metrics such as precision and recall  frakes and baeza-yates  1 . we denote  as the  rare  class  e.g.  spams or network intrusions . for the classifier  precision is defined as and recall is defined as 
  and analogously for the labeler. note 
that precision and recall can be derived from the confusion matrix and the class prior probabilities. 
1 	obtaining the true error rate 
in examining the relationship between true and apparent error rates  we make the constraint that we have a two-class problem  that is  the above derivation assumed that 	 
wise the noisy labels are meaningless. in practice  is always much less than 1. more important is the independence assumption that the labeler and the classifier make errors independently  or stated succintly  
. that is  knowing that the labeler had 
made an error on a pattern does not change the probability that the classifier would also make an error  and vice versa. section 1 will deal with some situations in which the independence assumption does not hold. in the meantime  we argue for this idealization and simplification based on the fact that human and computer generally classify samples using different methodologies  and thus they may not make similar kinds of mistakes. 
1 example: apparent error rate for various true error rates and mislabeling rates 
equation 1 gives us a way to account for noisy labels when calculating the true error rate. a natural question  then  is how important is it to correct for the influence of noisy labels  let's consider some classification systems with error rates between 1% and 1%! and testing data sets with 1% to 1% incorrect labels. table 1 shows the apparent error rate for classifiers of different accuracy and testing data of different mislabeling rates. the percentage increase over the true error rate is also shown. for example  even when only 1% of the testing labels are wrong  a classifier with true error rate of 1% will have an apparent error rate 1% higher  at 1% . the percentage increase is even more dramatic with noisier labels or more accurate classifiers. a quick rule of thumb is that  when the labels have relatively few errors  the denominator in eq. 1 is approximately 1 and can be ignored. the mislabeling rate of the testing labels  is then just an additive component to the true error rate. continuing the previous example  a 1% mislabeling rate for a classifier with true error rate of 1% makes the apparent error rate approximately 1%  when the actual is 1%. 
1 	noisy labels for estimating true error rate 
above we assumed knowledge of the apparent error rate   but in practice  we must estimate this rate us-
ing test data. 	in this section  we analyze the effects of 
¡¡¡¡1  wc note that many classifi ers on the uci datasets have accuracy within this range  kaynak and alpaydin  1 . 

1 	learning 


table 1: the left sub-columns of the table show the apparent error rates for different true error rates and different mislabeling rates based on eq. 1. it is assumed that the labeler and the classifier make errors independently. the right sub-columns of the table  with up-arrow signs  show the percentage increase of the apparent error 

rate over the true error rate  i.e.  

figure 1: the figure shows the number of noisy labels needed to achieve the same variance in the true error rate estimate as a single perfect label  see eq. 1 . the four plots represent different true error rates. as the mislabeling rate increases  more noisy labels arj needed to achieve the same confidence. note that  in the ranges shown in the figure  when the mislabeling rate is smaller than the true error rate  a single perfect label is equivalent to less than four noisy labels. 
such estimates. assume we have / objects in the test set  each with a feature vector x   true but unknown label yi  noisy label y   and classification in. assume further that t u p l e s a r e independent 
and identically distributed as the apparent error rate estimate is 	in which is the indicator function  i.e.  i evcnt  = 1 if event is 
true and 1 otherwise . an estimate of the true error rate is 

unbiased. intuitively we know that we have less confidence when the error estimates are based on test data with noisy labels. to formalize this intuition  we examine the variance of the true error rate estimate  

the variance of the error estimate given perfectly labeled data is  thus  to get the same vari-
ance  the ratio of noisy labels to perfect labels is 
	 1  
this ratio will help us understand the economic trade-offs between using perfect and noisy labels. collecting perfect labels  or collecting noisy labels first and cleaning them  is often much more expensive than just collecting noisy labels itself. therefore it may be economically justified to used noisy labels  as long as one does not need too many more of them. 
¡¡unfortunately  applying eq. 1 requires us to know the true error rate of the classifier  which is exactly what one is trying to estimate. however  we often already have a good idea of a reasonable range for the true error rate. in any case  we examine the ratio for a wide range of true error rate and mislabeling rate  and we found the ratio to fall within a relatively narrow range  as shown in fig. 1. even for relatively noisy testing data with 1% incorrect labels  unless the classifier is much more accurate  with true error rate of less than 1%   cleaning the testing data to be perfectly labeled increases its value by less than a factor of four. in other words  one needs much less than four such noisy labels to achieve the same effect as one perfect label. imagine that perfect labels need to be collected from a domain expert  whereas noisy labels can be collected from a non-expert  the high cost of a domain expert can often 
justify the use of noisy labels. 
1 example: evaluating with many noisy labels or few reliable labels 
in many labeling tasks  experts must make multiple passes through samples to ensure accurate labeling  eskin  1 . we now question the wisdom of that policy when the samples are free but labeling cost is a constraint. 
¡¡consider a hypothetical labeling situation with two labelers  each paid to look at / samples  and both labelers have an error rate of e. there are two choices in how to use these two labelers. one is to have them look at completely different samples  thus in the end we have a testing set of size 1/ and mislabeling rate e. another choice is to have them look at the 

exact same samples. assuming that they make independent 
learning 	1 

classifications  e.g.  false positives and false negatives  are 

figure 1: the figure shows which one of the two labeling policies is optimal for a range of mislabeling rate e and true classifier error rate  based on eq. 1. the problem is posed such that two labelers both have mislabeling rate c and are paid to label / samples. one policy is that they label different samples  creating a test set of  1 labels on 1/ samples   with e portion mislabeled. the other policy is that they both label the same samples  creating a testing set of  1 labels on / samples   with portion mislabeled  after various assumptions . 
labeling errors  and optimistically assume that a sample has a wrong label only if both labelers err  then we have a testing set of size / and mislabeling rate  which is the better policy  
¡¡based on the previous discussion  we can have an unbiased estimate of the true error rate from either testing set. we then should prefer one that gives us a lower variance estimate. that is  we go with the  1/ labels on 1/ samples  policy if its variance is lower than the  1 labels on / samples  policy  

which  after a little algebra  becomes 
   1  
an interesting observation is that in the realistic range of e the left hand side of eq. 1 is negative for which means that one should always choose the  1/ labels on / samples  policy for such inaccurate labelers  and such high mislabeling rate does occur in practice  smyth  1 . for other cases  we have plotted the policy boundary 
in fig. 1. 
fig. 1 shows 
that one should prefer the  1/ labels on 1 samples  policy unless the classifier error rate is very low. the hint for practitioners is that time spent cleaning labels is often not as effective as time spent labeling extra samples. 
1 	obtaining true confusion matrix 
in evaluating classification systems  we often need to know more than just the error rate. when the cost of different misnot equal  we may want to know the full confusion matrix. in addition  in some domains  such as classifying text or spam  the distribution of classes is highly skewed  and the error rate can be misleadingly low. in those situations  we are more interested in precision and recall statistics  which can be estimated from the confusion matrix. 
¡¡we define the joint distribution matrix between labeler and classifier as 

which can be estimated from data. note that unlike our analysis of the error rates  it is not necessary to assume two-class problems. 
¡¡our goal is to recover the classifer's confusion matrix given the labeler's confusion matrix and the joint distribution matrix between labeler and classifier. if we make the independence 
assumption t
the decomposition 	we can rewrite the decomposition in matrix form and solve for the classifier's confusion matrix. 
 1  
in which abilities  
¡¡when py is not given  it can be derived. to see this  define the probability vector 
i t i s the case t h a t i s a column vector of c 1 's. it is also the case that combining those two equations we have 
		 1  
1 	example: precision/recall breakeven points 
as mentioned earlier  one benefit of being able to recover the confusion matrix is that one can then work with precision and recall measures. we analyze the following system to see some effects of noisy labels on those measures. to reduce the number of variables examined  we only look at precision 

1 	learning 

recall breakeven points  defined as the points where precision and recall are equal. they will simply be denoted as precision/recall. for a given py  and precision/recall  the confusion matrix is uniquely determined. table 1 shows the apparent precision/recall  i.e.   for different actual classifier precision/recall and labeler precision/recall. table 1 a s s u m e s a l t h o u g h the values are almost exactly the same for both and 

1 	bounds on true error rate when the classifier and labeler are not independent 
in deriving the true error rate  eq. 1   we have made the assumption that the labeler and the classifier make errors independently. we argue for this assumption because human and computer use different methodologies to classify samples. even for algorithms inspired by human reasoning  e.g.  neural networks   they still do not learn human intuition but they do avoid psychological biases. it is even harder to imagine algorithms based on more abstract models  e.g.  support vector machine  to err in similar ways as humans. furthermore  in many application domains  e.g.  speech recognition   humans label the samples based on a full presentation of the object  whereas the feature vector x used for classification are mathematical notions  e.g.  linear vector coefficients  that have little neurological basis. in many other application domains  e.g.  statistical text classification   assumptions that blatantly violate how human reasons are often made  e.g.  assume words in a text are independently generated  rather than in a grammatical way . lastly  to make a stronger argument  we can require the training data to be labeled independently from the testing data  or better yet  be perfectly labeled   thus avoiding the possibility that the computer would learn biases and other  bad habits  from the training data that would correlate with labeling errors in the testing data. 
¡¡however  even with the above reasoning for the independence assumption  it is still conceivable for one to be more conservative and assume some non-negative dependency  

that is  the probability of a classifier misclassifying a sample is higher if the labeler has also mislabeled that sample   and vice versa. this can happen  for example  if the training data have been mislabeled in the same way as the testing data  and the classifier has learned to imitate those mislabelings. we have deliberately ignored the case of negative dependency  
  as we are hard-
pressed to find a justification for it in practice. 
¡¡the non-negative dependency assumption is easily incorporated into eq. 1 by changing the equal sign to a less-thanor-equal-to sign. propagating that change through the derivation  we have a lower bound on the true error rate  

the second inequality can be tight if the mislabeling rate is small  as the denominator of the first inequality becomes approximately one. 
separately we derive an upper bound for the true error rate. 

note that no assumption is used in deriving the upper bound  not even limiting to two-class problems . one can easily verify that the bound is exact when the mislabeling rate is zero . the bound is also exact when the apparent error rate is zero  such that the true error rate is equal to the mislabeling rate. thus with the looser assumption of non-negative dependency  the true error rate is in the range of pr ynot=y  ¡À pi y not= y . 
1 example: simulation of non-negative dependency between classifier and labeler 
in the above derivation  the lower bound is achieved exactly when the mislabeling rate is small and the independence assumption is true. we examine how tight the upper bound is by simulation. we have taken pairs of classes from ucl's opt-digit dataset  which is a handwritten digit recognition dataset  and trained both a naive bayes classifier and a nearest-neighbor classifier  duda et ai  1  on the training set of each pair. the nearest-neighbor classifier is then used to simulate a labeler and labeled the testing set. the naive bayes classifier is the classifier under evaluation. since we have the actual labels for the testing set  both the mislabeling rate  of the nearest-neighbor  labeler   and the true error rate  of the naive bayes classifier  can be determined. the output of the naive bayes classifier and the nearest-neighbor  labeler are compared to determine the apparent error rate. 
¡¡we have chosen the opt-digit dataset and the nearestneighbor algorithm because we know that this combination can give very low error rate  kaynak and alpaydin  1   thus closely matching the accuracy of many human labelers. in fact  for most pairs of classes  the nearest-neighbor algorithm has zero error. the opt-digit dataset is also interesting because the handwritten digit recognition task is a classical example in which much human labeling effort has been applied. the naive bayes classifier is chosen because it is a popular classifier and is sufficiently different from nearestneighbor to give interesting results. 
¡¡table 1 shows the results for some pairs of classes where the nearest-neighbor  labeler  has non-zero error. note that an insignificant positive dependence between the naive bayes classifier and the nearest-neighbor  labeler  should be expected since they both are trained from the same dataset  use the same features  and assume independence of those features  explicitly in naive bayes and implicitly in nearest-neighbor through its distance metric   even though they are different in other aspects  e.g.  naive bayes is generative while nearestneighbor classifier is discriminative . the naive bayes classifier's true error rate is almost exactly the upper bound for the pairs  1  and  1   but it is much closer to the apparent error rate for the pairs  1  and  1 . the simulation thus shows the upper bound to be tight in some non-trivial situations. 
1 	discussion and future work 
when designing classification systems there are frequently parameters that are not learned automatically from the train-

learning 	1 


table 1: error rates on pairs of digits from the uci opt-digit dataset. the classifier under evaluation is a naive bayes classifier  nb   and a nearest-neighbor classifier  nn  is used to simulate the  human  labeler. the error rate of the nb and nn classifiers are considered to be the true error rate and mislabeling rate  respectively. the fraction of time the two classifiers disagree is the apparent error rate. the upper and lower bounds are derived in sec. 1  which are simply the apparent error rate plus or minus the mislabeling rate. the true error rate does come very close to the upper bound in some cases   1 - 1 and 1 - 1 . 

ing data. some examples are the number of hidden units in a feedforward neural network  the number k: in a k-nearestneighbor classifier  and the window width in a parzen window classifier  duda et al  1 . validation is one technique to estimate those parameters. in validation  one conceptually creates several classifiers with different values of the parameter and train them with the same training set. the trained classifiers are evaluated on the validation data set  and the best classifier is chosen. our results for testing with noisy labels is directly applicable to validation. in fact  validation is not concerned with the actual value of the true error rates  but just their ordering. therefore the apparent error rate can work just as well  as long as the mislabeling rate is less than 1. 
¡¡in a world where  unlabeled  data is cheap  noisy labels arc easily obtained  e.g.  the open mind initiative  stork  1; stork and lam  1    but perfect labels are expensive  the findings in this paper allow one to confidently use noisy labels for testing and validating. an obvious area for future work is to use noisy labels for training as well. although some works do allow for training with noisy labels  this has not been an active research area  szummer and jaakkola  1 . 
¡¡so far in our derivations we have assume either knowledge of the mislabeling rate  or the labeler's confusion matrix  in practice those information must be estimated and be treated as random. the effects of such estimates and the cost/benefit analysis of obtaining more accurate estimates are unknown. we hope to investigate them in the future. 
1 	conclusion 
traditionally test data have been assumed to be perfectly labeled. increasingly this assumption is becoming a burden. we advocate the use of noisy labels as a cheaper alternative. we have shown that  under the assumption in which the labeler and the classifier make mistakes independently  the true error rate and true confusion matrix can be derived exactly. we have also examined the number of noisy labels to achieve the equivalent estimation confidence as one perfect label  and we found that number to be less than four in many practical situations. furthermore  if we loosen the independence assumption to the non-negative dependence assumption  the true error rate can be bounded to be between the apparent error rate plus or minus the mislabeling rate. 
