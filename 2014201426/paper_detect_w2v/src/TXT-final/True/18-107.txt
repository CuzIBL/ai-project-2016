 
　we discuss the automation of mathematical reasoning  surveying the abilities displayed by human mathematicians and the computational techniques available for automating these abilities. we argue the importance of the simultaneous study of these techniques  because problems inherent in one technique can often be solved if it is able to interact 
with others. 
keywords 
　reasoning  mathematics  deduction  search  learning  proof analysis. 
1. introduction 
　a major goal of artificial intelligence is to automate reasoning. solutions to this goal will have both technological applications  enabling us to build more powerful expert systems  and scientific applications  providing models to compare with human reasoning. mathematics is an excellent domain for exploring the automation of reasoning because: 
 a  it provides a wide range of examples of reasoning  from the simple and shallow to the complex and deep; 
ematicians. the main theme will be that mathematical reasoning consists of more than just theorem proving  and that the simultaneous automation of other reasoning processes  e.g. learning  simplifies the automation task. each reasoning process outputs knowledge but also demands knowledge as input. if a computational technique for automating a particular reasoning process is studied in isolation then the provision of its input knowledge may prove a major barrier to automation. but the input knowledge of one technique is the output knowledge of another  so that the techniques fit together in an intercommunicating network. some techniques for automated reasoning may involve search. in isolation the search involved may be computationally expensive. but one technique may be used to control the search of another  especially if the two techniques are co-routined or even merged into one. the power of a system in which the various techniques interact in well-crafted ways will be more than just the sum of the power of the parts. 
　i advocate the simultaneous study of all the the processes of mathematical reasoning with particular emphasis on the possible interactions between the techniques that automate them. 

 b  it is possible to detach this reasoning from other considerations  such as sensory input of data; and  
 c  to a first approximation  the problems of knowledge representation have been solved. 
1. reasoning abilities in mathematics 
　what processes are involved in mathematical reasoning  in this section we discuss the various processes and the ai techniques that have been proposed to automate them. more details about these techniques can be found in  chang &  lee 1. bundy 1 . 

compare mathematics with other ai domains in which reasoning plays a part  e.g. natural language understanding or visual perception. in mathematics one is not bogged down with huge amounts of noisy data  nor concerned that what seems to be a difficult reasoning issue may evaporate if the knowledge representation were changed. one is free to concentrate on the reasoning problems and then make an attempt to translate any solution found to other domains. 
　in this paper i discuss the automation of mathematical reasoning  by which i mean any cognitive activity that mathematicians engage in as math-
   this research was supported by serc grant. number gr/c/1. 
　the first problem we face is how to classify mathematical reasoning into different processes - what my psychology friends call defining the task ecology. our starting point is a 'folk ecology' of reasoning processes  i.e. the terms used in prescientific discussion of reasoning. we will call these mathematical abilities. with these abilities we must associate computational techniques which implement them. these techniques will form our scientific classification. but the association of abilities to techniques not a neat 1 mapping. we are likely to find many techniques to associate with each ability. some abilities  e.g. learning  have so many diverse techniques associated with them that they seem highly unsuitable as scientific categories  whereas the techniques associated with some abilities  e.g. theorem proving  all have a strong family resemblance. some abilities require a combination of techniques. consider  for instance  analogy which requires separate techniques for finding and then using the analogy. each of these techniques will require sub-techniques. some of these techniques may contribute to several different abilities. consider  for instances  resolution-type deduction techniques  which are ubiquitous. 
　we list below the mathematical abilities we will be considering under the heading of mathematical reasoning and mention some of the techniques which have been used to implement these abilities. 
- proving theorems: the ability that has received the most attention in the ai study of mathematical reasoning  almost to the exclusion of all others  has been theorem proving. automated theorem proving has been an important subfield of ai throughout its history. the main techniques required to automate theorem proving are deduction which involves search control. deduction traverses a search space of legal inference steps. search control decides which of these steps to try. 
- formalizing problems: however  mathematicians  especially applied mathematicians  spend a large part of their time translating informal problem statements into mathematical formulae to which deduction  etc. may be applied. the initial stages of this translation involve natural language understanding  visual perception  etc - but these are not specifically mathematical techniques. later stages involve the specifically mathematical technique of formula extraction from the internal meaning representation  which may itself involve deduction  see section 1 . 
- learning: this term covers a multitude of processes  for instance  the learning of new mathematical theorems  the learning of new proof methods  the defining of new concepts  the conjecturing of results  etc. new theorems must be assimilated into the theorem proving ability so that they may be used effectively in the future. the assimilation technique required will depend particularly on the search control technique being used  e.g. the new theorem will need to be labelled so that it can be accessed when needed. similarly  new proof methods must be incorporated into the current search control technique. this may involve a proof analysis technique to analyse new proofs and generalize them to extract control information  e.g. proof plans. defining new concepts can be done by inductive inference from descriptions of examples and non-examples of the concept.1 conjecturing of theorems can be done by considering the hypothesis of the conjecture to be a concept to be defined using the above techniques. 
- using analogy: mathematicians use analogy to suggest conjectures and new definitions  and to guide proofs. all uses require analogical matching techniques to find and apply the relationship between the target and the source of the analogy  see  owen 1  for a survey . this is all that is required for suggesting conjectures and definitions  but to use a source proof to control the search for a target proof a further proof plan application technique is required. 
　this is by no means an exhaustive list; mathematicians also find counterexamples  write  publish and deliver papers  teach students  read textbooks  etc. unfortunately  we have nothing to say about these other abilities  so they are omitted. 
1. mathematical reasoning techniques 
　in this section we classify and discuss some of the techniques that have been developed for automating the mathematical abilities described in the last section. there is only space to discuss those we consider particularly promising. we group these techniques according to their computational purpose and mutual similarity  e.g. deduction  search control. inductive inference. etc. this provides a more scientific classification of mathematical reasoning processes than the list of mathematical abilities above. but even this classification is bound to be improved as we discover new techniques and gain a better understanding of the existing techniques and their inter-relationships. it should therefore be regarded as both incomplete and highly preliminary. 
1. deduction 
　the most well known deduction technique is resolution   robinson 1 . resolution is a rule of inference for predicate calculus  that is. it is a rule for deducing new logical formulae from old. to prove that a conjecture is a theorem of a mathematical theory both the axioms of the theory and the negation of the conjecture are expressed as clauses  a normal form for predicate calculus formulae . resolution takes a pair of clauses and makes subparts of each identical using a matching technique called unification. the remaining parts are combined together to make a new clause. resolution applied repeatedly to the initial clauses and to their successors defines a search space of clauses. this is searched for a contradiction. if the search is successful then the conjecture is a theorem. 
　the representation of axioms and conjectures as logical formulae is given in the texts of mathematics and logic. it is in this sense that i claimed that the knowledge representation problem 
   note that inductive infaranca it not tha same as mathematical induction       action 1 below   which ia a daductive rule of inference. 
	a. bundy 	1 
was solved to a first approximation. 　the resolution search space is guaranteed to contain a contradiction if and only if the conjecture is indeed a theorem. the if part is called completeness and the 'only if part is called soundness. the snag is that if the conjecture is not a theorem then the search may never terminate. the number of new clauses generated rises exponentially  or worse  with the length of the proof. if the proof is non-trivial then either the storage capacity of the computer or the patience of the human operator is exhausted before the proof is found. this is an example of the phenomenon called the combinatorial explosion. 
　various refinements of or alternatives to resolution have been designed and implemented  with the aim of improving on its space and/or time efficiency without losing completeness. one of the most powerful alternative deduction techniques is the connection calculus   bibel 1 . these improvements reduce the combinatorial explosion  however  but do not conquer it. 
　there have also been attempts to avoid the combinatorial explosion by proposing radically new deductive techniques. this has sometimes led to techniques which are efficient at proving restricted classes of theorems  e.g. rewrite rules  huet & oppen 1 . it has also led to the reinvention of the wheel - some researchers have rejected the logical approach  only to reinvent it together with the attendant problems of incompleteness and/or combinatorial explosion. 
　rather than reject logical deduction techniques it is necessary to augment them with heuristic techniques to control the search for a proof   hayes 1 . search control techniques developed have ranged from weak but general purpose ones  e.g. evaluation functions based on the complexity of the formulae  to powerful but special purpose ones  such as those described in the next section. for a survey see  bledsoe 1 . 
1. search control 
　a number of powerful search control techniques have been developed by careful analysis of the proofs of human mathematicians to extract the underlying control ideas and express them computationally. 
　the simplest technique  called heuristic rules  is to add these control ideas into the mathematical formulae as preconditions for their application. for instance  the lex program   mitchell et a/ 1   uses rewrite rules to symbolically integrate algebraic terms  e.g the rule: 

is used to integrate by parts. one class of terms which this rule successfully integrates is those where u is a variable  x  and dv/dx is a constant 

note that this precondition is too general; trig includes tan  for which the rule does not work. we discuss this problem in section 1 below. 
　note that these preconditions can get very complicated since the same rule may be used successfully in a number of different circumstances and the precondition must be the disjunct of these circumstances. note also that the rule now contains a mixture of factual and control knowledge. these complications can cause difficulties  e.g. in the automatic learning of such rules  so some researchers prefer to separate factual and control knowledge. 
	for instance  	the 	boyer/moore 	theorem 	prover  
  boyer & moore 1   represents its control knowledge as procedures for manipulating the mathematical formulae. this theorem prover exploits the relationship between recursion and mathematical induction to guide inductive proofs of the properties of lisp functions. the recursive definitions of the lisp functions are first used to symbolically evaluate the conjecture to be proved. this may fail because the conjecture contains arbitrary constants and the evaluation process requires lists with internal structure. this failure is used to guide the choice of induction scheme and variable  so that when symbolic evaluation is applied to the induction conclusion just enough structure will be available to enable the induction step to be performed. we will call this technique  recursion guidance. 
　note that recursion guidance involves an analysis of the conjecture and its failed proof  and the choice of an appropriate proof technique on the basis of this analysis. this analysis uses meta-
1
concepts to describe the conjecture  its proof and the proof methods  e.g. the terms induction hypothesis  induction step  etc in the boyer/moore theorem prover or trigonometric function in lex. these meta-concepts must be discovered by study of existing proofs  introspection  etc. it is in this sense that the knowledge representation problem is only solved to a first approximation  since the number of possible meta-concepts is open ended and the choice of appropriate ones determines the suc-
cess or failure of the technique. 
or an additional rule uaad for aach circumtances. 
　1 tha term meta' ia uaad bacause tha concepts in quaation daacriba the rapraaantation of the problem  i.a. they ara about it rathar than of it. 
　this process of analysis and guidance using metaconcepts is itself a reasoning process and can be conveniently represented as deduction - but deduction in a meta-theory  rather than the theory itself. such a use of deduction to guide deduction is made explicit in the press system   bundy & welham 1   for solving equations. the press metaconcepts describe the equation to be solved  e.g. the number of unknowns  their distance apart  the kind of functions involved  etc  and they describe various methods of solving equations or of achieving useful subgoals  e.g. reducing the number of unknowns  moving them closer together  making them occur within identical subterms  etc. deduction with these meta-concepts induces an implicit  but highly controlled  search for the solution to an equation. we call this technique  meta-level inference  see figure 1 . 
applied and how this reason fitted into the overall proof plan. for instance  sin u.cos u = 1.sin 1u was applied to reduce the occurrences of u from 1 to 1 in order that that single occurrence could be isolated on the left hand side of the equation. this kind of analysis is performed by the lp program.  silver 1   which was able to learn new methods of equation solving and extend the range of problems that press could solve. the technique used by lp is called precondition analysis because it discovers new methods by considering how unexplained steps establish the preconditions of successive steps. 
　in the case of the lex program for symbolic integration  also described in section 1.   the technique of back propagation was developed to analyse successful solutions and extract the control infor-


　these meta-concepts can also be used to express proof plans  e.g. the plan to move two unknowns closer together  then merge them and then isolate the remaining occurrence. this plan might be provided by the programmer or learnt by proof analysis of a worked example  see section 1 . alternatively the proof plan may be at the objectlevel  e.g. the proof of an analogous theorem  or a generalized proof.  plummer & bundy 1 . in each case it must then be applied by a proof plan application technique. this application might be straightforward - the target proof exactly following the plan - or the plan may need to be relaxed or augmented at various points. a variety of techniques have been suggested for realising such a flexible plan application technique  see  e.g.  silver 
1   
1. proof analysis 
　the process of constructing and augmenting powerful search control techniques  by adding metaknowledge gained from analysing proofs  can itself be automated. this analysis will depend crucially on the search control technique in question. for instance  in the case of the press system  described in section 1. the solution to an equation must be analysed using the meta-concepts of press. for each step an account must be given of not only what algebraic identity was applied but also why it was mation. this technique was incorporated in the lex1 program   mitchell et al 1. in back propagation the successful sequence of rewrite rules was applied in reverse order to a generalized answer to see what constraints this would impose on the original problem. these constraints then became the preconditions of the first rule of the sequence. for 
                                                                       1 instance  given a successful integration of cos  x   lex  used back propagation to discover that the same sequence of rules would have worked on any term of the form cosn x . where n was an odd integer. the first rule of this sequence  

was then given the precondition  f-cos &  odd r . in forming this constraint. back propagation also defined the concept. odd* as any number of the form 1k+ 1  where k is an integer. 
　proof analysis can also be used to analyse faulty proofs and repair them. for instance  a classic faulty proof in the history of analysis is due to cauchy. namely that a convergent series of continuous functions is continuous. this proof can be analysed using deduction and the fault identified as a missing occurs check during unification  see section 1. this suggests an obvious patch  namely changing the order of quantifiers in the theorem statement  and this generates three new concepts and three new and correct theorems. one of these is the traditional replacement of convergence by uniform convergence; one is a trivial theorem whose conclusion is always true; and one is a new theorem involving the concept of equi-continuity. this reasoning process  which we will call argument removal  has been implemented in a program  seidel   described in  bundy 1 . 
   it would have been more powerful to remember the whole sequence as a proof plan and make the constraints the preconditions of the plan rather than just its first step. 

   in contrast to the deduction of the equation solution  which is called object-level inference. 

   see  lakatos 1  for a discussion of this proof and its history. 

　for a survey of the use of proof analysis for learning see  boswell 1 . 
1. inductive inference 
　inductive inference has received a lot of attention in the learning literature  with a number of techniques being developed. for a survey see 
  dietterich et al 1  and for an analytic comparison of some of these techniques see  bundy et al 1. they all learn a concept from examples and  sometimes  non-examples of it. from examples of the concept the techniques might generalise: replacing specific relations and terms with more general ones. from non-examples of the concept the techniques might discriminate: pruning away relations and terms that are not an essential part of the concept. 
　this can be used for the learning of both new object-level  factual  and new meta-level  search control . for instance  mitchell et al's lex program   mitchell et al 1   used induction to learn the meta-level conditions for applying a particular strategy for integrating by parts. given that the strategy worked correctly to integrate 1x.cos x  and 1x.sin x   lfx generalized these terms to hypothesise that the strategy will work for terms of the form nx.trig x   where n is any integer and trig any trigonometric function. 
　both the sub-techniques of generalization and discrimination are crucially dependent on the description space. for instance  in lex the description space contains concepts like 1  1  n  cos  sin  trig  etc. the generalization of two concepts is the most specific concept in the description space which includes both of them  e.g. trig is the generalization of cos and sin. however  if a term for sine or cosine  were added to the description space then it would replace trig as the generalization of cos and sin. in fact  this is the concept required  as discussed in section 1. without it  lex overgeneralises to trig. 
1. formula extraction 
　a key technique in the mathematical formalisation of problems is the extraction of formulae from a meaning representation  e.g. the semantic representation of an english description of the problem. formulae extraction techniques have received little attention in al despite their importance in human mathematical reasoning. however  the mecho program   bundy et al 1   contained a formula extraction technique called the marples algorithm  for forming equations to describe mechanics problems. 
　the marples algorithm is a kind of plan formation technique. equations are formed by instantiating physical laws  e.g. f=ma. with each law is stored a list of the things it is about and a logical description of how these things relate to the variables in the law. for instance  f=ma is about an object and a direction. the variable m is the mass of the object  a its acceleration in that direction  and f the sum of the forces acting in that direction. a law and a situation are chosen after an analysis of 
a. bundy 1 
the unknowns and givens of the problem. the variables are then instantiated by inferring the logical description from the meaning of the english problem statement. 
　this inference process may use deduction and default reasoning to fill gaps between the statement of the problem and the knowledge required to instantiate the law. mecho used resolution guided by meta-level inference for the deduction and the closed world assumption for the default reasoning. deduction was needed  for instance  to work out the contributions to the sum of forces from gravity  the tensions of strings  the reactions of contact surfaces  etc. default reasoning was used  for instance  to assume that the only surfaces in contact were those mentioned in the problem statement. 
1. the interaction of reasoning techniques 
　the mathematical reasoning techniques outlined above can interact in a variety of ways. for instance  successful deductions might provide the material for proof analysis and for analogy. proof analysis may suggest proof plans to aid deduction. the desire to make a particular theorem hold may trigger a process of proof analysis that leads to changes in definitions and axioms. sometimes these interactions can be more intimate. several deduction techniques may be co-routined or even merged so that each prunes the search space of the others. we discuss some of these possibilities in more detail below. 
1. the interaction of techniques in the press 
family 
　in my research group work has continued over a number of years on a family of programs working on the common domain of symbolic equation solving. 
- as described in  bundy &  welham 1  and section 1 above  the press program used the deduction technique of rewrite rules to generate solutions to equations. this deduction technique was guided by the search control technique of metalevel inference. 
- the lp  learning press  program  see  silver 1  and section 1  used the proof analysis technique of precondition analysis to extract and conjecture new methods of solving equations which were then used by press. 
- the impress  inferring meta-knowledge about press  program   sterling & bundy 1   was a theorem proving program for proving properties of logic programs. it was used to prove properties of the prolog code of press using a modified version of recursion guidance  see section 1 . for instance  it proved the correctness of some press equation solving methods  i.e. that under appropriate preconditions the methods would achieve 
their goal. 	gence. - as described in  bundy et al 1  and section 1 above  the mecho program solved mechanics problems stated in english by extracting and solving equations using the marples algorithm. the equation solving part was done using press as a subprogram 
　each of the above techniques is incomplete on its own. meta-level inference requires a rich supply of meta- level concepts to analyse problems and bring to bear appropriate methods of solution. these meta-level concepts can be extracted from example solutions by precondition analysis. the new methods conjectured by precondition analysis can be shown to be correct using recursion guidance. the example solutions required by preconditions can be supplied by deduction with a less constraining search control. the problems to be solved by deduction can be supplied by the marples algorithm  but this technique requires deduction together with metalevel inference to bridge gaps between the knowledge it requires and that provided in the problem statement. the interactions are summarised in table 1.1 
　but proof analysis techniques cannot merely replace inductive inference techniques as learning processes. proof analysis techniques work only on single examples and this limits the amount of generalization that they can do unaided. for instance  we saw that back propagation was able to generalize a particular problem from cos x to cosnx  where n is an odd integer. 1 is generalized to n by considering the constraints forced by the particular sequence of rules used in the successful solution. however  a similar sequence will also integrate terms of the form sinnx where n is an odd integer; a rule for cos needs to be replaced by a similar rule for sin. to recognise this similarity and build a general proof plan for both cases requires inductive generalization   boswell 1 . alternatively  one might use analogical matching to recognise the similarity and a flexible proof plan application technique to apply the cos sequence to the sin problem. note how back propagation narrows down the search which would otherwise be involved in finding an analogous solution  but then uses analogical matching to further narrow its own search. therefore  techniques of proof analysis  inductive inference analogical matching and proof application need to work in concert to achieve maximum learning power. 

1. the interaction of learning techniques 
　in section 1 we described the importance of the description space in constraining the kinds of inductive inference that were possible  old concepts can only be generalized or specialized to new concepts that are contained in the description space. 
in section 1 we described how the techniques of back propagation and argument removal could be used to define new concepts and thus extend the description space in a principled way by adding a needed concept  e.g. odd integer  uniform conver-
1  not all these interactions have been implemented. faced with formulae a -  b' and b  -  c the resolution rule applies substitutions to either formula in an attempt to make the bs identical so that cut can be applied. unification  see section 1  will find the most general such substitution. up to renamings of variables this substitution is unique - a far cry from the infinite branching of undirected substitution. thus the merging of modus ponens and substitution controls the search implicit in the later rule by making its application subser-
vi ent to the former. 
　similar mergings of other deduction techniques have been suggested. for instance  some axioms have been built-in to the unification algorithm. in associative resolution   plotkin 1   the associative axiom for a function  f  is deleted from the set of axioms and built-in to the unification algorithm  i.e. it can be used in the attempt to match two expressions. this merging of unification and associativity has the effect of controlling the applications of associativity by making it subservient to unification. other examples are higher order unification   huet 1   which builds the axioms and rules of lambda calculus into the unification algorithm  and e-resolution   morris 1   which builds the 
equality axioms into the unification algorithm. 
　the advantage of such mergings is not just in the shrinking of the search space; they can also assist the application of proof plans by bringing the key steps of a proof to the top of the search space. a proof plan may identify a particular step as a key one  but it might take several minor steps to transform the problem into a state where this key step can be applied. for instance  the key step may be to resolve with a particular clause  but several applications of associativity may be required to allow the resolution to go through. these minor steps may create a combinatorial explosion of their own before the key step is reached. by making the minor steps subservient to the key one the key step can be applied first and the application of the minor ones can be controlled. 
　merging may also be applicable to non-deduction techniques. for instance  given a problem to solve  an analogical matcher might be able to find a similar solved problem in order to use its solution as a proof plan. sometimes a solved problem will match the given one in several different ways. rather than work through each way in turn  the match may be left incomplete until further instantion is required to continue with the proof plan application. thus the analogical matching will be made subservient to the plan application technique and its search thus controlled. 
1. lenat's am program 
　the am program   lenat 1    is an interesting experiment in the interaction of a number of techniques for finding examples  defining concept  making conjectures  etc. these are set in a framework of heuristic rules controlled by heuristic search. an evaluation function is used to decide what concepts to define or find examples of  what conjectures to make. am's performance is impressive; starting with some simple set-theoretic concepts  it defines some relatively complex and interesting concepts  e.g. prime numbers  and makes some interesting conjectures  e.g. the prime unique factorization theorem. it also defines a lot of uninteresting concepts and makes a lot of silly conjectures. 
　am has no theorem proving ability. its definitions and conjectures are not motivated by problems it is 
a. bundy 1 
trying to solve  faulty proofs it is trying to correct or successful solutions it is trying to generalise. its sense of direction comes entirely from its evaluation function which is guided by the patterns  coincidences  etc that it notices in its example finding. it would be interesting to link am's techniques to those outlined above to get a better directed process of mathematical discovery. this would involve separating the different techniques used in am and implementing a wider interaction of mathematical techniques. however  such a programme would not be easy. the techniques used by am are not clearly explained  are embedded in complicated lisp code  and are difficult to disentangle from the heuristic rules. 
1. the dream programme 
　in this paper i have advocated the simultaneous study of a number of different techniques for mathematical reasoning  especially how these techniques may be fitted together. i believe that problems associated with the individual reasoning techniques can often be solved by combining them together  and i gave a number of examples of this phenomenon in section 1 above. 
to realise these ideas i have instituted the dream 
 discovery and reasoning in mathematics  programme at the department of artificial intelligence at edinburgh university. this programme gives explicit recognition to an implicit programme of development of mathematical reasoning programs over a period of several years. figure 1 explains the relationship between the various programs built in our group during this period. each node is a program and each arc represents some historical dependence of the earlier program upon the later one. press  lp  


1 a. bundy 
ecological 	modelling 	statistics packages  respectively. 
- rut   plummer 1   is a rational reconstruction of bledsoe's natural deduction theorem prover   bledsoe 1   and voyeur   plummer 1  bundy 1   extends 
rut with the gazing technique described below. 
　our specific  short term objectives are to extend our existing reasoning techniques and invent new techniques in a variety of domains  and more centrally to investigate the interaction of: deduction  search control  proof analysis and inductive inference within a single reasoning system. the understanding gained from this investigation will be exhibited in a program for reasoning primarily in mathematics  but adaptable  we hope  to other forms of problem domain. 
niques for search control  proof analysis  inductive inference  matching  formula extraction  etc. in this paper i have outlined some of the most promising such techniques drawn from the work of my own group and from that of others. 
　i have given examples of the interactions of these techniques and shown how these interactions can solve problems which can appear insuperable if a technique is studied in isolation. this constitutes a strong argument for the simultaneous study of reasoning techniques; to see how the total can be more than the sum of the parts. 
　the dream project aims to conduct such a simultaneous study. some of the preliminary results are reported above together with our plans to incorporate several techniques within a single system. 

the core of the system will be the mt program  
 wallen 1   which consists of two parts 
- the object-language: a logic for expressing problems; and 
acknowledgements 
　i am grateful to robin boswell  steve owen  lincoln wallen and mike uschold for assistance with and/or feedback on this paper. 

the meta-language: a logic for expressing proof plans. 
mt uses a process of meta-level inference to analyse a conjecture  choose an appropriate proof plan and use it to guide the search for a proof. 
　the object-level deduction technique is based on bibel's connection calculus.  bibel 1 . the connection calculus is particularly suitable as a vehicle for proof plans as it does not demand that the conjecture be put in a normal form and the proof is constructed using a detailed analysrs of the conjecture. no new formulae need to be constructed during the proof. this makes it particularly easy to relate the original analysis of the conjecture to the proof plan and hence to the subsequent proof. we plan to design and implement several proof plans in the mt system. heuristics developed from natural deduction proofs can be readily translated into the connection calculus. in particular  we will try to implement within mt one such technique  developed in the group  called gazing   plummer & bundy 1 . gazing is a heuristic technique for controlling the expansion of non-logical definitions and the use of previously proved theorems during a proof attempt. 
　we plan to add to mt a learning component based on precondition analysis and other analytic learning techniques. this will analyse proofs using the metalevel concepts already embodied in the mt proof plans and use this analysis to modify the existing plans and/or build new plans. these plans will then be added to mt to improve its theorem proving ability. 
1. conclusion 
　the automation of mathematical reasoning involves not just techniques for deduction  but also tech