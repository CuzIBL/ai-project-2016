 
estimating the location of people using a network of sensors placed throughout an environment is a fundamental challenge in smart environments and ubiquitous computing. id-sensors such as infrared badges provide explicit object identity information but coarse location information while anonymous sensors such as laser range-finders provide accurate location information only. tracking using both sensor types simultaneously is an open research challenge. we present a novel approach to tracking multiple objects that combines the accuracy benefits of anonymous sensors and the identification certainty of id-sensors. rao-blackwellised particle filters are used to estimate object locations. each particle represents the association history between kalman filtered object tracks and observations. after using only anonymous sensors until id estimates are certain enough  id assignments are sampled as well resulting in a fully rao-blackwellised particle filter over both object tracks and id assignments. our approach was implemented and tested successfully using data collected in an indoor environment. 
1 introduction 
accurate and reliable tracking of people using sensors placed throughout an environment is a fundamental problem relevant to several research communities. knowing the locations of people is of critical importance for research investigating highlevel state estimation  plan recognition  and learning of human activity patterns for applications such as work flow enhancement and health monitoring. 
　over the years  many location estimation approaches have been introduced using sensors such as cameras  laser rangefinders  infrared and ultrasound sensors  and wireless networking infrastructure  hightower and bordello  1 . a crucial aspect of these sensors is whether they provide explicit information about the identity of a person. anonymous sensors such as radar  reflective ultrasound transducers  and scanning laser range-finders provide accurate location and appearance information  but do not provide explicit identity information. id-sensors like infrared and ultrasound badge systems do provide explicit object identity information  but with relatively coarse location information  want et al  1; priyantha et al.  1 . various techniques have been proposed for tracking with multiple anonymous sensors or multiple id-sensors  but the problem of integrating anonymous and id sensor information has not been addressed so far. in this paper we present an approach that combines the accuracy benefits of anonymous sensors with the identification certainty of id-sensors. 
　our approach uses rao-blackwellised particle filters to efficiently estimate the locations and identities of multiple objects. each particle represents a history of associations between object tracks and observations. for each particle  the individual objects are tracked using kalman filters. since the initial id uncertainty makes a sample-based representation of id assignments extremely inefficient  our approach starts by tracking objects using only anonymous sensors and efficiently representing estimates over object id's by keeping track of sufficient statistics. once the id estimates are certain enough  the approach switches to sampling id assignments as well resulting in a fully rao-blackwellised particle filter over both object tracks and id assignments. when applied to anonymous sensors only  our method results in a new raoblackwellised approach to multi-hypothesis tracking  which has gained substantial attention in the target tracking community  bar-shalom and li  1 . 
　this paper is organized as follows: section 1 clarifies the problem. section 1 then presents our rao-blackwellised particle filter approach to tracking multiple objects using only anonymous sensor information and section 1 extends the approach to incorporate id-sensors. our implementation and experimental results are presented in section 1  followed by a discussion. 
1 problem description 
figure 1 illustrates the problem of tracking multiple people with anonymous and id-sensors. the solid and dotted lines are the trajectories of person a and b  respectively. in the beginning  the identity of the two people is not known. as they walk  the anonymous sensor observes their locations frequently. since the people are far enough apart  their positions can be tracked reliably using the anonymous sensor. however  until they reach id-sensor areas 1 and 1  both trajectories have the same probability of belonging to either person a or b. hence there are two different hypotheses for the id's of the two 

perception 	1 


figure 1: example scenario: the shaded circles indicate areas covered by id-sensors such as infrared receivers. when a person wearing a badge enters such an area  the corresponding sensor issues a reading indicating the id of the person. since these sensors provide no information about the person's location within the area  two people in the same area can not be distinguished. not shown is an additional anonymous sensor such as a laser range-finder. this sensor provides accurate information at a high rate about the locations of people  but no information about their id's. 
trajectories. after passing through the coverage of id-sensors 1 and 1  the ambiguity is resolved and both trajectories' id's are determined. then  after the paths cross there is confusion about the continuation of the two tracks. when the people leave the light gray area the anonymous sensor can not determine which observations to associate with which trajectory. this problem is known as the data association problem in the multitarget tracking community  bar-shalom and li  1 . were there no id-sensors  it would be impossible to resolve this ambiguity. in our scenario  the ambiguity can be resolved as soon as the people reach the areas covered by id-sensors 1 and 1. to do so  however  it is necessary to maintain the hypotheses for both possible track continuations  a going down and b going up  or b going down and a going up. 
　the use of a combination of anonymous and id-sensors requires us to solve two types of data association problems. the first problem is the classic multitarget tracking problem of assigning anonymous observations to object tracks. this data association problem has to be solved at each point in time  resulting in possible associations for tracks of length  involving people. fortunately  the probability distribu-
tions over these anonymous assignments are typically highly peaked  thereby allowing an efficient  sample-based representation of assignments. the second problem is the one of estimating the id of individual objects/tracks. as with anonymous assignments there are possible assignments of id's to tracks. fortunately  the number of id assignments does not increase over time since the identity of objects does not change. however  due to the low spatial resolution of id-sensors  the posterior over id assignments is almost uniform during early stages of the estimation process  see figure 1 . for such  flat  distributions  a sample-based representation requires in the order of  samples  which is certainly not feasible for online tracking. 
　figure 1 shows the graphical model for this tracking problem. here  time is indexed by subscripts and the current time is denoted by are the current positions of the people being tracked. following standard notation in the tracking community  observations are denoted by 

figure 1: graphical model for multi-object tracking with anonymous and id-sensors. is the state vector describing the locations of the individual objects at time objects generate anonymous observations and id observations . the assignments of individual observations to objects are given by the hidden nodes and raoblackwellised particle filters sample assignments and and solve the state updates analytically  using kalman filters conditioned on the samples. 
　and the complete sequence of observations up to time is given by 	 we use hat and tilde to distinguish anonymous from id observations . the associations between object tracks and observations are given by assignment matrices and 
　　for instance  is one if  assigns anonymous observation to object and zero otherwise. the observations only depend on the current object positions and assignments. 
　the goal of tracking is to estimate the posterior over the state based on all sensor information available up to time 
　a factored representation of this tracking problem allows us to use rao-blackwellised particle filters  rbpf  which sample assignments and track the objects using a bank of m kalman filters for each sample  doucet et ai  1 . however  due to the large number of anonymous assignments and the  fiat  distributions over id assignments  a straightforward implementation of rbpfs would require a prohibitive number of samples. in the next section we describe how to efficiently sample anonymous assignments over time. the resulting algorithm  mht-rbpf  multi-hypothesis tracking rbpf   can track multiple objects using anonymous sensor information. then  in section section 1  we will describe how to extend mht-rbpf to incorporate id-sensors. 
1 mht-rbpf: rao-blackwellised particle filters for multi-hypothesis tracking 
since the assignments between observations and objects/tracks are not given  we need to estimate the posterior over both object states  and assignments  this joint posterior can be factorized by conditioning the state xk on the assignments 
 1  
　the key idea of rao-blackwellised particle filters is to compute  1  by sampling assignments from p and then 

1 	perception 

computing the state xk conditioned on each sample. more specifically  each sample represents a history of data associations and is annotated with a bank ofkalman filters  one for each tracked person. the kalman filters arc conditioned on the data associations provided by the sample and thus can be updated efficiently using standard kalman filter update rules for known data association  bar-shalom and li  1 . the kalman filter estimates  or tracks  
 are represented by the mean and covariance 
of the persons' locations. 
　rbpf generate assignments incrementally by maintaining sample sets containing assignment histories distributed according to the posterior given by the rightmost term in  1 . more specifically  at time a sample set contains tv weighted samples  where each sample consists of a history of assignments and 
the current position estimates for the objects. the generic rbpf algorithm generates a set from the previous sample set  and an observation by first generating new assignments distributed according to the posterior each such assignment specifies which observations in belong to which object track. the final step consists of updating the kalman filter tracks of each sample using the observations assigned to them by the sample  doucet et al  1 . 
1 	importance sampling with lookahead 
the efficiency of rbpfs strongly depends on the number of samples needed to represent the posterior  in 
this section we will devise an efficient algorithm for generating such assignments/samples. due to the sequential nature of the estimation process  samples must be generated from the assignments of the previous time step. the posterior at time k is given by 

here   1  follows from bayes rule  and  1  follows from  1  since the position tracks of the objects are sufficient statistics for the previous observations and assignments 

　unfortunately  in most cases it is impossible to sample directly from  1 . the approach most commonly used in particle filters is to evaluate  1  from right to left in a three stage process  doucet et al  1 : first  draw samples from the previous sample set using the importance weights  then draw 
for each such sample a new sample from the predictive distribution and finally weight these samples proportional to the observation likelihood the last step  importance sampling  adjusts tor the tact that samples are not drawn from the actual target distribution. this approach has two main sources of inefficiency. first  the samples of the previous sample set are distributed according to and not the desired 
the rightmost term of the target distribution  1   moralesmenendez et al  1 . the second source of inefficiency is that sampling from the predictive distribution in the second sampling step can be very inefficient if the observation likelihood is highly peaked compared to the predictive distribution  pitt and shephard  1 . the second problem is extremely severe in our context since the predictive distribution for assignments is virtually uniform while the posterior is typically concentrated on a small set of assignments  bar-shalom and li  1 . 
mcmc sampling from the optimal distribution 
let us first discuss how to address the second problem  i.e. the problem of drawing samples from the posterior distribution  1   given the previous sample set. conditioned on a specific assignmentthe optimal sampling distribution follows from  1  as 

it is possible to efficiently generate samples from this distribution using markov chain monte carlo  mcmc  techniques  gilks et al  1 . we apply a version of the metropolis-hastings algorithm that has been adapted specifically to the data association problem  dellaert et al  1 . here  the idea of metropolis-hastings is to sample states from an ergodic markov chain with the posterior as stationary distribution. such a markov chain is constructed by first choosing a candidate for the next state s' given the current state s according to a proposal distribution  this state transition is accepted with probability 
 1  
where  s  is the intended stationary distribution. in our case the states are the possible assignments and  .s  is the optimal sampling distribution  1 . the efficiency of the metropolis-hastings method strongly depends on the choice of the proposal distribution  we use an efficient approach called smart chain flipping. smart chain flipping permutes the assignments of a subset of the objects on each transition  where the actual choice already takes the individual assignment likelihoods into account. this approach has been shown to result in improved mixing rates on assignment problems  dellaert et al  1 . 
assignment lookahead 
as mentioned above  another source of inefficiency is the fact that the previous sample set does not consider the most recent observation. this problem can be greatly reduced by using the most recent observation to re-weight the samples of the previous sample set  pitt and shephard  1; 
morales-menendez et al  1 . the weight for a sample is given by dividing the target distribution 
     by the sampling distribution can be shown to result in 

perception 	1 

table 1: mht-rbpf algorithm. 

that is  the importance weight of a sample is given by the ability of the tracks associated with the sample to predict the next observation. accurate computation of the right term in  1  requires summation over all possible next assignments as done by  morales-menendez et al.  1 . since in our case the number of assignments can be prohibitively large  we estimate  1  using the samples generated in the mcmc step. unfortunately  computing  is equivalent to comput-
ing the normalization factor of a markov chain  which is not possible in general  gilks et al.  1 . in our case  however  we do not need the absolute value of the normalization constant for each chain  but only the value relative to the normalizes of the other markov chains. since the distributions  1  have similar shapes for all chains  they are highly peaked   we can estimate the relative normalization constants by the average probabilities of the samples in the different markov chains. 
more specifically  let m samples be drawn from each 
markov chain. let denote the th sample drawn from the markov chain associated with sample then the updated weight of this sample is given by 
 1  
where proportionality is such that all weights sum up to one. 
mht-rbpf algorithm 
the algorithm is summarized in table 1. in step 1  new samples are generated from the previous sample set. the average probability of these samples is used to estimate the lookahead/predictive weights of each sample of the previous set  step 1 . this step also involves a normalization so that the weights sum up to one. in step 1  a sample is drawn from the previous sample set then  for each sample drawn from  we draw an assignment from the posterior re-using the samples generated in the markov chains in step 1. step 1 updates the actual position estimates for the individual objects  using the corresponding assignment 
1 tracking with anonymous and id-sensors 
in principle  mht-rbpfs can be readily extended to include id-sensors. instead of only sampling anonymous assignments  it is possible to sample both anonymous and id assignments. such a straightforward extension  however  results in an infeasible increase in the number of samples needed during early stages of the estimation process. this has two reasons. first  each hypothesis  sample  of mht-rbpf has possible assignments of id's to object tracks. second  due to the low spatial resolution of id-sensors  the posterior over id assignments is initially very uncertain and sample-based representations of such flat distributions are inherently inefficient. to overcome these difficulties we instead use a two-stage estimation process. during the initial stage  only anonymous sensors are used for object tracking while the id-sensors are simply used to estimate the identity of the different objects. once these estimates are certain enough  the process moves into the full raoblackwellisation phase  during which both anonymous and id assignments are sampled. the two phases are discussed below. 
identity estimation phase 
during this phase only anonymous sensors are used to track the objects. the id-sensors are used to estimate the identity of the different objects. more specifically  for each hypothesis of the mht-rbpf  there are  possible assignments  of identities to tracks. in order to avoid estimating distributions over this potentially too large number of assignments  we only keep track of sufficient statistics that allow us to recover distributions over assignments. such sufficient statistics are given by the individual assignments of id's  to tracks 
   the probabilities of these individual assignments can be updated recursively using the most recent id observation: 
	 1  
here  is an id observation corresponding to person to determine the assignment probabilities  we have to normalize these values by considering all possible assignments: 
		 1  

again  is one ifassigns id to object track 
and zero otherwise. the computational complex computation of  1  can be avoided by sampling id assignments using metropolis-hastings  based on the individual values this approach works identical to the method used to sample anonymous assignments in the mht-rbpf algorithm. 
　to summarize  in the identity estimation phase  each sample consists of kalman filters and an matrix 
representing the sufficient statistics   of the id assignments. whenever needed  the posterior over id assignments can be computed using mcmc sampling. the id estimation stage is ended as soon as the posterior over id assignments is sufficiently peaked to allow an accurate representation with a reasonable number of samples. currently  we estimate this condition by determining the average number of hypotheses in the markov chains run for id assignments. 
full rao-blackwcllisation phase 
once the id's are estimated accurately enough  we begin estimating the joint posterior of both anonymous and id assignments. this posterior is given by: 

note that id assignments are sampled only once during the complete estimation process  since after an id assignment is sampled  the identities of the attached object tracks arc fixed. from then on  id observations serve two purposes: first they provide information of object positions  and  second  they provide information for weighting anonymous hypotheses  thereby improving the object estimates considerably. 
1 experiments 
to validate our approach we captured trace logs of six people simultaneously walking around the cubical areas of the office environment outlined in figure 1. each person was wearing a small id-sensor track-pack consisting of two infrared badges and an ultrasound badge. infrared and ultrasound receivers were installed throughout the ceiling. the entire scene was continually observed by two wall-mounted laser range-finders scanning at chest height just above the cubical partitions. the duration of the log was 1 minutes  during which the individual people moved between 1 meter and 1 meter. in this challenging data log  the paths of people frequently crossed each other and there were situations in which up to 1 people were occluded by others. to validate our tracking algorithm quantitatively  we carried out a series of experiments based on this data log and on additional simulation runs  see www.cs.washington.edu/robotics/peopletracking/ for visualizations . 
tracking ability 
the path of the six people as estimated by our system is shown in figure 1. this result was obtained with an mht-rbpf using 1 samples and markov chains of length 1. with this setting  the algorithm was able to reliably track the six people if no lookahead was used. after determining these parameters  we carried out trial runs for different variants of our algorithm. a run was considered successful if at the end the 
perception 

figure 1: outline of the intel research lab seattle. the environment is equipped with ceiling mounted ultrasound and infrared receivers. cubical partitions are half-height  about 1 meters high . the two laser range-finders scan at chest height just above the partitions. also shown are the paths of the six people as estimated by our system. 
sample set contained at least one hypothesis with correct locations and identities. each method was tested on 1 trial runs using the real data set. without identity estimation phase  the algorithm was never able to successfully complete the data set. next  we tested the approach when not switching to the full rao-black well isation phase  i.e. the algorithm remained in the identity estimation phase. without lookahead  the update times of the approach were prohibitively large  more than 1 times real time . the lookahead resulted in significant speedup and the results are shown in the first row of the table below. finally  we tested our two-phase approach with and without lookahead. the algorithms were able to successfully track the complete data in most cases  as can be seen in the first column. the other three columns give average time  standard deviation  and maximum time per update in seconds. the results demonstrate that both the lookahead and the two-stage process improve the performance of the tracking algorithm. 
method succ. avg. std. max. id estimation w. look. 
two-phase rbpf 
two-phase rbpf w. look. 1 
1 
1 1 
1 
1 1 
1 
1 1 
1 
1 　in another set of experiments  we compared our mcmc based assignment generation to a deterministic sampling scheme  as used in traditional mht algorithms. here  assignments are enumerated in decreasing order wrt. their likelihood  cox and hingorani  1 . note that generating assignments by decreasing likelihood results in estimates that are strongly biased towards more likely assignments. we performed extensive tests using real data and data simulating 1 people and found that our approach is slightly more efficient while achieving the same robustness. since such results depend on implementational details  we did not consider them significant enough. however  the fact that smart chain flipping works at least as good as ranked assignments in practice is very encouraging  since the mcmc approach results in much less biased estimates. 
1 


figure 1: optimal rbpf sample weights plotted against the sample weights computed using relative mcmc normalizers. 
mcmc weight estimation 
in section 1  we introduced an apporach to using the most recent observation in order to weight samples before drawing them for the next time step. so far it is not clear whether this estimation based on the probabilities of samples drawn from the markov chains is a valid approximation. to test the quality of this approximation  we computed the optimal predictive weights by enumerating all possible assignments of the next time step. figure 1 shows the mcmc sample weights computed in  1  plotted against the optimal weights. the similarity to y ~ x suggests that our method of estimating markov chain normalizers results in accurate weight estimates. 
1 conclusions and future work 
we have presented a solution to the problem of tracking multiple people using a combination of anonymous and id sensors. the approach inherits the advantages of both sensor types  thereby being able to accurately track people and estimate their identity. our technique uses rao-blackwellised particle filters to make the estimation problem tractable. we introduced several improvements to the vanilla particle filter. first  the estimation process is separated into two stages  a first stage of identity estimation and a second stage of full raoblackwellisation. a second improvement is in using the most recent observation before sampling from the previous sample set. in contrast to  morales-menendez et ai  1   the state space of our problem can become too large to allow an accurate estimate of the predictive quality of samples. therefore  we estimate this quality using markov chains generating samples distributed according to the posterior. we demonstrate the robustness of our approach in a challenging experiment involving six people walking through a confined office environment. we also show that our mcmc prediction is an accurate approximation of the optimal weighting function. 
　the approach introduced in this paper is just the first step towards a reliable and efficient tracking system. currently  the transition between the identity estimation stage and the full rao-blackwellisation stage is based on a simple heuristic  namely the average number of different assignments in the markov chains. we intend to replace this measure by a more fundamental approach such as the overall complexity of the distribution  fox  1 . another source of potential improvement lies in the handling of hypotheses. in our current system  the number of hypotheses grows extremely fast whenever several people are close to each other. we intend to overcome this 
1 
problem by clustering people into groups for which we do not attempt to estimate the individual id's  as already introduced in a similar context by  rosencrantz et ai  1 . finally  the recovery from tracking failures is another important issue for future research. especially in the full rao-blackwellisation phase  the current approach can not recover from losing the correct id hypothesis  since id assignments do not change over time. 
acknowledgments 
this work has partly been supported by the national 
science foundation under grant number i1s-1  and by darpa's sdr programme  contract number nbchc1 . we are also very grateful for the help of various people at the intel research lab seattle for collecting the data needed for our experimental evaluation. 
