 
we show how node ordering can be combined with parallel window search to quickly find a nearly optimal solution to single-agent problems. first  we show how node ordering by maximum g among nodes with equal / = g + h values can improve the performance of ida*. we then consider a window search where different processes perform ida* simultaneously on the same problem but with different cost thresholds. finally  we combine the two ideas to produce a parallel window search algorithm in which node ordering information is shared among the different processes. parallel window search can be used to find a nearly optimal solution quickly  improve the solution until it is optimal  and then finally guarantee optimality  depending on the amount of time available. 
1 	introduction and overview 
heuristic search is a fundamental problem-solving method in artificial intelligence. common examples of single-agent search problems are the eight puzzle and its larger relative the fifteen puzzle. the eight puzzle consists of a 1 square frame containing 1 numbered square tiles and an empty position called the 'blank'. the legal operators slide any tile horizontally or vertically adjacent to the blank into the blank position. the task is to rearrange the tiles from some random initial configuration into a particular desired goal configuration. a real world 
   *this research was supported by an nsf presidential young investigator award to the second author  nsf grant iri-1. chris ferguson helped derive the limitation of pure window search and produced the figure. bob felderman and othar hansson's discussions and draft reviews substantially improved the paper. discussions with andy mayer also contributed. much of this work was done on machines provided by an equipment grant to ucla from hewlettpackard. earlier work was done on an intel hypercube. 
1 	tools 
example of single-agent search is the traveling salesman problem of finding the shortest simply connected tour among a set of cities to be visited. 
　an optimal-solution algorithm for single-agent heuristic search is a*  hart  1 . for each node n  the cost of a path from the initial state to node n  g n   is added to the estimated cost of reaching a goal from node n  h n   to arrive at an estimate of the cost of a path from the initial state to a goal state that passes through node n  f n . a* works by always expanding next a node of minimum / n  = g n  + h n  until a goal node is chosen for expansion. the solution length found by a* is guaranteed to be optimal  lowest cost  if the heuristic function never over-estimates the cost of the cheapest path to the goal. in practice  however  a* is not practical because it requires an exponential amount of memory. this limitation is overcome by an algorithm called iterative-deepcning-a*  ida*   korf  1 . 
　ida* works by iteratively searching in a depth-first manner. in each iteration  a branch is cut off when the f n  value of the last node on the path exceeds a cost threshold for that iteration. the threshold for the first iteration is set at the heuristic value of the initial state  and each succeeding threshold is set at the minimum / value that exceeded the previous threshold. successive iterations continue until a goal node is chosen for expansion. since at any point it is performing a depth-first search  ida*'s memory requirement is only linear in the solution length  but it guarantees optimal solutions in every case that a* does. 
　in this paper  we discuss how global node ordering by minimum h among nodes with equal / values can reduce the time complexity of serial ida* by reducing the time of the last iteration. this approach  however  is limited by the time to perform the iterations prior to the final iteration. we then discuss the notion of pure parallel window search in which different processors look to different thresholds simultaneously. this approach  however  is limited by the time to perform the final iteration. finally  we show how the combination of node ordering with pure parallel window search retains the 

good qualities of both approaches while ameliorating the bad. after describing our implementation of an ordered parallel window search  we analyze the performance of parallel window search with perfect ordering  and then present empirical data showing not only high speed  but high solution quality of the first solution found. 
　for a more detailed analysis of parallel window search  the reader is referred to  powley  1 . 
1 	node ordering 
an obvious strategy for reducing the time of the last iteration is to order the children generated in the depth-first search in the hope of reducing the nodes generated before a goal is reached. given a perfect ordering scheme  the goal iteration would immediately choose a goal node for expansion. in this case  the time complexity of the last iteration would be reduced from exponential to linear in the depth of solution. 
　the simplest type of node ordering is to explore the children of a node in increasing order of their static heuristic values  h n . experiments with this node ordering show that the improvement that results does not compensate for the additional overhead incurred by the ordering. 
　a more sophisticated form of node ordering is to order all nodes that are candidates for expansion  not just the children of a particular node. since a search frontier typically consists of a set of nodes with equal / values  we order the nodes in increasing order of h  or equivalently in decreasing order of g. this is beneficial for two reasons. 
1 	advantages of node ordering 
the main intuition behind this scheme is that we expect smaller h values to be more accurate. consider the analogy of having your car fixed: you would feel somewhat confident in your car being ready the day after tomorrow if it has already been in the shop for 1 days  g  and the mechanic says it will be ready in 1 days  h ; however  you would feel less confident in the car being ready in 1 days  /i   having just brought it in the day before yesterday  g = 1 . this tie-breaking rule among nodes of equal / value is probably well-known  but we know of no work that has studied its effect. 
　if we think of nodes in the search tree being explored in a 'left-to-right' manner  our primary motivation for using node ordering is to shift the first goal found to the left. however  in addition to this beneficial shift effect  node ordering also reduces the time to find a goal 
through the depth effect. 
　for each node expanded on the final iteration that does not lead to a goal within the threshold  a subtree of nodes must be explored to verify that fact. by picking a frontier node from the next-to-last iteration of minimum h  we reduce the average size of the subtree beneath it. this is because the maximum depth we can go below a non-goal node without a change in / is limited by the magnitude of h. in order for / to stay constant  as g  depth  increases  h must correspondingly decrease. the amount that h can decrease without reaching a goal  however  is limited by its starting value  since if h decreases to 1 it indicates a goal node. thus  nodes with smaller h values will tend to have smaller subtrees under them within a given iteration. as a result  even if node ordering did not shift the goal to the left  the nodes explored to find the goal would still be reduced by the depth effect. 
　the shift and depth effects thus combine to reduce the number of nodes that must be explored unsuccessfully until a goal is found. this is analogous to the problem of searching for buried treasure on an island given the probability of finding treasure in each location as well as the cost required to dig in each location  simon  1 . the goal is to maximize the treasure found and minimize the cost to find it. thus it is not only important to find a correct location quickly  but also to minimize the time spent digging empty holes. similarly  in a state space search  nodes should be ordered so that the goal is located under one of the first few nodes checked  shift effect   while at the same time minimizing the nodes explored under incorrect choices  depth effect . fortuitously  in this case the same ordering maximizes both effects. 
1 	limitation of serial i d a * node ordering 
even though good node ordering in ida* may dramatically decrease the time spent in the last iteration  it can have no effect on the previous iterations. the reason is that if a goal is not found  the entire iteration must be completed. if we consider nodes to be explored in a 'left-to-right' manner  the best possible improvement occurs when the first goal found is the rightmost node in the unordered case and the leftmost node in the ordered case. then the unordered search explores the entire goal iteration frontier  whereas the ordered search explores a single path during the goal iteration. this best case results in a 1/b reduction of the nodes generated in the unordered case  powley  1   where b is the heuristic branching factor. 1 is formally defined to be limd-oo y/t d   where t d  is the number of frontier nodes at threshold d. 
　note that even with perfect ordering  in problem instances where the unordered search happens to have a leftmost goal  there is no improvement with perfect ordering. in general  the goal in the unordered case will be some percentage of the way across the frontier  so perfect ordering would produce less than a factor of 1 improvement on the average. 
	powley and korf 	1 

1 	implementation of node ordering 
the ideal ordering scheme is to fully exploit the information available from the next-to-the-last iteration. this can be done by saving the entire frontier of nodes from the most recent iteration  and then ordering them for expansion by maximum g. we collected data from 1-puzzle problem instances selected from  korf  1   and compared the nodes generated in the ordered and unordered cases. on the average  the ordered case generated only 1% of the nodes in the final iteration that were generated by the unordered case. unfortunately  this approach to ordering is impractical since it must store the complete next-to-last iteration  requiring exponential memory and inordinate ordering overhead. it is of interest  however  because it reflects the best that can be achieved using all available information. 
   in our approach  we instead save an earlier frontier and dynamically reorder it based on later iterations. for each node in the saved frontier set  the deepest path achieved in searching under it is recorded. the saved frontier nodes are searched in decreasing order of the depth of this path  which corresponds to minimum h . this approach is used in our parallel implementation of ordering and is described in more detail in section 1. 
   to evaluate node ordering empirically  we compared unordered ida* with ordered ida* using the ordering scheme just discussed. for 1 problems of moderate difficulty selected from  korf  1   the average ratio of nodes generated in the unordered case to the nodes generated in the ordered case was 1. though almost a factor of two  this improvement is not of practical use because it is mitigated by the overhead associated with ordering. our ordering program runs about 1% slower than our serial ida* program  1 million versus 1 million node expansions per minute on a hewlett packard 1 workstation   resulting in no real-time improvement. however  this technique might still be worthwhile with a better ordering scheme or for problem domains with a higher branching factor. 
   in the next section we discuss a complementary approach to improving ida* which can overcome the limitations of serial ida* ordering. 
1 	pure parallel window search  pws  
the idea of parallel window search is to use different processes to search to different thresholds  windows  simultaneously  hoping that one of them will find a solution. because thresholds are not explored sequentially  the first solution found may not be optimal. optimality can still be guaranteed  however  by completing all shallower thresholds than that of the best goal found. the notion of window search originated in two-player game search and has been considered at length in that application  baudet  1    kumar  1   but until now it has not been applied to single-agent search. in game 
1 	tools 
tree search  the approach is due to baudet  and is called parallel aspiration search. the two approaches  though related by the concept of parallel windows  are fundamentally different. in single-agent search  having each process search to a different threshold is equivalent to having each of them perform a separate iteration of ida*  except that some may go beyond the goal iteration. 
　unfortunately  this approach by itself produces limited speedup. the reason is that the search time will still be dominated by the time to perform the last iteration  even if the others are performed in parallel. in the next section  we discuss the expected improvement due to pure parallel window search. 
1 	limitation of pure p w s 
for the moment  assume that pure pws finds only an optimal solution. what is the best improvement in elapsed time relative to ida* that we can achieve  assume there are enough processes so that the optimal solution threshold is being searched. expected speedup can be analyzed as a function of goal location. the analysis  powley  1  shows that speedup relative to ida* is approximately 

where 1 is the heuristic branching factor  and a is the fraction of the frontier nodes that must be searched to find the first goal. in the extreme case of a rightmost goal  a = 1 and this reduces to b/b-1 this represents the lowest possible speedup of pure parallel window search. for example  if 1 is 1  the minimum speedup is 1. if the goal is midway  a = 1 and speedup = 1 + 1/b-1  or 1. as a approaches 1 speedup increases and then approaches infinity. this is because the time for window search to find a leftmost goal is insignificant compared to the time for ida* to find a leftmost goal. the expected value of a will depend on the average number of goals  but in general it will not be low enough to produce large speedups. hence  the time to search the goal iteration will limit the speedup of pure parallel window search. 
1 	the density effect 
in the previous section we only considered the speedup to find an optimal solution. now we consider whether a non-optimal solution might be found before an optimal solution. consider two identical processors  po and p1. po searches to the optimal solution threshold and p1 searches one threshold past optimal. if the goal of p1 is located the same fraction of the way across its frontier as the goal of po  that is if ao = a1  where a is as defined in the previous section   then po will find a solution first because p1 must explore a factor of b times more frontier nodes to find the goal than po  where 1 
is the heuristic branching factor. however  if pi's goal is shifted to the left  again exploring in a 'left-to-right' manner  so that a1 is reduced to less than a1/b  then the non-optimal solution will be found first. 
　in general  there may be many solutions  both optimal and non-optimal. if the average non-optimal solution density is greater than the average optimal solution density  we would expect to find a non-optimal solution first. average solution density is a function of the problem domain. in the 1 and 1 puzzle problems  for example  the average non-optimal solution density appears to be higher than the optimal solution density  at least for several thresholds past optimal. in the 1 puzzle  for 1 random problem instances  we measured the average ratio of non-optimal goal density to optimal goal density for each of the first 1 thresholds past optimal. for thresholds 1  1  1  1  and 1 past optimal  the ratios were 1  1  1  1  and 1  respectively. 
　because of the size of the search space  we were unable to get similar data for the 1-puzzle  but we believe the corresponding numbers for the 1-puzzle are even higher because almost all the initial solutions found by our implementation of pws are non-optimal. 
　thus  solution density also affects the time for parallel window search to find its first solution. in problem domains which have relatively high non-optimal solution density  this effect will increase the average speedup possible from window search. 
1 	parallel window search with node ordering 
interestingly  the limitations and strengths of node ordering and pure window search are completely complementary. node ordering is limited by the time to perform the non-goal iterations but performs the final iteration very efficiently. conversely  pure parallel window search is limited by the final iteration and incurs no additional cost for the previous iterations. this suggests that a combination of the two approaches might be effective. we combine pure parallel window search with node ordering and refer to the combination as simply parallel window search. 
1 	parallel window search algorithm 
given p processes  our implementation works as follows. at the start  each process expands the root node to a relatively small  fixed frontier set  on the order of 1 nodes; all processes have an identical fixed frontier set 
 ffs . each process is assigned a different one of the first p thresholds  windows  to search. a process chooses a node from its ffs and does a complete search of it to the assigned threshold. after completing the search of an ffs node  the process records the minimum h value of all leaf nodes generated in searching the ffs node; additionally  the path from the ffs node to the minimum h leaf node is also recorded. this information is used later for ordering. then the process selects another unsearched ffs node and searches it. 
　when the entire ffs has been searched  the process broadcasts ordering information to all other processes; the message consists of the following:  1  a minimum h value for each of the ffs nodes  and the associated path  and  1  the value of the threshold on which the ordering information is based. the process also saves a copy of the ordering information for itself. then the process 'leapfrogs over' the other processes to the next threshold to be searched. when more than one ordering message is received by a process  only the message associated with the deepest threshold is saved. a process orders its search by picking unsearched ffs nodes of minimum associated h value. the process searches first beneath the saved path  and then searches the rest of the ffs nodes. 
　at some point  a process will find a solution. after finding a first solution  processes search for better solutions by searching shallower thresholds than that of the best solution found. this continues until an optimal solution is found and then verified. verification of optimally requires completing all thresholds less than optimal; thus the time between finding an optimal solution and verifying its optimality can be large. 
　the overall behavior of the program  then  is to find a solution quickly  improve it until optimal  and then guarantee optimality. at program completion pws exits with a verified optimal solution  just eus ida* does. the overall difference is that pws finds good solutions quickly in the course of computing the optimal solution. 
1 	analysis of pws algorithm w i t h perfect ordering 
what is the minimum time required by our pws algorithm on the average to find a solution in the case of perfect ordering  we make the following assumptions:  1  the number of solutions is not exponential at any depth  if it were  we could find a solution in linear time    1  ordering information from completed thresholds is perfect   1  ordering information below completed thresholds is random  and  1  when a process receives new ordering information it restarts its search using the new information. let d be the threshold at which the first solution is found by any processor  or alternatively  the length of the first solution found. d may or may not be optimal. 
　now consider the process that searches to threshold d and finds the first solution. when it finds the solution  it will be using ordering information from some threshold d.  see figure 1 . furthermore  since ordering information is perfect  it will find the solution under the first frontier node of threshold d  call this node x. however  since it has no further ordering information  its search 
	powley and korf 	1 
below node x is randomly ordered  assumption 1 . thus  on the average it will have to search 1 bd-d   nodes below node x to find the goal. 

   because a process is interrupted and restarts when it receives new ordering information  each process searching to a depth d will be interrupted by all processes searching to a shallower depth. this implies that the time to search to depth d is the sum of the times to search to each shallower depth. since the tree grows exponentially  this has no effect on the asymptotic time complexity  which is still 1 bd . 
   how long does it take to find a solution under these assumptions  the total time is the time for the ordering process to complete its search to depth d  plus the time for the solution process to complete the search below node x  which is of depth d-d. this is 1 bd+bd-d . for small values of d  the running time is dominated by the time to perform the unordered search below depth d. for large values of d  the running time is dominated by the time to determine the ordering information. note that ordering information doesn't help the ordering process  since the entire ordering threshold must be completed. the running time is minimized when d = d/1  which balances the two searches. this results in an overall complexity of 1 bdl1  in the best case. 
　in the above analysis  we defined d as the length of the first solution found. if we changed the definition of d to be the length of an optimal solution  then the same result applies to finding optimal solutions. 
1.1 	n u m b e r of processes 
   consider the minimum number of processes required to achieve this 1 bdl1  time complexity. all we need is enough processes so that the solution process can start as soon as the ordering process completes  or sooner. this is achieved with a minimum number of processes when the ordering process leapfrogs directly to become 
1 	tools 
the solution process; this requires d/1 processes. thus  d/1 processes are sufficient to find a solution in 1 bd/1  time. note that having too few processes will delay the availability of a process for the solution threshold and thus increase the time to find a solution. 
　what is the effect of having more than the minimum number of required processes  let d be the length of the optimal solution and consider what happens as we add more than d/1 processes. as discussed in section 1  if non-optimal solution density is greater than optimal solution density  we would find a non-optimal solution first and reduce the time to the first solution. on the other hand  if we add processes which search thresholds with relatively low non-optimal solution densities  then these processes will be unproductive since shallower solutions will generally be found first. this implies that  depending on the problem domain  we may want to limit the number of windows for efficiency. 
　even if the benefit of adding deeper windows decreases  however  parallel window search is not limited in how many processors it can effectively use. once the desired window 'length' has been achieved  extra processors can be used to share in the search of each window. that is  more than one processor can search to a given threshold  for example using the approach of  rao  1 . 
1 	e m p i r i c a l results for first solution found 
we ran the initial 1 of 1 problems of  korf  1   ordered by nodes generated by serial ida*  using a number of processors ranging from 1 to 1  and measured the time to find the first solution. the 1 most difficult problems weren't used because they would take too long to solve using 1 or 1 processors since this number is less than the minimum required as discussed in the previous section. all 1 problems  however were run using 1 processors with consistent results. in each case  we calculated the average effective heuristic branching factor. the effective branching factor is one way of measuring the reduction in nodes generated. it tells how much the branching factor would have to be reduced in the serial search to find a solution in the time taken by the parallel search. since the serial search requires 1 bd  time and the parallel search in the best case requires 1 bd'1  = 1   = 1  1   / d    we expect an effective heuristic branching factor of at least 1. 
1.1 	r u n n i n g t i m e relative t o i d a * 
　for 1  1  1  1  and 1 processors  the corresponding average exponents of 1 were .1  .1  .1  .1  and .1  respectively. that is  we see a reduction of the heuristic branching factor to about b 1 / 1 . we believe 1 monotonically decreases for two reasons:  1  for the most difficult problems  adding extra processes helps achieve the minimum number of processes required  section 1.1 ; and  1  adding additional processes reduces the time to a solution because of the density effect  section 1 . the 

consistency of this reduction lends support to the claim that parallel window search reduces the exponential complexity of ida* for finding non-optimal solutions. 
　to give an idea of specific performance  when we ran all 1 problems using seven hewlett packard 1 workstations  the average nodes generated in finding a solution was 1 million compared to an average of 1 million nodes generated in ida*. more specifically  the most difficult problem ida* solved required 1 hours and 1 billion node generations. using 1 processors  our program found a solution to this problem that was 1 moves from optimal  1%  in 1 minutes and 1 million node generations  by the process finding the solution . if total processor effort is considered and not just elapsed real time  this corresponds to 1 minutes and 1 million nodes generated; this is the time a single processor running pws with 1 processes would take. 
1.1 	solution quality 
　the relative speed of the program is only one measure of its performance  with the quality of solutions being the other primary measure. for seven processors  the solution lengths range from optimal to twelve moves over optimal  with the mode and average being six moves over optimal. since the average optimal solution length for this puzzle is 1 moves  the first solutions found are on the average within 1 percent of optimal. 
　to get a better indication of the strength of pws in finding non-optimal solutions  we also compared it to rta*  korf  1   a real-time variant of a* in which optimality is sacrificed in order to make real-time moves within fixed time limits. in a sense  this is an unfair comparison since rta* only searches from its currentposition in the sequence of moves made so far  but it is still instructive. 
　we compared the quality of solutions of pws with 1 processors and rta* as follows. rta* was run with varying search horizons until it generated more nodes in finding a solution than all 1 processors of pws. after removing any cycles from the solution  its length was compared to the pws solution length. on the average the pws solution lengths were half  1%  of the rta* solution lengths. in only three cases was the rta* solution shorter than the initial pws solution  and in those cases it was 1 moves less in one case and 1 moves less in the other two cases. these results show that pws is not dominated by either ida* or rta*  and thus it appears to be a competitive algorithm in terms of search effort versus solution quality. 
1 	conclusions 
the effectiveness of node ordering for improving the efficiency of ida* is limited by the time to complete the previous iterations of the search. conversely  window search in which different processes simultaneously perform different iterations  is limited by the time to complete the final iteration. combining effective node ordering with window search by sharing ordering information among processes makes it possible to find nearly optimal solutions quickly. 
　if the algorithm continues to run past the first solution found  it finds increasingly better solutions until an optimal solution is guaranteed or the available time is exhausted. such behavior is important in real problems since:  1  a quick solution is often more important than an optimal one  and  1  problem solvers often have limited resources available to make a decision  and often cannot predict those resources apriori. in those situations  a problem solver must always have a plausible solution available. parallel window search provides such a capability. 
