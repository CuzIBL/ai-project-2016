 
constraint satisfaction problems involve finding values for problem variables that satisfy constraints on what combinations of values are permitted. they have applications in many areas of artificial intelligence  from planning to natural language understanding. a new method is proposed for decomposing constraint satisfaction problems using inferred disjunctive constraints. the decomposition reduces the size of the problem. some solutions may be lost in the process  but not all. the decomposition supports an algorithm that exhibits superior performance. analytical and experimental evidence suggests that the algorithm can take advantage of local weak spots in globally hard problems. 
1 introduction 
constraint satisfaction problems  csps  involve finding values for a set of problem variables consistent with a set of constraints on which combinations of values are permitted. they arise often in artificial intelligence  in fields ranging from temporal reasoning to machine vision. 
　this paper proposes a new technique that utilizes inferred disjunctive constraints to guide decomposition of a csp. the decomposition reduces the size of the problem. the technique was used in an algorithm that achieved up to an order of magnitude improvement on difficult problems when compared with one of the most successful algorithms in the literature. the advantage of the new technique often increased as problem difficulty increased. the technique appears particularly well suited to taking advantage of local  weak spots  in globally difficult problems. 
　the following observation expresses the key idea: given a solvable problem and a value v for a variable v  if there is no solution involving v  then there must be a solution involving a value inconsistent with v. 
　the basic problem decomposition can be illustrated with a simple example. suppose we have a problem where one of the variables  x  is constrained by one other variable y. 
suppose value a for x is not consistent with values b and 
    this material is based on work supported by the national science foundation under grant no. lri-1. 
1 	constraint satisfaction problems c for y  i.e. the combination  a b  and the combination  a c  are not allowed  but a is consistent with all other values. suppose further that the domain of every variable  the set of possible values  is d; so a is consistent with d-{b c} for y. 
　we claim that we can divide and conquer by reducing the problem to two subproblems. one subproblem will be the same as the original except the domain of x will be {a} and the domain of y will be d-{b c}. the other subproblem will be the same as the original except the domain of x will be d- a} and the domain of y will be  b c . notice that we are pruning away a subproblem  where the domain of x is d-{a  and the domain of y is d-{b c   that may contain solutions. however  we shall see that we can guarantee that not all solutions are pruned away. 
　section 1 further develops the basic decomposition technique  and proves that it will not lose all solutions. section 1 embeds this technique in an algorithm for solving 
csps. section 1 adds some important refinements to the decomposition and the algorithm. section 1 investigates the potential of the technique analytically  section 1 experimentally. 
1 decomposition 
we will assume in this paper that we are dealing with binary csps  where constraints involve two variables  which we will say share the constraint. in a constraint graph a binary csp is represented with vertices for variables and edges for constraints. if variables share a constraint the corresponding vertices share an edge in the constraint graph  and thus the variables are called neighbors. 
　the weak inferred disjunctive constraint  weak idc  decomposition of problem p around value v for variable v consists of the following subproblems: the first subproblem  sv  is the same as p except the domain of v is restricted to {v} and the domains of the other variables are restricted to values consistent with v. then  for each variable vi that is a neighbor of v  we form a subproblem  s v i   that will be the same as p except that the domain of v will be its domain in p minus v  and the domain of vi will consist of those values inconsistent with v. 
　we will call the first subproblem  the precluded subproblem  because values are deleted using a preclusion 

process  golumb and baumert  1   and the rest the neighbor subproblems. we will call a subproblem empty if any of its variable domains is empty. 
　this decomposition generalizes the basic insight presented in section 1. it derives from the inferred disjunctive constraint: if there is a solution  either there is a solution containing v  or there is one not containing v  but containing a value inconsistent with v. 
　theorem /: if a problem p has a solution  given a weak inferred disjunctive constraint decomposition of p around any value v for any variable v  one of the subproblems will have a solution that is a solution to p. 
　proof: either p has a solution involving v for v  or it does not. if it does  that solution clearly will be a solution of the subproblem that contains v for v.  the removal of values inconsistent with v does not remove any solutions.  if it does not  then any solution must contain at least one value inconsistent with v for v. let us say that for a solution s the inconsistent value is v' for variable v'. the subproblem sv will contain the solution s.   
　note that there may be solutions to p that are not solutions to any subproblem. such solutions would neither contain v nor any values inconsistent with v. however  this does not matter if we do not require all the solutions. 
1 algorithm 
the decomposition process can be imbedded in a csp algorithm. we start with a very basic implementation. 
weak idc algorithm: 
stack  -  initial problem  empty solution  until stack empty: 
pop  problem  solution  from stack 
if problem has only one variable  u then exit with solution plus value s  of u 
else 
decompose around first value  v  in first variable  v 
for each non-empty neighbor subproblem  n: push  n  solution  onto stack 
if the precluded subproblem  sv  is not empty then 
sv  - sv minus v 
push  sv  solution plus v  onto stack 
this algorithm conducts a depth-first search through a 
 decomposition tree . the root is the original problem. the children of a node are produced by decomposing around the first value  v  in the list of potential values for the first variable  v  in the list of variables for the parent problem. the precluded subproblem is reduced further by eliminating v. 
　children are always smaller than parents  with fewer values   thus each branch of the tree terminates  assuming finite domains . leaf nodes are identifiable as solvable  they involve only one variable  and any value for this variable can be combined with the stored solution to form a complete solution to the problem  or unsolvable  they produce no non-empty children . theorem 1 ensures that a solvable problem cannot produce only unsolvable leaf nodes. 
　the algorithm can be viewed as a combination of the standard forward checking algorithm  haralick and elliott  1   and the decomposition process. the local inconsistency removal in the first subproblem corresponds to a forward checking step. 
1 refinements 
1 further domain reduction 
consider the decomposition of p around v for v. in svi  for each neighbor v i   we can reduce the domain of vj  for all j i  to the set of values consistent with v. we call the this decomposition that uses this further reduction the inferred disjunctive constraint  idc  decomposition. we call the set of neighbor subproblems after this further reduction the excised subproblems. 
　theorem 1: if a problem p has a solution  given an idc decomposition of p around any value v for any variable v  one of the subproblems will have a solution that is a solution to p. 
proof: sv1  which remains unchanged from the weak 
idc decomposition  considers all the possibilities that exclude v and include values for v1 inconsistent with v. thus values for v1 inconsistent with v can be ignored in 
svk for k l. sv1 where the domain of v1 is now reduced to the values consistent with v  considers all the possibilities that exclude v and include values for v1 inconsistent with v  except those where the value for v1 is also inconsistent with v; but those were considered in svlthus values for v1 inconsistent with v can be ignored in 
svk for k  1. this line of reasoning carries on to establish the validity of the overall reduction.   
1 ordering heuristics 
the algorithm presents new opportunities for ordering heuristics: choosing a variable domain in which to look for a value to decompose around  choosing the value  choosing the order in which subproblems are placed on the stack  maintaining the subproblems on an ordered agenda rather than a stack. 
　the variable ordering heuristic we test below is based on a well-known heuristic that we will refer to here as dynamic minimal domain variable ordering. this is a variable ordering heuristic that chooses a variable to instantiate with minimal current domain size. it has proven especially effective with forward checking  where preclusion effects changes in the current domain size of variables  haralick and elliott  1   we will refer to the combination of forward checking and dynamic minimal domain variable ordering as the fc-d algorithm  and use it as a basis for comparison in section 1. 
　we use this heuristic to a limited degree in conjunction with idc. after taking a precluded subproblem  or the initial problem  off the stack  the variable v from which we choose a value to decompose around is a variable with minimal current domain size. otherwise we will simply 

choose the first remaining uninstantiated variable to process next. note that for the remainder subproblem  by choosing the first uninstantiated variable to process we automatically get the effect of minimal domain size ordering  as this first variable will be v  whose domain has just been reduced further in size while the other domains have remained unchanged. we will refer to our our ordering strategy for idc as dynamic minimal domain precluding variable ordering. 
　we also experimented a bit with a simple  static value ordering scheme. the problem was preprocessed to obtain an inconsistency count for each value v: this is the total number of values for all other variables inconsistent with v. during search when choosing a value for a variable v  a value in the current domain of v with a minimal inconsistency count is chosen. call this the inconsistency count value ordering heuristic. when we test it below we add the constraint checks required to compute the inconsistency counts into the total search effort. 
1 conservative 	decomposition 	strategies 
when a problem p is popped from the stack we can choose not to perform an idc decomposition. instead  after choosing a value v for a variable v  we can decompose into only two subproblems. the first is the same precluded subproblem as in the idc decomposition; it includes v and the values consistent with v.  as before in the context of the algorithm we can further reduce the subproblem by removing v  adding v to the associated solution.  the second  which we will call the remainder subproblem  is the same as p except that the domain of v excludes v. if we always decompose in this way we end up with a nonstandard description of the standard forward checking algorithm. thus we call this decomposition the forward checking decomposition. 
　we identify two conditions under which we will choose this more conservative decomposition strategy  in effect reverting to forward checking in these two circumstances.  note that if we always used the forward checking decomposition and our dynamic minimal domain precluding variable search order that we would have an algorithm 
equivalent to fc-d.  
　the first condition is that the precluded subproblem is empty  one of the variable domains is empty . in this case we end up only putting one subproblem onto the stack  and taking it right back off. so  in effect  we simply move right on to consider the next value for v. we will term the principle of avoiding idc decomposition when the precluded subproblem is empty the empty domain decomposition heuristic. theorem 1 below implies that idc decomposition will not reduce the problem more than forward checking decomposition when the precluded subproblem is empty. 
　the second condition under which we choose the simpler decomposition utilizes an estimate of the relative complexity of the subproblems produced by the idc decomposition versus those produced by the forward checking decomposition. 
since both decompositions include the precluded 
1 	constraint satisfaction problems 
subproblem  we compare the excised subproblems from the idc decomposition with the remainder subproblem from the forward checking decomposition. the size of a problem  as measured by the number of possible value combinations  is a reasonable heuristic estimate of problem complexity here  and is obtained by simply multiplying the domain sizes for the variables. the size of the remainder subproblem is compared with the sum of the sizes of the excised subproblems.  actually we use the  consistent subproblem   defined below  to simplify the computations.  
　we will show below that the former will in fact never be smaller. however  it proved desirable only to choose the idc decomposition when the latter is smaller than the former to more than a specified degree. we will refer to this as our partial idc reduction heuristic. 
　this heuristic employs an idc choice factor. we experimented a bit with different idc choice factors. the results reported below are for a conservative factor of 1  meaning we only use idc decomposition when it results in more than an 1% decrease compared with the size of the remainder subproblem. more frequent use sometimes produced even more dramatic improvements  but was less consistent overall. we hope to discover strategies that will permit us to take even greater advantage of idc decomposition problem size reduction. estimates of complexity other than problem size may be of use here. 
1 reducing the stack size 
we do not have to generate and store all the subproblems of an idc decomposition at once. we can generate and push onto the stack one subproblem at a time along with information needed to generate the rest. we will refer to this as our stack reduction strategy. 
1 the idc algorithms 
we define the idc algorithm to be the weak idc algorithm described in section 1  together with the further domain reduction of full idc decomposition and the empty domain decomposition heuristic. the idc algorithm will be our primary target of analysis in section 1. we define the idc-pds algorithm to be the idc algorithm plus partial idc reduction  the dynamic minimal domain precluding variable ordering and the stack reduction strategy. the idc-pds algorithm  with an idc choice factor of 1  will be the primary target of experimental investigation in section 1. 
　in both section 1 and section 1 we compare idc-based algorithms with algorithms based on forward checking this is useful both because of the interesting relationship between the two approaches and because fc-d is one of the most successful algorithms in the literature. 
1 analysis 
1 reducing the problem size 
we arrived at the idc decomposition by observing that if there was no solution  for a solvable problem  involving a value v for a variable v  that there must then be a solution for a subproblem where at least one variable domain is restricted to those values inconsistent with v. consider  on the other hand  the subproblem where the domain of v contains every value but v and every other variable domain is restricted to just those values consistent with v. call this the consistent subproblem for v. 
　in the precluded subproblem the domain of v is reduced to v and the domains of all the other variables are reduced to the values consistent with v. in the consistent subproblem the domain of v is reduced by v  v is omitted; the other variables again contain all the values consistent with v. 
　theorem 1: given a problem p and a value v  we can prune the consistent subproblem for v from consideration without losing all solutions. 
　proof. if there is a solution  s  to the consistent subproblem  clearly we can substitute v for the v value in s and still have a solution.   
　theorem 1 says that a problem can be viewed as a sum of subproblems  and the weak idc algorithm demonstrated how we can process a sum of subproblems by processing each in turn. theorem 1 says that a problem can be viewed as a difference of problems. however  we do not know how to process a difference of problems  how to utilize this insight algorithmically. on the other hand  this new view is clearly guaranteed to reduce the original problem  by eliminating some possibilities from consideration  or at the very least not make it larger. for all we know at this point the idc decomposition could increase the number of possibilies to consider by adding some redundancy; the weak idc decomposition can do so in fact. we will now adopt a third view that will tie together these other two demonstrating that the idc decomposition in fact produces exactly the original problem minus the consistent subproblem  and minus the values removed by preclusion . thus the idc algorithm is an algorithmic method of pruning away the consistent subproblem. 
　theorem 1: idc decomposition around a value v for variable v prunes the consistent subproblem for v from consideration. 
　proof. we create a  decomposition tree  as follows. first we divide the original problem into two  children   the precluded problem and the remainder problem. next we divide the remainder problem into two children  subproblems that are the same except that in one the domain of a neighbor of v contains only the values inconsistent with v and in the other the domain of the neighbor contains only the values consistent with v.  this decomposition is reminiscent of mackworth's nc algorithm  mackworth  1 .  next we divide the second child into two subproblems  each of which is the same except for the domain of another neighbor of v. one subproblem will contain the values of that neighbor inconsistent with v  another the values consistent with v. we continue this process until we have run through all the neighbors. when we are done the leaf nodes of the tree of problems we have created will together represent exactly those combinations of values represented in the original problem  minus the combinations removed by preclusion  and solving the leaf node subproblems will be equivalent to solving the original subproblem. examination of the leaf nodes will reveal that they are precisely the idc decomposition subproblems plus the consistent subproblem.   
　further consideration of the decomposition tree used in the proof of theorem 1 suggests an alternative proof of theorem 1  and supports the following theorem. 
　theorem 1: the size of the idc decomposition around a value v  the sum of the sizes of the subproblems  is less than the size of the forward checking decomposition by an amount exactly equal to the size of the consistent subproblem. 
　thus the idc decomposition always reduces the size of the problem. in the extreme case where the consistent subproblem is empty we still have preclusion. in the extreme case where there is no preclusion  idc in effect removes the entire remainder problem. 
1 comparison with forward checking 
when considering a value v for a variable v  forward checking prunes away all the values inconsistent with v. as we have seen  idc will in addition prune away a subproblem where the domain of v omits v and the domains of all the other variables contain all the values consistent with v. 
　consider  for example  the classic coloring problem  which involves assigning colors to countries on a map so that neighboring countries do not have the same color. suppose we have four countries and three colors  red  green  blue  and we are considering the first color  red  for the first country  a. forward checking eliminates all possibilities that include coloring a neighbor of a red. our new technique eliminates at least an additional 1 possibilities  all 1 different ways of choosing a color for each country from the two choices green and blue. 
　computing the idc decomposition requires no more constraint checks than computing the precluded subproblem  except possibly when v is inconsistent with every value for some variable. in this case the empty domain decomposition heuristic avoids idc decomposition. idc decomposition can prune away more possibilities than forward checking decomposition  which may save constraint checks. however  more pruning does not guarantee fewer constraint checks. in particular  if there is more than one solution idc might prune away a solution that forward checking will find early.  however  if there is only one solution  that cannot happen.  
　the preclusion of forward checking with a value v does more pruning if v is inconsistent with more values. the consistent subproblem removal of idc decomposition on the other hand does more pruning if v is consistent with more values. thus idc decomposition nicely complements  or completes  forward checking. 
　for ordering heuristics  the fact that forward checking preclusion and idc consistent problem pruning are complementary unfortunately means that they will benefit most from complementary heuristics. forward checking benefits most when a lot of values for uninstantiated variables are inconsistent with the value v for a variable v used for preclusion. idc consistent problem pruning benefits most  the size of the pruned consistent subproblem is greatest  when a lot of values for uninstantiated variables are consistent with the value v for a variable v used for idc decomposition. the domain size of the variable v is irrelevant to the amount of pruning accomplished by forward checking  while the larger the domain size the larger the consistent subproblem pruned by idc  assuming there arc no empty domains in the consistent subproblem . 
　small domain sizes for v and extensive preclusion is consistent with the important  fail first  principle for variable ordering  haralick and elliott  1 . in particular heuristics favoring minimal domain size  such as those used in fc-d and idc-pds further the fail first principle without hindering preclusion  but at some cost to idc consistent problem pruning. for value ordering on the other hand  the  succeed first  principle suggests choosing least constraining values first. those will be least useful for preclusion  but will have the largest consistent subproblems  and thus benefit most from idc consistent problem pruning. 
1 weakly constrained values 
the consistent subproblem for value v for variable v  pruned away by idc  will be largest when v is weakly constrained  i.e. v is consistent with most of the other values for each of the other variables. of course  if all values in a problem are weakly constrained the problem itself will be easy to solve. however  idc should be able to take advantage of individual weakly constrained values in a problem of high overall difficulty. idc is also prepared to take advantage of weaknesses that arise during processing of the problem. if v is inconsistent with many values for u in the original problem  these values  or u itself  may not be present in a subproblem. 
1 space 	complexity 
we are not trading time for exponential space here. the size of the stack has an 1 n1  bound in the idc algorithm. with the stack reduction strategy the bound is only o n . 
1 experiments 
we used 1-variable test problems  generated for us by 
richard j. wallace. idc-pds  with an idc choice factor of 
1  was compared with fc-d. we measured constraint checks  a standard measure of csp algorithm performance  and cpu time. a constraint check asks whether a pair of values satisfies the relevant constraint  e.g. whether the combination of a for x and b for y is permitted by the constraint between x and y. cpu time was measured on a sun sparcstation elc. 
1 random problems 
all the problems had 1 variables  each with four values in its domain. which pairs of variables shared a constraint and which pairs of values were permitted by the constraints was determined by an  expected value  random generation 1 constraint satisfaction problems 
procedure. 
　for each problem we determined which pairs of variables shared a constraint  i.e. which constraints were present in the constraint graph  as follows. first we randomly chose 1 edges that connected the variables in a tree structure. this was to ensure that no problems were decomposable into two independent problems. then an expected value was set for the constraint density which measures the fraction of possible additional constraints present. a density of 1 corresponds to a completely connected constraint graph  all possible edges present; a density of 1 corresponds to a treestructured constraint graph. in these 1-variable problems  each .1 of additional density corresponds roughly to adding 1 edges and increasing the average degree of a vertex in the constraint graph by 1. for example  starting from a tree structure  with an average degree of approximately 1  a density of .1 corresponds to an average of about 1 constraints associated with each variable. once an expected density value was chosen  we considered each possible pair of variables not already sharing a constraint in the initial tree structure. a constraint between each such pair was included with probability equal to the expected density. 
　the pairs of consistent values that define each constraint were determined in a similar random  probabalistic manner. constraint tightness measures the fraction of possible value pairs excluded by a constraint. all problems had an expected tightness value of .1: each possible pair of values was considered for each constraint  and excluded with a probability of .1.  the problem generator did not permit all possible pairs to be excluded; but it did permit zero pairs to be excluded: this did not occur very often  but would have been better avoided also.  notice that this allows for some variation in the actual tightness of individual constraints  and certainly in the tightness of individual values with respect to individual constraints. the constraints for coloring problems with four colors have the same tightness  but they are much more uniform in structure. 
1 varying difficulty by varying density 
in the first set of experiments we generated problems supplying varying values for expected density  from .1 to .1  looking for  really hard  problems  employing recent experimental and theoretical insights  checseman et al.  
1; williams and hogg  1 . the results are shown in figure 1. each point plotted is the average number of constraint checks for five problems  with the exception of an  outlier  omitted from the .1 average. the hardest problem set is at an expected density value of .1. thus the average degree of the constraint graphs variables in the hardest problem set is approximately 1.  put another way  
what williams and hogg 	 call the critical 
connectivity appears to lie near 1.  
　the idc-pds averages are better than the fc-d averages for each set  except at .1 where the two are equal. the idc-pds improvement increases to over 1% on the hardest set. the outlier at .1 took about 1 million constraint checks with fc-d and about 1 million with idc-pds. however  when tested with fc-d and idc-pds  



with the inconsistency count value ordering heuristic  only 1 checks were required by each  most of which went to computing the inconsistency counts. 
1 weak spots 
for each problem in the .1 set we processed problem variables to create a sequence of problems  introducing local  weak spots  into the original problem  either by removing or by weakening constraints. as we shall see  such local weak spots can be present in problems of extreme overall difficulty. indeed  to a degree  it appears that local weak spots can actually increase overall problem difficulty. for forward checking  we might expect this  as forward checking around weakly constrained values produces relatively little pruning. 
　figure 1 plots constraint checks for problems in one of these sequences. the first problem represented is one of the original .1 problems. we added weak spots to this problem  one at a time  by repeatedly picking a variable v  at random  that was involved in more than three constraints  and then randomly removing constraints involving v until only three remained. each problem in figure 1  after the first  includes five more weak spots. 
　we observe a  complexity peak  induced  interestingly enough  by added weak spots. and we observe that idcpds dramatically  flattens  the complexity peak. idc-pds does very well both on the hardest problem that has no solution  1 weak spots added  and on the hardest problem that does have solutions  1 weak spots added . 
　idc-pds does have more overhead than fc-d  but idcpds can achieve a significant advantage in cpu time as well. in this sequence idc-pds has better cpu time on four of the seven problems  including the most difficult one: 1 seconds to 1 seconds. bear in mind also that we are conducting constraint checks here by consulting efficiently hash-coded tables. in a real problem constraint checks could require significant calculation  which is one reason they are a significant measure of csp algorithm performance. finally  observe that we are not trading constraint checks for some other form of table lookup  as can occur in  memory-based  algorithms. 
　figures 1 through 1 shows the constraint check results for similarly induced problem sequences starting with the other four problems in the .1 set. four of the five sequences exhibit a weakness-induced complexity peak  where idc-pds performance excels  though in two problems effort initially decreases . in one of these problems idc-pds completely eliminates the peak present in the fc-d results. in that sequence idc-pds achieves a full order of magnitude improvement over fc-d  1 constraint checks to 1 constraint checks  on a problem with solutions. the fifth sequence  the only one where even the original problem has solutions  exhibits no significant weakness-induced peak; however  idc-pds superiority is still manifest.  note that the scale on the constraint checks axis differs in the four graphs.  
　in figure 1 the effort for the last problem in the sequence is not plotted. fc-d tested at close to 1 million constraint checks while idc-pds with an idc choice factor of 1 1 constraint satisfaction problems 
tested at close to 1 million constraint checks. however  again adding the inconsistency count value ordering heuristic made a dramatic difference: fc-d and idc-pds both required only 1 checks  all but 1 of them for computing the inconsistency counts! this problem does have solutions. adding the value ordering for the problem at the plotted peak in this sequence  1 weak spots added   where there are no solutions  has a relatively small effect  as one might expect.  the value ordering does not always trivialize problems with solutions. the last problem in the sequence in figure 1 becomes much simpler with the value ordering  but still requires almost 1 constraint checks with fc-d  and about half that with idc-pds.  
　we also conducted a few experiments where weak spots were added by loosening rather than removing constraints  changing the local tightness rather than the local density. problem sequences were generated where each successive problem contained five more weak spots  generated in this new way  choosing variables randomly for creating weak spots. to create a weak spot at a variable all the constraints involving the variable had consistent pairs  chosen at random  added as needed for the tightness of each constraint times the degree of the variable to be less than one.  if this required adding all possible pairs to a constraint  the weakening was not carried out. if none of the constraints needed and permitted weakening  another variable was chosen at random to work on instead.  
　we observed a complexity peak pattern similar to the one induced by removing constraints. for a sequence beginning with the original problem used in figure 1  but inducing weak spots in this alternative manner  the peak occured at 1 added weak spots and went up to close to 1 million constraint checks for fc-d  but only about 1 million for idc-pds. 
　one might expect that many realistic problems will be inhomogeneous enough to contain some weak spots. ironically  it appears that local weakness can dramatically increase overall problem difficulty. thus a technique  like idc decomposition  that is able to exploit weaknesses in hard problems seems highly desirable. 
