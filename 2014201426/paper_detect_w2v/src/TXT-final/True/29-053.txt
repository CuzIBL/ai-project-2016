 
we describe an approach to goal decomposition for a certain class of markov decision processes  mdps . an abstraction mechanism is used to generate abstract mdps associated with different objectives  and several methods for merging the policies for these different objectives are considered. in one technique  causal  least-commitment  structures are generated for abstract policies and plan merging techniques  exploiting the relaxation of policy commitments reflected in this structure  are used to piece the results into a single policy. abstract value functions provide guidance if plan repair is needed. this work makes some first steps toward the synthesis of classical and decision theoretic planning methods. 
	1 	introduction 
markov decision processes  mdps  have become a standard conceptual and computational model for decision theoretic planning  dtp  problems  allowing one to model uncertainty  competing goals  and process-oriented objectives. one of the key drawbacks of mdps  vis-a-vis classical planning methods  is the exponential nature of standard  dynamic programming policy construction algorithms. these generally require explicit state space enumeration. 
　recent work in dtp has focused on techniques that exploit problem structure to solve mdps in efficient ways. such methods include problem decomposition based on reachability analysis  and exact and approximate abstraction algorithms  1  1  in which states are  implicitly  aggregated depending on whether they agree on the values of certain problem variables. indeed  these abstraction methods apply an insight fundamental to classical planning: feature-based search methods are usually much more tractable than state-based methods. this underlies  for instance  regression and causal link planning algorithms . 
　in this paper  we investigate another way of decomposing mdps for efficient solution  namely  goal decomposition. specifically  if utility is assigned to the achievement of certain features  as in multi-attribute utility theory    we will consider  components  of the utility function separately. in rough outline  we decompose the utility function  construct an abstract policy for each component that maximizes performance with respect to its objectives  and then attempt to merge the abstract policies into a single  coherent  approximately optimal policy for the entire set of objectives. 
1 	planning and scheduling 
　we view goal decomposition of this type as a step toward the further synthesis of decision-theoretic and classical planning methods; goal decomposition is essential in leastcommitment planning. if two subgoals are noninteracting  we can plan for each separately and  merge  the results. if the solutions interact  least-commitment planners facilitate the merging process by not committing to the ordering of actions required for a given subproblem when the order is not relevant. in fact  a representation using causal links makes clear the reasons for an action's presence in the plan. 
　this second insight seems to offer genuine benefits to planning  but it has not been exploited in any proposed solution methods for mdps. indeed  a major impediment to this approach is the requirement for some form of policy merging as opposed to simple plan merging. this is difficult because the concept of a policy for an mdp has no explicit flexibility: it simply assigns actions to every state. 
　we note that the value function associated with an mdp implicitly determines a set of optimal policies  not just a single policy; thus it offers some flexibility. intuitively  it can be used to tell when an alternative action can be used instead of that dictated by the policy with no or  under certain conditions  little loss in value. we will describe a state-based search process that allows us to merge policies using value functions in this way. 
　we also consider intuitions from least-commitment planning to provide an alternative means of policy merging. we provide a method for extracting causal structure from a policy  under certain assumptions  that reveals the aspects of the policy that are essential to its performance and those that are not. this flexibility permits policy merging to proceed more readily. since merging is not guaranteed to succeed  we also give priority to abstract policies associated with more important objectives. this prioritization of goals approximates the workings of optimal policy construction. 
　in section 1 we overview mdps and their representation  nonlinear plans  and the mdp abstraction technique of . section 1 outlines the major components of our framework- we describe: the construction of prioritized goals or objectives; the extraction of a nonlinear plan from a given policy; and methods for merging policies  including the use of leastcommitment structure. we describe directions for future research  including challenges facing the generalization of this approach  in section 1. 


	boutilier  brafman  & geib 	1 

1 	approximate abstraction 
the abstraction technique developed in  can reduce the size of the mdp one needs to solve to produce an approximately optimal policy. using regression-like operations  we can discover which atoms influence the probability of other propositions becoming true when certain action sequences are executed. when a reward function has independent components  we can sweep through the action descriptions determining which propositions  contexts and preconditions  influence the ability to make the reward proposition true or false  which propositions  in turn  influence these  and so on. such atoms are deemed relevant atoms  the set of which is denoted r. once this process is completed  any atoms not in r  i.e.  that do not influence the probability of a reward proposition through any action sequence  can be ignored. their truth values cannot influence the optimal choice of action. 
　for instance  suppose the only atom that influences reward is hole pl . since drilling affects this  its precondition -hot dictates that atom hot is relevant. since hot is affected by drilling if pressed pl   atom pressed pj  is also added to r. no atom influences any action's effect on pressed{pl   thus  a fixed point is reached  with the only relevant atoms being hole pj   hot and pressed pj . the abstract mdp over this reduced space has only 1 states  and can be more easily solved to produce an optimal policy. 
   more importantly  one can ignore certain reward propositions to produce an approximate abstraction  see  . for instance  in figure 1  atoms hole pj   pressed pl   and painted pj  have a larger influence on reward than hole{p1 . by ignoring the hole p1   we create an abstract space much smaller than the one needed to solve the mdp optimally- only atoms pertaining to pi  and hot  are introduced into r hence the resulting abstract mdp. this mdp has only 1 states and is easier to solve. one optimal policy  among several possibilities  for this mdp has the form: 

1 least-commitment plans 
we will use least-commitment  or partially-ordered  plans to represent the relevant parts of policies in a way that allows them to be more easily  merged.  we briefly review the relevant concepts here  but refer to  for more details. 
　a least-commitment plan  lcp  consists of a set of action instances a  a set of causal links involving these actions and a set of ordering constraints  i.e.  using the relations   and    over these actions. we ignore issues of quantification  e.g.  involving codesignation constraints  etc.  for simplicity. a causal link is a tuple  a  p  1  where a and b are actions and p is a proposition. intuitively  such a link represents the fact that a has effect p  b has precondition  or a context  p  and that no action that might occur between a and b will change the truth of p. we say that a produces p and that 1 consumes p. when constructing apian  we may introduce an action c in order to achieve a subgoal q  where c has the additional effect of making p false. if c can be consistently ordered between a and 1  we say that c threatens  a  p  1 .1 to ensure the plan is valid  we resolve the threat by insisting that c occur before a or after 1. such constraints restrict the legal sequences of action instances. we can think of an lcp as representing its set of linearizations: the sequences of action instances in a consistent with the ordering constraints. 
　we do not discuss here algorithms for producing lcps  but refer to   1  for a survey and to some of the key developments presented in  1  1 . 
	1 	goal decomposition 
the aim of goal decomposition is similar to that of abstraction: to solve smaller mdps by ignoring variables. unlike the abstraction method of   however  we do not solve a single abstract mdp  e.g.  for pi -objectives  and ignore other objectives completely. instead  we solve several abstract mdps  each pertaining to different sets of objectives  e.g.  one for pi and one for p1  and combine the results into a single policy of higher value than any individual abstract policy. 
　there are two major challenges that must be met to solve this task. first  we must decide how to decompose the reward function in order to generate  and solve  small  abstract mdps  section 1 . second  we must merge policies  which itself involves several difficulties. chief among these is the fact that policies offer no flexibility. in section 1  we describe how to extract essential causal structure from abstract policies that reveals the flexibility inherent in a policy. in section 1  we discuss policy merging. we first describe a merging procedure that uses the value functions of two abstract policies  rather than the policies themselves  to guide a search through state space. we then propose a method for merging that uses the causal structure of the first policy to constrain search through state space. 
　the details of the various algorithms described in this paper are based on several assumptions. we assume that our problems are goal-oriented; that is  we are only concerned with the achievement of propositions in the  final state  that 
　　1  we refer to   1  for a discussion of the complexities introduced by the existence of conditional effects. 
results from executing a sequence of actions.1 we also assume that each action has a primary or most probable effect  and that an initial state has been given. these assumptions legitimize the use of causal links and classical lcps. we conjecture  however  that the basic framework for mdp decomposition and policy merging can be applied in much more general settings with appropriate extensions. 
1 decomposing the reward function 
decomposing the reward function does not involve much more than partitioning the set of reward attributes in some way  say into sets ir1  ir1         and generating the appropriate sets of relevant atoms r1  r1         for instance  figure 1 naturally suggests the decomposition described in section 1   using the rewards associated with pi in one abstraction and the reward for p1 in another . several factors will influence the choice of abstract mdps. 
　when merging policies  section 1   we assume that certain abstract policies are given priority over others; that is  abstractions should be orderable in terms of importance. partitioning of reward attributes should therefore be such that features with large impact on reward should lie within more important partitions. in our example  the rewards associated with the completion of pi are more critical  so the partitions are ordered so that pi's success is given priority over p1. when objectives conflict and policies cannot be satisfactorily merged  e.g.  if pi and p1 require the same consumable resource   this prioritization allows the more important objectives to be achieved. this approximates the behavior of an optimal policy  which would make a similar tradeoff when faced with conflicting objectives. 
　attributes whose effect on reward is not independent should obviously be placed in the same abstraction. in our example  the contribution to reward made by hole pl  and 
pressed pj  is highly correlated. achieving one without accounting for the other makes little sense. 
　the odds of successful policy merging can be increased if there is little interaction between atoms in different abstract spaces; this can be a factor in the choice of ir sets. if an action influences an atom in two different spaces  the policies may interact in undesirable ways. the potential for interaction can be detected by a simple extension of the algorithm that generates r sets. such interaction cannot generally be avoided  but if the overlap is substantial one might consider merging different ir sets  or breaking them up differently. of course  larger mdps usually result and the  degree of potential interaction  should be balanced against the increased size of the mdp and the potential loss in value if one of the objective sets cannot be accommodated by merging. we refer to  for further discussion of reward attribute selection for abstraction and the tradeoff with abstract mdp size. 
1 	extracting causal structure from policies 
once the reward function has been decomposed  the abstract mdps associated with feature sets r1  r 1  ... can be formulated and solved  and optimal policies ... and value 
     1  we refer to  to see how such objectives can be encoded in an mdp using absorbing states. for example  we can have a distinguished stop action that precludes further actions. 
functions v1  v 1  ... determined. we note that this process can proceed in an incremental anytime fashion  with attention focused on the highest priority mdps  leaving lower priority mdps to be solved as time permits. 
　as we will see  it will be useful to extract the essential causal structure of a policy  in the form of a lcp: this flexibility will aid in merging. we assume each action has a unique most likely or primary effect. we define the most likely execution path  mlep  of an abstract policy to be the sequence of actions and states  given the initial state /  that is most likely. we assume that this path ends in some abstract goal state g  see below . we generate the lcp for policy it by extracting causal link information from the actions that occur along this path and the objectives that are actually achieved in g. any linearization of this lcp is guaranteed to be  optimal  in the sense that its mlep leads to the same abstract goal state. furthermore  should execution of the plan fall  off path  with an unlikely action effect   can be used to dictate a new action choice  a new lcp can also be generated at this point if replanning/remerging is desired . 
　lcp extraction for n proceeds as follows. we first generate the mlep for ’ given initial state / to goal state g.1 we then call a least-commitment planning algorithm  such as weld's  ucpop  with initial state / and goals consisting of those reward propositions in the current r set satisfied by g. planning will be very constrained  and efficient  however: when a subgoal q for action b is selected  the most recent action a  in mlep  that produces q is chosen as the producer and causal link  a  q  b  is generated. any requirements for action a  preconditions and specific context propositions  are then added as subgoals. we note that there is no ambiguity in choice of context for a  and hence a's conditional effects  as these are uniquely determined by the state in which a occurs in mlep. finally  threats to causal links are resolved respecting the ordering of mlep 
　at each step  the lcp produced by this algorithm has mlep as one of its linearizations. since this path is a solution to the planning problem we constructed  ucpop will return a plan without backtracking: mlep acts as an oracle. further efficiency can be gained by generating causal link information during mlep generation  leaving only threat resolution to complete the planning process. 
　to illustrate  consider the optimal abstract policy described in section 1  pertaining to pl  with initial state: 

the action sequence {press pj  drill pl   paint pi   appears on the mlep  and the goals pressed  pi   hole pl  and painted pi  are determined. ucpop then uses the actions above to generate the lcp shown in figure 1: causal links are denoted by solid lines and ordering constraints by dashed lines. note that press pl  and drill pi  are ordered before paint pi  because they threaten the effect painted pj . on the other hand  press pj  and drill pj  are unordered: though the policy commits to their ordering  this commitment 
   1  a terminal state g can be identified as one whose value is not significantly greater than the sum of the reward propositions it satisfies  or one for which a stop action has been selected. 
	boutilier  brafman  & geib 	1 

figure 1: causal structure with ordering constraints 
is discovered to be unessential to goal achievement in the planning process. execution in either order  assuming most likely outcomes  will be optimal. 
　as we will see  plan merging benefits from having lcps with as few constraints and causal links as possible  increasing the odds of success. the lcp extraction algorithm can be altered to produce  relaxed  plans that approximately correspond to mlep. in particular  we may ignore certain context conditions consumed by an action if these do not substantially influence goal achievement. for example  suppose smooth pi   while potentially important at certain states  is of marginal value on the current mlep for . if  say  smooth pl  is produced by the initial state and consumed by drill and only marginally increases the probability of hole  the link from the initial state to drill could be dropped. merging new plans with this relaxed lcp might result in a  slightly  less than optimal execution of if the new plan destroys smooth pj  before drilling; however  the added flexibility makes merging easier and may be worthwhile. 
1 	prioritized policy merging 
using state space search suppose we now have a set of abstract mdps that have been solved. for concreteness  assume two such mdps whose optimal policies and value functions v1  v1 have been determined. our goal is to produce a new policy that behaves well with respect to both sets of objectives  without solving a new mdp that combines both abstract spaces. to do this  we can use the value functions as heuristics to guide a search through state space. each value function tells us how good a state is with respect to its own objectives: if vi s  is high  the goals in mdp i are achievable. note  however  that this does not mean the goals are achievable jointly with those of some other mdp j. 
　more precisely  we define a search process whose nodes are states  whose arcs are labeled with actions  and whose initial state is the start state. at any active node of the search tree  we consider the effect of each possible action; that is  we add children corresponding to the unique state resulting from each action. each such state s has values vi s  the most promising such state is that with the highest such values.1 even if no action dominates all others  under the assumption 
   1the unique state associated with an action is due to the  pseudo-deterministic  assumption we made  but  in fact  the value of this resulting state is not the only consideration. if an action is likely to be successful but has some risk associated with it  the fact that its likely outcome is better than that of another action is not sufficient reason to select it  note that the abstract policies do take such 
1 	planning and scheduling 
that the reward function is additive  the sum of values vi s  can be used as the heuristic value of state s. 
　how this search process proceeds is rather flexible; but a best-first approach seems most appropriate. each value v   s  is a  perfect  heuristic in that  should the agent consider only objectives in space t  it could attain vi s . if the agent attempts to integrate other objectives  this can only cause the value vi s  to go down  or  at best  stay the same . thus  the  multivalued  heuristic  or the sum of values v   s   overestimates the true value of the state s  and is admissible-a simple best first search using this as the heuristic function will be appropriate.1 the search process terminates once an appropriate goal state is reached. 
　merging does require that we consider the  combined  abstract space containing the relevant atoms from all abstract mdps involved. however  while this space is considerably larger than the individual abstract spaces  we never need to construct policies  or plan in substantial ways  in the combined space. thus  the exponential  blow up  in state space size is not a factor in policy merging. 
　to illustrate the process  consider the example above  where the value functions with respect to parts p1 and p1 are denoted v  and v1 assume pressed p1  holds at the start state  along with the relevant propositions for pi . suppose press pi  is chosen as the first action: it causes no drop in v  or v1 however  at any subsequent point drill pi  or drill p1  will cause a drop in value because hot becomes true  preventing drilling of the other part. but an alternative initial action  drill pl   exists and subsequent search will lead directly to an appropriate sequence of actions. notice that this process leads to a policy in which the actions drill pl  and 
press pl  are reversed  w.r.t. the original  . note  however  that policy does not play a role in the search process. 
　modifying the example slightly  suppose that drilling causes hot to become true no matter what. in this case  drill p1  causes a drop in v1  while drill pl  causes a drop in v1  regardless of pressed  p1  . since p1 has higher value  the first drop is greater and we will  commit  to drilling pi  and using mdrill for p1. 
　finally  we note that pseudo-determinism is exploited by the search to find a merged action sequence rather than a policy per se. however  as mentioned  should any action have an unlikely effect  the agent can always resort to one of the abstract policies to decide how to act at the resultant   offpath  state  e.g.  choose to execute for that abstract policy with highest value vi s  . we could also imagine applying the search process to the most probable  unlikely  branches  or remerging online as circumstances warrant. 
using plan structure to guide search one difficulty with the search process described above is the fact that the actual search space explored can be quite large. furthermore  a number of alternatives may be explored before a genuine 
considerations into account . thus we use the expected value of an action w.r.t. v   or q-value of the action  as a guide. 
　1 multiple path  or cycle  checking is also appropriate in our pseudo-deterministic setting to ensure progress in being made. 
　1to be precise  a slight drop in value  corresponding to the discount factor will occur. we shall ignore this effect in our discussion. 
conflict is detected  as in the second case above . a second difficulty lies in the fact that this process does not lend itself to revision of policies or to the incremental construction of merged policies in an anytime manner. for example  if we merge policies for several abstract spaces  and then later solve another abstract mdp  merging the new policy requires revisiting the entire search process  unless some form of dependency information is recorded. 
　we now describe a process that alleviates these difficulties to some extent: when merging and we will first extract an lcp p1 for  and restrict our search to action sequences that have high value with respect to v1  on mdp 1 . any such action sequence must  however  respect the causal links in plan p1. this makes the search process easier for two reasons. first  the flexibility in ordering constraints in p1 allows us to consider several different optimal policies  w.r.t. v1  without explicit backtracking. second  we will not consider policies that are inconsistent with p1. this means we are committing to a particular means of achieving objectives in the first space. as such  the resulting merged policy may not be as good as one that doesn't adhere to such restrictions; but the computational benefits may make this worthwhile. once we've merged and we can extract a new lcp 
p1 that can be used when considering subsequent objectives. recording plans allows the merging process to proceed in an incremental fashion. we now make this more precise. 
　assume that an lcp p1 has been extracted for a particular set of objectives 1 and that an abstract value function v1 has been generated for a new abstract mdp. our merging process is prioritized: as before  we generate a path starting at the start state. only now  this path must be consistent with pi  i.e.  some linearization of p1 is a subsequence of this path   and v1 alone is used as the heuristic function. hence  we search for the best policy with respect to v1 subject to the constraints imposed by p1. this prioritization is necessary in instances where all objectives cannot be met. since some must be  sacrificed   we commit to the plan pertaining to higher valued objectives.1 the search proceeds as follows: 
1. each search node corresponds to a state and is annotated by a set of open links and permissible actions from p1. 
1. each arc in the search tree is annotated by an action. 
1. we use v1 as the heuristic function. 
1. we search from the start state: its open links are produced by start  and permissible actions are those that can consistently be executed first in p1. 
1. each node is expanded using only actions that are on the permissible list from p1  or that do not threaten any link on the open list. irrelevant actions  that do not belong to p1 and do not influence atoms in r1  are not considered. 
1. if an action a from p1 is used  the child node is annotated as follows: all links consumed by a are removed from the open list and those produced by a are added; a is removed from the permissible 
   1this need not be for a single abstract mdp: a set of such mdps may have been merged as in the previous section. 
   1 we assume without further mention that if abstract policy has very low value  compared to the set of objectives it is designed to achieve  it can be ignored completely. priority can be altered dynamically in this and other ways. 

figure 1: merged causal structure  1  
list; and any new action  all of whose predecessors  w.r.t. ordering constraints of p1  now lie on the search path  is added to the permissible list. 
the open list keeps tracks of links that must be respected by any action inserted to achieve objectives in  /1  while the permissible list serves as a  plan counter  for p1. a best-first search using v1 can be used. once this search is complete  we can extract  backtrack-free  a new lcp p1 from the chosen path. the process can then be repeated using p1 should one wish to consider further objectives  abstract mdps . 
it is often the case that more than one optimal policy exists. 
although our use of lcps allows us to capture the causal link structure of a number of policies  there may be different policies involving substantially different actions  e.g.  a different way of making holes . hence  at any point where merging results in a greatly reduced value in v1  we have the option of backtracking to a different optimal  or possibly suboptimal  policy for the earlier abstract mdp  generating a new plan for it and attempting to merge with this new plan. thus  our commitment to p1 need not be absolute. 
to illustrate the merging process  let our first abstract plan 
pi be that in figure 1  corresponding to p/   and let the second value function v1 correspond to the sole objective pertaining to p1  attained by drill p1  . assume again that pressed p1  holds. search using v1 suggests a number of potential actions: drill p1 y drill p1   and press pl . intuitively  the first action is desirable relative to v1 and the other two actions are from p1.  action mdrill p1  is suboptimal w.r.t. v1  and all actions involving p1 are irrelevant to v1.  the action drill p1  threatens drill pl  in p1 because of its effect hot. hence  we consider the next possible action  drill pl . suppose we choose to follow this path. as a subsequent action  we can choose either drill p1  or press pj . the conditional effect hot of drill p1  is no longer a threat. 
we can continue from here on in a straightforward manner  eventually generating the lcp shown in figure 1. note that merging reversed the ordering of press pi  and drill pl  in the original abstract policy without backtracking  showing the benefits of extracting causal structure. in addition  mdrill p1  is never considered because of its low value. 
　as a second example  again imagine that hot becomes true whenever drilling occurs. any attempt to use drill p1  as the first action will threaten drill pi  as above; but drill p1  precludes subsequent execution of drill p1 . the commit-
	boutilier  brafman  & geib 	1 

figure 1: merged causal structure  1  
ment to the initial plan means that an alternative action must be chosen: the value function v1 dictates that action mdrill p1  should be performed and the goal state is then reached  though with lower probability . an lcp can then be extracted which achieves both sets of objectives  as shown in figure 1. we note that action mdrill p1  is not on the mlep for the optima  policy ; but commitment to the first plan forced us to consider a suboptimal way of achieving the second objective. in general  the algorithm-using the optimal value function v1-effects  repair  of an unmergeable policy. indeed  if at that point the optimal policy dictated that a different  conflicting  objective should be attempted  e.g.  shipping a different part instead of p1   the merging algorithm would have stirred us to a completely new  goal.  
　the key difference between merging using only abstract value functions and merging using a plan is that the latter commits to a particular way of achieving the higher priority objectives and attempts to work around these commitments when determining how to achieve lower priority objectives. this commitment has some clear computational advantages  but comes at the cost of perhaps sacrificing some lower priority objectives because one is not willing to consider  substantially  different ways of attaining high priority goals. 
　the plan-based merging approach shares much of the motivation underlying work on intentions and resource-bounded reasoning in planning . although it may result in inferior policies as compared to the first  less constrained search method  the commitment embodied by the plan-based approach is suitable when  due to uncertainty and resource constraints  we prefer an anytime approach in which we first generate a plan for the most important objectives. later  we can modify such plans  taking into account other  less important objectives. in addition  efficient planning may often require us to accept certain assumptions about the state of the world or its dynamics. once we learn that one of these assumptions is false  it may not be possible to replan from scratch  as required by the first merging approach. revision of a plan to reflect changing assumptions may prove more suitable in such circumstances. 
　this use of commitment could be taken even further: we could extract an lcp for both  sets of  objectives and attempt to merge them  thus committing to a specific means of attaining both. we note that it may be possible to use and modify existing plan merging techniques  e.g.  see   in this re-
1 	planning and scheduling 
gard. however  strong commitment to earlier  higher priority  plans remains essential if plans prove unmergeable. 
1 concluding remarks 
we have suggested a method of goal decomposition for mdps based on abstraction and least-commitment planning. 
our technique allows certain classes of mdps to be solved efficiently  though suboptimally  through the solution of a set of considerably smaller mdps associated with different ob-
jectives. there are a number of questions relating classical planning  dtp and resource-bounded reasoning that may  in the future  be addressed satisfactorily within this framework. these include the derivation of goals from general decisiontheoretic considerations  the utility of classical plans in dtp settings  and the derivation of intentions or commitments from the decomposition of objective functions. 
　we are currently exploring generalizations of this approach  in particular  the development of least-commitment  causal link representations of complete policies and methods of extracting such from standard  state-based policies. this will enable the restrictive assumptions of goal-based  pseudo-deterministic problem structure to be dropped. we are also exploring other means of merging and methods for bounding the error associated with prioritized merging. 
references 
 r. e. bellman. dynamic programming. princeton  1. 
 c boutilier  r. dearden  and m. goldszmidt. exploiting structure in policy construction. in ijcaj-1  pp. 1 1  montreal  1. 
 m e. bratman. intentions  plans  and practical reason. harvard  1. 
 t. dean  l. p. kaelbling  j. kirman  and a. nicholson. planning with deadlines in stochastic domains. in aaai-1  pp.1  washington  d.c.  1. 
 t. dean and k. kanazawa. a model for reasoning about persistence and causation. comp. intel.  1 : 1 1. 
 r. dearden and c. boutilier. abstraction and approximate decision theoretic planning. artif. intel.  1-1  1. 
 r. a. howard. dynamic programming and markov processes. mit press  cambridge  1. 
 r. l. keeney and h. raiffa. decisions with multiple objectives: p