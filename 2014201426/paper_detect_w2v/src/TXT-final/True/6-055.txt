 
teamwork demands agreement among teammembers to collaborate and coordinate effectively. when a disagreement between teammates occurs  due to failures   team-members should ideally diagnose its causes  to resolve the disagreement. such diagnosis of social failures can be expensive in communication and computation overhead  which previous work did not address. we present a novel design space of diagnosis algorithms  distinguishing several phases in the diagnosis process  and providing alternative algorithms for each phase. we then combine these algorithms in different ways to empirically explore specific design choices in a complex domain  on thousands of failure cases. the results show that centralizing the diagnosis disambiguation process is a key factor in reducing communications  while run-time is affected mainly by the amount of reasoning about other agents. these results contrast sharply with previous work in disagreement detection  in which distributed algorithms reduce communications. 
1 introduction 
one of the key requirements in teamwork is that agents come to agree on specific aspects of their joint task  cohen and levesque  1;groszandkraus  1;tambe  1 . with increasing deployment of robotic and agent teams in complex  dynamic settings  there is an increasing need to also be able respond to failures that occur in teamwork  tambe  1; parker  1   in particular to be able to diagnose the causes for disagreements that may occur  in order to facilitate recovery and reestablish collaboration  e.g.  by negotiations  kraus et al 1  . this type of diagnosis is called social diagnosis  since it focuses on finding causes for failures to maintain designer-specified social relationships  kaminka and tambe  1 . 
　for instance  suppose a team of four robotic porters carry a table  when suddenly one of the robots puts the table down on the floor  while its teammates are still holding the table up. team-members can easily identify a disagreement  but they also need to determine its causes  e.g.  that the robot believed the table reached the goal location  while its teammates did not. given this diagnosis  the robots can negotiate to resolve the disagreement. 
　unfortunately  while the problem of detection has been addressed in the literature  e.g.   kaminka and tambe  1    social diagnosis remains an open question. naive implementations of social diagnosis processes can require significant computation and communications overhead  which prohibits them from being effective as the number of agents is scaled up  or the number of failures to diagnose increases. previous work did not rigorously address this concern:  kaminka and tambe  1  guarantee disagreement detection without communications  but their heuristic-based diagnosis often fails.  dellarocas and klein  1; horling et al.  1  do not explicitly address communication overhead concerns.  frohlich et al  1; roos et al  1  assume fixed communication links  an assumption which does not hold in dynamic teams in which agents may choose their communication partners dynamically  see section 1 for details . 
　we seek to examine the communication and computation overhead of social diagnosis in depth. we distinguish two phases of social diagnosis:  i  selection of the diagnosing agents; and  ii  diagnosis of the team state  by the selected agents . we provide alternative algorithms for these phases  and combine them in different ways  to present four diagnosis methods  corresponding to different design decisions. we then empirically evaluate the communications and runtime requirements of these methods in diagnosing thousands of systematically-generated failure cases  occurring in a team of behavior-based agents in a complex domain. 
　we draw general lessons about the design of social diagnosis algorithms from the empirical results. specifically  the results show that centralizing the disambiguation process is a key factor in dramatically improving communications efficiency  but is not a determining factor in run-time efficiency. on the other hand  explicit reasoning about other agents is a key factor in determining run-time: agents that reason explicitly about others incur significant computational costs  though they are sometimes able to reduce the amount of communications. these results contrast with previous work in disagreement detection  in which distributed algorithms reduce communications. 
　the paper is organized as follows: section 1 motivates the research. section 1 presents the diagnosis phases and alternative building blocks for diagnosis. section 1 specifies diagnosis methods which combine the building blocks in different ways  and evaluates them empirically. section 1 presents related work  and section 1 concludes. 
1 	motivation and examples 
agreement  e.g.  on a joint plan or goal  is key to establishment and maintenance of teamwork  cohen and levesque  1; jennings  1; grosz and kraus  1; tambe  1 . unfortunately  complex  dynamic settings  sometimes lead to disagreements among team-members  e.g.  due to sensing failures  or different interpretations of sensor readings   dellarocas and klein  1 . given the critical role agreement plays in coordinated  collaborative operation of teams  we focus on the diagnosis of disagreements in this paper. 
　the function of a diagnosis process is to go from fault detection  where an alarm is raised when a fault occurs   to fault identification  where the causes for the fault arc discovered. in diagnosing disagreements  the idea is to go beyond simple detection that a disagreement exists  to identification of the differences in beliefs between the agents  that lead to the disagreement. such differences in beliefs may be a result of differences in sensor readings or interpretation  in sensor malfunctions  or communication difficulties. once differences in beliefs are known  then they can be negotiated and argued about  to resolve the disagreements  kraus et al.  1 . unfortunately  diagnosis of such failures can be extremely expensive both in terms of computation as well as in communications  see section 1 for quantitative evaluation   and thus a key challenge is to minimize communications and computation. 
　to illustrate the problem we use an example  originally reported in  kaminka and tambe  1   from the modsaf domain  a virtual battlefield environment with synthetic helicopter pilots . in the example a team of pilot agents is divided into two: scouts and attackers. in the beginning all teammates fly in formation looking for a specific way-point  a given position   where the scouts move forward towards the enemy  while the attackers land and wait for a signal. kaminka and tambe  report on the following failure case  which there failure detection technique captures : there are two attackers and one scout flying in formation  when the attackers detect the way-point and land  while the scout fails to detect way-point and continues to fly. 
　a diagnosis system  running on at least one of the agents  must now isolate possible explanations for the disagreement  of which the real cause  agents different in their belief that the way-point was detected  is but one. each such explanation is a diagnosis hypothesis since it reports a possible reason for the disagreement between the agents. disambiguation between these hypotheses seems very straightforward: each agent can generate its own hypotheses  and then exchange these hypotheses with its teammates to verify their correctness. unfortunately  since each agent is potentially unsure of what its team-members are doing  the number of hypotheses can be quite large  which means communications will suffer greatly. furthermore  having each agent simply report its internal beliefs to the others  while alleviating communications overhead somewhat  is also insufficient. indeed  any approach requiring high bandwidth is not likely to scale in the number of agents  jennings  1 . 
1 	building blocks for diagnosis 
we distinguish two phases of social diagnosis:  i  selecting who will carry out the diagnosis;  ii  having the selected agents generate and disambiguate diagnosis hypotheses. to explore these phases concretely  we focus on teams of behavior-based agents. the control process of such agents is relatively simple to model  and we can therefore focus on the core communications and computational requirements of the diagnosis. 
　we model an agent as having a decomposition hierarchy of behaviors  where each behavior  node in the hierarchy  has preconditions  which allow its selection for execution when satisfied   and termination conditions  which terminate its execution if the behavior was selected  and the conditions are satisfied . each behavior may have actions associated with it  which it executes once selected. it may have child behaviors  which it can select based on their matching preconditions and preference rules. at any given time  the agent is controlled by a top-to-bottom path through the hierarchy  root-to-leaf only one behavior can be active in each level of the hierarchy. such a generic representation allows us to model different behavior-based controllers  e.g.  firby  1; newell  1  . 
　we assume that a team of such agents coordinates through designer-provided definition of team behaviors  which are to be selected and de-selected jointly through the use of communications or other means of synchronization. team behaviors  typically at higher-levels of the hierarchy  serve to synchronize high-level tasks  while at lower-levels of the hierarchy agents select individual  and often different  behaviors which control their execution of their own individual role. 
　for instance  in the modsaf example  the scout should fly to look for the enemy  while the attackers should wait at the way-point. all agents are executing in this case a team behavior called wait-at-point  in service of which the attackers are executing individual just-wait  land  behaviors  while the scout is executing a more complex fly-ahead behavior. 
　disagreement between team-members is manifested by selection of different team behaviors  by different agents  at the same time. since team behaviors are to be jointly selected  by designer specification   such a disagreement can be traced to a difference in the satisfaction of the relevant pre-conditions and termination conditions  e.g.  agent a believes p  while agent b believes -p  causing them to select different behaviors. it is these contradictions which the diagnosis process seeks to discover. a set of contradictions in beliefs that accounts for the selection of different team behaviors by different agents is called a diagnosis. 
1 disambiguating diagnosis hypotheses 
diagnosis 	1 the first phase in the diagnosis process involves selection of the agent s  that will carry out the diagnosis process. however  the algorithms used for selection may depend on the diagnosis process selected in the second phase  disambiguation   and so for clarity of presentation  we will begin by discussing the second phase. assume for now that one or more agents have been selected to carry out the diagnosis process. 
the agents must now identify the beliefs of their peers. 
　perhaps the simplest algorithm for this is to have all teammembers send their relevant beliefs to the diagnosing agent  the diagnosing agent can inform the team-members of the detection of a disagreement to trigger this communication . in order to prevent flooding the diagnosing agent with irrelevant information  each team-member sends only beliefs that are relevant to the diagnosis  i.e.  only the beliefs that are associated with its currently selected behavior. 
　upon receiving the relevant beliefs from all agents  the generation of the diagnosis proceeds simply by comparing all beliefs of team-members to find contradictions  e.g.  agent a believes p  while agent b believes - p . since the beliefs of the other agents are known with certainty  based on the communications   the resulting diagnosis must be the correct one. however  having all agents send their beliefs  may severely impact network load. 
　we thus propose a novel selective monitoring algorithm  in which the diagnosing agent controls the communications  by initiating targeted queries which are intended to minimize the amount of communications. to do ihis  the diagnosing agent builds hypotheses as to the possible beliefs held by each agent  and then queries the agents as necessary to disambiguate these hypotheses. 
　this process begins with resl  a previously-published behavior recognition and belief inference algorithm  kaminka and tambe  1   which is only presented here briefly as a reminder. under the assumption that each agent has knowledge of all the possible behaviors available to each teammember  i.e.  their behavior library  an assumption commonly made in plan recognition   each observing agent creates a copy of the fully-expanded behavior hierarchy for each of its teammates. it then matches observed actions with the actions associated with each behavior. if a behavior matches  it is tagged. all tagged behaviors propagate their tags up the tree to their parents  and down to their children  such as to tag entire matching paths: these signify behavior recognition  plan recognition  hypotheses that are consistent with the observed actions of the team-member. 
　once hypotheses for the selected behavior of an agent are known to the observer  it may infer the possible beliefs of the observed agent by examining the pre-conditions and the termination conditions of the selected behavior. to do this  the observer must keep track of the last known behavior s  hypothesized to have been selected by the observed agent. as long as the hypotheses remain the same  the only general conclusion the observer can make is that the termination conditions for the selected behaviors have not been met. thus it can infer that the observed agent currently believes the negation of the termination conditions of selected behaviors. 
　when the observer recognizes a transition from one behavior to another  it may conclude  for the instant in which the transition occurred  that the termination conditions of the previous behavior  and the pre-conditions of the new behavior are satisfied. in addition  again the termination conditions of the new behavior must not be satisfied; otherwise this new behavior would not have been selected. therefore  the beliefs of the observed agent  at the moment of the transition  are:  termination conditions of last behavior    pre-conditions of current behavior    termination conditions of current behavior . 
　for instance  suppose an attacker is observed to have landed. the observer may conclude that either the halt or wait-at-point behavior has been selected. furthermore  the observer can conclude that the termination conditions for halt do not hold  or that the termination conditions for waitat-point do not hold. if the observed action indicates that the agent has just transitioned into the behavior associated with landing  then the preconditions of halt and wait-at-point would also be inferred as true. 
　once the hypotheses are known  the agent can send targeted queries to specific agents in order to disambiguate the hypotheses. the queries are selected in a manner that minimizes the expected number of queries. intuitively  the agent prefers to ask first about propositions whose value  when known with certainty  will approximately split the hypotheses space. 
1 	selecting a diagnosing agent 
let us now turn to the preceding phase  in which the agents that will carry out the diagnosis are selected. several techniques are available. first  a design-time selection of one of the agents is the most trivial approach. it requires a failure state to be declared in the team  such that the selected agents know to begin their task. of course  one problem with this approach is that it requires all agents to be notified of the failure. a second technique that circumvents this need is to leave the diagnosis in the hands of those agents that have detected the failure  and allow them to proceed with the diagnosis without necessarily alerting the others unless absolutely necessary. 
　we present a novel third approach  in which selection of the diagnosing agent is based on its team-members' estimate of the number of queries that it will send out in order to arrive at a diagnosis  i.e.  the number of queries that it will send out in the disambiguation phase of the diagnosis  previous section . the key to this approach is for each agent to essentially simulate its own reasoning in the second phase  as well as that of its teammates. agents can then jointly select the agent with the best simulated results  i.e.  the minimal number of queries . 
　surprisingly  all agents can make the same selection without communicating  using a recursive modelling technique in which each agent models itself through its model of its teammates. this proceeds as follows. first  each agent uses the belief recognition algorithm to generate the hypotheses space for each team-member other than itself. to determine its own hypotheses space  as it appears to its peers   each agent uses recursive modelling  putting itself in the position of one of its teammates and running the belief recognition process described above with respect to itself. under the assumptions that all agents utilize the same algorithm  and have access to the same observations  an agent's recursive model will yield the same results as the modelling process of its peers. at this point  each team-member can determine the agent with the minimal number of expected queries  the queries that split the space of the queries . in order to guarantee an agreement on the selected agent  each team-member has an id number  
　
which is determined and known in advance. in case there are two agents with the same minimal number of expected queries  the agent with the minimal id is selected. this entire process is carried out strictly based on team-members' observations of one another  with no communications other than an announcement of a disagreement. 
　for instance  suppose there are three agents a  b and c. to determine the diagnosing agent  a puts itself in b's position and considers the hypotheses b has about a and c  given a's observable actions. a also uses the belief recognition process described earlier to determine the number of hypotheses available about b's beliefs  c's beliefs  etc. it now simulates selecting queries by each agent  and selects the agent  say  c  with the minimal number of expected queries. b  and c also run the same process  and under the assumption that each agent's actions are equally observable to all  will arrive at the same conclusion. 
　summary. we presented a space of social diagnosis algorithms: each such algorithm operates in two phases  and we presented alternative techniques for each phase. for the selection of the diagnosing agent  we have the following methods:  i  rely on pre-selection by the designer;  ii  let the agents that detected the fault do the diagnosis; or  iii  choose the agent most likely to reduce communications  using the distributed recursive modelling technique described . in terms of computing the diagnosis  two choices are available: either have all agents communicate their beliefs to the selected agents  or allow the diagnosing agents to actively query agents as to the state of their beliefs  while minimizing the number of queries as described above. 
1 	evaluation and discussion 
the design alternatives define a space of diagnosis algorithms. this section evaluates four intuitive design decisions in this space  and draws lessons about the effects of specific design choices on overall computation and communication overhead. 
　method 1. the first design choice corresponds to arguably the most intuitive diagnosis algorithm  in which all agents are pre-selected to carry out the diagnosis. when a failure is detected  and is made known to all agents  each agent communicates all its relevant beliefs to the others so that each and every team-member has a copy of all beliefs  and therefore can do the disambiguation itself. 
　method 1. method 1 uses redundant communications to achieve this distribution  while arguably only a single agent really needs to have the final diagnosis in order to begin a recovery process. thus in method 1  the agents that detected the disagreement automatically take it upon themselves to carry out the diagnosis  unbeknownst to their teammates  and to each other. because their team-mates do not know of the disagreement  or who has been selected to diagnose it   they cannot rely on their team-mates to communicate their beliefs without being queried  in phase 1 . instead  they use the querying algorithm discussed in the previous section. however  because the diagnosing agents also do not know of each other  in cases where more than one agent detects the disagreement  all such agents will query the other teammembers. 
　method 1. the next design choice we examine corresponds to a diagnosis algorithm  in which the designer preselects a neutral agent. when a failure is discovered  and is made known to all agents   all team-members communicate all their relevant beliefs to the pre-selected agent. 
　method 1. a final algorithm attempts to alleviate the communications overhead. it uses the recursive modelling technique to have all team-members agree on which agent is to carry out the diagnosis  this requires the detection of the disagreement to be made known . once the agent is selected  with no communications   it queries its teammates as needed. 
　table 1 summarizes the different methods. each method is presented in a different row. the columns correspond to the different phases of the diagnosis process. the choice of algorithm is presented in each entry  along with a marking that signifies the number of agents that execute the selected technique for the phase in question. 

table 1: summary of evaluated diagnosis methods 
　for instance row 1 should be read as follows: in method 1  the agents selected to perform the disambiguation are those who detected the disagreement. k such agents exist  where k is smaller or equal than the total number of agents in the team  n   and they each execute a minimal-queries algorithm. in contrast  row 1 indicates that a single pre-selected agent executes the diagnosis  and it relies on reports from all agents to carry out the diagnosis. 
　focusing on diagnosing teams of behavior-based agents in the modsaf domain  we performed experiments in which the different diagnosis methods were systematically tested on thousands of failure cases  varying the number of agents  the behaviors selected by each agent  and the roles of the agents. the experiments were executed with teams of two to ten agents. for each n agents there are three sets of tests:  1  one attacker and n-1 scouts;  1  n-1 attackers and one scout;  1  n/1 attackers and n/1 scouts. for each set of x attackers and y scouts  we systematically checked all possible disagreement cases for all team behaviors  a total 1 tests for each method . in each test we recorded the number of messages sent  and runtime of the diagnosis process. 
　in table 1 we present the results of a single test  where one scout and two attackersy y in formation  when the scout transitions to the wait-at-point behavior while the attackers continue to fly. the diagnosis is that the scout detected the way-point  while the attackers did not. 
diagnosis 	1 　the first column in table 1 reports the method used. the second column presents the number of messages sent reporting on beliefs  or querying about their truth  one message per belief . the third column reports the number of messages sent informing agents of the existence of failures  we assume point-to-point communications . the next three columns summarize the runtime of each agent. 

table 1: results of diagnosing a specific failure case 
　for instance  the number of messages reporting on beliefs for method 1 is 1  and 1 failure messages were sent  i.e.  one of the agents detected the failure and informed the others . the runtime of all the teammates for method 1 is 1 milliseconds  except for the scout  which disambiguated the beliefs in this case  and therefore took an additional 1 milliseconds  for a total of 1 milliseconds . 

figure 1: average number of messages per failure case 
figures 1 and 1 summarize the results of the experiments. 
in both figures  the x axis shows the number of agents in the diagnosed team. figure 1 presents the average number of belief messages for the different failure and role variations  for each team size  failure messages were ignored in the figure  since their effect is negligible . figure 1 presents the average run-time  in milliseconds  of those same tests. the run-time of each test was taken as the maximum of any of the agents in the test. 
　both figures show interesting grouping of the evaluated techniques. in figure 1  number of messages   methods 1 and 1 show a slow  approximately-linear growth  while methods 1 and 1 show a much faster non-linear growth. in figure 1  runtime   the grouping is different: methods 1 and 1 grow significantly slower than methods 1 and 1. 
　the first conclusion we draw from these figures is that runtime is affected by the choice of a disambiguation method  figure 1  ref. table 1 . methods  here  methods 1 and 1  that rely on the agents to report their relevant beliefs do not reason about the hypothesized beliefs of others. therefore  

figure 1: average run-time per failure case 
their run-time is much smaller than methods  here  methods 1 and 1  which hypothesize about the beliefs of others. however  as figure 1 shows  the goal of reducing communications is actually achieved  as method 1 does indeed result in less communications then method 1. the question of whether the cost in run time is worth the reduction in communications is dependent on the domain. 
　indeed  we draw a second conclusion from figure 1. despite the additional savings provided by the minimal query algorithm  the choice of a centralized diagnosing agent is the main factor in qualitatively reducing the number of messages sent  as well as in shaping the growth curve as the number of agents is scaled up. these results contrast sharply with previous work in disagreement detection  in which distributed algorithms reduce communications  kaminka and tambe  1 . 
1 	related work 
while diagnosis of a single-agent system is relatively well understood  and known to be computationally difficult  social diagnosis  diagnosis of social failures such as disagreements and coordination failures  remains an open area of research. in particular  to our best knowledge  an in-depth exploration of design choices in terms of communication and computation has not been done before. the most closely-related work to ours is reported in  kaminka and tambe  1 . this previous investigation provides guarantees on detection of disagreements  but only presented a heuristic approach to diagnosis  which indeed does not always succeed. the algorithms we present here succeed in the same examples where the previous heuristic approach fails. parker  reports on a behavior-based architecture which is very robust and is able to recover from failures by having robots take over tasks from failing teammates. this is done using continuous communications  but with an out explicit diagnosis process such as those described in this paper. 
　dellarocas and klein  described failures handling services that use a knowledge base of generic exceptions and a decision tree of diagnoses; the diagnosis process is done by a centralized agent traversing down the tree by asking questions about the relevant problem. similarly  horling  uses a casual model of failures and diagnoses to detect and respond to multi-agent failures. both investigations do not address communications overhead. 
　frohlich  suggested dividing a spatially distributed system into regions  each under the responsibility of a diagnosis agent. if the fault depends on two regions the agents that are responsible to those regions cooperate in making the diagnosis. this method is inappropriate for dynamic team settings  where agents cannot pre-select their communication partners. roos  expanded frohlich's work to semantically distributed systems  where each agent looks at different aspects of the whole system. in this system each agent makes diagnosis separately and the correct diagnosis is found by exchanging the diagnoses between the agents. however  the communication links are fixed  such that each failure is diagnosed strictly by the agents that are associated with its communication link. in contrast  in teams disagreements can arise between any two agents  and thus fixed communication commitments cannot be made in design time. 
　brodic  tries to minimize communication costs in computer networks. he uses intelligence probes which are sent through the network in order to query remote nodes. by combining the results of different probes  failing nodes can be identified and isolated. thus brodie's work essentially determines the liveliness status of agents  while our work focuses on fine-grain diagnosis of causes for disagreements  in terms of contradictory beliefs held by different agents. 
1 	summary and future work 
in this paper we present a novel design space for algorithms of social diagnosis  and evaluate specific design decisions in terms of their communications and computation overheads. we presented four methods of diagnosing a team of behaviorbased agents  using familiar and novel algorithms  among these an algorithm that minimizes the number of diagnosis queries by using a recursive modelling technique to select the agent which will use the smallest number of queries. we then empirically and systematically evaluated the different combinations to draw general conclusions about the design of diagnosis algorithms. a first conclusion is that centralizing the diagnosis disambiguation task is critical in reducing communications. furthermore  techniques where agents reason explicitly about the beliefs of their peers are computationally inferior  in run-time  to techniques where agents do not reason about others. however  such computation does result in a slight reduction in communications. 
　all methods find only the contradictions between agent beliefs  where the beliefs are derived directly from the hypothesized behaviors. but in complex behavior-based control systems  chains of inference may lead from one belief to the next. our system is currently not able to back chain through such inference pathways  and thus is unable to draw conclusions beyond the beliefs that immediately tied to preconditions and termination conditions. we plan to tackle this challenge in the future. 
acknowledgments 
we thank meirav hadad for help in the submission process. as always  thanks to k. ushi and k. raviti. 
