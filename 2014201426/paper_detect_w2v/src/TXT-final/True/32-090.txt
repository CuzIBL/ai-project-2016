 
a large portion of real-world data is stored in commercial relational database systems. in contrast  most statistical learning methods work only with  flat  data representations. thus  to apply these methods  we are forced to convert our data into a flat form  thereby losing much of the relational structure present in our database. this paper builds on the recent work on probabilistic relational models  prms   and describes how to learn them from databases. prms allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. although prms are significantly more expressive than standard models  such as bayesian networks  we show how to extend well-known statistical methods for learning bayesian networks to learn these models. we describe both parameter estimation and structure learning - the automatic induction of the dependency structure in a model. moreover  we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. we present experimental results on both real and synthetic relational databases. 
1 	introduction 
relational models are the most common representation of structured data. enterprise business information  marketing and sales data  medical records  and scientific datasets are all stored in relational databases. indeed  relational databases are a multi-billion dollar industry. recently  there has been growing interest in making more sophisticated use of these huge amounts of data  in particular mining these databases for certain patterns and regularities. by explicitly modeling these regularities  we can gain a deeper understanding of our domain and may discover useful relationships. we can also use our model to  fill in  unknown but important information. for 

*the institute of computer science  hebrew university  
jerusalem 1  israel 
　　+ computer science department  stanford university  gates building 1a  stanford ca 1 
example  we may be interested in predicting whether a person is a potential money-launderer based on their bank deposits  international travel  business connections and arrest records of known associates  jensen  1 . in another case  we may be interested in classifying web pages as belonging to a student  a faculty member  a project  etc.  using attributes of the web page and of related pages  craven et al.  1 . 
　unfortunately  few inductive learning algorithms are capable of handling data in its relational form. most are restricted to dealing with a flat set of instances  each with its own separate attributes. to use these methods  one typically  flattens  the relational data  removing its richer structure. this process  however  loses information which might be crucial in understanding the data. consider  for example  the problem of predicting the value of an attribute of a certain entity  e.g.  whether a person is a money-launderer. this attribute will be correlated with other attributes of this entity  as well as with attributes of related entities  e.g.  of financial transactions conducted by this person  of other people involved in these transactions  of other transactions conducted by these people  etc. in order to  flatten  this problem  we would need to decide in advance on a fixed set of attributes that the learning algorithm can use in this task. thus  we want a learning algorithm that can deal with multiple entities and their properties  and can reach conclusions about an entity's characteristics based on the properties of the entities to which it is related. until now  inductive logic programming  ilp   lavrac and dzeroski  1  has been the primary learning framework with this capability. ilp algorithms learn logical horn rules for determining when some first-order predicate holds. while ilp is an excellent solution in many settings  it may be inappropriate in others. the main limitation is the deterministic nature of the rules discovered. in many domains  such as the examples above  we encounter interesting correlations that are far from being deterministic. 
　our goal in this paper is to learn more refined probabilistic models  that represent statistical correlations both between the properties of an entity and between the properties of related entities. such a model can then be used for reasoning about an entity using the entire rich structure of knowledge encoded by the relational representation. 
　the starting point for our work is the structured representation of probabilistic models  as exemplified in bayesian networks  bns . a bn allows us to provide a compact rep-

1 	uncertainty and probabilistic reasoning 

resentation of a complex probability distribution over some fixed set of attributes or random variables: the representation exploits the locality of influence that is present in many domains. we build on two recent developments in the field of bayesian networks. the first is the deep understanding of the statistical learning problem in such models  heckerman* 1; heckerman et al.  1  and the role of structure in providing an appropriate bias for the learning task. the second is the recent development of representations that extend the attribute-based bn representation to incorporate a much richer relational structure  koller and pfeffer  1; 
ngo and haddawy  1; poole  1   in this paper  we combine these two advances. indeed  one 
of our key contributions is to show that many of the techniques of bayesian network learning can be extended to the task of learning these more complex models. this contribution generalizes  koller and pfeffer  1 's preliminary work on this topic. we start by describing the semantics of probabilistic relational models. we then examine the problems of parameter estimation and structure selection for this class of models. we deal with some crucial technical issues that distinguish the problem of learning relational probabilistic models from that of learning bayesian networks. we provide a formulation of the likelihood function appropriate to this setting  and show how it interacts with the standard assumptions of bn learning. the search over coherent dependency structures is significantly more complex than in the case of learning bn structure and we introduce the necessary tools and concepts to do this effectively. we then describe experimental results on synthetic and real-world datasets  and finally discuss possible extensions and applications. 
1 underlying framework 
1 relational model 
we describe our relational model in generic terms  closely related to the language of entity-relationship models. this generality allows our framework to be mapped into a variety of specific relational systems  including the probabilistic logic programs of  ngo and haddawy  1; poole  1   and the probabilistic frame systems of  koller and pfeffer  1 . our learning results apply to all of these frameworks. 
　　the vocabulary of a relational model consists of a set of classes and a set of relations each entity type is associated with a set of attributes each attribute takes on values in some fixed domain of values each relation r is typed. this vocabulary defines a schema for our relational model. 
　consider a simple genetic model of the inheritance of a single gene that determines a person's blood type. each person has two copies of the chromosome containing this gene  one inherited from her mother  and one inherited from her father. there is also a possibly contaminated test that attempts to recognize the person's blood type. our schema contains two classes person and blood-test  and three relations father  mother  and test-of. attributes of person are name  gender  
p-chromosome  the chromosome inherited from the father   
m-chrvmosome  inherited from the mother . the attributes of blood-test are serial-number  date  contaminated  and result. 
an instance of a schema defines a set of entities 
for each entity type for each entity and each attribute the instance has an associated attribute its value in is denoted for each relation and each 
	i specifies whether 	holds. 
　we are interested in describing a probability model over instances of a relational schema. however  some attributes  such as a name or social security number  are fully determined. we label such attributes as fixed. we assume that they are known in any instantiation of the schema. the other attributes are called probabilistic. a skeleton structure  of a relational schema is a partial specification of an instance of the schema. it specifies the set of objects  for each class  the values of the fixed attributes of these objects  and the relations that hold between the objects. however  it leaves the values of probabilistic attributes unspecified. a completion of the skeleton structure  extends the skeleton by also specifying the values of the probabilistic attributes. 
　　one final definition which will turn out to be useful is the notion of a slot chain. if is any relation  we can project r onto its -th and -th arguments to obtain a binary relation which we can then view as a slot of for any in we let denote all the elements in such that holds.  in relational algebra notation 
objects in this set are called -relatives 
of we can concatenate slots to form longer slot chains defined by composition of binary relations. 
 each of the 	's in the chain must be appropriately typed.  
1 	probabilistic relational models 
we now proceed to the definition of probabilistic relational models  prms . the basic goal here is to model our uncertainty about the values of the non-fixed  or probabilistic  attributes of the objects in our domain of discourse. in other words  given a skeleton structure  we want to define a probability distribution over all completions of the skeleton. 
　our probabilistic model consists of two components: the qualitative dependency structure   and the parameters associated with it   the dependency structure is defined by associating with each attribute x.a a set of parents pa x.a . these correspond to formal parents; they will be instantiated in different ways for different objects in x. intuitively  the parents are attributes that are  direct influences  on x.a. 
　we distinguish between two types of formal parents. the attribute x.a can depend on another probabilistic attribute b of x. this formal dependence induces a corresponding dependency for individual objects: for any object x in 
x.a will depend probabilistically on x.b. the attribute can also depend on attributes of related objects where  is a slot chain. to understand the semantics of this formal 
dependence for an individual object recall that represents the set of objects that are -relatives of except in cases where the slot chain is guaranteed to be single-valued  we must specify the probabilistic dependence of on the multiset  the notion of aggregation from database theory gives us precisely the right tool to address 
friedman  getoor  koller  and pfeffer 	1 


figure 1: the prm structure for a simple genetics domain. fixed attributes are shown in regular font and probabilistic attributes are shown in italic. dotted lines indicate relations between entities and solid arrows indicate probabilistic dependencies. 
this issue; i.e.  x.a will depend probabilistically on some aggregate property of this multiset. there are many natural and useful notions of aggregation: the mode of the set  most frequently occurring value ; mean value of the set  if values are numerical ; median  maximum  or minimum  if values are ordered ; cardinality of the set; etc. 
　more formally  our language allows a notion of an aggregate takes a multiset of values of some ground type  and returns a summary of it. the type of the aggregate can be the same as that of its arguments. however  we allow other types as well  e.g.  an aggregate that reports the size of the multiset. 
we allow to have as a parent  the semantics is that for any will depend on the value of  we define in the obvious way. 
returning to our genetics example  consider the attribute 
blood-test.result. since the result of a blood test depends on whether it was contaminated  it has blood-test.contaminated as a parent. the result also depends on the genetic material of the person tested. since test-of is single-valued  we add blood-test.test-of m-chrvmosome and blood-test.test-of .p-chromosome as parents. figure 1 shows the structure of a simple prm for this domain. 
　　given a set of parents for we can define a local probability model for we a s s o c i a t e w i t h  a conditional probability distribution  cpd  that specifies 
 more precisely  let u be the set of par-
ents of x.a. recall that each of these parents whether a simple attribute in the same relation or an aggregate of a set of  relatives - has a set of values in some ground type. for each tuple of values the cpd specifies a distribution the parameters in 
all of these cpds comprise  
　　given a skeleton structure for our schema  we want to use these local probability models to define a probability distribution over completions of the skeleton. first  note that the skeleton determines the set of objects in our model. we associate a random variable with each probabilistic attribute a of each object the skeleton also determines the relations between objects  and thereby the set of -relatives associated with every object for each relationship chain  also note that by assuming that the relations between objects are always specified by  we are disallowing uncertainty over the relational structure of the model. 
　　to define a coherent probabilistic model over this skeleton  we must ensure that our probabilistic dependencies are acyclic  so that a random variable does not depend  directly or indirectly  on its own value. consider the parents of an attribute xa. when x.b is a parent of x.a  we define an edge w h e n i s a parent of x.a and we define an edge we say that a dependency structure is acyclic relative to a skeleton if the directed graph defined by   over the variables is acyclic. in this case  we can define a coherent probabilistic model over complete instantiations 
 1  
　we briefly sketch a proof of this proposition  by showing how to construct a bn over the probabilistic attributes of a 
　skeleton using  this construction is reminiscent of the knowledge-based model construction approach  wellman et al.  1 . here  however  the construction is merely a thought-experiment; our learning algorithm never constructs this network. in this network there is a node for each variable and for aggregate quantities required by parents. the parents of these aggregate random variables are all of the attributes that participate in the aggregation  according to the relations specified by  the cpds of random variables that correspond to probabilistic attributes are simply the cpds described by  and the cpds of random variables that correspond to aggregate nodes capture the deterministic function of the particular aggregate operator. it is easy to verify that if the probabilistic dependencies are acyclic  then so is the induced bayesian network. this construction also suggests one way of answering queries about a relational model. we can  compile  the corresponding bayesian network and use standard tools for answering queries about it. 
although for each skeleton  we can compile a prm into a 
bayesian network  a prm expresses much more information than the resulting bn. a bn defines a probability distribution over a fixed set of attributes  a prm specifies a distribution over any skeleton; in different skeletons  the set  and number  of entities in the domain will vary  as will the relations between the entities. in a way  prms are to bns as a set of 
rules in first-order logic is to a set of rules in propositional logic: a rule such as .parent  
grandparent induces a potentially infinite set of ground  propositional  instantiations. 
1 	parameter estimation 
we now move to the task of learning prms. we begin with learning the parameters for a prm where the dependency 

1 	uncertainty and probabilistic reasoning 

structure is known. in other words  we are given the structure s that determines the set of parents for each attribute  and our task is to learn the parameters  that define the cpds for this structure  our learning is based on a particular training set  which we will take to be a complete instance  while this task is relatively straightforward  it is of interest in and of itself. in addition  it is a crucial component in the structure learning algorithm described in the next section. 
　the key ingredient in parameter estimation is the likelihood function  the probability of the data given the model. this function captures the response of the probability distribution to changes in the parameters. as usual  the likelihood of a parameter set is defined to be the probability of the data given the model:  as usual  we typically work with the log of this function: 

　　the key insight is that this equation is very similar to the log-likelihood of data given a bayesian network  heckerman  1 . in fact  it is the likelihood function of the bayesian network induced by the structure given the skeleton. the main difference from standard bayesian network parameter learning is that parameters for different nodes in the network are forced to be identical. thus  we can use the well-understood theory of learning from bayesian networks. consider the task of performing maximum likelihood parameter estimation. here  our goal is to find the parameter setting that maximizes the likelihood  for a given and  this estimation is simplified by the decomposition of log-likelihood function into a summation of terms corresponding to the various attributes of the different classes. each of the terms in the square brackets in  1  can be maximized independently of the rest. hence  maximal likelihood estimation reduces to independent maximization problems  one for each cpd. 
　for multinomial cpds  maximum likelihood estimation can be done via sufficient statistics which in this case are just the counts  of the different values u that the attribute x.a and its parents can jointly take. 
proposition 1: assuming multinomial cpds  the maximum 

　as a consequence of this proposition  parameter learning in prms is reduced to counting sufficient statistics. we need to count one vector of sufficient statistics for each cpd. such counting can be done in a straightforward manner using standard databases queries. 
　note that this proposition shows that learning parameters in prms is very similar to learning parameters in bayesian networks. in fact  we might view this as learning parameters for the bn that the prm induces given the skeleton. however  as discussed above  the learned parameters can then be used for reasoning about other skeletons  which induce a completely different bn. 
　　in many cases  maximum likelihood parameter estimation is not robust  as it overfits the training data. the bayesian approach uses a prior distribution over the parameters to smooth the irregularities in the training data  and is therefore significantly more robust. as we will see in section 1  the bayesian framework also gives us a good metric for evaluating the quality of different candidate structures. due to space limitations  we only briefly describe this alternative approach. roughly speaking  the bayesian approach introduces a prior over the unknown parameters  and performs bayesian conditioning  using the data as evidence  to compute a posterior distribution over these parameters. to apply this idea in our setting  recall that the prm parameters are composed of a set of individual probability distribution for each conditional distribution of the form 
u . following the work on bayesian approaches for learning bayesian networks  heckerman  1   we make two assumptions. first  we assume parameter independence: the priors over the parameters ' for the different x  a and are independent. second  we assume that the prior over is a dirichlet distribution. briefly  a dirichlet prior for a multinomial distribution of a variable w is specified by a set of hyperparameters a distribution on 
the parameters of p w  is dirichlet if 
 for more details see  degroot  1 .  
　for a parameter prior satisfying these two assumptions  the posterior also has this form. that is  it is a product of independent dirichlet distributions over the parameters which can be computed easily. 
proposition 1 if is a complete assignment  and the prior 
satisfies parameter independence and dirichlet with hyper-
parameters  then the posterior 
is a product of dirichlet distributions with hyperparameters 

　once we have updated the posterior  how do we evaluate the probability of new data  in the case of bn learning  we assume that instances are iid  which implies that they are independent given the value of the parameters. thus  to evaluate a new instance  we only need the posterior over the parameters. the probability of the new instance is then the probability given every possible parameter value  weighted by the posterior probability over these values. in the case of bns  this term can be rewritten simply as the instance probability according to the expected value of the parameters  i.e.  the mean of the posterior dirichlet for each parameter . this suggests that we might use the expected parameters for evaluating new data. indeed  the formula for the expected parameters is analogous to the one for bns: 
proposition 1: 	assuming multinomial cpds  prior independence  and dirichlet priors  with hyperparameters we have that: 
friedman  getoor  koller  and pfeffer 	1 

　unfortunately  the expected parameters are not the proper bayesian solution for computing probability of new data. there are two possible complications. 
the first problem is that  in our setting  the assumption of 
iid data is often violated. specifically  a new instance might not be conditionally independent of old ones given the parameters. consider the genetics domain  and assume that our new data involves information about the mother  of some person already in the database. in this case  the introduction of the new object also changes our probability about the attributes of we therefore cannot simply use our old posterior about the parameters to reason about the new instance. this problem does not occur if the new data is not related to the training data  that is  when the new data is essentially a disjoint database with the same scheme. more interestingly  the problem also disappears when attributes of new objects are not parents of any attribute in the training set. in the genetics example  this means that we can insert new people into our database  as long as they are not ancestors of people already in the database. 
　the second problem involves the formal justification for using expected parameters values. this argument depends on the fact that the probability of a new instance is linear in the value of each parameter. that is  each parameter is  used  at most once. this assumption is violated when we consider the probability of a complex database involving multiple instances from the same class. in this case  our integral of the probability of the new data given the parameters can no longer be reduced to computing the probability relative to the expected parameter value. the correct expression is called the marginal likelihood of the  new  data; we use it in section 1 for scoring structures. for now  we note that if the posterior is sharply peaked  i.e.  we have seen many training instances   we can approximate this term by using the expected parameters of proposition 1  as we could for a single instance. in practice  we will often use these expected parameters as our learned model. 
1 structure selection 
we now move to the more challenging problem of learning a dependency structure automatically  as opposed to having it given by the user. there are three important issues that need to be addressed. we must determine which dependency structures are legal; we need to evaluate the  goodness  of different candidate structures; and we need to define an effective search procedure that finds a good structure. 
1 	legal structures 
when we consider different dependency structures  it is important to be sure that the dependency structure we choose results in coherent probability models. to guarantee this property  we see from proposition 1 that the skeleton must be acyclic relative to of course  we can easily verify for a given candidate structure that it is acyclic relative to the skeleton  of our training database. however  we also want to guarantee that it will be acyclic relative to other databases that we may encounter in our domain. how do we guarantee acyclicity for an arbitrary database  a simple approach is to ensure that dependencies among attributes respect some order  i.e.  are stratified . more precisely  we say that directly depends on if either  a   and is a parent of  or b  is a parent of x. aand the -relatives 
of x are of class y. we then require that x.a directly depends only on attributes that precede it in the order. 
　while this simple approach clearly ensures acyclicity  it is too limited to cover many important cases. consider again our genetic model. here  the genotype of a person depends on the genotype of her parents; thus  we have person.p-chmmosome depending directly on person.p-chromosome  which clearly violates the requirements of our simple approach. in this model  the apparent cyclicity at the attribute level is resolved at the level of individual objects  as a person cannot be his/her own ancestor. that is  the resolution of acyclicity relies on some prior knowledge that we have about the domain. to allow our learning algorithm to deal with dependency models such as this we must allow the user to give our algorithm prior knowledge. we allow the user to assert that certain slots are guaranteed acyclic; i.e.  we are 
guaranteed that there is a partial ordering such that if is a relative for some o f t h e n we 
say that is guaranteed acyclic if each of its components is guaranteed acyclic. 
　　we use this prior knowledge determine the legality of certain dependency models. we start by building a graph that describes the direct dependencies between the attributes. in this graph  we have a yellow edge x.b  x.a if x.b is a parent of x.a. if is a parent of x.a  we have an edge which is green i f i s guaranteed acyclic and red otherwise.  note that there might be several edges  of different colors  between two attributes . the intuition is that dependency along green edges relates objects that are ordered by an acyclic order. thus these edges by themselves or combined with intra-object dependencies  yellow edges  cannot cause a cyclic dependency. we must take care with other dependencies  for which we do not have prior knowledge  as these might form a cycle. this intuition suggests the following definition: a  colored  dependency graph is stratified if every cycle in the graph contains at least one green edge and no red edges. 
proposition 1: if the colored dependency graph of and is stratified  then for any skeleton for which the slots are jointly acyclic defines a coherent probability 
distribution over assignments to  
　this notion of stratification generalizes the two special cases we considered above. when we do not have any guaranteed acyclic relations  all the edges in the dependency graph are colored either yellow or red. thus  the graph is stratified if and only if it is acyclic. in the genetics example  all the relations would be in thus  it suffices to check that dependencies within objects  yellow edges  are acyclic. 
proposition 1: stratification of a colored graph can be determined in time linear in the number of edges in the graph. 
we omit the details of the algorithm for lack of space  but it relies on standard graph algorithms. finally  we note that it 

1 	uncertainty and probabilistic reasoning 

is easy to expand this definition of stratification for situations where our prior knowledge involves several sets of guaranteed acyclic relations  each set with its own order  e.g.  ob-
jects on a grid with a north-south ordering and an east-west ordering . we simply color the graph with several colors  and check that each cycle contains edges with exactly one color other than yellow  except for red. 
1 	evaluating different structures 
now that we know which structures are legal  we need to decide how to evaluate different structures in order to pick one that fits the data well. we adapt bayesian model selection methods to our framework. formally  we want to compute the posterior probability of a structure given an instantiation using bayes rule we have that  
this score is composed of two main parts: 
the prior probability of the structure  and the probability of the data assuming that structure. 
the first component is  which defines a prior 
over structures. we assume that the choice of structure is independent of the skeleton  and thus  in the context of bayesian networks  we often use a simple uniform prior over possible dependency structures. unfortunately  this assumption does not work in our setting. the problem is that there may be infinitely many possible structures. in our genetics example  a person's genotype can depend on the genotype of his parents  or of his grandparents  or of his greatgrandparents  etc. a simple and natural solution penalizes long indirect slot chains  by having log p  proportional to the sum of the lengths of the chains  appearing in s. the second component is the marginal likelihood: 

if we use a parameter independent dirichlet prior  as above  this integral decomposes into a product of integrals each of which has a simple closed form solution.  this is a simple generalization of the ideas used in the bayesian score for bayesian networks.  
proposition 1: if  is a complete assignment  and p  satisfies parameter independence and is dirichlet with hy-
  
　　hence  the marginal likelihood is a product of simple terms  each of which corresponds to a distribution  where  moreover  the term for  
depends only on the hyperparameters and the sufficient statistics for  
　the marginal likelihood term is the dominant term in the probability of a structure. it balances the complexity of the structure with its fit to the data. this balance can be made explicitly via the asymptotic relation of the marginal likelihood to explicit penalization  such as the mdl score  see  e.g.  {heckerman  1  . 
　finally  we note that the bayesian score requires that we assign a prior over parameter values few each possible structure. since there are many  perhaps infinitely many  alternative structures  this is a formidable task. in the case of bayesian networks  there is a class of priors that can be described by a single network  heckerman et al  1 . these priors have the additional property of being structure equivalent  that is  they guarantee that the marginal likelihood is the same for structures that are  in some strong sense  equivalent. these notions have not yet been defined for our richer structures  so we defer the issue to future work. instead  we simply assume that some simple dirichlet prior  e.g.  a uniform one  has been defined for each attribute and parent set. 
1 	structure search 
now that we have a test for determining whether a structure is  legal   and a scoring function that allows us to evaluate different structures  we need only provide a procedure for finding legal high-scoring structures. for bayesian networks  we know that this task is np-hard  chickering  1 . as prm learning is at least as hard as bn learning  a bn is simply a prm with one class and no relations   we cannot hope to find an efficient procedure that always finds the highest scoring structure. thus  we must resort to heuristic search. the simplest such algorithm is greedy hill-climbing search  using our score as a metric. we maintain our current candidate structure and iteratively improve it. at each iteration  we consider a set of simple local transformations to that structure  score all of them  and pick the one with highest score. we deal with local maxima using random restarts. 
　　as in bayesian networks  the decomposability property of the score has significant impact on the computational efficiency of the search algorithm. first  we decompose the score into a sum of local scores corresponding to individual attributes and their parents. now  if our search algorithm considers a modification to our current structure where the parent set of a single attribute is different  only the component of the score associated with . will change. thus  we need only reevaluate this particular component  leaving the others unchanged; this results in major computational savings. 
　there are two problems with this simple approach. first  as discussed in the previous section  we have infinitely many possible structures. second  even the atomic steps of the search are expensive; the process of computing sufficient statistics requires expensive database operations. even if we restrict the set of candidate structures at each step of the search  we cannot afford to do all the database operations necessary to evaluate all of them. 
　　we propose a heuristic search algorithm that addresses both these issues. at a high level  the algorithm proceeds in phases. at each phase we have a set of potential parents  for each attribute we then do a standard 
structure search restricted to the space of structures in which the parents of each are in  the advantage of 
this approach is that we can precompute the view correspond-
friedman  getoor  roller  and pfeffer 	1 

ing to most of the expensive computations 
- the joins and the aggregation required in the definition of the parents - are precomputed in these views. the sufficient statistics for any subset of potential parents can easily be derived from this view. the above construction  together with the decomposability of the score  allows the steps of the search  say  greedy hill-climbing  to done very efficiently. 
　the success of this approach depends on the choice of the potential parents. clearly  a wrong initial choice can result to poor structures. following {friedman et al.  1   which examines a similar approach in the context of learning bayesian networks  we propose an iterative approach that starts with some structure  possibly one where each attribute does not have any parents   and select the sets  based on this structure. we then apply the search procedure and get a new  higher scoring  structure. we choose new potential parents based on this new structure and reiterate  stopping when no further improvement is made. 
　　it remains only to discuss the choice of  at the different phases. perhaps the simplest approach is to begin by setting to be the set of attributes in in successive phases  would consist of all of  
as well as all attributes that are related to via slot chains of length  of course  these new attributes would require aggregation; we sidestep the issue by predefining possible aggregates for each attribute. 
　　this scheme expands the set of potential parents at each iteration. however  it usually results in large set of potential parents. thus  we actually use a more refined algorithm that only adds parents to if they seem to  add value  beyond there are several reasonable ways of evaluating the additional value provided by new parents. some of these are discussed in  friedman et al  1  in the context of learning bayesian networks. their results suggest that we should evaluate a new potential parent by measuring the change of score for the family of  if we add the  to its current parents. we then choose the highest 
scoring of these  as well as the current parents  to be the new set of potential parents. this approach allows us to significantly reduce the size of the potential parent set  and thereby of the resulting view  while being unlikely to cause significant degradation in the quality of the learned model. 
1 	implementation and experimental results 
we implemented our learning algorithm on top of the postgres object-relational database management system. all required counts were obtained simply through database selection queries  and cached to avoid performing the same query twice. during the search process  we created temporary materialized views corresponding to joins between different relations  and these views were then used for computing the 
counts. 
　we tested our proposed learning algorithm on two domains  one real and one synthetic. the two domains have very different characteristics. the first is a movie database1 that contains three relations: movie  actor and appears  which relates actors to movies in which they played. the 

database contains about 1 movies and 1 actors. while this database has a simple structure  it presents the kind of problems one of ten encounters when dealing with real data: missing values  large domains for attributes  and inconsistent use of values. the fact that our algorithm was able to deal with this kind of real-world problem is quite promising. our algorithm learned the model shown in figure 1 a . this model is reasonable  and close to one that we would consider to be  correct . it learned that the genre of a movie depended on its decade and its film process  color  black & white  technicolor etc.  and that the decade depended on its film process. it also learned an interesting dependency combining all three relations: the role-type played by an actor in a movie depends on the gender of the actor and the genre of the movie. 
　the second database  an artificial genetic database similar to the example in this paper  presented quite different challenges. for one thing  the recursive nature of this domain allows arbitrarily complex joins to be defined. in addition  the probabilistic model in this domain is fairly subtle. each person has three relevant attributes - p-chromosome  mchrvmosome  and bloodtype - all with the same domain and all related somehow to the same attributes of the person's mother and father. the gold standard is the model used to generate the data; the structure of that model was shown earlier in figure 1. we trained our algorithm on datasets of various sizes ranging up to 1. a data set of size  consisted of a family tree containing  people  with an average of 1 blood tests per person. we evaluated our algorithm on a test set of size 1. figure 1 b  shows the log-likelihood of the test set for the learned models. in most cases  our algorithm learned a model with the correct structure  and scored well. however  in a small minority of cases  the algorithm got stuck in local maxima  learning a model with incorrect structure that scored quite poorly. this can be seen in the scatter plots of figure 1 b  which show that the median log-likelihood of the learned models is quite reasonable  but there are a few outliers. standard techniques such as random restarts can be used to deal with local maxima. 
1 discussion and conclusions 
in this paper  we defined a new statistical learning task: learning probabilistic relational models from data. we have shown that many of the ideas from bayesian network learning carry over to this new task. however  we have also shown that it also raises many new challenges. 
　scaling these ideas to large databases is an important issue. we believe that this can be achieved by a closer integration with the technology of database systems  including indices and query optimization. furthermore  there has been a lot of recent work on extracting information from massive data sets  including work on finding frequently occurring combinations of values for attributes. we believe that these ideas will help significantly in the computation of sufficient statistics. 
　there are also several important possible extensions to this work. perhaps the most obvious one is the treatment of missing data and hidden variables. we can extend standard techniques  such as expectation maximization for missing data  

1 	uncertainty and probabilistic reasoning 


figure 1:  a  the prm learned for the movie domain  a real-world database containing about 1 movies and 1 actors  
 b  learning curve showing the generalization performance of prms learned in the genetic domain. the shows the databases size; the  shows log-likelihood of a test set of size 1. for each sample size  we show 1 independent learning experiments. the curve shows median log-likelihood of the models as a function of the sample size. 

to this task  see  koller and pfeffer  1  for some preliminary work on related models.  however  the complexity of inference on large databases with many missing values make the cost of a naive application of such algorithms prohibitive. clearly  this domain calls both for new inference algorithms and for new learning algorithms that avoid repeated calls to inference over these very large problems. even more interesting is the issue of automated discovery of hidden variables. there are some preliminary answers to this question in the context of bayesian networks  friedman  1   in the context of ilp 1   and very recently in the context of simple binary relations  hofmann et al.  1 . combining these ideas and extending them to this more complex framework is a significant and interesting challenge. 
　another direction extends the class of models we consider. here  we assumed that the relational structure is specified before the probabilistic attribute values are determined. a richer class of prms  e.g.  that of  roller and pfeffer  1   would allow probabilities over the structure of the model; for example: uncertainty over the set of objects in the model  e.g.  the number of children a couple has  or over the relations between objects  e.g.  whose is the blood that was found on a crime scene. ultimately  we would want these techniques to help us automatically discover interesting entities and relationships that hold in the world. 
acknowledgments 
nir friedman was supported by a grant from the michael sacher trust. lise getoor  daphne koller  and avi pfeffer were supported by onr contract n1-c-1 under darpa's hpkb program  by onr grant n1-1  by the aro under the muri program  integrated approach to intelligent systems   and by the generosity of the sloan foundation and of the powell foundation. 
