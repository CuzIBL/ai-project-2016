
we present a reinforcement learning game player that can interact with a general game playing system and transfer knowledge learned in one game to expedite learning in many other games. we use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. to capture the underlying similarity of vastly disparate state spaces arising from different games  we use a game-tree lookahead structure for features. we show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. furthermore  knowledge transfer using lookahead features can capture opponent-specific value-functions  i.e. can exploit an opponent's weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax  pessimistic  search against the same opponent.
1 introduction
the general game playing  ggp  domain  introduced by pell   allows description of a wide range of games in a uniform language  called the game description language  gdl   genesereth and love  1 . the challenge is to develop a player that can compete effectively in arbitrary games presented in the gdl format. in this paper we focus on the problem of building a learning agent that can use knowledge gained from previous games to learn faster in new games in this framework.
﹛knowledge transfer has received significant attention recently in machine learning research  asgharbeygi et al.  1; taylor and stone  1; ferns et al.  1 . instead of developing learning systems dedicated to individual applications  each beginning from scratch  inductve bias is transferred from previous learning tasks  sources  to new  but related  learning tasks  targets  in order to
  offset initial performance in the target tasks  compared to learning from scratch  and/or
  achieve superior performance faster than learning from scratch.
oftentimes  specific skills required for target tasks are acquired from specially designed source tasks that are very similar to the targets themselves  asgharbeygi et al.  1 . we consider the more challenging scenario where skills are more general  and source target pairs bear little resemblance to one another. specifically  we consider the genre of 1-player  alternate move  complete information games and require that knowledge acquired from any such game be transferrable to any other game in the genre.
﹛we develop a td 竹  based reinforcement learner that automatically discovers structures in the game-tree  that it uses as features  and acquires values of these features from the learned value-function space. it then uses these values learned in one game to initialize parts of the value-function spaces in other games in the genre. the intention is to reuse portions of the value-function space that are independent of the game in our chosen genre in order to learn faster in new games. this is accomplished by focusing exploration in the complementary regions of the value function space where foresight is not informative in a game-independent way.
﹛we use game-tree lookahead for generating features. we show that features acquired in this way against some opponent are also indicative of how to play against that opponent  even in new games. we assume that the transfer learner can identify whether it had played against a given opponent before  in the same or a different game  and if so  retrieve the feature values learned against that opponent for reuse. however  a simple lookahead search player would additionally need to know  or learn  the opponent's strategy to select the most effective heuristic. without the right heuristic  we show that the lookahead search player will not perform as well as the transfer learner against a given opponent.
1 reinforcement learning
reinforcement learning  rl   sutton and barto  1  is a machine learning paradigm that enables an agent to make sequential decisions in a markovian environment  the agent's goal being to learn a decision function that optimizes its future rewards. learning techniques emanating from rl have been successfully applied to challenging scenarios  such as game playing  particularly the champion backgammon player  td-gammon  tesauro  1   involving delayed rewards  rewards only on termination leading to the credit assignment problem of which actions were good/bad  . rl problems are usually modeled as markov decision processes or mdps  sutton and barto  1 . an mdp is given by the tuple {s a r t}  where s is the set of environmental states that an agent can be in at any given time  a is the set of actions it can choose from at any state is the reward function  i.e.  r s a  specifies the reward from the environment that the agent gets for executing action a ﹋ a in state is the state transition probability function specifying the probability of the next state in the markov chain consequential to the agent's selection of an action in a state. the agent's goal is to learn a policy  action decision function  that maximizes the sum of discounted future rewards from any state s 

where are samplings from the distribution t following the markov chain with policy 羽.
﹛a common method for learning the value-function  v as defined above  through online interactions with the environment  is to learn an action-value function q given by

﹛q can be learned by online dynamic programming using the following update rule

﹛while playing action a = argmaxb q s b  in any state s  where 汐 ﹋  1  is the learning rate  rsa is the actual environmental reward and is the actual next state resulting from the agent's choice of action a in state s. the q-values are guaranteed to converge to those in equation 1  in the limit of infinite exploration of each  s a   ensured by a suitable exploration scheme  sutton and barto  1 .
1 rl in ggp
in a general game playing system  the game manager acts as the environment  and a learner needs to interact with it in almost the same way as in an mdp as outlined above. important differences are  1  the game manager returns rewards only at the end of a game  1 for a win  1 for a draw  and 1 for a loss  with no intermediate rewards   1  the agent's action is followed by the opponent's action which decides the next state that the agent faces. if the opponent chooses its action following a stationary  but not necessarily deterministic  policy  then the learner faces a stationary mdp as defined before. if  however  the opponent is adaptive  then the distribution t is effectively non-stationary and the above technique for value function learning is no longer guaranteed to converge. in this paper  we focus on stationary  non-adaptive  opponents.
﹛let 考 √  s a  ﹋ 曳 be the state resulting from the learner's execution of action a in state s; this is actually the state that its opponent faces for decision making. the state 考  also called an afterstate  can be reached from many different states of
the learner as a result of different actions. so usually |曳|   |s ℅ a|  and it is popular for game playing systems to learn values of afterstates  instead of state-actions. accordingly  we learn q 考 .
1 the ggp learner
we have developed a complete ggp learner that caters to the ggp protocol  ggp  ; genesereth and love  1 . the protocol defines a match as one instance of a game played from the start state to a terminal state  between two players that connect to the game manager  henceforth just the manager  over a network connection. each match starts with both players receiving a gdl file specifying the description of the game they are going to play and their respective roles in the game 1 the game manager then waits for a predefined amount of time  startclock  when the players are allowed to analyze the game. it is possible that both players signal that they are ready before the end of this  startclock  phase  in which case the manager terminates this phase and proceeds with the next.
﹛in the next phase the manager asks the first mover for its move  and waits for another while  playclock . if the move is not submitted by this time  the manager selects a move at random on behalf of that player and moves to the next player  and this continues. if a move is submitted before the end of a playclock  the manager moves to the next player early. when the manager senses a terminal state  it returns the appropriate rewards to the players  and terminates the match. in this paper we consider games where everyplayer unambiguouslyknows the current state.
﹛to measure learning performance  our learner plays a series of matches of the same game against a given opponent  and notes the cumulative average of rewards that it gets from the manager. since the computations for transfer learning are often expensive  we perform these and the q-updates for all states visited in the course of a match  during the startclock of the next match. we keep the sequence of afterstates  考1 考1 ... 考k  考k being terminal   in memory and use the fast td 竹   sutton and barto  1  update
	忖q 考p  = 汐竹t p rt+1 + 污q 考t+1    q 考t  	 1 
at any time t = 1 ... k  for p = 1 ... t and 竹 ﹋  1  
.
where only rk+1 is potentially non-zero  with q 考k+1  = 1. such batch update of q-values is effectively no different from online updates since a player cannot face the same afterstate more than once in a match  unless the game allows noop as a move. by relegating the bulk of our computation to the startclock period  our transfer learner makes rapid moves  involving simple q-value lookup  which is useful in large games  such as chess derivatrives  where move-time can otherwise exceed the playclock some times.
1 features in value space
in rl  a feature usually means a property of some states in s. for instance  the gps location is a feature for a mobile robot. if a set of features can be found such that the union of their joint values partitions s  then each state can be described uniquely in terms of those features. in this work we use game-specificfeatures  or simply the state space  s  in order to enable detailed learning within each game  but for the purpose of transfer  we constrain the system to identify gameindependent features  the feature space  that are nonetheless correlated with the value function. these features describe the transition structure under an afterstate in the game-tree  up to a certain depth  in a game-independent way. for our purpose  a feature is a game tree template such that if the lookahead from a state matches the template  that feature is said to be active in that state. a feature is generated/matched by starting at the afterstate generated by one move of the learner from its current position  opponent's state shown as a red square at the root of each subtree in figure 1  and expanding the game tree fully for up to two further moves  one move of the opponent  followed by one move of itself . the learner then classifies each node in this subtree as win  loss  draw or non-terminal. both the tree expansion and the determination of these node classes is enabled by a game simulator  using a prolog based theorem prover  that the learner generates from the given game description. once all nodes in the subtree are classified  siblings of the same class in the lowermost level are coalesced. after this step  all siblings in the next higher level  i.e. the mid level in figure 1  that have the same subtree structure under them are coalesced. the resulting structure is a feature that does not incorporate any game-specific information  such as the number of moves available to any player in any state  or the semantics of a state. figure 1 illustrates this process. extending this scheme to arbitrary number of lookahead levels is straightforward.

figure 1: illustration of an actual subtree  top  rooted at a given afterstate  matching/generating a feature  bottom . circular  green  nodes represent the learner's states  solid  red  square  nodes are the opponent's states  or learner's afterstates . empty squares stand for a win for the learner.
﹛figure 1 shows the 1 features discovered by our transfer learner in the tic-tac-toe game. note that although the features are all distinct  the associated semantics can often be overlapping; for instance  figure 1  j   k  and  l  are really variants of the concept  fork opponent   since the learner's move results in a state for the opponent where no matter what move it makes  the learner can win in its next move. the transfer learner also needs to check if the starting afterstate

figure 1: the 1 features discovered by the learner in tictac-toe game. empty circle/square are terminal states  with square  circle  meaning a win  loss  for the learner. a crossed square is a draw. to be considered a feature there must be at least one terminal node at some level.
is a terminal and consequently  can identify winning moves 1-step ahead.
﹛once the training runs are complete in the source game  in our case tic-tac-toe   we extract feature information from the acquired value-function space. this involves matching each afterstate from the subset of 曳 that was actually visited during source learning  against each of these discovered features using the simulator for lookahead. if an afterstate 考 matches a feature  we note the value q 考  against that feature. the value of a feature fi is then calculated as a weighted average val fi  = avgw{q 考 |考 matches fi}  where w is the weight associated with a 考  specifying the number of times 考 was visited during the source game experience. thus  the abstract features in game-tree space are associated with their values in the source task under the assumption that they will have similar values in the target task.
﹛after the feature values have been computed  we use them to initialize q 考  in the target game for each 考 that matches fi  i.e.  qinit 考  = val fi s.t. 考 matches fi  once for each new 考 encountered in the target game. during the startclock of a match  we look at the afterstates visited during the preceding match. if an afterstate has not been visited in any previous match  it is matched against our set of features discovered in the source game  and initialized as above. if there is no match  we initialize to the default value 1. next the td 竹  updates are done according to equation 1.
﹛the idea behind this transfer mechanism is to save the cost of a few value-backup steps near terminal states  i.e.  when the states gain predictivepotential  and thus guide exploration to focus more in the regions where foresight is not usually available. in this way  our transfer learner behaves more like human learners.
characteristics of feature transfer
the features do not depend on the exact game  as long as it is within the genre of chosen games. specifically  the size of the board  the number of available actions at each level  the semantics of states or actions  and win/loss criteria have been effectively abstracted away by exploiting the gdl. consider the diverse natures of games in these aspects: in tictac-toe the number of available moves steadily diminishes  in connect-1 it diminishes at intervals  while in othello it may actually increase. the winning criteria are widely varying in these games; they are similar in tic-tac-toe and connect-1 but completely different in go or othello. a key motivation behind this research is to develop simple techniques that can transfer knowledge effectively from one game to a markedly different game which is why we have focused on such a high level of abstraction.
﹛the distinct leaf-types used in the features  figure 1  depend on the possible outcomes of the games from which they are acquired. in this paper  we have assumed all games have 1 possible outcomes  viz.  win  loss or draw  identified by distinct rewards 1  1 and 1 respectively. if some game offers a different set of rewards  e.g.  { 1 1 1}  the transfer learner can create a distinct leaf-type for each of these outcomes to acquire features from this game. but if it is to apply features from previous games to this game  then it needs to be provided with some equivalence relation that maps these rewards to previous reward sets  e.g.  that -1 and 1 in this game corresponds to 1 in the previous games  and so on.
﹛it is worthwhile to note that in several games such as tictac-toe and connect-1  a terminal move by any player can cause its win or a draw  but never a loss for that player. however  in other games such as go or othello  a player's move can cause its immediate defeat. the features discovered from tic-tac-toe naturally cannot capture this aspect; as figure 1 shows  there are no  win  nodes in the mid-level  or  loss  nodes in the lowest level. our transfer learner can treat any of these games as source  and consequently it can capture a variety of possible types of features. in fact it can treat every game as both the application domain for previously acquired features  and at the end  as a source for new features to carry forward to future games. in this paper  however  we focus on specific source-target pairs  and learn against specific opponents to study the effects of transfer in controlled experiments.
﹛one concern when using complex feature spaces for transfer is that the time overhead for computing transfer knowledge should not overwhelm the learning time. by having a small number of features and limiting the depth of lookahead  we are ensuring a low computational complexity for transfer knowledge. moreover  since a single source game serves many target games  the time spent in acquiring the features is amortized  so we do not consider this as an added complexity to target learning. the limited lookahead depth also serves to keep the features somewhat indicative of the outcome of the subsequent moves. note however  this indication is not always unambiguous  e.g.  the outcome of figure 1 g  cannot be specified without knowing the opponent's disposition. this ambiguity justifies transfer learning; if merely looking ahead would give a concrete idea of the ultimate outcome of playing a in state s irrespective of the opponent's style of play  then we could well have initialized the corresponding q-value in the target game to the known value of that outcome  perhaps by minimax search. in the experiments  we actually show the transfer learner learning faster than rl with minimax-lookahead  against some opponents.
1 experimental results
in this section  we report empirical results that isolate the impact of our general game-tree-feature-based transfer scheme in a variety of games. we will consider our method to be a success if it can lead to quicker and/or better asymptotic learning in the new games when compared to learning the new games from scratch.
﹛we extracted the feature values from the tic-tac-toe game  the source  and tested the transfer learner on 1 different target games: connect1  capturego and othello. connect-1 is a variant of connect-1 where the board size is 1 ℅ 1 and the goal is to make a line of 1 instead of 1 pieces. capturego is a variant of go  or gomoku  where the board size is 1 ℅ 1 and a match terminates if a player captures an opponent's piece following the usual rules of go. if no player has a move but there has been no capture yet  then the player with larger territory wins  just as in the regularversion of go. othello follows the same rules as the regular game but is played on a smaller board of size 1 ℅ 1.
﹛for all games  we compared the learning speeds of a baseline learner to our transfer learner using feature knowledge acquired from tic-tac-toe. the baseline learner uses afterstate td-learning as in equation 1 with a value function initialized uniformly to the default value. for comparison purposes and to isolate the effect of knowledge transfer from lookahead search  we also compare with a lookahead learner that uses the same depth of lookahead as the transferlearner  with minimax search to estimate the value of a new afterstate. in this search  non-terminal states at the leaf level are evaluated to the default value  while terminals at any level are evaluated to their actual values. the value estimate for an afterstate  thus reached  is used to initialize its q-value for td-learning using the same method as the other 1 learners  i.e.  equation 1 .
﹛we use three different types of opponents against which our 1 learners are made to compete in the ggp framework. these are
-greedy this opponent uses a small fixed probability   for exploration  and otherwise uses the following policy. it looks ahead one full turn and seeks terminal nodes. it takes winning moves  avoids losing moves  but otherwise plays randomly. this is similar to a shortsighted novice player.
random this opponent picks actions using a uniform probability distribution over the set of available actions at any turn.
weak this opponent is the opposite of an -greedy player. it explores in the same manner  but picks worst moves at decisive turns. in effect  this opponent plays randomly most of the time  but in the vicinity of a terminal state  it makes particularly poor decisions.
﹛the purpose of considering a weak opponent is to study how fast the different learners can learn to exploit certain weaknesses in an opponent. table 1 shows the feature values for the 1 features of figure 1  computed by the transfer learner in the tic-tac-toe game when competing against each of these 1 types of opponents. note that the minimaxlookahead learner would initialize the afterstates that would table 1: values of the features  from figure 1  acquired in tic-tac-toe game against various opponents.
feature id from figure 1-greedyrandomweak a 111 b 111 c 111 d 111 e 111 f 1.1.1 g 111 h 11 i 111 j 111 k 111 l 111have matched these features to the values of 1  1 or 1. in other words the initializations of the minimax-lookahead learner are more accurate  since these are the true values for those states  than the transfer learner  assuming the opponent is perfectly rational. for all experiments  the learners' parameter values were 汐 = 1  污 = 1  since the task is episodic   竹 = 1  and a fixed exploration probability of
.

figure 1: learning curves for transfer learner  baseline learner  and rl with lookahead only  in 1 ℅ 1 connect1  all against -greedy opponent
﹛figures 1  1 and 1 show the learning curves for the 1 learners against the -greedy opponent. the transfer learner uses the feature values learned against this player in tic-tac-toe  table 1 . the cumulative average reward from last 1 of 1 matches are averaged over 1 runs and plotted against the number of matches in these figures. although the transfer learner outperforms the baseline learner  we see that the lookahead learner is the ultimate winner since its assumption of a rational opponent is realized in this case  and it uses superior initializations compared to the transfer learner. also since all the learners use fast td methods and afterstate learning  their learning rates are high  typically crossing 1% performance level in less than 1 matches. another thing to note from figure 1 is that 1 matches is insufficient for the

figure 1: learning curves for transfer learner  baseline learner  and rl with lookahead only  in 1 ℅ 1 othello  all against -greedy opponent

figure 1: learning curves for transfer learner  baseline learner  and rl with lookahead only  in 1 ℅ 1 capturego  all against -greedy opponent
learners to converge in the othello game since the terminal states in this game are not as shallow as in the other games.
﹛in order to verify the learning rates against a weak or a random opponent  we pitted the transfer learner and the lookahead learner against each of these opponents  in the othello game. this game is challenging to both learners because of the depth of the terminal states. the transferlearnerused the feature values learned against each opponent for the matches against that opponent. the learning curves are shown in figures 1 and 1. since the opponents are quite unlike what the minimax-lookahead learner assumes  its learning rate is poorer than the transfer learner. the transfer learner not only learns the values of features  but also learns them in the context of an opponent  and can reuse them whenever it is pitted against that opponent in the future. note that the lookahead learner could have used a maxmax heuristic instead of minimax to learn much faster against the weak opponent  and similarly an avgmax heuristic against the random opponent. because of the random policy of this opponent  the learners typically have to deal with enormous sizes of afterstate space and hence learn much slower  figure 1  than against other opponents in the previous experiments.
﹛these experiments demonstrate that knowledge transfer would be a beneficial addition to a baseline learner  but that if

figure 1: transfer in 1 ℅ 1 othello against a weak opponent  compared to rl with lookahead.

figure 1: transfer in 1 ℅ 1 othello against a random opponent  compared to rl with lookahead.
it implements a lookahead approach that our transfer learner uses as well  then its performance may be superior to the transfer learner  depending on the opponent. this is true if the lookahead scheme involves a heuristic that precisely matches the opponent's disposition. however  if the heuristic is a mismatch  then knowledge transfer is the better option. we argue that since selecting a heuristic  e.g.  minimax  to fit an opponent is a difficult task wihtout knowing the opponent's strategy  knowledge transfer  does not need to know the opponent's strategy  is superior to lookahead learning.
1 related work
lookahead search has been shown to be an effective technique in conjunction with reinforcement learning  tesauro  1 . automated feature discovery in games has been explored before  fawcett  1   which can form the basis of further work in feature transfer. asgharbeygi et.al.  have recently developed a relational td learning technique for knowledge transfer in the ggp domain. their technique exploits handcrafted first order logic predicates that capture key skills in a given game and their values are learned in the same way as we do for features. the main advantage of our technique is that we do not need to define game-specific or opponent-specific features for transfer to be successful in a wide variety of games. some of the literature on transfer learning for mdps has looked into constructing correspondences between state and action spaces of two different but related mdps  taylor and stone  1   whereas we face mdps that have very little in common in terms of syntax or semantics of states/actions. our approach matches the philosophy in  ferns et al.  1  where similarity of states/actions is determined by their effects  viz. rewards and transitions through bisimulation metrics  which we accomplish by gametree lookahead.
1 conclusions
we have presented a transfer learner that uses automatic feature discovery in conjunction with reinforcement learning to transfer knowledge between vastly different 1-person  alternate move  complete information games  in the ggp framework. the key to feature construction is lookahead search of the game tree. this paper demonstrates that gameindependent features can be used to transfer state value information from one game to another even better than lookahead minimax  or a fixed heuristic  search  particularly when the opponent is suboptimal. we believe the lookahead search player needs to know  or learn  the opponent's strategy  unlike the transfer learner  in order to select the appropriate heuristic. even so  it is unclear whether appropriate heuristics are readily availablefor a variety of opponent-weaknesses  we have only studied two simple cases   and how well they work for the lookahead learner compared to the transfer learner. this paper shows evidence that knowledge transfer offers a simpler alternative to the complex issue of constructing appropriate heuristics for lookahead search. this work also opens up new directions in ggp and transfer learning. in the future we will extend feature definition to apply at higher levels in the game-tree  incorporate deeper features built hierarchically without deeper lookahead  and experiment with other and/or larger games.
acknowledgements
this work was supported in part by darpa/afrl grant fa1-1 and nsf career award iis-1. the authors also thank gregory kuhlmann for providing the ggp player codebase  and kurt dresner for his implementation of the -greedy player.
