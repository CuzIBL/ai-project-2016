 
this paper illustrates how a multi-agent system implements and governs a computational linguistic model of phonology for syllable recognition. we describe how the time map model can be recast as a multi-agent architecture and discuss how constraint relaxation  output extrapolation  parse-tree pruning  clever task allocation  and distributed processing are all achieved in this new architcture. 
1 	introduction and motivation 
this paper investigates the deployment of multi-agent design techniques in a computational linguistic model of speech recognition  the time map model  carsonberndsen  1 . the architecture of the model is illustrated in figure 1. in brief  phonological features are extracted from the speech signal using hidden markov models resulting in a multi-tiered representation of the utterance. a phonotactic automaton  network representation of the permissible combinations of sounds in a language  and axioms of event logic are used to interpret this multilinear representation  outputting syllable hypotheses  carson-berndsen and walsh  1 . in what follows  we firstly introduce the multi-agent paradigm and discuss the use of agents equipped with mental models. secondly we illustrate how the model can be recast within a multi-agent architecture. the paper concludes by highlighting the benefits of such an approach. 
1 	the multi-agent paradigm 
the multi-agent paradigm is one which promotes the interaction and cooperation of intelligent autonomous agents in order to deal with complex tasks  see  ferber  1  for an overview of the area . the agents used to recast the time map model are deliberative reasoning agents which use mental models.the particular architecture chosen to recast the time map model is known as the belief-desire-intention  bdi  architecture  rao and georgeff  1 .according to the bdi paradigm an agent is equipped with a set of beliefs about its environment and itself  a set of computational states which it seeks 

figure 1: the time map model architecture 
to maintain  essentially desires and a set of computational states which the agent seeks to achieve  that is intentions. 
¡¡the agents are created through agent factory  o'hare et al.  1   a rapid prototyping environment for the construction of agent based systems. agents created using agent factory are equipped with a mental state model and a set of methods  the actuators . agents used in speech techonology in the past  erman et al.  1g  exhibited only weak agenthood. however  the agents delivered through agent factory are intentional agents with rich mental states governing their deductive behaviour; they exhibit strong notions of agenthood. the model presented below is both pioneering and in stark contrast to prior research in that it represents the first attempt to explicitly commission a multi-agent approach in subword processing. the benefits of this model are discussed in section 1. 
1 the time map model as a multi-agent system 
a number of agents are required in order to recast the 
time map model accurately  namely the feature extrac-

poster papers 	1 

tion agents  the windowing agent  the segment agents  and the chart agents. these agents are detailed below. the architecture is illustrated in figure 1. 
the feature extraction agents 
numerous feature extraction agents  operating in parallel  output autonomous temporally annotated features  i.e. events . these events are extracted from the utterance1 using hidden markov model techniques  for more details see  abu-amer and carson-berndsen  1 .the feature output is delivered to the windowing agent. 
the chart agent 
chart agents have a number of roles to play in the syllable recognition process. one role is to inform the windowing agent of all phonotactically anticipated phoneme segments for the current window. this information is extracted from all transitions traversable from the current position in the phonotactic automaton. for example in figure 1 an fs  segment has already been recognised at the onset of a syllable. based on this the chart agent can predict a number of subsequent segments  including a  p  as illustrated. thus  it communicates its segment predictions  along with their constraint  rank  threshold and corpus-based distributional information  to the windowing agent. 
¡¡chart agents also monitor progress through the phonotactic automaton by maintaining records of the contiguous transitions that have been traversed as a result of recognising phonemic segments in the input. the chart agent is informed of which segments have been recognised by segment agents. if the segment is one which was anticipated  i.e. a phonotactically legal segment  by the chart agent then the associated transition is traversed.chart agents begin tracking the recognition process from the initial state of the phonotactic automaton. if a chart agent reaches a final state in the phonotactic automaton then a well-formed syllable is logged. it is also possible that the segment recognised is not one anticipated by the chart agent. in this case an ill-formed structure is logged and the chart agent returns to the initial state of the automaton in anticipation of a new syllable. in certain cases the chart agent receives recognition results from segment agents based on underspecified input. in these cases the chart agent can augment the results by adding anticipated feature information provided that there is no conflicting feature information in the input. this is known as output  extrapolation. 
the windowing agent 
as previously mentioned the windowing agent receives segment predictions from chart agents for the current window. the windowing agent also takes the current output produced by the feature extraction agents  constructs a multilinear representation of it  and proceeds to window through this representation. for each window examined it identifies potential segments that may be present based on partial examination of the feature content of the window. potential segments can be identified by using a resource which maps phonemes to their respective features. priority is placed on attempting to recognise predicted segments first. therefore  the windowing agent spawns a segment agent for each potential segment that is also predicted by a chart agent  before spawning a segment agent for potential segments not predicted by a chart agent. the incremental spawning of segment agents for potential segments is dependent on the progress made by segment  agents already activated. in figure 1 the windowing agent has received predictions from the chart agent. having placed an initial window over the feature-extracted multilinear representation of the utterance  the windowing agent identifies a number of potential segments in the window. segment agents are spawned  two of which are illustrated  one for the predicted potential segment  p   and one for the unpredicted potential segment |bj. the segment agent is discussed below. 
the segment agent 
each segment agent  spawned by the windowing agent has a specific phonemic segment which it seeks to recognise. segment agents for segments which were not  predicted have default information from the resource which maps phonemes to their respective features. for example  according to figure 1 a segment agent attempting to recognise a  b  requires that voiced  voi-f   stop and labial features all overlap in time. however  segment  agents for segments which were predicted may have altered rankings  provided by the chart agent and founded on corpus-based distributional information or cognitive factors. a segment agent  attempts to satisfy each of its overlap relation constraints by examining the current  window. each time a constraint is satisfied its rank value is added to a running total known as the segment agent's degree of presence. if the degree of presence reaches the threshold then the segment  agent is satisfied that its segment has been successfully recognised. for predicted segments certain constraints may be relaxed  i.e. not all constraints need to be satisfied in order to reach the threshold. rather than all segment  agents reporting back to the windowing agent  each 
segment agent communicates its degree of presence to the other segment agents in the environment. only if a segment agent identifies that it has the greatest  degree of presence and has reached its threshold will it ask the windowing agent to proceed to the next window. it will also inform chart agents that its segment has been successfully recognised. when a segment  agent finds that one of its constraints is satisfied it can share this result with other segment agents. thus a segment agent attempting to satisfy the same constraint may not have to do so. 
1 	benefits of the multi-agent approach 
the recasting of the time map model results in an innovative multi-agent system for speech recognition which is significantly different from more traditional approaches. 

1 	poster papers 

this new architecture provides a principled means of distributing a computationally heavy work load among several task-specific agents  operating in parallel  and collaboratively  thus alleviating the computational strain. the use of agents facilitates information sharing  search space pruning  constraint relaxation and output extrapolation. from a computational linguistic viewpoint this architecture allows an explicit separation of the declarative and procedural aspects of the model. in this way the knowledge sources can be maintained independently of the application which is particularly important in speech recognition applications to ensure scalability to new task domains and migration to other languages. 
acknowledgments 
this research is part-funded by enterprise ireland under grant no. if/1 and part-funded by the science 
foundation ireland under grant no. 1 / i m / i 1 . the opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of either granting body. 
