 
　　the decision-making component of a robot that operates in a poorly known environment is considered. the usual problem-solving approach to handling a task is not suitable when each decision may turn out in several ways  and many decisions are needed to complete the task. an alternative approach  which is based on maximizing the estimated u t i l i t y resulting from each decision  is i l l u s t r a t e d . the ex-
ample describes the plans  u t i l i t y functions and decision procedures of a simulated insectl i k e robot called percy. in spite of i t s limited a b i l i t y to perceive and store information about the environment  percy can achieve satisfactory performance on i t s task. 
1. introduction 
　　robot experiments are currently carried on in limited task environments. in such circumstances  it is feasible to provide the system 
with comprehensive  if not perfectly accurate  information covering those features of the environment that relate to i t s tasks. but to depend on planning methods that require such comprehensive information w i l l hinder the extension of the work that is now being done. systems w i l l have to be able to operate in environments where much of the knowledge that would help in carrying out their tasks cannot be provided beforehand  and where the approach of storing detailed information as it is received is unsatisfactory  because there is more information than can be stored  or because it changes too often. 
　　comprehensive models of the environment are needed when planning is based on a problemsolving approach. this approach makes an exploratory analysis of i t s model  and lays out an intended course of action before the f i r s t action is taken. but if it is l i k e l y that the course of action w i l l not reach i t s intended outcome  because the system's knowledge of i t s environment is incomplete or inaccurate  planning as though the outcome were certain can be a wabte of e f f o r t . 
　　the p o s s i b i l i t y of using multiple-outcome operators in planning has been discussed by munson1 and by fikes  hart and nilsson 1 . however  these discussions make it appear that multiple outcomes do not mesh well with a problem-solving approach. every plan with a desired goal state as one of i t s possible outcomes may have to assign low probability to that outcome  because the conditions to be met in getting there are not known  and not because it is really improbable that the goal w i l l be reached. it is not obvious how to formulate the decision process when that case is l i k e l y to come up. 
　　a different approach to the use of multipleoutcome operators has been described in a series of a r t i c l e s 1   1   1 this type of planning focuses on intermediate objectives  and uses probability estimates to take account of i t s i n a b i l i t y to predict how a course of action 
w i l l turn out. the approach has been tested in experiments with a simulated robot that has only crude knowledge of i t s environment. 
　　the design of the robot involves a hierarchy of control  with selection of plans handled at the highest l e v e l   and the direct control of perception and action delegated to the lowest l e v e l . 	various types of planning can function 
with t h i s hierarchical organization. in the experiments  the selection of plans has been based on a policy of maximizing u t i l i t y . thus  
the robot's decision-making component has some features of the  cost-effective executive  
described by munson. but there are differences. for one thing  u t i l i t y is associated not with a specific goal  but with events that occur in the course of the task  and contribute to or detract from the overall measure of the success in performing the task. 
　　the system that is simulated  in the experiments with this approach  is an insect-like robot called percy. i t s task is to build a nest  and that calls for a number of t r i p s about the environment to locate material and add it to the evolving nest. percy can complete the nest in a very large number of ways; i t s problem is to find a way that keeps it adequately fed  and s t i l l does not excessively delay the completion  in the latest experiment  percy must also minimize encounters with an enemy - the stinger; the risk of painful stings can be t o t a l l y eliminated only at the cost of an unacceptable slowing of progress on the nest. 
　　percy has very poor knowledge of i t s environment. i t s a b i l i t y to perceive the objects of importance in i t s task is l i m i t e d   and it has 
no map of the environment to use in locating those objects. it finds i t s way about by means of landmarks  which it can perceive at a distance. these landmarks help it to locate the nest s i t e   several places where material can be found  and the entrances to the area where i t s food can be hunted. a diagram of the environment is shown in figure 1. 
　　in this task  there is no experimenter whose commands set problems for the robot to solve. the situation is more l i k e the playing of a game  in which the course of action is limited by and responsive to the moves of the opponent. and in fact  decisions are treated as the system's moves in a game played against the environment. in this game  the environment replies to a decision by choosing one of i t s possible outcomes  and that outcome identifies the stage of the task at which the next decision must be made. a game tree summarizes the set of complete sequences of alternating decisions and outcomes; each sequence describes a possible instance of performing the system's task. each such complete sequence has associated with it a u t i l i t y value  which measures the quality of performance when the task eff o r t has followed the course described by the sequence. a component of the system produces the system's strategy in the game  and the success of the strategy is measured by the average u t i l i t y over a series of performances 
of the task. 
　　the game played against the environment d i f f e r s in important respects from the usual games played by programs*. for one thing  the environment as opponent is not trying to win. therefore  it is not appropriate to plan as though the opponent w i l l make the move that is least advantageous to the system. a policy of maximizing expected u t i l i t y takes the place of minimaxing. secondly  the payoffs occur during the course of the task rather than at the end. percy does not win or lose; i t s u t i l i t y rises when things go w e l l   and f a l l s in the contrary case. and f i n a l l y   it is not necessary for the system to make a new decision each time it must act. each decision picks an operator that controls a course of action  and a new decision is called for only when the outcome of the previous one is determined. 
1. the percy simulations 
there are three types of stages at which 
percy must make a decision. the f i r s t type occurs when an immediate objective of a preceding decision has been realized: material has been picked up  or placed at the nest  or food has been eaten. in the second type  new information has been obtained: food has been spotted while looking for material  or material has been seen while hunting food  or the stinger has been sighted. the third type of decision stage is reached when food is being tracked and is lost to sight. 
table  1  	percy's outcome 
type of outcome 
1. material added to nest  task continues 
1. material taken near landmark 1. material taken near landmark 1 
1. material taken near landmark 1 
1. food eaten  material not held 
1. food eaten  material held 
1. material seen while hunting food 
1. food seen while finding material 
1. stinger seen 
1. food lost to sight  material not held 
1. material added to nest  task done 
1. task abandoned 
　　decisions are again of three types. percy can hunt for food. if it ib carrying material  it can head for the nest to place the material. and if it is not carrying material  it can try to locate some. 
　　a decision is f u l l y specified by giving the l i s t of targets that are of interest while the decision is being executed. the l i s t contains a primary target - food  material  or the nest; it usually contains one or more secondary targets. a secondary target may be one of the landmarks used by percy in finding i t s way around the environment. also  either food or 
material may be a secondary target when the other is the primary one  and the stinger w i l l be a secondary target when seeing it w i l l c a l l for a decision whether to avoid it or not. 
　　the outcomes of these decisions have descriptions similar to those of the decision stages that arise when the outcomes are reached. in addition  there are two outcomes that end the task; completion of the nest  or abandonment of the task. note that in general a decision stage can be named by the outcome that triggers i t   and we w i l l make no distinction hereafter between outcomes and decision stages. 
table  1  gives a complete l i s t of outcomes for percy's decisions. also  for each outcome that does not end the task  the decisions open at that stage are l i s t e d . 
and decisions 
               decisions available l a . find material near landmark 1 l b . find material near landmark 1 lc. find material near landmark 1  watch for food and stinger 
i d . hunt food  watch for material. stinger 
1a. add material to nest 
1b. hunt food via landmark 1 
1a. add material to nest 1b. hunt food 
1a. add material to nest 1b. hunt food 
1a. find material near landmark 1 
1b. find material near landmark 1 
1a. add material to nest via landmark 1b. add material to nest via landmark 1 
1a. take material 1b. hunt food 
1a. find material near landmark 1b. hunt food 
1a. find material near landmark 1b. find material near landmark 1 
1c. hunt food via landmark 1 
1d. hunt food via landmark 1 
1a. find material near landmark 1 
1b. hunt food via landmark 1 
1 1a. wait for new task 1a. wait for new task an important point is brought out by this table. 	there are in fact many more decision stages in percy's task then are shown  for each outcome represents a class of equivalent decision stages. 	two stages are equivalent 
when they have identical l i s t s of decisions  and each decision uses the same operator in both stages. this does not mean that the same decision w i l l be made at the two stages  for equivalent stages may d i f f e r in certain para-
meters that enter into the making of decisions. 
　　the use of equivalence among decision stages means that the system is applying generalization or abstraction to i t s task. the decision stage when material is f i r s t added to the nest is evidently different from that where the addition of material leaves the nest nearly completed  but percy's decisions are not affected by the difference. in fact  it cannot recognize the difference. 
　　abstraction is important because it can greatly simplify the problem of providing a strategy. clearly  the fewer the distinguishable outcomes  the less information w i l l be needed in making decisions. the importance of this w i l l be evident when the way percy makes i t s decisions is described. also  abstraction aids learning  as w i l l be seen. on the other hand  abstraction may omit information that is relevant to decisions  and so may adversely affect performance of the task. 
　　the way percy assigns u t i l i t y values w i l l now be described. four kinds of payoffs occur during a t r i a l of i t s task. each placement of material at the nest  each feeding  each pickup of material  and each s t i n g   a l l make their contribution - positive or negative - to the measure of u t i l i t y for the t r i a l . the u t i l i t y measure ua t  for the placement of material depends on the time t that has elapsed since the previous placement. one may think of this measure as combining two offsetting factors; a fixed increment of satisfaction 
when the material is placed  and a small fixed charge against satisfaction at every instant of time that placing of material is delayed. similarly  the u t i l i t y measure ue t  for the consumption of food depends on how long it has 
been since percy ate l a s t . the dependence on the elapsed time is more complicated than in the case of placement  because the satisfaction in eating is small if percy is not yet hungry  and the dissatisfaction of not eating increases sharply as percy gets hungrier. when the elapsed time is so great that the pleasure of eating is outweighed by the pains of hunger before eating  the net u t i l i t y ue is negative. finally  the positive u t i l i t y um gained when material is picked up and the negative u t i l i t y ug that accompanies a sting are constant. the u t i l i t y functions are sum-
marized in figure 1. 
　　the measure of u t i l i t y for a t r i a l of percy's task is the sum of the u t i l i t i e s for the eight pickups of material  for their additions to the nest  for the various feedings  and for the stings received. the problem of achieving good performance is not a t r i v i a l one. percy w i l l get i t s best results by prop-
erly spacing i t s feedings  and making quick round t r i p s to the nest whenever the time since it last ate permits t h i s . 	to complicate 
matters  when it sees the stinger it may have to decide whether it is better in the circumstances to accept the sting and save time  or avoid the pain and instead put up with a substantial delay. there are two important para-
meters that distinguish equivalent decision stages. these are the time since percy last ate  and the time since material was last added to the nest. they must be involved in the way decisions are made if good performance is to be attained. 
1. planning strategies 
　　we consider strategies that make decisions by exploring the game tree. exploration i n volves the evaluation of plans. 
　　a plan p at a decision stage d consists of one decision d available at d  together with some or a l l of i t s possible outcomes  and perhaps further decisions and outcomes. in the game against the environment  the plan is analogous to a protocol that begins:   i f i make this decision  the following outcomes may occur; if this outcome should happen  i can then make one of these decisions; . . .   it may be formally represented as a subtree of the game tree; this subtree is rooted at the node corresponding to d and it contains a single branch from that node - the branch for the decision d. the nodes that terminate the plans examined at a decision stage d mark the 
l i m i t s of the planning horizon at that stage. values or u t i l i t i e s are estimated for the stages of the task that correspond to these terminal nodes  and on the basis of these values  a value is derived for each plan - or equally  for the decision implied by the plan. 
　　when a planning strategy is used  at each decision stage d one or more plans are evaluated  u n t i l a decision with a suitable value is obtained. this may be the decision with the highest value  among those available at d; or the f i r s t decision with a value exceeding some appropriate threshold; or even the best decision found among the f i r s t n evaluated  for some suitable n. 
　　thus a planning strategy involves four elements. there must be a way of providing a set of plans for each decision stage. a rule for assigning values to the situations that terminate a plan must be present. a way of deriving from these terminal values the value of the decision that i n i t i a t e s the plan is necessary. and f i n a l l y   there must be a rule for selecting a decision at a decision stage  based on the values that are obtained for the individual decisions. 
　　in a planning strategy that uses the policy of maximizing u t i l i t y in making i t s decisions  the last three of these elements take specific forms. a u t i l i t y estimate is provided for 
every situation that terminates a plan. also  for every decision d  a set of estimates of the probabilities {qa 1 } of the possible outcomes 1 must be available. then  u t i l i t i e s can be obtained by working backwards from the terminal stages of a plan  using the fact that the u t i l i t y assigned to a set of uncertain outcomes is the expected value of their u t i l i t i e s . 
that i s   
u d -♀qd o* u 1*  
1' 
where the sum is over a l l outcomes 1' of the decision d that have non-zero probabilities. and because of the policy of selecting the decision that has the highest u t i l i t y   
u 1 *max u d  d 
where the maximum is taken over the decisions open at the decision stage that corresponds to outcome 1. continued application of these two rules  to the u t i l i t i e s of the outcomes that end the plan  w i l l arrive at the u t i l i t y of the plan and i t s decision  and w i l l also decide among the available plans. 
　　both problem-solving strategies and minimax policies can be treated as special cases of this approach  although they are not usually so considered. in problem solving  uncertainty in the environment is usually ignored; this 
means that the most probable outcome of a decision is estimated to have probability 1  and a l l others probability 1. application of a minimax policy is equivalent to estimating probability 1 for the outcome with the lowest u t i l i t y   and probability 1 for other outcomes  when working back from the situations that terminate a plan. 
　　in a stable environment where the probabili t y distributions {q d  1 } accurately reflect the responses of the environment to the execution of decision d  there exists a set of ideal u t i l i t i e s {u 1 } for every outcome that can occur in the course of handling the task. these are derived from the u t i l i t i e s of the payoffs in the following way: each outcome that terminates the task is assigned a u t i l i t y equal to the sum of the u t i l i t i e s of the payoffs that occur in the corresponding t r i a l . then  by repeated application of the two equations given above  u t i l i t i e s for a l l other outcomes are obtained. in the process  the strategy that yields the highest expected u t i l i t y against an environment with the specified probabilities is also determined. 
　　in actual systems dealing with tasks i n volving f a i r l y large game trees  the probabili t y and u t i l i t y estimates w i l l at best crudely approximate the ideal values just described. in a dynamic environment  a stable set of probability values need not exist. and even when they do  the estimates of the values available to the system may be inaccurate. in addition  the calculation of u t i l i t y estimates from the payoffs  in the manner just described  
may be far beyond the capabilities of the 
system. 
　　nevertheless  given such a task  there may be no practical alternative to the policy of maximizing u t i l i t y as a basis for designing a strategy component for the system that must deal with i t . thus  the experiments with percy are useful in showing what can be accomplished with such a strategy  despite the reliance on approximations to probabilities  u t i l i t i e s   and other parameters that enter into the making of decisions. 
1. how percy makes decisions 
　　in the percy experiments  the planning horizon extends only as far as the outcomes of 
each single decision. the use of the most limited possible exploration of the future in decision making is consistent with the 
system's crude perceptual capabilities and 
poor knowledge of i t s environment  i t s lack of memory of e a r l i e r stages of a t r i a l   and i t s consequent i n a b i l i t y to distinguish among equivalent decision stages. 
　　however  when the capacity for cognition that percy is given is limited in these ways  it is possible to use a very simple structure and a minimum of data in making i t s decisions. each decision becomes a plan merely by appending a l i s t of the possible outcomes. only one probability distribution {qa 1   is needed in finding the u t i l i t y of the plan that involves the decision d. and a uniform method of estimating the u t i l i t y of any outcome takes care of the remaining requirement for decision making. the u t i l i t y estimates given by this method do not satisfy the theoretical relationships stated earlier for ideal u t i l i t i e s . but in view of the inaccuracy of percy's i n formation  they may well be l i t t l e worse than those that would result from more extensive calculations calling for a much more complicated strategy component. 
　　the u t i l i t y estimate is closely related to the u t i l i t y function for payoffs described in section 1. suppose percy is at the decision stage following outcome 1  that a time h has elapsed since it last ate  and that a time n has passed since material was last added to the nest. percy must estimate a u t i l i t y u
d f ＜ '   for outcome 1 '   when that outcome is 
reached after making decision d. the estimate is based on the u t i l i t y functions ue  ua um  
us given in figure 1  and on three time estimates: 
td - the estimated time to reach outcome 1' if decision d is executec and 1' occurs 
te - the estimated time between occurrence of 1' and the next feeding  1 if eating occurs at 1'  
　　　　ta - the estimated time between occurrence of 1' and the next addition to the nest  1 if placement occurs at 1*  then  see figure 1 : 
u d  1' =u e  h+t d +t e  +u a  n+t d +t a  +q s u s +eu m 
where q g is the estimated probability that percy w i l l be stung in reaching outcome 1 '   also e is 1 if material is picked up at 1 '   and is 1 otherwise. 
　　this u t i l i t y estimate is an approximation to the u t i l i t y of the payoffs that w i l l take place u n t i l the next addition to the nest and the next feeding. the approximation is rough  because the values estimated by te and ta depend on h and n and also on subsequent decisions  but t e and t a are treated as constants.* 
　　since percy has estimates of te and ta for every outcome 1 '   and also has estimates of t i   qs and q a fo'  for every decision-outcome pair 	 d 1'   	it can calculate the u t i l i t y of i t s decisions d  using the equation 
1 * the method of estimating u t i l i t i e s described in this section is the one used in the most recent experiments with percy. it differs in several respects from the method reported in earlier a r t i c l e s . 
1' 
at a decision stage it makes the calculation for each available decision and selects the 
decision with the highest u t i l i t y . 
　　of course  the probabilities and elapsed times are characteristic of the particular environment in which percy finds i t s e l f . percy is not endowed with good estimates of these quantities. rather  i n i t i a l estimates are supposed to have been developed as percy explored i t s environment in tasks engaged in e a r l i e r ; the estimates are improved on the basis of i t s actual experience in the nestbuilding and related tasks. that i s   percy has a capability to learn by developing better values on which to base i t s decisions. 
　　the estimates for q d   1 '     t d and q s are provided by means of a set of totals -- one set for each decision stage 1: 
n d 1'  -- the number of times decision d has resulted in 
outcome o  
t d 1'  -- the t o t a l time elapsed in reaching outcome 1' on these occasions 
s d 1'  - the number of times that a sting was received in reaching 1' 
　　at a point where the estimates are needed  they are calculated by means of the relations t d = t   d   1 '   / n   d   1 '     qs - s d 1' /n d 1'    
qd o'  = rn{d 1' +i /zln d 1' +l ; 
1* 
the sum in the last of these expressions is over a l l possible outcomes of decision d . * 
　　estimates of t e and t a are stored for each outcome. when t e  1  is not zero - i . e .   when eating has not occurred at 1 - i t s value is revised each time a decision d  made at 1  arrives at an outcome 1 ' . the new estimate is obtained by adding to the old one the quantity k   t   1 '   + t e   1 '   - t 1   1       
where t o'  is the actual time it took to reach o' after decision d  and k is a small constant  say .1  the corresponding adjustment for t a { 1     when i t s value is not fixed at zero  is k l t   1 '   + t a   1 '   - t a   1     
the derivation of these revision formulas is omitted. they are approximations  which are used in order to l i v e with the constraints of limited memory and a b i l i t y to calculate that are assumed for percy. 
　　we have now described how percy  in the course of carrying on i t s task  continually 
* this is the so-called bayes estimate for the probability of o'. it gives a non-zero estimate for the probability of events that are possible but not yet observed. 
revises i t s estimates of time and probability. 
　　this process of revision constitutes percy's learning  or adjustment to the environment. this is the only type of learning provided in the simulations thus far  although other types can readily be added to the general structure that underlies percy. 
1. percy's performance 
　　the simulation as described has been run with a number of different sets of u t i l i t y parameters. 	various sets of values were chosen for the constants k i in the u t i l i t y functions 
given in figure 1. the relative weights given to eating regularly  making progress on the nest  and avoiding stings were changed thereby. with each set of values  a series of t r i a l s of the nest-building task was run. as soon as a nest was finished  it was wiped out  and a new t r i a l began. the estimates with which percy began this new t r i a l were the ones arrived at by the end of the preceding t r i a l   so that there was an opportunity to demonstrate learning during each series. a l l series started 
with the same r e l a t i v e l y poor estimates. 
　　it was evident at the start of the experiment that percy's efforts were doomed to 
frustration i f i t s u t i l i t y functions were i n compatible with the r e a l i t i e s of the environ-
ment. for example  when both k1 and k1 are less than the time required for a round t r i p to get material and food and bring it back to the nest  a successful performance of the task is impossible. ideally  the parameters should 
make more than one satisfactory strategy a v a i l able  so as to test whether the learning feature in the system helps to offset the limited 
horizon used in planning. 
　　tables  1  and  1  give information about the kind of behavior percy exhibited in these experiments. they describe the f i r s t t r i a l of a series  and the seventh t r i a l of the same series  by which time the behavior had converged to a locally optimal strategy which was quite good  though not the best available. in the i n i t i a l t r i a l   percy often made decisions that exposed it to the risk of a s t i n g   and 
with rather baa luck buffered a t o t a l of five stings  by the seventh t r i a l   it had substant i a l l y reduced the likelihood of a sting. it did this by going the long way round to get material and food  and returning the short way. whenever it got back s u f f i c i e n t l y quick-
l y   it made a rapid round t r i p to the nearby material location before going after food again. 
　　the convergence after a half dozen or so t r i a l s to a rather r i g i d pattern of decisions occurred with each set of u t i l i t y parameters used. it appears that adjusting the estimates t a and t .   when this is done as if the values 
being estimated are constants  cannot f u l l y make up for the limitations percy has in i t s a b i l i t y to plan. one of the several directions for further work is to see whether these characteristics of i t s decision making can be improved without an appreciable increase in percy's memory and a b i l i t y to calculate. 



1. concluding remarks 
　　the way percy makes decisions has now been explained in d e t a i l . this account completes the series of articles that have described the percy simulation as an example of the purposive system. the term refers to a general structure for integrated systems  with a l o g i -
cal organization that reflects the game against the environment. the structure disengages the 
making of decisions from their execution. it bases decision making on u t i l i t y evaluations. it c l a r i f i e s the role of generalization in handling tasks. and above a l l   i t s way of operating faces squarely the problem of incomplete knowledge of the task environment. 
each of these points is i l l u s t r a t e d by the 
percy simulation. the present a r t i c l e   which treats the problem and the process of decision making without concern for the course of action touched o f f by a decision  reflects the separation of these two aspects of system operat i o n . percy's u t i l i t y evaluations are based e x p l i c i t l y on the payoffs associated with feeding  progress on the nest  and meetings with the stinger; they are the only factors influencing i t s decisions. and generalization in dealing with the task takes the form of equivalences among decision stages that select the same set of targets for immediate attent i o n . 
　　in consequence of this design  percy achieves good performance in a task calling for a long series of decisions. moreover  i t s success is realized in spite of i t s very limited capacity to obtain or store information about the environment  or to explore the implications of the l i t t l e information it possesses. 
　　to turn this last point around  the simulation shows how an organism with a rather primi t i v e central nervous system can operate purposefully in an environment even though the program it is born with cannot contain specific information about that environment. thus the simulation may be treated as a theory of how creatures that are relatively low in the evolutionary scale can show selective behavior on tasks of moderate complexity. u t i l i t y functions based on payoffs can readily account for the  drives  that are assumed to guide animal - and sometimes human - behavior. 
　　indeed  the way people tackle well understood tasks may be closer to the percy approach than to the problem-solving framework that underlies much of the work on robots. a r t i f i c i a l intelligence  l i k e human intelligence  undoubtedly ought to incorporate both ways of planning. 

1 

1 

1 

1 
1 



1 













