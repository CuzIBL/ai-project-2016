
stochastic shortest path problems  ssps   a subclass of markov decision problems  mdps   can be efficiently dealt with using real-time dynamic programming  rtdp . yet  mdp models are often uncertain  obtained through statistics or guessing . the usual approach is robust planning: searching for the best policy under the worst model. this paper shows how rtdp can be made robust in the common case where transition probabilities are known to lie in a given interval.
1 introduction
in decision-theoretic planning  markov decision problems  bertsekas and tsitsiklis  1  are of major interest when a probabilistic model of the domain is available. a number of algorithms make it possible to find plans  policies  optimizing the expected long-term utility. yet  optimal policy convergence results all depend on the assumption that the probabilistic model of the domain is accurate.
　unfortunately  a large number of mdp models are based on uncertain probabilities  and rewards . many rely on statistical models of physical or natural systems  such as plant control or animal behavior analysis. these statistical models are sometimes based on simulations  themselves being mathematical models   observations of a real system or human expertise.
　working with uncertain models first requires answering two closely related questions: 1- how to model this uncertainty  and 1- how to use the resulting model. existing work shows that uncertainty is sometimes represented as a set of possible models  each assigned a model probability  munos  1 . the simplest example is a set of possible models that are assumed equally probable  bagnell et al.  1; nilim and ghaoui  1 . rather than construct a possibly infinite set of models we represent model uncertainty by allowing each probability in a single model to lie in an interval  givan et al.  1; hosaka et al.  1 .
　uncertain probabilities have been investigated in resource allocation problems  munos  1  - investigating efficient exploration  strehl and littman  1  and state aggregation  givan et al.  1  - and policy robustness  bagnell et al.  1; hosaka et al.  1; nilim and ghaoui  1 . we focus on the later  considering a two-player game where the opponent chooses from the possible models to reduce the longterm utility.
　our principal aim is to develop an efficient planner for a common sub-class of mdps for which some policies are guaranteed to eventually terminate in a goal state: stochastic shortest path  ssp  problems. the greedy real-time dynamic programming algorithm  rtdp   barto et al.  1  is particularly suitable for ssps  as it finds good policies quickly and does not require complete exploration of the state space.
　this paper shows that rtdp can be made robust. in section 1  we present ssps  rtdp and robustness. then sec. 1 explains how rtdp can be turned into a robust algorithm. finally  experiments are presented to analyse the behavior of the algorithm  before a discussion and conclusion.1
1 background
1 stochastic shortest path problems
a stochastic shortest path problem  bertsekas and tsitsiklis  1  is defined here as a tuple hs s1 g a t ci. it describes a control problem where s is the finite set of states of the system  s1 （ s is a starting state  and g   s is a set of goal states. a is the finite set of possible actions a. actions control transitions from one state s to another state s1 according to the system's probabilistic dynamics  described by the transition function t defined as t s a s1  = pr st+1 = s1|st = s at = a . the aim is to optimize a performance measure based on the cost function c : s 〜 a 〜 s ★ r+.1
　ssps assume a goal state is reachable from any state in s  at least for the optimal policy  so that one cannot get stuck in a looping subset of states. an algorithm solving an ssp has to find a policy that maps states to probability distributions over actions π : s ★ Π a  which optimizes the long-term cost j defined as the expected sum of costs to a goal state.
　in this paper  we only consider ssps for planning purposes  with full knowledge of tuple defining the problem: hs s1 g a t ci. in this framework  well-known stochastic dynamic programming algorithms such as value iteration  vi  make it possible to find a deterministic policy that corresponds to the minimal expected long-term cost j . value
iteration works by computing the function j  s  that gives the expected sum of costs of the optimal policies. it is the unique solution of the fixed point bellman equation:

updating j with this formula leads to asymptotic convergence to j . for convenience  we also introduce the q-value:
q s a  = ps1（s t s a s1  c s a s1  + v  s1  .
　ssps can easily be viewed as shortest path problems where choosing a path only probabilistically leads you to the expected destination. they can represent a useful sub set of mdps  as they are essentially finite-horizon mdps with no discount factor.
1 rtdp
trial based1 real-time dynamic programming  rtdp   introduced in  barto et al.  1   uses the fact that the ssp costs are positive and the additional assumption that every trial will reach a goal state with probability 1. thus  with a zero initialization of the long-term cost function j  both j and q-values monotonically increase during their iterative computation.
　the idea behind rtdp  algorithm 1  is to follow paths from the start state s1  always greedily choosing actions with the lowest long-term cost and updating q s a  as states s are encountered. in other words  the action chosen is the one expected to lead to the lowest future costs  until the iterative computations show that another action may do better.
algorithm 1 rtdp algorithm for ssps
rtdp s:state  // s = s1 repeat
   rtdptrial s  until // no termination condition
rtdptrial s:state  while  goal s  do a =greedyaction s  j s =qvalue s a  s =picknextstate s a 
end while

　rtdp has the advantage of quickly avoiding plans that lead to high costs. thus  the exploration looks mainly at a promising subset of the state space. yet  because it follows paths by simulating the system's dynamics  rare transitions are only rarely taken into account. the use of a simulation makes it possible to get good policies early  but at the expense of a slow convergence  because of the bad update frequency of rare transitions.
1 robust value iteration
pessimism and optimism - we now turn to the problem of taking the model's uncertainty into account when looking for a  best  policy. the  possibly infinite  set of alternative models is denoted m.
　a simplistic approach computes the average model over m  or the most probable model in m  then uses standard ssp optimization methods. such approaches guarantee nothing about the long-term cost of the policy if the true model differs from the one chosen for optimization.
　we follow the approach described in  bagnell et al.  1   that consists of finding a policy that behaves well under the worst possible model. this amounts to considering a twoplayer zero-sum game where a player's gain is its opponent's loss. the player chooses a policy over actions while its  disturber  opponent simultaneously chooses a policy over models  as this is a simultaneous game  optimal policies can be stochastic . this results in a max-min-like algorithm:
.
 in this ssp game  value iteration does converge to a fixed solution  patek and bertsekas  1 .
　it is also possible to be optimistic  considering that both players collaborate  as they endure the same costs   which turns the max into a min in previous formula. this second case is equivalent to a classical ssp where a decision consists of choosing an action and a local model.
locality - such a max-min algorithm would be particularly expensive to implement. even restricting the search to a deterministic policy over models  it requires computing the optimal long-term cost function for each model before picking the worst model found and the optimal policy associated with it. however  a simpler process may be used to compute j while looking simultaneously for the worst model. it requires the hypothesis that next-state distributions t s a ，  are independent from one state-action pair  s a  to another. this assumption does not always hold. however  this makes things easier for the opponent because it now has larger set of models from which to choose. this consequence is conservative for producing robust policies.
　because we assume independence at the state-action level  not only at the state level   it is equivalent to a situation where the second player makes a decision depending on the current state and the first player's action. this situation amounts to a sequential game where the previous players move is known to the next player: both players can act in a deterministic way without loss of efficiency.
   the result of this assumption is that the worst model can be chosen locally when q is updated for a given state-action pair. as can be seen from algorithm 1  the worst local model  may change while q-values evolve. previous updates of reachable states' long-term costs may change their relative ordering  changing which outcomes are considered to be worst. the key contribution of this paper is to show that rtdp can be made robust  allowing for planning in very large and uncertain domains  retaining worst  or best  case behaviour guarantees.
1 robust rtdp
from now on  we consider interval-based uncertain ssps  where t s a s1  is known to be in an interval  prmin s1|s a  prmax s1|s a  . figure 1 is an example. we algorithm 1 robust value iteration  for an ssp 
1: initialize j to 1.
1: repeat
1:	for all s: state do
1:	for all a: action do
1:	q
1:	end for 1:	j i  ○ mina（a qmax s a 
1:	end for
1: until j converges

discuss the pessimistic approach  the optimistic one leading to similar results.

	a  certain ssp	b  uncertain ssp
figure 1: two views of one ssp  depending on whether model uncertainty is taken into account  costs in parenthesis . in the uncertain ssp  action a1 will be prefered as it quickly reaches the goal s1.
　for a given state-action pair  s a   there is a list r =  of reachable states. for each reachable state t s a s1i  （ ii =  pmini  pmaxi  . thus  possible models are the ones that comply with these interval constraints while ensuring pi t s a s1i  = 1. fig. 1 illustrates this with three reachable states.
	s1	s1

figure 1: a triangle is a probability simplex representing all possible probability distributions with three different outcomes   at the vertex . on the left triangle is the trapezium showing the interval constraint for. the right triangle shows possible models at the intersection of the three interval constraints.
worst local models - the maximisation step to compute q s a  in alg. 1 is done by giving the highest probability to the worst outcome. this requires firstly sorting reachable states in decreasing order of the values:
. then  the worst
distribution is the one giving the highest probability to the first state  then to  and so on up to. as pointed out in  givan et al.  1   this is equivalent to finding the highest index r （  1..k  such that
	r 1	k
	xpmaxi	+ xpmini	＋ 1.
	i=1	i=r
the resulting transition probabilities are
max if i   r
 1 
if i   r
	.	 1 
using the pre-computed bound  alg. 1 gives a complete implementation. the insertion sort algorithm1 is chosen as the list will usually be ordered from previous j updates.
algorithm 1 worst model for state-action pair  s a 
worstmodel s: state  a: action 
 reachablestates s a  sort r  i = 1  bound = bmin while  bound   pmini + pmaxi   1  do
　bound ○ bound   pmini + pmaxi pr s1i  ○ pmaxi i ○ i + 1 end while
r = i
pr s1r  ○ 1    bound   pminr   for all i （ {r + 1 ... k} do pr s1i  ○ pmini end for return  r pr ，  

　to summarise  robust vi on an interval-based ssp consists of applying normal value iteration with the transition probabilities updated through alg. 1.
　we need only a single worst model to compute the corresponding q-value. yet  because several reachable states s1i may have the same value c s a s1i +j s1i  as s1r  we call this set of states   there may be an infinite number of equivalent worst local models. any model differing only in how the probability mass is distributed among the equally bad states of sr1 is also a worst local model.
worst global models - contrary to vi  rtdp does not necessarily visit the complete state-space. this is why  barto et al.  1  introduces the notion of relevant states  which we extend to the uncertain case: a state s is relevant for m if there exists a start state s1  a model m （ m and an optimal policy π under that model such that s can be reached from state s1 when the controller uses π under m.
　this notion is important because two equally worst local models on a given state-action pair may forbid different states  so that for two models m1 and m1  a state may be relevant in m1 but not in m1. yet  rtdp should not find an optimal policy just for the relevant states of only one worst global model. neither does the policy have to apply to all possible states. it should apply to all reachable states under any model  for optimal policies  i.e.  for relevant states. but covering all relevant states in the worst model used for updating q-values does not necessarily cover all relevant states in m: it depends on the model used to choose the next state  i.e.  to simulate the system's dynamics.
　to avoid missing relevant states  each local model used for the simulation should ensure that all reachable states can be visited. as can be seen in fig. 1  the set of possible local models for a state-action pair is an n-dimensional convex polytope. any model inside this polytope  excluding the boundary  is therefore adequate because  for all s1i  it ensures that p s1i|s a    1.
　so there exists a global model md that can be used to effectively simulate the system's dynamics without missing any potentially reachable state.
1 robust  trial-based  rtdp
robust rtdp differs from the original rtdp in that:
  each time the algorithm is updating a state's evaluation  an opponent is looking for a worst local model which serves to compute the q-values.
  for exploration purposes  the algorithm follows dynamics of the system that consider all possible transitions  using model md .
   relevant  states are now the states reachable by following any optimal policy under any possible model.
　from this  we adapt to our context convergence theorem 1 from  barto et al.  1  and the corresponding proof  discussing mainly its changes.
proposition 1. in uncertain undiscounted stochastic shortest path problems  robust trial-based rtdp with the initial state of each trial restricted to a set of start states  converges  with probability one  to j  on the set of relevant states  and the controller's policy converges to an optimal policy  possibly nonstationary  on the set of relevant states  under the same conditions as theorem 1 in  barto et al.  1 .
proof. the proof outline is:
1. as shown in sec. 1  robustness is achieved by considering a stochastic shortest path game  sspg . in particular  this one is a sequential sspg  for which bertsekas and tsitsiklis  establish value iteration and q-learning algorithms. provided all states are visited infinitely often  asynchronous value iteration can also be used to solve sequential sspgs  avi-sspg .
1. we adapt the proof of barto et al.  to transform avi-sspg to an rtdp algorithm  rtdp-sspg . the transformation ensures: 1- the additional maxm（m in the update formula  alg. 1  line 1  does not break the requirement that jt is increasing and non-overestimating; and 1- relevant states are visited infinitely often  guaranteeing a complete and optimal policy.
1. rtdp-sspg and robust rtdp differ because rtdpsspg assumes an optimal opponent. in the robust case we wish to plan for opponents that may choose the wrong model. thus robust rtdp is rtdp-sspg following model md - for simulation only - that allows states to be visited that rtdp-sspg optimal opponents would forbid. under md  described in sec. 1   no state reachable under any model in m is excluded from the search. thus we have redefined an expanded set of relevant states for added robustness. all relevant states are still visited infinitely often.

　our use of the model md for simulation ensures that the policy will cover all states any opponent could lead us to  but assumes the worst model will apply afterwards.
1 experiments
labelled rtdp  bonet and geffner  1  is a modified version of rtdp which can be made robust in a similar way. the experiments conducted illustrate the behavior of robust lrtdp. to that end  it is compared to bagnell et al.'s robust value iteration  as well as lrtdp. in all cases  the convergence criteria is  for lrtdp  and for vi we stop when the maximum change in a state long-term cost over one iteration is less than 1.
1 heart
in this first experiment  we compare a non-robust optimal policy with a robust one on the small example of fig. 1-b. table 1 shows the theoretical expected long-term costs of each policy on the normal  most probable  model  as well as the pessimistic and optimistic ones. the robust policy is largely better in the pessimistic case.
table 1: theoretical evaluation of robust and non-robust policies on various models  which match empirical evaluation.
normalpessimisticoptimisticnon-robust111robust1111 mountain-car
we use here the mountain-car problem as defined in  sutton and barto  1 : starting from the bottom of a valley  a car has to get enough momentum to reach the top of a mountain  see fig. 1 . the same dynamics as described in the mountain car software1 have been employed. the objective is to minimize the number of time steps to reach goal.

figure 1: the mountain-car problem.
　the continuous state-space is discretized  1 〜 1 grid  and the corresponding uncertain model of transitions is obtained by sampling 1 transitions from each state-action pair  s a . for each transition  we computed intervals in which the true model lies with 1% confidence.
results - preliminary remark: simulating a path generally shows a car oscillating several times before leaving the valley. this has two main explanations: 1- the speed gathering is just sufficient to reach the summit  but not over it; and 1- the discretized model is not accurate enough and applying the policy obtained on the true mathematical model  instead of the discretized one  should be much better.
　fig. 1 shows the long-term cost function obtained by using value iteration  lrtdp  and their robust counterparts on the mountain-car problem. the x and y axes are the car's position and speed. the z axis is the expected cost to the goal. on the surface is an example path from the start state to a goal state:
it follows the greedy policy under the average model.
　the general shape of the surface obtained is always the same  with some unexplored parts of the state-space in lrtdp and robust lrtdp  as expected . the vertical scales are much larger in robust cases. this reflects the fact that reaching the goal is much more time-consuming under a pessimistic model. because j can here be interpreted as the average time to the goal  these graphs show how a small uncertainty can lead to longer policies. here the times are multiplied by more than 1.
　while executing the four different algorithms  an evaluation of the current greedy policy was made every 1   nstates = 1 q-value updates. the result appears in fig. 1  the y axis being the expected cost to the goal from the start state. on both sub-figures  lrtdp-based algorithms obtain good policies quickly  but have slow convergence times of vi=1 〜 1 updates  lrtdp=1 〜 1  rvi=1 〜 1  rlrtdp=1 〜 1.
　other experiments  buffet and aberdeen  1  also confirm these results  with one example showing that lrtdp can have a faster convergence than vi  and another one illustrating this approach on a temporal planning problem.
1 discussion and conclusion
a simple extension to this work  suggested by hosaka et al.   is to find the set of worst models as we do in this
 1
 1
 1
 1
 1
 1
-1 1
a  value iteration
 1
 1
 1
 1
 1
 1
 1
 1
-1 1
v x v 
 1
 1
 1
 1
 1
 1
-1 1
c  lrtdp
 1
 1
 1
 1
 1
 1
 1
 1
-1 1
figure 1: long-term cost functions for the mountain-car problem. in each case  an example path is generated based on the most likely model.

a  non-robust algorithms

b  robust algorithms
figure 1: average cost to goal for the mountain-car problem  measured every 1   nstates updates of q-values.
paper  but then to choose from these the model that has the lowest cost under an optimistic model. this improvement is presented in  buffet and aberdeen  1 .
　the approach to robustness adopted in this paper considers the knowledge of a set of possible models. an open question is whether it is possible to use more information from our uncertain model  by taking the probability distribution over possible models into account.
　model uncertainty has similarly been considered in model learning while planning  strehl and littman  1 . the algorithm proposed is optimistic  but does not seem to adapt well to our framework as an evolving model can break the  no-overestimation  assumption:  s （ s  t − 1  jt s  ＋ j  s . it is still important to notice that robust rtdp would not suffer on-line  as the real dynamics can be employed to pick a next state  the worst model appearing only in the longterm cost update formula .
　finally  a crucial assumption in rtdp is that a goal state must be reachable from any state. we present an algorithm tackling this problem in  buffet  1 .
conclusion - recent works show that model uncertainty is a major issue in decision-theoretic planning. we have proposed a modification of the rtdp algorithm enabling it to compute robust policies efficiently in large and uncertain domains. model uncertainty is represented through confidence intervals on transition probabilities. the convergence of the resulting algorithm is sketched. we demonstrate robust lrtdp on a domain where statistics are used to estimate appropriate intervals.
acknowledgments
national ict australia is funded by the australian government. this work was also supported by the australian defence science and technology organisation.
