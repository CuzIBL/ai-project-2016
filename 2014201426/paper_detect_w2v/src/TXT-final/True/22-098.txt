 
in real-world domains a concept to be learned may be unwieldy and the environment may be less than ideal. one combination of difficulties occurs if the concept is probabilistic and the learning situation is dynamic. in this case  the data may be noisy and biased. these difficulties arise when learning evaluation functions  which can be considered as concepts. a representative problem  the fifteen puzzle  is used to test six different learning systems: some that fit  count  or partition data in instance  space; some that optimize measures derived from data in hypothesis space; and some that perform combinations of such procedures. these six systems are described  tested  and analyzed. from quantitative differences in several experiments  we extract specific properties. by combining two or three kinds of techniques  we gauge the extent to which they complement each other. combinations of strengths can overcome difficulties in domains that are simultaneously probabilistic  dynamic  noisy  and biased. 
1. 	introduction 
although concepts and evaluation functions can be learned from examples using various methods  existing techniques are often inadequate for harder problems. if the domain is uncertain or the environment is dynamic  langley  1   a simple induction algorithm may have difficulty. in many cases we may be forced to elaborate old methods  combine them  or develop new ones. 
　　　to complicate matters  there are several approaches to choose from. to learn uncertain concepts  we might consider a modification of id1  quinlan  1  or some statistical technique  draper & smith  1 . or  because concept learning involves the parameterization of descriptions  we might base our method on traditional optimization  gill et al.  1 ; one such approach uses genetic algorithms  holland  1 . of course  some methods are known to be especially suited for particular situations; e.g.  genetic algorithms can be applied to badly behaved problems having strong nonlinearities. however  many problems and methods are largely unexplored. 
　　　the situations considered in this paper include probabilistic concepts  dynamic environments  and noisy and biased data. in an attempt to help develop methods for such cases  we explore a problem that taxes current systems. the problem involves a representation of the fifteen puzzle that can be managed by elaborations of several methods. 
　　　the next section defines a general problem of which the fifteen puzzle is a special case. the analysis suggests certain methods. section 1 describes the methods and discusses their strengths before we evaluate and compare them in section 1. finally  section 1 summarizes implications for empirical learning. 
1. 	the problem 
this section analyzes a representation of the fifteen puzzle  whose solution may be viewed as a special case of concept learning. in this and other problems  the data may be biased  which complicates learning. 
1 . 1 . generalised concepts 
　　　our basic problem is learning a concept or evaluation function from examples. by definition  a concept is a rule that describes a class of instances. if we represent an instance as a tuple of attributes  a concept will have an associated instance space whose dimensionality is the total number of attributes. as shown in figure 1 a   an  all-or-none  concept is a binary-valued function over instance space. this diagram shows two attributes; in general a concept is a function over whatever attributes are used to express it. if we allow a concept to be probabilistic  fig. lb   the function becomes graded: it has values between 1  certain class exclusion  and 1  certain class membership . here we interpret and learn the graded values as probabilities. hence a concept is a binary or probabilistic function over instance space. 
　　　viewing a concept as a function helps to draw parallels among learning methods and the representations they use. as one example  consider an evaluation function h for a state-space problem such as the fifteen puzzle. if x is k-tuple of attributes  x1 x1  ... xk  representing a state  then h x  could represent the probability of finding the goal  quickly  optimally  
	rendell 	1 

etc.  if the solution path goes through x  rendell  1 . but because h x  has the form of a probabilistic concept  it could also be interpreted as the concept  good state to develop.'' 
　　　typically  a concept is expressed as a logic expression or decision tree involving the attributes  mitchell  1; quinlan  1   sometimes with annotated probabilities  breiman et al.  1; rendell  1   although other expressions are possible. in contrast  an evaluation function h is often defined as a linear combination of attributes h1 + b1 1- .... + bkxk = b.x  where the bi are weights to be learned  samuel  1   however  these preferred forms are not mandatory. 
　　　to reiterate: when expressed in terms of attributes  concepts and evaluation functions are both functions over their instance space. only the details vary. the terms concept and evaluation function are sometimes synonymous. 
1. 	noisy and biased data 
　　　the data for empirical learning are often binary  to represent definite knowledge about class membership  mitchell  1 ; but data can also be graded  to indicate probable categorization  draper & smith  1 . for example  we could annotate a value of 1 to a patient's state to express a belief that he has some disease. probabilistic data can also be uncertain  e.g.  1 ＼ 1   or even biased  e.g.  1 is an overestimate . biased data arise if the sampling is not random  which is undesirable in statistics  draper & smith  1   but not always in machine learning  winston  1 . 
　　　in some problems  not all the data are available before some decision must be made that utilizes the results of analyzing the first batch. in such dynamic learning  how might the results of one run affect the gathering of future data  a physician might have 
compiled some data for diagnosis. if these data support disease a  then the physician's subsequent tests may be biased toward observations that will confirm it. the consequences of such a bias may often be good  but sometimes detrimental. if the correct diagnosis is disease b  time may be wasted  or worse  the proper evidence may never be found. 
　　　this problem can also arise in domains such as the fifteen puzzle. because this puzzle has about 1 states  most problems cannot be solved breadth-first  to give unbiased data . in contrast  a good evaluation function solves many problems  but produces biased data: states assessed favorably tend to predominate. data can become increasingly biased as the evaluation function improves in successive iterations. new data are incommensurable with early data  and if used directly  can give erroneous results  rendell  1 . 
1. 	representative problem characteristics 
　　　the issues explored here involve biased and uncertain data  uncertain concepts  and dynamic learning. all these arise in the fifteen puzzle when an evaluation function is used for best-first search. in experiments  several attributes were defined  all relative to the goal. the most important is the total city-block distance of tiles from their goal positions  doran & michie  1 . the other attributes are various impediments  such as the tiles in a row being correct  except out of order  rendell  1 . this high-level representation presumes considerable knowledge  and compresses the 1 states into about 1 or 1 descriptions  depending on the exact choice of attributes. 
　　　although this compression is great  it is not the main benefit of the representation. the main benefit is to tame the evaluation function h. h maps states into probabilities  which estimate the likelihood that a state will appear on a shortest-solution path. this probability h varies with the attributes xi monotonically. for example  the smaller the city-block distance  the more likely the state will be useful. although attributes may interact somewhat  the monotonic relationship allows us to assume a linear combination: h x  = bix1 + b 1 x 1 +- + bk xk = b.x  and we need only learn the weights bi. 
　　　h can also be viewed as a class membership function  where the class is probabilistic  cf. fig. lb . h is comparable to other concepts that begin with favorable representations. in a favorable representation the attributes are matched to the problem so that  over their instance space  concepts exhibit few disjuncts or peaks  holte & porter  1; rendell  1 . if our linear model b.x is appropriate  then h has just one peak - where each xi has its extreme value. 
　　　associated with h is its weight space  over which is defined some objective function  gill et al.  1 . our objective is task performance. figure 1 shows that a performance function over a weight space defined by our attributes tends to be smooth  and may have a single optimum. the ordinate shows the average number of nodes developed before a solution was found  for a large set of arbitrarily difficult puzzles. weight space is hypothesis space  the continuous analog of a discrete version space  mitchell  1 . in version space  hypotheses are correct or incorrect. in weight space the hypotheses have degrees of correctness  shown in figure 1 with the best at the central position of each graph. such optima are surprisingly difficult to find  even for simpler spaces. 

　　　although our representation gives typical membership and objective functions  this problem is harder than some because it requires dynamic learning. this produces biased data  which are already uncertain. the data come from search trees; each node becomes a training example. an example is positive if and only if it appears on a solution path. although failures to solve give only negative examples  incomplete search trees are useful for training when combined with successful searches. depending on the learning method  several searches may compose a single iteration. an iteration is a set of searches to gather many data  followed by the computation of an improved evaluation function h to guide new searches. because h favors useful states  iterated learning requires  unbiasing'' procedures  rendell  1 . 
1. 	a selection of learning methods 
the weights of an evaluation function or the parameters of any concept description can be learned in many different ways. however  there are two basic approaches. one learns the function over instance space  fig. 1 ; the other works with hypothesis space  fig. 1 . methods that use instance space directly include curve fitting and decision tree induction. methods that work in hypothesis space include candidate elimination and optimization techniques. we first outline  then evaluate three instance space methods  two hypothesis space methods  and one combination method  see table i . 
 1  direct curve f i t t i n g . if attributes have real  integer  or binary scales  we can use statistical regression  draper & smith  1   which fits the best hyperplane h x  = b1 + b1 -+-- +- b k x k by minimizing the least squared-error for different choices of weights bi. abstractly  this is like searching weight space  fig. 1  to minimize the objective function - here the squared-error. operationally  however  regression is algorithmic - no search is necessary  because the technique inverts matrices to solve minimization equations directly. hence the method is very fast. 
　　　if our class membership values were probabilities  normal regression techniques  draper & smith  1  would apply. however  in our experiments  and in many machine learning applications  the data are binary. fortunately  binary data can be managed by analogous techniques  one of which is probit analysis  finney  1 . 
 1  probability regions and discrete evaluation. in machine learning the best known empirical technique may be induction of decision trees or partitioning of instance space  quinlan  1 . for graded concepts  the instance space is divided into regions of similar probability  breimah et al.  1 . if we have learned a concept  these regions classify instances into their probable class; if we have learned an evaluation function  the regions classify instances into their probable utility  rendell  1 . 
　　　for our fifteen puzzle problem  one pass of a partitioning algorithm is insufficient. because of the difficulty of this puzzle  early data must come from easier problems  and later data from intelligent search  causing sample bias . hence  the partitioning algorithm becomes just one operation in a scheme to form and revise regions and their probability estimates  rendell  1 . after the initial partitioning  later passes use the biased data and three other operations: unbiasing  probability updating  and region refinement. unbiasing begins by comparing averaged data within existing regions  which already provide an unbiased estimate of the probabilities. these unbiased estimates are now compared with the biased data  pairwise for each region  to extract a relationship. the relationship allows an operation to be applied to the biased probabilities to convert them to unbiased estimates. now the new probability estimates are averaged with the old to provide probability updating. the newly unbiased data are also used for a different operation: region refinement  which further subdivides the exist-
ing partition. 
 1  fitting probability regions for smoothed evaluation. although the regions output by an induction algorithm allow refined classification  the discrete nature of this approach may be insufficient. as we see in section 1  search performance may be poor if the discrete probability regions do not 

discriminate well enough. to address this problem  we could use the probability regions as data in statistical regression to find the best smoothed function h x  = 
b1 + b1 -k...1- b k x k . 
　　　instead of this complicated process of inducing regions  then fitting them  why not simply fit the original data  the answer is that the computational resources required to unbias are too great if we retain all the data. in contrast  if we iteratively repeat the operations of data gathering  unbiasing  partitioning  then regression  then our primary information structure is the compressed regions. regions are easier to update. 
 1  o p t i m i s a t i o n using response surface f i t t i n g . unlike the previous three methods which work in instance space  fig. 1   optimization methods search hypothesis space to minimize  maximize  the objective function μ. because the weights in figure 1 have real values  and because μ is smooth and appears to have only one peak  a hill-climbing method is suitable  gill et al  1 . a hill-climbing technique selects weight vectors b by moving in the direction of improving μ b . one technique suitable for noisy domains is response surface fitting with a diminishing grid. in 
this method we select points b at corners and midpoints of a hypercube in weight space  then gather data  μ values  to fit a quadratic  a parabola . the optimum predicted by the parabola allows us to gather more refined data  as we gradually hone in on the optimum μ by repeatedly shrinking the grid. starting with a large grid detects broad tendencies; shrinking the grid improves accuracy. 
　　　despite the quality of this method  a serious problem in our case is that most values of μ cannot be found! this is because most choices of b give such poor performance that typical problems cannot be solved within reasonable time. to offset this problem  initial runs used easier problems. 
 1  o p t i m i z a t i o n using a genetic algorithm. genetic algorithms are designed to optimize an objective function μ called the fitness  holland  1 . given a population  hypotheses b are selected stochastically for breeding  with probabilities proportionate to μ b . hypotheses are usually represented as bit vectors called genotypes. binary operations such as crossover are applied to pairs of genotypes; unary operations such as mutation are applied to single genotypes. the operations produce a new generation of hypotheses. because they implement parallel search  genetic algorithms can manage badly behaved objective functions. 
　　　for this reason  a genetic algorithm seems unnecessarily powerful for the problem illustrated in figure 1. massively parallel search is not required and it may be costly. we still have the problem of computing μ for mediocre choices of b  though once again we can do some preliminary search. 
 1  c o m b i n i n g p a r t i t i o n i n g and parallelism. our final method is a combination of partitioning in instance space and parallel search in hypothesis space. 
this method uses a modified genetic algorithm to govern multiple partitioning  rendell  1 . the  genotype'' is compressed and variable - it is a set of probability regions  fig. lb . each of these structures produces a different evaluation function h  which is then given some puzzles to produce data of two types: detailed data for updating probabilities and refining regions  and overall data for measuring performance μ. this allows the selection of individual regions for ksexual crossover. 
1. 	comparative results 
table i summarizes representative results of many experiments for each of the six methods described in the previous section. to assess these methods we use the number of nodes developed μ. because other computation is negligible  μ is a good measure of both the learning time and the quality of the resulting evaluation function.  another measure  the length of the solution  was found to track μ.  on a vax 1  1 nodes take about 1 minute cpu time. the average values of μ shown in the table were obtained by solving 
1 puzzles  giving a standard error of about 1  or 1% . although some related work appeared in  rendell  1  and  rendell  1   most of these results are new. all experiments used the same four attributes of figure 1. 
 1  direct curve fitting. to fit the best hyperplane h x  = b1 x1 + b 1 x 1 +...... + b k x k to the binary data from search trees  probit analysis  finney  1  was used. because the technique is algorithmic  it is fast. however  the quality  nodes developed  is 1- 1% worse than optimal  and this is after favorable interpretation of the results. to begin the experiment  puzzles nine moves from the goal were given  greater difficulty requires too much computation . the resulting data gave a non-zero weight only for the cityblock distance x1 because easy puzzles can hardly have the impediments described by the other attributes x1  x1  and x1. to continue the experiment  the resulting evaluation function was used to solve harder puzzles and find impediment weights. these weights   b1  b1  and b1   now approximated the correct ones  but because of the biasing effect of the city-block distance already in h  the new value of b1 was in error by 1%. further experiment gave similarly biased and unpredictable values  although if we take the value of b1 from the first iteration and the values of b1  b1  and b1 from the second  we obtain the performance shown. 
 1  probability regions and discrete evaluation. the second method is to partition instance space into regions of similar probability  of a state's appearing on a short solution . because the technique is iterative  it requires not only initial partitioning  but also partition refinement after data unbiasing  see section 1 . the improving quality of the evaluation function over repeated iterations allows the solution of harder problems  which provide more representative data. this alternation of sampling and learning allows each process to speed the other. but the evaluation is discrete: 

for task performance the states are classified into discrete regions. this lack of smoothing or interpolation causes poor performance: at least four times optimal quality.  the number of nodes developed could not be tested precisely  because resource limits were often exceeded.  
 1  f i t t i n g probability regions for smoothed evaluation. this is the same as the previous method except that at the end of each iteration the probability regions are used as data to find weights bi for the linear combination h x  = b 1 x i + b 1 x 1 + + b k x k . after convergence in half a dozen iterations  this smoothed evaluation function gave near-optimal performance of 1  compare fig. 1 and see  rendell  1  . although h contains the only knowledge used for solving  the probability regions provide the primary information  and are more suitable for dynamic learning. one advantage of this and other instance space methods is that every state counts. one drawback of this and most methods is that for fast learning  the difficulty of the training problems must be just at the current performance limit. this requires some user experience. moreover  the combination of approximate techniques  e.g.  unbiasing  and search that relies on previous learning  over repeated iterations can cause problems. to some extent the iterative learning seems to be self-correcting  but often performance degrades slightly or levels off. one cure is user experience; another is repeated runs. experiments have shown that about ten runs are required for a result within 1% of optimal. a similar criticism applies to most methods  so their learning times are multiplied by ten in cololmn 1 of table i   effective cost  . 
 1  optimisation using response surface fitting. rather than probabilities over instance space  optimization methods use summary measures of performance μ over hypothesis space. summary measures cost more to obtain: concept accuracy requires the classification of many instances; search performance requires the solution of whole problems. for each problem solved  only a single value is obtained - the number of states developed. this contrasts with the first three methods  which identify each count with a point in instance space. this design difference explains the difference in learning times: hill climbing in weight space is slower. initially  most values of the weight vector b are so poor that problems cannot be solved. to counteract this problem  a preliminary round of curve fitting  row l  was used to obtain approximately correct weights. 
 1  optimization using a genetic algorithm. approximately correct weights were also given to the genetic algorithm  row 1 . genetic algorithms need a well-chosen representation. if the genotype is too short  resolution will be lost; if this bit string is too long  time will be wasted. to ensure better performance  the graphs in figure 1 and some preliminary runs were analyzed to choose a genotype length of six bits. another variable is the population size. several experiments used populations up to 1. the less than optimal performance of 1  1% worse than optimal  for the best individual in a population of 1 is perhaps not too surprising because genetic algorithms are not designed for obtaining 1% accuracy when the objective function μ is unimodal  but rather for approaching multiple optima in parallel when the function is badly behaved. the high cost results from so much search  hundreds of states in each of many problems  for so little  a single performance value for each solution . another problem is that verification of an individual weight vector requires a larger sample than during learning. for each weight vector suspected to be close to optimal  many test problems must be solved  which typically costs 1 nodes per candidate vector. 
 1  combining 	partitioning 	and 	parallelism. 
perhaps surprising is the sixth result. this gives the best performance  although not significantly better than row 1 because the standard error is about 1  or 1%  in all rows . the cost appears higher than for the extended partitioning method  by a factor of six. superficially  then  it seems that extended partitioning 

is better when used alone than when combined with a genetic algorithm. however  this cursory assessment is misleading. the favorable learning speed of extended partitioning in row 1 results from considerable user experience with training. in fact the learning speeds given in rows 1 through 1 are about an order of magnitude too low  reflected in col. 1 . in contrast  the combined method of row 1 needs little user guidance because it is much less sensitive to training problems and evaluation errors. even with a small population of 1 or 1  this method is extremely stable. it is reliable and easy to use. furthermore  this method avoids the cost of verification  because individual weight vectors need not be tested. rather  all the regions from all the individual partitions can be used as a single large data set to fit a very accurate evaluation function   rendell  1  elaborates . 
1. 	discussion 
this study of six empirical-learning methods suggests ways to cope with domains that are simultaneously probabilistic  dynamic  noisy  and biased. one recommendation is to combine types of methods. even for our numeric domain  the standard methods of curve fitting  1  and optimization  1  were limited because of biased data  in l  and lost information  in 1 . furthermore  a standard method of instance space partitioning or decision tree induction  despite its extension for progressive refinement after data  unbiasing   1   was inadequate because discrete classification rules were too unrefined  even though they were probabilistic. but when partitioning  1  was combined with curve fitting  1   the learning was fast and the task performance was optimal. incorporating a third technique improved learning behavior still more. like method 1  the genetic algorithm  1  could use only some of the information available in the search domain  and although this method  1  behaved relatively poorly when used alone  it stabilized method 1 and made it easier to use  cf.  quinlan  1  . 
　　　because other problems exhibit characteristics similar to the search problem we analyzed  the phenomena should generalize. probabilistic evaluation functions are probabilistic concepts  section 1 . in many real-world domains the concept is probabilistic  the learning situation is dynamic  and the data are noisy and even biased  section 1 . to reiterate the phenomena we observed: 
  instance space algorithms find class membership values h as a function of attribute values x; hypothesis space algorithms optimize overall quality values μ of entire functions h x . combinations of these two methods exploit both kinds of information. 
  instance space algorithms are fast; hypothesis space algorithms are stable. combinations may have both advantages and also be easier to use. 
the varied strengths of different techniques may provide a net gain when the methods are combined  ackley  1   ultimately  systems for very general learning may owe much to a structured combination of techniques  buchanan et al.  1 . 
