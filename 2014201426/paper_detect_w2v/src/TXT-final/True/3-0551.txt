
we study the backbone of the travelling salesperson optimization problem. we prove that it is intractable to approximate the backbone with any performance guarantee  assuming that p1=np and there is a limit on the number of edges falsely returned. nevertheless  in practice  it appears that much of the backbone is present in close to optimal solutions. we can therefore often find much of the backbone using approximation methods based on good heuristics. we demonstrate that such backbone information can be used to guide the search for an optimal solution. however  the variance in runtimes when using a backboneguided heuristic is large. this suggests that we may need to combine such heuristics with randomization and restarts. in addition  though backbone guided heuristics are useful for finding optimal solutions  they are less help in proving optimality.
1 introduction
in recent years  there has been much researchinto what makes a search problem hard. in decision problems  we now have a rich picture based upon rapid transitions in solubility and a corresponding thrashing in search algorithms for the critical constrained problems close to such  phase transitions   cheeseman et al.  1; mitchell et al.  1 . the picture is much less clear for optimization. optimization problems do not have a transition in solubility  since optimal solutions always exist. a strong candidate to replace the transition in solubility is a transition in the backbone size  monasson et al.  1 . in this paper  we study the complexity of computing and of approximating the backbone. we also look at using such backbone information to help find optimal solutions  and to prove optimality. throughout this paper  we focus on the symmetric travelling salesperson  tsp  optimization problem. this is the problem of computing the shortest tour which visits every city  where the inter-city distance matrix is symmetric. however  most of our results easily generalise to the asymmetric case.
1 backbones
in decision problems  the concept of the backbone has proven to be very useful in understanding problem hardness. for example  the backbone of a satisfiability  sat  problem is the set of literals which are true in every model  monasson et al.  1 . backbone size appears closely correlated to problem hardness  parkes  1; monasson et al.  1 . if a sat problem has a large backbone  there are many opportunities to assign variables incorrectly. such problems tend to be hard therefore for systematic methods like davis-putnam. a large backbone also means that solutions are clustered. such problems therefore can be hard to solve with local search methods like walksat.
　backbones have been less well studied in the context of optimization. in  slaney and walsh  1   the backbone of an optimization problem is defined to be the frozen decisions: those with fixed outcomes for all optimal solutions. for example  the backbone of a tsp problem is the set of edges which occur in all tours of minimal cost. such a concept again seems useful in understanding problem hardness. for instance  the cost of finding optimal  or near optimal  solutions is positively correlated with backbone size  slaney and walsh  1 .
1 computing the backbone
it is not difficult to see that it is np-hard to find the backbone of a tsp problem. if the optimal tour is unique  then the backbone is complete. a procedure to compute the backbone then trivially gives the optimal tour length. complications arise when the optimal tour is not unique and the backbone is incomplete. for example  suppose there are two disjoint but optimal tours. the backbone is then empty. nevertheless  we can find the optimal tour length using a polynomial number of calls to a procedure that determines backbone edges.
theorem 1 the tsp backbone problem  the problem of deciding if an edge is in the backbone of a tsp problem  is np-hard.
proof: we rescale the tsp problem so that only one optimal tour remains. without loss of generality  we assume that inter-city distances are integers. first  for a problem with n cities  we multiply each inter-city distance by 1n n+1 . this gives n n + 1  new bits in the least significant digits of each number. note that this only requires polynomial space. we divide these new bits into n sections  one for each city  of length n + 1 bits. for the inter-city distance between city i and city j  we set the jth bit in the ith section and the ith bit in the jth section. each tour length now encodes the list of edges visited. in particular  the ith section of the tour length has two bits  say  j and k set. this represents the fact that the tour includes edges from j to i and then to k. even if the old distance matrix had several optimal tours  the new distance matrix only has one of them. let ci be the two cities on a tour connected to the ith city. then the optimal and unique tour of the rescaled problem is the optimal tour of the unrescaled problem in which hmax c1  min c1  ... max cn  min cn i is lexicographically least. as the optimal tour is now unique  the backbone is complete. we can compute this by a polynomial number of calls to a procedure for deciding if an edge is in the backbone. 1
　it remains open whether the tsp backbone problem is np-complete or not. completeness would seem to require a short witness that a tour was optimal. we can  however  say it is both np-hard and np-easy. it is np-easy as deciding if an edge is in the backbone can be reduced to a polynomial number of calls to a tsp decision procedure. garey and johnson suggest that problems which are both np-hard and np-easy might be called np-equivalent  garey and johnson  1 . although this class contains problems which do not belong to np  the class has the property of np-complete decision problems that: unless p=np  no problem in the class can be solved in polynomialtime  and if p=np then all problems in the class can be solved in polynomial time. in other words  the tsp backbone problem is polynomial if and only if p=np.
1 approximating the backbone
even though computing the backbone is intractable in general  might we be able to approximate it 
1 sound approximation
suppose that we have an approximation procedure that returns some subset of the backbone edges. it is easy to see that  assuming p 1= np  no such procedure can be guaranteed to return a fixed fraction of the backbone in polynomial time: first  we rescale the distance matrix as before so that the optimal tour is unique and the backbone complete. the sound approximationprocedurecould be called to return at least one backbone edge - say  a b . create a new tsp by collapsing a and b into a single node a1. the cost c a1 x  for node x would be set to min  c a x  c b x  ; all other costs as before. the procedure could be repeatedly called to construct the optimal tour edge by edge in polynomial time.
　a similar argument shows that no sound approximation procedure can be guaranteed to return at least one backbone edge if the backbone is non-empty in polynomial time.
1 unsound approximation
suppose that edges returned by an approximation procedure are not guaranteed to be in the backbone. if we do not limit the number of edges incorrectly returned  then there exists a polynomial time approximation that meets any approximation ratio  that is  returns any given fraction of the backbone .
for example  the approximation procedure that returns all o n1  possible edges returns all the backbone. we therefore consider approximation procedures which limit the number of edges falsely assigned to the backbone. an approximation procedure is a  majority-approximation  iff  when the backbone is non-empty  more edges are returned that come from the backbone than do not come from the backbone. if the backbone is empty  any number of edges can be falsely returned.
theorem 1 if p 1= np then no majority-approximation procedure can be guaranteed to return a fixed fraction α or greater of the backbone edges in polynomial time.
proof: we show how such a procedure could determine if a graph  v e  has a hamiltonian path with a designated starting and ending vertex in polynomial time  contradicting p 1= np. we construct a tsp problem which can have two sorts of shortest tours. for every hamiltonian path between the starting and ending vertex  there is a corresponding tour of length n   1 where |v | = n. these tours have a non-empty set of backbone edges taken from some set s. on the other hand  if there is no hamiltonian path between the starting and ending vertex  the shortest tour is of length n  and has a non-empty set of backbone edges  denoted t  disjoint with s.
　since the approximation procedure returns at least a fixed fraction of the backbone  it returns at least one correct backbone edge. as it is a majority-approximation procedure  the number of edges in the backbone correctly returned is more than the number incorrectly returned. by computing the ratio of the number of edges of the two types returned  we can determine if the backbone is drawn from edges in s or from those in t. that is  we can determine if there is a hamiltonian path or not in polynomial time.
　we need two special gadgets. the first is a  backbonefree  connector. this connects together two nodes without introducing any backbone edges. for example  to connect node i to node j  we use the following small circuit which introduces 1 new intermediate nodes: k1  k1 and k1. all marked edges are of cost 1  and all unmarked edges are of cost n1: kj1	$
i   kj1   jj  
	kj1	%
 
a tour from i to j goes through some permutation of k1  k1  k1. no edge is in all possible tours  and thus no edge appears in the backbone. note that we can modify this  and the other circuits  to have non-zero edges costs by increasing all other edge costs appropriately. we will draw this gadget as a rectangular box between nodes i and j.
　the second gadget is a  switch  circuit. this uses three of the backbone free connector gadgets. four edges go into this circuit  two horizontally and two vertically. the switch has two modes. in the first mode  the tour enters and exits by the vertical edges. in the other mode  the tour enters and exits by the horizontal edges. the switch circuit contains 1 nodes  all of which are visited in both modes. note that no backbone edges are common between the two modes. the gadget is show below in a . as before  all marked edges are of cost 1  and all unmarked edges are of cost n1. we give the two different modes of the circuit in b  and c . we will draw this switch gadget as a square box containing an  x . xfffxxfff xfffxxfff ffffff
	a 	b 	c 
　we use these gadget to construct a tsp problem with the required shortest tours. the problem has 1 n 1 +1 nodes  n of which correspond to vertices in the graph  v e  and the rest are in n   1 switch gadgets. if the graph  v e  has an edge between vertexi and j then the tsp problemhas an edge of cost 1 between node i and j. we shall assume that the starting and ending vertices/nodes are 1 and n respectively. to finish the tour  we put a zero cost edge between node 1 and n. there are also n   1 switch circuits. the horizontal wires in the ith switch circuit are connected to node i and i + 1  1 ＋ i ＋ n   1 . the vertical wires in the jth switch circuit are connected to the j  1th and j +1th switch circuit  1   j   n   1 . the vertical wires in the 1st switch circuit are connected to node 1 and the 1nd switch circuit. the vertical wires in the n   1th switch circuit are connected to the n   1th switch circuit and to node n. all edge costs between switch circuits and nodes  and between switch circuits are zero except for the edge between node 1 and the horizontal input to the 1st switch circuit which has cost n. the cost between any two unmarked nodes is n1 as before.
　we give an example for n = 1 in which the graph  v e  has edges between nodes 1 and 1  1 and 1  and 1 and 1: m	1mzz
1x

	1m  m 1m	1mzz x
	1 zz
x
1m

	the tsp problem	derived graph
　if there is a hamiltonian path in the graph  we follow the correspondingpath in the tsp starting with node1 and ending at node n. we then exit from node n  and enter the vertical wire in the n   1th switch  and continue through the vertical wires to switch 1 where we exit and take the zero cost edge back to node 1. this completes a tour of length n   1. on the other hand  if there is not a hamiltonian path  the shortest tour visits nodes 1 to n by alternating with the n   1 switch circuits. it then takes the zero cost edge from node n back to node 1. this completes a tour of length n. 1
　similar arguments show that if p 1= np then no majorityapproximation procedure can be guaranteed to return at least one backbone edge when the backbone is non-empty in polynomial time
1 epsilon backbone
despite these negative complexity results  backbones may still be easy to compute in practice. we study here the possibility of using approximation procedures. we define the backbone of a tsp problem as the set of edges which occur in all tours within a factor  1+  of the optimal. in figure 1  we plot the fractional size of the -backbone  that is  the size of the -backbone normalized by n  against 1 +  for random euclidean tsp problems.
epsilon vs size of backbone

figure 1: fractional size of the -backbone  y-axis  plotted against approximation ratio  = 1 +   for 1 random euclidean tsp problems with 1 nodes. degree 1 regression line fitted.
　we see that tours within 1% of optimal have approximately 1% of the backbone of optimal tours. we observe similar results with non-random instances taken from tsplib. it appears that much of the backbone of a tsp problem is present when we are near to the optimal solution. as heuristics can often find near optimal solutions in a short time  it may be easy to compute a large part of the backbone in practice.
1 approximation methods
we now see if tsp heuristics can be used as the basis of an approximation method for computing the backbone. to compute if an edge is in the backbone or not  we can commit to the edge and compute the optimal tour  and then throw the edge out and re-compute the optimal tour. the edge is in the backbone iff the first tour is shorter in length than the second. suppose now that instead of computing the two optimal tour lengths  we run some good heuristic method like  lin and kernighan  1  for a polynomially bounded time. if it is true that the heuristic is good  it will frequently find close to optimal length tours  though we have no guarantee that it does . this yields a simple approximation method for computing edges that are likely to be in the backbone.
　tests were run on random euclidean tsp problems of size 1  1 and 1 nodes. most problems have unique solutions  and hence complete backbones. exceptions were: 1 problems of size 1  1 of size 1 and 1 of size 1 had backbones of size   n. table 1 shows the percentage of the backbone correctly identified using the heuristic proceedure  as well as the percentage of  false positive  results. the linkernighanheuristic is very accurate at these problem sizes  so
n = 1n = 1n = 1ave approx ratio111correct1%111 % median1111%111false1%111positivemedian111 % 1%111table 1: estimation of backbone using the lin-kernighan heuristic. 1 trials of random euclidean tsp problems at each size. approx ratio is mean  heuristicsolution / optimal-solution  of original problem.
approximation ratios were small. the heuristic is very good at identifying backbone edges and gives few false-positives. its main weakness is missing backbone edges.
1 backbone guided heuristics
one motivation for identifying backbone is to try to reduce search. for example  climer and zhang use backbone variables in asymmetric tsp problems to preprocess and simplify  climer and zhang  1 . as a second example  dubois and dequen use a backbone guided heuristic to solve 1 variable hard random 1 sat problems  dubois and dequen  1 . in the following section  we show that whilst backbone guided heuristics can be helpfulin findingoptimal solutions  we must use them with care. median runtime are good  but there are a few long runs  suggesting that a randomization and restarts strategy may be useful . in addition  we show both experimentally and theoretically  that backbone guided heuristics can be very poor when it comes to proving optimality. to make the demonstration very direct  we use a method for solving tsp problems that relies heavily on the branching heuristic. the method does not use the  cuts  that characterise state-of-the-art solvers. however  it lets us compare directly the efficacy of various branching decisions.
　we use a branch and bound solver with lower bounds provided by lagrangean relation with 1-trees  reinelt  1 . upper bounding is by a single  deterministic run of or-opt  or  1  testing all forward and reverse moves of blocks of size n/1 down to 1. the mst bound is enhanced by eliminating all other edges into a node if two incident edges are already forced into the solution. depth-first search is used  with the node having the least lower bound explored. the algorithm branches on an edge currently in the tour representing the upper bound. the edge can be chosen in one of several ways:
　bb we exclude in turn every possible edge from the solution. a comparison is then made between the new objective and the current upper bound. if the new objective is reduced  then an improvement to the upper bound has been found. if such an improving move is found  the edge giving the largest improvement is chosen. if no such improvement is found  then the edge giving the largest increase in objective is most likely to be in the backbone  so that edge is chosen.
　freqbb a frequency table is kept during search. as each new upper-boundtour is created  the frequency of appearence
meanmedianmethodtimes/snodestime/snodesbb1111freqbb1111long1111longish1111next1111rand1111short1111table 1: branching heuristics - finding the optimal solution
meanmedianlimitmethodtime/snodestime/snodesexceededbb11111freqbb11111long11111longish11111next11111rand11111short11111table 1: branching heuristics - proving optimality
of each edge is updated. edges with higher frequency are more likely to be backbone edges  so the branch choice is the edge with the greatest frequency.
　next the next unused edge in the tour is chosen long the longest unused edge in the tour is chosen short the shortest unused edge in the tour is chosen longish a method suggested by the results of this test. use long for the first n branches  then use next.
　rand a random edge is chosen. as this is nondeterministic  the heuristic was run five times on each problem.
　a branch-and-boundsolver has two phases: finding the optimal solution  and proving optimality. different heuristics do better in different phases  so we report the results separately in table 1  and table 1. we used 1 node random euclidean tsp problems with a time limit of 1 seconds on each run. table 1 shows the number of times this limit was exceeded. the time and node results include problems for which the time limit was exceeded.
　these results show a  heavy-tailed  distribution  gomes et al.  1  with many problems being solved quickly  hence a low median  but a few taking a very long time  hence a high average . randomised algorithms and restarts have been shown to reduce the average time in np-hard problems  meisels and kaplansky  1; selman et al.  1 . however  we wished to look at the  pure  algorithms for comparison.
　these results show that the bb heuristic is quite effective at finding the optimal in terms of the median number of nodes visited. however  the average is quite large  indicating the method could benefit from multiple restarts. this reduced number of nodes comes at a price in terms of runtime. other methods  such as long outperform it in runtime. in terms of proving optimality  the bb heuristic performed relatively poorly  being little better than random.
1 pathological example
as a further caution to using backbone guided branching heuristics  we present a pathological instance of the tsp problem. if we branch on a non-backbone edge  this instance can be solved in a single branch. however  if we branch first on backbone edges  we visit an exponential number of branches before proving optimality. since a search space is defined by a particular algorithm  we must define a solution method. the following algorithm is sensible  but simple enough that we can predict its behavior theoretically. it lacks the refined lp-based cuts that allow modern tsp codes to solve huge problems  but is a fairly standard basic algorithm   reinelt  1  . it has the following features:
1. uses branch and bound.
1. stops when the lower bound equals the upper bound.
1. upper bound provided by current best tour.
1. lower bound provided by 1-tree relaxation. a 1-tree issimply a minimum spanning tree  mst  with one extra edge added from a leaf to another node already in the tree. the 1-tree contains exactly one cycle  which may be a complete tour.
1. if two edges are forced to be incident to a single node then the lower-bounding procedure does not consider any other edges incident to that node.
1. preprocessing identifies nodes which have only two useful  incident edges. these two are forced into the solution. a  useful  edge here is defined to be an edge with cost less than any available upper bound  an edge with cost greater than an estimate for the entire tour will never be used in an optimal solution . an initial upper bound can be calculated using the tour  1  1  1  ... .
　the tsp problem is given in figure 1 a . node 1 can be  cloned  arbitrarily many times  and has been cloned at nodes 1 and 1. the clones connect to each neighbour at cost 1  node 1 at cost 1  and all other clones at cost 1. arcs not shown have cost 1 + 1n  and hence will never be used in an optimal solution. the upper bound for this graph is given by the tour 1  1  .. 1  1 with cost 1. the minimum spanning tree is shown in figure 1 b  and has cost 1. the bold edges  1  and  1  are fixed in due to the preprocessing. the one-tree adds  for example  1  at cost 1. the lower bound is therefore 1.
　forcing the non-backbone edge  1  excludes the  spoke  edges at nodes 1  as node 1 now has 1 edges incident. the mst  shown in figure 1 a   now has cost 1  and the 1-tree bound  using edge  1  or  1  to connect  is 1. as the lower bound is the same as the upper bound  the branch and bound algorithm completes. so if  1   or  by a symmetric argument  1   were chosen as the branch node  the procedure terminates immediately. on the other hand  branching on backbone edges would force in edges like  1 . if 1 were cloned as described above  there could be arbitrarily many of these edges. figure 1 b  shows the effect on the lower bound

is 1 ... 1 - cost 1	tree - cost 1
figure 1: the tsp problem  and its mst

1-tree bound 1-tree bound 1
figure 1: forcing edges. forced edges in bold.
of forcing edgec  1  into the solution. forcing  1  gives an mst with cost 1. the 1-tree is connected using an edge like  1  or  1  with cost 1  giving a lower bound of 1. if there were other clones of 1  when they were forced into the solution  then the lower bound would not approach the upper bound until edges between all clones have been forced. forcing out  1  leaves a problem that looks very similar to the original. it still has backbone edges round the perimeter  but an mst/1-tree which uses the  spoke -type edges around node 1 and hence gives an unachievable lower bound.
　on backtracking  forcing out  1  gives the same lower bound as the parent problem. a new upper bound can be found using the cost 1 edges. as the lower-bound is still less than the upper  we can not stop. a recursive argument shows that the right branch must be completely explored before it can be excluded. it is not until all backbone edges are forced in  or forced out  and the algorithm moves on to branch on edge  1  or  1  that the optimal is deterimend. hence the backboneheursitic would need to visit an exponentialnumber of branches to solve the problem.
1 related work
slaney and walsh demonstrated that the cost of finding optimal  or near optimal  solutions is postively correlated with backbone size  slaney and walsh  1 . however  they also showed that the cost of proving optimality is negatively correlated with backbonesize. if we have a small backbone  then there are many optimal and near-optimal tours. an algorithm like branch and bound has to do a lot of work to ensure there are no shorter tours.
　zhang has shown that asymmetric random tsp problems undergo a phase transition in the tour cost and backbone size as the precision of the distance matrix is varied  zhang  1 . he argues that similar results hold for the symmetric tsp problem  and for structured tsp problems provided distances are drawn from a common distribution. the search cost of his branch-and-bound solver also changes from easy to hard as the precision of the distance matrix is increased and the backbone size increases.
climer and zhang used an approximation method they call
 limit-crossing  to identify backbonevariables in asymmetric tsp problems  as well as their dual   fat  variables which are not part of any optimal solution   climer and zhang  1 . by committing to backbone variables and eliminating fat  we can reduce the size of the problem and thereby reduce search.
　beacham has considered the complexity of computing the backbone for a range of decision problems like the satisfiability and hamiltonian path problem  beacham  1 . he considers a slightly modified definition of backbone: the set of decisions whose negation give an unsatisfiable subproblem. this definition is equivalent to the usual one for satisfiable problems only. he shows that recognising when the backbone is empty is np-complete.
　zhang has demonstrated experimentally that there is a sharp transition in the size of the backbone of random max 1sat problems  zhang  1 . this appears to be correlated with the transition in the random 1sat decision problem.
1 conclusion
we have looked at the backbone of the travelling salesperson problem. it is not hard to see that computing the backbone is np-hard. however  we have also shown that it is hard to approximate the backbone with any performance guarantee  assuming p=1 np and there is a limit on the number of edges falsely returned . nevertheless  in practice  it appears that much of the backbone is present in close to optimal solutions. approximation methods based on good heuristics can often therefore find much of the backbone. such backbone information can be used to guide the search for an optimal solution. however  it should be used with care as the variance in runtimes when using a backbone guided heuristic is large. in addition  backbone guided heuristics are less good at proving optimality.
　what general lessons can be taken from this study  first  these intractability results are likely to generalize to other problems domains. we conjecture that it will also be hard to compute or approximate the backbone in other optimization problems  both fundamental theoretical problems like maxsat  and more practical problems like job shop scheduling . second  we predict that much of the backbone will also be present in close to optimal solutions in these other domains. approximation based methods may also work well in these cases. third  whilst backbones are a useful concept in explaining some aspects of problem hardness in optimization  it is clear that other concepts are still needed. a number of candidate measures have been proposed for decision problems  e.g. backdoor variables  that have yet to be explored for optimization.
