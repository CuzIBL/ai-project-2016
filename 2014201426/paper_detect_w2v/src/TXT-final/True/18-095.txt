 
　　a system is described that integrates vision and tactile sensing in a robotics environment to perform object recognition tasks. it uses multiple sensor systems  active touch and passive stereo vision  to compute three dimensional primitives that can be matched against a model data base of complex curved surface objects containing holes and cavities. the low level sensing elements provide local surface and feature matches which arc constrained by relational criteria embedded in the models. once a model has been invoked  a verification procedure establishes confidence measures for a correct recognition. the three dimen* sional nature of the sensed data makes the matching process more robust as does the system's ability to sense visually occluded areas with touch. the model is hierarchic in nature and allows matching at different levels to provide support or inhibition for recognition. 
1. introduction 
　　robotic systems are being designed and built to perform complex tasks such as object recognition  grasping  parts manipulation  inspection and measurement. in the case of object recognition  many systems have been designed that have tried to exploit a single sensing modality  1 1 1 . single sensor systems are necessarily limited in their power. the approach described here to overcome the inherent limitations of a single sensing modality is to integrate multiple sensing modalities  passive stereo vision and active tactile sensing  for object recognition. the advantages of multiple sensory systems in a task like this are many. multiple sensor systems supply redundant and complementary kinds of data that can be integrated to create a more coherent understanding of a scene. the inclusion of multiple sensing systems is becoming more apparent as research continues in distributed systems and parallel approaches to problem solving. the redundancy and support for a hypothesis that comes from more than one sensing subsystem is important in establishing confidence measures during a recognition process  just as the disagreement between two sensors will inhibit a hypothesis and point to possible sensing or reasoning error. the complementary nature of these sensors allows more powerful matching primitives to be used. the primitives that are the outcome of sensing with these complementary sensors are throe dimensional in nature  providing stronger invariants and a more natural way to recognize objects which are also three dimensional in nature . 
　　most object recognition systems are model based discrimination systems that attempt to find evidence consistent with a 
　　hypothesized model and for which there is no contradictory evidence . systems that contain large amounts of information about object structure and relationships potentially reduce the number of false recognitions. however  the model primitives must be computable from the sensed data. more complex object models are being built  but they are of limited power unless the sensing systems can uncover the structural primitives and relationships they contain. the approach used here allows complex and rich models of objects that extends the kinds of generic objects that can be recognized by the system. this is due to the rich nature of the surface and feature primitives the sensors compute. surfaces are the actual parts of an object that we see; the primitive computed is exactly this. holes and cavities are important visual and tactile features; these are also computed by integrating touch and vision. further  the system described here is viewpoint independent. the problems caused by visual occlusion are overcome by the ability to use active touch sensing in visually occluded areas. 
　　the domain that the system works in is one of common kitchen items; pots  pans  cups  dishes  utensils and the like. this is a rich domain and in fact contains objects representative of many other domains as well. the objects arc planar as well as volumetric  contain holes and have concave and convex surfaces. they are also decomposable into separate components that have functional semantic meaning; handles are distinct geometric parts that are used for grasping  a cup's central cavity is used to hold liquids  a spout allows one to pour a liquid  a lid covers a cavity. by basing the models of these objects on geometry and topology the system is extensible beyond this domain. the objects are modeled in a 
　　hierarchical manner which allows the matching process to proceed at different levels with support or inhibition from higher or lower levels of mode! matching. 
　　figure 1 is an overview of the system. the vision system consists of a pair of stereo mounted ccd cameras. they are mounted on a 1 dof camera frame  x  y  pan  tilt  under computer control. the tactile system consists of a one fingered tactile sensor attached to the wrist of a puma 1 robot. the control module is the overall supervisor of the system. it is responsible for guiding and directing the vision and tactile sensing modules. it also communicates with the model data base during the recognition cycle as it tries to interpret the scene. it is able to use both low level reasoning about sensory data and high level reasoning about object structure to accomplish this task. both kinds of reasoning are needed and the system's ability to toggle between the two kinds of reasoning makes it powerful. the high level reasoning allows us to use the object model as a guide for further active sensing which is accomplished by the low level sensing modules. 
　　the recognition cycle consists of initial low level sensing which limits the number of object models consistent with the sensed data. the low level sensing elements provide data for local surface and feature matches which are constrained by relational criteria embedded in the models. the system is able to eliminate models that lack the structure uncovered by the low level sensing elements. the system then invokes an object model that is globally consistent with the sensed data and proceeds to verify this model by further active sensing. verification is done at different levels  component  feature  surface  patch  and according to different 1 p. allen and r. bajcsy 
confidence measures at each level. the remaining sections of this paper are a detailed explanation of the various parts of the system. 
1. structure of the object models 
　　objects are modeled as collections of surfaces  features and relations  organized into four distinct hierarchic levels. a hierarchic model allows us to do matching on many different levels  allowing support or inhibition for a match from lower and higher levels. it also allows us to nicely separate the low level or bottom up kinds of sensing from the top down or knowledge driven sensing. 
　　the four levels of the model are the object level  the component/feature level  the surface level  and the patch level. figure 1 is a partial description of the model of a coffee mug. the details of the model are described below. 
1. object level 
the top level of the hierarchy is composed of a list of all 
 ojcct nodes in the data base. an object node corresponds to an .nstance of a single rigid object. associated with this node is a list containing a bounding box description of the object and a list of all the components  subparts  and features of this object which make up the next level of the hierarchy. for gross shape classification  a bounding box volumetric description of the object is included. the bounding box is a rectangular parallelepiped whose size is determined by the maximum extents of the object in the x  y and z directions of the model coordinate system. a complexity attribute is also included for each object which is a measure of the number of features and components that comprise an object. 
1. component/feature level 
the second level of the model is the component/feature level. 
each object consists of a number of component  subpart  nodes that are the result of a functional and geometric decomposition of an object. the components of c coffee mug are the body of the mug  the bottom of the mug  and the handle a teapot consists of a body  bottom  spout  handle and lid. they are the major subdivisions of an object  able to be recognized both geometrically and functionally. each component also has an attribute list consisting of its bounding box  surface area  and priority. the priority field is an aid for recognition in which the components are ordered as to their likelihood of being sensed. high priorities are assigned large components or isolated components in space that protrude  handles  spouts . the protruding parts may show up as outliers from the vision analysis. obscured components  such as a coffee mug bottom when in a normal pose  are assigned lower priorities. the priority is used to aid the matching probabilistically. if the object is in a regular pose  then certain parts of the object are more prominent which can- aid the matching process. each component node contains a list of one or more surfaces that make up this functional component and that constitute the next level of the hierarchy. 
　　features are entities that are useful in the recognition process. the features modeled are holes and cavities. these features are important in discrimination tasks for humans and are able to be sensed by the low level sensing. holes are modeled as right cylinders with a defined axis  centroid and regular cross section. each hole node contains the hole's axis vector  centroid vector  and a boundary curve that contains the cross section. this curve also encloses a two dimensional area   a  slice'' through the hole . the hole node contains the inertial axes of this 1d slice computed from its central moments. by defining holes in this manner  we are treating them as a negative volumetric entity  which has implications in matching. volumetric elements have an object centered coordinate system that contains an invariant set of orthogonal axes  inertial axes . by discovering such entities and computing these axes  transformations between model and world coordinates can be effected which is a requirement of viewpoint independent matching. 
　　cavities are features that are similar to holes but are not completely surrounded by surfaces. cavities may only be entered from one direction while holes can be entered from either end along their axis. an example is the well of the coffee mug where the liquid is poured. these features are modeled as containing an axis vector  a depth  a bottom point and a boundary curve. the boundary curve is closed as in a hole  allowing for a computation of inertial axes for the cavity opening. 
1. surface level 
　　the surface level consists of surface nodes that embody the constituent surfaces of a component of the object. the objects are modeled as collections of surfaces. the sensing elements that are used are vision and touch both of which sense surface information. each surface contains attributes such as bounding box  surface area  a flag indicating whether the surface is closed or not and a symbolic description of the surface such as planar  cylindrical or curved. the surfaces are decomposed according to continuity constraints. each surface is a smooth entity containing no surface discontinuities. the surfaces contain a list of the actual bicubic surface patches that comprise this surface. 
1. patch level 
　　each surface is a smooth entity represented by a grid of bicubic spline surfaces that retain c1 continuity on the composite surface . each patch contains its parametric description as well as an attribute list for the patch. patch attributes include  surface area  mean normal vector   symbolic form  planar  cylindrical  curved  and bounding box. patches constitute the lowest local matching level in the system. the patches themselves are represented in matrix form as a matrix of coefficients for a coons* patch. 
1. relational constraints 
　　it is not enough to model an object as a collection of geometric attributes. one of the more powerful approaches to recognition is the ability to model relationships between object components and to successfully sense them. the relational constraints between geometric entities place strong bounds on potential matches. the matching process is in many ways a search for consistent. between the sensed data and the model data. relational consistency enforces a firm criteria that allows incorrect matches to be rejected. this is especially true when the relational criteria is based on three dimensional entities which exist in the physical scene as opposed to two dimensional projective relationships which vary with viewpoint. 
　　in keeping with the hierarchical nature of the model  relationships exist on many levels of the model. the first level at which relational information is included is the component level. each component contains a list of adjacent components  where adjacency is simple physical adjacency between components. the features  holes and cavities  also contain a list of the components that comprise their cross sectional boundary curves. thus  a surface sensed near a hole will be related to it from low level sensing  and in a search for model consistency  this relationship should also hold in the model. at the surface level  again each surface contains t list of physically adjacent surfaces that can be used to constrain surface matching. the patch relations are implicit in the structure of the composite surface patch decomposition being used. each patch is part of an ordered larger grid of knots that contains relational adjacency automatically. thus  each patches neighbors are directly available from an inspection of the composite surfaces defining knot grid. 
1. low level sensing 
　　the sensing modalities the system uses are stereo vision and tactile sensing. there are tradeoffs in speed  accuracy and noise in using each of these sensors. the limitations of the low level sensors will constrain the accuracy of our recognition process. 
　　the sensory data received from the two sensors needs to be integrated. in designing rules for integrating data from these two sensors  there arc a few general observations. vision is global  has high bandwidth  and is noisy. touch is a low bandwidth  local  sequential process with better noise properties than vision. vision gives a sometimes confusing view of an object due to the coupling of geometry  surface reflectance and lighting. touch is better able to measure directly the properties of objects that are desired: their shape and surface properties. it also retains more degrees of freedom in sensing objects than a static camera which is limited by its viewpoint. 
　　the most important difference between these sensors though is the active  controlled nature of touch versus the passive nature of visual sensing. to use touch it needs to be guided and supplied with high level knowledge about its task. blind groping with a finger is an inefficient and slow way to recognize an object. 
1. vision sensing 
　　the object to be sensed is placed on a known support surface in an arbitrary position and orientation. it is assumed to be a single rigid object. the cameras have previously been calibrated with the world coordinate system of the robot arm. the stereo algorithm first uses a marr-hildreth operator on the images and then matches zero crossings with a stereo matcher developed by smitley . the output of this is a sparse set of 1d points that form closed contour regions. 
　　these closed contour regions can be analyzed from a connectivity standpoint to form a region adjacency graph. this graph establishes constraints on local surface matches. figure 1  upper left  contains the closed contour region analysis that results after the edge finding and stereo matching processes. stereo cannot provide information about the interiors of these regions  and the tactile system will provide this information. 
1. tactile sensing 
　　the tactile sensor being used is a rigid finger like device that is made up of 1 pressure sensitive sites. the sites are covered by a conductive elastomer that senses pressure at each site with an eight bit gray scale. the geometry of the finger also allows limited amounts of surface normal information. the sensor is mounted on the wrist of the puma 1 robot and is continuously monitored by a microprocessor that is capable of thresholding and ordering the tactile responses at each of the sites. the arm is controlled by val-ii programs that receive feedback from the tactile sensor's microprocessor. the arm can be commanded to move in an arbitrary path until the sensor reports an over threshold contact or contacts. these contact points  along with their normals are then reported back to the control module. 
1.1. surface tracing algorithms 
　　algorithms have been developed to have the tactile system do surface tracing. given a starting and ending point on a surface  the sensor traces along the surface reporting its contact positions and normals as it moves along. there are many potential paths between these two points on the surface of the object. the movement cycle of the sensor begins with contact at the starting point on the surface. the surface orientation can be determined by the location of the contact point on the sensor. the arm then moves then sensor a small distance off the surface in the direction of the surface normal. at this point a movement vector is calculated that is the weighted average of the vector to the ending point of the trace and the vector formed from previous contact points on the surface. the sensor is then moved a small distance along this vector  monitoring contact continuously. if contact occurs  the cycle repeats until the 
p. allen and r. bajcsy 1 
ending point is reached. if no contact occurs  then contact with the surface is re-established by moving in the surface normal direction towards the surface. this allows the sensor to make progress towards the goal and to stay in contact with a smoothly changing surface. using the straight line vector to the goal alone will cause cycles and no progress towards the end point of the trace. 
1.1. hole tracing 
　　holes arc a useful recognition feature that arc difficult to find using machine vision alone. a hole explorer algorithm has been developed to find and quantify a hole with the tactile sensor. holes are modeled as cylindrical  negative volume elements with an arbitrary cross section. they have an axis  the cylindrical axis  from which they can be entered and a ccntroid which is defined on the cross sectional slice of the hole. 
　　the hole tracing algorithm starts with the tactile sensor probing the region to determine if it is in fact a hole. if the tip does not come in conflict with a surface  then the region is determined to be a hole. the arm is then moved in conjunction with tactile feedback around the contour of the hole  reporting the contact points of the hole's cross section. 
1. integration of sensory data 
　　the stereo matching from vision yields a sparse set of 1d points that form closed contours. the interior surfaces of these contours cannot be determined from vision alone. to find out the nature of these closed contour regions  the tactile sensor is used to explore and quantify the interior of these regions. each region has a contour of 1d points obtained from stereo vision. a least squares plane can be fit to these points and the tactile sensor aligned normal to this plane  forming an approach angle for the sensor. the sensor then moves towards the plane  seeking to establish contact with a surface. if contact occurs  then the surface interpolation process described below proceeds. if no contact is found within a distance threshold of the least squares plane  a hole is hypothesized and a trace of the hole's contour as described above is begun. if surface contact is made after the distance threshold  then a cavity has been discovered  and its cross section is traced similarly to the hole. 
　　the surface interpolation process assumes that the region inside the 1d contour discovered by vision is curvature continuous. if the region was not curvature continuous  then a zero crossing would have been seen inside the closed contour region  1 . determining the true nature of these surfaces is the heart of the integration process. these contours can be analyzed and points of high curvature chosen as knot points for a bicubic spline interpolation process described in . once these know points are chosen  the tactile system is actively guided in tracing the interior of the closed contour region using the surface tracing algorithms described earlier. the knots create 1 boundary curves that comprise the closed contour region. each surface is traced from the midpoint of a boundary curve to the midpoint of the boundary curve opposite. these surface traces are then combined with the contour data to create a composite bicubic spline surface which preserves the smooth nature of the surface and interpolates the sensed data points. this interpolation can be done to arbitrary precision by tracing each surface at finer and finer resolutions. typically  one set of traces across the surface is sufficient to obtain a reasonable interpolation of the surface. these surface patches are powerful primitives. they are described by a set of parametric equations that allow easy and efficient calculation of surface areas  normals and curvature properties of the patches which are useful for matching. 
　　the integration of sensory data shows its complementary nature. visual data can be used to guide active tactile sensing for determination of a visual regions properties. further  this visual region can be extended into a three dimensional surface by the addition of small amounts of actively guided touch sensing. 
1 p. allen and r. bajcsy 
1. model invocation 
　　the recognition process begins with the low level vision modules in a bottom up fashion. these modules create a list of closed contour regions and a region adjacency graph. this is input to the tactile system which will either interpolate a surface patch or determine that the region is a hole or cavity. this low level sensing defines the primitives that are sent to the higher level matching system to try to instantiate a model consistent with this sensory data-
1. local matching 
　　local mulching is controlled by a set of rules that drive the process. the rules establish local surface matches for a sensed surface patch by finding surfaces in the model surface list that satisfy all the constraints below: 
  the area of the sensed surface patch must be less than or equal to the model surface area. 
  the bounding box of the sensed surface patch must be contained within the bounding box of the model surface. 
  if the sensed surface patch is planar  the model surface must be planar. 
  if the sensed surface patch is cylindrical  the model surface must be cylindrical and the difference of the radii of the cylinders must be small. 
  if the sensed surface patch is curved  the model surface must be curved 
there is also a set of local matching rules for features: 
  if the feature is a hole or cavity  the diameter of the cross section must be within a threshold ft of the model diameter. 
  if the feature is a cavity  its depth must be within a threshold of the model cavity depth. 
　　if the local surface/feature matching is also rclationaiiy consistent  then the model is considered feasible. all local surface/feature matches must be consistent with the model relations. if a relationship exists in the scene between two matched surfaces that are not adjacent in the model  the local surface matches are rejected. if a feature is found  its related surfaces must also be consistent. by applying these relational constraints to the local matches  incorrect matches are rejected. 
1. choosing a model 
　　once the low level initial sensing has been done  a model must be chosen to drive the recognition process to the next stage. there are three possible outcomes of the initial sensing and local matching process: 
  no model is consistent with the data. 
  one model is consistent with the data. 
  more than one model is consistent with the data. 
　　the first case reflects a possibility of sensory error. the moat effective strategy here is to redo the sensing or change the view point to reflect new sensory input. the second case is a desirable state of affairs; this one model is then a candidate for further verification sensing. the third case points to the need for further discrimination among the models. typically  the local matching process with its geometric and relational constraints will prune the number of consistent interpretation* to a small number of models  two or three . the ambiguity between models can be resolved at the next level of sensing. in this case  one of the two or three models left will be chosen as the invoked model to be verified. choosing this model will rely on the object priorities discussed above and the model complexity measure. each consistent model has a matching measure computed based upon the normalized probability of the model components and features matched and the complexity of the object's structure. 

where m is the matching probability measure  pk is the 
priority of each of the matched components and n is the total number of component/ features for this object. 
　　the choice is weighted in favor of complexity of the objects  and with equal complexity  on finding higher probability matches. the sensory subsystems are more likely to sense the high priority components and features. this is then reflected in the choice of model that is invoked. an incorrect choice will be found during verification  at which time this model is rejected and the remaining ones are again chosen by this rule. 
1. verification 
　　once a model has been chosen  the recognition process becomes a verification process  trying to substantiate the hypothesis that the chosen model is the object to be recognized. verification proceeds initially on a component/ feature level. some of the component/features of this model have been partially verified by the local surf ace/feature matching. partial verification means that a component  possibly made up of a number of surfaces  has had at least one of these surfaces matched at the lower surface level. any component/feature that has not been partially verified is then put on a list for verification. an important aspect of this verification is using the model to drive the recognition process. to reflect the model in the imaged scene  a transformation matrix relating model coordinates to imaged world coordinates is necessary. the calculation of this matrix will allow accurate scene prediction for features and components. 
1. scene to model transformation 
　　the transformation between model and scene consists of three translational parameters and three rotational parameters. this transformation can be computed from matched features or surfaces. holes have an axis vector and inertial axes of their cross section in the model. if hole is matched  these axes can be used to align the model with the sensed hole  thus satisfying the rotational component of the transformation. the centroid of the hole can then be translated to the sensed centroid to calculate the translational parameters. similarly  cavities are modeled as having an axis and bottom point. these can be used in the same manner as the hole axis and centroid to compute the transformation matrix. 
　　sensed surface data can also be used to calculate the transformation matrix. the approach is to match points on sensed surfaces with model surface points. candidates for these matching points are points of maximum or minimum curvature or at surface border vertices. by using several match points a least square fit of the transformation matrix can be calculated . planar surfaces can also be used to define part of the transformation by aligning sensed surface normals with model normals. once this matrix is calculated  it can be used to verify that the initial set of local matches of surfaces and features is consistent with the calculated transformation matrix. if they are not  the system returns to the local matching level and determines a new relation ally consistent set of matches  recalculating the transformation matrix. if there is only one rclationally consistent set of local matches  then those local matches that are inconsistent with the transformation are rejected and the others accepted. 
1. verification sensing 
　　the verification of the object model takes place at many levels. the top level is at the component/feature level. verification at this level is done by requiring every component to be at least partially verified or an occlusion computed from the transformation matrix which accounts for its inability to be sensed. in verifying a component  the surfaces that comprise that component must be sensed. this is done by examining the patches that comprise each surface. the center point of each patch  calculated as the parametric center from the parametric patch equations  is transformed by the matrix and projected into the camera space. if these projected points lie inside a closed contour region that has been locally matched  they are occluded. if they lie outside a 
　　closed contour region or inside an unmatched region  then they can be sensed by integrating vision and touch as described earlier. if the points are determined to be occluded  then they must be sensed 
by the tactile system alone. this is done by actively guiding the tactile sensor to these points and sensing the surface normal at this point  which should be consistent with the transformed mean normal for the patch. by verifying patch center points and normals  partial verification of a component is accomplished. 
　　holes arc verified by applying the transformation matrix to the hole axis and centre id and actively using the tactile system to verify the holes existence and cross sectional curve. cavities similarly are verified by applying the transformation to the cavities axis and bottom point and again using the active tactile system to sense the cavity. 
　　some components will not be able to be sensed due to support surface occlusion. an example of this is the coffee mug where the bottom component of the mug cannot be sensed since it is on the support surface. the application of the computed transformation matrix to this surface will reveal that the surface is coincident with the support surface. 
1. confidence measures 
　　verification can be a time consuming and lengthy process as it involves many levels of sensing. once the component/feature level is verified  a further verification can proceed at the surface level and finally at the patch level. rather than continue sensing all parts of the object  a confidence measure for each level is established. in the limit  by sensing all modeled surfaces and features and patches  verification will be complete. however  it will not be physically possible to verify all of these parts of the model due to occlusion and inherent limitations in the sensors themselves. therefore  a measure needs to be established that computes the confidence of the match. one can then predetermine what confidence measures arc necessary for acceptance or rejection of a model. 
　　the models used are hierarchic in nature. confidence measures can be set up at each level of the model allowing acceptance or rejection based on different requirements for each level. at the component/feature level  a measure of confidence is the fraction of total component/features partially matched. we can extend this idea of a partial match at each level by including threshold criteria for accepting a partial match. a partial match at the component level means that some fraction of the surface's that comprise that component are matched. a partial match at the surface level means that some fraction of the patches that comprise a surface are matched. by specifying vi  a verification fraction at each level i of the model  hierarchical acceptance criteria will determine the amount of sensing to be done. these verification criteria can be global or used on a per object basis  implying different amounts of active sensing for different hypotheses. 
　　one advantage to this approach is that partial matching can be carried out. since the matching is local with global constraints  partial matches can be made and reported even though a global match is rejected. a further extension would be to articulated parts  where local transformations between components and surfaces would have to be accounted for in the global constraints. 
1. conclusion 
　　the system described is currently being tested. the experimental hardware systems  model data base and surface and feature 
p. allen and r. bajcsy 1 
matching routines are built. work is presently continuing on the higher level reasoning modules. as the number of objects in the database grows  more sophisticated access mechanaisms for indexing into the models will be needed. 
1. acknowledgements 
　　this work was supported in part by the following grants: aro daa1-1-k-1  afosr 1-nm-1  nsf mcs-t:1-
cer  nsf mcs 1  avro daab1-k f1  and nih 1-ro1-hl-1. 
