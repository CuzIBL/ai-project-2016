
we consider a class of games with real-valued strategies and payoff information available only in the form of data from a given sample of strategy profiles. solving such games with respect to the underlying strategy space requires generalizing from the data to a complete payoff-function representation. we address payoff-functionlearning as a standard regression problem  with provision for capturing known structure  symmetry  in the multiagent environment. to measure learning performance  we consider the relative utility of prescribed strategies  rather than the accuracy of payoff functions per se. we demonstrate our approach and evaluate its effectiveness on two examples: a two-player version of the first-price sealed-bid auction  with known analytical form   and a five-player marketbased scheduling game  with no known solution .
1 introduction
game-theoretic analysis typically begins with a complete description of strategic interactions  that is  the game. we consider the priorquestion of determiningwhat the gameactually is  given a database of game experience rather than any direct specification. this is one possible target of learning applied to games  shoham et al.  1 . when agents have few available actions and outcomes are deterministic  the game can be identified through systematic exploration. for instance  we can ask the agents to play each strategy profile in the entire joint strategy set and record the payoffs for each. if the joint action space is small enough  limited nondeterminism can be handled by sampling. coordinating exploration of the joint set does pose difficult issues. brafman and tennenholtz  for example  address these carefully for the case of commoninterest stochastic games  brafman and tennenholtz  1   as well as the general problem of maintaining an equilibrium among learning algorithms  brafman and tennenholtz  1 .
﹛further difficulties are posed by intractably large  or infinite  strategy sets. we can make this problem tractable by reducing the number of profiles that agents are allowed to play  but this comes at the cost of transforming the game of interest into a different game entirely. instead  we seek to identify the full game  or at least a less restrictive game  from limited data  entailing some generalization from observed instances. approximating payoff functions using supervised learning  regression  methods allows us to deal with continuous agent strategy sets  providing a payoff for an arbitrary strategy profile. in so doing  we adopt functional forms consistent with prior knowledge about the game  and also admit biases toward forms facilitating subsequent game analysis  e.g.  equilibrium calculation .
﹛in this paper  we present our first investigation of approximating payoff functions  employing regression to low-degree polynomials. we explore two example games  both with incomplete information and real-valued actions. first is the standard first-price sealed bid auction  with two players and symmetric value distributions. the solution to this game is well-known  krishna  1   and its availability in analytical form proves useful for benchmarking our learning approach. our second example is a five-player market-based scheduling game  reeves et al.  1   where time slots are allocated by simultaneous ascending auctions  milgrom  1 . this game has no known solution  though previous work has identified equilibria on discretized subsets of the strategy space.
1 preliminaries
1 notation
a generic normal form game is formally expressed as
 i {=  |is|iis }the {unumberi s }   whereof players.i refers to the set of players andsi is the set of strategies m
availableis the payoffto playerfunctioni ﹋siiof. finally andplayertheusetii whens    :ssi1all ℅﹞﹞﹞℅isplayersthe simplexsmjointly↙ rof mixed strategies over play s =  s1 ... sm   with each sj ﹋ sj.utility as isallow-common  we assume von neumann-morgenstern ing an agent i's payoff for a particular mixed strategy profile to be ui 考  = ps﹋mixeds 考1 sstrategy1 ﹞﹞﹞考mof splayerm  ui si   assign-where ing考j : sj ↙  1 toiseacha pure strategy sj ﹋addsj tosuch1  thati.e. 
a probability
all probabilities over the agent's strategy set
考jit﹋will  softenj  . be convenient to refer to the strategy  pure or mixed  of player i separately from that of the remaining strategy of all players other than player i .i to denote the joint players. to accommodate this  we use s
1 nash equilibrium
in this paper  we are concerned with one-shot normal-form games  in which players make decisions simultaneously and accrue payoffs  upon which the game ends. this single-shot nature may seem to preclude learning from experience  but in fact repeated episodes are allowed  as long as actions cannot affect future opportunities  or condition future strategies. game payoff data may also be obtained from observations of other agents playing the game  or from simulations of hypothetical runs of the game. in any of these cases  learning is relevant despite the fact that the game is to be played only once.
﹛faced with a one-shot game  an agent would ideally play its best strategy given those played by the other agents. a configuration where all agents play strategies that are best responses to the others constitutes a nash equilibrium.
definition 1 a strategy profile s =  s1 ... sm  constitutes a  pure-strategy nash equilibriumof game  i {si} {ui s }  if for every i ﹋ i  s1i ﹋ si  ui si s i  ≡ ui s1i s i .
a similar definition applies when mixed strategies are allowed.
definition 1 a strategy profile 考 =  考1 ... 考m  constitutes a mixed-strategy nash equilibrium of game u i i {考  i 考s ii  } ≡{uuii  s考 i1} 考   ifi for. every i ﹋ i  考i1 ﹋   si  
﹛in this study we devote particular attention to games that exhibit symmetry with respect to payoffs.
definition 1whenever i j ﹋ i si a=a  ssgameji and= s si j iand{=  ss  bij  }u i{ usii  ss  }i  is=symmetricuj sj s jif 
symmetric games have relatively compact descriptions and may present associated computational advantages. given a symmetric game  we may focus on the subclass of symmetric equilibria  which are arguably most natural  kreps  1   and avoid the need to coordinate on roles. in fairly general settings  symmetric games do possess symmetric equilibria  nash  1 .
1 payoff function approximation
1 problem definition
we are given a set of data points  s v   each describing an instance where agents played strategy profile s and realized value v =  v1 ... vm . for deterministic games of complete information  v is simply u. with incomplete information or stochastic outcomes  v is a randomvariable  more specifically an independent draw from a distribution function of s  with expected value u s .
﹛the payoff function approximation task is to select a functiondeviationu  fromfroma candidatethe true payoffset u minimizingfunction u. somebecausemeasurethe trueof function u is unknown  of course  we must base our selection on evidence provided by the given data points.
﹛our goal in approximating payoff functions is typically not predicting payoffs themselves  but rather in assessing strategic behavior. therefore for assessing our results  we measure approximation quality not directly in terms of a distance between u  and u  but rather in terms of the strategies dictated by u  evaluated with respect to u. for this we appeal to the notion of approximate nash equilibrium.
definition 1 a strategy profile 考 =  考1 ... 考m  constitutes an -nash equilibrium of game  i {  si } {ui s }  if for every.
﹛we propose using  in the above definition as a measure of approximation error of u   and employ it in evaluating our learning methods. when u is known  we can compute  in a straightforward manner. let s i denote i's bestresponse correspondence  defined by s i 考
snashargmax i 考 i  sto be single-valued.i ui ofsigame 考 i } i . for{  letgamesclarityi }考  be a solution  e.g.  a nash {i u ofi{   sexposition  }s  .i ithen} . ={ui{ xswe }:  x takefor﹋equilibrium 	equilibrium of the true
since in general u will either be unknown or not amenable to this analysis  we developed a method for estimating  from data. we will describe it in some detail below.
﹛for the remainder of this report  we focus on a special case of the general problem  where action sets are real-valued intervals  si =  1 . moreover  we restrict attention to symmetric games and further limit the number of variables in payoff-function hypotheses by using some form of aggregation of other agents' actions.1 the assumption of symmetry allows us to adopt the convention for the remainder of the paper that payoff u si s i  is to the agent playing si.
1 polynomial regression
one class of models we considerare the nth-degreeseparable polynomials:

played by  agentsi  representsother thansomei. aggregationfor two-playerof thegames strategies耳 is where 耳 s
simply the identity function. we refer to polynomials of the form  1  as separable  since they lack terms combining si and sthe i.non-separable quadraticwe also consider models: with such terms  for example 

note that  1  and  1  coincide in the case n = 1 and c = 1. in the experiments described below  we employ a simpler version of non-separable quadratic that takes b1 = b1 = 1.
﹛one advantage of the quadratic form is that we can analytically solve for nash equilibrium. given a general nonseparable quadratic  1   the necessary first-order condition
reducesnon-separableto si case=  witha1additivea1 in theaggregationseparable  case.耳i  sum/1a fors1 . ithis the= for an interior solution is si =   a1 + c耳 s
s   we can derive an explicit first-order condition for j1=i j symmetric equilibrium: si =  a1/ 1 +  m   1 c .
while a pure-strategy equilibrium will necessarily exist for any separable polynomial model  it is only guaranteed to exist in the non-separable case when the learned quadratic is concave. in the experiments that follow  when the learned non-separable quadratic does not have a pure nash equilibrium  we generate an arbitrary symmetric pure profile as the approximate nash equilibrium.
﹛another difficulty arises when a polynomial of a degree higher than three has more than one nash equilibrium. in such a case we select an equilibrium arbitrarily.
1 local regression
in addition to polynomial models  we explored learning using two local regression methods: locally weighted average and locally weighted quadratic regression  atkeson et al.  1 . unlike model-based methods such as polynomial regression  local methods do not attempt to infer model coefficients from data. instead  these methods weigh the training data points by distance from the query point and estimate the answer-in our case  the payoff at the strategy profile point-using some function of the weighted data set. we used a gaussian weight function: w = e d1  where d is the distance of the training data point from the query point and w is the weight that is assigned to that training point.
﹛in the case of locally weighted average  we simply take the weighted average of the payoffs of the training data points as our payoff at an arbitrary strategy profile. locally weighted quadratic regression  on the other hand  fits a quadratic regression to the weighted data set for each query point.
1 support vector machine regression
the third category of learning methods we used was support vector machines  svms . for details regarding this learning method  we refer an interested reader to  vapnik  1 . in our experiments  we used svm light package  joachims  1   which is an open-sourceimplementation of svm classification and regression algorithms.
1 finding mixed strategy equilibria
in the case of polynomial regression  we were able to find either analytic or simple and robust numeric methods for computing pure nash equilibria. with local regression and svm learning we are not so fortunate  as we do not have access to a closed-form description of the function we are learning. furthermore  we are often interested in mixed strategy approximateequilibria  and ourpolynomialmodelsand solution methods yield pure strategy equilibria.
﹛when a particular learned model is not amenable to a closed-form solution  we can approximate the learned game with a finite strategy grid and find a mixed-strategy equilibrium of the resulting finite game using a general-purpose finite-game solver. we employed replicator dynamics  fudenberg and levine  1   which searches for a symmetric mixed equilibrium using an iterative evolutionary algorithm. we treat the result after a fixed number of iterations as an approximate nash equilibrium of the learned game.
1 strategy aggregation
as noted above  we consider payoff functions on two-
dimensional strategyas longprofilesas 耳 s iin  istheinvariantin sform i functionstheunderu payoffsi sdifferent forifunc-  our= permutations i  .of the same strategies f si 耳 s
tion is symmetric. since the actual payoff example games are also known to be symmetric  we constrain thatin our experiments  we compared three variants of耳 s i  preserve the symmetry of the underlying game.耳 s  .
first and most compact is the simple sum   where耳sum耳 sss  si . sec- i
pondj=1 isi sthej the1. orderedstrategies inthe thirdpairvariant their direct  unaggregated form. 耳sum耳 耳identityss	 s i  = s  i i sim-to=
ply takes
enforce the symmetry requirement in this last case  we sort the strategies in s i.
1 first-price sealed-bid auction
in the standard first-price sealed-bid  fpsb  auction game  krishna  1   agents have private valuations for the good for sale  and simultaneously choose a bid price representing their offer to purchase the good. the bidder naming the highest price gets the good and pays the offered price. other agents receive and pay nothing. in the classic setup first analyzed by vickrey   agents have identical valuation distributions  uniform on  1   and these distributions are common knowledge. the unique  bayesian  nash equilibrium of this game is for agent i to bid mm 1xi  where xi is i's valuation for the good.
﹛note that strategies in this game  and generally for games oftionsincompleteof the agent'sinformation  private information.bi :  1  ↙we 1consider 1   are func-a restricted case  where bid functions are constrained to the form bactioni xi  = kixi  ki ﹋  1interval  1 . thiscorrespondingconstraint transformsto choicetheof space to a real
 parameter ki. we can easily see that the restricted strategy space includes the known equilibrium of the full game  with  for all i  which is also an equilibrium of the restricted game in which agents are constrained to strategies of the given form.
﹛we further focus on the special case m = 1  with corresponding equilibrium at s1 = s1 = 1. for the two-player fpsb  we can also derive a closed-form description of the actual expected payoff function:
	1	if s1 = s1 = 1 
 	 1 
.
﹛the availability of known solutions for this example facilitates analysis of our learning approach. our results are summarized in figure 1. for each of our methods  classes of functional forms   we measured average  for varying training set sizes. for instance  to evaluate the performance of separable quadratic approximation with training size n  we independently draw n strategies  {s1 ... sn}  uniformly on  1 . the corresponding training set comprises o n1  points:   si sj  u si sj    for i j ﹋ {1 ... n}  with u as given by  1 . we find the best separable quadratic fit u  to these points  and find a nash equilibrium corresponding to u . we then calculate the least  for which this strategy profile is an -nash equilibrium with respect to the actual payoff function u. we repeat this process 1 times  averaging the results over strategy draws  to obtain each value plotted in figure 1.

figure 1: epsilon versus number of training strategy points for different functional forms.

figure 1: learned and actual payoff function when the other agent plays 1. the learned function is the separable quadratic  for a particular sample with n = 1.
﹛as we can see  both second-degree polynomial forms we tried do quite well on this game. for n   1  quadratic regression outperforms the model labeled  sample best   in which the payoff function is approximated by the discrete training set directly. the derived equilibrium in this model is simply a nash equilibrium over the discrete strategies in the training set. at first  the success of the quadratic model may be surprising  since the actual payoff function  1  is only piecewise differentiable and has a point of discontinuity. however  as we can see from figure 1  it appears quite smooth and well approximated by a quadratic polynomial. the higher-degree polynomials apparently overfit the data  as indicated by their inferior learning performance displayed in this game.
﹛the results of this game provide an optimistic view of how well regression might be expected to perform compared to discretization. this game is quite easy for learning since the underlying payoff function is well captured by our lowerdegree model. moreover  our experimental setup eliminated the issue of noisy payoff observations  by employing the actual expected payoffs for selected strategies.
1 market-based scheduling game
the second game we investigate presents a significantly more difficult learning challenge. it is a five-player symmetric game  with no analytic characterization  and no  theoretically  known solution. the game hinges on incomplete information  and training data is available only from a simulator that samples from the underlying distribution.
﹛the game is based on a market-based scheduling scenario  reeves et al.  1   where agents bid in simultaneous auctions for time-indexed resources necessary to perform their given jobs. agents have private information about their job lengths  and values for completing their jobs by various deadlines. note that the full space of strategies is quite complex: it is dependent on multi-dimensional private information about preferences as well as price histories for all the time slots. as in the fpsb example  we transform this policy space to the real interval by constraining strategies to a parametrized form. in particular  we start from a simple myopic policy- straightforward bidding  milgrom  1   and modify it by a scalar parameter  called  sunk awareness   and denoted by k  that controls the agent's tendency to stick with slots that it is currently winning. although the details and motivation for sunk awareness are inessential to the current study  we tradeoffs  generally dependent on other agents' behavior.note that k ﹋  1   and that the optimal setting of k involves
﹛to investigate learning for this game  we collected data for1all.1strategy ... 1}strategy profiles.profilesaccountingover theforfor evaluation purposes  wediscretesymmetry set thisof valuesrepresentsk ﹋
{  1 distinct . 1
treat the sample averages for each discrete profile as the true expected payoffs on this grid.
﹛the previous empirical study of this game by reeves et al.  estimated the payoff function over a discrete grid of profiles assembled from the strategies
{equilibrium1 1 1using 1 replicator1}  computingdynamics.an approximatewe thereforenash generated a training set based on the data for these strategies  1 samples per profile   regressed to the quadratic forms  and calculated empirical  values with respect to the entire data set by computing the maximum benefit from deviation within the data: 
maxi﹋i maxset ofsi﹋playersi  ui isrepresented withini s  i    ui s     wherethe datasset.i issincethe strategy
the game is symmetric  the maximum over the players can be dropped  and all the agent strategy sets are identical.
﹛from the results presented in table 1  we see that the nash equilibria for the learned functions are quite close to that produced by replicator dynamics  but with  values quite a bit lower.  since 1 is not a grid point  we determined its  post hoc  by running further profile simulations with all agents playing 1  and where one agent deviates to any of the strategies in {1.1 ... 1}. 
methodequilibrium siseparable quadratic11non-separable quadratic11replicator dynamics 1.1.1 1 1table 1: values of  for the symmetric pure-strategy equilibria of games defined by different payoff function approximation methods. the quadratic models were trained on profiles confined to strategies in {1 1 1 1 1}.
﹛in a more comprehensivetrial  we collected 1 million additional samples per profile  and ran our learning algorithms on 1 training sets  each uniformly randomly selected from the discrete grid {1.1 ... 1}. each training set included profiles generated from between five and ten of the twentyone agent strategies on the grid. since in this case the profile of interest does not typically appear in the complete data set  we developed a method for estimating  for pure symmetric approximate equilibria in symmetric games based on a mixture of neighbor strategies that do appear in the test set. let us designate the pure symmetric equilibrium strategy of the approximated game by s . we first determine the closest neighbors to s  in the symmetric strategy set s represented within the data. let these neighbors be denoted by s1 and s1. we define a mixed strategy 汐 over support {s1 srelative1} as the probability of playing s1  computed based on the
note that symmetry allows a more compact representation ofdistance of s  from its neighbors: 汐 = 1   |s   s1|/|s1   s1|. a payoff function if agents other than i have a choice of only two strategies. thus  we define u si j  as the payoff to a choose whether toagents symmetricplay strategyplayerplayfors1.playingsif1 with probabilitym  strategy1 agentssieach汐﹋ sthen the proba-independentlywhen j other bility that exactly j will choose s1 is given by
pr .
we can thus approximate  of the mixed strategy 汐 by
.
﹛using this method of estimating  on the complete data set  we compared results from polynomial regression to the method which simply selects from the training set the pure strategy profile with the smallest value of . we refer to this method as  sample best   differentiating between the case where we only consider symmetric pure profiles  labeled  sample best  symmetric    and all pure profiles  labeled  sample best  all   .1
	x 1	separable quadratic

figure 1: effectiveness of learning a separable quadratic model with different forms of 耳 s i . x 1 non separable quadratic

figure 1: effectiveness of learning a non-separable quadratic model with different forms of 耳 s i .
﹛from figure 1 we see that regression to a separable quadratic produces a considerably better approximate equilibrium when the size of the training set is relatively small. figure 1 shows that the non-separable quadratic performs similarly. the results appear relatively insensitive to the degree of aggregation applied to the representation of other agents' strategies.
﹛the polynomial regression methods we employed yield pure-strategy nash equilibria. we further evaluated four methods that generally produce mixed-strategy equilibria: two local regression learning methods  svm with a gaussian radial basis kernel  and direct estimation using the training data. as discussed above  we computed mixed strategy equilibria by applying replicator dynamics to discrete approximations of the learned payoff functions.1 since we ensure that the support of any mixed strategy equilibrium produced by these methods is in the complete data set  we can compute  of the equilibria directly.
﹛as we can see in figure 1  locally weighted average method appears to work better than the other three for most data sets that include between five and ten strategies. additionally  locally weighted regression performs better than replicator dynamics on four of the six data set sizes we considered  and svm consistently beats replicator dynamics for all six data set sizes.1
﹛it is somewhat surprising to see how irregular our results appear for the local regression methods. we cannot explain this irregularity  although of course there is no reason for us to expect otherwise: even though increasing the size of the training data set may improve the quality of fit  improvement in quality of equilibrium approximation does not necessarily follow.

figure 1: effectiveness of learning local and svm regres-
strategy symmetric equilibria  with
ss 
1 conclusion
while there has been muchwork in gametheoryattemptingto solve particular games defined by some payoff functions  little attention has been given to approximating such functions from data. this work addresses the question of payoff function approximation by introducing regression learning techniques and applying them to representative games of interest. our results in both the fpsb and market-based scheduling games suggest that when data is sparse  such methods

was used as input to the replicator dynamics algorithm. for the other three methods we used a fixed ten-strategy grid as the discretized approximation of the learned game.
﹛﹛﹛1note that we do not compare these results to those for the polynomial regression methods. given noise in the data set  mixedstrategy profiles with larger supports may exhibit lower  simply due to the smoothing effect of the mixtures.
can provide better approximations of the underlying game- at least in terms of -nash equilibria-than discrete approximations using the same data set.
﹛regression or other generalization methods offer the potential to extend game-theoretic analysis to strategy spaces  even infinite sets  beyond directly available experience. by selecting target functions that support tractable equilibrium calculations  we render such analysis analytically convenient. by adopting functional forms that capture known structure of the payoff function  e.g.  symmetry   we facilitate learnability. this study provides initial evidence that we can sometimes find models serving all these criteria.
﹛in future work we expect to apply some of the methods developed here to other challenging domains.
