 
this paper reports work on automated meta-data creation for multimedia content. the approach results in the generation of a conceptual index of the content which may then be searched via semantic categories instead of keywords. the novelty of the work is to exploit multiple sources of information relating to video content  in this case the rich range of sources covering important sports events . news  commentaries and web reports covering international football games in multiple languages and multiple modalities is analysed and the resultant data merged. this merging process leads to increased accuracy relative to individual sources. 
1 	introduction 
multimedia repositories of moving images  texts  and speech are becoming increasingly available. this together with the needs for 'video-on-demand' systems requires fine-grain indexing and retrieval mechanisms allowing users access to specific segments of the repositories containing specific types of information. annotation of video is usually carried out by humans following strict guidelines. video material is usually annotated with 'meta-data' such as names of the people involved in the production of the visual record  places  dates  and keywords that capture the essential content of what is depicted. still  there are a few problems with human annotation. firstly  the cost and time involved in the production of fine-grained semantic  surrogates  of the programme is extremely high; secondly  humans are rather subjective when assigning descriptions to visual records; and thirdly  the level of annotation required to satisfy a user's needs can hardly be achieved with the use of mere keywords. in order to tackle these problems  indexing methods based on image processing have been developed  chang et al  1 . content-based indexing and retrieval of visual records is based on features such as colour  texture  and shape. yet visual understanding is not well advanced and is very difficult even in closed domains. as a consequence  various ways to explore the use of collateral linginstic material have been studied for tasks 
    funded by ec's 1th framework hlt programme under grant number 1st-1 
such as automatic indexing  de jong et al  1   classification  sable and hatzivassiloglou  1   or understanding  srihari  1  of visual records. 
　in this paper  we present an integrated solution to the problem of multimedia indexing and search: the mum1 concept. our solution consists of using information extracted from different sources  structured  semi-structured  free  etc.   modalities  text  speech   and languages  english  german  dutch  all describing the same event to carry out data-base population  indexing  and search. the novelty of our approach is not only the use of these 'heterogeneous' sources of information but also the combination or crosssource fusion of the information obtained from the separate sources. single-document  single-language information extraction is carried out by independent systems that share a semantic model and multi-lingual lexicon of the domain. the results of all information extraction systems are merged by a process of alignment and rule-based reasoning that also uses the semantic model. in the rest of this paper we describe in detail the context of the project  and each of the modules involved in the automatic derivation of annotations. however  the emphasis is on the merging component. 
1 the mum1s project 
in mum1s various software components operate off-line to generate formal annotations from multi-source linguistic data in dutch  english  and german to produce a composite timecoded index of the events on the multimedia programme. the domain chosen for tuning the software components and for testing is football. 
　a corpus of collected textual data in the three languages was used to build a multi-lingual lexicon and shared ontology of the football domain. based on this shared model  three different off-line information extraction components  one per language  were developed  see section 1 . they are used to extract the key events and actors from football reports and to produce xml output. a merging component or cross-document co-reference mechanism has been developed to merge the information produced by the three ie systems 
 see section 1 . audio material is being analysed by phicos  steinbiss et al  1   an hmm-based recognition system  
　　1  multimedia indexing and searching environment  see http://parlevink.cs.utwente.nl/projects/mumis/ 


table 1: different accounts of the same event in different languages 
in order to obtain transcriptions of the fooiball commentaries  spontaneous speech . it uses acoustic models  word-based language models  unigram and bigram  and a lexicon. for dutch  english  and german different recognition systems have been developed  i.e.  different phone sets  lexicons  and language models are used . 
　jpeg keyframe extraction from mpeg movies around a set of pre-defined time marks - result of the information extraction component - is being carried out to populate the database. the on-line part of mumis consists of a state of the art user interface allowing the user to query the multimedia database. the interface makes use of the lexica in the three target languages and domain ontology to assist the user while entering his/her query. the hits of the query are indicated to the user as thumbnails in the story board together with extra information about each of the retrieved events. the user can select a particular fragment and play it. 
1 	domain and ontology 
an analysis of the domain and a user study led us to propose 
1 types of event for a football match {kick-off  substitution  goal  foul  red card  yellow card  etc.  that need to be identified in the sources in order to produce a semantic index. the following elements associated with these events are extracted: 
players  teams  times  scores  and locations on the pitch. 
　an ontology has been developed for these events and their actors  it contains some 1 concept nodes related as an 'is-a' hierarchy. the link between the concepts and the three languages consists of a flexible xml format which maps concepts into lexical entries. 
　sources used for information extraction are: formal texts  tickers  commentaries  and audio transcriptions  see table 1 . ticker reports are particularly important in the generation of formal annotations. these texts are a verbal account of events over time stamps. they also follow a specific text structure consisting of a 'ticker header'  in which information about lists of players and the result of the game is usually stated  and 'ticker sections' grouping together sentences describing events under single time stamps. another very valuable source for the generation of the annotations are the spoken transcriptions that  even with the many errors they contain  still provide exact temporal information. 
1 	extracting information from heterogeneous sources 
information extraction is the process of mapping natural language into template-like structures representing the key  semantic  information from the text. these structures can be used to populate a database  used for summarization purposes  or as a semantic index as in our approach. key to the information extraction process is the notion of  domain   scenario or template that represents what information should be extracted. ie has received a lot of attention in the last decade  fuelled by the availability of on-line texts and linguistic resources and the development of the message understanding conferences  grishman and sundheim  1 . traditionally  ie applications have tended to concentrate on a small number of events  typically one   mumis addresses the challenge of multi-event extraction. 
　multi-lingual ie has been tried in the m-lasie system  gaizauskas et al  1   where the same underlying components and a bi-lingual dictionary are used for two different languages  english and french . mumis differs from that system in that it operates with three different off-line information extraction components  one per language  that produce the same  language-free  representation. in this paper we give only a brief description of the english and german ie systems. 
1 	extraction from english sources 
ie from english sources is based on the combination of gate1 components for finite state transduction  cunningham et al.  1  and prolog components for parsing and discourse interpretation. the components of the system are: tokeniser  segmenter  gazetteer lookup  based on lists of entities of the domain   semantic tagger  shallow pronominal co-referencer  part-of-speech tagger  lemmatiser  chart parser  discourse interpreter  ontology-based co-referencer   and template extractor. these components are adapted and combined to produce four different system configurations for processing different text-types and modalities  transcriptions  formal texts  semi-formal texts  and free texts . the analysis of formal texts and transcriptions is being done with finite state components because the very nature of these linguistic descriptions make appropriate the use of shallow natural language processing techniques. for example  in order to recognise a substitution in a formal text it is enough to identify players and their affiliations  time stamps  perform shallow co-reference and identification of a number of regular expressions to extract the relevant information. in our system  regular expressions operate on annotations  not on strings  and produce semantic information. we make use of the java annotation pattern engine  jape  formalism  cunningham et al.  1  to code our regular grammar. below  we present 
1 gate is a free architecture for natural language engineering. 

1 	information extraction 

one example of the use of jape that accurately identify substitutions in speech transcriptions: 

complex linguistic descriptions are fully analysed because of the need to identify logical subjects and objects as well as to solve pronouns and definite expressions  e.g.   the barcelona striker   relying on domain knowledge encoded in the ontology of the domain. domain knowledge establishes  for example  that the two players involved in a substitution belong to the same team. this semantic constraint is used in cases such as  he is replaced by ince  to infer that the antecedent of the pronoun  he  belongs to the  english  team  because  ince  does . 
　logic-based information extraction rules operate on logical forms produced by the parser and enriched during discourse interpretation. they rely on the ontology to check constraints  e.g.  type checking  ontological distance  etc. . the following logic-based rule is used to extract the participants of a substitution when syntactic and semantic information is available: 

msubj' and iobj represent the logical subject and object of an event. note that  contrary to the regular case  the 'lsubj' and iobj' relations  being semantic in nature  are long distance relations. 
1 	extraction from german sources 
the german ie system is based on an integrated set of linguistic tools called schug: shallow and chunk based unification grammar  declerck  1 . the chunking procedure of schug consists of a rule-based sequence of cascades  based on the work by  abney  1    which produces a rich linguistic representation  including grammatical functions and resolution of co-reference and ellipsis. in order to detect these accurately  an analysis of the clauses of a sentence is required. clauses are subparts of a sentence that correspond to a  possibly complex  semantic unit. each clause contains a main verb with its complements  grammatical functions  and possibly other chunks  modifiers . 
　applied to the football domain  schug inspects the common mumis ontology and enriches the linguistic annotation produced with domain-specific information encoded in the ontology. below  see table 1  we show one example of the semantic annotation generated by schug when applied to an on-line ticker text  game england-germany . here  various relations  player  location  etc.  and events {free-kick  fail to score a goal  that are relevant to the football domain are recognised. 
some relations are not explicitly mentioned  but can still be inferred by the mumis system. for example  the team for which  ziege  is playing can be inferred from the ontological information that a player is part of a team and the instance of 

table 1: semantic annotation in schug 
this particular team can be extracted from additional texts or meta-data. in this way  information not present in the text directly can be added by additional information extraction and reasoning. 
　since formal texts require only little linguistic analysis  but rather an accurate domain-specific interpretation of the jargon used  a module has been defined within schug  which in a first step maps the formal texts onto an xml annotation  giving the domain semantic of the expressions in the text  the approach taken for formal texts is similar to the one followed by the english ie system . in a second step schug merges all the xml annotated formal texts about one game. those merged annotations are generated at a level that requires only little linguistic analysis  and basically reflect domain specific information about actors and events involved in the text. the schug module applied at this level also extracts meta-data information: name of the game  date and time of the game  intermediate and final scores etc. this is quite important  since the meta-data can guide the use of the annotations produced so far for supporting linguistic analysis and information extraction applied to more complex documents. 
1 merging 
merging  also known as cross-document co-reference  bagga and baldwin  1   is the process of deciding whether two linguistic descriptions from different sources refer to the same entity or event. the merging component in mumis combines the partial information as extracted from various sources  such that more complete annotations can be obtained. radev and mckeown  developed a knowledgebased multi-document summarization system based on information extraction and merging of single 'terrorist' events. the novelty of our approach consists in applying merging to multiple events extracted from multiple sources. 
　as is to be expected  complete recognition of events in natural language sentences is extremely difficult. often  events will be only partially recognised. the merging component of the mumis project aims to fill in missing aspects of events with information gathered from other documents. for example  the dutch information extraction system recognised in document a on the match netherlands-yugoslavia from the european championships 1 that a save was performed in the 1st minute. in addition  it recognised the names of two players: van der sar  the dutch goalkeeper  and mihajlovic  a yugoslavian player   but it could not figure out which of these two players performed the save. in document b it recognised a free-kick in the 1th minute  and the names of the same two players. again  it did not succeed in finding out which player took the free-kick. 
　the fact that the same two players are involved  plus the small difference between the time-stamps  strongly suggests that both descriptions are about the same event in reality. the merging part of the mum1s project matches these partial data together  and concludes that it was mihajlovic who took the 
free-kick  followed by a save by van der sar. 
　the merging component consists of several parts which will be described in more detail below. 
1 	scenes 
given the example above  it is clear that matching together individual events from two different documents is not the right approach: a save event cannot be matcled with a free-kick event  they are two totally different events. besides  it is clear that players' names will play an important role in the matching of information from one document with information from another document. in order to take players' names into consideration  an unknown event was introduced  such that if it was not clear what a player did  this could be represented by letting that player perform this unknown event  the information extraction systems provide this information . 
　now the event is still the fundamental concept  but the merging process aims at matching together groups of events instead of single events. such a group of events is called a 
scene. the author of a text is considered as a  semantic filter   which determines which events should be taken together in the same scene. if events are mentioned in the same text fragment  they belong to the same scene. in the ticker documents this does not give rise to ambiguities  since their text fragments are clearly distinguished from each other  see table 1 . 
1 	two document alignment 
the merging algorithm compares all the scenes extracted from document a with all the scenes extracted from document b  and examines whether or not a scene from document a might be matched with a scene from document b. there are several aspects to be taken into account: players involved in the scenes  distance between time stamps  whether the scenes contain the same events or not  etc. 
　a scene from document a may match more than one scene from document b  see figure 1 . 
　the strength of a matching can be calculated in different ways  and it will not be surprising that the number of players involved in both scenes will be of great influence. this influence is that great  that taking only the number of common players' names as a measure of the strength of a binding  gave the best results. we restricted matching of scenes to those scenes which were not further than  arbitrarily  five minutes apart. 

figure 1: two document alignment. vertical lines denote documents  numbers are time stamps  thin lines indicate possible bindings  thick lines denote strongest bindings. 

figure 1: finding complete subgraphs. the graph as a whole is found by the alignment process. inside 1 complete subgraphs are found  two triangles indicated by the thick lines  and one single node . 
　in order to choose the best matchings  the algorithm starts by selecting two scenes s  and s1 such that the binding between s1 and s1 is  one of  the strongest. in general  such a choice will remove other bindings  in particular the bindings between .si and .s1 with other scenes. since these two describe the same fragment of the game  scenes before s1 can now no longer match scenes after s1  and vice versa . that is to say  bindings which  cross  the matching of s1 and s1 are removed. thus the possible bindings are cut in two parts  and the algorithm continues recursively with both halves  until all choices have been made. this two document algorithm is applied to every pair of documents. 
1 	multi document alignment 
the next step is to join connected scenes from various documents together. start with a set consisting of one  arbitrary  scene  and extend this set by those scenes which are connected to the starting scene. repeat this for all the scenes added to the set until no further extension is found. this set of scenes  together with the bindings between the scenes  chosen by the two document algorithm  naturally form a connected graph. 
　repeat this graph building procedure for the remaining scenes  until all scenes are included in a graph. notice  that such a graph may consist of one scene only. notice also  that a graph may contain more scenes than there are documents  since scenes may be connected through a sequence of two document matches. 

1 	information extraction 

1 	complete sub-graphs 
ideally  a graph should be complete  expressing that every two scenes in the graph do match  and thus all scenes do contain some common information. that is to say  the scenes in a complete graph are all about the same fragment during the football game which is described in the documents. however  in practice not every graph which results from the procedure above  is complete  see figure 1 . 
　scenes may partially overlap  and thus give rise to sequences of connections where the first and the last arc no longer connected. we isolate the strongest complete subgraphs from every non-complete graph  since such a subgraph describes one fragment in reality. a given  noncomplete  graph is divided into its strongest complete subgraphs by going through all the bindings in the graph  ordered by their strength  starting with the strongest one   and adding scenes and edges to a sub-graph whenever possible without violating the completeness of this sub-graph. if that is not possible  a new sub-graph is started  and some bindings may be removed. the final result is a set of complete graphs of matching scenes from many documents  such that the scenes inside a graph describe the same fragment in reality. 
1 	rules 
consider the example given before  about the save event and the free-kick event  with the players van der sar and miha-
jlovic. these scenes are now in the same graph  and thus may be combined  see figure 1 . however  not every combination will be correct  for example  mihajlovic and van der sar will not both take the free-kick. also  it will not be correct to let van der sar  the dutch keeper  take the free kick  and mihajlovic perform the save. in order to combine this partial information into scenes containing complete events  rules are needed. in the mumis project several kinds of rules  all expressing some domain knowledge  have been developed. a first kind are the event internal rules  for example  rules saying that the two players involved in a substitution event belong to the same team  or that a keeper typically will not take a corner. 
　a second kind of rule takes into account the role of the teams in the fragment  i.e.  whether a team is attacking or defending at that moment. to determine whether a team is attacking or defending  all players involved in the scene are checked for their normal position in the field. then events arc characterised as offensive events  such as corner  shot on goal   defensive events  save  clearance   and neutral events  throw-in  yellow card . the second kind of rule makes sure that offensive events are performed by players from the attacking team  and defensive events are combined with payers from the defending team. 
　a third kind of rule makes sure that players will not perform impossible combinations of events  e.g.  the player who takes a corner will  unless he's very fast  not deliver a shoton-goal as well. 
　to apply these rules  a background database of player information is created  containing names of players  their normal position in the field  the team they belong to  etc. 
　yet another kind of rule is based on an ontolgy of events and is used to unify certain events which according to the 

figure 1: merging. bottom left is the result of merging all information in all nodes of the graph. bottom right is the remaining information that could not be decided upon  in this case nothing . 
chosen semantics in mumis are not the same. for example  a clearance and a save are two different events according to the mumis semantics  but an author may well use these terms in a way different from the mumis semantics. therefore the relationships between such concepts are expressed in an ontology  such that rules may use sub-typing or super-typing relationships between concepts. 
1 	scenarios 
after merging the scenes extracted from the various documents into more complete scenes  the order of the events within a scene may be incorrect with respect to the order as it was in reality. the merging process itself does not take the ordering of events into account  and besides  authors often mention events in the opposite order. for example  a player scores after a corner - but the scoring is mentioned first. 
　based on the original texts in the source documents  a series of scenarios is extracted  describing typical orders in which events occur  schank and abelson  1 . 
1 	evaluation 
given the limited availability of background information on players  to which team they belong  on what position they play  etc.   the merging component could only be tested in a case study. the match netherlands-yugoslavia from the european championships in 1 to perform such a case study. based on this example  the results of the merging approach are very promising. 
　the result of the alignment process applied to this match produced 1 complete graphs  of which 1 consist of only one node  a scene   containing information from one source text only. looking at the original texts  only five of these onescene graphs might have been combined with other graphs  the main problem being that some texts mention some information in one scene  whereas other documents divide the same information over several scenes. in such cases the alignment algorithm chooses the best match  leaving the other part unmatched. 

　of the remaining 1 multi scene graphs  no graph contained unjustified bindings. this seems to be a very promising aspect  since the rules used in the graph forming part of the algorithm are formulated in a very general way  without using any specific information on the concrete football match. 
　the result of the merging process consisted partly of the elimination of several errors caused by syntactical ambiguities in the single document information extraction  and partly of joining together partial events into more complete events. like the alignment process  this merging process is also based on general semantical rules  which do not use any concrete details about the specific football match taken as a case study. there were no errors introduced by these merging rules  and in many cases the quality of the extracted information improved considerably. 
1 	conclusions and future work 
the development of huge multimedia databases and the need for accurate navigation of its content require a new generation of  intelligent  tools for producing fine-grained surrogates or indices of the multimedia content. by taking advantage of ontology  domain lexica  and a  language-free  representation of the contents  mum1s facilitates conceptual search overcoming the keyword barrier. 
　mumis takes advantage of reliable  but coarse-grained content  obtained from formal texts and fine-grained  but sometimes partial  content obtained from free texts and transcriptions. it indirectly solves problems of incomplete information found in any source by combining results from multiple sources. by relying on the analysis of textual instead of visual sources  mumis makes possible the derivation of fine-grained semantic indices. 
　cross-document co-reference is still in its infancy  mumis advances research in that direction by providing a methodology that uses robust named entity alignment from multiple sources together with domain specific  semantic rules. 
　there are still many points on which the merging algorithm may be improved. for example  some documents are more reliable than other documents. furthermore  not all texts are of the same type  so-called formal texts differ from ticker texts  and from more free texts. as yet  differences in quality of the source documents has not been taken into account  and a weighted integration of formal texts and other texts still has to be performed. a related point of improvement is to check the consistency of certain elements of information in comparison to other elements of information  and in particular to the state on the football field as may be derived from all previously mentioned events. finally we mention the improvements in connection to overlapping scenes  where events mentioned in one text fragment in one document may be spread over several text fragments in another document. at this point improvements are also possible. 
acknowledgements 
we wish to thank eduard hoenkamp and marco puts for many useful discussions. 
1 
