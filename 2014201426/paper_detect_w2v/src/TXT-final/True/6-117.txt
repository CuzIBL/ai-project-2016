 
after many successes  statistical approaches that have been popular in the parsing community are now making headway into natural language generation  nlg . these systems are aimed mainly at surface realization  and promise the same advantages that make statistics valuable for parsing: robustness  wide coverage and domain independence. a recent experiment aimed to empirically verify the linguistic coverage for such a statistical surface realization component by generating transformed sentences from the penn treebank corpus. this article presents the empirical results of a similar experiment to evaluate the coverage of a purely symbolic surface realizer. we present the problems facing a symbolic approach on the same task  describe the results of its evaluation  and contrast them with the results of the statistical method to help quantitatively determine the level of coverage currently obtained by nlg surface realizers. 
1 introduction 
like parsing  text generation offers enormous potential benefits for more natural interaction with computers. examples of applications which could be greatly improved include automatic technical documentation  intelligent tutoring systems  and machine translation  among many others. historically  natural language generation  nlg  has focused on the study of symbolic pipelined architectures which receive knowledge structures and goals from knowledge-based applications and which proceed to progressively add linguistic information. 
　in the last few years  the same paradigm shift which occurred in the parsing community  the use of statistical/empirical methods  has begun to influence the nlg community as well. as with parsing  statistical generation promises benefits such as robustness in the face of bad data  wider coverage  domain and language independence  and less need for costly resources such as grammars. but unlike parsing  which starts with a very flat representation  text  which is easily accessible in large quantities to both statistical and symbolic methods  the semantic input for nlg is typically associated with large knowledge-based systems. the types of corresponding corpora which would be necessary for using statistical processes  pairs of subgraphs of knowledge bases and their texts  do not currently exist in large quantities. 
　because of this representation problem  most statistical systems have concentrated on replacing existing individual components in the standard nlg pipelined architecture  reiter  1  without changing the remaining original symbolic modules. the most popular candidate has been the surface realization module  elhadad  1; bateman  1; lavoie and rambow  1; white and caldwell  1   which is responsible for converting the syntactic representation of a sentence into the actual text seen by the user. thus current statistical generators are still dependent on remaining architectural modules in a system to function and do not by themselves account for a large amount of linguistic phenomena: pronominalization  revision  definiteness  etc. 
however  statistical surface realizers  langkilde and 
knight  1; bangalore and rambow  1; ratnaparkhi  1; langkilde-geary  1  have focused attention on a number of problems facing standard  pipelined nlg that have until now been generally considered future work: largescale  data-robust and language- and domain-independent generation. in addition  as langkilde points out  empirical evaluation has not been standard practice in the nlg community  which has instead relied either on the software engineering practice of regression testing with a suite of examples or theoretical evaluations  robin and mckeown  1 . 
　this paper presents the analogue of this recent statistical experiment using a well-known off-the-shelf symbolic surface realizer  using an augmented generation grammar that includes support for dialogue and additional syntactic coverage. we first describe in the following section the representations and processes needed to understand its evaluation. we then detail our implemented system for converting sentences from a large corpus into a systemic functional notation  present an evaluation of that system and the grammar itself using section 1 of the penn treebank  marcus el al  1   and finally discuss the implications of that evaluation. 
1 sentence representations 
to undertake a large-scale evaluation of a symbolic surface realizer  we must first find a large quantity of sentence plans with which to produce text. however  most text planners cannot generate either the requisite syntactic variation or quantity of text  and we thus cannot turn to implemented gener-

natural language 	1 

figure 1: a perm treebank annotated sentence and corresponding fuf/surge functional description 
ation systems as a source. to solve this problem  langkilde trained a statistical algorithm  langkilde-geary  1  on a substitute set of sentence plans: the penn treebank  marcus et ai  1   a collection of sentences from newspapers such as the wall street journal  which have been hand-annotated 

for syntax by linguists. an example sentence is shown on the left side of figure 1. hierarchical syntactic/semantic bracketing is provided along with the syntactic categories of lexemes and symbols in the newspaper texts. 
　unfortunately  text planners currently in use do not generate representations of the form found in the penn treebank  opting instead to use more fully-developed syntactic theories  such as hpsg  pollard and sag  1   from the linguistics community. because annotated texts do not exist in this form  langkilde created a pre-processing system to translate from the treebank annotation into the language accepted by the halogen statistical surface realizer  langkilde-geary  1 . halogen uses these translations to create a forest lattice whose paths from start to finish represent many possible versions of a single sentence. separately  a larger corpus is processed to obtain bigram or trigram frequencies  which are then used to rank the possible sentence versions based on word adjacency. the highest ranked sentence is then presented as the final output of the system. 
　in contrast  most deep surface realizers are symbolic rather than statistical  and consist of components that check grammatical constraints  appropriately linearize constituents  and adjust for morphology and formatting. one such system in wide use  fuf/surge  elhadad  1   combines ideas from systemic functional grammars and head-driven phrase structure grammars. an example of the fuf representation  known as afunctional description is shown on the right side of figure 1. surge is the largest generation grammar for english  and has the largest regression test suite available. but as langkilde pointed out  1 test examples are insufficient to empirically demonstrate the coverage of a grammar. 
　to arrive at a set of sentence plans which is representative of english  as well as to evaluate the coverage of the fuf/surge surface realizer in a way which can be directly compared to the halogen evaluation  we likewise used the penn treebank as a sentence source. because our representations are also different  we  as langkilde  needed a preprocessing system to convert from the treebank notation into the functional descriptions expected by the surface realizer. 
　our pre-processor thus performs top-down structure traversal of a sentence annotated in penn treebank format and figure 1: penn treebank notation and normalized form 
builds the corresponding functional description. the preprocessor is organized as a context-sensitive  proceduralized rewriting grammar which matches input symbols to output symbols. the resulting functional descriptions can then be given to the fuf/surge surface realizer  and the sentence string it produces can be lexically compared to the original sentence in various ways to determine how well the surface realizer performs at sentence generation. 
1 	implementation 
the implementation necessary for evaluating the coverage of fuf/surge comprised three processes:  1  normalizing the syntactic/semantic representations   1  transforming the normalizations into functional descriptions  and  1  generating the sentence itself with a surface realizer. the normalizing phase is necessary to convert the original penn treebank structures into a llsp-readable format  figure 1   which was accomplished with a series of regular expression transformations on the original text file. 
　the most time-consuming aspect of the procedure was creating the transformation component  which was highly analogous to writing parsing rules by hand. the resulting component contained 1 lines of code and approximately 1 rules  although most of the actual computational effort was spent instead in surface realization. most of the problems encountered were the result of differences in the underlying grammars themselves. for example  the penn treebank has a more hierarchical noun phrase structure than the flatter representation of surge's systemic functional grammar. 
　the final task involved changing the surface realization component  1  to add additional branches and surface forms to the grammar that were not originally present in order to produce surface forms not previously possible   1  to add new punctuation and capitalization rules1  and  1  to update irreg-
        the penn treebank  because it is a newspaper corpus  contains many newspaper headlines and stock quotes with domain-specific 

1 	natural language 


table 1: comparison with halogen flangkilde  1j 

ular morphology due to the vast number of words the system had not previously seen. the principal linguistic problems uncovered by this phase include: 
  quotations: newspaper text generally contains large amounts of complex quotations  such as splitting a quoted phrase to insert the speaker in the middle  or merging a quote into an unquoted part of the sentence:  1 have this feeling that it's built on sand   she says  that the market rises  but there's no foundation to it.  
  punctuation scoping: problems related to the use of punctuation with tree structuresldoran  1. for example  s u r g e has a flat representation for noun phrases  causing difficulties with phrases such as the major  circuit breakers  where surge cannot insert punctuation between the adjective and nominal classifier. 
  adverb and clause ordering: because satellite clauses in surge are placed using semantic information  they sometime appear in different  though still grammatically acceptable  positions than were specified in the original sentence  elhadad et ai  1 j. this can oftentimes cause a perfectly acceptable sentence to be produced  but highly skew automatic measurements of correctness such as tree edit distance  simple string accuracy . for example  contrast:  exports fell 1% in the first few months  vs.  in the first few months  exports fell 1%.  
  semantic roles: surge has a hybrid syntactic/semantic representation  whereas the penn treebank is purely syntactic. thus some guessing must be done to fill in semantic roles in the corresponding functional description. wrong guesses can lead to incorrect surface forms even when the remainder of the transformation was accomplished successfully. 
   while the normalization process is slow  1 hours each for wsj1 and wsj1   it occurs offline and only once. the transformation process is quite fast  requiring on average 1 seconds per sentence. meanwhile  the fuf/surge system is relatively slow  as it requires the use of functional unification  a task of inherent complexity due to backtracking. section 1 needed 1.1 seconds to generate 1 sentences  with a longest exact match of 1 words   while section 1 needed 1.1 seconds to generate 1 sentences  with a longest exact match of 1   for a combined average of 1 seconds per sentence.1 this contrasts with statistical approaches like langkilde's  which require 1-1 seconds depending on 
formatting not typically used in nlg systems  such as:  1/1% high  1/1% low  1/1% near closing bid  1/1% offered.  
   additionally  a compiled c version of fuf can produce sentences using the same grammar on the order of 1 seconds. algorithm parameters  and gets exponentially worse if it uses trigrams or larger models in an attempt to improve quality. 
1 	experiments and results 
in order to evaluate the coverage of the surge grammar  we used the standard train and test methodology. unlike typical machine learning experiments  adjustments to the transformation rule set were done by hand  although the evaluation of the resulting sentences was performed automatically. training took place over a period of several months  consisting of multiple iterations over penn treebank sections 1 and 1 to both improve the number of sentences which could be generated and to match as closely as possible the original sentences. these two goals were accomplished solely by adding rules to the transformation set and by updating surge grammar rules  notably aspects pertaining to the stock market domain  in addition to support for extended quotations which was added in previous work icallaway and lester  1. 
   we considered two types of string matches: exact matches that were identical character-by-character  and  close  matches which were two words or less longer or shorter than the original sentence. there were several motivations for this second choice:  1  many sentences were equivalent except for a minor missing/extra punctuation mark or wrongly capitalized word  especially with newspaper headlines ;  1  as mentioned previously  movable clauses  socalled circumstantials in surge nomenclature  could be put in multiple acceptable locations; and  1  sentences with almost the exact number of words  especially sentences with more than 1 words  were much more likely to at least have all of the various phrases present when they were within two words or the original sentence's length. we utilized the n1st simple string accuracy  ssa  as an automatic evaluation score  the same as used in langkilde's work   where the smallest number of adds  deletions  and insertions were used to calculate accuracy: 1 -   a + d + t /#characters. 
　the only previous measure of generation coverage for section 1 of the penn treebank is that of  langkilde-geary  1   who defined coverage as the number of sentences for which the surface realizer produced strings. as seen in table 1  our system achieved 1% on one of the training sets and 1% on the test set compared to between 1% and 1% for halogen depending on its algorithm parameters. 
　a more detailed examination of the coverage and accuracy of the system is found in table 1 for wsj section 1 and table 1 for section 1. both tables are broken down by sentence length  which shows that the results are highly skewed towards sentence of smaller length  as would be expected in a test for exact matches. it should be noted  however  that surface realizers are rarely called upon to generate sentences 

natural language 	1 


table 1: sentence coverage/accuracy for the unseen wsj1 sentences grouped by word length 

with the extended lengths and complexities found in highly 
educated newspaper text. finally  the  valid fd  column indicates that the transformation program is very good at producing valid functional descriptions  even if they eventually are discarded by the grammar as being erroneous. 
　the test set had a slightly higher coverage than the training set shown above  as well as a higher number of perfect matches and a better score using the nist ssa measure. although we trained on other sections of the penn treebank  time constraints due to the large amount of time required to generate all test sentences prevented us from having a fuller comparison set. additionally  section 1 was the first section we trained on  and it is likely that later sections were more similar to section 1  or that the amount of domainspecific stock market constructions were imbalanced. finally  the  close matches  column shows how many candidate sentences might be nearly exactly matching  and the sum of these two is reflected in the final category  combined.  
　one interesting observation is that this evaluation  and correspondingly  the evaluation of halogen  is not only an evaluation of the underlying surface realizer  but also of the accompanying transformation program that converts the penn treebank notation into the specifications it expects. we thus set out to perform a minor  secondary evaluation to determine if it were possible to find a baseline metric for how many sentences could still be generated by the surface realizer even if the corresponding fds could not be produced for them by the transformation program. 
　we thus randomly selected 1 sentences from each of the two sections which were not either perfect matches or  close  matches  i.e.  they were not in the  combined  match category. while these sets included some of the problems listed in section 1  1 of the 1 were capable of being rendered by hand as fds which produced exact matches without changes to the grammar  1 required minor changes to the grammar which were quickly performed  and the remaining 1 sentences required major grammar changes which have still not yet been made. the latter were sentences that still do not have satisfactory linguistic analyses in the linguistic literature. we thus conclude that with better transformation rules  we could then obtain close to 1% coverage. 
1 	discussion 
surface realization is probably the most understood and competent task in nlg today. there is a high possibility that surface realization can already be considered a solved problem  except with regard to problems introduced by new languages or highly specialized domains. however  there are two related unsolved problems inherent in the process described in this paper. 
1 	automatic evaluation of output 
evaluation of nlg systems face the same problems as those that confront machine translation systems: given a set of generated sentences  how do you tell how  good  they are in general  and how often you can produce good sentences in a given context or application. work in machine translation has shifted to large-scale evaluations which require automatic evaluation techniques  papineni et al.  1; 

1 	natural language 

doddington  1  because human graders cannot hope to examine all of the responses in a short enough period of time. 
　yet current evaluation techniques are completely numeric/statistical in nature and do not attempt to measure semantic content  such as the well-known example of a missing  not  in a system's output . furthermore  these techniques are ill-equipped to evaluate the types of sentences produced by symbolic nlg systems. for example  by changing a feature specifying that  say  a particularly lengthy purpose clause should go at the beginning rather than the end of a sentence  a string edit distance metric will report a very large error when in terms of the system's input  only one  error  has occurred. 
　as an example of this type of problem  consider that the string edit distance between the following original sentence and that produced by the transformation program and fuf/surge with input from the penn treebank would be 1  resulting in a n1st ssa 1% match: 
freddie mac said the principal-only securities were priced at 1/1 to yield 1%  assuming an average life of eight years and a prepayment of 1% of the psa model. 
freddie mac said assuming an average life of eight years and a prepayment of 1% of the psa model  the principal-only securities were priced at 1/1 to yield 1%. 
　obviously 1% is a poor score for such sentences  but many such examples were found in our corpus and their low scores were factored into the nist simple string accuracy ratios in tables 1 and 1. 
1 	symbolic vs. statistical approaches 
almost all fully-developed nlg systems to-date operate on data specified in a knowledge base from some other system. the fact that this data has typically been represented as highly-structured data has been an impediment to traditional machine learning techniques which have previously operated mostly on fiat  unstructured text data. it is also generally stated that statistical methods are more robust than their symbolic counterparts and more easily adapted to new data sets. the data in the previous section seems to indicate that halogen  a statistical system  performs substantially better on longer sentences  even if it has lower overall coverage. but there are several advantages in favor of symbolic techniques. 
　first  the transformation program presented above can be tweaked to an arbitrary level of perfection by progressively adding more rules. most statistical and machine learning systems however have eventually reached a boundary where progress becomes seemingly exponentially more difficult. second  errors which are encountered during processing can be examined and fixed because the grammars and other resources are logically and semantically connected to the language being generated  rather than being a set of numbers. if however a statistical generator must create new output forms not contained in the initial corpus or model  it must be retrained from scratch. 
　finally  symbolic surface realizers allow a wide range of optional operations which statistical programs currently can't offer  for example  the capability of adding formatting statements in html  modifying punctuation  generating dialect differences  adding prosody for tts  etc. while this may be due to the multiple decades of history over which symbolicsystems have been developed  it may also be due to the lack of annotated corpora that support statistical algorithms  or even potentially the impossibility of having a corpus at all  such as in large narratives  callaway and lester  1 . 
　additionally  there are some disadvantages to the current approaches undertaken in statistical nlg research. for instance  symbolic nl generation systems are already considered slow  and fuf/surge is generally considered to be the slowest in the nlg community. and yet the data from table 1 shows that halogen is anywhere from 1 to 1 times slower  and thus a 1-sentencc paragraph might need 1 minutes or longer to be generated. moreover  these approaches use techniques such as n-gram models  where n must be increased to improve quality  but results in even slower generation times and exponentially larger storage space. 
1 	potential applications 
the transformation program presented here has additional side benefits besides helping calculate the coverage of a grammar. for example  in generation systems where nonlinguists must maintain old data and add new data  such a program allows them to write sample sentences in the syntax-only treebank notation  which is much easier for non-linguists  and then convert those sentences directly into a more linguistically-manageable form for generation  e.g.  functional descriptions . graphical editing tools for linguistic data such as gate  bontcheva et a/.  1  or similar authoring tools could quicken the process even more. 
　additionally  the transformation program can be used as an error-checker for the well-formedness of sentences contained in the treebank. rules could be  and have been  added alongside the normal transformation rules that detect when errors are encountered  categorize them  and make them available to the corpus creator for correction. this extends not only to annotation errors by the corpus creator detectable at the syntax tree level  but even morphology errors such as incorrect verbs  typos  or british/american english differences by the original author of the text. both of these tasks are much more difficult for a statistical system to accomplish  requiring separate retraining in the first case and locating or creating a corpus of possible mistakes in the second  much like what is done with tutoring systems where databases of potential student errors are painstakingly constructed. 
1 	conclusions and future work 
recent years have seen the arrival of statistical approaches to the field of natural language generation  much as was seen in parsing a decade ago. of the many possible components in the standard nlg pipelined architecture  almost all of these statistical systems have focused on the surface realization component  offering the same robustness  wide coverage  and domain- and language-independence as for parsing. 
recent experiments with one statistically-based system  
halogen  showed that it could achieve respectable cover-

natural language 	1 

age without the typical development costs inherent in producing grammars for symbolic nlg. by taking as input sentences from the penn treebank  h a l o g b n was able to generate a substantial enough quantity of sentences to allow for an empirical analysis of grammatical coverage of english. this paper represents the analogous effort for a symbolic generation system using the fuf/surgh systemic realization system  which includes the largest generation grammar. 
　we presented the results of a grammatical coverage evaluation experiment that showed the symbolic system had a higher level of coverage of english as represented by the penn treebank. we also contrasted the statistical and symbolic methodologies and concluded with ideas for future applications for easier creation of nlg systems and automatic errorchecking for large-scale corpora. we also plan to further develop our italian generation grammar developed at itc-irst  novello and callaway  1j with a similarly annotated corpus of italian newspaper text. 
1 	acknowledgements 
this work was funded by the peach project  granted by the autonomous province of trento in italy. 
