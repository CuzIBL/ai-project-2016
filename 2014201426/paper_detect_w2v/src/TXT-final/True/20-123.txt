 
　　the level of abstraction of much of the work in know ledge-based systems  the rule  frame  logic level  is too low to provide a rich enough vocabulary for knowledge and control. i provide an overview of a framework called the generic task approach that proposes that knowledge systems should be built out 
of building blocks  each of which is appropriate for a basic type of problem solving. each generic task uses forms of knowledge and control strategies that are characteristic to it  and are generally conceptually closer to domain knowledge. this facilitates knowledge acquisition  and can produce a more perspicuous explanation of problem solving. the relationship of the constructs at the generic task level to the rule-frame level is analogous to that between high level programming languages and assembly languages. 1 describe a set of generic tasks that have been found particularly useful in constructing diagnostic  design and planning systems; diagnostic reasoning is used to illustrate the approach. 1 describe the generic task toolset for constructing knowledge systems  which embodies the generic task approach. i conclude with the implications of this approach for the functional architecture of intelligence. 
1. overview of the paper 
　　the first part of the paper is a critique of the level of abstraction in much of the current discussion on knowledge-based systems. it will be argued that the level of rules-logic-frames-networks is inappropriate for discussing many issues of knowledge organization and control. we advocate instead the level of abstraction associated with the language of generic tasks  types of knowledge  and types of control strategies. 
　　following this i will outline the elements of a framework for the design of knowledge-based systems that we have been developing in our laboratory over the last several years. complex knowledge-based reasoning tasks can often be decomposed into a number of generic tasks  each associated with certain types of knowledge and a family of control strategies. at each stage in the reasoning  the system will engage in one of the generic tasks  depending upon the knowledge available and the state of problem solving. 
diagnostic reasoning will be used to illustrate the application of these ideas. i will discuss the advantages of this approach for knowledge acquisition  knowledge representation  control of problem solving  and explanation. these advantages are made possible by the richer vocabulary in terms of which knowledge and control are represented for each task. 
　　i then describe how the above approach leads naturally to a new technology: a toolset which helps one to build expert systems by using higher level building blocks. 1 will review the toolset  and discuss the advantages that accrue from its use. finally  i discuss what this approach entails for the architecture of intelligence  and discuss a number of related theoretical issues. 
　　these ideas have evolved over the years as a result of work in diagnostic and design problem solving. a number of earlier publications  chandrasekaran 1  1  1  trace the development of the ideas. 
1. critique of uniform architectures 
　　knowledge representation has been a major concern of a l   in particular and quite naturally  for knowledge-based problem solving  or  expert systems1 . the general assumption  and consequently the methodology  has been that there is something called domain knowledge that needs to be acquired- quite independent of the problems one might wish to solve-and that the role of the knowledge representation formalism is to help encode it. logics of various kinds  rule-based languages and frame representations have been three popular kinds of proposals for knowledge representation. each of these representations has a natural family of inference mechanisms that can operate on it. each of the knowledge representations  along with an inference scheme that is appropriate for it  defines an architecture. when the inference scheme is fixed  the representation formalism is also said to provide a shell for inserting knowledge. 
　　these architectures  with relatively small additions  if needed  are computationally universal. thus the important point about building knowledge systems 
with them is not whether a task can be performed  but whether they offer knowledge and control constructs that are natural to the task. all of these  and other similar  languages fall short when one considers tasks of some complexity such as planning or diagnosis. 
1. lack of expressiveness for higher level tasks 
　　the level of abstraction of these languages obscures the essential nature of the information processing that is needed for the performance of higher level tasks. they are like knowledge system assembly languages  rather than programming languages with constructs for capturing the essence of the information processing phenomena. 
　　intuitively one thinks that there are types of knowledge and control strategies that are common to diagnostic reasoning in different domains  and similarly that there are common structures and strategies for  say  design as a cognitive activity; but that the structures and control strategies for diagnostic reasoning and design problem solving will generally be different. however  when one looks at the formalisms  or equivalently the languages  that are commonly used in expert system design  the knowledge representation and control strategies do not typically capture these distinctions. for example in diagnostic reasoning one might generically wish to speak in terms of malfunction hierarchies  rule-out strategies  setting up a differential  etc.  while for design the generic terms might be device/component hierarchies  design plans  ordering of subtasks  etc. ideally one would like to represent diagnostic knowledge in a domain by using the vocabulary that is appropriate for the task  but the languages in which the expert systems have been implemented have sought uniformity across tasks  and thus have had to lose perspicuity of representation at the task level. 
　　in addition  the control strategies that these languages come with  such as forward or backward chaining for rule systems  do not explicitly indicate the real control structure of a task that a problem solver is performing. for example  the fact that rl  mcdermott 1  performs a linear sequence of subtasks as a way of performing its design task is not explicitly encoded. this task-specific control structure is  encrypted  and hence invisible at the level of the pattern-matching control of ops1. the knowledge base of a system that is built in one of these architectures ends up accumulating a large number of programming devices as if they were part of domain knowledge. this detracts from the modularity of domain knowledge  since debugging a piece of knowledge involves studying the interaction between domain knowledge and the task as mediated by the implicit programming knowledge in the knowledge base. 
1. artifactual versus real issues 
　　because of the mismatch between architecture and information processing task  control issues arise that are artifacts of the architecture  but are often misinterpreted as issues having to do with control at the task level. for example rule-based approaches often concern themselves with conflict resolution strategies. yet if the knowledge were viewed at a different level  one can often abstract organizations of knowledge that only a small  highly relevant body of knowledge is brought up  without any need for conflict resolution at all. in many rule-based systems  these organizational constructs can be  programmed  in the rule language by the use of context setting rules and metarules  but because the rules and metarules  per se  have been considered to be knowledge-level phenomena  as opposed to the implementation-level phenomena  which they often are   knowledge acquisition has often been directed towards strategies for conflict resolution  when they ought to be directed to issues of knowledge organization. 
　　in sum  these architectures  by encouraging knowledge acquisition and representation at a level far removed from the organization and control of a task  create barriers in building understandable  modifiable knowledge systems. 
1. generic tasks 
   our work is based an alternative view  viz.  that knowledge representation and use cannot be separated  gomez and chandrasekaran 1 . that is  knowledge should be in different forms  depending upon the type of function for which it is used. by  use  i do not mean a highly domain-specific thing such as   this piece of knowledge will be useful in the treatment of cancer   but a more generic problem solving use that can be applied to a variety of domains. the following information specifies the generic task abstractly. 
  the function of the task. what type of problem does it solve  what is the nature of the information that it takes as input  and produces as output  
  the representation and organization of knowledge. what are the primitive terms in which the forms of knowledge needed for the task can be represented  how should knowledge be organized and structured for that task  
  the control strategy. what control strategy  inference strategy  can be applied to the knowledge to accomplish the function of the generic task  
1. generic tasks and generic task problem solvers 
	specification of a shell. 	when domain knowledge is 
1 	panels and invited talks 　　in order to understand how problem solvers are built using generic tasks  we can think of a task specification in accordance with the above as a virtual encoded using the primitive terms  organization and structure that are specified for the task  and then combined with the inference strategy that comes with the task  we have a generic task problem solver. the totality of domain knowledge is distributed among a number of such problem solving modules  representing a variety of generic tasks. thus a generic task problem solver is not merely an information processing strategy: it is a inference strategy that uses knowledge to solve parts of the problem. the interaction between modules is based on their information needs and functions: a module which needs information of a certain type gets it from a module whose function matches it. 
1. generic tasks in diagnostic reasoning 
   the motivation in our work on diagnosis is to make a connection between diagnostic problem solving  and the general properties of intelligence as information processing strategies. i will now describe a functional architecture approach to performing diagnosis  which uses and integrates information provided by a number of generic problem solvers. this approach originated with the mdx system 
 chandrasekaran and mittal 1   and has been refined in the construction of the red system  smith et al. 1  josephson et al. 1 . 
　　let us view diagnostic problem solving as a task in which the goal is to explain a set of observations of a system. the explanation is to be in terms of malfunctions that may have caused the observations. since the malfunctions can interact in a number of ways  the problem solver has to produce a set of malfunction hypotheses that explain all the data without including unnecessary hypotheses and also taking into account the possible interactions between hypotheses. 
   let us consider domains satisfying the following properties: 
1. knowledge is available in the form of malfunction hierarchies mirroring the class/subclass relationship among the malfunctions. 
1. given a typical observation  only a relatively small subset of these malfunctions could be implicated. also interactions between malfunctions with respect to an observation are limited. 
property 1 requires that sufficient variety of observations are available from several parts of the system to substantially disambiguate the diagnostic situation. this property holds true in medicine and in a number of mechanical domains. de kleer's work on diagnosing multiple malfunctions  de kleer and williams 1  involves a situation where the worst-case assumptions about interactions apply. in those domains  such as digital circuits  property 1 does not hold. 
   the knowledge available in domains satisfying these properties makes possible a decomposition of problem solving into two submodules: a hierarchical classifier which uses the hierarchy to select a small set of plausible malfunctions  and an abductive assembler that uses a subset of these hypotheses to make a composite that provides a coherent and parsimonious explanation of data. the malfunction hierarchy has more general classes of malfunctions in its higher level nodes and more specialized ones as their successors  e.g   liver  has as a successor   hepatitis.  the forms of knowledge for the former module use terms in which hierarchies are described  while the forms of knowledge for the latter module deal with interactions among the hypotheses. i will describe the problem solving behavior of these and other modules in a rather oversimplified manner. references are given where details can be found. 
   gomez  gomez and chandrsekaran 1  proposed hierarchical classification as a core process in medical diagnosis  and investigated the inference methods useful for that task. the inference mechanism for hierarchical classification can be thought of as variations on the strategy of establish-refine  i.e.  the hypothesis space is explored top down  by attempting to establish the top level hypotheses first. when a hypothesis is ruled out  all its more specialized successors can also be ruled out  while if it is established  the conclusion can be refined by considering its successors. this process can be repeated until a number of terminal nodes of the hierarchy are established. this process can be expected to produce a small number of highly plausible hypotheses  each of which explains a portion of the data. 
　　josephson et a/.   josephson et al. 1  have investigated the inference method needed for abductive assembly. it can be thought of as a means-ends process  driven by the need to explain the most significant unexplained observation at any stage of problem solving. as hypotheses are accumulated to explain the significant observations  the composites are critiqued for redundancy  logical compatibility and so on. driven by these criteria  a coherent collection of hypotheses that best explain the data is made. 
　　returning to the classification process  domain knowledge is required to establish or reject each of the hypotheses in the hierarchy. what is needed is a way to decide how well a hypothesis fits a set of observations. this function can be accomplished in a number of ways in different domains depending upon what kind of domain knowledge is available. 
	a 	generic 	strategy 	called 	hypothesis 	matching 
 chandrasekaran et al. 1  chandrasekaran 1  or structured matching  bylander and johnson 1  is useful in a number of domains for performing this function. we can build up a hierarchy of abstractions 

from the data to the hypothesis. for example  given the hypothesis   liver disease   one way to decide on the degree of fit between the data and the hypothesis is first to see how the data match the intermediate abstractions   chemical test evidence    physical evidence   and  historical evidence.  each of the abstractions will normally involve a subset of the data applicable to liver as a whole. each step in the abstractions can be computed by pattern matching  and the values passed up for further matching. this pattern matching is actually implemented in our work as tables analogous to samuel's signature tables  samuel 1   but the details are not important for the current discussion. for each of the hypotheses in the hierarchy  a structured matcher can be built using the scheme. 
the matcher requires values for specific data items. 
 in the above example  data relevant to a decision about  historical evidence  might be  evidence of exposure to anesthetics    or  fever responds to drugs  . in the first example  the associated database might have the information that the patient had undergone major surgery a few days earlier  and in the latter  the data base may have the complete information about the patients prescriptions and temperatures. in either case  the needed information can be obtained by making appropriate inferences from domain knowledge: about the relationship between anesthetics and surgery in the first example  and some complex processing of raw data to make the abstraction about fever response in the latter. thus there is need for an inferencing database to make the necessary data abstractions. i will not go into the details of the representation and inference control here  save to note that they are different from the representation and inference for the other problem solvers: hierarchical classification  abductive assembly and structured matching. mittal  mittal tt al. 1  recognized the inferencing database as an important component generic activity in the design of mdx. 
　　the overall problem solving for diagnosis can now be traced as follows. the hierarchical classifier's problem solving activity proceeds top down  and for each hypothesis that is considered  the structured matcher for that hypothesis is invoked for information about the degree of fit with the data. the structured matcher turns to the inferencing database for information about the data items that it is interested in. the database completes its reasoning and passes on the needed information to the structured matcher. after acquiring all the needed data from the database in a similar fashion  the structured matcher completes its matching activity for that hypothesis and returns that value of the match to the classifier  along with the data that the hypothesis can explain. the classifier's activity now proceeds along the lines of its control strategy: i.e.  it either rules out the hypothesis  or establishes it and pursues the successors. this process of each problem solver invoking other problem solvers who can provide the information needed for the performance of its own task is repeated until the classifier concludes with a number of high plausibility hypotheses  and information about what each of them can explain. at this point  the abductive assembler takes over and proceeds to construct the composite explanatory hypothesis for the problem. 
　　these are not the only problem solvers that could be useful for diagnosis  of course. additional problem solvers with their own types of knowledge and control can be helpful. if the classificatory structure or the structured matcher is incomplete in its knowledge  case-based reasoners  kolodner and simpson 1   or deeper domain models such as qualitative reasoners and functional reasoners  sembugamoorthy and chandrasekaran 1  can be invoked. mdx1 
 sticklen 1  is an example of a diagnostic system whose classifier interacts with a cognitive deep model of parts of its domain for obtaining information about the relationship between observations and hypotheses. 
the four generic tasks that we have just described 
 hierarchical classification  abductive assembly  structured matching  and database inference  are all distinct: the knowledge representation and inference method for each is distinctly different. they are also generic in the sense that each could be used by any other problem solver needing its functionality in that domain. 
　　the description that i just gave of problem solving omits many of the subtleties in the inference strategies of each of the problem solvers  since my aim was to introduce the methodology rather than present the complete theory of diagnosis. however  the following additional points are relevant since they refer to the important role played by the generic task architecture. 
1. the interaction between the abductive assembler and the classifier may be much more dynamic  where the refinement of medium confidence hypotheses in the classificatory structure can be done at the command of the assembler  which is looking for hypotheses to explain some unexplained data. 
1. the control of classificatory behavior in the presence of additive and subtractive observations can be complicated. for these and other complexities in control of classification problem solving  see  sticklen tt al. 1 . 
1. multiple and tangled hierarchies can be incorporated as is being done in mdx1  sticklen 1 . since a particular hypothesis can occur in more than one hierarchy  that is  it can be classified according to more than one perspective  the restriction of the hierarchies to tree 

structures is not burdensome or unnatural. 
1 	panels and invited talk1 

1. because of the property that for each observation  only a small number of malfunctions at each level of the hierarchy can be implicated  multiple malfunctions are a natural for this architecture. interactions are allowed as long as they can be reasoned about during classification  gomez and chandrasekaran 1  and assembly within the general assumption of near-independence. 
1. test ordering and data validation: in the description of the approach i have assumed that all the data are available. however  the architecture makes it easy to focus the test ordering decisions in a natural way. for each hypothesis knowledge is available about what tests are useful for it to be established or rejected  and as mentioned in 1 above the assembler can propose that hypotheses which were not strongly enough established for refinement be refined further in the interests of explaining remaining observations. this may actually call for additional tests to establish or reject the successors. also  the assembler will often be able to identify equally plausible  but alternative  hypotheses. in order to resolve them  additional tests may be necessary. this architecture makes possible test ordering that can be driven by the goals of each of the tasks. 
the localization of goals and knowledge that is helpful in test ordering can also be employed to provide a form of know ledge-based sensor validation  since sensors that conflict in their contribution to a hypothesis can be located and critiqued. see  chandrasekaran and punch 1  for further details. 
1. the fallacy of surface phenominalism 
　　if one were to look at the behavior of the diagnostic system described above  without any awareness of its internal architecture of generic tasks  almost all of the standard architectures could be ascribed to it  depending upon the level of abstraction at which the behavior is observed. as the hypothesis matcher is working through the hierarchical abstraction  it would appear to be a rule processor using evidences on the antecedent side to reach a conclusion about the intermediate abstractions or the hypothesis. at this level it would also appear to be a data-directed activity. at the classifier level  the system may seem to be a frame system moving from hypothesis concept to hypothesis concept. at this level it would appear to be a hypothesis-directed activity. the term uthe fallacy of surface phenominalism  refers to the pitfalls that are possible in going from external problem solving behavior  such as protocols of human experts  directly to an architecture. 
1. conceptualization and design of the generic task toolset 
   each of the generic tasks can be used as a programming technique within a more general programming language like lisp  prolog  or ops1. however  this does not prevent an knowledge engineer from going outside the boundaries of a generic task. these bounds are important to specify because they ensure that the advantages of generic tasks will be maintained. one natural way to do this is to implement a software tool for each generic task to be used in a general programming environment. such tools also provide an empirical means for testing the clarity of these ideas and the usefulness of the approach in actual systems. 
　　we have been motivated by the problems of diagnosis  design and planning in developing our toolset. in addition to the generic tasks that were described in connection with the diagnosis  we have found two other generic tasks very useful for our purposes: object synthesis using plan selection and refinement  brown and chandrasekaran 1  for certain classes of design problems  and state abstraction  chandrasekaran 1  for certain types of prediction of system-level consequences as a result of state changes to subsystems. due to space limitations  i do not describe them here. 
   for each generic task  we have developed a tool that can encode the problem solving and knowledge that is appropriate for the task. below is a list of the tools that correspond to the generic tasks that we have studied: csrl  conceptual structures 
representation language  is the tool for hierarchical classification  bylander and mittal 1 ; dspl  design specialists and plans language  is the tool for object synthesis using plan selection and refinement 
 brown and chandrasekaran 1 ; id able  intelligent data base language  is the tool for knowledge-directed information passing  sticklen 1 ; hyper  hypothesis matcher  is the tool for hypothesis matching  johnson and josephson 1 ; peirce  named after the philosopher c. s. peirce  is the tool for abductive assembly of hypotheses  punch et al. 1 ; and ww hi  what will happen if  is the 
tool for state abstraction. 
   the tools are intended to ensure the following advantages of the generic tasks: 
  multiformity. the more traditional architectures for the construction of knowledge based systems emphasize the advantages of uniformity of representation and inference. however  in spite of the advantage of simplicity  we argued earlier that uniformity results in a level of abstraction problem. a uniform representation cannot capture important distinctions between different kinds of knowledge-use needs. a uniform inference engine does not provide different control structures for different kinds of problems. 
	the generic 	task approach provides multiformity. 
each generic task provides a different way to organize and use knowledge. the knowledge engineer can choose which generic task is the best for performing a particular function  or can use different generic tasks for performing the same function. different problems can use different generic tasks and different combinations of generic tasks. 
  modularity. a know ledge-based system can be designed by making a functional decomposition of its intended problem solving into several cooperating generic tasks  as illustrated in our discussion on diagnosis. each generic task provides a way to decompose a particular function into its conceptual parts  e.g.  the categories for hierarchical classification  and allows domain knowledge of other forms to be inserted into a generic task  e.g.  evidence combination knowledge in hierarchical classification  sticklen 1 . each generic task localizes the knowledge that is used to satisfy local goals. 
  knowledge acquisition. each generic task is associated with its own knowledge acquisition strategy for building an efficient problem solver  bylander and chandrasekaran 1 . for example in hierarchical classification  the knowledge engineer needs to find out what specific categories should be contained in the classification hierarchy and what general categories provide the most leverage for the establish-refine strategy. 
  explanation. this approach directly helps in providing explanations of problem solving in expert systems in two important ways: how the data match local goals and how the control strategy operates  chandrasekaran et al. 1 . also  the control strategy of each generic task is specific enough for generating explanations of why the problem solver chose to evaluate or not to evaluate a piece of knowledge. this is because of the higher level of abstraction in which control is specified for generic tasks. 
  exploiting interaction between knowledge and inference. rather than trying to separate knowledge from its use  each generic task specifically integrates a particular way of representing knowledge with a particular way of using knowledge. this allows the attention of the knowledge engineer to be focused on representing and organizing knowledge for performing problem solving. 
  tractability. under reasonable assumptions  each generic task generally provides tractable problem solving  allemang et al. 1  goel et al. 1 .  one major exception is abductive assembly  
1 	panels and invited talks 
which can become intractable under certain conditions  making it hard then for humans and machines to perform the task.  the main reasons why they are tractable are that a problem can be decomposed into small  efficient units  and knowledge can be organized to take care of combinatorial interactions in advance. 
　　it should be noted that these advantages are attained at the cost of generality. each generic task is purposely constrained to perform a limited type of problem solving and requires the availability of appropriate domain knowledge. 
　　the generic task toolset is implemented as a collection of specialists which interact by passing messages for control and information exchange. this is a natural implementation at the level of generic tasks. as is turns out  the problem solving activities within a generic task problem solver are also implemented in our toolset as a collection of specialists; e.g.  within the hierarchical classifier  each of the classificatory hypotheses is a problem solving module in its own right. 
　　how the different generic problem solvers interact is an active research issue. the current theory and the toolset are based on each of the problem solvers explicitly invoking another problem solver for needed information. i believe that a more attractive long-term approach would be one where the problem solver broadcasts the need for some information  and other problem solvers which can deliver the information respond to the request. this reduces the degree of explicitness needed at system design time. this also increases the possibility that a piece of knowledge or a module added somewhere in the system will be able to contribute its problem solving power without the designer needing to foresee this possibility. 
1. abstract representation of control: 
　　it is often stated that using rule-based approaches makes it possible to have control knowledge explicitly represented as collections of rules. this declarative form of representation of control is said to help in reasoning about  modifying or explaining the control behavior of the system. 
　　the gt problem solvers are active agents. thus it might appear that the advantages of explicitness of control may be lost in the approach. as it turns out  the control strategies associated with each of the generic task architectures are implemented as a family of message types. the user can modify them within limits permissible for each generic task and the explanation generation system uses the content of the messages directly in its description of the control behavior. the fact that the messages have both a procedural content to them as well as a declarative representation gives them all the advantages of abstract representation of control. 
1. generic tasks and other use-specific architectures 
　　in the late 1's  when we embarked on this line of research - characterised by an attempt to identify generic tasks and the forms knowledge and control required to perform them - the dominant paradigms in knowledge-based systems were rule and frame type architectures. while our work on use-specific architectures was evolving  dissatisfaction at the limited vocabulary of tasks that these architectures were offering was growing at other research centers. clancey in particular noted the need for specifying the information processing involved by using a vocabulary of higher level tasks. task-level architectures have been gathering momentum lately: mcdermott and his coworkers  marcus and mcdermott 1  have built salt  a shell for a class of design problems  where critiquing proposed designs by checking for constraint-violations is applicable. clancey  clancey 1  has proposed a shell called heracles which incorporates a strategy for diagnosis: he calls it heuristic classification. bennett  bennett 1  presents coast  a shell for the design of configuration problem solving systems. gruber and cohen  gruber and cohen 1  offer a system called mum for managing uncertainty and so on. all these approaches share the basic thesis of our own work  viz.  the need for task-specific analyses and architecture support for the task. however  there are some differences in assumptions and methodology in some cases that needs further discussion. further  once we identify task-level architectures as the issue for highest leverage  then the immediate question is: what is the criterion by which a task is deemed to be not only generic but is appropriate for modularization as an architecture  how about an architecture for the generic task of  investment decisions   diagnosis  diagnosis of process control systems  is uncertainty management a task for which it will be useful to have an architecture  are we going to proliferate a chaos of architectures without any real hope of reuse  what are the possible relationship between these architectures  which of these architectures can be built out of other architectures  i do not propose to answer all these questions here  but they seem to be the appropriate kinds of questions to ask when one moves away from the comfort of universal architectures and begins to work with different architectures for different problems. 
   at this stage in the development of these ideas  empirical investigation of different proposals from the viewpoint of usefulness  tractability and composability is the best strategy. from a practical viewpoint  any architecture that has a useful function and for which one can identify knowledge primitives and an inference method ought to be considered a valid candidate for experimentation. as the tools evolve  one may find that some of the architectures are further decomposable into equally useful  but more primitive  architectures; or that some of them do not represent particularly useful functionalities  and so on. nevertheless  the following distinctions can be made on conceptual grounds  and can be used to drive the empirical investigations. 
   building blocks  out of which more complex problem solvers can be composed  such as the tasks in the theory presented earlier in the paper. 
  explicit high level strategies which we want a system to follow  where the strategies are expressed in terms of some set of tasks. 
heuristic classification is an example. 
  compound tasks  such as the form of diagnosis described in earlier in the paper. an architecture for this compound task will bring with it its constituent generic tasks and also show how to integrate them from the viewpoint of the overall task. 
  tasks which do not necessarily correspond to those human experts do well  but nevertheless can be captured as appropriate combinations of knowledge and inference and a clear function can be associated with them  e.g.  constraint satisfaction schemes. 
   i want to examine how the tasks in these different senses relate to the generic task theory. 
　　heuristic classification  clancey 1  is a strategy in the sense of its being an appropriate behavior for diagnosis  i.e.  it is a collection of tasks to be performed to accomplish the goal. heraeles  the architecture that supports heuristic classification  uses metarules as a way of programming this strategy in a rule system: the metarules represent abstractly the control behavior that would be required for each of the tasks in the strategy. heracles as a shell will enable the designer to build diagnostic systems using all or portions of the above strategy. in our work  we wish to get as much of the strategy as possible to emerge from the interaction of more elementary problem solvers. it is not clear that each of the tasks in a higher level strategy such as heuristic classification necessarily corresponds to one of the problem solvers in our sense. for example  as i have mentioned  the data abstraction part of his strategy actually emerges in our architecture for diagnosis from the abstraction step in the hypothesis matcher and data-to-data reasoning processes in the knowledge-directed data base. from a theoretical viewpoint this is not an irrelevant distinction: our theoretical goal is an  atomic  theory of knowledge use  so that more complex problem solving behaviors can be seen to emerge from the interaction of such atoms. from a practical viewpoint  while our architecture produces many of the needed behaviors in an emergent manner  the constituent parts still may need to be integrated to make the overall system 
	chandraaakaran 	1 
produce the needed behavior. thus  direct study of such  compound  problem solving behaviors such as heuristic classification is illuminating and technically useful. eventually  however  if the atomic theory is right  it will be able to show how the subtasks in heuristic classification arise from the form in which domain knowledge is available  and also provide a more principled vocabulary of subtasks in behaviors: e.g.  what's ugroup and differentiate  and where does it come from  
　　regarding compound tasks  1 need to point out that some of the generic tasks in our repertoire  such as object synthesis by plan selection and refinement  seem to me to be more complex than others and represent several generic tasks organized and integrated for a certain purpose. the dspl planner  e.g.  has plan selectors which use structured matching. the current set in our laboratory has evolved empirically. as we experimentally discover decomposability  we will proceed to separate compound tasks into simpler constituent tasks. we are also building even higher level architectures for practically important applications. for example  we are currently building a diagnostic shell which will integrate the problem solvers as outlined in our discussion. the shell can then be directly used for the design and implementation of diagnostic systems of this particular type. 
   there may be significant technological value in generic tasks whose function is to solve problems 
which human experts find it difficult to do without pencil and paper or computers  if they are integrated appropriately with other generic tasks. constraint satisfaction problems are of this type. design by constraint satisfaction is a useful method  but humans are not in general very good at solving problems in this way. the only caution in using this form of reasoning for design is that such architectures may encourage formulating design problems which have interesting and useful decompositions other than as constraint satisfaction problems  causing difficulties in debugging and explanation  i.e.  they may preclude a study of domain knowledge that helps a human designer in producing explainable und maintainable  if only satisficing  designs. 
1. what makes a building block 
   clearly  highly domain specific tasks  such as a shell for designing drug-therapy administration systems  are not generic in an interesting ai sense  though they may be generic within the domain  e.g.  the drug-therapy shell can be instantiated for different drugs . 
the work of cohen and his coworkers  gruber and 
cohen 1  in regard to the mum system  and its successor mu for uncertainty management  raises interesting issues related to this. it incorporates a strategy for explicitly reasoning about balancing 
1 	panels and invited talks 
uncertainty reduction with costs of tests to reduce uncertainty. they regard diagnostic reasoning as an instance of the uncertainty management problem. on the other hand  it can also be argued that uncertainty management is a component of diagnostic problem solving. clearly all problem solving  whether design or diagnosis  can be thought of as instances of uncertainty reduction. 
　　it seems to me that there are clearly situations in which managing uncertainty requires explicit and conscious strategies  and mu can be useful in those cases. however  it seems unlikely that uncertainty management is a generic building block activity in the sense of this paper  since the forms and strategies for handling uncertainty seem to be conditioned by the demands of the problem being solved and the form in which knowledge is available. the structured matcher handles uncertainty in one fashion  while the classifier deals with it in another. in  chandrasekaran and tanner 1   1 have discussed this view that uncertainty handling is not a unitary activity. i can see mu serving as a local advisor for uncertainty decisions within each of the generic tasks  when it is felt necessary to make some decisions  say test ordering  requiring explicit uncertainty manipulation. 
1. towards a functional architecture of intelligence 
   the generic tasks that are represented in our toolset were specifically chosen to be useful as technology for building diagnosis  planning and design systems with compiled expertise. for capturing intelligent problem solving in general  we will undoubtedly require many more such elementary 
strategies and ways of integrating thern. for example  the problem solving activities in qualitative reasoning and device understanding  e.g.  qualitative simulation  consolidation  and functional representation  qualify as generic problem solving activities  as do weak methods such as means-ends analysis. the work of schank and his associates has also generated a body of such representations and inference processes that have a generic character to them. all these tasks have well-defined information processing functions  specific knowledge representation primitives and inference methods. thus candidates for generic information processing modules in our sense are indeed many. what does all this mean for an architecture of intelligence  
   i am led to a view of intelligence as an interacting collection of functional units  each of which solves an information processing problem by using knowledge in a certain form and corresponding inference methods that are appropriate. each of these units defines an information processing faculty. i discuss elsewhere  chandrasekaran 1  the view that these functional units share a computational property: they provide the agent with the means of transforming essentially 

intractable problems into versions which can be solved efficiently by using knowledge and inference in certain forms. for example  goel et a/.   goel et al. 1  show how classification problem solving solves applicable cases of diagnosis with low complexity  while diagnosis in general is of high complexity. knowledge is indeed power  but how it acquires its power is a far subtler story than the first generation knowledge based systems made it appear. 
   this view generates its own research agenda: as a theory  the generic tasks idea has quite a bit of work ahead of it to tell a coherent story about how the tasks come together and are integrated  and how more complex tasks such as planning come about from more elementary ones. how complex inference methods develop from simpler ones and how learning shapes these functional modules  are issues to be investigated. 
   the theory does not take a position on the information processing architecture over which these functional units may be defined  i.e.  this is not an argument for or against the architecture of the substratum being realized as a rule processor or as a frame system. in fact  a continuing issue in ai is how the rule/frame viewpoints may be integrated within a principled framework. 
   the generic tasks idea has strong implications to knowledge representation and suggests a view of what mentalese  the language of thought  might be. what ultimately characterizes the generic task approach is not the proposals for specific generic tasks  which will undoubtedly evolve empirically  but a commitment to the following view: knowledge and its use need to be specified together in knowledge representation. because how knowledge is used depends upon the form in which knowledge appears  the enterprise of knowledge representation is one of developing vocabularies simultaneously for the expression and use of knowledge. the languages in which each of the generic information processing units encode their knowledge and communicate their information needs and solutions collectively defines the language of thought. 
acknowledgments 
   i acknowledge the central role grants 1 and 1 from air force office of scientific research have played in the development of these ideas. the support from defense advanced research projects agency  radc contract f 1-c-1  has also been invaluable. battelle university grants program helped in the development of portions of the tool box. fd like to acknowledge the contributions of fernando gomez  sanjay mittal  jack smith  david brown  john josephson  tom bylander  jon sticklen  and my current graduate students to the evolution of these ideas. the help of ashok goel and cindy sergent in the preparation of this paper is. gratefully acknowledged. 
