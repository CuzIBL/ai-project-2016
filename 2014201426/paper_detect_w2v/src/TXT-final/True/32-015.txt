 
	this 	paper 	describes 	and 
evaluates the confidence-based dual reinforcement q-routing algorithm  cdrq-routing  for adaptive packet routing in communication networks. cdrq-routing is based on the qlearning framework of q-routing. the main contribution of this work is the increased quantity and improved quality of exploration in cdrq-routing  which lead to faster adaptation and better routing policies learned as compared to q-routing  the state-of-the-art adaptive bellman-ford routing  and the nonadaptive shortest path routing. experiments over several network topologies have shown that at different loads  cdrq-routing learns superior policies significantly faster than qrouting. moreover  cdrq-routing learns policies that sustain higher load levels than qrouting. analysis shows that overhead due to exploration is insignificant as eqmpared to the improvements in cdrq-routing. 
1 	introduction 
in a communication network information is transferred from one node to another as data packets  tanenbaum  1 . the process of sending a packet p s  d  from its source node s to its destination node d is referred to as packet routing  bellman  1 . normally this packet takes multiple  hops  and on its way  spends some time waiting in the queues of intermediate nodes  while they are busy processing the packets that came earlier. thus the delivery time of the packet  defined as the time it takes for the packet to reach its destination  depends mainly on the total time it has to spend in the queues of the intermediate nodes. normally  there are multiple routes that a packet could take  which means that the choice of the route is crucial to the delivery time of the packet for any  s d  pair. if there was a global observer with current information about the queues of all nodes in the network  it would be possible to make optimal routing decisions: always send the packet through the route that has the shortest delivery time at the moment. 
1 	machine learning 
in the real world  such complete  global information is not available  and the performance of the global observer is an upper bound on actual performance. instead  the task of making routing decisions has to be shared by all the nodes  each using only local information. thus  a routing policy is a collection of local decisions at the individual nodes. when a node x receives a packet p s  d  originating at node s and destined for node d  it has to choose one of its neighboring nodes y such that the packet reaches its destination as quickly as possible. 
¡¡the simplest policy is the shortest-path algorithm  which always routes packets through the path with the minimum number of hops. this policy is not always good because some intermediate nodes  falling in a popular route  might have large queues. in such cases it would be better to send the packet through another route that may be longer in terms of hops but results in shorter delivery time. hence as the traffic builds up at some popular routes  alternative routes must be chosen to keep the average packet delivery time low. this is the key motivation for adaptive packet routing strategies that learn alternate routes through exploration as the current routing policy begins to lead to degraded performance. 
¡¡learning effective routing policies is a challenging task for several reasons  kumar  1 .  i  the goal is to optimize a global metric  the average packet delivery time of all packets  using local information   ii  there is no ''training signal  available for directly evaluating a routing policy until the packets have reached their destination.  iii  when a packet reaches its destination  such a training signal could be generated  but to make it available to the nodes responsible for routing the packet  the training signal would have to travel to all these nodes  consuming a lot of network resources   iv  finally  it is not known which particular decision in the sequence of routing decisions deserves credit for the performance  and how much  the credit assignment problem . thus  a way of efficiently exploring the network environment and continually updating the decision makers based on the local information is necessary in order to learn good routing policies. 
¡¡bellman-ford routing  bellman  1   bf  is by far the mast widely used distance vector adaptive routing algorithm. in bf  each node has two tables which con-

tain  for each possible destination   1  a cost  cost table  or minimum delivery time for sending a packet to that destination and  1  the node's neighbor {routing table  to which the packet should be forwarded to reach 
the destination for the corresponding cost. neighboring nodes exchange their cost tables frequently for adaptation. the drawback being an enormous overhead of exploration  exchange of routing information between nodes  and a slow rate of learning. q-routing  boyan and littman  1; littman and boyan  1  uses the q-learning framework  watkins and dayan  1  for this task. each node makes its routing decisions based on the local routing information  which is a table of qvalues that estimate the quality of the alternative routes. these values are updated each time the node sends a packet to one of its neighbors. this way  as the node routes packets  its q-values gradually incorporate more global information. such exploration has been shown capable erf adapting to load changes and to perform better than the non-adaptive shortest-path routing with high loads. 
¡¡this paper presents a new adaptive routing algorithm called confidence-based dual reinforcement q-routing  cdrq-routing;  kumar  1   that improves both the quality and quantity of exploration of q-routing. the quality of exploration is improved by associating confi-
dence values  between 1  with each of the q-values in the network. these values represent how reliably the corresponding q values represent the state of the network. the amount of adaptation for a q-value  in other words the learning rate  depends on the confidence values of the new and the old q-values  whereas in q-routing a fixed learning rate is used for all updates . this component of cdrq-routing is called confidence based-q-routing  cq-routing;  kumar and miikkulainen  1  . the quality of exploration is increased by including backward 
exploration  whereas in q-routing only forward exploration is used . as a result  with each packet hop  two qvalue updates take place  one due to forward exploration and the other due to backward exploration. this component of cdrq-routing is called the dual reinforcement q-routing  drq-routing;  kumar and miikkulainen  1  . essentially  drq-routing combines qrouting with dual reinforcement learning  which was first developed for a satellite communication problem  where the two ends of the communication system co-adapt using the reinforcement signal for the other end  goetz et al.  1 . cdrq-routing balances the complementary and independent improvements due to both these components into one algorithm. the q-routing algorithm is described in section 1  followed by the cdrq-routing in section 1. the performance of the two algorithms are evaluated experimentally in section 1 and compared to the standard shortest-path algorithm. the amount of overhead generated by these algorithms is analyzed in section 1  and a number of directions for future research outlined. 
1 	q-routing 
in q-routing  the routing decision maker at each node x makes use of a table of values   where each value is an estimate  of how long it takes for a packet to be delivered to node d  if sent via neighbor y  excluding time spent in node x's queue. when the node has to make a routing decision it simply chooses the neighbor y for which  is minimum. learning takes place by updating the q values. on sending p s d  to y  x immediately gets back y's estimate for the time remaining 
in the trip    where n n  denotes the set of neighbors of node n. node x can revise qx y d  based on this feedback and the queue length qy of y  using a learning rate nf; 

in other words  the information about the remaining path is used to update the q value of the sending node. such exploration can be termed forward exploration  figure 1 . 
1 confidence-based dual reinforcement q-routing 
both the quantity and quality of q-routing are improved in cdrq-routing using two components  cqrouting and the drq-routing  respectively. 
1 	cq-routing 
when a q-value is not updated for a long time  its reliability in representing the true state of the network goes down. in q-routing there is no way of quantifying the reliability of a q-value. moreover  in equation  1  the learning rate is constant for all updates  although it should depend on how reliable the updated and estimated  q-values are. these issues are addressed in cq-routing. in cq-routing  the accuracy  or reliability  of each q-value is quantified by an associated confidence value  c-value   
 1 .  close to 1 indicates that qa: y d  represents the network state accurately  while  dose to 1 indicates that  is almost random. the base case c-values corresponding to the base case q-value  q* y y  = s  are c. y y  = 1  vy € n x  and v* € v  the set of all nodes in the network. the c-values corresponding to all the other q-values  which are initialized randomly  are initially set to 1. 
¡¡in cq-routing  the learning rate depends on the cvalues of the q-values that are part of the update. more specifically  when node x sends a packet p s  d  to its neighbor y  it gets back not only the best estimate of node y for the remaining part of p s d 's journey  namely   but also the confidence value associated with this q value  namely  now when node x updates its qs y d  value  it first computes the learning rate . . . . which depends on both 
	kumar and miikkulainen 	1 

exploration of q-values is done locally between neighboring nodes during a packet hop to avoid excessive exploration overhead. in q-routing  only one q-value is updated when a packet p s d  hops from 
x to y  forward exploration   however  one more qvalue   ~ ' can also be updated in the same hop. this idea of using information about the traversed path for exploration in the reverse direction is called backward exploration  kumar  1   figure 1  and is derived 
from dual reinforcement learning  which was first developed for adaptive signal predistorters in satellite communications  goetz et al.  1 . drq-routing incorporates backward exploration into the q-routing algorithm. when node x sends a packet p $  d  to its neighbor y  the packet can take along q-value information of node x  which can be used by y for updating its own estimate pertaining to x. later when node y has to make a decision  it has the updated q-value for x. the only exploration overhead is a slight increase in the size of the packets. more specifically  p s  d   currently at node x  carries to node y the estimated time it takes for a packet destined for node s from node xf that is 
with this information  node y 
can update its estimate   of sending a packet to node s via its neighbor x  using the queue length qx of node x and a learning rate nb: 
		 1  
¡¡in other words  the information about the path the packet has traversed so far is used to update the q value of the receiving node. this way the packet is used to carry routing information from one node to the next as it moves from source to destination. in drq-routing  both forward exploration and backward exploration are used to update two q-values in each hop. figure 1 illustrates these two updates as the packet p s  d  hops from node x to its neighbor y. 
1 	cdrq-routing 
cdrq-routing combines both the cq-routing and 
drq-routing components. 	at each hop of packet 
1 machine learning 
1 	experiments 
the experiments described in this paper are based on a simulated communication network. packets destined for random nodes are introduced into this network at random nodes. the number of packets introduced per unit simulation time step  pkt/sim-time  is called the network load. multiple packets at a node are stored in its unbounded fifo queue. in one time step  each node removes the packet in front of its queue  examines the destination of this packet and uses its routing decision maker to send the packet to one of its neighboring nodes. the delivery time of a packet is defined as the time between its introduction at the source node and its removal at the destination node. delivery time is measured in terms of simulation time steps. average packet delivery time  computed at regular intervals  1 time steps in the current experiments   is the average of all the packets arriving at their destinations during the last interval. this measure is used to monitor the network performance while learning is taking place. average packet delivery time after learning has settled measures the quality of the final routing policy. 
¡¡the performance of cdrq-routing was tested against q-routing  bellman-ford routing and nonadaptive shortest-path routing  on a number of network topologies including 1 node 1d-hypercube  1-node lata telephone network  and an irregular 1 x 1 grid 
 due to  boyan and littman  1; littman and boyan  1  and shown in figure 1 . the results were similar in all cases; the discussion below focuses on the last one since it best illustrates adaptation. in this network there are two ways of routing packets between the left cluster  nodes 1 through 1  and the right cluster  nodes 1 through 1 : the route including nodes 1 and 1 {r   and the route including nodes 1 and 1  rz . 
¡¡the shortest-path routing algorithm  which chooses the route with minimum hops  routes most of the traffic between the left cluster and the right cluster via route 

figure 1: the 1 x 1 irregular grid. 
r1 for low loads  e.g. 1 pkt/sim-time   this routing policy works fine and throughout the simulation  the average packet delivery time is low  figure 1 . bellmanford shows a small learning period after which it also converges to close-to-optimal routing policy by around 1 sim-time. at medium loads  such as 1 pkt/simtime   the shortest-path strategy breaks down as nodes 1 and 1 become flooded with packets. the average delivery time increases linearly with simulation time  figure 1 . bellman-ford learns a stable but inferior routing policy at medium load. at high loads  such as 1 pkt/sim-time  the flooding of node 1 and 1 takes place at an even faster rate. bellman-ford breaks down at high loads and shows similar trends as shown by shortest path at low loads. thus bellman-ford is good only at low loads but its performance degrades as load increases. 
the main result  however  is the comparison between 
q-routing and cdrq-routing. to demonstrate the independent and complimentary contributions of the components of cdrq-routing  the performance of cqrouting and drq-routing are also shown. the q-tables in q-routing and cdrq-routing at all nodes were initialized to low random values. in q-routing and drqrouting  the learning rate for forward exploration  nf  was set at 1  while that of backward exploration in drq-routing  nb was set at 1. in cq-routing and 
cdrq-routing  the learning rates are computed using the confidence values of equation  1 . the decay constants a in cq-routing and cdrq-routing were set at 1 and 1 respectively. these learning rates and decay constants were found experimentally to give the best performance. 
¡¡the results shown in figures 1 and 1 for low  medium and high loads  are averages of 1 test runs  each time with different random start. statistical significance was computed using standard t-test  press et a/.  1  at 1% confidence. during the first few hundred time steps the average packet delivery times are small because the packets destined for distant nodes have not yet reached their destinations  and statistics are available only for figure 1: learning at low load 

figure 1: learning at medium load 
packets destined for nearby nodes  with small delivery times . as distant packets start arriving  the average packet delivery time increases  while learning is still in progress. eventually the learning converges  and each of the curves settles down indicating a stable routing policy. 
¡¡at low loads  cdrq-routing learns an effective routing policy almost three times faster than q-routing 
 figure 1 . cq-routing is only slightly better than qrouting while drq-routing is significantly better. this is because at low loads the number of packets in the network and consequently the number of q-value updates is low. hence increasing the amount of exploration per packet hop has a significant effect on the speed of learning. at medium loads  cdrq-routing learns a more effective policy nearly twice as fast as q-routing  figure 1 . cq-routing and drq-routing  are both significantly better than q-routing throughout the learning phase  1 through 1 time steps . cdrq-routing is better than cq-routing and drq-routing  which shows that both components of cdrq-routing contribute indepen* dently and in complementary ways to cdrq-routing. 
at high load levels  cdrq-routing converges to a rout-
	kumar 	and 	miikkulainen 1 


ing policy which is more than twice as good  in terms of average packet delivery time  as the policy to which q-routing converges  figure 1 . again  cdrq-routing learns faster than both its components. 
¡¡the results are summarized in figure 1  which shows the average packet delivery times at different load levels after the learning has converged. this measures the quality of the final policy in terms of how much load it can sustain. the plots are averages over 1 simulations. the performance of the  global  routing policy with complete information about all nodes  section 1  is also shown as a benchmark. shortest path routing breaks down as load increases beyond 1 pkts/sim-time due to excessive traffic buildup along r1. bellman-ford can sustain a little higher load levels but breaks down at around 1 pkt/shn-time. cdrq-routing learns significantly better policies than q-routing at all loads. moreover  the performance of cdrq-routing is close to the global routing until 1pkts/sim-time and it can sustain load levels up to 1 pkt/sim-time while q-routing breaks down after 1 pkt/sim-time. 
1 	machine learning 
1 	discussion and future work 
exploration makes it possible for a routing algorithm to adapt. in this paper  q-routing was compared with cdrq-routing  which uses both forward and backward exploration  together with confidence values for exploration. cdrq-routing has more exploration per packet hop and the quality of exploration is also higher. as a result  cdrq-routing learns better routing policies than q-routing significantly faster. moreover  compared with bellman-ford routing  cdrq-routing is much superior in terms of speed of convergence  quality of the routing policy learned  load level sustenance and amount of exploration overhead  making them more practical for use in communication networks. 
¡¡exploration adds overhead into the routing algorithm. it is important to analyze the tradeoff between the improvements and the overhead incurred. in bellman-ford  exploration overhead comprises of frequent exchanges of cost tables  which are as large as the number of nodes in the network  between every pair of neighboring nodes. this leads to prohibitive overhead and is a serious drawback for bellman-ford. the q-routing and cdrqrouting  on the other hand do not suffer from this drawback. in forward exploration  when a node y receives a packet from node x  it sends back an estimate and a 
¡¡confidence value to node x. the estimate does not enter node x's queue  but instead node x waits for the estimate and processes it before the next packet in its queue. the transmission of this estimate over the link takes time  where p is the size of the data packet  that takes s units of transmission time  and e is the size of the estimate packet containing a q-value and a c-value. the percentage overhead due to forward exploration is e/p  since the transmission time is proportional to packet size . in backward exploration  an additional q value and its c-value are appended to the packet  increasing the packet size to  p+e . the transmission time of the larger packet is . again the percentage overhead due to backward exploration is only e/p. the total overhead due to both forward and backward exploration is therefore  1e/p. since estimate packet contains only a q-value and a c-value  this overhead is less than 1%  while the adaptability they establish improves the performance of the routing algorithm multi-fold. 
¡¡unbounded fifo queues were used in the current simulations for simplicity. in the real world  the queue buffers of the network routers are finite  leading to possible congestion in heavily loaded parts of the networks. extension of the cdrq-routing to address the problem of finite buffer networks is an important future direction. this extension would make cdrq-routing a more realistic routing strategy that does not only route optimally  but can also sustain higher loads in finite buffer networks to avoid congestion. also the processing speeds and link delays 1 are assumed in this paper. while this assumption is realistic for a small scale lan  it is not valid for a heterogeneous communication network like the internet. ability of cdrq-routing to learn effective routing 

policies on such networks is another direction of future 
1 	conclusion 
in this paper a new adaptive network routing algorithm  
cdrq-routing  was presented. it combines q-routing and dual reinforcement learning to get increased explorative capabilities  and introduces a confidence measure to quantify the reliability of q-values. cdrq-routing is superior to q-routing and bellman-ford routing both in terms of quality and quantity of exploration as shown by simulation results. cdrq-routing learns a better routing policy more than twice a fast as q-routing at various load levels. at high loads  the cdrq-routing policy performs more than twice as well as q-routing policy in terms of average packet delivery time  while bellman-ford routing breaks down at high loads. moreover  cdrq-routing can sustain higher load levels than shortest-path routing  bellman-ford routing and qrouting. the additional overhead of adding backward exploration and c-values is less than 1%  which makes cdrq-routing an efficient and practically viable adaptive network routing algorithm specially as compared to the bellman-ford routing. 
