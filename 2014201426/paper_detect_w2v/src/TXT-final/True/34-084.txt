 
¡¡¡¡¡¡the possibility of saving various computational resources is an argument often advanced in favor of permitting question-answering systems to make occasional errors. in this paper we establish absolute bounds on the amount of memory savings that is achievable with a specified error level for certain types of question-answering systems. question-answering systems are treated as communication channels carrying information concerning the acceptable answers to an admissible set of queries. shannon's rate-distortion theory is used to calculate bounds on the memory required for several question-answering tasks. for data retrieval  pattern-classification  and position-matching systems it was found that only small memory gains could be 
materialized from error-tolerance. in pair-ordering tasks on the other hand  more significant memory savings could be accomplished if small errorrates are tolerated. similar limitations govern the tradeoffs between error and computation time. 
	1. 	introduction 
¡¡¡¡¡¡loosely speaking a question-answering  qa  system 1s any mechanical system which accepts data of a specified type and at a later time produces output symbols in response to queries from some admissible set. the output symbols are interpreted as answers to the queries about the specific dataset and are expected to conform to some acceptable test of correctness. practically all questionanswering systems in operation today are deductive in nature. when a query is presented concerning an assertion not derivable from the data  either the query is declared inadmissible  or the data is declared incomplete. 
¡¡¡¡¡¡it is apparent that human information processing is not confined to deductive reasoning; qualifiers such as 'probably'  'perhaps'  '   doubtfully'  and many more are very common in everyday language. people quite often assert or deny propositions which are either themselves incomplete  concern i n complete data  or are too complicated to be computed within the available time constraint. we usually interpret such behavior as an attempt to economize some computational resource   e . g .   data spec i f i c i t y   storage space  search time  at the expense of answer accuracy. correspondingly  it has been suggested that a substantial amount of computational resources could be gained if one allowed computerized qa systems to make occasional errors via approximate representations of data or worldmodels. zadeh  has proposed the use of fuzzysets to cope with the  incompatibility principle.  in zadeh*s words:  the essence of this principle is that as the complexity of a system increases  our ability to make precise and yet significant statements about its behavior diminishes until a threshold is reached beyond which precision and 
significance  or relevance  become almost mutually exclusive characteristics.  rabin  conjectured 
that the disparity between the super-exponential complexity of proofs in even the simplest algebras and the apparent simplicity of everyday human planning behavior can perhaps be explained by man's willingness to tolerate a small amount of error. 
     these notion  while in harmony with one's intuition and experience  have not yet been submitted to a careful quantitative analysis. this paper should be considered as a step toward such analysis. it concerns the economy of storage space and establishes absolute bounds on the amount of storage that can be saved by tolerating a specified percentage of errors in certain simple  yet typical   qa tasks. as a starting point  we ignore the question of computational-complexity or searchtime  thus allowing the qa system any required  but finite  amount of time for organizing and filing data into storage  as well as for computing the answers from the condensed data. a more recent 
work  demonstrates that the relative savings of most complexity measures in use are asymptotically bounded by the relative savings of storage space. hence  the latter should be considered a fundamental characteristic of error-tolerating computations. 
¡¡¡¡¡¡four question-answering tasks are analyzed: data-retrieval  pattern-classifications  pos i t i on etching and pair-ordering. the first and second tasks  are concerned with the economical description of one-place predicates  while the third and fourth concern the description of an order relation. 
     data-retrieval qa systems accept as data an arbitrary subset of individuals  from some universe   and respond to queries of the type  has this individual been observed before   a typical example would be the task of determining whether a certain node in a game tree has been expanded before  or whether it has been recognized before as a  forced-win  position. a pattern-classification system accepts data in the form of a subset of individuals classified into  mutually exclusive categories  property-values . it responds to queries demanding the proper classification of a given individual from the observed l i s t   permitting an arbitrary classification of unobserved individuals. 
     both position-matching and pair-ordering systems accept data in the form of an ordered l i s t of individuals. position-matching systems admit queries of the type  what individual stands in the kth position   or  who is the immediate successor of individual  pair-ordering systems respond to 
queries of the type  does x precede y  . the numerical encoding of the order in position-matching tasks is known as a nominal scale  e.g.  the number on the shirts of football players  and that of pair-ordering tasks as ordinal scale  e.g.  ranking of players in order of ability . 
¡¡¡¡¡¡in all these tasks it is possible to answer all queries correctly if an exact replica of the original data is kept in storage. in many cases  
1 


1 

     for the purpose of calculating the minimum size of s it is convenient to regard a qa system as a communication channel which receives at its input the data-set p and reproduces at its output the answer-string a. thus  the source alphabet 1s m  and the reproducing alphabet  a  1s theqcollection of all possible answer-strings  a= aq. 
     let i designate an input data and j an output answer-string  and let dij be the  per letter  distortion measure between the two. shannon's rate-distortion function for this source 1s defined by : 

where i is the average mutual information between m and a  and pq is the set of all conditional probabilities yielding a mean distortion not exceeding d: 

     shannons source-coding theorem states that it is not possible with any coding scheme to transmit the source information through any channel of capacity less than r d  with average distortion less than d. conversely  the positive form of the theorem states that given any channel with capacity c   r d   coding schemes exist which result in an average distortion arbitrarily close to d when used over this channel. if one regards b*.   1n figure 1  as the source encoder  b find  . as the decoder  and the memory s as a noiseless channel with capacity s  then shannon's source-coding theorem implies that in order for a qa system to exhibit a mean distortion not exceeding d  it must be provided with a memory space at least as large as r 1 . conversely  the theorems guarantee that it is possible to realize a mean error arbitrarily close to d with an average memory space not exceeding r d . 
the latter is true only if coding very long blocks of data is permitted  such as would be required for serving many users. if a code is to be assigned to each separate data-set as demanded by  1b   then r d  only provides a lower bound on the memory space required. 
     to obtain the rate-distortion function for a qa system one needs to minimize expression  1  with p1 * 1/m and dij given by  1 . shannon has shown that r d  is a continuous  strictly decreasing  convex u function for 1   d   d l   . where d 
	- 	- max 	max 
is the minimum distortion achievable with zero 

     the calculation of the rate-distortion function for an arbitrary distortion matrix is  in general  a difficult task. for our purposes  however  it would be sufficient to derive bounds on r d  using well known methods of rate-distortion theory . the derivations of these are omitted in this paper and can be consulted in references    and . 
¡¡¡¡¡¡¡¡1. predicate-related qa systems data-retrieval systems 
     consider a collection c of m objects from which a subset c of n objects 1s selected and presented to a qa system during the filing phase. at a later time queries of the type  has object x x€c  been observed   are presented to the system for confirmation or rejection. an error 1s committed when either a member in c is declared 
 unobserved  or an object in c -c 	fs declared observed . 
¡¡¡¡¡¡it is clear that if no error at all can be tolerated  then at least log1 m  bits of storage must be devoted to describe the observation set. when memory size is limited and  m  large  a typical example: rn = 1 1 n = 1 1    one 1s tempted to  summarize  the data at the expense of accuracy. the most straightforward way to reduce storage would be to simply ignore a portion of the data  and produce arbitrary answers to queries referring to the missing objects. we shall call this scheme  data-erasure   and will use it to gauge the performance of more sophisticated description schemes. 
     the pattern-recognition literature  may provide many ideas for such schemes. one may be tempted  for instance  to store only 'representative' samples of the observed set  and base the decisions on the 'proximity' between the query object x and the stored 'representatives'. another possibility is to approximate the binary representation of the observed sample by a boolean expression of a limited length  store this expression  and use it in the decision phase. a more popular method is provided by the so-called  statistical pattern recognition . here the data-set 1s merely used to calculate a set of parameters governing certain statistical models; the parameters are stored and subsequently used to generate the answer with the highest probability of correctness. an even more sophisticated description'method can be devised  based on the so-called  linguistic pattern recognition  philosophy. the names of the observed objects are regarded as sentences in some language  a grammar generating this language is inferred and stored by b f i l e   while bf nd parses the query sentences and decides whether it is in the language. it will be shown that in the absence of any special structures underlying the selection of the subsets c and for large m  all such schemes possess memory vs. error characteristics equal or i n ferior to those obtained by data-erasure. 
     the data-set space  for this problem  consists of all subsets of cm which contain exactly n 
1  a discussion of the storage vs. search-time tradeoff for such tasks is qiven by mlnsky and papert . 
objects  while the query set 1s isomorphic to c . therefore: 

it demonstrates that as the ratio m/n increases  the rate-distortion function approaches the dataerasure line 1 - d/dmax at a logarithmic rate. thus  no amount of error  below the trivial level of d = n/m obtainable by answering all queries in the a f f i r m a t i v e   can make memory demands grow slower than n log m/n. 
     the implications of these results for the prospects of finding a general language that simplifies descriptions of subsets are rather discour-
aging. they essentially imply that no filing scheme exists which performs appreciably better than one which simply ignores a certain fraction 
of the observed data. only for sample sizes on the order of m/1 can the statistical dependence among the samples be utilized for achieving a somewhat higher memory saving. 
     preliminary investigation of the effect of structure  using markovian model  shows that the essential features of the memory vs. error characteristics remain unaltered with the introduction of structure into the data. while the memory required to achieve zero error is substantially reduced  the additional percentage reduction in memory require-
ment induced by error-tolerance remains close to 
that of structureless data. 
pattern classification systems 
¡¡¡¡¡¡consider a collection c of objects  or patterns  which is partitioned into three classes: n objects are in class c   n in c   and the remaining m-1n objects in class c¡ã. during the filing phase a qa system is shown the elements of c and c  with their proper class identity. during the query phase an arbitrary object from c is presented  and the system is asked to classify it either as c or c . an error is counted whenever an element of c or c  is misclassified  but not when an element of c¡ã is labeled  +  or  - . c+ and c* stand for what is commonly called the  training-set   while 
c¡ã represents unobserved patterns. the neutrality of c¡ã reflects our commitment to evaluate system performance strictly on the basis of observed data  leaving aside considerations of inductive-power. 
     it is clearly possible to answer all queries correctly if we store the identities of the n ele-
ments of c   we simply label every query   +   if it is in storage and  -  if it is not. this scheme requires on the order of n logo m bits of memory and thus could become prohibitively large for practical values of the sample-size and the data dimensionality  e.g. m = 1¡ã n = 1 1  . most pattern classification methods  however  are motivated by the belief that classification systems lend themselves to a greater storage economy. it is prompted by noting that unlike data-retrieval systems the exact reconstruction of the data is not required; only information pertaining to the difference between the classes need be stored. it follows that one can always add any number of elements from c to either c or c~ at no extra cost  and s  simplify the description of the separating rule. it will be shown that this argument is justified; the number of dichotomies needed for correct separation of all pairs of classes is much smaller than the number of all possible choices of c . thus  storing the description of any one such dichotomy could be made more economical than storing the exact description of c . 

     this does not mean  however  that more sophisticated schemes cannot provide better storage-error 
characteristics under special circumstances. if  for instance  there exists a strong a priori knowledge that the observed sample is in some sense compact  then it becomes quite reasonable to store only descriptions of its boundaries  not the entire sample. the results obtained here refer to query sets and data sets which are uniformly and independently distributed. they are valid in cases 
where no structural knowledge exists  except the sample size n ; any combination of n objects from cffl could constitute a data set and any member of cm could represent a query. 
1 


1 


thus  given any non-1ero d  there is always an m sufficiently large to make the bound zero. this can be attributed either to the fact that the system permits increasing amounts of memory savings  or simply to the looseness of the bound. we will later demonstrate that the former is true  by using an upper bound on r d . 
     note that the essential difference in the behavior of equations  1  and  1  comes from the increased size of the query set q. the two are special cases of a general bound : 
	r d  m. log m - q h. d  	 1  
which holds for all question-answering systems with symmetric distortion measure  1 . we may conclude that in order for r d /r 1  to vanish for all d   1 it is necessary that q increases  with m  faster than log m. this can be expressed by the general statement: a necessary condition to enable a qa system to convert any small error tolerance into a slower growth of memory demand is that its query set be increasingly redundant. positionmatching  for instance  has a query set with only a slight redundancy; one must know the position of m-1 items before the position of the remaining mtn item can be deduced with certainty. in pair-ordering systems  on the other hand  knowing the correct precedence of only a small fraction  m-1  of selected pairs may be sufficient to deduce  by transitivity  precedence in all the remaining  m-l  m-1 /1 pairs. 
     we now derive an upper bound on r d  and demonstrate that a small error tolerance can reduce memory demands by a factor of log m. to that end we calculated the rate-distortion function r  d  of a restricted system where only transitivity-preserving answer-strings are allowed. this yields a parametric representation for rr d  : 

where z is the independent parameter varying between 1 and 1. for values of z not too close to 1 the summations in  1  converge rapidly  permitting numerical computations of rr d  even for very large values of m. 	equation  1  is depicted by 
the solid lines of figure 1 for m * 1  1  and 1. it is seen that the  normalized  curves undergo a sharp drop in r near d = 1  and that this drop continues to grow with increasing m. 
 d  
is given by: 

     recalling that shannon's theorems guarantee only the existence of codes for very long data blocks  it remains to show that a memory reduction similar to  1  could also be achieved by schemes which encode each data-set separately. this can be demonstrated using the popular filing scheme known as  clustering . assume that the sequence  represents the correct order of 
elements. divide the sequence into  groups each containing exactly  consecutive elements  and retain in memory the name of each item coupled 
with its group name. in the answering phase  adopt the following strategy: 1f  and o  belong to two different groups  use the group label to decide precedence; if  and  belong to the same group  choose an arbitrary answer. it 1s easily shown that the memory vs. error characteristics of this scheme are governed by: 
		 1  
implying a linear memory growth similar to that predicted by the restricted rate-distortion function  d . the two are compared in figure 1. 
1 ¡¡¡¡¡¡it is tempting to relate these results to fuzzy-sets representation  schemes. here too  only a fixed number of linguistic labels such as  very small    medium    large   and  extremely large  are used to characterize linear orders of any size. equation  1  indicates that for the purpose of pair-ordering the scheme is close to optimal. however  clustering methods employ sharp partitioning of the element-set into non-fuzzy subsets. it is not clear whether additional memory 
gain could be achieved by the use of numerical indicators for 'the degree of set membership'. it is conceivable that the computational advantage of fuzzy-set coding schemes can only be demonstrated in more complex data structures such as those containing relations among several orders. 
1. conclusions and relations to computational complexity 
¡¡¡¡¡¡four question-answering tasks were analyzed to test the conjecture that significant memory savings could be materialized with only a slight tolerance for errors. it was shown that the tasks of dataretrieval and pattern-classification  which in the past have absorbed considerable efforts toward economy of descriptions  are subject to a basic limitation. no data-reduction scheme exists which exploits error-allowances in an appreciably more effective way than a straightforward data-erasure. position-matching  namely  the task of representing a linear order for the purpose of rank identification  or successor identification   is subject to a similar limitation. in applications where the basic decision unit served by the order is the determination of precedences on pairs of elements  a 
more substantial reduction in memory size can be achieved when a small but f i n i t e error is permitted. this difference in behavior is attributable to the fact that in pair-ordering applications the elementary decision tasks are highly interdependent  via t r a n s i t i v i t y     whereas those in position-matching applications are almost independent. 
¡¡¡¡¡¡in a more recent work  it was found that the results reported in this paper go beyond our o r i g i nal intent of limiting the storage-error exchange. the work demonstrates that many complexity measures on a variety of computation models can be effectively bounded using the rate-distortion function. given a computational task   e . g .   sorting  function computation  sequence generation   a class of algorithms for accomplishing that task   e . g .   straight line algorithms  f i n i t e state machines  and a com-
plexity measure on the class   e . g .   program length  time or space complexity  combinational complexity   there exists a characteristic function c r d   which provides a lower bound on the mean complexity necessary to produce a mean distortion not exceeding d. moreover  in almost all interesting cases the function approaches a straight line c r d   * a r 1  as the task size becomes very large. this implies that the relative reduction in the mean complexity i n duced by tolerating a mean distortion d cannot exceed the relative change of the rate-distortion function r d . 
typical results obtained by this method are: 
¡¡¡¡¡¡1. the mean number of binary comparisons required for sorting n-sequences is bounded by 
	c ds  	  n l o g n   l - d s   	  	n - 	--  	 1  
where d$ is the fraction of terms in the output sequence which are followed by wrong successors. it implies that no sorting scheme exists which matches a fixed percentage of the term with their correct successors and which requires less than 1 n log n  comparisons. 
     1. if d is the fraction of pairs in the output sequence which are out of order  then the mean number of comparisons is bounded by 

¡¡¡¡¡¡this work was supported by the national science foundation under grant no. gj 1. 
