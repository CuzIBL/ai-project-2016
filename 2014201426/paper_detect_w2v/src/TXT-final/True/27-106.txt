 
non-deductive reasoning systems are often representation dependent  representing the same situation in two different ways may cause such a system to return two different answers. this is generally viewed as a significant problem. for example  the principle of maximum entropy has been subjected to much criticism due to its repre-
sentation dependence. there has  however  been almost no work investigating representation dependence. in this paper  we formalize this notion and show that it is not a problem specific to maximum entropy. in fact  we show that any probabilistic inference system that sanctions certain important patterns of reasoning  such as a minimal default assumption of independence  must suffer from representation dependence. we then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata. 
1 	introduction 
it is well known that the way a problem is represented can have a significant impact on the ease with which people solve it  and on the complexity of an algorithm for solving it. we are interested in what is arguably an even more fundamental issue: the extent to which the answers that we get depend on how our input is represented. 
   to make our discussion more concrete  we discuss this issue in one particular context: probabilistic inference. we focus on probabilistic inference both because of the recent interest in using probability for knowledge representation  e.g.   pearl  1   and because it has been the source of many of the concerns expressed regarding representation. however  our approach should be applicable far more generally. 
   suppose we have a procedure for making inferences from a probabilistic knowledge base. how sensitive is it to the way knowledge is represented  consider the following examples  which use perhaps the best-known non-deductive notion of 
    research sponsored in part by the air force office of scientific research  afsc   under contract f1 -c-1  and by a university of california president's postdoctoral fellowship. probabilistic inference  maximum entropy  jaynes  1 .l 
example 1: suppose we have no information whatsoever. what probability should we assign to the proposition colorful! symmetry arguments might suggest 1: since we have no information  it seems that an object should be just as likely to be colorful as non-colorful. this is also the conclusion reached by maximum entropy. but now suppose we consider a more refined view of the world where we have colors  and by colorful we actually mean red v blue v green. in this case  maximum entropy dictates that the probability of red v blue v green is 1. note that  in both cases  the only conclusion that follows from our constraints is the trivial one: that the probability of the query is somewhere between 1 and 1. i 
example 1: suppose we are told that half of the birds fly. there are two reasonable ways to represent this information. one is to have propositions bird and fly  and use a knowledge base kb =def  pr fly | bird  = 1 . a second might be to have as basic predicates bird and flying -bird  and use a knowledge base kbfly -def  {flying-bird =  bird  a pt flying'bird bird  - 1 . although the first representation may appear more natural  it seems that both representations are intuitively equivalent insofar as representing the information that we have been given. but if we use an inference method such as maximum entropy  the first representation leads us to infer pr bird  = 1  while the second leads us to infer pr bird  = 1. i 
   examples such as these are the basis for the frequent criticisms of maximum entropy on the grounds of representation dependence. but other than pointing out these examples  there has been little work on this problem. in fact  other than the work of salmon  salmon  1; salmon  1   there seems to have been no work on formalizing the notion of representation dependence. one might say that the consensus was:  whatever representation independence is  it is not a property enjoyed by maximum entropy.  but are there any other inference procedures that have it  in this paper we attempt to understand the notion of representation dependence  and to study the extent to which it is achievable. 
     1  although much of our discussion is motivated by the representation dependence problem encountered by maximum entropy  an understanding of maximum entropy and how it works is not essential for understanding our discussion. 
	halpern and koller 	1 

   to study representation dependence  we must first understand what we mean by a  representation . the real world is complex. in any reasoning process  we must focus on certain details and ignore others. at a semantic level  the relevant distinctions are captured by using a space x of possible alternatives or states. in example 1  our first representation focused on the single attribute colorful. in this case  we have only two states in the state space  corresponding to colorful being true and false  respectively. the second representation  using red  blue  and green  has a richer state space. clearly  there are other distinctions that we could make. at a syntactic level  we often capture relevant distinctions using some formal language. for example  if we use prepositional logic as our basic knowledge representation language  our choice of primitive propositions characterizes the distinctions that wc have chosen to make. in this case  wc can take the stales to be truth assignments to these propositions. similarly  if we use belief networks  pearl  1  as our knowledge representation language  we must choose some set of relevant variables. the states are then then possible assignments of values to these variables. 
   what does it mean to shift from a representation  i.e.  state space  x to another representation y  for us  this amounts to associating subsets of a' with subsets of y. thus  for example  the state where colorful holds can be associated with the set of states where either red  blue  or green holds. we capture the notion of representation shift formally by embeddings. an embedding f from a' to y maps subsets of a to subsets of y in a way that preserves complementation and intersection. an embedding is just the semantic version of the standard logical notion of interpretation  enderton  1  pp. 1  which has also been used in the recent literature on abstraction  giunchigliaand walsh  1; nayak and levy  1 . essentially  an interpretation maps formulas in a vocabulary ¦µ to formulas in a different vocabulary ¦µ by mapping the primitive propositions in ¦µ  e.g.  colorful  to formulas over ¦µ  e.g.  redvblue / green  and then extending to complex formulas in the obvious way. the representation shift in example 1 can also be captured in terms of an interpretation  this one taking flying-bird to fly a bird. 
   when doing probabilistic reasoning  we are actually interested in the probability of the various states in a'. we therefore assume that a user's knowledge base kb consists of a set of probabilistic assertions  such as pr fly   bird  = 1  that place constraints on distributions over x. a representation shift from x to y induces a corresponding shift  from information about x to information about y. more formally  an embedding / from x to y can be extended to a mapping /* from constraints on distributions over x to constraints over distributions over y in a straightforward way. for example  if the embedding / maps colorful to red v blue v green  then /*   r{colorful    1  is pr red v blue v green    1. 
   a probabilistic inference procedure |- takes a probabilistic knowledge base and uses it to reach conclusions about the probability of various events over the space a'. such a procedure is said to be invariant under f if f does not change the conclusions that we make; that is  if for any kb and 1  kb ¦µ iff f  kb  f 1 . roughly speaking  misrepresen-
tation independent if it is invariant under all embeddings. this captures the intuition that |~ gives us the same answers no matter how we shift representations. of course  not all embed-
1 	reasoning under uncertainty 
dings count as legitimate representation shifts. for example  consider an embedding / defined in terms of an interpretation that maps both the propositions p and q to the proposition r. then the process of changing representations using / gives us the information that p and q are equivalent  information that we might not have had originally. intuitively  / gives us new information if it tells us that certain situations-e.g.  those where p a - q holds-are not possible. formally  / is said to be faithful if f{{x}  = 1forallx € x. we show that / is faithful if and only if for any kb and 1  the assertion $ necessarily follows from kb if and only if f 1  follows from f kb . that is  faithful embeddings are precisely those that give us no new information. 
   at first glance  representation independence seems like a reasonable desideratum. however  as we show in section 1  it has some rather unfortunate consequences. in particular  we show that any representation independent inference procedure must act essentially like logical entailment for a knowledge base with only non-probabilistic information. in fact  if we also require that our inference procedure ignore blatantly irrelevant information  then it must act like logical entailment for every knowledge base. finally  representation independence is completely incompatible with even the simplest default assumption of independence: even if we are told nothing about the basic propositions p and q  representation independence does not allow us to jump to the conclusion that p and q arc independent. 
   this seems to put us in a rather awkward situation: it seems we must either give up on representation independence  or make do with an inference procedure that is essentially incapable of even minimal inductive reasoning  jumping to conclusions . but things are not quite as bleak as they seem. in practice  we would claim that the choice of language does carry a great deal of information. that information is what gives us the intuition that certain embeddings are legitimate  while others that have the same abstract structure are not. for example  suppose that certain propositions represent colors while others represent birds. while we may be willing to transform colorful to red v blue v green  we may not be willing to transform red to fly. there is no reason to demand that an inference procedure behave the same way if we suddenly shift to a wildly inappropriate representation  where the symbols mean something completely different. given a class of  appropriate  embeddings  where the notion of appropriate might be application-dependent   we may well be able to get an interesting notion of representation independence with respect to that class. 
   in section 1  we provide a general approach to constructing inference procedures that are invariant under a specific class of embeddings. we assume that the user starts with some set of initial prior probability distributions that characterize his beliefs in the absence of information. we show that if the prior distributions are chosen appropriately  so that they are invariant under the class of embeddings of interest  then we can  bootstrap  up to obtain a general inference procedure that is invariant under the same class of embeddings  by using cross-entropy  kullback and leibler  1   a well-known generalization of probabilistic conditioning. this result can be used in a number of ways. for example  it shows us how to construct an inference procedure that is invariant under a given set of embeddings: we simply choose a class of priors 


	halpernandkoller 	1 


1 


	halpern and k1ller 	1 


1 

theorem 1 allows us to define an inference procedure 
 y- t that enforces minimal default independence  for formulas in different cells   and at the same time is invariant under a large and natural class of embeddings. given our negative result in theorem 1  this is the best that we could possibly hope for. in general  theorem 1 allows us to understand the tradeoffs between inductive reasoning patterns and invariance under representation shifts. 
	1 	related work 
given the importance of representation in reasoning  particularly inductive reasoning  and the fact that one of the main criticisms of maximum entropy has been its sensitivity to representation shifts  it is surprising how little work there has been on the problem of representation dependence. indeed  
	halpernandkoller 	1 

to the best of our knowledge  the only work on representation independence in the logical sense that we have considered here is that of salmon. salmon  salmon  1  defined a criterion of linguistic invariance  which seems essentially equivalent to our notion of representation independence. he tried to use this criterion to defend one particular method of inductive inference but  as pointed out by barker in the commentary at the end of  salmon  1   his preferred method does not satisfy his criterion either. salmon then tried to find a modified inductive inference method that did satisfy his criterion  salmon  1   but it is not clear that it does; in any case  our results show that his modified method certainly cannot be representation independent in our sense. 
   although statisticians have not considered representation independence in the sense we have defined it here  bayesian statisticians have been very concerned with related issue of invariance under certain transformations of parameters. for example  we would expect that our beliefs about a person's height should be invariant under a transformation from feet to meters. their hope is that once we specify the transformation under which we want a distribution to be invariant  the distribution will be uniquely determined  jaynes  1; kass and wasserman  1 . in this case  the argument goes  the uniquely determined distribution is perforce the  right  one. this idea of picking a distribution using its invariance properties is in the same spirit as the approach we take in section 1. but unlike the standard bayesian approach  we do not feel compelled to choose a unique distribution. this enables us to explore a wider spectrum of inference procedures. 
   another line of research that is relevant to representation independence is the work on abstraction  giunchiglia and walsh  1; nayak and levy  1 . although the goal of this work is again to make connections between two different ways of representing the same situation  there are significant differences in focus. in the work on abstraction  the two ways of representing the situation are not expected to be equivalent. rather  one representation typically abstracts away irrelevant details that are present in the other. on the other hand  their treatment of the issues is in terms of deductive entailment  not in terms of general inference procedures. it would be interesting to combine these two lines of work. 
1 	conclusions 
this paper takes a first step towards understanding the issue of representation dependence in probabilistic reasoning  by defining notions of invariance and representation independence  showing that representation independence is incompatible with most types of inductive inference  and defining limited notions of invariance that might that allow a compromise between the desiderata of inductive reasoning and representation independence. our focus here has been on inference in probabilistic logic  but the notion of representation independence is just as important in many other contexts. our definitions can clearly be extended to non-probabilistic logics. it is interesting to see in what circumstances our results also carry over. are there any non-deductive logics that are representation independent  we intend to examine this question in future work. 
1 
