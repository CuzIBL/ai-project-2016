 
this paper presents a simple logical framework for abduction  with probabilities associated with hypotheses. the language is an extension to pure prolog  and it has straight-forward implementations using branch and bound search with either logic-programming technology or atms technology. the main focus of this paper is arguing for a form of representational adequacy of this very simple system for diagnostic reasoning. it is shown how it can represent model-based knowledge  with and without faults  and with and without non-intermittency assumptions. it is also shown how this representation can represent any probabilistic knowledge representable in a bayesian belief network. 
1 introduction 
determining what is in a system from observations  diagnosis and recognition  are an important part of al. there have been many logic-based proposals of what a diagnosis is  reiter  1; de kleer and williams  1; poole  1; de kleer et al.  1 . one problem with these proposals is that for any problem of a reasonable size there are far too many  logical possibilities  to handle  for a human or a computer . for example  when considering fault models  de kleer and williams  1; poole  1   there is almost always an exponential number of logical possibilities  e.g.  each component could be in its normal state or in the abnormal unknown state . for practical problems  we find that many of the logically possible diagnoses are so unlikely that it is not worth considering them. there is a problem  however  in removing the unlikely possibilities a priori: it may happen that the unlikely occurrence is the actual truth in the world. 
　such analysis of the combinatorial explosions would tend to suggest that we need to take into account probabilities of the diagnoses  de kleer and williams  1; peng and reggia  1; neufeld and poole  1   and not generate the unlikely diagnoses. similar experience has been found in natural language understanding  hobbs et a/.  1; goldman and charniak  1 . 
probabilistic models of diagnostic reasoning  pearl  
1; heckerman and horvitz  1; andreassen et al.  1   being purely propositional by nature  do not have the modelling power of the logic-based models. 
this paper points to one direction in which probabilistic diagnostic frameworks can be extended to a nonprop ositional form. 
　this paper presents a very simple form of abduction  where the background knowledge is horn  and the assumptions are atomic. associated with hypotheses are probabilities. the main features of the approach are: 
  we are trying to carry out a empirical study of auto-mated reasoning. in order to carry this out we try to determine where very simple frameworks work and fail. the best way to show that we need certain features is to try to do without them. it is in this spirit that we try to use the simplest framework that seems plausible  and only add features when they can be demonstrated to be needed. 
  we are trying to get a good compromise between representational  epistemic  adequacy and procedural  heuristic  adequacy  mccarthy and hayes  1 . 
  as a prima facie case for representational adequacy  we note that the language incorporates pure prolog as a special case  and also an atms1  reiter and de kleer  1   and the language can represent any probabilistic information that can be represented in a bayes net  pearl  1   see section 1 . we also demonstrate representational adequacy by showing how some common diagnostic representational problems can be represented in this framework. the representational adequacy can only be verified empirically  and we are currently trying to test the framework on a variety of problems. 
  it is straight forward to implemented using either logic programming  poole  1a  or atms  de kleer  1  technology. in this paper we use a specification of what is to be implemented that is independent of the actual implementation strategy used. once we have the specification of what  it is we want to compute  we can then compare different implementation strategies to determine which is more efficient in space and/or time. 
1
　　 note that we are using the assumption based framework as the object language and not as a book keeping mechanism for a problem solver. 
	poole 	1 
in all of the implementations  we do not generate the unlikely explanations unless we need to. hopefully we can cut down on the combinatorial explosions that are inherent in considering the set of all logically possible explanations  but this is beyond the scope of this paper. 
1 t h e s y s t e m 
1 	abductive framework 
the formulation of abduction used is in terms of theorist  poole et al.  1; poole  1 . 
　given a language l  and a consequence relation  written  on l  and an abductive scheme is a pair  f h  where f and h are sets of sentences in l. 
definition 1  poole et al.  1; poole  1  if g is a ground formula  an explanation of g from  f  h  is a 
is an atom representing false. the first condi-
tion says that  d is a sufficient cause for obs  and the second says that d is possible  i.e.   is consistent . 
definition 1 a minimal explanation of g is an explanation of g such that no strict subset is also an explanation. 
　associated with each minimal explanation d  is a measure   neufeld and poole  1 . this measure could be an assumption cost that is added  hobbs et a/.  1   but in this paper we investigate the use of probability as a measure over explanations. 
1 	probabilistic horn abduction 
in probabilistic horn abduction we restrict the language l to be horn clauses. 
　we use the normal prolog definition of an atomic symbol  lloyd  1 . a horn clause is of the form: 

where a and each a  are atomic symbols  false is a special atomic symbol that is not true in any interpretation1. all variables in f are assumed universally quantified. 
　we restrict the elements of h to be ground instances of atoms. if we are given a set of open atoms as possible hypotheses we mean the the set of ground instances of these atoms. 
1
　　notice that we are using horn clauses differently from how prolog uses horn clauses. in prolog  the database consists of definite clauses  and the queries provide the negative clauses  lloyd  1 . here the database consists of definite and negative clauses  and we build a constructive proof of an observation. 
1 	qualitative reasoning 
1 	probabilities 
the measure we use is the probability of the explanation. 
　associated with each possible hypothesis  i.e.  with each ground instance of an open possible hypothesis  is a prior probability. the aim is to compute the posterior probability of the explanations given the observations. abduction gives us what we want to compute the probability of and probability theory gives a measure over the explanations  neufeld and poole  1 . 
we use the declaration 
assumable h p . 
where h can contain free variables  to mean each ground instance of h is in h with prior probability p. 
　to compute the posterior probability of an explanation  given observation obs  we use bayes rule and the fact that p obs h  = 1 as the explanation logically implies the observation: 

the value  p obs  is the prior probability of the observation  and is a constant factor for all explanations. we compute the prior probability of the conjunction of the hypotheses using: 

with p true  = 1. the only other thing that we need to compute is 

the first thing to notice is that if hn is inconsistent with the other hypotheses  then its probability is zero. these are exactly the cases that are removed by the inconsistency check. similarly if hn is implied be the other hypotheses  its probability is one. this will never be the case if the explanations are minimal. while any method can be used to compute this conditional probability  the assumption of conditional independence is often an appropriate assumption in many domains  de kleer and williams  1; peng and reggia  1 . we make this assumption here and in later sections we show how to allow arbitrary probabilistic interactions  without changing the underlying system. the system uses the following assumption: 
assumption 1 logically independent instances of hypotheses are probabilistically independent. 
definition 1 a set h of hypotheses are logically independent  given f  if there is no such that 

　the assumptions in a minimal explanation are always logically independent. minimality ensures that no hypothesis in an explanation can be implied by other hypotheses in the explanation. consistency ensures the negation of a hypothesis cannot be implied by other hypotheses. 
　under assumption 1  if  are part of a minimal explanation  then 
thus  	
to compute the prior of the explanation we multiply the priors of the hypotheses. the posterior probability of the explanation is proportional to this. one problem that arises is in determining the value of p obs . 
　when using abduction we often assume that the diagnoses are covering. this can be a valid assumption if we have anticipated all eventualities  and the observations are within the domain of the expected observations  usually if this assumption is violated there are no explanations . this is also supported by recent attempts at a completion semantics for abduction  poole  1; console et al.  1; konolige  1 . the results show how abduction can be considered as deduction on the  closure  of the knowledge base that includes statements that the given causes are the only causes. the closure implies the observation are logically equivalent to the disjunct of its explanations. we make this assumption explicit here: 
assumption 1 the diagnoses are covering. 
　for the probabilistic calculation we make an additional assumption: 
assumption 1 	the diagnoses are disjoint  mutually exclusive . 
　it turns out to be straightforward to ensure that these properties hold  for observations that we can anticipate1. we make sure that the rules for each possible subgoal are disjoint and covering  see section 1 . 
　under these assumptions  if {e 1   ... en} is the set of all explanations of obs: 

1 	implementation 
the very simple definition of the framework makes implementation straight forward  although some difficult problems do arise in trying to make it very efficient . we are currently experimenting with implementations based on logic programming technology and based on atms technology  similar to  de kleer and williams  1  . both implementations keep a priority queue of sets of hypotheses that could be extended into explanations   partial explanations  . at any time the set of all the explanations is the set of already generated explanations  plus those explanations that can be generated 
　　1 like other systems  e.g.   pearl  1    we have to assume that unanticipated observations are irrelevant. 
from the partial explanations in the priority queue. it is possible to put a bound on the probability mass in the queue  and this allows us to estimate errors on the results before the computation is completed  forming an  anytime  algorithm . see  poole  1b  for details. 
　the difference between these two represents a difference between  interpreted  and  compiled  approaches  reiter and de kleer  1 . as far as the rest of the paper is concerned  it is irrelevant as to how the system is implemented. given a specification of what it is we want to compute we can now experiment with trade-offs between various implementation strategies. 
　note that the problem is np-complete  provan  1   thus we are never going to expect efficient polynomial worst-case algorithms. the best we can expect is good average-case behaviour; but this is  of course  what we are interested in. 
1 r e p r e s e n t a t i o n a l m e t h o d o l o g y 
once we have a tool  it is important to know how to use it. the problem of a representational methodology  poole  1  is an important and much overlooked part of automated reasoning research. 
　it may seem that the assumptions used in designing the system were so restrictive that the system would be useless for real problems. in this section  i argue that this is not the case. 
1 	disjoint and covering explanations 
for our probabilistic analysis  section 1   we assumed that the explanations were disjoint and covering. if we want our probabilities to be correct1  we must ensure that the explanations are disjoint and covering. 
　if the rules for an atom a are not covering  we can invent another cause for the goal representing  all the other possible causes  of the atom  de kleer and williams  1; poole  1   and add 

assumable  some  other reason for a p . 
where p is the prior probability that something else would have caused a. 
　we can locally ensure that any explanations generated are disjoint. the following proposition can be easily proved: 
proposition 1 if for any two rules with the same consequent the antecedents are inconsistent then the minimal explanation are disjoint. 
　although disjointedness of explanations places a restriction on the knowledge base  it does not place a restriction on the sorts of knowledge that we can represent. 
in general  if we have rules 

1
　　 it may be the case that they are  good enough  for any decisions that we may want to make  even though they are not accurate. 
these can be made disjoint by adding hypotheses 
h1 ...  hn to the rules 

and making sure these rules are disjoint by having  for each different i and j  the fact 
false  
　we need to associate a probability with each hypothesis such that this probability represents the probability that the particular body was  the cause  for a. 
　sometimes we can make the rules naturally disjoint  by ordering the rules and making sure that the bodies of rules are false if the bodies of previous rules are true. example 1 suppose we want to represent an  andgate  that should have value 1 if either of the inputs are zero. suppose we represent the proposition that port g has output v at time t as val g  v  t . we can ensure that the explanations are disjoint locally by ensuring that only one body can ever be true: 

　this has repercussions in biasing the most likely explanation to the first rule which is more general than the others. to make it more fair the first rule could be split into two cases depending on the value of input 1. this problem of the most likely diagnosis depending on the representation seems endemic to approaches that try to find the diagnosis  either explanation or interpretation  that is  most likely   pearl  1; poole and pro van  1 . 
1 	parametrizing hypotheses 
the next important part of the methodology for abduction concerns parametrizing possible hypotheses and the interaction with the independence assumption. i have argued elsewhere  poole  1; poole  1  that there is much power obtainable and subtlety involved in parametrizing hypotheses appropriately.in this section we expand on previous analysis  poole  1   and show how probabilities affect parametrization considerations by considering some case studies on different proposals. 
1.1 	hypotheses w i t h indeterminate output 
　as an example  suppose we have a gate g that takes two values as input  and outputs a value that can be in the range 1 to n. suppose we want to represent the gate being in an unknown state  this is applicable whether or 
1 	qualitative reasoning 
not we have fault models  de kleer and williams  1; poole  1  . suppose we represent the proposition that gate g has output v at time t as val g  v  t . 
　we cannot representing the hypothesis that the gate is in the unknown state by using the hypothesis u g  and the fact 

the problem is that the above fact states that a gate in the unknown state produces all values of output  rather than saying that it produces some output. knowing a gate is in an unknown state does not imply any value for the output. 
　when there are no probabilities involved  poole  1; poole  1  we parametrize the hypothesis by the values on which it depends. this could be done by having the hypothesis produces g  v  t  and the rule 

we would say that a port has only one value at a time by having the constraint 

　suppose we know that gate g  has probability e of being in the unknown state. if we assume that each possible output value has equal chance  and that there are n possible output values  then the prior probability that it produces output value v is  
　when we have more than one observation  there is another problem. for the probabilities we assumed that the hypotheses were independent. we would not expect that 

once we know that the gate is in an unknown state at time t1 it should not be so unlikely that it is in an unknown state at time t1. put another way  once we have paid the price once for assuming that the gate is in an unknown state at time t  we should not pay the price again for assuming that it is in an unknown state at time t1. 
　to work in general  we need a mixture of the above two ideas. suppose a gate g has probability of c of being in the unknown state  and that there are n possible output values  each of which has an equal prior chance of being produced by a gate in the unknown state. this can be represented as the hypotheses 

and the rule 

u g  means g is in the unknown state  and produces g  v  t  means that given gate g is broken  it produces value v at time t. we assume once that the gate is broken  and then make other assumptions of what values it is producing at different times. 
　it is interesting to note that this analysis of dividing by n can be done when building the knowledge base and does not need to be carried out dynamically  as  de kleer and williams  1  seem to need to do  . this means that distributions other than the uniform distribution can be given if appropriate. 
1.1 	intermittent versus non-intermittent faults 
　because of the way we parametrized the hypotheses  the above representation of faults says that the output is only a function of the time. the hypothesis prod g  v  t  and the above rules places no constraints on the values of the outputs at different times. this is a way to represent the fact that the gate can have an intermittent fault  it depends only on the time of observation . there is no constraint that says the gate produces the same output when given the same inputs at different times. 
　we can give the non-intermittency assumption by saying that the fault only depends on the input and not on the time. this can be done instead by having the hypothesis   meaning gate g produces 
output v when given i  and i1 as input  and a rule 

　with the same integrity constraint as before  it is inconsistent to assume that the gate has different outputs for the same input. 
1 	causation events 
when using abduction we run into the problem of a cause not actually implying a symptom. for example  having a cold does not imply sneezing  but could cause sneezing. to implement this idea we introduce another hypothesis that the cold caused the sneezing. this idea is analogous to the notion of a  causation event  of peng and reggia . 
　to implement the causation events  we can use the relations has jdisease d  to mean that the patient has disease d; actually jcauses d m  to mean that disease d  actually caused  manifestation m; and has manifestation m  to mean that the patient has manifestation m. 
　we can say that a manifestation is caused by the disease that actually causes it by: has nanifestation m   
cause of a manifestation by: 
false  
　this rule ensures that the explanations for having a manifestation are disjoint. 
the conjunction 

corresponds to peng and reggia  l1 's causation event m : d. the completion semantics of abduction  poole  1; console et al.  1; konolige  1  show that  under the covering explanation assumption  we implicitly have the relationship 

we have the possible hypothesis 

where pij is the  conditional causal probability    causal strength   of  peng and reggia  1 . it can be seen as the fraction of the cases of d - being true that mj is actually caused by di. 
we also have the possible hypotheses 

where pi is the prior probability of the disease di. 
1 	r e p r e s e n t i n g b a y e s i a n n e t w o r k s 
in this section we give the relationship between bayesian networks and our probabilistic abduction. the analysis here is  in some sense  the dual of the analysis given by charniak and shimony . we show how any probabilistic knowledge that can be represented in a bayesian network  can be represented in our formalism. 
suppose we have a bayes net with random variables such that random variable a  can have values 
 we will represent random variable a  having value vij as the proposition  
　the first thing we need to do is to state that the values of variables are mutually exclusive. for each t and for each j  k such that we have the rule 
　a bayes net  pearl  1  is a directed acyclic network where the nodes represent random variables  and the arcs represent a directly influencing relation. terminal nodes of a bayes net are those variables that do not influence any other variables. a composite belief  pearl  1  is an assignment of a value to every random variable. 
　suppose variable a is directly influenced by variables b1 ...  bm in a bayes network. this can represented in our system by the rule: 
is that a has value v because b1 has value v1 ...  and bm has value vm. 
associated with the bayes net is a contingency table 
 pearl  1  which gives the marginal probabilities of the values of a depending on the values of b1  ... b m . this will consist of probabilities of the form 

this is translated into the assertion 

　the following propositions can be proved  poole  1b : 
lemma 1 the minimal explanations of the terminal variables having particular values correspond to the composite beliefs in the bayes net with the terminals having those values. the priors for the explanations and the composite beliefs are identical. 
　as the same procedure can be used to get from the priors of composite hypotheses and explanations to the posteriors given some observations  the following theorem is a direct corollary of lemma 1. theorem 1 if the observed variables include all terminal variables  the composite beliefs with the observed variables having particular values correspond exactly to the explanations of the observations  and with the same posterior probability. 
   if the observed variables do not include all terminal values  we need to decide what it is that we want the probability of  poole and provan  1 . if we want to commit to the value of all variables  as in the composite belief of pearl   then we consider the set of possible observations that include assigning values to terminal nodes. that is  if o was our observation that did not not include observing a value for variables a   then we need to consider the observations 
to find the accurate probabilities we need to normalise over the sum of all of the explanations. whether or not we want to do this is debatable. 
　it is not only the probability of a composite hypothesis that has a characterisation in terms of explanations. 
　let expl a  be the set of minimal explanations of proposition a. define 

lemma 1 if if is a set of assignments to variables in a bayesian network  and h' is the analogous propositions to h in the corresponding probabilistic horn abduction system  then  

　a simple corollary of the above lemma can be used to determine the posterior probability of a hypothesis based on some observations: 
theorem 1 

　the denominator can be obtained by finding the explanations of the observations  or can be approximated by finding some of the explanations that cover some proposition of the probability mass . the numerators can be obtained by explaining  from these explanations  see  poole  1a  . 
　what is important about the comparison with the bayes net is that any probability distribution that can be represented as a bayes net can be represented using the probabilistic horn abduction. the opposite is not the case  however  because our horn abduction is not restricted to a propositional language. 1 qualitative reasoning 
1 c o m p a r i s o n w i t h o t h e r d i a g n o s t i c s y s t e m s 
the closest work to that presented here  namely the work of de kleer and williams  1; 1  and peng and reggia   both incorporate probabilistic knowledge to find the most likely diagnoses. 
1 de kleer and williams de kleer and williams  1; 1  have explored the idea of using probabilistic information in consistencybased diagnosis  see  poole  1; poole  1; console et a/.  1; konolige  1  for comparisons between 
abductive and consistency-based diagnoses . 
　they differ from us in what they compute the probability of. de kleer and williams are finding the most likely interpretations  this is the same as the diagnoses of peng and reggia  and the composite beliefs of pearl   but is different to the kernel or minimal diagnoses of de kleer  mackworth and reiter  . we are computing the most likely explanations; we want to remain agnostic about the value of the irrelevant hypotheses  de kleer and williams cannot distinguish between the remaining diagnoses that differ in substantial ways from the most likely interpretation  and those that differ only in varying values that are irrelevant to the diagnosis. in our system  hypotheses that are not part of an explanation are ignored  and play no part in the probability of a diagnosis. 
　we differ in the use of the assumption-based framework. we are using the assumption-based reasoning  with variables  as the object language. they use the atms as a book keeping mechanism for their diagnostic engine. 
1 peng and reggia 
peng and reggia  also consider an abductive definition of diagnosis and incorporate probabilities  and best-first search. like  de kleer and williams  1; pearl  1  they are trying to find probabilities of interpretations. we also do not assume that the set of manifestations is complete. the main difference  however  is in the underlying language. they use the notion of  hyper-bipartite  graphs made up of causation relations on sets of manifestations  can be observed   disorders  can be hypothesised   and pathological states. we  however  allow the full power of horn clauses. we can represent the probabilistic knowledge of peng and reggia  see section 1 . 
1 c o n c l u s i o n 
this paper presented a simple but powerful mechanism for diagnostic reasoning and showed how it can be used to solve diagnostic representation problems. one main advantage of the simple specification of what we want to compute is that we can investigate different implementation techniques to determine which works best in practice. 
　one question that needs to be asked is whether a set of most likely explanations is really what we want to compute  poole and provan  1 . we conjecture that for real problems  the probability mass of the most likely explanations will be so close to one to make the question moot. by ignoring the large number of very unlikely explanations  we will not make many mistakes. whether this is true in practice remains to be seen. 
　we are also investigating the use of the abductive framework for differential diagnoses  and for making decisions  but that is beyond the scope of this paper. 
a c k n o w l e d g e m e n t s 
thanks to alan mack worth and mike horse h for valuable comments on a previous version of this paper  and mike for work on the implementation. this research was supported under nserc grant ogp1  and under project b1 of the institute for robotics and intelligent systems. 
r e f e r e n c e s 
 andreassen et al.  1  s. and reassen  m. wold bye  b. falck  and s. k. andersen. munin - a causal probabilistic network for interpretation of electromyographic findings. in proc. 1th international joint conf. on artificial intelligence  pages 1  milan  italy  august 1. 
 charniak and shimony  1  e. charniak and s. e. shimony. probabilistic semantics for cost based abduction. in proc. 1th national conference on artifictal intelligence  pages 1  boston  july 1. 
 console et al.  1  l. console  d. theseider dupre  and p. torasso. abductive reasoning through direct deduction from completed domain models. in w. r. zbigniew  editor  methodologies for intelligent systems 1  pages 1. elsiever science publishing co.  1. 
 de kleer and williams  1  j. de kleer and b. c williams. diagnosing multiple faults. artificial intelligence  1 :1  april 1. 
 de kleer and williams  1  j. de kleer and b. c. williams. diagnosis with behavioral modes. in proc. 1th international joint conf on artificial intelligence  pages 1  detroit  august 1. 
 de kleer et al.  1  j. de kleer  a. k. mackworth  and r. reiter. characterizing diagnoses. in proc. 1th national conference on artificial intelligence  pages 1  boston  july 1. 
 de kleer  1  j. de kleer. an assumption-based tms. artificial intelligence  1 :1~ 1  march 1. 
 goldman and charniak  1  r. p. goldman and e. charniak. a probabilistic atms for plan recignition. in proceedings of the plan recognition workshop  1  august 1. 
 heckerman and horvitz  1  d. e. heckerman and e. j. horvitz. problem formulation as the reduction of a decision model. in proc. sixth conf. on uncertainty in artificial intelligence  pages 1  cambridge  mass.  july 1. 
 hobbs et al.  1  j. r. hobbs  m. e. stickel  p. martin  and d. edwards. interpretation as abduction. in proc. 
1th annual meeting of the association for computational linguistics  pages 1  buffalo  june 1. 
 konolige  1  k. konolige. closure + minimization implies abduction  technical report     sri international  menlo park  ca  1. 
 lloyd  1  j. w. lloyd. foundations of logic programming. symbolic computation series. springer-verlap berlin  second edition  1. 
 mccarthy and hayes  1  j. mccarthy and p. j. hayes. some philosophical problems from the standpoint of artificial intelligence. in m. meltzer and d. michie  editors  machine intelligence 1  pages 1. edinburgh university press  1. 
 neufeld and poole  1  e. m. neufeld and d. poole. towards solving the multiple extension problem: combining defaults and probabilities. in proc. third workshop on reasoning with uncertainty  pages 1  seattle  july 1. 
 pearl  1  j. pearl. distributed revision of composite beliefs. artificial intelligence  1 :  1  october 1. 
 pearl  1  j. pearl. probabilistic reasoning in intelligent systems: networks of plausible inference. morgan kaufmann  san mateo  ca  1. 
 peng and reggia  1  y. peng and j. a. reggia. abductive inference models for diagnostic problem-solving. 
symbolic computation - al series. springer-verlag  new york  1. 
 poole and provan  1  d. poole and g. provan. what is an optimal diagnosis  in proc. sixth conference on uncertainty in ai  pages 1  boston  july 1. 
 poole et al.  1  d. poole  r. goebel  and r. aleliunas. theorist: a logical reasoning system for defaults and diagnosis. in n. cercone and g. mccalla  editors  the knowledge frontier: essays in the representation of knowledge  pages 1. springer-verlag  new york  ny  1. 
 poole  1  d. poole. a logical framework for default reasoning. artificial intelligence  1 l :1  1. 
 poole  1  d. poole. representing knowledge for logicbased diagnosis. in international conference on fifth generation computing systems  pages 1  tokyo  japan  november 1. 
 poole  1  d. poole. normality and faults in logic-based diagnosis. in proc. 1th international joint conf. on artificial intelligence  pages 1  detroit  august 1. 
 poole  1  d. poole. a methodology for using a default and abductive reasoning system. international journal of intelligent systems  1 :1  december 1. 
 poole  1a  d. poole. probabilistic h orn abduction: comparing two implementation techniques. technical report  department of computer science  university of british columbia  vancouver  b.c.  canada  1. 
 poole  1b  d. poole. probabilistic h orn abduction: theory and representational adequacy. technical report  department of computer science  unoversity of british columbia  vancouver  b.c.  canada  1. 
 provan  1  g. provan. a complexity analysis for assumption-based truth maintenance systems. in b. m. smith and r. kelleher  editors  reason maintenance systems and their applications  pages 1. ellis howard  1. 
 reiter and de kleer  1  r. reiter and j. de kleer. foundations of assumption-based truth maintenance systems: preliminary report. in proc. 1th national conference on artificial intelligence  pages 1  seattle  july 1. 
 reiter  1  r. reiter. a theory of diagnosis from first principles. artificial intelligence  1 l :1  april 1. 


	poole 	1 



