 
a program is bounded optimal for a given computational device for a given environment  if the expected utility of the program running on the device in the environment is at least as high as that of all other programs for the device. bounded optimality differs from the decision-theoretic notion of rationality in that it explicitly allows for the finite computational resources of real agents. it is thus a central issue in the foundations of artificial intelligence. in this paper we consider a restricted class of agent architectures  in which a program consists of a sequence of decision procedures generated by a learning program or given a prion. for this class of agents  we give an efficient construction algorithm that generates a bounded optimal program for any episodic environment  given a set of training examples. the algorithm includes solutions to a new class of optimization problems  namely scheduling computational processes for real-time environments. this class appears to contain significant practical applications. 
1 	introduction 
since before the beginning of artificial intelligence  philosophers and economists have looked for a satisfactory definition of rational behaviour. this is needed to underpin theories of ethics  inductive learning  reasoning  decision-making and economic modelling. doyle  has proposed that ai itself be defined as the computational study of rational behaviour. the decisiontheoretic definition of rational behaviour as maximization of expected utility  and analogous definitions in the context of logical planning  have been extremely valuable in allowing ai research to be done at an abstract level  independent of specific implementations. several systems can already be said to satisfy the required inputoutput relations. 
　unfortunately  these approaches are of limited value because they ignore the total impracticality of reaching optimal decisions in realistic situations. in this paper  we propose instead the concept of bounded optimality 
distributed ai 
in which the utility of a decision is a function of both its quality and the time taken to choose it. we give a formal definition below; informally  we say that an agent exhibits bounded optimality if its program is a solution to the constrained optimization problem presented by its architecture. 
　we begin in section 1 with a necessarily brief discussion of the relationship between bounded optimality and earlier notions of rationality. we note in particular that some important distinctions can be missed without precise definitions of terms. thus in section 1 we introduce formal definitions of agents1  their programs  their behaviour and their rationality. section 1 exam ines a class of agent architectures for which the problem of generating bounded optimal configurations is efficiently soluble. the solution involves a new class of interesting and practically-relevant optimization problems. we examine several types of episodic  real-time environments and derive bounded optimal programs for these regimes. finally  we describe a set of open theoretical and experimental issues  including an asymptotic version of bounded optimality that may be more robust and tractable than the strict version. 
1 	historical perspective 
the classical idea of perfect rationality  which developed from mill's utilitarianism  was put on a formal footing in von neumann's decision theory . it stipulates that a rational agent always act so as to maximize its expected utility. the expectation is taken according to the agent's own beliefs; thus  perfect rationality does not require omniscience. 
　in artificial intelligence  the logical definition of rationality  known in philosophy as the  practical syllogism   was put forward by mccarthy   and reiterated strongly by newell . under this definition  an agent should take any action that it believes is guaranteed to achieve any of its goals. if ai can be said to have had a theoretical foundation  then this definition of rationality has provided it.1 
　　1  we use the term ralph  rational agent with limited performance hardware  to denote an agent that exhibits bounded optimality. 1 this is not to say that this was the wrong approach at the time. mccarthy believed  probably correctly  in the halcyon 
   economists have used rationality as an abstract model of economic entities  for the purposes of economic forecasting and designing market mechanisms. unfortunately  as simon  pointed out  real economic entities have limited time and limited powers of deliberation. he proposed the study of bounded rationality  investigating  ... the shape of a system in which effectiveness in computation is one of the most important weapons of survival.  simon's work focussed mainly on satisficing designs  which deliberate until reaching some 
solution satisfying a preset  aspiration level.  the results have descriptive value for modelling various actual entities and policies  but no prescriptive framework for bounded rationality was developed. 
   i. j. good  emphasized the conceptual distinction between classical or  type i  rationality  and what he called  type i i   rationality  or the maximization of expected utility taking into account deliberation costs.1 what this means is that an agent exhibits type ii rationality if at the end of its deliberation and subsequent action  its subjective utility is maximized compared to all possible deliberate/act pairs in which it could have engaged. good does not define the space of possible deliberations  but from his informal descriptions  it is clear that type ii rationality is intended to prescribe optimal sequences of computational steps. unfortunately  these may be even harder to select than actions themselves. 
   recognizing these problems  cherniak  suggested a definition of  minimal rationality   specifying lower bounds on the reasoning powers of any rational agent  instead of upper bounds. a philosophical proposal generally consistent with the notion of bounded optimality can be found in dennett's  moral first aid manual  . 
   many researchers in a i   some of whose work is discussed below  have worked on the problem of designing agents with limited computational resources; the 1 a a a i symposium on ai and limited rationality  contains an interesting variety of work on the topic. 
   metareasoning - reasoning about reasoning - is an important technique in this area  since it enables an agent to control its deliberations according to their costs and benefits. combined with the idea of anytime  or 
flexible algorithms   l l     that return better results as time goes by  a simple form of metareasoning allows an agent to behave well in a real-time environment. breese and fehling  apply similar ideas to controlling multiple decision procedures. russell and wefald  give a general method for precompiling certain aspects of metareasoning so that a system can efficiently estimate the effects of individual computations on its intentions  giving finegrained control of reasoning. these techniques can all be seen as approximating type ii rationality; they provide useful insights into the general problem of control of reasoning  but there is no reason to suppose that the 
days before formal intractability results in computation were known  that in the early stages of the field it was important to concentrate on  epistemological adequacy  before  heuristic adequacy . 
1
　　simon  also says:  the global optimization problem is to find the least-cost or best-return decision  net of computational costs.  
approximations used are optimal in any sense. 
   horvitz  uses the term bounded optimality to refer to  the optimization of computational utility given a set of assumptions about expected problems and constraints in reasoning resources.  russell and wefald  say that an agent exhibits bounded optimality  if its program is a solution to the constrained optimization problem presented by its architecture.  the philosophical  move   from optimizing over actions or deliberation sequences to optimization over programs  is the key to our proposal.1 in reality  designers of intelligent agents do not have direct control over the agent's actions or deliberations; these are generated by the operation of the agent's program. prescriptive specification of actions  type i  or deliberations  type ii  may produce impossible constraints  if these specifications are not realized by any program for the agent. precise definition of the space of agent programs is therefore an important part of the study of bounded optimality. 1 in the next section  we build a suitable set of definitions from the ground up  so that we can begin to demonstrate examples of provably bounded optimal agents. 
1 	agents  architectures and programs 
intuitively  an agent is just a physical entity that we wish to view in terms of its perceptions and actions. what counts in the first instance is what it does  not necessarily what it thinks  or even whether it thinks at all. this initial refusal to consider further constraints on the internal workings of the agent  such as that it should reason logically  for example  helps in three ways: first  it allows us to view such 'cognitive faculties' as planning and reasoning as occurring in the service of finding the right thing to do; second  it makes room for those among us  1; 1  who take the position that systems can do the right thing without such cognitive faculties; third  it allows more freedom to consider various specifications  boundaries and interconnections of subsystems. 
　an agent can be described abstractly as a mapping  the agent function  from percept sequences to actions; this mapping is implemented by an agent program. the design and evaluation of agents are based on the behaviour of the agent program in an environment. 
   let p be the set of percepts that the agent can receive at any instant  and a be the set of possible actions the agent can carry out in the external world. then we have d e f i n i t i o n 1 agent function: a mapping 

where p* is the set of all possible percept sequences. the agent function is an entirely abstract entity  unlike the agent program. computability theory relates these abstract functions to their finite representations as pro-
grams running on a machine. 
　　this move is analogous to the development of 'rule utilitarianism ' from 'act utilitarianism*. 
1
　　 recent work by etzioni  and russell and zilberatein  can be seen as optimizing over a well-defined set of agent designs. 
1 

　we will consider a physical agent as consisting of an architecture and a program. the architecture is responsible for interfacing between the program and the discrete  deterministic environment  and for running the program itself. with each architecture m  we associate a finite programming language  which is just the set of all programs runnable by the architecture. an agent program is a program  when a sequence of per-
cepts is provided by the architecture to the program  a sequence of actions is generated: the  action occurs between percept i and percept i+ 1. thus an architecture maps a percept sequence of length k  for any k  to an action sequence of the same length according to the program it is running. the behaviour of an agent is the sequence of actions it generates. 
definition 1 architecture: a fixed interpreter m for the agent 's program: 

　for the purposes of this paper  we will define the environment as a set of world states together with mappings defining the effects of actions and the generation of percepts: 
definition 1 environment: a set of world states w and mappings 

　it is important to note that although every program induces a mapping in the above sense  the action following a given percept is not necessarily the agent's  response  to that percept; because of the delay incurred by deliberation  it may only reflect percepts occurring much earlier in the sequence  and it may not be possible to associate each action with a particular prior percept sequence. 
1 	bounds on rationality 
the expected utility of an action ai that has possible outcomes  for an agent with prior evidence e about the environment  is given by 

where u is a real-valued utility function on states. 
definition 1 perfect rationality: an agent is perfectly rational iff it selects action  so as to maximize its expected utility. 
　this definition is a persuasive specification of the agent function / and underlies several current projects in intelligent agent design. a direct implementation of this specification  which ignores the delay incurred by deliberation  does not yield a reasonable solution to our problem the calculation of expected utilities takes time for any real agent. 
　by neglecting the fact of limited resources for computation  classical decision theory fails to provide an adequate theoretical basis for artificial intelligence. the 'finitary predicament'  arises because real agents have 
distributed al 
only finite computational power and because they don't have all the time in the world. in terms of our simple formal description of agents introduced above  it is easy to see where the difficulty has arisen. in designing the agent program  logicists and decision theorists have concentrated on specifying an optimal agent function fopt in order to guarantee the selection of the best possible action a in each situation. the function fopt is independent of the architecture m. unfortunately  the behaviour of any program that implements this function may not be desirable. the delay in computing means that the kth action may be optimal only with respect to some much earlier subsequence  and is now totally inappropriate. 
1 	bounded optimality 
to escape this quandary  we propose a machinedependent standard of rationality  in which optimality constraints are imposed on programs rather than agent 
functions  deliberations or behaviours. to formalize this idea  it will be helpful to assume a real-valued utility function u on histories  that is  sequences of world states. then we assign values to a program / based on the sequence of states through which it  drives  the environment e when run on m starting in the world wo: 

where result denotes the state sequence generated by the execution of the program /  defined in the obvious way using ta and tp. 
　if the initial state wo and environment e are known  then the optimal agent program is given by: 
definition 1 optimal agent program: 

if instead the  designer  has only a probability distribution over the initial state and environment model  then this will introduce a distribution over the state sequences generated  and the optimal program has the highest expected value v l  m  e  over this distribution. 
1 	provably ralphs 
in order to construct a provably rational agent with limited performance hardware  ralph   we must carry out the following steps: 
  specify a class of machines on which programs are to be run. 
  specify the properties of the environment in which actions will be taken  and the utility function on the behaviours. 
  propose a construction method. 
  prove that the construction method succeeds in building ralphs. 

1 	production system architectures 
we begin our study with a simple form of production system  in which condition-action rules of the form  if  are applicable whenever their left-hand sides become true. even such a simple system can easily overtax the resources of a real-time agent  if the rule base becomes large and the conditions become complex. an agent implemented as a production system will therefore contain approximate rules  for which the action a  is not guaranteed to be the best possible whenever condition  obtains in the world. 
　an illustrative example is provided by the image processing algorithms in an automated mail sorter. these machines scan handwritten or printed addresses on mail pieces and dispatch them accordingly. the scanned image is processed by any or all of several procedures designed to read with varying degrees of accuracy and re source expenditure; each procedure may have many possible variants - for example  we can vary the number of hidden nodes in a neural network recognizer. to prevent jams  the mail piece must be sorted appropriately  or rejected  in time for the  stochastic  arrival of the next piece. the object is to maximize the accuracy of sorting while minimizing the reject percentage and avoiding 
jams. 
　here we consider only a special case of production system  in which complete rules  or decision rules  are matched in a fixed sequence. we call a sequence of production rules a strategy. each rule i has associated with it a match time t  and a quality  which corresponds to the utility of the rule's recommended action if taken at the beginning of the episode. 
　we assume that each rule is drawn from some rule language   in keeping with the scheduling literature  
we will often use the term  job  to refer to a rule in a sequence.  let denote a production system architecture that can accommodate rules of maximum size n from rule language  for example  we might consider feedforward neural networks with at most n nodes. 
in executing a typical sequence  the 
agent matches each rule in turn against the current percept  generating a recommended action. at some time t after the beginning of the episode  it will decide to act  selecting the highest-quality action recommended by any of the rules it has previously matched. if the quality of the rule chosen is qi  the value of the episode for the agent will be some function of qi  and t  according to the regimes described below. before the episode begins  the agent may not know when it will act  for example  in the stochastic deadline case   so generally speaking the value of a given strategy will be an expectation. 
1 	episodic real-time environments 
first  we place some restrictions on the environment in order to simplify our problem. we need c to learn utility information  so in order to avoid  temporarily  the credit assignment problem we assume an  episodic  en-
vironment  where after each non-null action by the agent  a reward is received and the environment then reaches an unknown state drawn from a probability distribution that remains stationary over time. the initial state wo is also drawn from this distribution. in the mail-sorting example  the reward is 1 for a correct dispatch; 1 for an incorrect dispatch; 1  say  for a reject; and  for a 
jam. 
　in order to talk about deadlines and time cost  it is useful to have a notion of inaction. let noopt denote a sequence of null actions lasting t time steps. also   a | w  will denote the sequence of world states through which the action sequence a drives the environment given an initial state w. we use ' ' to denote concatenation of sequences. 
　now we can define three typical real-time regimes: fixed time cost  fixed deadline and stochastic deadline. 
definition 1 fixed time cost: for any action a  any state w and any state sequences some constant c. 
definition 1 fixed deadline: for any action a  any 

deadlines are thus represented by a utility  cliff occurring at some time  after the beginning of an episode. in the case of a stochastic deadline  which describes the mail sorter  the location of the utility cliff is not known exactly. we assume that  for the designer  the value of  noop1 | w  is a random variable  distributed in such a way that an action following it has probability p t  of being of zero utility for the episode. this corresponds to p being the cumulative distribution function for the deadline arrival time.1 
1 	the construction algorithm 
our general scheme at present is to exhibit an algorithm that is capable of learning sets of individual decision procedures  and arranging them in a sequence such that the performance of an agent using the sequence is at least as good as that of any other such agent. that is  the construction algorithm c  operating in e  returns a program / for architecture m such that  c will work by first observing a set e of training episodes in e  and then building an approximately optimal strategy using a learning algorithm for learning rules of size k in the rule 
　in addition to constructing the decision rules  lj k outputs estimates of the rule quality g . for now  we will assume that the rule match times ti and the deadline distribution p  for the stochastic case  are known. algorithm optimize extracts an optimal strategy from 
　　1  we assume that some percept immediately precedes the actual deadline  allowing the agent to respond at the next step. without this  the agent is walking blindfolded towards the utility cliff  also an interesting problem . 
1 

the rule set r. in the following subsections we describe the three variants of optimize. 
1 	results for fixed time cost 
this case is straightforward. the value of a strategy is given by 
		 i  
theorem 1 the optimal strategy for fixed time cost utility functions is the singleton 
1 	results for fixed deadline 
this case is also straightforward. the value of a strategy s is given by: 
		 1  
theorem 1 the optimal strategy is the singleton sequence i  where qi is the highest among all rules for which 

1 	results for stochastic deadlines 
with a stochastic deadline distributed according to p t   the value of a strategy s = s 1  . ..  sm is an expectation. it is calculated as a summation  over the jobs that can be interrupted  of the probability of interruption times the quality of the best completed job: 
		 1  
where mi = max q 1  ...  qsi   and for convenience we define tsm+1 = oo. we will use this formula to prove a number of properties of optimal strategies. 
　a simple example serves to illustrate the value function. consider r = {r1 r1 r1}}. the rule r1 has a quality of 1 and needs 1 seconds to run: we will represent this by r1 =  1 1 . the other rules are r1 =  1  1   r1 =  1 1 . the deadline density function  is a uniform distribution over 1 to 1 
seconds. the value of the sequence r1r1 is 

　a geometric intuition is given by the notion of a performance profile shown in figure 1. for a uniform deadline density function  the value of a sequence is proportional to the area under the performance profile up to the last possible interrupt time. note that the height of the profile during the interval of length ti while job i is running is the quality of the best of the previous jobs. 
　the following lemma gives an extremely useful property of optimal sequences: 
lemma 1 there exists an optimal sequence that is sorted in increasing order of qi. 
henceforth we need consider only q-ordered strategies. 
this means that m  in equation  1  can be replaced by 
　the following lemma establishes that a strategy can always be improved by the addition of a better job at the end: 
distributed al 

figure 1: performance profile for r1 r1  with p superimposed. 
lemma 1 for every sequence s = s 1  ...  sm sorted in increasing order of quality  and single step z with 

corollary 1 there exists an optimal strategy ending with the highest-quality job in r. 
proofs of all the above results appear in . 
1.1 	a dynamic programming algorithm 
　the dynamic programming method can be used to obtain an optimal sequence of decision rules in pseudopolynomial time. we assume that the time ti associated with each rule is an integer  this is almost without loss of generality  using standard rounding and scaling methods . in keeping with the lemma above  we assume that optimal strategies are ordered by qi . 
　the algorithm constructs the table s i  t   where each entry in the table is the highest value of any sequence that ends with rule i at time t. we assume the rule indices are arranged in increasing order of quality  and 
t ranges from the start time 1 to the end time l = . the update rule is: 
from corollary 1  we can read off the best sequence from the highest value in row n of the matrix m. 
theorem 1 the dp algorithm computes the optimal 
sequence m time 1 n 1 l  where n is the number of deci-sion procedures in r
uniform distributions if the deadline is uniformly distributed over some initial interval  we can obtain an optimal sequence in strongly polynomial time. initially  we assume an interval longer than any possible sequence. then the probability that the deadline arrives during job si of sequence s is just  hence we have a simple recursive specification of the value v as  of a sequence beginning with job a: 
		 1  
a dynamic programming algorithm can then use the state function s f  i   which is the highest value of any rule sequence ending in / and beginning with rule i. from lemma 1 and equation 1  the update rule for s f  i  
is 
for any 
given 

in time 1 n 1    from which we can obtain the optimal sequence. 
　if all the rules can fit before the end of the deadline distribution  then from corollary 1 above the last rule / must be the rule with highest quality. otherwise  any rule might be the last rule with a non-zero chance of completing  so we check each candidate: 
theorem 1 the optimal sequence of decision procedures for a uniformly distributed stochastic deadline can be determined in 1 n 1   time where n is the number of decision procedures in r. 
1 	agnostic learning of decision rules 
our learning algorithm  searches for the best rule in  in order to work in any environment  it must be agnostic  in that it makes no assumptions about the target function  that is  the form of the correct decision rule. it searches for the best subject to a complexity parameter k that bounds the size of the rules. kearns  schapire and sellie have shown  theorems 1 and 
1 in   that  for some languages   the error in the learned approximation can be bounded to within e of the best rule in that fits the examples  with probability 
     the sample size needed to guarantee these bounds is polynomial in the complexity parameter k  as well as and once an approximate rule with a certain com-
plexity is learned  we statistically estimate its quality q. standard chernoff-hoeffding bounds can be used to limit the error in the quality estimate to be within eq with probability the sample size needed is also polynomial in 
　thus the error in the agnostically learned rules is bounded to within of the best rule in its complexity class with probability the error in the quality es timation of these rules is bounded by with probability 
. we can show that the policy selection methods for the three real-time regimes will incur a deficit of at most  in the choice of the optimal sequence of rules. 
1 	the bottom line 
we can now state the bounded optimality theorem for our construction procedure. 
theorem 1 assume a production system architecture  operating in any episodic environment. for each 
of the three real-time regimes defined above  with probability greater than the construction procedure c returns a program i such that after seeing a number of episodes that is polynomial in 
	furthermore  the computation time 
of c is  at worst  polynomial in the above quantities and in l  the sum of the rule execution times. 
　although the theorem has practical applications  it is mainly intended as an illustration of the derivation of a bounded optimal agent. with some additional work  more general error bounds can be derived for the case in which the rule execution times ti and the real-time utility variation  time cost  fixed deadline or deadline distribution  are all estimated from the training episodes. 
1 	further work 
we plan to extend this work in several directions  as follows. 
1. foundational issues: 
learning agents: when the agent  whose design is to be optimized  includes a learning component  the notion of bounded optimality becomes even more interesting because we must take into account how the agent's configuration will evolve over time  reflecting its own expected obsolescence. 
asymptotic bounded optimality: the strict notion of bounded optimality may be a useful philosophical landmark from which to explore artificial intelligence  but it may be too strong to allow many interesting  general results to be obtained. just as in complexity theory  where absolute efficiency is the aim but asymptotic efficiency is the game  so to speak   in studying bounded optimality an asymptotic version might help. first  we need a class of environments  e  which is unbounded in a complexity measure on environments n e . then we will say that an agent program / is timewise asymptotically bounded optimal iff 

where km denotes a version of the machine m speeded up by a factor k. in english  this means that the program is basically along the right lines if it just needs a faster machine to be as good as the best program on all problems above a certain level of difficulty. 
　asymptotic bounded optimality generalizes the standard definition of asymptotic complexity. let e be a class of environments in which a problem is input to the machine at time t = 1  where n e  is the input size measure. let v return  or any decreasing function of t  for a program that outputs the correct solution at time t  and 1 for all other programs. then a program is bounded optimal iff it has asymptotic complexity equal to the tight lower bound for the class e. note that in standard complexity  we allow any constant factor c in the execution time of the program  whereas our definition uses a constant factor in the speed of the machine. in the standard setting these are equivalent  but with more general time-dependent utilities only the latter is appropriate. 
1. scheduling issues: 
the existence of polynomial-time algorithms and approximation schemes for variants of the computation scheduling problem; scheduling algorithms for situations in which the solution qualities of individual processes are interdependent  such as when one can use the results of another ; scheduling combinations of computational and physical  e.g.  job-shop and flow-shop  processes  where objective functions are a combination of summation and maximization; and computation scheduling for parallel machines or multiple agents. 
1. learning issues: 
relaxing the stationarity requirement on the environment - this entails generalizing the pac model to handle the case in which the fact that the agent learns may have some effect on the distribution of future 
1 
episodes; relaxing the episodic requirement to allow nonimmediate rewards - this entails addressing the credit assignment problem; examining variants of the agnostic learning model to find the most practically useful theoretical scenario. 1- applications: 
despite the deliberate simplicity of the architecture  our construction algorithm can be applied directly to the problems such as scheduling image-processing algorithms for a mail sorter. scheduling of mixed computational and physical processes  mentioned above  broadens the scope of applications considerably. an industrial process  such as designing and manufacturing a car  consists of both computational steps  design  logistics  factory scheduling  inspection etc.  and physical processes  stamping  assembling  painting etc. . one can easily imagine many other applications in real-time financial  industrial and military contexts. 
1 	conclusions 
in short  we are proposing a new line of inquiry into bounded optimal agents in which the value of a decision is judged in terms of the effect it has on the actions performed by the agent  noting that both actions and computations have time value. bounded optimality may provide a suitable basis for theoretical research in artificial intelligence. asymptotic bounded optimality in particular promises to yield useful results on composite agent designs  using the optimality-preserving composition methods in . as a robust measure of rationality  it's possible that it could do for ai what  big-o  descriptions did for complexity theory. 
   bounded optimality also has philosophically interesting implications. for example  like the rule-utilitarians we no longer talk about rational actions  because individual actions  and even deliberations  by a bounded optimal agent may be arbitrarily irrational in the classical sense. 
   furthermore  this theoretical research should  by design  apply to the practice of artificial intelligence in a way that idealized  infinite-resource models may not. we have given  by way of illustrating this definition  a bounded optimal agent: the design of a simple conditionaction rule system that  given a learning mechanism  provably and efficiently converges to a rational configuration. 
