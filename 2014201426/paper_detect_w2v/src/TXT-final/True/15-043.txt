a method of computing generalized bayesian probability values 
for expert systems 
peter cheese man 
sri international  1 ravenswood road menlo park  california 1 

a b s t r a c t 
　　　this paper presents a new method for calculating the conditional probability of any multi-valued predicate given particular information about the individual case. this calculation is based on the principle of maximum entropy  me   sometimes called the principle of least information  and gives the most unbiased probability estimate given the available evidence. previous methods for computing maximum entropy values shows that they are either very restrictive in the probabilistic information  constraints  they can use or combinatorially explosive. the computational complexity of the new procedure depends on the inter-connectedness of the constraints  but in practical cases it is small. in addition  the maximum entropy method can give a measure of how accurately a calculated con-
ditional probability is known. 
′1 i n t r o d u c t i o n 
　　　recently computer-based expert systems have been developed that store probabilistic knowledge obtained from experts and use this knowledge to make probabilistic predictions in specific cases. similarly  analysis of data  such as questionnaire results  can reveal dependencies between the variables that 
can also be used to make probabilistic predictions. the essential problem in such systems is how to represent all known dependencies and relationships  and how to use such information to make specific predictions. for example  knowledge of interrelationships between factors such as age  sex  diet  cancer risk etc. should allow the prediction of say an individual's cancer risk  given information on the individual. however because of possible interactions between the factors  it is not sufficient to combine the effects of each factor separately. 
　　　the major problem faced by all such probabilistic inference systems is that the known constraints usually underconstrain the probability space of the domain. for example  if the space consists of 1 predicates  then 1 - 1 joint probability constraints are needed to fully specify all the probabilities. when a space is under- constrained  any desired probability usually has a range of possible values. the problem is to find a unique probability value within the allowed range that is the best estimate of the true probability  given the available information  and to determine how reliable this estimate is. such an estimate is given by the method of maximum entropy  me   sometimes called the method of least information. this method gives a probability value that is the least commitment value  subject to the constraints. to choose any other value has been shown by shore and johnson  1  to be inconsistent  because any other choice would imply more information than was given in the problem. 
　　　this paper focuses on a type of expert system in which all the probability constraints are in the form of conditional probabilities or joint probabilities  sometimes called marginal probabilities because they occur in the margins of contingency tables . such probability constraints may have come from an expert or from an analysis of data that has shown that particular subsets of factors are significantly correlated. the problem of making probabilistic predictions in under-constrained probability spaces is of sufficient importance that many solutions have been tried. one method is to acknowledge that the desired probabilities are under constrained and return the range of pos-
sible values consistent with the known constraints  rather than a point value . such an approach is implicit in the method proposed by shafer  1 . 
 another method is to make the strong assumption of conditional independence when combining different evidence. this is the assumption behind prospector  duda et al  
1  and dependence trees  chow and liu 1  and used most recently by pearl  1 . use of the conditional independence assumption with given conditional probabilities is usually sufficient to constrain the desired probabilities to a unique value. however  this assumption is not always satisfied by actual data and can lead to inconsistent and over-constrained probability values  as pointed out by konolige  1 . 
　　　the main purpose of this paper is to introduce a new method for computing the maximum entropy probability of a predicate of interest  given specific evidence about related predicates  and subject to any linear probability constraints. this method avoids the combinatorial explosion inherent in previous methods without imposing strong limitations on the constraints that can be used  and it is therefore useful for computer-based expert systems. 
′1 the maximum entropy method 
　　　the method of maximum entropy was first applied by jaynes to the statistical mechanics problem of predicting the most likely state of a system given the physical constraints  e.g. conservation of energy . in jaynes  1   the maximum entropy method was used to provide prior probabilities for a bayesian analysis. lewis  1  applied the method of least information  an equivalent method  to the problem of finding the best approximation to a given probability distribution baaed on knowledge of some of the joint probabilities  i.e.  constraints on the possible distributions . 

p. cheeseman 1 

1 p. cheeseman 
to lewis  1 . he showed that if the given probabilities are conditionally independent then the underlying probability space can be represented by simple product formulas and that this is the maximum entropy distribution. this product form is the basis of dependence trees  chow and liu 1  and the tree based bayesian update method of pearl  1 . an iterative techinique for computing the me distribution given some of the joint probabilities without requiring conditional independence was developed by brown  1 . this method was extended by ku and kullback  1   but both authors put strong restrictions on the constrints that must be given  and their method combinatorially explodes if the space of predicates is large. the new method of computing me distributions presented in this section avoids these difficulties. 
　　　the problem of optimizing a continuous function subject to constraints is a well-known one in applied mathematics and a general solution is the method of lagrange multipliers. the specific problem of maximizing the entropy function  1  subject to constraints was first applied to the domain of statistical mechanics  and specifically to joint  marginal  constraints by gokhale and kullback  1 . this section derives the necessary formulae in a form suitable for efficient computation. 
　　the first step is to form a new entropy function as defined below: 
　　　equation  1  gives the me distribution in terms of the xs  so if the values of all xs can be found  the me space is known implicitly. note that equation  1  is the so-called loglinear form  but here this form is a direct consequence of the maximization of // rather than an ad hoc assumption. from  1  it is clear that there is only one x per constraint and that these are the only unknowns. if equation  1  is substituted into  1  etc.   i.e.  into each of the given constraints   then the resulting a set of simultaneous equations can be solved for the xs. it is more convenient first to apply the following transformations: 

i.e.  the basic distribution pijki is given implicitly as a product of as. equation  1  is the key to the new me calculation method  as it implicitly gives the underlying probability space in terms of a product of parameters  the as   and there are only as many as as there are constraints. note that for any particular pijki  only those as with the corresponding indices appear in the product. 
with these substitutions  equation  1  becomes: 

p. cheeseman 1 
resulting . abo  the conditional probability evaluation procedure is nondirectional because  unlike other expert systems  this procedure allows the conditional probability of any predicate to be found for any combination of evidence. that is  it has no specially designated evidence and hypothesis predicates. 
　　　the above probability evaluation method can be extented to include the case where the evidence in a particular case is in the form of a probability distribution over the values of a predicate that is different from the prior distribution  rather than being informed that a particular value is true. in this case  it is necessary to compute new as that correspond to the given distribution and use these new as in place of the prior corresponding as in probability evaluations such as those above. for instance  if a new distribution is given for p ai   then the new as are given by: 

note that the revised a values used in the above method are just multiplicative factors whose value is identical to the correction factors of lemmar and barth  1   and so the methods are equivalent in this case. the major difference is that here the probability space is represented implicitly by the as  and the corresponding summation procedure will work even when the space cannot be partitioned. 
　　　the above conditional probability evaluation procedure  a type of expert system inference engine  has been implemented in lisp and has been tested on many well known me examples. in me conditional probability calculations when given specific evidence  it has been found that only short strong chains of prior joint or conditional probabilities can significantly change the probability of a predicate of interest from its prior value. 
       when a point probability value is computed by the proposed method  it is useful to also estimate its accuracy as well. there are two sources of uncertainty in a computed me value. one is the possibility that the known constraints used are not the only ones operating in the domain. this type of uncertainty is hard to quantify and depends on the methods used to dud the known constraints. if a constraint search is systematic  over the known data   then we can be confident that we know all the dependencies that can contribute to a specific me value. if a constraint search is ad hoc  it is always possible that a major contributing factor has been overlooked. if any important factors are missing  the calculated me probability values will differ significantly from the observed values. if such deviations are found  it indicates that factors are missing  and an analysis of the deviations often gives a clue to these missing factors. 
　　　the other source of uncertainty is the accuracy with which the constraints are known. this accuracy depends on the size of the sample from which the constraints were extracted or the accuracy of the expert's estimates. this uncertainty is also hard to quantify  but it provides a lower limit on the accuracy of any calculated value. in the analysis given here  the constraints were assumed to be known with complete accuracy. 

1 p. cheeseman 
′1 summary 
　　　this paper presents a new method of computing maximum entropy distributions and shows how to use these distributions and some specific evidence to calculate the conditional probability of a predicate of interest. previous methods of computing maximum entropy distributions are either too restrictive in the constraints allowed  or too computationally costly in nontrivial cases. the new method avoids both these difficulties. justifications for preferring maximum entropy values are given  as are ways of estimating their certainty. 
　　　further research is necessary to further improve the efficiency of this method  particularly by automatically finding the optimal ‘ evaluation order and discovery of approximations that would allow the exclusion from the summation of any predicates that could not significantly effect the final result. such improvements should increase the usefulness of this me computation technique as an expert system inference engine. 
