ion in semi-markov decision processes 
balaraman ravindran and andrew g. barto 
department of computer science university of massachusetts amherst  ma  u. s. a. 
{ravi | barto}  cs.umass.edu 

abstract 
to operate effectively in complex environments learning agents require the ability to selectively ignore irrelevant details and form useful abstractions. in this article we consider the question of what constitutes a useful abstraction in a stochastic sequential decision problem modeled as a semi-markov decision process  smdps . we introduce the notion of smdp homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for smdps. we present an smdp minimization framework and an abstraction framework for factored mdps based on smdp homomorphisms. we also model different classes of abstractions that arise in hierarchical systems. although we use the options framework for purposes of illustration  the ideas are more generally applicable. we also show that the conditions for abstraction we employ are a generalization of earlier work by dietterich as applied to the options framework. 
1 introduction 
the ability to form abstractions is one of the features that allows humans to operate effectively in complex environments. we systematically ignore information that we do not need for performing the immediate task at hand. while driving  for example  we may ignore details regarding clothing and the state of our hair. researchers in artificial intelligence  ai   in particular machine learning  ml   have long recognized that applying computational approaches to operating in complex and real-world domains requires that we incorporate the ability to handle and form useful abstractions. in this article we consider the question of what constitutes a useful abstraction. since this is a difficult problem when stated in general terms  much of the work in this field is specialized to particular classes of problems or specific modeling paradigms. in this work we will focus on markov decision processes  mdps   a formalism widely employed in modeling and solving stochastic sequential decision problems and semi-markov decision processes  smdps   an extension to mdps recently employed in modeling hierarchical systems  sutton et ai  1; dietterich  1; parr and russell  1 . 
　our objective in this article is to introduce the concept of an smdp homomorphism and argue that it provides a unified view of key issues essential for a rigorous treatment of abstraction for stochastic dynamic decision processes. the concept of a homomorphism between dynamic systems  sometimes called a  dynamorphism   arbib and manes  1   has played an important role in theories of abstract automata  hartmanis and stearns  1   theories of modeling and simulation  zeigler  1   and it is frequently used by researchers studying model checking approaches to system validation  emerson and sistla  1 . although those studying approximation and abstraction methods for mdps and smdps have employed formalisms that implicitly embody the idea of a homomorphism  they have not made explicit use of the appropriate homomorphism concept. we provide what we claim is the appropriate concept and give examples of how it can be widely useful as the basis of the study of abstraction in a dynamic setting. 
　informally  the kind of homomorphism we consider is a mapping from one dynamic system to another that eliminates state distinctions while preserving the system's dynamics. we present a definition of homomorphisms that is appropriate for smdps. in earlier work  ravindran and barto  1  we developed an mdp abstraction framework based on our notion of an mdp homomorphism. this framework extended the mdp minimization framework proposed by  dean and givan  1  and enabled the accommodation of redundancies arising from symmetric equivalence of the kind illustrated in figure 1. while we can derive reduced models with a smaller state set by applying minimization ideas  we do not necessarily simplify the description of the problem in terms of the number of parameters required to describe it. but mdps often have additional structure associated with them that we can exploit to develop compact representations. by injecting structure in the definition of smdp homomorphism we can model abstraction schemes for structured mdps. in this paper we present one simple example of such an abstraction scheme and mention other factored abstraction methods that can be modeled by suitable structured homomorphisms. 
probabilistic planning 	1 　in the second part of the paper we extend the notion of smdp homomorphism to hierarchical systems. in particular  we apply homomorphisms in the options framework introduced by  sutton et aly 1  and show that this facilitates employing different abstractions at different levels of 

figure 1:  a  a symmetric gridworld problem. the goal state is g and there are four deterministic actions. this gridworld is symmetric about the ne-sw diagonal. for example  states a and b are equivalent since for each action in a  there is an equivalent action in b. taking action e  say  in state a is equivalent to taking action n in state b  in the sense that they go to equivalent states that are each one step closer to the goal   b  an equivalent reduced model of the gridworld in  a . the states a and b in the original problem correspond to the single state in the reduced problem. a solution to this reduced gridworld can be used to derive a solution to the full problem. 
a hierarchy. we also argue that homomorphisms allow us to model aspects of symbolic ai techniques in representing higher level task structure. finally  we show that the smdp homomorphism conditions generalize the  safe  state abstraction conditions for hierarchical systems introduced by  dietterich  1 . 
after introducing some notation  section 1   we define 
smdp homomorphisms and discuss modeling symmetries 
 section 1 . then we present a brief overview of our model minimization framework  section 1   discuss abstraction in factored mdps  section 1  and abstraction in hierarchical systems  section 1 . we conclude with some discussion of directions for future research  section 1 . 
　a discrete time semi-markov decision process  smdp  is a generalization of an mdp in which actions can take variable amounts of time to complete. as with an mdp  an smdp is a tuple where s  a and are the sets of 
states  actions and admissible state-action pairs; is the transition probability function with being the probability of transition from state 
s to state s' under action  in n time steps  and is the expected discounted reward function  with being the expected reward for performing action 
a in state s and completing it in n time steps.1 
　a stochastic policy  is a mapping from to the real interval  1  with for any gives the probability of executing action 
in state s. the value of a state-action pair  under policy  is the expected value of the sum of discounted future re-
wards starting from state s  taking action a  and following thereafter. when the smdp has well defined terminal states  we often do not discount future rewards. in such cases an smdp is equivalent to an mdp and we will ignore the transition times. the action-value function  corresponding to a policy  is the mapping from state-action pairs to their values. the solution of an mdp is an optimal policy  that uniformly dominates all other possible policies for that mdp. 
   let b be a partition of a set x. for any denotes the block of b to which belongs. any function  from a set x to a set y induces a partition  or equivalence relation  on x  with if and only if  
1 smdp homomorphisms 
a homomorphism from a dynamic system to a dynamic system is a mapping that preserves 's dynamics  while in general eliminating some of the details of the full system 
 one can think of as a simplified model of  that is nevertheless a valid model of with respect to the aspect's of  state that it preserves. the specific definition of homomorphism that we claim is most useful for mdps and smdps is as follows: 
we call the homomorphic image of m under and we use the shorthand to denote the surjection maps states of to states of and since it is gener-
ally many-to-one  it generally induces nontrivial equivalence classes of states s of m: . each surjection recodes the actions admissible in state s of to actions admissible in state f s  of this state-dependent recoding of actions is a key innovation of our definition  which we discuss in more detail below. condition  1  says that the transition probabilities in the simpler smdp  are expressible as sums of the transition probabilities of the states of  that / maps to that same state in  this is the stochastic version of the standard condition for homomorphisms of deterministic systems that requires that the homomorphism commutes with the system dynamics  hartmanis and stearns  1 . condition  1  says that state-action pairs that have the same image under h have the same expected reward. an mdp homomorphism is similar to an smdp homomorphism except that the conditions  1  and  1  apply only to the states and actions and not to the transition times. 
　the state-dependent action mapping allows us to model symmetric equivalence in mdps and smdps. for example  

　the set of all automorphisms of an smdp m  denoted by autm  forms a group under composition of homomorphisms. this group is the symmetry group of m. in the gridworld example of figure 1  the symmetry group consists of the identity map on states and actions  a reflection of the states about the ne-sw diagonal and a swapping of actions n and e and of actions s and w. any subgroup of the symmetry group of an smdp induces an equivalence relation on $  which can also be induced by a suitably defined homomorphism  ravindran and barto  1 . therefore we can model symmetric equivalence as a special case of homomorphic equivalence. 
1 minimization 
the notion of homomorphic equivalence immediately gives us an smdp minimization framework. in  ravindran and barto  1  we extended the minimization framework of dean and givan ldean and givan  1; givan et a/.  1  to include state-dependent action recoding and showed that if two state-action pairs have the same image under a homomorphism  then they have the same optimal value. we also showed that when m' is a homomorphic image of an mdp my a policy in ml can induce a policy in m that is closely related. specifically a policy that is optimal in m' can induce an optimal policy in m. thus we can solve the original mdp by solving a homomorphic image. it is easy to extend these results to smdp models. 
　thus the goal of minimization is to derive a minimal image of the smdp  i.e.  a homomorphic image with the least number of admissible state-action pairs. we also adapted an existing minimization algorithm to find minimal images employing state-action equivalence. employing state-dependent action recoding allows us to achieve greater reduction in model size than possible with dean and givan's framework. for example  the gridworld in figure 1 a  is minimal if we do not consider state-dependent action mappings. 
1 abstraction in structured mdps 
smdp homomorphisms can also be used to model various smdp abstraction frameworks. the definition of homomorphism in section 1 assumed a monolithic representation of the state set. while we can derive an equivalent mdp model with a smaller it does not follow that the description of the 
probabilistic planning 
state set is necessarily simpler and hence might not lead to a simpler problem representation. many classes of problems that are modeled as mdps have some inherent structure. we define structured forms of homomorphisms that can exploit this structure in deriving simpler problem representations. 
　factored mdps are a popular way to model structure in where pre denotes the parents of node s  in the 1-tbn corresponding to action a and each of the prob is given by a conditional probability table  cpt  associated with node the reward function may be similarly represented. 
　structuring the state space representation allows us to consider morphisms that are structured  i. e. surjections from one structured set to another. an example of a structured morphism is a simple projection onto a subset of features. here the state set of the homomorphic image is described by a subset of the features  while the rest are ignored. we introduce some notation  after  zeigler  1   to make the following 

　the first condition implies that the subset f should be chosen such that the features chosen are sufficient to describe the block transition dynamics of the system. in other words  the 
1 

subgraph of the 1-tbn described by the projection should include all the incoming arcs incident on the chosen nodes. the second condition requires that all the parents and the incoming arcs to the reward node are also included in the subgraph. to find such homomorphic projections  we just need to work back from the reward node including arcs and nodes  until we reach the desired subgraph. such an algorithm will run in time polynomial in the number of features. it is evident that the space of such simple projections is much smaller than that of general maps and in general may not contain a homomorphism reducing a given mdp. without suitable constraints  often derived from prior knowledge of the structure of the problem  searching for general structured homomorphisms results in a combinatorial explosion. abstraction algorithms developed by boutilier and colleagues can be modeled as converging to constrained forms of structured morphisms assuming various representations of the cpts-when the space of morphisms is defined by boolean formulae of the features  boutilier and dearden  1   when it is defined by decision trees on the features  boutilier et al  1  and when it is defined by first-order logic formulae  boutilier et a/.  1 . 
1 abstraction in hierarchical systems 
in the previous section we showed that smdp homomorphisms can model various abstraction schemes in  flat  mdps and smdps. smdp homomorphisms are a convenient and powerful formalism for modeling abstraction schemes in hierarchical systems as well. before we explore various abstraction approaches we first introduce a hierarchical architecture that supports abstraction. 
1 	hierarchical markov options 
recently several hierarchical reinforcement learning frameworks have been proposed  parr and russell  1; sutton et al  1; dietterich  1  all of which use the smdp formalism. in this article the hierarchical framework we adopt is the options framework  sutton et ai  1   though the ideas developed here are more generally applicable. options are actions that take multiple time steps to complete. they are usually described by: the policy to follow while the option is executing  the set of states in which the option can begin execution  and the termination function fi which gives the probability with which the option can terminate in a given state. the resulting systems are naturally modeled as smdps with the transition time distributions induced by the option policies. we present an extension to the options framework that readily facilitates modeling abstraction at multiple levels of the hierarchy using smdp homomorphisms. 
　we consider the class of options known as markov options  whose policies satisfy the markov property and that terminate on achieving a certain sub-goal. in such instances it is possible to implicitly define the option policy as the solution to an option mdp  or an option smdp if the option has access to other options. accordingly we define a hierarchical markov sub-goal option: 
definition: a hierarchical markov sub-goal option of an 
smdp 
where is the initiation set of the option  is the termination function and m o is the option smdp. the state set of mo is a subset of 1 and constitutes the domain of the option. the action set of mo is a subset of a and may contain other options as well as  primitive  actions in a. the reward function of mo is chosen to reflect the sub-goal of the option. the transition probabilities of mo are induced by p and the policies of lower level options. we assume that the lower-level options are following fixed policies which are optimal in the corresponding option smdps. the option policy is obtained by solving mo  treating it as an episodic task with the possible initial states of the episodes given by and the termination of each episode determined by the option's termination function 
　for example  in the gridworld task shown in figure 1 a   an option to pick up the object and exit room 1 can be defined as the solution to the problem shown in 1 b   with a suitably defined reward function. the domain and the initiation set of the option are all states in the room and the option terminates on exiting the room with or without the object. 
1 	option specific abstraction 
the homomorphism conditions  1  and  1  are very strict and frequently we end up with trivial homomorphic images when deriving abstractions based on a non-hierarchical smdp. but it is often possible to derive non-trivial reductions if we restrict attention to certain sub-problems  i.e.  certain sub-goal options. in such cases we can apply the ideas discussed in sections 1 and 1 to an option smdp directly to derive abstractions that are specific to that option. the problem of learning the option policy is transformed to the usually simpler problem of learning an optimal policy for the homomorphic image. 
1 	relativized options 
consider the problem of navigating in the gridworld environment shown in figure 1 a . the goal is to reach the central corridor after collecting all the objects in the environment. no non-trivial homomorphic image exists of the entire problem. but there are many similar components in the problem  namely  the five sub-tasks of getting the object and exiting roomt. we can model these similar components by a  partial  homomorphic image-where the homomorphism conditions are applicable only to states in the rooms. one such partial image is shown in figure 1 b . employing such an abstraction lets us compactly represent a related family of options  in this case the tasks of collecting objects and exiting each of the five rooms  using a single option mdp. we refer to this compact option as a relativized option. such abstractions are an extension of the notion of relativized operators introduced by  iba  1 . formally we define a relativized option as follows: 


figure 1:  a  a simple rooms domain with similar rooms and usual stochastic gridworld dynamics. the task is to collect all 1 objects  black diamonds  in the environment and reach the central corridor. the shaded squares are obstacles   b  the option mdp corresponding to a get-object-and-leave-room option. 
here the state set of  where so is the domain of the option  and the admissible state-action set is // ty . going back to our example in figure 1 a   we can now define a single get-object-and-leave-room relativized option using the option mdp of figure 1 b . the policy learned in this option mdp can then be suitably lifted to m to provide different policy fragments in the different rooms. figure 1 demonstrates the speed-up in performance when using a single relativized option as opposed to five regular options. in this experiment the option policies and the higher level policy were learned simultaneously.1 
1 modeling higher level structure 
smdp homomorphisms have the power to model a broader class of abstractions  those not supported by the base level dynamics of the system but which can be induced at some intermediate levels of the hierarchy. for example  consider a multi-link assembly line robot arm that needs to move objects from one assembly line to another. the state of the system is described by the joint angles and velocities  objectshape  object-orientation  and boolean variables indicating whether the object is firmly-grasped and whether the object is at-target-location. the primitive actions are various joint torques. depending on the configuration of the arm and the shape and orientation of the object  this task requires different sequences of actions  or policies  even though conceptually the higher level task has the same  structure -grasp object  move object  and place object. 
　we can model this higher level task structure using a relativized option. first  we define suitable grasp-object options  such as grasp-cylinder  grasp-block etc.   and similar options for moving and placing objects. then we form a partial homomorphic image of the resulting smdp-the state set of the image is described by the two boolean features and the action set consists of grasp  move and place actions. the partial homomorphism  which applies only to the admissible state-option pairs of the smdp  consists of 


figure 1: comparison of performance of learning agents employing regular and relativized options on the task shown in figure 1. 
object-shape and similarly for the move-object and placeobject options. a relativized option with this partial image and partial homomorphism as the option mdp and option homomorphism  respectively  captures the desired conceptual structure. executing the optimal policy for this option results in action sequences of the form: grasp  move  place  - depending on the object-shape these abstract actions get bound to different lower level options. while techniques in symbolic al have long been able to model such higher level task structure  the reinforcement learning community lacked an obvious mechanism to do the same. our work gives us new tools to model conceptual task structure at various temporal and spatial scales. 
1 relation to maxq safe state-abstraction 
 dictterich  1  introduced safe state-abstraction conditions for the maxq architecture  a hierarchical learning framework related to the options framework. these conditions ensure that the resulting abstractions do not result in any loss of performance. he assumes that the sub-problems at different levels of the hierarchy are specified by factored mdps. the maxq architecture employs a form of value function decomposition and some of the safe abstraction conditions apply only to this form of decomposition. the following condition is more universal and applies to the hierarchical markov options framework as well: 

　condition  i  states that the transition probability can be expressed as a product of two probabilities  one of which describes the evolution of a subset of the features and depends only on that subset. condition  ii  states that if two states project to the same abstract state  then they have the same immediate reward. from equations 1  it is evident that the above conditions are equivalent to the smdp homomorphism conditions if we restrict our attention to simple projection homomorphisms and do not consider action remapping. thus the smdp homomorphism conditions introduced here generalize dietterich's safe state-abstraction condition as applicable to the hierarchical markov option framework. 

probabilistic planning 	1 

1 discussion 
the equivalence classes induced by smdp homomorphisms satisfy the stochastic version of the substitution property  hartmanis and stearns  1 . this property is also closely related to lumpability in markov chains  kemeny and snell  
1  and bisimulation homogeneity  givan et al.  1  in mdps. we chose the smdp homomorphism as our basic formalism because we believe that it is a simpler notion and provides a more intuitive explanation of various abstraction schemes. 
　the homomorphism conditions  1  and  1  are very strict conditions that are often not met exactly in practice. one approach is to relax the homomorphism conditions somewhat and allow small variations in the block transition probabilities and rewards. we have explored this issue  ravindran and barto  1   basing our approximate homomorphisms on the concept of bounded-parameter mdps developed by  givan et al.  1 . we are currently working on extending approximate homomorphisms to hierarchical systems so as to accommodate variations in transition-time distributions. 
　although smdp homomorphisms are powerful tools in modeling abstraction  finding a minimal image of a given smdp is an np-hard problem. while taking advantage of structure allows us to develop efficient algorithms in special cases  much work needs to be done to develop efficient general purpose algorithms. currently we are investigating methods that allow us to determine homomorphisms given a set of candidate transformations in a hierarchical setting. 
　in this article we presented a novel definition of smdp homomorphism that employs state-dependent recoding of actions. this allows us to extend existing minimization and abstraction methods to a richer class of problems. we developed notions of equivalence specialized to factored representations and hierarchical architectures. we have shown that smdp homomorphism can serve as the basis for modeling a variety of abstraction paradigms. 
acknowledgments 
we wish to thank dan bernstein and mike rosenstein for many hours of useful discussion and bob givan and matt greig for clarifying certain ideas from their work. this material is based upon work supported by the national science foundation under grant no. ecs-1 to andrew g. 
barto and sridhar mahadevan. any opinions  findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation. 
