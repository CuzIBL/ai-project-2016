
although necessary  learning to discover new solutions is often long and difficult  even for supposedly simple tasks such as counting. on the other hand  learning by imitation provides a simple way to acquire knowledge by watching other agents do. in order to learn more complex tasks by imitation than mere sequences of actions  a think aloud protocol is introduced  with a new neuro-symbolicnetwork. the latter uses time in the same way as in a time delay neural network  and is added basic first order logic capacities. tested on a benchmark counting task  learning is very fast  generalization is accurate  whereas there is no initial bias toward counting.
1 introduction
learning to count is a very difficult task. it is well-known that children do not learn only by themselves: their parents and their teachers provide an important active help.
﹛on the other hand  in the field of on-line machine learning where the teacher does not provide help  some recurrent neural networks are interestingly successful  such as the long short-term memory  lstm   hochreiter and schmidhuber  1 . however  to learn some counters the network still requires to be trained through tens of thousands of sequences  and it already contains a counting feature  although it had not been specifically designed for this.
﹛between active teaching and automatic discovery stands learning by imitation  schaal  1 : the learner watches the  teacher  solve the task  but the teacher may not be aware of the presence of the learner. this is helpful for sequences of actions  for example to make a robot acquire a complex motor behavior  schaal  1; calinon et al.  1 : the learner can see what intermediate actions should be done.
﹛fewer studies on imitation focus on learning sequencesthat are more complex than sequences of physical actions  i.e.  algorithms. programming by demonstration  cypher  1  deals with this issue  but is more interested in human-machine graphicalinteraction and thus makes many assumptions about how tasks are represented. maybe the work that is the closest to the one presented in this paper is that of furse  furse  1 . he uses a paper-and-pencil approach to learn  by imitation  algorithms as complex as making a division. however  still many assumptions are made that clearly ease learning: the paper-and-pencil approach provides an external infinite memory and useful recall mechanisms; the system also already contains some basic arithmetic tools  such as the notion of number  of numerical superiority  etc. furthermore  it is able to learn to make function calls  access to subroutines   but the teacher has to explicitly specify which function to use  with which inputs and outputs. calling a function is thus a special action.
﹛in this paper  there is no specific action: the sequences are composed of completely a priori meaningless symbols  i.e.  all these symbols could be interchanged without changing the task. it is able to make basic automatic function calls  without explicitly specifying it. there is also no initial internal feature that biases the system toward counting.
﹛in section 1  think aloud imitation learning is described  with a usual neural network. the latter is superseded in the next section by the presence network  a growing neurosymbolic network  orseau  1a . in section 1  the latter is then augmented by special connections  and its development is explained on a simple task. then experiments such as counting are given  showing that learning can be faster by several orders of magnitude than with a usual neural network  and that some important learning issues can be circumvented  so that learning complex tasks on-line in a few examples is possible.
1 think aloud imitation learning
in imitation  the environment  which gives the task to solve  produces events env t  and the teacher  who knows how to solve it  produces tch t   where t denotes the present time. during learning  the agent tries to predict the teacher's next action. when it succeeds a t  = tch t .
﹛the agent is given sequences of events such as /abcdefgh/  where the events provided by the environment are written in lowercase letters and those provided by the teacher are in capitals  but should be seen as the same symbols. the 1 letters of the alphabet are used as events all along this paper. for better legibility only  spaces will sometimes be introduced in the sequences. the goal of the learner is to predict the teacher's events  so that once it is on its own  it not only predicts but in fact acts like the teacher would do.
﹛in imitation  one problem with recurrent neural networks  for example  is that they have internal states; the learner cannot  see  these states inside the teacher  which are thus difficult to learn.
﹛therefore  in the think aloud paradigm  the teacher is forced to  think aloud   this means that it does not have internal states but it can  hear  what it itself says: the recurrence is external  not internal. then it can be easily imitated by the learner  who  listens  to the teacher when learning. once on its own  the agent  not learning anymore  also thinks aloud and can hear itself. this allows the teacher to make intermediate computationsteps that can be imitated by the learner. during learning  the agent thus receives the sequence of events of the teacher and of the environmente t  = env t +tch t . after learning  during testing  the agent is on its own  so tch t  = 耳 and the agent listens to itself  not to the teacher: e t  = env t  + a t . this can be seen as some teacher forcing technique  williams and zipser  1 .
﹛a first  simplified view of the architecture is now given. in order to take time into account without having internal states  we use a sort of time delay neural network  tdnn   sejnowski and rosenberg  1   that computes its outputs given only the last 而 time steps. at present time t  it computes: net t  = f e t  e t   1  ...e t   而 + 1  .
﹛in the sequel  the indices i  j  k are used respectively only for inputs  hidden neurons  output neurons. xi is the activation of input i. as with other variables  writing yi is a shortcut for yi t   where t is the present time  with the only exception that xdi stands for xi t   d . writing e t  = i means that input i is activated at time t  where inputs are encoded in a 1-of-n way.
﹛usually  the tdnn architecture is explained with delayed copy sets of inputs; here we describe it as follows: each connection  weight  wjid from input unit i to hidden unit j has a delay d  so that if e t  = i  wjid transmits this activation to unit j at time t + d. a hidden neuron j is then connected to input i through a number 而 of connections wjid   each one having a different delay 而   d ≒ 1. such a network cannot produce recurrent  infinite behaviors.
﹛it is the fact that the agent can hear what it says during testing that gives it recurrent capacities: since actions are reinjected on inputs  the resulting recurrent network is quite similar to a narx neural network  lin et al.  1   without learning considerations through the recurrence  no backpropagation through time . for example  if the agent is given sequences like /abababab. . . /  it then learns to say a after any /ab/  and b after any /ba/. then  once it hears /ab/  it says a. it has therefore heard /aba/  and then says b  has heard /abab/  says a  and so on. it then produces an infinite behavior  but learning occurs only in the tdnn.
﹛instead of direct supervision of outputs  we use instant reinforcements  kaelbling et al.  1  on one single output. this is more general for an autonomous agent like a robot and may be used not only for imitation. while imitating  acting like the teacher is innerly rewarding. actions are selected among the inputs. thus  at each time step  to predict the next action/input  each input is tested and the one that predicts the best reinforcement for the next time step is chosen.
1 the presence network
in a tdnn  the number of input connections of a single hidden neuron j is |wj| = 而ni  where ni is the number of inputs. it grows quickly when 而 or ni grows and makes learning difficult: the number of training examples is often said to be in |w|1. in the experiments described in section 1  many time steps  neurons and inputs are needed  so that using a mere tdnn is unpractical. tdnn and backpropagation also have important limitations when learning several tasks  robins and frean  1 . they are prone to catastrophic forgetting: learning a concept b after a concept a often modifies too much the knowledge of concept a. another issue arises with unmodified weights: the network will tend to provideanswers even before learning. thus  even if weights are frozen after concept a  adding new neurons may  sometimes not  temporarily mask other already acquired knowledge. learning must then be adapted to prevent this.
﹛instead of a mere tdnn  we use a presence network  orseau  1a   which is a neuro-symbolic network more tailored for learning sequences of events such as those dealt with here. it uses time in the same way as a tdnn and its main properties are very fast learning  local connectivity and automatic creation of neurons. a slight modification is made:  specificity  is introduced to circumvent an issue of the initial network. the presence network is described below but for its use in a less specific context  see  orseau  1a .
﹛the heuristic idea is that new hidden neuronsare connected only to inputs that were active in the short past  that are in the short-term 而-memory   so that the number of connections does not fully depend on the number of inputs. this is particularly useful when one event type is coded on a single input. since sequences are composed of mutually exclusive events  there is only one event per time step  so a neuron would have only one connection per time step in such a network. initially there is no hidden neuron  and new ones are added on-line when errors occur.
﹛first  the hidden neuron and its optimization rule is given  in order to show what the network can represent. the action selection scheme can then be given to show how to circumvent one issue of the representation capacities of the network. then the learning protocol follows.
1 hidden neuron
if  on a sequence e  a new neuron must be created  when specified in section 1   the new neuron j is connected to the inputs that were active in the short past with a corresponding delay d. its set wj of input connections is then:
wj = {wjid |  t t 而   t ≒ t   !wjid   i = e t   d = t  t}.
﹛the sequence used to create j is called its sequence of reference  rj.
﹛let 牟j = 1/|wj|  which is not modifiable  and then set the initial weight value  wjid ﹋ wj wjid = 牟j  which ensures that
. the output weight wkj toward the reinforcement output is set to 1.
let 考j be the weighted sum. the activation function yj is:
if 考j ≒ 1   牟j  inactive neuron  if 1   牟j   考j   1  active neuron  if 考j ≡ 1  active neuron 
﹛note that 1   牟j is the activation threshold. thus  if rj is presented again to the agent  j recognizes it and yj = 1. before optimization  only rj can activate j. if at least one event is substituted for another one  yj = 1.
1 optimization of a hidden neuron
to optimize the neuron j  each w can be seen as either informative  close to 牟  or not  close to 1 . there is no negative weight. if the neuron should be more activated   quanta  of weights are moved from inactive connections  that transmitted no activation  to active ones. the converse is done if the neuron should not be active.

algorithm 1: update rule for input weights of a hidden neuron j. 汐 is the learning rate. er is the error received from algorithm 1.

	汛 = 汐.牟j.er	// the maximum modification of w
 wjid ﹋ wj:
if  xdi   1 and er   1  or  xdi = 1 and er   1  then
   忖di = min 汛 1   wjid   else 忖di =  min 汛 wjid  

 then if xdi   1 then sid = s+else sid = s 


﹛this learning algorithm ensures  i.e.  the sequence or reference rj can always be recognized by j. all weights also stay in  1  1 : since corresponding events were present when the neuron was created  it would not be logical that the weights become inhibitory. a neuron is thus a filter that is active when it recognizes a conjunction of events appearing at different time steps  which describes a potentially general sequence of events  and where weights of noninformative time steps are pushed toward zero.
﹛the output weight of a chosen neuron j is updated by a simple delta rule and is bounded in  1  1 : wkj ↘ wkj +灰er  where 灰 is a learning rate  but its value is not critical. in fact  the output weight has not much influence on the concept embodied by the hidden neuron and may be used only to  discard  a neuron/concept if statistically found wrong.
﹛for example  a single neuron can represent the sequence /abcz/  meaning that if the agent receives the sequence /abc/  this neuron proposes to answer z. after some examples such as /adcz/  /aecz/  this neuron can get generalized to /a*cz/  where * means any possible symbol.
1 action selection
the action selection scheme is basically the same as in section 1. in passive mode  a neuron is a filter that recognizes a sequence and predicts a reinforcement value. in active mode  the agent predicts an action  so that at the next time step the sequence is recognized .
﹛since neurons only consider events that are present  i.e.  active inputs  the main drawback of the presence network is that inactive inputs are not taken into account  even if this inactivity is important. said in another way  the network cannot distinguish exception cases from general cases. for example  consider the sequences /abcz/  /adcz/  /aecz/  etc. suppose now that there is also the exceptioncase /aycx/. if oneneuron embodies this case  then after /ayc/  it naturally predicts x . but at the same time  the neuron representing the general case /a*cz/ predicts z. then how to select the right action  the neuron /a*cz/ should then have an inhibitory link with input y  but to preserve the interesting properties of the presence network  i.e.  not to connect inactive inputs  in potentially great number  to a new neuron  precedence in action selection is automatically given to cases that are the most specific: more specific neurons take more events into account  and thus  explain  more accurately the given sequence.
﹛first  the usefulness of a connection is defined: u wjid   = min 1 wjid /牟j . specificity 肉j means how well the neuron j explains the current sequence: . then it is preferred to select the most specific neuron  among the activated ones  see algorithm 1 .
﹛the specificity of the neuron that recognizes /a*cz/ is 肉 = 1  because the middle weight has become 1; and the specificity of the neuron that recognizes /aycx/  which has not been optimized  is 肉 = 1. this means that after the sequence /ayc/  the agent will always choose x preferentially. a neuron representing an exception case similar to a general case has more non-zero weights  which ensures it to be selected in priority.
﹛a interesting property is that the general case needs no modification at all when learning one  or more specific cases.

algorithm 1: action selection.

// find event that activates each j most at next time step:
 j ej = argmaxe t+1  yj t + 1 
compute corresponding specificity 肉j t + 1  // keep only active and most specialized neurons:
a = {j | yj t +1    1 肉j t +1  = maxh 肉h t +1 }
// select p  the most specialized and active neuron: p argmax y t
exists  耳 otherwise

1 learning protocol
catastrophic forgetting  e.g. with backpropagation  happens partly because knowledge is shared between many neurons  and many neurons can be modified at the same time. to avoid this  here at most 1 neurons can be modified at the same time. first  in order to compare two neurons on the present sequence  when a neuron is not active  yj = 1  preactivation is defined1: y j = 考j/ 1   牟j .
﹛let n  be the most preactivated neuron  which must have a connection with d = 1 pointing to e t   in order to be able to predict it:
n  = argmaxj {y j | yj = 1 y j   1 and  wjid  d = 1  i = e t }. if none exists  n  = 耳.
﹛with an accurate selection mechanism of the nearest neuron  a high learning rate can be used. of course  the nearest neuron may not always be the correct one to optimize  but it then will probably be optimized again in the other way.
﹛algorithm 1 describes the learning protocol  to decide when neurons must be created or modified. a neuron can be added at the same time another one is modified  because the latter may be wrongly optimized. if a neuron is already active  it means it has recognized the sequence  can thus predict the right answer  and no neuron needs to be added.

algorithm 1: learning protocol.

time t: predict teacher action: a t + 1 
t ↘ t + 1
	if a t  = e t  then	// correct action selected
optimize selected neuron p with error
er =  1   wkp.yp 
else
create new neuron on current specific case if  h yh   1  wkh.yh = maxj wkj.yj then
// h should have been selected  not p
optimize p if exists  er =  1   wkp.yp 
   optimize h  er =  1   wkh.yh  else
optimize n  if exists  er = 1

1 time delayed identity connections
time delayed identity connections  tdic  allows to know when there has been a repetition of an event in the short term. in  orseau  1b   they are presented in the form of short term memory special inputs. here  they are dynamic connections  but it is the same. they enhance the network generalization capacity  and allow both to compare two events in passive mode  and to repeat a past event in active mode.
﹛when creating a neuronj at presenttime t with a sequence of events e  if two events e t1  and e t1  are identical such that 而   t  t1   t  t1  a connection wjidd is added  before setting 牟j  to the neuron with the following properties. in addition to the normal delay d = t   t1  it has another one d = t   t1  pointing to the event to compare with. this connection is dynamic: at each time step t  the target input i of wjidd is changed1 such that i = e t   d . then the connection behaves exactly as a normal one of delay d. note that the special case d = d is a normal connection.
 never happens: the neuron has a single connection and then either. 1
if e t  d  does not exists  i can be an idle input unit.
﹛now since a single event can be referred to by several connections  the definition of specificity must be modified so that at most one event by delay d is taken into account. for example  on /abc/  initially 肉 = 1. on /aba/  there are 1 normal connections plus one tdic  thus 肉 = 1  but should logically be 1. thus:.
﹛the resulting system can represent almost prolog logic programs  muggleton  1  without lists  but in an on-line way  and with no explicit function name and parameter. the tdics bind variables  through time  and allow comparison between variables  here variables are events  or time steps   a typical first order logic capacity.
1 a simple task: equality
to give an example of how the network works  its development is shown when trained on a simple equality task  where the agent must answer true or false whethertwo given events are identical or not. it is a static task in essence. examples of sequences: /ab f/  /dd t/  /ph f/  /cc t/  /vv t/  etc.
﹛the network has 1 inputs and 而 = 1  or higher . initially there is no hidden neuron.
﹛the first sequence /ab f/ is provided to the agent. during the 1 first time steps  nothing happens  except that the network puts these symbols in its 而-memory. on symbol f  the network should have predicted f  but did not  because there is no neuron. therefore  a new neuron n1 is created  in order to make it correlate f with /ab/. it is connected to input a with a delay d = 1  to b with a delay d = 1  and to f with d = 1. input weights are set to 1/|wn1| = 1. if the sequence /ab/ is presented again to the agent  the network predicts f  then the teacher says f  n1 is activated  expecting a reward  effectively receives a reward  and no modification is needed.
﹛the sequence /dd t/ is then provided to the agent. once again  there is no active or preactive neuron that could predict t  so a new neuron n1 is added. n1 has 1 connections: 1 are similar to the previous case  and 1 is a tdic. the latter is created because of the repetition of d  with delays d = 1 and d = 1. n1 can now answer correctly for /dd t/. note that the tdic is not necessary to learn this case by heart; it is useful only if n1 is optimized.
﹛the sequence /ph f/ is presented. n1's tdic is now pointing to p  but does not transmit an activation. again  no neuron predicted f. so a new neuron n1 is added to answer correctly to this sequence. but now n1 is preactivated so it is chosen to be slightly modified to predict f. the weight of its active connections  those that transmitted an activation  are increased  whereas the other input weights are decreased. thus  input connections from a and b of n1 are considered as noise.
﹛on the sequence /cc/  n1 predicts f because it would slightly activate n1  although there is a repetition. but the teacher says t instead. no neuron is activated  so a new neuron n1 is added  and n1 is not modified. and n1  being the most preactivated neuron  is modified: in particular  the weight of the tdic is increased because it is pointing to c.
﹛on the sequence /vv/  n1 predicts f and n1 predicts t. since 肉n1 = 1  approximately  and 肉n1 = 1  n1 explains more specifically this sequence and thus the network predicts t. this is correct  so no neuron is added  and n1 is optimized again to better predict t  with a higher value .
﹛now the network predicts correctly any sequence like /污污 t/  recognized by n1  and /污汕 f/  recognized by n1  where 污 and 汕 are any two letters.
1 experiments
the present work is close to the continual learning framework  ring  1  where the agent must re-use previously acquired knowledge in order to perform better on future tasks. here  we are interested in tasks where the re-use of knowledge is not only useful  but necessary to generalize  and is called as a function. thus  even for static tasks  using a temporal framework can give access to these generalization capacities.
﹛in order to show the usefulness of context and how the agent can make automatic function calls  in section 1 we use the task described in  orseau  1b   where it is tested with a tdnn with backpropagation and  tdic  inputs.
﹛the previoustasks can be seen as static tasks. in section 1 is given a typical dynamic one: in the domain of languages  a sequence of xnyn is a sequence of a finite number of x followed by a sequence of the same amount of y.
﹛inputs are the 1 letters of the alphabet  汐 = 1 and 灰 = 1. 而 is set to 1  but shorter sequences may create less connections with the presence network  而 should be set to a sufficiently high value so as to take enough events into account . learning stops after a given number of correctly predicted sequences. this number is not taken into account in the table. since learning in the presence network is deterministic  the order of the training sequences is randomized. during testing  the agent is on its own  so its predicted actions take the place of the teacher's events. 1 successive trials  learning and testing  are done for each task  see results in table 1 .
table 1: average results for 1 successive trials. tr  ts  is the number of training  testing  sequences; etr  ets  is the number of sequences on which the agent made errors during training  testing ; #n is the number of neurons created only for the task being learned; #w is the total number of weights for the #n neurons; the * is for tasks trained with a normal tdnn.
tasktretrtsets#n#wgp*1n.a.11gp11111is gp*1n.a.11is gp11111flw11111xnyn11111﹛the system could hardly be directly compared with other methods. the closest one is  furse  1   which has yet many important differences  section 1 . there are also some similarities with inductive logic programming  muggleton  1   but the aims are different. ilp is not on-line  has explicit function names and uses global search mechanisms.
1 groups
the alphabet is decomposed into 1 groups of consecutive letters: a to e are in group a  f to j are in group b  ...and z is in group f. the agent must first learn by heart in which group is each letter. then  given a letter and a group  it must learn and generalize to answer yes if the letter is in the group  and

no otherwise.
﹛this task nicely shows how intermediate computations to re-use background knowledge is necessary to generalize.
﹛on the first task  the agent is given the 1 sequences of the type /gpaa/  /gpzf/  etc. here  gp is the name of the task  the context. learning stops when the whole training set is correctly processed. after learning  neurons are frozen to facilitate learning on the second task.
﹛on the second task  the agent is given sequences like /is fb gpfb y/   is f in the group b  the group of f is b 

so yes    /is bc gpba n/  etc. the context gp is used to

call the appropriate function. also  without this context in the first task  there would be conflicts between the two tasks: neuronswould be too general and would giveanswers most of the time when not needed. the context also allows to choose the nearest neuron more accurately. after a sequence /is bc/  the agent makes intermediate computations by saying gpb. thus  since it has now heard gpb  it is auto-stimulated to answer a  given its knowledge of the first task. the tdics allows to repeat the letter b given by the teacher to put it automatically after gp. the agent has learned to call the function gp. but there is no specific symbol to tell what is the function or the parameter. the function call is only a consequence of how knowledge is represented and used. then the tdics are used to compare the group given by the teacher and the one the agent knows.
﹛during training  only the 1 first letters and the 1 first group letters are used. learning stops after 1 successive correctly processed sequences. with the tdnn used in  orseau  1b  it is necessary to have 1 more letters  generalization still induces some errors  and more importantly it is necessary to force the agent not to say anything during 1 time steps after the end of the sequences of the first task in order to prevent these neurons from disturbing learning on the second task. none of these problems arise with the presence network.
﹛furthermore  the results  see table 1  show that learning is much faster  which is important for on-line learning  generalization is perfect to any letter and any group.
1 counting
the agent has no special internal feature that can generalize a counterby itself  so it must acquire knowledgeabout numbers beforehand. first  the agent needs to learn to increment any 1digit number  and then uses this incrementation as a function to solve the task xnyn.
incrementation
in this task  given a 1-digit number the agent must answer its successor: which number follows aa  ab. examples of sequences: /flw aa ab/  /flw ab ac/  .../flw az ba/  /flw ba bb/  .../flw zy zz/.
﹛there are 1 general cases to learn  /flw 污a 污b/  /flw 污b 污c/  .../flw 污y 污z/   and 1 specific cases  /flw az ba/  /flw bz ca/  .../flw yz za/   for a total of 1 cases.
﹛the training set contains all the 1 specific cases and the 1 first cases  in which sequences are then chosen randomly in order not to have only small a污 numbers. training stops after 1 correctly classified sequences. after learning  the network has always generalized to the 1 cases. finally  weights are frozen.
﹛here  a minimal tdnn would have 1 inputs  而 =1  1  tdic  inputs  and probably at least 1 neurons would be necessary. this would make a total of more than 1 weights. for this task and moreover for the next one  training the tdnn does not seem reasonable.
recurrence: 1-digit xnyn
the incrementation can now be used as a function inside a recurrent task. the outline is to initialize the counter to 1 at the beginning of the sequence  and then to call the incrementation function after each x and each y  and when the two numbers match  stop. one difficulty is that during the whole sequence of y  the number of x must be memorized. since the teacher does not have internal states  it must use its 而-memory to keep track of it. one way to do that is to repeat it aloud at constant intervals.
﹛example of a sequence: /start stpx flw aa ab stpx flw ab ac . . . flw df dg stpy dg flw aa ab stpy dg flw ab ac . . . stpy dg flw df dg stop/  where the number dg of x is repeated after each stpy. the context stp  for  step   is added to x and y in order not to be confused with anything else  since x and y appear also elsewhere.
﹛after each stpx  the agent learns to say flw  then to repeat  with tdics  the last number  and this automatically stimulates itself to call the incrementation function and say the following number. tdics are also used to compare the numbers and end the sequence if they match: the activated tdic provides a higher specificity than the general case. such sequences can have more than 1 events.
﹛during training  the agent learns to use its tdics for recurrence instead of learning only specific cases by heart. the training set contains the first 1 values of n. after 1 correctly classified sequences  learning stops. it is then tested on 1 randomly chosen sequences with n ≒ 1.
﹛again  there is no single error  see table 1   the network has generalized very quickly to unseen sequence lengths. this is far away from the tens of thousands of iterations needed by the lstm  hochreiter and schmidhuber  1  to learn anbn  although the learning paradigms are completely different.
﹛preliminary results also show that on xnyn  about 1 neurons can be automatically deleted afterwards  often with no loss in generalization accuracy.
1 conclusion
within think aloud imitation learning  an agent can learn on-line and accurately generalize complex sequences such as counters in very few examples shown by the teacher. the presence network can take long delays and many events into account and its size grows automatically. for such tasks  compared to a usual time delay neural network  learning is faster by several orders of magnitude and no problem occurs when re-using knowledge during learning. time delayed identity connections allow to make automatic function calls and provide some simple first-order logic capacities.
﹛current work focuses on the re-usability of knowledge so that called functions can be of unbounded size to hierarchically learn such complex sequences.
