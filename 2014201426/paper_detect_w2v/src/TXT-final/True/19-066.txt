 
   connectionist networks are powerful techniques  inspired by the parallel architecture of the brain  for discovering intrinsic structures in data. however  they are not well suited for implementation on serial computers. in this paper  we discuss the first implementation of a connectionist learning algorithm  error back-propagation  on a fine-grained parallel computer  the connection machine. as an example of how the system can be used  we present a parallel implementation of nettalk  a connectionist network that learns the mapping from english text to the pronunciation of that text. currently  networks containing up to 1 million links can be simulated on the connection machine at speeds nearly twice that of the cray-1. we found the major impediment to further speed-up to be the communications between processors  and not processor speed per se. we believe that the advantage for parallel computers will become even clearer as developments in parallel computing continue. 
1 introduction 
massively parallel  connectionist networks have undergone a rediscovery in artificial intelligence and cognitive science  and have already lead to broad application in many areas of artificial intelligence and cognitive simulation  including knowledge representation in semantic networks   speech recognition  1   dimensionality reduction  1   and backgammon . however  connectionist networks are computationally intensive  and days or weeks are often required to train even moderate-sized networks using the fastest serial computers; the exploration of large networks consisting of more than a million links or connections is barely feasible given current technology. 
   one option being explored is the development of special-purpose hardware using vlsi technology  1 1 . initial estimates of these so-called neuromorphic systems indicate that tremendous speed-up may be achievable  perhaps up to five to six orders of magnitude over a vax1 implementation. however  one problem with special purpose hardware is that it does not allow one to explore different patterns of connectivity and different learning algorithms. although neuromorphic systems will certainly have an important impact on the field  they may be limited as research tools. 
   a more flexible alternative is seen in the development of general purpose  fine-grained  parallel computers. the connection machine  cm  is a massively parallel computer consisting of up to 1  1  one-bit processors arranged in a hypercube architecture . in this paper we discuss the implementation of a connectionist network learning algorithm  back-propagation   charles r. rosenberg 
cognitive science laboratory princeton university 
princeton  new jersey 1 
on the connection machine. currently  the connection machine offers a factor of 1 speedup over a previous implementation on a vax1 and a factor of two speed-up over an implementation of a similar network on a cray-1. considering that parallel computing is only in it's infancy  we expect the speed-up to be much greater in the near future. finally  we present an application in the domain of speech synthesis  called nettalk   that uses our implementation of back-propagation on the connection machine. 
1 error back-propagation 
connectionist network models are dynamic systems composed of a large number of simple processing units connected via weighted links  where the state of any unit in the network depends on the states of the units to which this unit connects. the values of the links or connections determine how one unit affects another; the overall behavior of the system is modified by adjusting the values of these connections through the repeated application of a learning rule. 
   the back-propagation learning algorithm is an error-correcting learning procedure for multilayered network architectures. information flows forward through the network from the input layer  through the intermediate  or hidden layer s   to the output layer. the value of a unit is determined by first computing the weighted sum of the values of the units connected to this unit and then applying the logistic function  1+e-z to the result. this forward propagation rule is recursively applied to successively determine the unit values for each layer. 
   the goal of the learning procedure is to minimize the average squared error between the values of the output units and the correct pattern provided by a teacher. this is accomplished by first computing the error gradient for each unit on the output layer  which is proportional to the difference between the target value and the current output value. the error gradient is then recursively determined for layers from the output layer to the input  by computing the weighted sum of the errors at the previous layer. these error gradients  or deltas  are then used to update the weights1. 
   computationally the forward and backward propagation steps are very similar. forward propagation consists of four basic steps: distributing the activation values of the units to their respective fan-out weights  multiplying the activations by the weight values  summing these values from the weights into the next layer of units  and applying the logistic function to this value. the backward propagation of error consists of four similar steps: distributing the error values of the units to their respective fan-in weights  multiplying the error by the weight values  summing these values from the weights into the previous layer of units  and evaluating the derivative of the logistic function. in addition to forward and 


figure 1: the layout of weights and units of a simple network on the connection machine.  a  a simple two layer network.  b  the layout of the network on the processors of the connection machine. 
backward propagation  the inputs and outputs must be clamped to the appropriate values. in the next section  we will show how each of these steps is executed on the connection machine. 
1 	the connection machine 
the connection machine is a highly parallel computer with between 1 and 1 processors. each processor has two single-bit arithmetic logic units  alus   and some local memory - currently 1k bits. in addition  every 1 processors shares a floating point unit. all the processors are controlled by a single instruction stream  simd  broadcast from a microcontroller. figure 1 shows a block diagram of the connection machine. processors can communicate using a few different techniques - the only two of concern in this paper are the router and the scan operations. the router operations allow any processor to write into the memory or read from the memory of any other processor. the sean operations allow a quick1 summation of many values from different processors into a single processor  or the copying of a single value to many processors . 
in the implementation of back-propagation  we allocate one 
'usually fatter than a router cycle. 
1 	knowledge acquisition 

processor for each unit and two processors for each weight1. the processor for each unit is immediately followed by all the processors for it's outgoing  or fan-out  weights  and immediately preceded by all of it's incoming  or fan-in  weights  see figure 1 . the beginning and ending of these contiguous segments of processors are marked by flags. a $can operation called segmented copy-scan is used to quickly copy a value from the beginning or 
end of a segment of contiguous processors to all the other processors in the segment and an operation called segmented plus-scan is used to quickly sum all the values in a segment of processors and leave the result in either the first or last processor of the segment. thus our layout enables us to use the scan operations to distribute the unit values from the units to their output links and to sum the values from their input links. 
   the forward propagation step proceeds as follows. first  the activations of the input units are clamped according to the input text. the algorithm then distributes the activations to the weights using a copy-scan operation  step a in figure 1   and then all weights multiply their weight by these activations. the result is sent from the output weights to the input weights of the unit in the next layer  step b  using the router. a plus-scan then sums these input values into the next layer of units  step c   and the logistic function is applied to the result at all the unit processors to determine the unit activations. this forward propagation step must be applied once for each layer of links in the network. 
   once forward-propagation is completed  we determined the error at the output layer  and propagate this error backward. this error back-propagation consists of running copy-scan backwards  copying the deltas from the output units into their input weights  then sending the deltas from the input weights to the output weights of the units at the previous layer. the deltas can then be multiplied by the weight values  and summed in the reverse direction into the units at the previous layer. the derivative of the logistic function is then evaluated to determine the error of units at the previous layer. as in forward propagation  this step must be applied once for each layer of links. 
   the algorithm as described so far uses the processors inefficiently for two reasons. firstly  we use two processors for each weight when only one of them is busy at a time  and secondly  when we are running one layer  the processors for all the other layers are idle. to overcome the first inefficiency and use one pro-
   1 later in this paper we will show how the processors can be ahared between unite and weights  requiring only one processor per weight. 

cessor per weight  we overlap the input and output weights. we also overlap the units with the weights. to overcome the second problem and keep the processors for each layer busy we pipeline the layers as follows. given a set of n input vectors vi  1   i   n   and m layers /j  pipelining consists of propagating the iih input vector across the first layer  while propagating the previous input vector  vi-1  across the second layer  vi 1 across the third layer  ...   and vi-m across the last layer. we also interleave the backpropagation with the forward-propagation so that immediately after presenting vi  to the input  we start back-propagation vi m backward from the output. the depth of the whole pipe for m layers is 1m. 
   this implementation has some important advantages over other possible implementations. firstly  with pipelining  the implementation unwraps all the potential concurrency.1 since it is possible to simulate a concurrent computer with a serial computer but the opposite is not true  our method can be used efficiently on any machine ranging from a completely serial computer to a very fine grained parallel computer. if we did not expose all the concurrency  we would not utilize as many processors in a very fine grained computer. secondly  the implementation works well with sparse connectivity. methods based on dense matrix multiplies  such as some of the serial implementations  although faster for dense connectivity  are extremely inefficient with sparse connectivity. thirdly  in our implementation  all processors are kept active even if different units have different fan-ins. this would not be true if we used one processor per unit and serially looped over the fan-ins of each unit - as one might be tempted to do in a more coarse grained parallel computer. lastly  the time taken by each step is independent of the largest fan-in. 
   networks with more links than physical processors can be simulated in the connection machine using an abstraction called the virtual processor  vp  . a virtual processor is a slice of memory within a physical processor. many such vps can exist within each physical processor. looping over the vps in general causes a linear slow-down. 
   similar layouts of static networks have been used to implement a rule based system   a spice circuit simulator and a maximum-flow algorithm. 
1 c m - n e t t a l k 
nettalk is a connectionist network that uses back-propagation to learn the pronunciations of english words. we have implemented nettalk on the connection machine  and present the results of our implementation here1. 
   nettalk is is composed of three layers of units  an input layer  an output layer  and an intermediate or hidden layer. each unit in each layer is connected to each unit in the layer immediately above and/or below it. the representations at the input and output layers are fixed to be representations of letters and phonemes respectively. the representation at the hidden layer  on the other hand  is constructed automatically by back-propagation. 
   nettalk uses a fixed-size input window of seven letters to allow the textual context of three letters on either side of the current letter to be taken account in the determination of that letter's pronunciation  see figure 1 . this window is progressively stepped through the text. at each step  the output of the network generates its guess for the pronunciation of the middle  
1 this is not strictly true sinca we could get another factor of 1 by running the forward and backward propagation concurrently. 
1 interested readers should consult the original sources for details. 

or fourth  letter of the sequence of letters currently within the input window. this guess is compared to the correct pronunciation  and the values of the weights are iteratively adjusted using back-propagation to minimize this difference. good pronunciations  1% of the phonemes correct  of a thousand-word corpus of high-frequency words are typically achieved after ten passes through the corpus. 
   we have experimented with a network consisting of 1 input units  1 letters with 1 units each   1 hidden units and 1 output units. this required a total of 1 links  processors  - 1 in the first layer  1 in the second layer and 1 to the true units1. the learning rate was approximately the same as that achieved by a c implementation on various serial machines. 
   in the current implementation  using a 1 processor machine  the time required for each letter during the learning stage was 1 milliseconds. this includes the forward propagation  the backward propagation  the time necessary to clamp the input and output  and the time required to calculate the error. the time is broken up by the type of operation as follows: 
  scanning 1% - this includes two segmented plus-scans and two segmented copy-scans. 
  routing 1% - this includes two routing cycles. 
  arithmetic 1% - this includes seven multiplies and several additions  subtractions and comparisons. 
  clamping 1% - this is the time needed to get the characters to the input units and the expected phonemes to the output units. 
  other 1% - mostly for moving values around. 
with improvements in the coding of the implementation and in microcode  we expect that this time could be improved by a factor of three or more. 
   table 1 shows comparative running times of the error backpropagation algorithm for several machines. on an existing implementation of back-propagation on a vax 1  the same network required 1 microseconds per letter. this represents a 1 to 1 improvement in speed. on a 1k machine and larger networks  we could get a 1 to 1 improvement. this is about twice the speed of an implementation of the algorithm on a cray-1  yet a connection machine costs a quarter of the price. fanty  has 
   1 true units are units that are always kept iin the active state. their function is to allow the thresholds of the other units in the network to be modified in a simple way. 
	blelloch and rosenberg 	1 


figure 1: comparison of running times for various machines. 
mlps stands for millions of links per second. some of these times are from  1 1 . 
implemented a connectionist network using the bbn butterfly  a coarse grained parallel computer with up to 1 processors  but because the type of networks he used were considerably different  we cannot compare the performances. 
   using virtual processors  on the connection machine it is possible to simulate up to 1 million links in physical memory. with software currently being developed to use the connection machine's disks  the cm will be able to simulate many more than this. 
1 	conclusions 
we have discussed the first implementation of a connectionist learning network on a fine-grained parallel machine. our experiments indicate that the connection machine can currently simulate rather large networks of over 1 thousand connections at speed over twice as fast as the most powerful serial machines such as the cray-1. the method outlined here should generalize to any highly concurrent computer with a routing network and  with small modifications  can be used with many variations of connectionist network. unlike neuromorphic  hardware-based systems  our method places no restrictions on the computations performed at the links or the units  nor on the topology of the network. 
   in our implementation we were able to keep all of the processors busy most of the time using a single instruction stream; multiple instruction streams do not seem to be necessary. rather  communication was the bottleneck - at least on the current connection machine. effort needs to be spent designing faster routing networks. 
   the lack of computational power was a major factor in the dissolution of the first wave of connectionism in the 1's and 1's. alternative  symbolic techniques were more successful in part because they better fit the computational resource available at the time. with the advent of fine-grained parallel computers  this situation is beginning to change; the exploration of large-scale connectionist networks is starting to become computationally feasible. 
