 
many types of problem exhibit a phase transition as a problem parameter is varied  from a region where most problems are easy and soluble to a region where most problems are easy but insoluble. in the intervening phase transition region  the median problem difficulty is greatest. however  occasional exceptionally hard problems  ehps  can be found in the easy and soluble region; these problems can be much harder than any problem occurring in the phase transition. we show that  in binary constraint satisfaction problems  ehps are much more likely to occur when the constraints are sparse than when they are dense. ehps occur when the search algorithm encounters a large insoluble subproblem at an early stage; the exceptional difficulty is due to the cost of searching the subproblem to prove insolubility. this cost can be dramatically reduced by using conflict-directed backjumping  cbj  rather than a chronological backtracker. however  when used with forward checking and the fail-first heuristic  it is only on ehps that cbj gives great savings over backtracking chronologically. 
1 	introduction 
it has been observed by several authors in recent years  beginning with cheeseman  kanefsky and taylor  cheeseman et al  1   that for many types of problem where a large population of problem instances can be examined  there is a phase transition as a problem parameter is varied. the phase transition is from a region where almost all problems have many solutions  and are relatively easy to solve  to a region where almost all problems have no solution  and are relatively easy to prove insoluble. the intervening region  where the 
   * stuart grant is partly supported by a studentship from british telecom ple. 
1 
probability that a problem is soluble falls from close to 
1 to close to 1  is termed the mushy region  smith  1; smith and dyer  1 ; as the problem size increases  the mushy region becomes increasingly narrow  and in the limit there is an instantaneous phase transition from solubility to insolubility. within the mushy region  problems are on average hard to solve  or prove insoluble: it has been observed experimentally that the peak in average difficulty occurs at the crossover point  crawford and auton  1   where the probability that a problem is soluble is 1. 
　it has been observed by  hogg and williams  1  in graph colouring problems and by  gent and walsh  1  in satisfiability problems  that although there is a welldefined peak in the median cost to find a solution in the region of the phase transition  this is often not where the hardest individual instances occur. given a large sample of problems  individual problems which are very hard to solve may occur in the region where most problems are relatively easy to solve. these problems may be so hard that their cost significantly affects the value of the mean cost; it is for this reason that authors reporting phase transition behaviour have often used the median rather than the mean as a measure of average difficulty. 
　this paper presents experimental investigations into these exceptionally hard problems occurring in the easy region  in the case of binary constraint satisfaction problems  csps . in this paper  a problem instance is said to be an exceptionally hard problem  ehp  for a particular search algorithm if: 
1. it occurs in the region where almost all problems are soluble  and on average easy to solve  that is  outside the mushy region ; 
1. it is much more difficult  by at least an order of magnitude  than almost all other problems with the same parameter values; 
1. it is more difficult than almost all the problems occurring in the mushy region. 
　this is intended to be a description of ehps  rather than a precise definition. as will be seen  the individual ehps that we have found are highly dependent on 

the algorithm being used: even a minor change in variable instantiation order may convert an ehp into a much easier problem. so although individual ehps  and what makes them so difficult for a particular algorithm  should be investigated  we can also ask about ehps in relation to populations of problems and in relation to search algorithms: for instance  do ehps occur in all populations of problems  and are some search algorithms more susceptible to ehps than others  
1 	the random generation model 
the experiments reported here were done using randomly-generated binary csps. each set of problems is characterized by four parameters: n  the number of variables; m  the number of values in each variable's domain; p   the proportion of pairs of variables which have a constraint between them  i.e. the constraint density ; and p1  the proportion of pairs of values which are inconsistent for a pair of variables if there is a constraint between them  i.e. the constraint tightness . 
　when constructing the constraint graph  we choose p n n - l /1 of the possible variable pairs at. random; for each selected pair there will be a constraint between the corresponding variables. for each constrained pair of variables  we choose m1 of the possible pairs of values  at random  to be the pairs forbidden by this con-
straint. the phase transition from under-constrained to over-constrained problems is observed as p1 varies  while n  rn and p1 arc kept fixed; below  sets of randomlygenerated problems with fixed n  m and p1 and varying p1 will be referred to by the tuple  n m p1 . 
　many of the experiments described later involve sparse constraint graphs  i.e. with small p1 . the problem generator described above will then produce a proportion of disconnected graphs. since a problem with a disconnected graph would in practice be dealt with by solving the subproblems separately  we did not include disconnected graphs in our samples. if the problem generator produced such a graph  it was rejected and a new graph generated  until a connected graph was found. 
　for most of these experiments  the problems were solved using the forward checking algorithm with a variable ordering heuristic using the fail-first principle: the first variable to be instantiated is the one which is most constrained  and thereafter  the next variable to be instantiated is one with fewest remaining values in its domain. 
1 	where do ehps occur  
figure 1 shows the results of a set of experiments with n 
- 1 and m - 1 and the constraint density  p1  equal to 1  1  1  1 or 1.1 for each value of p1 a range of values of p1 was chosen  so as to cover the crossover point 
　　1 we did not consider p  =1  as we did in later experiments with n = 1  because  as already described  the constraint graph had to be connected  and connected graphs 

figure 1: randomly-generated binary csps with n = 1 and m - 1: 1 samples at each data point 
and the region of increasing average difficulty leading up to it. in order to have a reasonable chance of seeing the extremes of behaviour  1 problems were generated at each set of values of the four parameters. the cost of solving each problem was measured by the number of consistency checks required to either find one solution or prove the problem insoluble. 
　following the graphs shown by  hogg and williams  1  and  gent and walsh  1   figure 1 shows a number of higher percentiles as well as the median for each set of 1 problems. the behaviour of the median as p1 decreases is as shown in prosser's extensive series of experiments  prosser  1 : the peak median cost coincides with the crossover point  in each case  but as the constraint graph becomes less dense  the phase transition becomes less sharp. when p  - 1  all the percentile curves are close together; as p1 decreases  they become more widely separated  and the maximum cost begins to behave erratically. to the left of the peak  when p1 - 1  the maximum cost is sometimes two orders of magnitude higher than the median. 
　it is noteworthy that although  in general  as p1 decreases  each succeeding phase transition gives rise to easier problems  the single most difficult problem in the whole set of experiments occurs when p1 = 1  shown by the 'spike' in the maximum when p1 = 1. this problem is insoluble. it is not an ehp by the criteria given earlier  since it is not in the region where most problems are easy: there are  however  ehps when p1 = 1 and 1  which are both soluble problems. the most difficult problem occurring when p1 = 1  at p1 = 1  might also be classed as an ehp  although it is on the edge of the mushy region. 
　these experiments suggest that ehps are not a universal phenomenon of randomly-generated csps  but occur 
with 1 nodes and 1 edges are not very interesting for our purposes. 

only in sparse problems. since it is conceivable that ehps would be found in denser problems if the sample sizes were sufficiently large  we have carried out further experiments with  1 1  problems  generating up to 
1 problems for each value of p1; no ehps were found  the results are shown in  smith and grant  1  . we have also examined larger dense problems  {1 1  and  1 1   and found no ehps. if these are typical  then at the least  ehps in dense problems must be extremely rare compared with those in sparse problems. 
1 	larger sparse problems 
as noted in the last section  the phase transition is less sharp in sparse problems than in dense problems  given the same values of n and m. it might be suspected therefore that the ehps seen in the  1 1  problems were a side-effect of this  and that in larger problems  as the phase transition becomes more abrupt  ehps would tend to disappear. figure 1 shows the results of experiments with larger sparse problems: again  1 problems were generated for each data point.1 as ex-

figure 1:  1 1  csps: 1 samples at each data point 
pected  the  1 1  problems show a much sharper peak in the median than the  1 1  problems. the increase in n from 1 to 1 has caused the phase transition to become more abrupt  even though the density is lower. however  far from disappearing  ehps are if anything more common  and are more extreme  than in the smaller problems. 
one difference between the set of problems shown in 
figure 1 and  say  the  1 1  problems of figure 
1 is that the former includes many different constraint graphs  whereas all the  1 1  problems have the 
1 figure 1 is therefore based on solving 1 individual 
csps; it takes hours to solve some of the worst individual problems  on a sparcstation ipx  using c  and altogether figure 1 represents well over 1 hours of cpu time. 
1 
same constraint graph. we considered whether the occurrence of ehps depends on the topology of the constraint graph. this seemed possible  because it has been shown in  smith and dyer  1  that the location of the phase transition in randomly-generated sparse csps depends on the constraint graph: the more irregular the constraint graph  the lower the value of p1 at which the phase transition occurs. for example  it might be that some constraint graphs give rise to problems which are very susceptible to ehps  whereas others produce more homogeneous behaviour  similar to the denser constraint graphs of figure 1. the same set of 1 constraint graphs was used at each value of p1 in figure 1. we selected the most regular  graph 1  and the most irregular  graph 1   and used each graph as the basis of two new sets of problems  again with 1 instances generated at each value of p1 all the problems in one set had constraint graph 1 and all the other set had graph 1. 
　the results for the two problem sets are shown in figure 1. as expected on the basis of previous experience  the peak in the median cost occurs at a higher value of p1 for the more regular graph than for the more irregular graph. from the higher percentiles  it can be seen that the irregular constraint graph gives more variable behaviour over the phase transition than does the regular graph. however  it is hard to see any significant difference in the behaviour of the maximum before the phase transition in the two graphs: both graphs seem more or less equally susceptible to ehps. 
　we also took one of the ehps from figure 1  that occurring at p1 = 1  with graph 1  and generated a set of problems with this constraint graph. if any randomlygenerated constraint graph is likely to produce more ehps than others  a graph which has already produced an ehp should be a good choice. graph 1 does not appear to be exceptional in any way  and solving the problems produced similar results to figure 1  shown in  smith and grant  1    so that once again  it does not appear to be especially susceptible to ehps. 
　we have not  therefore  found any randomly-generated sparse constraint graphs which do not produce ehps  nor any which appear to be more likely than others to give rise to them. a more thorough investigation  considering a wide range of constraint graphs and much larger sample sizes  might show that the incidence of ehps is higher for some constraint graphs than for others  but since ehps are by definition rare and difficult to solve  this would be a daunting task. 
1 	anatomy of an ehp 
to understand better the causes of ehps  we examined carefully the three ehps shown in figure 1  i.e. the most difficult problems at p1 = 1  1 and 1. for these values  the median cost is about 1 consistency checks; the easiest of the three ehps takes more than 1 

million consistency checks. these values of p1 are well outside the mushy region  and all three problems have solutions. why  then  are they so difficult to solve  
　one of these problems is problem 1 at p1 = 1. the first four instantiations made by the forward checking algorithm are v1 = l 1 vl1 = 1  v1 - 1  v1 - 1. it  eventually  becomes clear that this set of assignments leads to an insoluble subproblem. however  proving insolubility takes more than 1 million consistency checks; the algorithm frequently finds partial solutions with 1 or more variables instantiated before detecting an infeasibility and backtracking. once it has been proved that there is no solution to the subproblem  the alternative assignment of v1 = 1 is tried and leads almost immediately to a solution. since only one possible instantiation of the first variable  v1  has been tried  it is very likely that this problem has many solutions. 
　in the other two ehps  similarly  the first few assignments lead to a subproblem which has no solutions  and almost all the search effort is expended in proving 
1
i.e. variable 1 is assigned the value 1. 
this. partial solutions involving most of the variables are found in the course of searching the subproblem  resulting in a great deal of backtracking. as with problem 1  the solutions eventually found for these two problems have the first variable assigned its first value  so that there are very probably many solutions in other areas of the search space. we have seen the same kind of behaviour in other ehps which we have found. 
　these results are very similar to the experience reported by  gent and walsh  1 ; they found a satisfiability problem which required more than 1 million branches to solve  using a simplified version of the davis-putnam algorithm. the first choice made by the algorithm led to a very difficult unsatisfiable problem  which required almost all the search effort; the alternative choice led immediately to a solution. they conclude that:  these difficult problems are either hard unsatisfiable problems or are satisfiable problems which give a hard unsatisfiable subproblem following a wrong split.  
　it appears that  in csps also  ehps are problems in which the search algorithm gets into a hard insoluble subproblem early in the search. all the ehps that we have seen in these experiments are themselves soluble; if an insoluble problem were to occur at these values of p1 it would be extremely hard to prove insoluble  since a complete exploration of the search space is required. however  it seems that ehps of this type are exceptional even amongst ehps. 
1 	can ehps be avoided  
there are  in theory  two potential ways of avoiding ehps which arise through encountering insoluble subproblems early in the search: one is to avoid getting into such subproblems in the first place  and the other is to find some search algorithm which can detect that the subproblem is insoluble more quickly. 
　in the problems we have considered  clearly the insoluble subproblem would have been avoided if a different instantiation order had been chosen  or if a different value had been chosen for one of the variables. however  it seems unlikely that different ordering heuristics could eliminate ehps altogether: they might avoid particular instances that the fail-first heuristic encounters  and so find those problems easy to solve  but might then meet insoluble subproblems in other problems that the fail-first heuristic would avoid. 
　the alternative is to search the insoluble subproblem more quickly. an obvious candidate for consideration is some kind of informed backtracking  rather than chronological backtracking as in the basic forward checking algorithm.  prosser  1  describes an informed backtracking algorithm  conflict-directed backjumping  cbj   and shows that it can be combined with forward checking  fc  to produce a new search algorithm  fc-cbj. in his experiments with the zebra problem  he showed that fc-cbj was the best of the algorithms 

that he considered  including plain fc: on individual instances  fc-cbj almost always did better than fc and was never worse. 
　we have used both algorithms in combination with fail-first  as described earlier: these will be termed fcff and fc-cbj-ff. figure 1 shows the results of applying fc-cbj-ff to the problems which were previously solved using fc-ff to produce figure 1. it is evident that cbj does significantly reduce the difficulty of ehps for this population of problems: the most difficult problem in the easy region  at p1 = 1  takes 1 million consistency checks  which is much less than the three ehps from figure 1 considered earlier. 
　problem 1 at p1 = 1 and problem 1 at p1 = 1 were ehps for fc-ff because the first four variable instantiations led to an insoluble subproblem: since we are using the same variable and value ordering heuristics with fc-cbj-ff  the same subproblems are encountered. these problems are not ehps for fc-cbjff precisely because the algorithm can detect that the subproblem has no solutions much more quickly than chronological backtracking can. there is a vestige of the earlier difficulty with problem 1: fc-cbj-ff takes more than 1 million consistency checks to prove insolubility and this is one of the most difficult problems at that value of p1- problem 1  on the other hand  which was the most difficult of all the problems plotted in figure 1  taking more than 1 consistency checks  succumbs to cbj quite quickly: the subproblem is proved insoluble in only 1 consistency checks. 
　hence  using an informed backtracker such as cbj instead of chronological backtracking moderates the difficulty of ehps  but does not eliminate them altogether. it remains an open question whether any algorithm will be able to eliminate ehps entirely from the populations of sparse csps that we have considered. 
1 
1 	the benefits of cbj 
if figures 1 and 1 are compared  it can be seen that they are very similar  apart from the 1th and 1th percentiles. this suggests that cbj's biggest effect is on the most difficult problems  and that its performance is otherwise similar to chronological backtracking  when combined with fc and ff. 
　figure 1 shows the cost of solving a problem using fcff compared with the cost using fc-cbj-ff when p1 = 
1  where fc-ff encounters one of the ehps considered earlier  problem 1 . the 1 problems which were generated with these parameter values for figures 1 and 1 are plotted as individual points in figure 1  although most of them cannot be distinguished. the median cost is 1 consistency checks for fc-cbj-ff  1 for fc-ff  and for well over half the problems  the performance of fc-cbj-ff is not very different from that of fc-ff. it is only for the 1 or so most difficult problems that there is a dramatic difference in cost between the two algorithms. similar plots for other values of p1 confirm that fc-cbj-ff does not offer great savings over fc-ff except for the most difficult problems occurring before the phase transition. 
　these findings to some extent contradict prosser's; he found that on average fc required about 1 times as many consistency checks as fc-cbj. however  prosser used a fixed instantiation order  rather than the dynamic order given by ff. it appears that most of the benefit of using cbj in conjunction with fc can be obtained more simply by using the fail-first heuristic  perhaps because  for most problems  the ordering given by fail-first ensures that chronological backtracking usually results in backtracking to the real culprit for a failure  so that informed backtracking does not add very much. since cbj carries an additional overhead  which is not reflected in the count of consistency checks 1 it may be that for some 
1  we have found that fc-ff can do about 1% more con-

regions of the parameter space fc-ff would be a better choice. for instance  we have compared the two algorithms on the problems shown in figure 1  and for p1   1  the number of consistency checks is identical in almost every instance. however  for sparse problems  fc-cbj-ff does offer a great benefit in overcoming the most difficult problems in the region where most problems are easy and soluble. 
1 	discussion 
we have shown that when the constraint density is high  there is much less variation in problem difficulty at a given value of the constraint tightness than when the constraints are sparser. as a corollary of this  we have found no instances of ehps except at low constraint densities  from samples of up to 1 problems with each set of parameter values. whether ehps might arise even when the constraints are dense  given sufficiently large sample sizes  remains an open question. in sparse problems  the incidence of ehps does not seem to depend greatly on the constraint graph  although it is difficult to be sure from the small numbers of ehps that we have found. 
　it should be noted that our graphs exhibiting ehps show no sign of the double peak in the higher percentiles found by  hogg and williams  1 . however  their experiments  on 1-colouring problems  used far larger samples than ours; they had between 1 and 1 million samples for each data point  and were thus able to see smooth behaviour in the 1 percentile. we have increased our sample size to 1  at considerable cost in cpu time  without seeing any sign of a double peak  but if it were feasible to solve 1 million samples in cases such as the  1  1.1  problems considered earlier  the double peak might then appear. 
we have shown that adding an informed backtracker 
 cbj  to forward checking and the fail-first heuristic can make a huge difference to the difficulty of ehps; cbj allows the algorithm to search insoluble subproblems much more quickly than a chronological backtracker can do. ehps thereby become much less significant  but do still occur. 
　however  except for the hard problems in the easy region  and in particular the ehps  cbj does not give great savings over chronological backtracking  when applied to sparse problems. with dense problems  our experiments suggest that it is not worthwhile to use cbj at all  in conjunction with fc-ff. 
　it should be remembered  however  that all our experiments are based on random problems generated by the model described earlier. problems with more structured constraint graphs  varying domain sizes and/or varying constraint tightness might well behave differently. how each of these factors might affect the incidence of ehps  sistency checks per second than fc-cbj-ff. 
or the performance of the algorithms we have considered  has yet to be explored. 
　this paper illustrates that the full story of the relative performance of two algorithms needs to be based on a wide range of problems  including the extremes of problem difficulty. judging by the median cost alone  fc-ff is at most 1% more expensive than fc-cbj-ff  on the  1 1  problems. however  on some of the hard problems  the performance of fc-cbj-ff is orders of magnitude better than fc-ff; this would be sufficient to give a considerable difference in the mean cost of the two algorithms at these points. in order to be able to design a comprehensive set of experiments comparing different csp algorithms  it is therefore essential to consider phase transition behaviour  and  in particular  the existence of occasional exceptionally hard problems. 
