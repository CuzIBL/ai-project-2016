 
recent results in induction theory are reviewed that demonstrate the general adequacy of the i n -
duction system of solomoncff and w i l l i s . several problems in pattern recognition and a . i . are i n vestigated through these methods. the theory is used to obtain the a p r i o r i p r o b a b i l i t i e s that are necessary in the a p p l i c a t i o n cf stochastic languages to pattern recognition. a simple  q u a n t i t a t i v e s o l u t i o n is presented for part of winston's problem of learning s t r u c t u r a l descriptions from examples. in contrast to work in non-probabilist i c p r e d i c t i o n   the present methods give probab i l i t y values t h a t can be used with decision. theory to make c r i t i c a l decisions. 
introduction 
　　the kind of induction theory that we w i l l consider rray be regarded as a bayesian method in which the a p r i o r i p r o b a b i l i t y of a hypothesis is related to the shortest descriptions of that hypothesis that are obtainable by programming a reference universal turing machine. 
　　the p r o b a b i l i t y values obtained are o r d i n a r i l y not e f f e c t i v e l y computable. they car  become e f f e c t i v e l y computable if we make c e r t a i n reasonable r e s t r i c t i o n s on the source cf the data  but in either case they do not appear to be p r a c t i c a l ly calculable. however  various approximation methods r e a d i l y suggest themselves  and indeed  a l l known methods of obtaining p r o b a b i l i t y e s t i mates may be regard&d as approximations to the idealized method and they car be compared and c r i t i c i z e d on t h i s basis. 
　　several problems in patterr recognition and a . i . w i l l be discussed with respect  to t h i s general formulation of induction. 
　　induction and pattern r e c o g n i t i o n through stochastic grammar construction is discussed in a general way. the best wcrk in t h i s area involves a bayesian analysis and the presert induction theories t e l l how to obtain the necessary a p r i o r i p r o b a b i l i t i e s of various grammars. 
　　next we discuss in scmo det.eil part of winston's program for learning s t r u c t u r a l descriptions from exampler. a simple vector model of the problem is described and various q u a n t i t a t i v e results are readily obtained. these agree f o r the 
　　this research was sponsored in part by the advanced research projects agency of the department of defense urder office of naval research cont r a c t s n1-o1 and m1o-a-1; usaf contract af-1 -1; afosr cont r a c t af-1 -1  grant af-afosr 1. and public health service nih grant gm 1. 
most part with winston's q u a l i t a t i v e h e u r i s t i c discussions of the r e l a t i v e likelihoods of various models of concepts  but the results are obtained very d i r e c t l y and do not involve tree search of any kind. 
　　in addition to resolving many otherwise ambiquous decisions  the q u a n t i t a t i v e p r o b a b i l i t i e s obtained enable ue to use decision theory to make c r i t i c a l decisions as in the mechanization of medical diagnosis. 
	1. 	recent work in induction 
　　we w i l l present some recent results in the theory of inductive inference - discussing what is possible and what is not possible. 
　　for the purposes of t h i s discussion  the problem of inductive inference w i l l be the e x t r a polation of a sequence of symbols emitted by an unknown stochastic source. it is not d i f f i c u l t to show that almost a l l   if not a l l problems usually regarded as i n d u c t i o n   can be expressed in t h i s form. discovery of multidimensional p a t t e r n s   curve f i t t i n g   time series extrapolation and weather p r e d i c t i o n are but a few of the kinds of problems that can be readily dealt w i t h . 
　　　although induction has always been the most important thing going on in science and has cons t i t u t e d a large part of the study of the p h i l o sophy of science  there has not  u n t i l r e c e n t l y   
beer a rigorous formulation of the process with a clear understanding of the expected errors involved. we w i l l discuss these recent results and what they imply about what is possible  impossible and approximatable. 
　recent work in induction has centered about the concept of the   d e s c r i p t i o n   of a s t r i n g of symbols. 	a   d e s c r i p t i o n   cf a s t r i n g with respect to a p a r t i c u l a r reference computer is an input to that computer that gives the described s t r i n g as output. 	solomonoff  1  used the lengths of short 
descriptions of a s t r i n g and i t s possible c o n t i n u ations to define the a p r i o r i p r o b a b i l i t y of t h a t s t r i n g . bayes' theorem was then used to find the p r c b a b i l i t y of any p a r t i c u l a r continuation of the s t r i n g . he also showed that using a universal turing machine f o r reference made the a p r i o r i p r o b a b i l i t i e s r e l a t i v e l y i n s e n s i t i v e to choice of reference computer. 

1 

other descriptions c a n ' t be ignored. 
　　later  kolmogorov  1  and chaitin  1  proposed that a random sequence be defined to be one whose shortest d e s c r i p t i o n w i t h respect to a universal turing machire is about the same length as the sequence i t s e l f . martin lof   1     loveland  1  and schnorr  1  continued work on randomness d e f i n i t i o n s . for a review of t h i s work as well as subsequent research in the soviet union  see zvormn and levin  1 . 
　　more r e c e n t l y   c h a i t i n  1  has proposed expressions for entropies of sequences based on descriptions that form a p r e f i x set. the expressions  however  have error terms that do not occur in the e a r l i e r   more exact formulation of w i l l i s  1 . 
　　w i l l i s refined solomonoff's model and overcame several d i f f i c u l t i e s i n i t . the theorems i n the present paper usually f o l l o w d i r e c t l y from his work. because of t h e   h a l t i n g problem' * it is often impossible to t e l l if one s t r i n g is a d e s c r i p t i o n of another with respect to a specific machine. w i l l i s dealt with t h i s problem by considering an i n f i n i t e sequence of machines  each more powerful than the l a s t   but a l l of them s u f f i c i e n t l y l i m i t e d so that they have no   h a l t i n g problem.  associated with each of these machines is a computable p r o b a b i l i t y assigned to the s t r i n g 1r question. 
　　one sequence of such machines is obtainable by considering a universal 1 tape turing machine with u n i d i r e c t i o n input and output tapes and a b i d i r e c t i o n a l working tape. the tth machine in the sequence is obtained by stopping the universal machine a f t e r t steps   i f it has not already stopped . it is not d i f f i c u l t to show t h a t the sequence of p r o b a b i l i t i e s obtained by these machines approaches a l i m i t as t approaches i n f i n i t y   bt:t that the l i m i t is not e f f e c t i v e l y computable. 
　　suppose that a   m   is a s t r i n g of length m  and that we have a stochastic generator that assigns a p r o b a b i l i t y p a  m    to a   m   . suppose t h i s generator is describable by a f i n i t e s t r i n g b b i t s in length. then for s u f f i c i e n t l y powerful r e f e r ence machines  

　　this expression is the ratio of the products of the conditional probabilities for the nth symbols. 
　　if we want to know the mean error in this ratio we take the mth root and obtain  it is clear that it must approach unity as m becomes very large. 
　　　although this is a very powerful result and gives real assurance t h a t c o n v e r g e s very rapidly as m increases  it lacks a certain 

i n t u i t i v e appeal. one asks whether it could not be possible that the ratio might b e   t l for some factors and  1 for others. be close to unity the individual deviations could 
1 

　　a few objections suggest themselves. f i r s t   t h i s error is unreasonably smaller than those obtained in ordinary s t a t i s t i c a l analysis. for a simple bernoulli seqvence  the expected squared error in the mth symbol is proportional to i  and 

　　the reason f o r t h i s discrepancy is t h a t we have assumed that the stochastic source had a f i n i t e 
　　d e s c r i p t i o n . if the source consisted of zeros and onet with p r o b a b i l i t i e s r e s p e c t i v e l y   we 
could  indeed have a f i r l t e d e s c r i p t i o n - perhaps of the order of 1 cr 1 b i t s . o r d i n a r i l y in s t a t i s t i c s   however  the stochastic sources are describable as continuous functions of a f i n i t e number of parameters. the parameters themselves each have i n f i n i t e l y long d e s c r i p t i o n s . 
　　for situations of t h i s s o r t   the expected t o t a l squared error i s b o u n d e d   but i s roughly proportional to the of the sequence length as in conventional s t a t i s t i c a l analysis. the error i t s e l f   however  approaches zero. 
　　if there are k d i f f e r e n t parameters in the model  the expected t o t a l squared error w i l l be bounded by 

here  m is the number of symbols in the s t r i n g being described  a is a constant that is characte r i s t i c of the accuracy of the model and b is the number of b i t s in -the d e s c r i p t i o n of the expression containing the k parameters. 
　　if we are comparing several d i f f e r e n t models and we have much data   i . e . large m   then the 'v terms w i l l be of l i t t l e significance and we need only compare the corresponding ak values to 
determine the best model. 
　　a technique very similar to t h i s has been successfully used by akaike  1  to determine the optimum tuimber of parameters to use in l i n e a r regression analysis. the present methods are  however  usually of most interest -when the amount of d i r e c t l y relevant data is r e l a t i v e l y small. 
	another objection is that y 	is incomputable. 
this makes it impossible to calculate it exactly. is there not a better s o l u t i o n possible  we can obtain progressively more computable solutions by making more and wore r e s t r i c t i o n s on the storhastic source. 
　　if there are no r e s t r i c t i o n s at a l l   there is no p r e d i c t i o n at a l l possible. if we r e s t r i c t th  stochastic source only to be f i n i t e l y describable then it is w e l l known that there can be no effecti v e l y computable s o l u t i o n s   though as we have seen  there is a ncr e f f e c t i v e l y computable 
solution. 
　　if we restrict the  computational complexity  of the stochastic source  sc that we have an upper 
bcund on how long it takes the source to compute the probability of a sequence of length m  then the probabilities are  indeed computable  and they converge as rapidly as they do in incomputable cases. they are  however  no mere practically computable than the incomputable solutions. 
　　in any case  we must use approximation method;;. it is clear that the incomputable method described converges very rapidlj - it is likely that it has less error for a given amount cf data than any other probability evaluation method  and so we will do well to try to approximate i t . 
　　in most cases  our approximations consist cf finding not the shortest description of the string of interest  this  too  being incomputable   but rather the set of the shortest descriptions that we can find with the resources available to us. all of the methods ordinarily used tr estimate probability by finding regularities in data can be expressed in the form of discoveries of* short 
descriptions cf the data and mnerefore arp approximations to the ideal method. in this common format  various approximations car be compared  so we can select the best ones - the ones most likely 
to give good predictions. 
i i . application to pattern reconniiaon stochastic language 
　　to illustrate the problem  suppose we are given a set of strings of symbols  such as aa.abba  aabbaa  baaabbaaab etc.  and we are told that these strings were acceptable sentences in some simple formal language. we are required to find a grammar that could generate these strings. 
　　in general  there will be an infinite nuitber of grammars that can generate the set even if we restrict the grammars  say to be finite state or to be context free grammars. early investigators proposed that  some criterion of  simplicity  be imposed on the grammar  and various explications of this concept were devised  but with no basis for preferring one explication over any other. 
　　by assuming the set of strings was produced by some unknown stochastic generator  and using some of the previously described methods to give an a priori probability distribution over all such generators  a very general solution to this problem is obtained   and the concept of  simplicity  is unnecessary. a stochastic language is an assignment of probabilities to all strings being considered. a stochastic generator or grammar is a means for carrying out this assignment. 
　　one very general form of stochastic grammar consists of a turing machine. one inserts the string into the machine and it prints out the probability of thet string. many of the strings may have zero probability assigned to therr. 
　　another very general form of stochastic grammar is a generative grammar. again we have a turing machine  but we insert a random strive of zeros and ones. the output strings of this machine then have the probability distribution associated with 
1 

the desired stochastic language. 
　　to -use stochastic languages to solve induction problems  such as the one of guessing the grammar that produced a given set of s t r i n g s   we f i r s t assume an a p r i o r i d i s t r i b u t i o n on a l l possible stochastic languages. if p i is the a p r i o r i p r o b a b i l i t y of the i t h stochastic language  and l i j i s the p r o b a b i l i t y t h a t the i t h language w i l l produce the j t h sample s t r i n g  there being m sample s t r i n g s     then the rrost l i k e l y grammar is the one for which 

is maximum. 
　　often we are not interested in knowing which grammar produced the set of s t r i n g s   we only want to know the p r o b a b i l i t y t h a t a p a r t i c u l a r new s t r i n g is in the set - t h i s new s t r i n g being the m + 1th. 	a bayesian analysis gives us 

for t h i s p r o b a b i l i t y   the suimation being taken over a l l grammars for which p i   1. 
　　the a p r i o r i p r a b a b i l i t y of a language corresponds roughly to the e a r l i e r concept of s i m p l i c i t y   but it is a more c l e a r l y defined q u a n t i t y . 
   to generate a stochastic grammar from a nonstochastic generative grammar is usually very easy. in the generative grammar  at each point in the construction of the f i n a l o b j e c t   there w i l l be choices to be made. if we assign p r o b a b i l i t i e s 
to each of these choices  we have a stochastic generative grammar. the p r o b a b i l i t y of any p a r t i c u l a r d e r i v a t i o n w i l l be the product of the p r o b a b i l i t i e s of the choices involved in that d e r i v a t i o n . if the language is ambiguous  some objects w i l l be derivable in more than one way  and the p r o b a b i l i t i e s of each of these derivations must be added to obtain the t o t a l p r o b a b i l i t y of the f i n a l object-
　　many d i f f e r e n t kinds of grammars have been dev sod f o r various problem areas  1 . in p a r t i c u l a r   they have been used for two dimensional scene analysis  r e c o g n i t i o n of handwritten characters  chromosome pattern r e c o g n i t i o n   recognition of spoken words  etc. 
　　the basic model that stochastic grammars propose is a very a t t r a c t i v e one. 	it assumes that the sample set was created by some sort of mechanism and that the mnrhanism had various probabil i s t i c elements. 	it enables us to put intc the model any information that we have about the problem  	either d e t e r m i n i s t i c or p r o b a b i l i s t i c . for some time  however  the assignment of a p r i o r i 	p r o b a b i l i t i e s t o the d i f f e r e n t possible mechanisms was a problem of uncertain s o l u t i o n . 
　　induction theory made an important breakthrough by providing a gereral method to assign p r o b a b i l i -
t i e s tc these mechanisms  whether the. assignment is to be purely a p r i c r i or whether they are dependent in any way on available information. 
　　if the p r o b a b i l i t y is purely a p r i o r i   one method of p r o b a b i l i t y assignment proceeds by w r i t i n g out a minimal d e s c r i p t i o n of the nonstochastic grammar from which the stochastic grammar is derived. the d e t a i l s of how t h i s is done for a kind of f i n i t e state grammar and f o r a general context free grammar are given in ref. 1  pp. 1. 
　　if there is some data available on the r e l a t i v e frequencies with which the p r i m i t i v e concepts have been used in the past f o r other induction problems  t h i s information can be used to assign i n i t i a l   b i t c o s t s   to these concepts when constructing new grammars. 
i i i . 	a p p l i c a t i o n t o a . i . learning s t r u c t u r a l descriptions from examples 
　　winston's program f o r learning various s t r u c t u r a l concepts from both p o s i t i v e and c a r e f u l l y chosen negative examples of those concepts  1  is perhaps the most competent induction program y e t completed. 
　　the program does much more than l e a r n concepts  but we s h a l l discuss only t h i s p a r t i cular part of the program. it begins w i t h a l i n e 
drawing of three dimensional objects. this cons i s t s of one or more objects in various p o s i t i o n s   having various r e l a t i o n s w i t h respect to one another. there is a preprocessing program t h a t translates t h i s drawing i n t o a d e s c r i p t i o n t h a t is in the form of a net. 
　　the nodes of the net are objects in the drawing  properties of objects and classes of objects. 	there are arrows connecting the nodes t h a t denote r e l a t i o n s between them. 
　　for an example  suppose the o r i g i n a l drawing pictured a cube on the l e f t and a v e r t i c a l b r i c k on the r i g h t . a possible net describing t h i s scene would have four nodes: a b r i c k ; a cube; the property   standing  and the class of s o l i d s    prism . the b r i c k and cube are connected by an arrow labelled  to the r i g h t of . the b r i c k has an arrow l a b e l l e d  has the property o f   connecting it to  standing . both the brick and cube have arrows going to prism l a b e l l e d   a - k i n d - o f     i n d i cating class i n c l u s i o n . 
　　after being shown several scenes t h a t are given as p o s i t i v e   and several as negative examples of a c e r t a i n concept  such as a t a b l e   the program t r i e s to induce the concept by making a model of i t . these models consist of networks s i m i l a r to those used to describe scenes  but the nodes and arrows of the net are usually classes of objects  classes of properties and classes of r e l a t i o n s . these classes may sometimes be expressed as negations  e.g.  not a b r i c k     or   i s not to the l e f t of . 
　　for purposes of comparing scenes to one another and scenes to models  we w i l l consider a scene 

1 

d e s c r i p t i o n to be an ordered sequence of objects and r e l a t i o n s - a vector whose components are a mixture of objects and r e l a t i o n s . a  model  is a vector whose components are classes.* formalized in t h i s way  it is clear that given a set of p o s i t i v e and negative examples  there w i l l o r d i n a r i l y 
be an enormous number of models such that 
　　1. in each of the p o s i t i v e examples  a l l of the components are members of the corresponding classes in the model. 
　　1  in each of the negative examples  at least one component is not a member of the corresponding class in the model. 
　　winston has devised a system f o r ordering the models  so that a f t e r each example is given  it picks the  best  model that is consistent with the data thus far - i . e . highest in the ordering. his ordering of models is very close to that of a p r i o r i p r o b a b i l i t i e s as calculated by induction theory. 
　　tn one case we are given as a p o s i t i v e example  a b r i c k that is standing on one end. the negat i v e example is a b r i c k l y i n g on one side. the classes that are considered for the  property  component of the vector of the model are: 
1  standing 
1  not l y i n g 
winston notes that since most concepts are defined in terms of properties rather than anti-propert i e s    standing  is more l i k e l y . in the language of induction theory  positive concepts that have names in the language are usually of high a p r i o r i p r o b a b i l i t y . a negative concept consists of a p o s i t i v e concept with a negation symbol which increases description length and decreases a p r i o r i p r o b a b i l i t y . if a negative concept is of much u t i l i t y in d e s c r i p t i o n   and of high a p r i o r i p r o b a b i l i t y   it w i l l have been useful to define a special word for i t   e.g.   d i r t y   is the word for  not c l e a n .   reference 1  pp. 1 treats the problem of when it is worth while to define new symbols. 
　　another reason why  standing  is a better choice than  not l y i n g   is that  standing  probably has fewer members. this is brought out more c l e a r l y in the next example. 
　　here we have two positive cases; 	1  	apple 1  	orange. 	three classes for the model are considered. 	f i r s t   the boolian sum class of apples and oranges. 	second  the class   f r u i t   . t h i r d   the universal class. 	though t h i s is not the example winston gives  the choice he would make would be   f r u i t   . 	he regards this as a s o r t of middle-of-the-road stand in a d i f f i c u l t i n -
duction problem. 
　　induction theory gives a q u a n t i t a t i v e discussion. 	to make this p a r t i c u l a r problem nont r i v i a l   we assume that the p o s i t i v e examples given are in some sense   t y p i c a l   . 	if they are not constrained in t h i s way  then the universal class is the best response. 	let p i and n i . be the 
* this d i f f e r s s l i g h t l y from winston's d e f i n i t i o n of  model   but should cause no great difficulty. 
respective a p r i o r i p r o b a b i l i t y of a class and the number of members in that class. then if the p o s i t i v e examples are   t y p i c a l   or  more narrowly  if a l l p o s i t i v e cases of the concept have equal l i k e l i h o o d of being given as examples  then by 
 bayes  the most l i k e l y class is the one for which  is maximum  n being the number of p o s i t i v e 
cases 	- two in the present s i t u a t i o n . 
　　the universal class is of high a p r i o r i prob a b i l i t y   but it has very many members. the boolian sum class has only two members  but i t s a p r i o r i likelihood tends to be low. this is because the symbol for boolian sum is  expensive  - i . e . concepts formed using i t   tend to be rather ad-hoc and not very useful in p r e d i c t i o n . if the class   f r u i t   i s   indeed  of f a i r a p r i o r i p r o b a b i l i t y and there a r e n ' t too many kinds of f r u i t   it may well be the best choice. we might also consider  eatable f r u i t   or  plants  depending on the sizes and a p r i o r i p r o b a b i l i t i e s of these classes. 
　　from these and similar h e u r i s t i c arguments  winston is able to order the possible models with respect to l i k e l i h o o d . the r e s u l t is a remarkably capable program in the problem area he has selected. however  even in t h i s limited area it is easy to get i n t o complexities that are well beyond the capacity of the program. winston deals with these by a r b i t r a r i l y c u t t i n g off the consideration of c e r t a i n p o s s i b i l i t y branches. 
　　with expansion of the language to deal with a l a r g e r universe of problems - e.g. the i n c l u s i o n of f a c i l i t i e s f o r making recursive d e f i n i t i o n s the necessary complexity would r a p i d l y get beyond the c a p a b i l i t i e s of the h e u r i s t i c discussions of l i k e l i h o o d that winston uses. 
　　we w i l l describe a system of learning concepts that is similar to winston's  but obtains q u a n t i t a t i v e p r o b a b i l i t i e s and appears to be f a r simpler to implement. it is hoped that t h i s s i m p l i f i c a t i o n w i l l make it possible to extend winston's methods to much r i c h e r worlds of problems. 
　　the system consists of a proceedure for obt a i n i n g an i n i t i a l model and for modifying the model as each new p o s i t i v e or negative example is given. at each point in time several of the best models are stored and these can be used to give the p r o b a b i l i t y of any p a r t i c u l a r object being an example of the concept being learned. 
　　we s t a r t out with a p o s i t i v e example of the concept. 
　　our f i r s t model has f o r i t s components  those classes of which the corresponding components of the example are members  and f o r which p i /n j is maximum  i being the component considered. often these classes w i l l have but one component and winston uses classes of t h i s kind for his i n i t i a l model. 
1 　　subsequently   there are four possible example situations with respect to the model. a p o s i t i v e example can be e i t h e r accepted or rejected by the model or a negative example can be accepted or rejected by the model. we w i l l t r e a t these one by one. 
　　if a negative example is rejected by the model  we leave the model invariant. our criterion of choice of class is m a x i m u m s i n c e n is the number of positive examples far  the new negative example does not modify this quantity in any way for any class  so a class that was optimal before the example must be optimal after the example. no change need be made in the model-
　　on the other hand  if a positive example is given and this is accepted by the model  the model may or may not need to be changed- each of the components must be examined individually. the class with m a x i m u m o r may not be the class with maximum the modification of 
the model is relatively simple because the components are optimized independently of one another* 
　　a similar situation arises if we are given a positive example that is rejected by the model. each of the component classes in the model that rejects the corresponding example component must be expanded to include the example component. there w i l l usually be many ways to expand and in each case we chose the class for which pi/n is maximum  n+1 being the number of positive examples thus far. 
struetion that can sometimes be more appropriate. 
　　one possibility is to assume that the positive examples are not necessarily typical. the pro-
blem is then merely to find the model of highest a p r i o r i probability that accepts a l l known positive and rejects a l l known negative examples. 
instead o   the quantity that w e want t o maximize is this is mathematically equivalent to having a l l of the ni constant - the same for a l l classes. the discussions of what to do when a positive case f i t s the model or when a negative case doesn't are similar to those used 
before  but if a positive case f i t s the model  the model is always l e f t invariant. 
　　the other alternative is to use the more general model of induction given by stochastic languages. here  the classes usually do not have sharp edges. the description of a stochastic language  which we w i l l identify with a component class in our model  at once gives the a p r i o r i probability of that class as well as the probab i l i t y of any particular element being chosen. the latter corresponds to the 1/ni that was used earlier. 
	if pi is as before and 	a i j 	is the probability 


　　the treatment of these four cases appears to be simpler and more precise than winston's heuristic discussions on the relative likelihoods of several possible models. no tree exploration is necessary. 
　　the pj values are readily approximated by counting the relative frequencies of various classes. if the samples are small  the accuracy can be improved by considering how the classes were constructed. 
　　the ni are obtainable by counting the number of different members of each class that have occurred up to now. 
there are two other methods of class con-
that the i t h class assigns to the jth positive example  then we want a class that assigns zero probability to a l l negative examples such that  is maximum. this criterion corresponds to the earlier approximation 
　　the three methods of class definition described above are not mutually exclusive. it is possible to describe some vector components by means of the  model  others by means of the p. model and s t i l l others by means of the stochast i c language model. 
　　in the foregoing analysis  we have assumed that the vector components are s t a t i s t i c a l l y independent  and the conclusions obtained follow rigorously from t h i s . if there is reason to believe that several components are s t a t i s t i c a l l y dependent  a suitable joint a p r i o r i probability distribution for them can be obtained. this dependent set would be treated as a single vector component  but the rest of the analysis would be the same. 
　　if we have training sequences with only positive examples  neither winston's system nor the pi system can operate  but both the pi/ni n system and the stochastic language system have no particular d i f f i c u l t y . 
   arother important advantage of the analysis techniques described is the use of quantitative probabilities for the vector classes cf the model. from them it is possible to calculate readily the probability that any unidentified new example is or is not an example of the concept being learned. 
　　to compute t h i s   take the total probabilities of a l l models of the concept that are consistent with the data thus far   i . e . they accept a l l 

1 

probabilities of a l l models of the concept that are consistent with the data thus far  whether or not they accept the new example. 
　　quantitative probability estimates are necessary to make c r i t i c a l decisions through decision theory- the mechanization of medical diagnosis is one important area. decisions in economics  ecology and agriculture are but a few other areas 
where probabilities of this kind are of paramount importance. 1. 
1. positive and reject all negative examples  that 1. pu. k.s.  syntactic methods in pattern accept the new example. divide this by the total 
recognition   academic press  1. 

1 

winston  p.h.  learning structural descriptions from examples   repcrt ai-tr1  a r t i f i c i a l intelligence lab.   mass. i n r t . 
technol.  1. also can be found in the psychlogy of computer vision.  edited by p.h. winston; mcgraw h i l l   1**. 
chaitin  g.j.  on the length of programs for computing finite binary sequences   
journal of the assoc  of computing machinery. vol. 1  no. 1  oct 1   pp. 1. 
