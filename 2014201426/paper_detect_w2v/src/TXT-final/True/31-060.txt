 
this paper explores the relationships among reactivity  heuristic reasoning  and search. it describes a hybrid  hierarchical reasoner that first has the opportunity to react correctly. if no ready reaction is computed  the reasoner activates a set of reactive triggers for time-limited search procedures. if they too fail to produce a response  the reasoner resorts to collaboration among a set of heuristic rationales. in a series of experiments  this hybrid reasoner is shown to be effective and efficient. the data also show how each of the three processes  correct reactions  time-limited search with reactive trigger  and heuristic rationales  plays an important role in problem solving. reactivity is demonstrably enhanced by brief  knowledge-based  intelligent searches to generate solution fragments. 
1. introduction 
when confronted with an intractable search space  people employ a variety of devices to make what they hope will be expert decisions. some of this behavior is automatic; certain perceptions of the world trigger action without conscious reasoning. ai researchers have modeled this automaticity with reactive systems. other portions of this behavior are heuristic; limitedly rational reasoning principles are applied in some combination. ai researchers have modeled this pragmatic behavior with rule-based systems. there is  however  another important mechanism people use. situationbased behavior is the serial testing of known  triggered techniques for problem solving in a domain. the integration of situation-based behavior with reactivity and heuristic reasoning is the subject of this paper. the contributions of this work are an architecture that integrates situation-based behavior with reactivity and heuristic reasoning  and empirical evidence that situation-based behavior is indeed an effective method when resources are limited. 
　situation-based behavior is based upon psychologists' reports about human experts in resource-limited situations  klein and calderwood  1 . for example  an emergency rescue team is called to the scene of an attempted suicide  where a person dangles from a sign after jumping from a highway overpass. time is limited and the person is semiconscious. during debriefing after a successful rescue  the 
1 
commander of the team describes how they immediately secured the semiconscious woman's arms and legs  but then needed to lift her to safety. he retrieved  instantiated  and mentally tested four devices that could hold her while the team lifted  one device at a time. when a device failed in his mental simulation  he ran the next. when the fourth scenario ran several times in simulation without an apparent flaw  he began to execute it in the real world. klein and calderwood describe the predominance of this situation-based behavior in 1 such incidents  and cite additional evidence from studies of army commanders  business executives  juries in deliberation  judges setting bail  highway engineers  and nuclear power plant operators. its key features  for the purposes of this discussion  are that a situation triggers a set of procedural responses  not solutions  and that those responses are not tested in parallel. the purpose of this paper is to explore the role of situation-based behavior with respect to reactivity and heuristic reasoning under time limitations. 
1. forr: the architecture 
the architecture described in this section supports the development of a reactive reasoner. the problem solvers whom klein and calderwood studied did not have the leisure to research similar situations or to explore many alternatives. they had to decide quickly. reactive systems are intended to sense the world around them and respond with a quick computation  brooks  1; maes and brooks  1 . they are meant to iterate a  sense-compute-execute  loop where the sensing is predetermined and the heuristic computation is either hardwired or extremely rapid. in most complex dynamic problems  however  a simulation of intelligence is strengthened by learning. in the spirit of reactivity  such learning should be quick to do and easy to apply in subsequent loop iterations. 
　forr  for the right reasons  is a problem-solving and learning architecture that models the transition from general expertise to specific expertise  epstein  1a . a forrbased system begins with a domain of related problem classes  such as board games or mazes  and some domainspecific but problem-class-independent knowledge  such as  do not set the other contestant up for a win  or  avoid dead-ends.  with experience  such as contests played or trips from one location to another  a forr-based program gradually acquires usefiil knowledge  problem-class-specific data that is potentially useful and probably correct. this use-

ful knowledge  such as good game openings or shortcuts in a particular maze from one area to the next  should enhance the performance of a forr-based system. 
　forr's three-tiered hierarchical model of the reasoning process  is shown in figure 1. an advisor is a domain-specific but problem-class-independent  decision-making rationale  such as  minimize the other contestant's material  or  get closer to your destination.  each advisor is a  right reason   implemented as a time-limited procedure. input to each advisor is the current state of the world  the current permissible actions from that state  and any learned useful knowledge about the current problem class. each advisor outputs any number of comments that support or discourage permissible actions. a comment lists the advisor's name  the action commented upon  and a strength  an integer from 1 to 1 that measures the intensity and direction of the advisor's opinion. although there are no constraints on the nature of the comment-generating procedures themselves  a forr-based system is intended to sense the current state of the world and react with a rapid computation  i.e.  to avoid extensive search. 
　tier-1 advisors are consulted in a predetermined  fixed order. each advisor may have the authority to make a decision alone or to eliminate a legal action from any further consideration. tier-1 advisors are reactive and reference only correct useful knowledge. they  sense  the current state of the world and what they know about the problem class; if they make a decision  it is fast and correct. the commander had a tier-1 advisor which insisted that the victim's limbs be secured. if one were building a forr-based system to play games  an important tier-1 advisor would be  if you see an immediately winning move  take it.  only when the first tier of a forr-based system fails to make a decision does control default to the next tier. 
　tier-1 advisors  in contrast  are not necessarily correct in the full context of the state space. each of them epitomizes a heuristic  specialized view of reality that can make a valid argument for or against one or more actions. tier-1 advisors are reactive too  but far less trustworthy  because neither their reasoning process nor the useful knowledge on which they rely is guaranteed correct. all of the tier-1 advisors have an opportunity to comment before any decision is made. the decision they arrive at is the action with highest total strength; this represents a consensus of their opinions.  a tie is broken by random selection.  in a forr-based game-learning program  a good tier-1 advisor is  maximize the number of your pieces on the board and minimize those of the other contestant.  for the rescue situation  however  tier-1 advisors are too slow and too risky. 
　situation-based behavior has recently been incorporated into forr as tier 1. each tier-1 advisor has a reactive 
trigger and a procedure that generates and tests a highlyconstrained set of possible solution fragments. a solution fragment emerges from a tier-1 advisor as a sequence of decisions  rather than a single reactive one  a digression from the  sense-compute-execute  loop. this new tier is prioritized like the first  but lacks any guarantee of correctness. a tier-1 advisor triggers when it recognizes that its method may be directly related to the current situation  the way the need to hoist triggers the rescue team's holding devices. execution of a tier-1 advisor instantiates and tests 

figure i: how forr makes decisions. 
one or more possible solution fragments. tier 1 is prioritized like tier 1  but is not guaranteed correct. the first tier1 advisor to trigger is ceded control and given limited time to develop a solution fragment. if no tier-1 advisor triggers or produces a sequence of recommended steps  the second tier will make the decision. if a tier-1 advisor constructs a sequence of decisions that it believes relevant  they are executed and then  regardless of the outcome  control is returned to tier 1. 
　forr is implemented in common lisp. to apply forr to a domain  one describes the domain  its problem classes  its useful knowledge  and procedures to learn it.  although learning supports some of the tier-1 advisors  it is search  not learning  that is the focus of this discussion. learning is addressed in  epstein  1 .  the effectiveness of situation-based advisors and their role in reasoning is best demonstrated with an example. 
1. ariadne: an implementation 
ariadne simulates robot path-finding.  ariadne  daughter of king minos of crete  helped theseus find his way through the labyrinth.  ariadne is implemented as a set of common lisp files that run with forr. a problem class in ariadne is a particular maze  a rectangular grid with discrete internal obstructions  like the one in figure 1. a location  r  c  is the position in the rth row and cth column of the maze  addressed as if it were an array. in figure 1 the robot is at  1  1  and the goal is at  1  1 . a problem is to travel from an initial robot location r to some goal location g in a sequence of legal moves  that is  to find a  not necessarily optimal  path to the goal. 


1 

table j: ariadne's advisors for path finding. 
	advisor 	description 
 tier 1 	no way 	forbid entry to a dead-end not containing the goal. victory 	take the robot to a visible goal. 
tier 1 	outta here 	seek a pair of moves that minimize the robot's distance to the goal and leave the confined area. 
	probe 	seek to identify and apply an access point for the chamber. 
super-quadro seek a gate out of the current quadrant  preferably into the goal quadrant. 
　　　　wander 	seek a well-directed  long  l-shaped path with a new view to a thus far unvisited location. roundabout 	if the robot is aligned with the goal but there is a wall between them  go around the wall. 
tier 1 goal row 	move so that the robot's row coordinate matches that of the goal. 
goal column move so that the robot's column coordinate matches that of the goal. 
	giant step 	make the longest possible move  with higher strengths for moves in the direction of the goal. 
	plod 	move one step in any direction  with higher strengths for moves in the direction of the goal. 
mr. rogers move into the immediate neighborhood of the goal  with higher strengths for closer locations. 
been there object to returning to a location already visited during this problem  with lower strengths for those visited more frequently. done that object to moving in a direction already taken from a location already visited in this problem  with lower strengths for directions taken more often. chamberlain move into a chamber whose extent indicates that the goal might lie within  and discourage moves into those that do not. quadro when the robot and the goal are in different quadrants  move to gates that afford access to another quadrant  most strongly for those into the goal quadrant  less strongly for others. opening reuse previously successful path beginnings when applicable 
were to trigger in figure 1 when the robot was at  1  1   it would generate the path   1  1    1  1   too. 
　super-quadro scans to change the quadrant. it tries to move the robot in a pair of orthogonal steps to a location whose quadrant is different  preferably the goal quadrant. super-quadro would not find a gate from the robot's position in figure 1. 
　wander tests the 1 l-shaped paths that are pairs of longest possible steps in two orthogonal directions  with preference toward those in the direction of the goal. the path it selects ends at a previously unvisited location and makes additional unvisited locations visible. if wander were to trigger in figure 1  depending upon its recent experience  it could force the path   1  1    1  1    not a particularly helpful sequence. 
　roundabout triggers when the robot is in the same row or column as the goal but cannot see it because of an obstruction. when the robot is so aligned  roundabout attempts to go around the wall between it and the goal. if  for example  the robot of figure 1 were at  1   roundabout would take it to  1  1    1  1   and  1  1  before stopping at  1  1  where the goal is in sight. note that roundabout  like any tier-1 advisor  is time-limited and heuristic. roundabout may fail  or it may only get closer to the goal than it had been when it started  without actually bringing the goal in sight. 
　ariadne's 1 tier-1 advisors embody path-finding common sense and do no forward search in the problem space. goal row and goal column attempt to align the robot with the goal. giant step and plod advocate large and small steps  respectively. mr. rogers attempts to minimize the euclidean distance to the goal. been there and done that discourage repetition of prior behavior. chamberlain and quadro apply learned knowledge about chambers and gates. opening encourages the reuse of previously successful path beginnings when applicable.  although such moves may seem odd if the goal is in a different location  the heuristic works well if the old path was successful because it began by moving to an area that offered good access to other parts of the maze.  the simple ideas behind the tier-1 advisors support rapid computation. given 1 seconds  no tier-1 ariadne advisor has yet run out of time  and no problem has occupied more than a minute. 
1. experimental results 
the data described here arose when ariadne randomly generated a maze and tested the performance there of different reasoning agents. because forr is non-deterministic  results from 1 runs  i.e.  1 randomly-generated mazes  were averaged to produce an experiment. experiments were performed for problems with levels of difficulty 1  1  1  and 1 in a 1 x 1 maze that was 1% obstructed. these parameters were selected to provide at least 1 possible problems at the specified level of difficulty. effectively  the level of difficulty of a problem is the minimum number of  left or right  turns the robot must make to reach the goal. 
　in each maze the full version of ariadne was given 1 learning problems  robot location and goal location pairs  at a fixed level of difficulty. then 1 newly-generated testing problems for the same maze and level of difficulty were offered to all the agents with learning turned off. a problem of either kind was terminated when the reasoning agent reached the goal or when it had made 1 moves.  the learning problems established a useful knowledge base for those advisors that depend on it. all agents using such ad-

visors had equal access to the learned knowledge.  the 1step limit incorporated any exploration during tier-1 search. 
　four agents were tested. the forr agent is the full version of ariadne; it used all 1 of the advisors to simulate reactive decision making with situation-based behavior. the random agent simply selected random legal moves; this is equivalent to blind search. three ablated agents were formulated by omitting tiers in the hierarchy. 
  the reactive agent used only the tier-1 advisors to simulate correct  reactive decision making alone. if more than one option was left after tier 1  a move was made at random. 
  the reactive+ agent used the tier-1 and tier-1 advisors to simulate reactive decision making with situation-based behavior but without heuristic reasoning. if no decision was made after tier 1  a move was made at random.   the reactive-heuristic agent used the tier-1 and tier-1 advisors to simulate correct and heuristic reactive decision making without situation-based behavior. during early trials the random agent  solved only 1% of 1 level 1 problems and 1% of 1 level 1 problems; it therefore serves merely as a benchmark and was eliminated from subsequent testing. once any of the other agents performed poorly at some level of difficulty  it too was elimi-
nated from testing at any higher level. 
　table 1 reports the results for the ablated agents and forr averaged across the 1 runs in each experiment.  solved  is the percentage of the test problems the agent could solve with at most 1 moves in the same maze.  path length  is the manhattan distance along the path to the goal. since a step may move through more than one location  path length varies among problems of the same difficulty.  moves  is the number of moves in the solution. the number of distinct locations actually visited during those moves is reported as  locations.   triggers  measures the reliance of the system on tier 1; it is the number of passes through figure 1 during which any situation-based advisor executed. path length  moves  and locations are computed only over solved problems.  this makes the ablated agents look 
1 
somewhat better than they actually are.   repetition  measures how repetitive an agent's paths are  calculated as 
1 locations moves 
on each problem level   bfs%  indicates the percentage of the space reachable from the robot's initial position that breadth-first search would have visited on the same test problems. bfs% understates the cost of a physically executed breadth-first search  whose many repetitive subpaths go uncounted here. 
　the reactive agent managed to find the goal about a quarter of the time on level 1 problems  not enough to continue testing it at higher levels. its paths tend to be much more repetitious and several times as long as those of the other agents. the infrequent successes of the reactive agent occur when large portions of the randomly-generated maze are unreachable from the robot's starting point  the way  1  1  is in figure 1. in such a maze the substantial random component of the reactive agent's behavior was more likely to be effective. 
　the reactive-heuristic agent  forr's original formulation  began to fail on the level 1 problems. although it solves about as many problems there as the reactive+ agent does  its paths are considerably longer and more repetitious  a precursor  in our experience  of dismal performance on slightly harder problems. the reactive-heuristic agent was eliminated from testing after level 1. 
　the situation-based search advisors of tier 1 make a clear contribution when combined with tier 1 as reactive*  but they have a limited repertoire of behaviors. as the problems become more difficult. reactive+ is clearly inadequate. it solved only 1% of the level 1 problems. 
　in summary  as the problems become more difficult  several things happen: breadth-first search reaches an increasing percentage of the accessible unobstructed locations  the situation-based advisors trigger more frequently  and the ability of the ablated agents to solve the problems becomes markedly inferior. forr with tier-1 offers a measure of reliability and achievement the other versions lack. the 

number of successes by the full forr agent represents a statistically significant improvement over the others.  indeed  inspection indicates that forr often  had  the crucial solution fragment to many of its non-successes  but needed a few more steps to finish.  although this work was predicated on the acceptability of suboptimal solutions  the successful paths of the ablated agents are extremely long. with all of forr* s tiers in place  ariadne gets the robot to the goal more often  more quickly  and considers fewer alternatives along the way. 
1. discussion 
this is not a domain in which one would want to do a standard search. the robot's knowledge is so limited that an ordinary evaluation function would be difficult to construct.  for example  closer to the goal is not necessarily better; there may be a very long wall there.  depth-first search would require fairly elaborate backtracking and loop prevention; very few of the test problems would be solvable in 1 steps with depth-first search. breadth-first search  while it will always solve the problem  does so at the cost of visiting a high proportion of nodes in the search space and maintaining a very large structure for open paths. indeed  the data indicate that explicit  breadth-first search in these mazes is nearly exhaustive as the problems become more difficult. means-ends analysis is not possible because the robot knows little  if anything at all  about the immediate vicinity of the goal. for a very large maze  then  explicit search would be extremely inefficient  perhaps intractable. 
　without tier 1  forr is a reactive system augmented by learned useful knowledge. the results with the reactiveheuristic agent  however  simply are not good enough. this agent regularly gets stuck in regions where giant step cannot extricate it; it needs maneuvers like wander's l-shaped path to get out. it also regularly gets close to the goal but cannot see it because of an intervening wall; it needs roundabout's determined circumvention to get closer. 
　the situation-based advisors  however  are insufficient on their own. they consistently trigger more often with reactive+ than with ariadne  because most of their triggers measure lack of recent progress  something the robot experiences more often with reactive*. tier 1 is  effectively  a device to execute subgoals. the subgoal is the opposite of the trigger  e.g.  wander tries to  get out of here   and roundabout tries to  get around the wall.  subgoals are detected by the program  but their nature is predetermined by the programmer. 
　there is a complex relationship among the tiers. tier 1 offers the commonsense inherent in any problem solving task. tier 1 tries to avoid search and effectively sets up the situation-based advisors in tier 1 so that they can trigger. for example  goal row and goal column push the robot into a situation where roundabout can trigger. in turn  the situation-based advisors of tier 1 set up the heuristic reasoners in tier 1. for example  wander puts the robot where all the tier-1 advisors are more likely to make new  constructive comments. another important side effect of the search in tier 1 is the acquisition of useful knowledge.  see  epstein  1  for further details.  
the initial impulse behind reactive programming was to avoid search. when one augments the reactive advisors of tier 1 and tier 1 with tier 1  it is easy to forget that. tier 1 advisors are kept within the search minimization philosophy in two ways. first  forr only allocates each advisor  in any tier  a limited amount of computing time. solution fragments that take too long to construct will not be considered. second  tier-1 advisors have hand-coded routines intended to address their particular subgoals. these routines generate and test solution fragments  just the way the commander did  but the proposed partial solutions must be highly constrained  just as the commander's were. this constraint saves the tier-1 advisor from a combinatoric explosion. for example  roundabout follows a procedure that first moves it as close to the wall between it and the goal as possible  the goal direction  and then  kicks  it away in an orthogonal direction  the secondary direction   with a preference for clockwise. after that  roundabout seeks the goal  trying to move in the goal direction  then the secondary direction  and then their opposites. roundabout has some mechanisms to prevent loops in a path  and initially the kick is of length one. as long as roundabout does not find a way around the wall and still has time left  it will increment the kick by one and try again. although this search is quite deep  it also severely curtailed by knowledge; that is why it is effective. 
　there are two ways to view ariadne's task as resourcelimited. if cpu time is a scarce resource  then the agent that makes the fewest passes through figure 1 is best. if traveling time or fuel is a scarce resource  then the agent that constructs the shortest paths is best. on both metrics the full forr agent achieves a synergy that its individual components lack. 
1. related work 
situation-based behavior is not case-based reasoning  cbr   although they have much in common. in cbr  experiences are indexed and stored. later  one or more potentially relevant cases are retrieved  and an attempt is made to modify their solutions to solve the current problem  kolodner  1   although situation-based behavior is triggered by an abstraction of the current state that could have been used as an index for cbr  situation-based behavior does not retrieve specific solutions to be modified  only procedures intended to generate solution fragments. situation-based behavior and cbr both constrain solution generation  but cbr does it by searching from old solutions  while situation-based behavior does it by the knowledge inherent in its procedures. klein and calderwood emphasize that the human experts they study do not perceive their problem solving as reminding.  this is not a claim that cbr has no parallel in people  only that it is less likely to be used when resources are very limited.  
　situation-based behavior is not planning either. a plan is a set of actions intended to reach a specific goal  tate  et al.  1 . the commander tested holding devices by incorporating them into plans and then simulating those plans until one promised success. the advisors of tier 1  however  are not planners because they actually execute their behavior  even if they do not eventually recommend it. for example  wander can investigate as many as eight l's  by moving one longest step in each direction and then testing for possible second steps  before it chooses one to execute. rather than planners  situation-based advisors are procedures that reactively seize control of a forr-based program's resources for a fixed period of time. when that time elapses the situation-based advisor either returns control to tier 1 or returns a sequence of actions whose execution it requires. tier 1 constitutes a reactive decision maker  much like pengi  agre and chapman  1 . the principal difference is that pengi's problem is living in its world; it is not held to an explicit decision standard like ariadne's  solved in 1 decision steps.  
　situation-based behavior is a resource-grabbing  heuristic digression intended to produce a solution fragment  not a production rule or a macro-operator. although the trigger of a tier-1 advisor could be the condition of a production rule  the comment generator's response is too complex  particularly since it can be rescinded  to be the action part. a macro-operator is a generalization across the variables entailed in a successful procedure  whereas a situation-based advisor is a procedural generalization over several kinds of behavior appropriate to a situation. 
　this work has some clear counterparts with korf s analysis of heuristic search in the tile puzzles  korf  1 . his minimin search  strategy of least commitment  is shared by ariadne's plod  but it of necessity lengthens the number of steps in a solution. for example  if the correct  unobstructed move is from  1  1  to  1  1   plodding will take 1 moves to get there  although an immediate move to  1  1  would be legal. ariadne's mr. rogers uses the euclidean distance heuristic much the way korf tried heuristic node ordering in the tile puzzles  and with the same disappointing results. once you get close in this domain  too  there may be better ways to reach the goal. korf s permission to backtrack with loop prevention is analogous to some of the decision-making in roundabout. ariadne has no foolproof loop prevention  but been there and done that discourage loops. in ariadne's graph  rather than tree  search space  korf indicates that one cannot expect locally optimal solutions. if the search horizon were limited only by how far the robot could see ahead  the reactive-heuristic agent should solve few problems  since it sees only one step ahead. that agent's better than expected performance is attributable to its useful knowledge about a particular maze and its general mazetraveling knowledge in the tier-1 advisors. it is possible to dictate the level of difficulty in ariadne's problems  but with the tile puzzles there remains some uncertainty about how the level of difficulty impacts upon the ability of the problem solver. 
　although ariadne's maze problems may be reminiscent of recent machine learning work in reinforcement learning  it is important to note that the program's task and fundamental approach are significantly different  moore and atkeson  1; sutton  1 . such programs seek convergence to an optimal path through repeated solution of what  according to our definition in section 1  would be a single problem. in contrast  ariadne constructs satisficing paths for a set of problems  applying knowledge learned from one problem to the others  instead of from one problem-solving attempt to another attempt at the same problem. the complexity of a maze problem for the reinforcement learners is a 
1 
function of both goal strength and the number of state-action pairs  the number of reachable locations and directed onestep movements from them . the complexity of a problem for ariadne  on the other hand  is the numbers of turns required  independent of the size of the maze and the strength of the goal. memory use is different  too. ariadne learns abstractions  while the reinforcement learners refine estimates for the value of each one-step move attempted from each state. 
　finally  situation-based behavior sheds some light on the ongoing debate about representation and reactivity  hayes  et al.  1 . ariadne's conceptual knowledge includes  the last 1% of the moves have been in no more than 1% of the locations in the maze  and  a wall lies between the aligned robot and the goal.  this paper demonstrates that  at least in this domain  the representation of conceptual knowledge is an essential component in a reactive learner. 
1. future work 
an important difference between forr with tier 1 and the rescue team commander is the fact that he ran his successful simulation four times before he implemented it. if ariadne were to do that  it could substantially shorten its path lengths by removing loops and checking for shortcuts. in the experiments  if a tier-1 advisor discovered a partial solution but could not execute it within the 1-move limit  its agent was still considered to have failed on the problem. this accounts for most of the forr agent's failures. with path reconstruction  ariadne would solve even more problems  but not more efficiently  since one would have to count the nodes revisited once the path was refined. the alternative  one we are pursuing now  is to have ariadne learn additional useful knowledge about past paths. 
　based on preliminary empirical evidence  there is every reason to believe that ariadne will scale up  i.e.  that it will continue to perform well in much larger and more tortuous mazes than these. hoyle  a forr-based game-learning program  progressed from expertise in spaces with several thousand nodes to spaces with several billion nodes after the addition of only a few tier-1 advisors. ariadne has already performed well on preliminary tests in 1 x 1 mazes and continues to improve as we refine its advisors and its learning algorithms. 
　ariadne's continued development addresses the balance between exploration and rapid solution. since the data indicate that repetitive paths are a forewarning of weakness  we intend to strengthen its penchant for novelty. there are also several new tier 1 advisors on the drawing board. 
　the thesis of this work is that some carefully constrained search can play an important role in the performance of an otherwise reactive and heuristic system. one could make the task more difficult  say by permitting the goal to shift during the problem  and still expect ariadne to do well. preliminary work with hoyle in the domain of game playing indicates that situation-based behavior does well there too  epstein and gelfand  1 . 
　ideally one would like to see a forr-based program learn tier 1  not just use it. awl is an algorithm that learns weights for tier-1 advisors  epstein  1b . it was devised to exploit empirical evidence that the accuracy of tier-1 advisors varies with the problem class. work is currently under way to apply awl so that forr can learn to prioritize tier-1 advisors for each problem class automatically.  recall that now those priorities are assigned by the programmer.  the next step would be to learn the triggers. we have done some early work on trigger learning with hoyle  epstein  and gelfand  1 . the triggers learned there are visual patterns; we expect to analogize some of that work for ariadne. finally  one would like to learn the commentgenerating procedure to accompany a learned trigger. this  hayes  ford and agnew  1  p.j. hayes  k.m. ford and n. agnew. on babies and bathwater: a cautionary tale. ai magazine  1 : 1  1. 
 klein and calderwood  1  g.a. klein and r. calderwood. decision models: some lessons from the field. ieee transactions on systems  man  and 
cybernetics  1 : 1  1l 
 kolodner  1  j.l. kolodner. introduction to the special 
may be possible as a generalization over successful solutions 	1 : 1  1. issue on case-based reasoning. machine learning  

after the trigger is satisfied  but it is a difficult problem. 
1. conclusions 
for many intractable real-world problems  a suboptimal solution may be acceptable. situation-based behavior is modeled on human production of suboptimal solutions under time constraints. a reactive system goes from perception  sensing the state of the world  to an associated action  without any opportunity to reason about the state. with tier 1  forr  like klein and calderwood's subjects  perceives and then reasons about the current state of the world before it elicits an associated action  but still maintains some of the advantages of reactivity. ariadne's success at maze search is a clear indication that some highly-restricted  intelligent search is an important component in the simulation of efficient  effective decision making under resource limitations. 
acknowledgments 
jack gelfand originally formulated a somewhat different ariadne problem  and continued to ask constructive questions as this research evolved. david sullivan's preliminary work on tonka convinced us that a forr-based version of ariadne could be a success. rich korf  jim hendler  and rich sutton offered helpful comments. 
