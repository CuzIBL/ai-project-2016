
in this paper we define a general framework for activity recognition by building upon and extending relational markov networks. using the example of activity recognition from location data  we show that our model can represent a variety of features including temporal information such as time of day  spatial information extracted from geographic databases  and global constraints such as the number of homes or workplaces of a person. we develop an efficient inference and learning technique based on mcmc. using gps location data collected by multiple people we show that the technique can accurately label a person's activity locations. furthermore  we show that it is possible to learn good models from less data by using priors extracted from other people's data.
1 introduction
activity recognition and context-aware computing are gaining increasing interest in the ai and ubiquitous computing communities. most existing systems have been focused on relatively low level activities within small environments or during short periods of time. in this paper  we describe a system that can recognize high level activities  e.g.  working  shopping  and dining out  over many weeks. our system uses data from a wearable gps location sensor  and is able to identify a user's significant places  and learn to discriminate between the activities performed at these locations - including novel locations. such activity information can be used in many applications. for example  it could be used to automatically instruct a user's cell phone not to ring when dining at a restaurant  or it could support home rehabilitation of people suffering from traumatic brain injuries  salazar et al.  1  by providing automatic activity monitoring. beyond estimating high-level activity categories  our system can be expanded to incorporate additional sensor information  thereby recognizing fine-grained indoor household tasks  such as those described in  philipose et al.  1 .
　because behavior patterns can be highly variable  a reliable discrimination between activities must take several sources of evidence into account. our system considers  1  temporal information such as time of day;  1  spatial information extracted from geographic databases  including information about the kinds of businesses in various locations;  1  sequential information such as which activity follows which activity; and  1  global constraints such as the number of different homes or workplaces. additionally  it uses data collected by other users so as to improve the classification of a specific user's activities. all these constraints are soft: for example  on some days a person may do something unusual  such as go to a movie in the middle of a workday. furthermore  the norm for some individuals may be different than our general commonsense prior: for example  some people work two jobs  or shuttle between two different homes. it is necessary to use a rich and flexible language to robustly integrate such a wide variety of both local and global probabilistic constraints.
　our system builds upon previous work on extracting places from traces of users' movements  gathered by gps or other localization technologies  ashbrook and starner  1; hariharan and toyama  1; liao et al.  1 . our work goes beyond theirs in that our system also recognizes the activities associated with the places. moreover  previous approaches to modeling personal movements and place patterns require a large amount of training data from each user  and cannot be generalized to new places or new users. by contrast  our relational approach requires less individual training data by leveraging data collected by others. in summary  the contributions of this paper are:
1. a general framework for sensor-based activity recognition based on relational markov networks  rmns   taskar et al.  1   which are both highly expressive and well-suited for discriminative learning;
1. an extension of rmns to incorporate complex  global features using aggregations and label-specific cliques;
1. efficient markov-chain monte-carlo  mcmc  algorithms for inference and learning in extended rmns  and in particular  an mcmc algorithm to simultaneously evaluate a likelihood function and its gradient;
1. positive experimental results on real data from multiple subjects  including evidence that we can improve accuracy by extracting priors from others' data.
this paper is organized as follows. we will introduce our relational activity model in section 1. inference and learning will be discussed in section 1  followed by experimental evaluations. conclusions and future work are given in section 1.
1 the relational activity model
in this section we first discuss rmns and our extensions. then we show how to use them for modeling activities.
1 relational markov networks
rmns are extensions of conditional random fields  crfs   which are undirected graphical models that were developed for labeling sequence data  lafferty et al.  1 . crfs are discriminative models that have been shown to out-perform generative approaches such as hmms and markov random fields in areas such as natural language processing  lafferty et al.  1  and computer vision  kumar and hebert  1 . rmns extend crfs by providing a relational language for describing clique structures and enforcing parameter sharing at the template level. thereby rmns are an extremely flexible and concise framework for defining features that can be used in the activity recognition context.
　an rmn consists of three parts: a schema e for the domain  a set of relational clique templates c  and corresponding potentials Φ. the schema e specifies the set of classes  i.e.  entity types  and attributes in each class. an attribute could be a content attribute  a label attribute  or a reference attribute that specifies reference relation among the classes. an instantiation i of a schema specifies the set of entities for each class and the values of all attributes for each entity. in our context an instantiation consists of the sequence of all significant locations visited by a user along with the temporal and spatial attributes.
　a relational clique template c （ c is similar to a relational database query  e.g.  sql  in that it selects tuples from an instantiation i; the query result is denoted as c i . we extend the definition of such templates in two ways. first  we allow a template to select aggregations of tuples. for example  we can group tuples and define potentials over counts or other statistics of the groups. second  we introduce label-specific cliques  whose structures depend on values of the labels. for example  our model can construct a clique over all activities labeled as  athome.  because labels are hidden during inference  such cliques potentially involve all the labels. labelspecific cliques can be specified by allowing label attributes to be used in the  where  clause of an sql query.
　each clique template c is associated with a potential function φc vc  that maps a tuple  values of variables or aggregations  to a non-negative real number. using a log-linear combination of feature functions  we get the following representation: φc vc  = exp{wct ，fc vc }  where fc   defines a feature vector for c and wct is the transpose of the corresponding weight vector. for instance  a feature could be the number of different homes defined using aggregations.
　for a specific instantiation i  an rmn defines a conditional distribution p y|x  over labels y given the observed attributes x. to compute such a conditional distribution  the rmn generates an unrolled markov network  in which the nodes correspond to the content attributes and the label attributes. the cliques of the unrolled network are built by applying each clique template c （ c to the instantiation  which can result in several cliques per template  see fig. 1 b  for an example . all cliques that originate from the same template must share the same weights wc. the resulting cliques factorize the conditional distribution as

where the normalizing partition function z x  = py1 qc（c qvc1 （c φc vc1  .  1  follows by moving the products into the exponent and combining all summations into w and f.
1 relational activity models
we will now describe our relational activity models. even though we illustrate the concepts using the example of location-based activity recognition  our model is very flexible and can be applied to a variety of activity recognition tasks.
　the schema for activity recognition based on temporal and spatial patterns is shown in fig. 1 a . it includes three classes: activity  place  and transition.
activity: activity is the central class in the domain. its attribute label is the only hidden variable. the set of possible labels in our experiments is {'athome'  'atwork'  'shopping'  'diningout'  'visiting'  'others'}. attribute id serves as the primary key. the class also contains temporal information associated with an activity  such as timeofday  dayofweek  and duration  whose values are discretized when necessary. finally  place is a reference attribute that points to a place entity where the activity has been performed. place: the class place includes two boolean attributes: nearrestaurant and nearstore  which indicate whether there are restaurants or stores nearby.
transition: transition captures temporal succession relationship among activities. the reference attributes from and to refer to a pair of consecutive activities.
　based on the schema  we define the following relational clique templates. each of them takes into account a number of discriminative features.
1. temporal patterns: different activities often have different temporal patterns  such as their duration or time of day. such local patterns are modeled by clique templates that connect each attribute with the activity label.
1. geographic evidence: information about the types of businesses close to a location can be extremely useful to determine a user's activity. such information can be extracted from geographic databases  such as microsoft mappoint  hariharan et al.  1  used in our experiments. since location information in such databases is not accurate enough  we consider such information by checking whether  for example  a restaurant is within a certain range from the location.
1. transition relations: the first-order transitions between activities can also be informative. for example  staying at home followed by being at work is very common
activity
   id
	place	   label   timeofday	transition
	   id	   dayofweek	   from
	   nearrestaurant	   duration	   to
	   nearstore	   place	 a 
figure 1:  a  the schema of the relational activity model. dashed lines indicate reference relations among classes.  b  an example of an unrolled markov network with six activity locations. solid straight lines indicate cliques generated by the templates of temporal  geographic  and transition features; bold solid curves represent spatial constraints  activity 1 and 1 are associated with the same place and so are 1 and 1 ; dashed curves stand for global features  which generate label-specific cliques  e.g.  activity 1 and 1 are both labeled 'athome' .while dining out immediately followed by another dining out is rare. the sql query for this clique template is:
select a1.label  a1.label
from activity a1  activity a1  transition t
where t.from=a1.id and t.to=a1.id
1. spatial constraints: activities at the same place are often similar. in other words  the number of different types of activities in a place is often limited. we can express such a constraint using an aggregation function count  :
select count distinct label 
from activity
group by place
1. global features: such features model global  soft constraints on activities of a person. the number of different home locations is an example of global constraints. such a constraint is modeled by a clique template that selects all places labeled as home and returns how many of them are different:
select count distinct place 
from activity
where label='athome'
note that the label variable appears in the  where  clause  so this is an example of label-specific clique. in a different activity recognition context  global features could also model information such as  the number of times a person has lunch per day. 
　in the first three templates  the feature functions fc   are just indicator functions that return binary values. they can also return numbers  such as in the last two templates.
1 inference and learning
1 labeling activities
in our application  the task of inference is to estimate the labels of activities given a sequence of locations visited by a person. to do so  our rmn converts a location sequence into unrolled markov networks  as illustrated in fig. 1 b . inference in our relational activity model is complicated by the fact that the structure of the unrolled markov network can change during inference because of the label-specific cliques. using standard belief propagation in such networks would require the construction of cliques over all labels  which is obviously inefficient  taskar et al.  1 . we overcome this problem by using mcmc for inference  gilks et al.  1 . in a nutshell  whenever the label of an object is changed during sampling  we determine all cliques that could be affected by this change and re-compute their potentials.
　we first implemented mcmc using basic gibbs sampling. unfortunately  this technique performs poorly in our model because of the strong dependencies among labels. to make mcmc mix faster  we first make an additional spatial constraint that all activities occurring in the same place must have the same label  the relaxation of this constraint will be addressed in future work . this hard constraint allows us to put all activities occurring in the same place into a so-called block. we then develop a mixture of two transition kernels that converges to the correct posterior.
　the first kernel is a block gibbs sampler. at each step we update the labels in a block simultaneously by sampling from the full conditional distribution
	p yk | y k x w 	『	exp{wt ， f x y k “ yk }  1 
where k is the index of the block  yk is the label of block k  y k are the labels for blocks other than k. the second kernel is a metropolis-hasting  mh  sampler. to update the label for block k  the mh sampler randomly picks a block j and proposes to exchange label yk and yj. the acceptance rate of the proposal follows as
	a y 	 1 
where y and y1 are the labels before and after the exchange  respectively.
　the numbers of different homes and workplaces are stored in the chains as global variables. this allows us to compute the global features locally in both kernels: in the gibbs kernel we increase or decrease the numbers depending on the labels of the given block and in the mh kernel the numbers remain intact. at each time step  we choose the gibbs sampler with probability γ  and the mh sampler with probability 1   γ.
1 supervised learning
we show how to learn generic activity models from labeled activity sequences of n different users. learning a customized model for an individual user is a special case when n = 1. the parameters to be learned are the feature weights w that define clique potentials in  1 . to avoid overfitting  we perform maximum a posterior  map  parameter estimation and impose an independent gaussian prior with constant variance for each component of w  i.e.  p w  『 exp{  w   μ t ，  w   μ /1σ1}  where μ is the mean and σ1 is the variance . we define the map objective function as the negative log-likelihood of training data from n subjects plus the prior:


 where j ranges over different users and yj are the activity labels for each user. since  1  is convex  the global minimum can be found using standard optimization algorithms  taskar et al.  1 . we apply the quasi-newton technique to find the optimal weights  sha and pereira  1 . each iteration of this technique requires the value and gradient of  1  computed at the weights returned in the previous iteration.
evaluating the objective function
it can be intractable to compute exact objective values in  1  for all but the simplest cases. this is due to the fact that  for a specific w  it is necessary to evaluate the partition function z xj w   which requires summation over all possible label configurations. we approximate the objective value using monte-carlo methods  geyer and thompson  1 . suppose we already know the value of l w   for a weight vector w . then for each subject j  we use our mcmc inference to get m random samples    from the distribution p y | xj w  . then l w  can be approximated as:
 1 
 where  is the difference between sampled feature counts using w  and the empirical feature counts in the labeled data.
　eq.  1  can only be used to estimate values of l w  relative to l w  . fortunately  such relative values are sufficient for the purpose of optimization. it can be shown that the best approximation in  1  is obtained when w  is close to the optimal w. therefore  during optimization  our algorithm updates w  with better weight estimates whenever possible.
evaluating the gradient
the gradient of the objective function   l w   equals to the difference between the sampled feature counts and the empirical feature counts  plus a prior term. to generate the sampled feature counts under w  we again run mcmc inference. suppose we have obtained m random samples  y  from the distribution p y | xj w . we can compute the gradient as:
 1 
input : the weights w provided by the optimizer output: l w  and  l w 
//evaluate the gradient  l w  foreach subject j do
run mcmc with w and get m samples;
   get feature count difference   ; end
compute the gradient  l w  using eq.  1  ;
//evaluate the objective value l w  if first time calling this function then
l w   = l w  = 1; w  = w ;
;
else
compute l w  using eq.  1  ; if l w    l w   then
l w   = l w ; w  = w ;
;
   end endalgorithm 1: mcmc-based algorithm for simultaneously evaluating objective function and its gradient.
where  is the difference between the sampled and the empirical feature counts.
algorithm
if we compare  1  and  1   we see both require the difference between the sampled and the empirical feature counts. while samples in  1  are based on the weights w   those in  1  are based on w. therefore  if we always keep the best weight estimate as w   we can reuse the sampled feature counts from gradient estimation  thereby making objective value evaluation very efficient.
　our algorithm simultaneously estimates at each iteration the value and the gradient of the negative log-likelihood  1  for given weights w. these estimates are used by the quasinewton approach to compute new weights  and then the estimation is repeated. as shown in alg. 1  both l w   and l w  are initialized as 1 and thus all the objective values are evaluated relative to the objective value of initial weights. in later iterations  when we find a better weight estimate that makes l w  less than l w    we update w  with the new w and also keep the new l w   and . by doing that  we not only evaluate objective values very efficiently  but are also able to get more accurate approximations as w  approaches closer to the optimal weights.
1 experiments
to evaluate our location-based activity recognition technique  we collected two sets of location data using wearable gps units. the first data set  called  single   contains location traces from a single person over a time period of four months  see fig. 1 . it includes about 1 visits to 1 different places. the second data set  called  multiple   was collected by five different people  about one week for each. each person's data include 1 to 1 visits and 1 to 1 different places. we extracted places / visits from the gps logs by detecting locations at which a person spends more than 1 minutes  hariharan and toyama  1 . each instance corresponds to an activity. we then clustered nearby activity locations into places. for training and evaluation  we let the subjects manually label the types of activities. then  we trained the models and tested their accuracy. accuracy was determined by the activities for which the most likely labeling was correct.
applying learned models to other people
in practice  it is of great value to learn a generic activity model that can be immediately applied to new users without additional training. in this experiment  we used the  multiple  data set and performed leave-one-subject-out crossvalidation: we trained using data from four subjects  and tested on the remaining one. the average error rates are indicated by the white bars in fig. 1 a . by using all the features  the generic models achieved an average error rate of 1%. it can be seen that global features and spatial constraints significantly improve classification. to gage the impact of different habits on the results  we also performed the same evaluation using the  single  data set. in this case  we used one-month data for training and the other three-month data for test  and we repeated the validation process for each month. the results are shown by the gray bars in fig. 1 a . in this case  the models achieved an error rate of only 1% by using all the features. this experiment shows that it is possible to learn good activity models from groups of people. it also demonstrates that models learned from more  similar  people can achieve higher accuracy. this indicates that models can be improved by grouping people based on their activity patterns.
　table 1 shows the confusion matrix of one experiment on generic models  rightmost white bar in fig. 1 a  . as can be seen  our approach is able to perfectly label homes and workplaces. the technique performs surprisingly well on the other activities  given that they are extremely difficult to distinguish based on location information alone. the confusion matrix also shows that simply labeling places by the most frequent activity  home  would result in an error rate of 1%.
truthinferred labelshomeworkshopdiningvisitotherhome111work111shop111dining111visit111other111table 1: confusion matrix of cross-validation on generic models with all features.
　to evaluate the impact of number of people available for model learning  we trained our model using data from different numbers of subjects and tested on the remaining one  all features were used . the average error rates of the crossvalidation are shown in fig. 1 b . when trained using only one subject  the system does not perform well  error rate of 1%   mainly because many patterns specific to that person are applied onto others. when more subjects are used for training  the patterns being learned are more generic and the models achieve significantly higher accuracy.

figure 1: part of the locations contained in the  single  data set  collected over a period of four months  x-axis is 1 miles long .
improved learning through priors extracted from others
when estimating the weights of rmns  a prior is imposed in order to avoid overfitting. without additional information  a zero mean gaussian is typically used as the prior  taskar et al.  1 .  peng and mccallum  1  demonstrated that better accuracy can been achieved if feature-dependent variances are used. our experiment shows that performance can also be improved by estimating the prior means of the weights  μ in eq.  1   using data collected from other people.
　in this experiment  we compared the models of a specific person trained using a zero-mean prior with the models trained using an estimated prior. in the latter case  we first learned the feature weights from other people and used those as the mean of the gaussian prior. we evaluated the performance for different amounts of training data available for the test person. the results are shown in fig. 1 c   in which the error rates are counted only on the novel places  i.e.  places that were not visited in the training data and thus often very irregular. we can see that using data from others to generate a prior boosts the accuracy significantly  especially when only small amounts of training data are available.
　the bayesian prior allows the model to smoothly shift from generic to customized: on one end  when no data from the given subject are available  the approach returns the generic  prior  model; on the other end  as more labeled data become available  the model adjusts more and more to the specific patterns of the user.
additional experiments
for comparison  we also built basic hmms in which the hidden states are the labels and all the observations are independent given the states. parameter estimation in hmms with labeled data is done via frequency counting. the most likely labels can be found using the viterbi algorithm. in the one-month-training cross-validation on the  single  data set  the hmm produced an average error rate of 1% by using the temporal  geographic  and transition features. 1 because of the advantages of discriminative learning  even using the same features  rmns performed better than hmms and reduced the relative error rate by about 1%.
　in a separate set of experiments  we tested the performance of our mcmc sampler. by visualizing the standard gelmanrubin statistics  gilks et al.  1  generated from parallel

figure 1:  a  error rates of models using different features: white bars represent errors of models learned from data collected by other people  and gray bars are for models learned and tested using data collected by the same person   previous  means all previous features are also used .  b  error rates of generic models with respect to different numbers of training subjects.  c  error rates of zero-mean prior vs. priors learned from other people.  d  convergence comparison of mcmc using different γ's: g-r statistics approaching 1 indicates good convergence  γ = 1 corresponds to using only mh sampler and γ = 1 corresponds to the block gibbs sampler .chains  we observed that by combining the gibbs and the mh kernels  mcmc converged much faster than using only one of them  see fig. 1 d  . all results reported here were achieved with a mixing parameter γ = 1  see section 1 .
1 conclusions and future work
in this paper  we presented a discriminative relational approach for activity recognition based on the framework of rmns  which are well-suited to model constraints for activity recognition. we showed how to perform efficient inference and learning using mcmc with a mixture of kernels.
　using our relational approach  we developed and tested a specific model for location-based activity recognition. the results are very promising: the system is able to learn models that can accurately label human activities solely based on gps data. we demonstrated that spatial and global features are very important to achieve good recognition rates. we also showed how to obtain good priors using data from other people so as to learn an improved model for a specific person that requires less labeled data.
　we plan to extend our model in a number of ways. first  by collecting data from more subjects  we can learn a set of generic models by clustering the subjects based on their similarities; then we can use a mixture of these models to better recognize activities of a new person. second  we will relax the hard spatial constraint of one activity per location and thus recognize different activities performed at the same place. third  we will integrate information from other wearable sensors  e.g.  microphones or accelerometers  into our general framework  thereby enabling much finer-grained activity recognition. we will also apply the model to the problem of estimating a person's indoor activities from rfid sensor data. by incorporating constraints such as  a person typically has lunch once per day   we expect strong improvements over the results reported in  philipose et al.  1 .
acknowledgments
we thank anthony lamarca  don j. patterson  and xiaolin sun for collecting data used in our experiments. we would also like to thank john krumm for suggesting the use of geographic databases such as microsoft mappoint. this research is based on the work that has been supported by darpa's calo project and nsf.
