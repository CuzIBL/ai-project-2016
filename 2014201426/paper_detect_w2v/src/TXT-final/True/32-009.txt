 
methods to avoid overfitting fall into two broad categories: data-oriented  using separate data for validation  and representation-oriented  penalizing complexity in the model . both have limitations that are hard to overcome. we argue that fully adequate model evaluation is only possible if the search process by which models are obtained is also taken into account. to this end  we recently proposed a method for process-oriented evaluation  p1e   and successfully applied it to rule induction  domingos  1b . however  for the sake of simplicity this treatment made a number of rather artificial assumptions. in this paper the assumptions are removed  and a simple formula for error estimation is obtained. empirical trials show the new  better-founded form of poe to be as accurate as the previous one  while further reducing theory sizes. 
1 	introduction 
overfitting avoidance is a central problem in machine learning. if a learner is sufficiently powerful  whatever representation and search methods it uses  it must guard against selecting a model that fits the training data well but captures the underlying phenomenon poorly. current methods to address this problem fall into two broad categories. data-oriented evaluation uses separate data to learn and validate models  and includes methods like cross-validation  breiman et a/.  1; stone  1   the bootstrap  efron and tibshirani  1   and reducederror pruning  brunk and pazzani  1 . it has several disadvantages: it is often computationally intensive  reduces the data available for learning  can be unreliable if the validation set is small  and is itself prone to overfitting if a large number of models is compared  ng  1 . representation-oriented evaluation seeks to avoid these problems by using the same data for training and validation  but a priori penalizing some models. bayesian approaches in general fall into this category  e.g.   chickering and heckerman  1  . representation-oriented measures typically contain two terms  one reflecting fit 
1 	machine learning 
to the data  and one penalizing model complexity  e.g.   rissanen  1  . this approach is only appropriate when the simpler models are truly the more accurate ones  and there is mounting evidence that this is typically not the case  domingos  1a; jensen and cohen  1 . structural risk minimization  vapnik  1; shawe-taylor et al.  1  and pac learning  kearns and vazirani  1  are representation-oriented methods that seek to bound the difference between training and generalization error using a function of the model space's  effective  dimension. this typically produces bounds that are overly broad  and requires severely restricting the model space. 
¡¡we believe the limitations of representation-oriented evaluation stem from ignoring the search process by which candidate models* are obtained. a learner with an unlimited model space can avoid overfitting as long as it attempts only a limited number of models  even if it is not possible a priori to predict which . intuitively  the more search has been performed to obtain a model  the higher its expected generalization error for a given training-set error. in a recent paper  domingos  1b  we made this intuition precise and applied the resulting formulas to the cn1 rule learner  clark and niblett  1   obtaining systematic improvements in generalization error and theory size. however  for the sake of simplicity the treatment in  domingos  1b  made two rather artificial assumptions: that all error rates are a priori equally likely  and that a model's generalization error can be roughly estimated by treating all previously-generated models as having similar generalization errors. in this paper we remove these two assumptions  interpret the result  and successfully apply it to cn1. 
1 	process-oriented evaluation 
suppose learner lm consists of drawing m hypotheses at random  independently  from some model space  and returning the one with lowest error on a training sample 1. let hm i be the ith hypothesis generated by lm. if hm i 's true error rate is em  i and s consists of n independently 
1
¡¡¡¡by  model  we mean model structure and parameter values. 

drawn examples  the number of errors committed by on 1 is a binomially distributed variable with parameters n and 
let 
notice that this notation is the opposite of the usual notation for a cumulative distribution function  i.e.  
 it will be more 
convenient for what follows. 
¡¡the probability of lm returning a hypothesis hm that misclassifies em training examples is the probability that at least one of the m hypotheses  makes em errors  and all the others make em or more errors. equivalently  it is the probability that all hypotheses  make more let: 

 1  
substituting equation 1 into 1 and 1 into 1  using the assumption of independent hypotheses  and assuming the same prior  for all hypotheses  we obtain the following expression: 

than e - 1 errors  minus the probability that they all make more than e errors: where rem: stituting these into equation 1 and simplifying  we ob-
and 
of €m c. suppose a beta or similarly bell-shaped prior is 
where the integral is multiple  over all components of 	used  bernardo and smith  1 ; this is what makes in-
tuitive sense for error rates. in general 	 the inflecas a function of 	will fall 
 the peak of the prior   since em will tend 
to zero as more hypotheses are generated and the one with lowest error selected. then  for sufficiently large n  over the entire range where  is 
significantly greater than zero  leaving out only the left 
tail of the distribution   and   equa-
tion 1 . making these substitutions we finally obtain 
 omitting the c indexes  since 
	domingos 	1 

¡¡this formula is quite similar to the well-known laplace correction or m-estimate  cestnik  1 . its role for the number of hypotheses is similar to the m-estimate's role for the number of examples. the m-estimate gradually changes from the maximum likelihood estimate to the prior as the number of examples decreases; similarly  equation 1 gradually uncovers the prior as the number of hypotheses generated increases. the intuitive meaning of equation 1 is clear: when a learner generates a series of hypotheses and returns the one with lowest training-set error  the more hypotheses it generates the less sure we are that the observed error corresponds to the true error  and the more weight should be given to the a priori expected error. 
¡¡this result is intuitively satisfying  because it gives a mathematical basis for increasing model uncertainty as the amount of search performed increases. however  equation 1 as it stands is of limited practical use  because it converges very rapidly to  as more independent hypotheses are generated. as a result  for all but the earliest few hypotheses  the error estimate is quite insensitive to the empirical error 
¡¡¡¡¡¡this effect  however  is at least partly due to the fact that hypothesis dependences are being ignored  and as a result the empirical error of one hypothesis carries no information about the true error of another. in particular  only the empirical error of the chosen hypothesis carries information about its true error  resulting in the chosen hypothesis' expected error being the unalloyed prior in all a priori possible situations where the minimum empirical error is not the chosen hypothesis'  equation 1 . in practical learners  on the other hand  the hypotheses generated are typically very strongly dependent. thus  in general  all the empirical errors observed will carry information about the true error of the 
chosen hypothesis  and equation 1 should converge correspondingly slower to the prior term  we propose to model tins by replacing m in equation 1 by a slower-growing function of m  which can be thought of as the  effective number of independent hypotheses attempted.  for example  attempting ten hypotheses with given dependences between them may be equivalent  with respect to the convergence of equation 1 to  to attempting two independent hypotheses. thus  equation 1 provides a simple way of combining data-oriented  representation-oriented and processoriented information when estimating generalization error:  is the data-oriented component  the model's empirical error    is the representation-oriented component  a function of the model's form   and m is the process-oriented component  a function of the search process that led to the model . 
1 	application to rule induction 
most rule induction systems employ a set covering or 
 separate and conquer  search strategy  michaiski  1; 
1 machine 	learning 
clark and niblett  1 . rules are induced one at a time  and each rule starts with a training set composed of the examples not covered by any previous rules. a rule is induced by adding conditions one at a time  starting with none  i.e.  the rule initially covers the entire instance space . the next condition to add is chosen by attempting all possible conditions. conditions on symbolic attributes are typically of the form  where vij is a possible value of attribute ai. conditions on numeric attributes are typically of the form  or  where the thresholds  are usually values of the attribute that appear in the training set. in the beam search process used by many rule learners  at each step the best b versions of the rule according to some evaluation function are selected for further specialization. aq {michaiski  1  continues adding conditions until the rule is  pure   i.e.  until it covers examples of only one class . this can lead to severe overfitting. the latest version of the cn1 system  clark and niblett  1; clark and boswell  1  uses a simple and effective 
bayesian method to combat this: induction of a rule stops when no specialization improves its error rate  and the latter is computed using a laplace correction or mestimate. if is the number of examples covered by a 
rule and is the number of those examples it misclassifies  the conventional estimate of the rule's error rate is  but its m-estimate is: 

where eo is the rule's a priori error  which cn1 takes to be the error obtained by random guessing if all classes are equally likely:  =  c- 1 /c  where c is the number of classes. this prior value is given a weight of m examples  i.e.  the behavior of equation 1 is equivalent to having m additional examples covered by the rule  one of each class . cn1 uses m=c. as conditions are added  the rule covers fewer and fewer examples  and  tends to eo. thus a rule making more misclassifications may be preferred if it covers more examples  causing induction to stop earlier and reducing overfitting. clark and boswell  found this version of cn1 to be more accurate than c1  quinlan  1  on 1 of the 1 benchmark datasets they used for testing. however  this scheme ignores that  as more and more conditions are attempted  the probability of finding one that appears to reduce the rule's error merely by chance increases. this will lead the m-estimate to underestimate the chosen condition's true error  and cn1 to overfit. the upward correction made to er should increase with the number of conditions attempted. the process-oriented evaluation framework described in the previous section allows us to do this in a systematic way  as follows. 
¡¡equation 1 can be used to compare the hypotheses returned by k learners and choose the one with lowest predicted error. it can also be used to compare successive stages of the same learner  by taking to be the result of continuing the search of learner more hypotheses. in 

table 1: empirical results: error rates and theory sizes of default cn1 and cn1 with two versions of process-oriented evaluation  cn1-p1 and cn1-poe1 . 


particular  the successive stages can be the successive versions of a rule returned by cn1 or a similar  separate and conquer  rule learner. a natural choice for the prior expected error  for all rule versions is the default error rate  obtained by always predicting the most frequent class in the training set. the choice of slower-growing function of m is less obvious. one possibility is m' = logm  for m  based on an analogy with decision tree induction. when learning a tree using an algorithm like c1  each new hypothesis is obtained by modifying the previous one in only a fraction of the instance space  the fraction corresponding to the node currently being expanded   and this fraction becomes exponentially smaller as induction progresses. only an entire new level of the decision tree corresponds to an entirely new hypothesis. since the depth of the tree grows approximately with the logarithm of the number of nodes  we can take the equivalent number of independent hypotheses attempted m' to be proportional to the logarithm of the total number of hypotheses attempted m. since a rule corresponds to a path through a decision tree  both in its content and in the way it is induced by a system like cn1  we can apply a similar line of reasoning to the number of rules attempted.1 
¡¡let each hypothesis be one version of the rule attempted during the beam search. equation 1 does not need to be computed for every rule version generated during the beam search. this would introduce a preference for adding some conditions instead of others  which is unlikely to produce good results unless there is domain knowledge supporting such preferences. instead  equation 1 can be computed only once for each round. 
one round consists of generating every possible one-step specialization of each rule version in the beam  and selecting the b best. thus  if there are a attributes and v is the maximum number of values of any attribute 
¡¡¡¡1 in the experiments described below  the results were not sensitive to the base of the logarithms used. base 1  base e and base 1 all yielded practically indistinguishable error rates and theory sizes. the results reported are for base 1.  in the worst case   for numeric attributes   one round corresponds to 1 bav  rule versions. let mk be the total number of rule versions generated up to  and including  round k. round 1 consists of the initial rule with no conditions  and m1 = 1. induction stops when 
1. 
1 	experiments 
in order to test the effectiveness of process-oriented evaluation  default and process-oriented versions of cn1 were compared on the benchmark datasets previously used by clark and boswell  .1 the process-oriented versions were implemented by adding the necessary facilities to the cn1 source code. details of the earlier version of poe and its implementation can be found in  domingos  1b . cn1's laplace estimates are still used to choose the best b specializations in each round. this is preferable to using uncorrected estimates  since as implemented poe has no preference between hypotheses within the same round  and this is also a factor in avoiding overfitting. however  the laplace correction distorts the value of  used in equation 1. this will be particularly pronounced when there are many classes  since cn1 uses m = c. in order to minimize this problem  m = 1 was used with poe.1 
the experimental procedure of  clark and boswell  
1  was followed. each dataset was randomly divided into 1% for training and 1% for testing  and the error rate and theory size  total number of conditions  were measured for default cn1  cn1-poe1  the earlier version  and cn1-poe1  the version described in this paper . this was repeated 1 times. the average results and their standard deviations are shown in table 1 
¡¡¡¡1 with the exception of pole-and-cart  which is not available in the uci repository  blake et al.  1 . 
1
¡¡¡¡simply changing m = c to m = 1 in default cn1 does not change its performance on the datasets used. 
	1	 
¡¡¡¡there are some differences between cn1 s results and those reported in  clark and boswell  1 . this may be due 
	domingos 	1 

the results for cn1 and cn1-p1 are from  domingos  1b   
compared to cn1-p1  cn1-poe1 roughly main-
tains accuracy  lower error in five datasets  higher in five  same in one; 1% lower error on average  while reducing theory size in most datasets  lower in seven  higher in four  1 fewer conditions on average . this indicates that equation 1 is successfully deleting unnecessary conditions that the previous method retained. being in closed form  equation 1 is also much more efficient to evaluate than the integrals in  domingos  1b . experiments on two larger uci databases  shuttle and letter  showed cn1-poe1 learning faster than cn1  while maintaining accuracy and reducing theory size. 
1 	related work 
the literature on model selection and error estimation is very large  and we will not attempt to review it here. several pieces of previous work take into account the number of hypotheses being compared  and so can be considered early steps towards process-oriented evaluation. this includes notably systems that use bonferroni corrections when testing significance  e.g.   gaines  1; jensen and schmill  1 ; see also  kloekars and sax  1  . a key difference between these systems and what is proposed here is that they require a somewhat arbitrary choice of significance threshold  while this paper directly attempts to optimize the end goal  expected generalization error . also  the bonferroni correction does not take hypothesis dependencies into account  while the present framework offers  at least in principle  a way of doing so. 
¡¡quinlan and cameron-jones's   layered search  method for automatically selecting cn1's beam width can also be considered a form of process-oriented evaluation. while layered search and the approach proposed here have similar aims  their biases differ: layered search limits the search's width  while the present method limits its length. the latter may be more effective in reducing the fragmentation and small disjuncts problems  pagallo and haussler  1; holte et al.  1 . the assumptions made here are also clearer than those implicit in quinlan and cameron-jones's  measure. 
¡¡freund  recently proposed a form of processoriented evaluation that is closer to the pac-learning framework. it is an extension of the statistical query model  kearns  1  that attempts to obtain tighter bounds on generalization error by considering the tree of queries that the learner could make. while the general algorithm to obtain these bounds has exponential computational cost in the number of queries made  freund proposes a specialized version for algorithms based on local search  e.g.  cn1  that is more efficient  at the price of loosened bounds. how tight the bounds will be 
to the fact that the default version of cn1 uses a beam size of 1  whereas clark and boswell used b = 1. the distribution version of cn1 may also differ from the one used in  clark and boswell  1 . 
1 	machine learning 
in either case is still an open question; no empirical testing of freund's  method has been carried out so far. these bounds could be used for model selection by preferring the model with the lowest upper bound  for given parameters . however  as with bonferroni corrections  the result will in general depend on the choice of parameters  for which there is no clear criterion. while the approach proposed in the present paper directly obtains an estimate of the generalization error  it would also be useful to have a confidence interval for it  and freund's  method may be a path to it. 
¡¡evaluating models that are the result of a search process  not just of fitting the parameters of a predetermined structure  has traditionally not been a concern of statisticians. however  this is beginning to change  chatfield  1 . 
1 	conclusion 
two main types of model selection are currently available. in data-oriented evaluation  a hypothesis's score does not depend on its form or how the hypothesis was found  but only on its performance on the data. in representation-oriented evaluation  the score depends on the data and on the hypothesis's form  but not on the search process that led to it. recently  domingos  1b  we argued that the latter cannot be ignored  and proposed process-oriented evaluation  poe . however  in  domingos  1b  we assumed that all models searched had similar true error rates  and that all error rates were equally likely a priori. in this paper we removed these assumptions  and derived a simple approximation for the generalization error of the returned hypothesis as a function of the number of hypotheses searched. this approximation is a weighted average of the maximum likelihood estimate of the error and the prior expected error  that increasingly favors the prior as more models are attempted. this approximation gives a mathematical basis to the intuition that model uncertainty should increase with the amount of search conducted. 
¡¡in the future we plan to: study the statistical properties of equation 1  in particular when the sample size is not large enough to approximate it by equation 1; compare the method proposed here with other forms of process-oriented evaluation  e.g.  bonferroni corrections and layered search ; apply it to other learners; and study methods for accurately estimating the growth of the effective number of hypotheses m' in each of these learners. 
