 
relaxation in a conceptual hierarchy is proposed as a useful principle in the design of a visual recognition system. 
the domain 	of 	the 	implementation 	is 	kanade's 
origami world. the hierarchy is described along with the connection patterns that define the complete network. advantages of this system are a uniform approach to intermediate and low level vision  inherent parallel approach and sharing of partial results to dynamically cut down the search for a correct match. 
1. introduction 
     the paper argues for relaxation in a conceptual hierarchy as a useful principle in structuring parallel formulations of visual recognition tasks. a conceptual hierarchy is defined as an active semantic network of 
computing units logically partitioned into abstration levels. each level is in turn defined by unique sets of parameters enumerated over a fixed grain. these parameter spaces form the basis for recognizing the features associated with the various levels. relations extracted from physical constraints imposed by the world domain produce the connection patterns necessary for communication among the units in the network. relational connection patterns represent an extension to strict next neighbor communication of previous relaxation networks used in visual pattern recognition. goal directed vision uniformly blends into the overall recognition computation and provides model directed disambiguation at all abstraction levels. lateral inhibition also plays a role in dealing with various forms of missing information and  gaussian  noise in the input. 
　　　previous work has successfully concentrated on parallel computation of low level image processing functions  davrosen  ballard  zucker . rosenfeld's methods of cooperative  fuzzy   probabilistic  computation use relaxation as a control mechanism and next neighbor relations as connection patterns  rosen1. although these methods work well  even on noisy data  they do not readily extend to the multiple abstraction level approach necessary to attack higher level recognition tasks. ballard's hough transformsfballardl do allow generalization to multiple 
this work was supported in part by darpa under contracts n1-c1 and n1-c1. 
also by ibm under graduate residency scholarship. levels necessary tor recognition of complex visual features but do not allow for dynamic interaction of partial results and feedback that a control mechanism such as relaxation permits. evidence for relaxation as a control paradigm in higher levels of vision is pointed out by hinton. his puppet finders use relaxation of multiple hypotheses about existence of puppets in a scene to do segmentation and labeling hinton . unfortunately  his system says little about the role of low level vision. the method presented here uniformly manages computation of both low level intrinsic functions pre segmentation tasks  and intermediate level feature extraction  segmenting the input into semanticaly meaningful units . 
1. vision world domain 
     to build a system incorporating these principles  1 chose a relatively simple visible world. kanade's origami world kanadel  provides a domain in which the computations necessary to  see  were straightforward and readily available  kanade . . origami world was already divided into suitable feature sets and the use of the skew symmetry heuristic made extraction of three dimensional orientation simpler than other known methods. objects in the origami world  such as the ones in figure 1  were easy to represent and yet captured all important aspects of building a parallel network. 


figure 1. origami world objects 
     kanade's world also proves important in illustrating one of the main advantages of the present formulation. missing information in the input scene due to self-occlusion forces kanade to use a parallel line heuristic to disambiguate an otherwise underdetermined problem. he cannot sequentially enumerate all the possibilities in the search space of gradients matched to the search space of allowable objects. this search problem is never encountered by this system. goal directed disambiguation combined with continual communication of partial results dynamically prunes the search tree thereby eliminating the problem without incorporation of any heuristic. 
     in terms previously described  objects in this system are complex features  ie many parameters  that have explicit names associated with them such as box  cube  squat  rhomboid prism. only objects modeled at the various levels of the system are recognized. the purpose here is to label already modeled entities goal oriented pattern matching . there is no explicit validation of arbitrary origami world objects by doing  waltz world  validation labeling. recogniton of objects can be extended to be view independent and  in fact  allows for the generation of the inverse transformation between instance and model. 
     input to the system is in the form of raw edge data  such as that extracted by convolving digitized images to produce primal sketches marr . we assume two dimensional orthographic projection of origami world objects. the current implementation takes directed line segments as input. real world input may be added later. 
     although origami world might seem too simplistic  we were able to generalize the representations used in this model to describe more complex 1d rigid solids ballsabb  and effectively generalize the applicability of this approach to a more complex domain. the general solution assumes appropriate breakdown of the domain into levels  hierarchical component parts  and appropriate computation of suitable intrinsic parameters  ballard1 . how this was done in origami world will be the next topic 
1. hierarchical levels in the network 
     segmenting knowledge about the domain intc abstraction levels is one of the key parts of the recognition algorithm. first  a level in this model is defined by: 
1. - a unique set of parameters representing the features extracted on that level of abstraction. in effect  this defines the semantics associated with the level although it is the connection pattern that truly implements meaning. 
1. - a fixed grain for each of the parameters. for example  the spatial location corresponding to points on the field of view retinotropic map  is represented by a 1d parameter space:  x y  with a  1  grain. this allows association of any feature with any projected spatial location. 
1 

output of the unit reflects its current confidence level. 
　　　as the level of complexity rises  only the  important  combinations of parameters are represented.  important  means important to recognize in the particular world the system is trying to see. this reduces the number of units drastically since many arbitrarily detailed units can be effectively ignored. such an unknown combination may be parsed by the system but no unit exists to represent it at the top level. for example  the object in figure 1  a folded paper w  can be parsed up to the plane level in the model  but has no explicit representation in the objects level and therefore causes no explicit recongnition  ie. a single unit to saturate  as such at the top level. the resulting selective enumeration can be seen as the dividing line between low level image parsing and higher level image interpretation. adding the w-object requires the addition of an unit at the object level and the addition of the connections to that unit. how the unit is connected into the existing pattern is described in the next section. 

figure 1. w-folded paper object 
1. connections in the network 
     the relations that define the connections of a unit on any level define the semantics associated with it. the units comprising each level behave uniformly regardless of the parameters associated. at present  the only difference between instantiations of units is the connection patterns. behavior is therefore completely determined by connected inputs to that unit. the connection pattern for any one unit on level n with a group of associated parameters  l..n  is defined by three relations: 
1. rbu  bottom up  is the projection of the complete relation resulting in the set of connections mapping units onto the level above. the relation maps one onto many. it is only necessary that the relations allow parameters to be enumerated in some regular way. in most cases the relation can be derived from physical constraints. for example  in the current implementation  all relations are a result of geometric principles  making it possible to generate them in a regular and exhaustive manner. 
1. rtd  top down  is the projection of the complete relation resulting in the set of connections mapping a given unit onto the level below. the relation also maps one onto many. it intuitively represents the top down  goal directed information feeding back onto the lower levels. 
1. r j n h  inhibit  defines the set of inhibitory connections between competing units. for a given unit  elements related in this way exist on the same conceptual level and represent certain predefined ambiguities arising from noise and occlusion. these ambiguities give rise to subsets of probable interpretations for imperfect input. competition among these alternatives  to be resolved by additional input from bottom up and top down relations  

figure 1. possible up connections for ijoint to plane. 
     for the top down connection sets  the inverse relation is used. all possible ijoints are mapped that could have generated the instance of the skewed rectangle represented by the parallelogram. if the description of the plane level parameters is complete  some parameters may be considered spurious to the recognition algorithm  this set is one parallelogram generating only its directly constituent ijoints providing strong feedback indications for the existense of those ijoints in the input scene. figure 1 shows the resulting ijoints. 

figure 1  possible down connections for plane to ijoint. 
     paramater values for computation of specific ijoint parms are gotten from simple trigonometry applied to plane parameters. if it had been determined  for example  that scale parms where unimportant to the unique determination of objects in this case  the number of top down connections for this parallelogram would have to be 

multiplied by the grain of the scale space since all possible ljoints giving rise to this plane have to be enhanced. otherwise  possibly valid ljoints receive no feedback. 
     the third and final example is of the inhibitory set of an ljoint. this portion of the inhibitory set is meant to deal with noisy input  gaussian noise . figure 1 shows a computation of a next neighbor relation along each of the parameters describing the ljoint and in this case  the inhibitory set. competing units will serve to sharpen the perception of a specific ljoint at the cost of its logical close neighbors in feature space. 

figure 1. inhibitory set for ijoints 
     a value difference of one was used to define next neighbor in this case. one is not a magic value and may be changed depending on the desired immediate scope of 
influence of the inhibition exerted. a milder form of enlarging the scope of inhibition is realized by letting the effects ripple to larger neighborhoods in later iterations of the overall computation. 
     having detailed examples of the connection patterns among the levels  we now turn to to a discussion of the black box of figure 1  the computing unit. 
1. computing with connections 
this section is organized in two parts: 
     the first discusses the computing unit. the second defines the notion of iterative convergence in this model. 
1 computing units 
     to understand the behavior of an individual unit  activation level must be defined. activation level is the value that the unit computes at every iteration of the relaxation process. it reflects the current confidence level that the feature represented exists. since the simulation cannot instantaneously digest all the input necessary to make the recognition decisions  the output of a unit must be a partial result and must be stored as the current level and updated according to the iteration number and the input. the activation level is represented by a real number in the somewhat arbitrary  small range  1 . a higher activation level implies a more active and confident state for this unit. it attains maximum confidence at 1 and can go no lower than 1. the max value represents the absolute decision point that the feature really exists in the input image. enhancement and inhibition across all parms and units proceeds at the maximum rate at this point. 
     the output of the unit is a direct function of the current activation level and therefore communicates the partial result desired at the current iteration in the 
relaxation process. 
activation levels are updated according to the following formulas: 
     i am currently simulating the model and can as of yet only make simple claims about complex behavior. a scheme for dealing with occlusion and multiple objects is in the process of implementation. also  studies of small networks with the same properties are encouraging  feldball . convergence in the system is defined  however. 
1 iterative convergence 
     absolute convergence occurs on a unit level when that unit attains the maximum activation level and its inhibitive 

1 


     for example  if we are provided with perfect information about a single object in an input scene we get a convergence pattern as in figure 1. in this case we see a cone of convergence . in the case of multiple objects in the scene there may be more than one cone  and they may overlap in their sphere. as long as no two conflicting features  groups of parameters  converge  there is no inherent contradiction in what is seen. conservation of this property is one motivation for inhibitory connections in the network. 
     it is not always desirable to converge to a specific single value at the top level of the cone of convergence. if the input information is too noisy or ambiguous  due to excessive occlusion    the system should rightfully never attain a stable state. if the object in the image is not represented explicitly in network  it should be parsed to the best ability of the lower levels and no convergence should occur at the top of the cone. a cone with the top cut off is the result. it remains to prove that the system converges in  ill behaved  situations. this can only be defined in relation to human performance on the same recognition tasks and assumes the system is used in robot vision. 
1. conclusions 
     in conclusion  we can see that relaxation in a conceptual hierarchy is a useful principle in visual relaxation tasks for several reasons: 
1. it provides a method for uniformly managing the computation of low level intrinsic functions and intermediate level feature extraction. the goal driven segmentation and interpretation of parts of the scene directly influences computation of such low level features as edges and lines. a consistent  understandable role for top down influence blends with bottom up parsing and competition among alternate hypotheses in the presence of imperfect input. 
1. also clear is that in computing the same information as kanade  kanade1  the system does not suffer the problems of a sequential formulation. namely  the sequential formulation of the recognition algorithm must explicitly choose an order for exploring the large search space of alternate hypotheses. the order may or may not result in finding the most suitable hypothesis for the current recognition problem. exhaustively exploring all possible search strategies is at best time consuming and at worst  impossible. the parallel formulation presented here does not suffer this problem since it allows the pruning of the search tree dynamically by incorporating results from partial computations in the well formed hierarchy. as pointed out by kornfeld  korn   this benefit is reaped regardless of whether the algorithm is simulated or actually run on parallel hardware. 
1. the system is inherently parallel and therefore can be readily extended to form the basis for parallel architectures. 
     finally  we are studying relaxation in conceptual hierarchies as a general paradigm  sabbah  feldball . it seems to extend to higher levels of abstraction readily. it promises to have nice noise resilience and disambiguation properties. amazingly enough  it seems to share many similarities with animal vision. 
