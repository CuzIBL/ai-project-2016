 
this paper presents a method to generate nonmonotonic rules with exceptions from positive/negative examples and background knowledge in inductive logic programming. we adopt extended logic programs as the form of programs to be learned  where two kinds of negation-negation as failure and classical negation-are effectively used in the presence of incomplete information. while default rules axe generated as specialization of general rules that cover positive examples  exceptions to general rules are identified from negative examples and are then generalized to rules for cancellation of defaults. we implemented the learning system lelp based on the proposed method. in lelp  when the numbers of positive and negative examples are very close  either parallel default rules with positive and negative consequents or nondeterministic rules are learned. moreover  hierarchical defaults can also be learned by recursively calling the exception identification algorithm. 
1 introduction 
inductive logic programming  ilp  is a research area which provides theoretical frameworks and practical algorithms for inductive learning of relational descriptions in the form of logic programs  1  1  1 . most previous work on ilp consider definite horn programs or classical clausal programs in the form of logic programs to be learned. however  research work on knowledge representation in ai has shown that such monotonic programs are not adequate to represent our commonsense knowledge including notions of concepts and taxonomies. in this respect  there have been much work on nonmonotonic reasoning in ai. to learn default rules or concepts in taxonomic hierarchy  we thus need a learning mechanism that can deal with nonmonotonic reasoning. 
¡¡on the other hand  recent advances on theories of logic programming and nonmonotonic reasoning have revealed that logic programs with negation as failure  naf  is an 
automated reasoning 
yoshimitsu kudoh 
division of electronics and information engineering 
hokkaido university 
n-1 w-1  sapporo 1  japan kudoodb.huee.hokudai.ac.jp 
appropriate tool for knowledge representation . normal logic programs  nlps  are the class of programs in which naf is allowed to appear freely in bodies of rules. nlps are useful not only to represent default rules or rules with exceptions but also to write shorter and clearer programs than definite programs in many cases . learning nlps has recently been considered in such as  1  1  1  1 . 
¡¡while learning nlps is an important step towards a better learning tool  there is still a limitation as a knowledge representation tool: nlps do not allow us to deal directly with incomplete information . nlps automatically applies the closed world assumption  cwa  to all predicates  and any query is answered either yes or no  in which the latter negative answer is the result of cwa. in the context of inductive concept learning  the automatic application of cwa is not appropriate in the presence of both positive and negative examples. positive examples represent instances of the target concept  while negative examples are non-instances. by cwa other objects are assumed non-instances  but then the role of negar tive examples is not clear because it is as if we supply a complete classification of all objects. this causes the paradox pointed out by de raedt and bruynooghe : if everything is known  why should we still learn something  in the real world  we may not know whether some objects are positive or negative. but such incomplete information cannot be represented by nlps. 
¡¡to overcome the above problem of nlps  we propose in this paper a new learning method which can deal with incomplete information in the form of extended logic programs  elps . elps are introduced by gelfond and lifschitz  to extend the class of nlps by including classical negation  or explicit negation . the semantics of elps is given by the notion of answer sets  and is an extension of the stable model semantics. the answer to a ground query a is either yes  no  or unknown  depending on whether the answer set contains a     or neither. using elps  the role of negative examples becomes clear  and any object not contained in either positive or negative examples is considered unknown unless the learned theory says that it must or must not be in that concept. 
in this paper  we present a system  called lelp 
 learning elps   to learn default rules with exceptions 
in the form of extended logic programs given incomplete positive and negative examples and background knowledge. lelp first generates candidate rules from positive examples  or negative examples if non-instances are much more than instances  and background knowledge in an ordinary ilp framework. exceptions can be identified as negative examples  or positive examples if candidate rules have negative consequents  that are derived from the generated monotonic rules and background knowledge. default rules with naf are then computed by specializing candidate rules using the open world specialization  ows  algorithm. this ows algorithm is closely related to bain and muggleton's cws algorithm   but works better in the three-valued semantics. then  default cancellation rules are generated to cover exceptions using an ordinary ilp framework. 
¡¡in the real world  it is not easy to know that a general default rule should have the positive or negative consequent. in lelp  it is determined according to the ratio of positive examples. nevertheless  if it is still hard to know which is more general  lelp can generate nondeterministic rules in the context of the answer set semantics. furthermore  by calling the ows algorithm recursively  lelp can generate hierarchical default rules. 
¡¡the rest of this paper is organized as follows. section 1 outlines how our system lelp produces elps to learn simple default rules. section 1 extends lelp to deal with complex concept structures with hierarchical exceptions. section 1 presents related work  and section 1 concludes the paper. 


where l stands for the literal complementary to l. then  each answer set is the set of atoms in an extension of the default theory. we say that a literal l is entailed by an elp p if l is contained in every answer set of p. while we adopt the answer set semantics in this par per  other semantics for elps may be applicable to our learning framework with minor modification. 
¡¡we call a rule having a positive literal in its head positive rule  and a rule having a negative literal in its head negative rule. in the following  we denote classical negation  as - and naf not as  + in programs. 
¡¡the completeness and consistency of concept learning  see  1  1  for instance  can be reformulated in the three-valued setting as follows. let bg be an elp as background knowledge  e a set of positive/negative literals as positive/negative examples  and ra set of rules as hypotheses. r is complete with respect to bg and e if for every e € 1  e is entailed by bg u r  r covers e . r is consistent with bg and e if for any e € e  eis not entailed by bgur  r does not cover e . note here that positive examples are not given any higher priority than negative ones. namely  both positive and negative examples are to be covered by the learned rules that are consistent with background knowledge and examples. thus  we will learn both positive and negative rules: no cwa is assumed to derive non-instances  see also  . 
¡¡although both positive and negative rules are generated by lelp  each default rule for the target concept should be either positive or negative. in lelp  it is determined according to the ratio of positive examples to all objects. in the following  we assume that positive rule is learned as a general rule unless otherwise specified. 
1 	generating general rules 
in algorithm 1  given positive  resp. negative  examples e and background knowledge bg lelp generates general rules t to cover every example in e using an ordinary ilp technique. we denote this part of algorithm as genrules e bg t . in generating positive  resp. 
	inoue & kudoh 	1 
1 	specializing rules using naf 
the general rules computed to cover the positive  resp. negative  examples by genrules e bg t  may also cover the complements of some of negative  resp. positive  examples. to specialize general rules  we propose the algorithm of open world specialization  ows . the ows algorithm is closely related to bain and muggleton's closed world specialization  cws  . like cws  ows produce rules with naf as default rules. unlike cws  however  ows does not apply the closed world assumption  cwa  to identify non-instances of the target concept. in ows  exceptions are identified as objects contained in negative examples  or positive examples if the general rule is negative  such that they are proved from the general rule with background knowledge and positive  or negative  examples. 
¡¡in the following ows algorithm  we assume here that each general rule in t is positive. 
1 	cancellation rules 
in the ows algorithm  the set ab of exceptions is output as a set of ground atoms. however  if exceptions have some common properties  this expression is not informative and rules about exceptions are useful. these rules work as default cancellation rules. 
¡¡after applying ows  each exception is in the form of ground atom whose predicate is ab i rules about exceptions have such abnormal predicates in their heads and are results of generalizations of some abnormal atoms. when such a common rule cannot be generated or there are some exceptions that cannot be covered by such a rule  those exceptions are left as they are. 
¡¡since exceptions are not anticipated in general  rules about exceptions should be used to derive only exceptions. in fact  exceptions are usually minimized in nonmonotonic reasoning. to this end  we apply a limited form of cwa here. if a rule about exceptions is too gen-

eral  that is  it derives negative facts more than expected  it should be rejected. this test can be done easily using a bottom-up model generation procedure. the algorithm to generate rules about exceptions is as follows. 
automated reasoning 

	inoue & kudoh 	1 

automated reasoning 

1 	related work 
bain and muggleton's cws algorithm  has been applied to non-monotonic versions of cigol and golem in  l  and a learning algorithm that can acquire hierarchical programs in  l1  cws produces default rules with naf in stratified nlps. since cws is based on cwa in the two-valued setting  it regards every ground atom that is not contained in an intended model as an exception. in lelp  on the other hand  ows is employed instead of cws  and incomplete information can be represented in elps with the three-valued semantics. 
¡¡tracynot by bergadano et al.  learns stratified nlps using trace information of sldnf derivations. 
since this system needs the hypothesis space in advance  it does not invent a new predicate like abiexpressing exceptions  and hence seems more suitable for learning rules with negative knowledge and cwa rather than learning defaults. martin and vrain  ll  use the threevalued semantics for nlps in their inductive framework. since they do not adopt elps  cwa is still employed and two kinds of negation are not distinguished. 
¡¡while no previous work adopts full elps in the form of learned programs  a limited form of classical negation has been used in  1  1 . de raedt and bruynooghe  firstly discussed the importance of the three-valued semantics in ilp. however  since they did not allow naf  an explicit list of exceptions is necessary for each rule  which causes the qualification problem in al wrobel  also used exception lists to specialize over-general rules  but their underlying language is monotonic first-order. dimopoulos and kakas  propose a learning method that can acquire rules with hierarchical exceptions. they also do not use naf to represent defaults  but adopt their own nonmonotonic logic. moreover  using the approach of   one has to determine whether each negative information should be used in the usual specialization process or in the exception identification process. in our approach  such distinction can be clearly done by an appropriate usage of naf and classical negation. 
¡¡finally  in any previous work  nondeterministic rules cannot be generated  and hence commonsense knowledge with multiple extensions cannot be learned. 
1 	conclusion 
this paper proposed new techniques to learn nonmonotonic rules with exceptions  and introduced the learning system lelp. extended logic programs are adopted as program forms  in which two kinds of negation are effectively used in the presence of incomplete information. default rules are generated using ows  and their exceptions are then generalized to cancellation rules. lelp can also learn parallel/nondeterministic rules and hierarchical defaults within the three-valued semantics. 
¡¡in this paper  we treated every explicit negative information as an exception to a positive hypothesis. in the real world  however  negative knowledge may often be irrelevant to the concepts to be learned. in this respect  a method of separation of noise from exceptions has been proposed in   another approach is that we may add information that each concept can have exceptions or not or that cwa can be applied or not. these extensions can easily be accommodated within lelp. 
