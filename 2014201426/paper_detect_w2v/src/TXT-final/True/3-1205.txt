
in this paper we present a novel algorithm to learn a score distribution over the nodes of a labeled graph  directed or undirected . markov chain theory is used to define the model of a random walker that converges to a score distribution which depends both on the graph connectivity and on the node labels. a supervised learning task is defined on the given graph by assigning a target score for some nodes and a training algorithm based on error backpropagation through the graph is devised to learn the model parameters. the trained model can assign scores to the graph nodes generalizing the criteria provided by the supervisor in the examples. the proposed algorithm has been applied to learn a ranking function for web pages. the experimental results show the effectiveness of the proposed technique in reorganizing the rank accordingly to the examples provided in the training set.
1 introduction
　in this paper we propose an algorithm able to learn a
　generic distribution of values over the nodes of a graph with symbolic labels. in many applications  the input data is naturally represented by a labeled graph. however  the actual lack of adequate algorithms to directly process this structured data commonly forces to convert the input patterns to vectors of real numbers  loosing significant information. in particular   web like  data structures are common in many pattern recognition tasks and possible applications range from object recognition in segmented images to molecular analysis in bioinformatics  and so on.
　like in the pagerank algorithm  page et al.  1   the score of a node is assumed to correspond to the probability that a random walker will visit that node at any given time. the score distribution computed by the random walker depends both on the connectivity graph and the labels assigned to each node. markov chain theory allows to define a set of constrains on the parameters of the random walker in order to guarantee some desired features like convergence and independence of the final distribution on the initial state. a learning algorithm which is based on error back-propagation through the graph is used to optimize the parameters of the random walker minimizing the distance of the scores assigned by the walker from the target values provided by a supervisor for some nodes in the graph.
　previous work in the same field achieved very limited results. for example  in  kondor and lafferty  1  the authors present an algorithm that processes only non labeled graphs  while the technique presented in  chang et al.  1  is very application specific and features very limited generalization capabilities. in  tsoi et al.  1  a method to modify pagerank scores computed over a collection of pages on the basis of a supervisor's feedback is proposed. this algorithm has limited approximation capabilities  since it controls only a term of the pagerank equation and it does not optimize how the scores flow though the connectivity graph.
　the algorithm presented in this paper allows to learn target scores assigned as real numbers  and this makes it more general than the one proposed in  diligenti et al.  1  which allows only to specify a set of nodes whose scores should be increased or decreased.
　the paper is organized as follows. in the next section the web surfer model is introduced. then  in section 1 the learning algorithm based on error back-propagation is described  while the experimental results are presented in section 1. finally  in section 1 the conclusions are drawn.
1 random walks
　we consider the model of a graph walker that can perform one out of two atomic actions while visiting the web graph: jumping to a node of the graph  action j  and following a hyperlink from the current node  action l .
　in general  the action taken by the surfer will depend on the label and outlinks from the node where it is located. we can model the user's behavior by a set of probabilities which depend on the current node: x l|q  is the probability of following one hyperlink from node q  and x j|q  = 1   x l|q  is the probability of jumping from node q.
　the two actions need to specify their targets: x p|q j  is the probability of jumping from node q to node p  and x p|q l  is the probability of selecting a hyperlink from node q to node p. x p|q l  is not null only for the nodes p linked directly by node q  i.e. p （ ch q   being ch q  the set of the children of node q in the graph g. these sets of values must satisfy the probability normalization constraints
 q （ g pp（g x p|q j  = 1 and pp（ch q  x p|q l  = 1.
　the model considers a temporal sequence of actions performed by the surfer and it can be used to compute the probability xp t  that the surfer is located in node p at time t. the probability distribution on the nodes of the web is updated by taking into account the actions at time t+1 using the following equation
xp t + 1 	= x x p|q j  ， x j|q  ， xq t  +	 1  q（g
	+	x x p|q l  ， x l|q  ， xq t   
q（pa p 
where pa p  is the set of the parents of node p. the score xp of a node p is computed using the stationary distribution of the markov chain defined by equation  1   i.e. xp = limt★± xp t  . the probability distribution over all the nodes at time t is represented by the vector x t  =  x1 t  ... xn t  1  being n the total number of nodes.
　an interesting property of this model is that  under the assumption that x j|q  1= 1 and x p|q j  1= 1   p q （ g  the stationary distribution x  does not depend on the initial state vector x 1   see  diligenti et al.  1  . for example this is true for the pagerank equation for which x j|q  = 1   d  being 1   d   1  and x p|q j  = n1 .
　the random walker model of equation  1  is a simplified version of the model proposed in  diligenti et al.  1   where two additional actions are considered by allowing the surfer to follow backlinks or to remain in the same node. the learning algorithm proposed in the next section can be easily extended to this more general model.
1 learning the score distribution
　the random surfer assigns the scores according to the probabilities x p|q j   x j|q   x p|q l   and x l|q . in most of the approaches proposed in the literature  these parameters are predefined or computed from some features describing the node p and/or the node q. for example  setting the parameters to   and
  we obtain the popular pagerank random surfer  page et al.  1 .
　in the general case  the model has a number of parameters quadratic in the number of nodes and it is infeasible to estimate these parameters without any assumption to reduce their number. in particular  we assume that the labels of nodes can assume n distinct values c = {c1 ... cn} and that the surfer's behavior for any node p is dependent only on its label cp （ c and the labels of the nodes pointed by p. therefore  the parameters of the model are rewritten as:

where cq （ c is the label of node q and |cp| is the number of nodes with label cp. the parameter x cp|cq l  represents the tendency of the surfer to follow a hyperlink from a node with label cq to a node with label cp  the parameter x cp|cq j  is the probability of the surfer to jump from a node with label cq to a node with label cp and x l|ci  = 1   x j|ci  represents the probability that the surfer decides to follow a link out of a node with label ci. in the following  we indicate as p the set of model parameters. t   j   b indicate transition  jump and behavior parameters  respectively. in particular  it holds that: p = {t  j b}  t = {x ci|cj l  i j = 1 ... n}  j = {x ci|cj j  i j = 1 ... n}  and b = {x l|ci  i =
1 ... n}. under these assumptions  the random surfer equation becomes 
　 1  |c | q（g
  
typically  the values x cp|cq l  are initialized to 1  such that all links are equally likely to be followed. because of the model assumptions  the stronger is the tendency of the surfer to move to a node with a specific label  the higher are the scores associated to the nodes with that label. these assumptions reduce the number of free parameters to 1， n1 n +n = 1 ， n1   n  being n the number of distinct labels.
1 learning the parameters
　we assume that a supervisor provides a set of examples e in the graph g  where each example s （ e is a node associated to a target score.
　consider the following cost function for the scores computed by a surfer model using the set of parameters p 
	  	 1 
where eps is called instant error for the node s. such error takes into account the distance between the target score and the value returned by the ranking function.
　we want to minimize the cost function  1  by gradient descent with respect to a generic parameter w （ p  i.e. 
	 .	 1 
　using a procedure similar to the back-propagation through time algorithm  bptt   haykin  1   commonly used in training recurrent neural networks  for each node s in the training set we unfold the graph starting from s at time t and moving backward in time through the path followed by the random surfer. the graph unfolding is basically a backward breadth-first visit of the graph  starting from the target node at time t  where each new unfolded level corresponds to moving backward of one time step. each unfolded node is associated to the variable xq t t  where q is the corresponding graph node  t is the number of backward steps from the target node. similarly  the parameters of the model are split into an independent set of parameters corresponding to the

	 a 	 b 
figure 1:  a  a graph composed by three nodes.  b  the first four levels of the unfolding for the graph  starting from node b at time t. u t  indicates the set of nodes at the level t of the unfolding  corresponding to the time step t . in the example  u t  = {b}  u t  1  = {a c}  u t  1  = {a} and u t  1  = {a}.
different time steps. we indicate with p t  the model parameters at time t and with w t  （ p t  the value of any given parameter at time t. an infinite unfolding is perfectly equivalent to the original graph with respect to the computation of the score for a target node s. however  the unfolding takes into account only the score propagation due to the forward links  whereas the contribution of jumps  coming from any node in the graph  is neglected.
　in the following  we indicate as us t  the set of nodes contained in the unfolding centered on node s for the time step t. see figure 1 for an example of graph unfolding.
　thanks to the parameter decoupling in the unfolding  a parameter w t  influences only the scores at the following time step. thus  the derivative of the instant error with respect to a parameter w t  can be written as 
	.	 1 
 w t 	 xp  t + 1 	 w t  p（g
　assuming that the parameters of the surfer are stationary  i.e. they do not depend on time: w t  = w  t   the derivative with respect to a single parameter can be obtained summing all the contributions for the single time steps 
	 w	t= ± p（g  xp  t + 1 	 w t 
　the term  in equation  1  can not be directly computed  unless when t = t   1 s = p. in this latter case  it
holds that 
	 .	 1 
let δp t  indicate . if p is not a target node  then
 can be rewritten as 
 xp    	 xz   + 1 	 xp    
z（g
		 1 

where ch p  is the set of children of node p in the unfolding and p p t1 ★ z t1  indicates the probability of the surfer to follow a path in the unfolding connecting the node p at time t1 to the node z at time t1. only the influence of transition parameters on the score of node p was taken into account in the propagation of the delta values.
　neglecting the influence of the jump parameters  i.e. taking into account only the influence due to link following  is an accurate approximation when the probability of following a link is much higher than the probability of jumping  like it is commonly assumed in pagerank . however  the experimental results show that such an approximation provides good results in the general case.
　a generic node p at time t has influence over a supervised node s at time t proportional to the probability p p t ★ s t  that the surfer will follow the path from the node p at time t to the central node s at time t. an implication of this result is that farther nodes will have less influence  given the exponential decrease of the probability of following a path with its length. in particular  a node p influences only if p p t ★ s t  1= 1. since us t   t  is the set of nodes from which it is possible to reach node s at time t starting at time t  only for the nodes in this set it holds that p p t ★ s t  1= 1. thus  in equation  1  not all the score variables at time t + 1 must be considered in the application of the chain rule. inserting equation  1  into  1   and limiting the chain rule only to the nodes with a non-zero influence  yields 

　since the stable point of the system does not depend on the initial state  without loss of generality  the unfolding can be interrupted at time 1  the graph is unfolded t times  assuming that t is large enough so that the state before time 1 has no effect on the final state  this is also expressed by the fact that the probability of following a path in the unfolding p p t ★ s t  converges to 1 when the path length t   t increases  
  1 
　in the following sections  we compute the derivative  to be inserted into equation  1  for a transition  jump and behavior parameter.
1 learning the transition parameters
for a transition parameter w = x ci|cj l   equation  1  p
shows that. on the other hand if cp = ci  it holds that 

　the application of the derivative rule for a fraction of functions  yields that when cp = ci 
		 1 
where |k （ ch q  : ck = ci| is the number of children nodes of q having label ci.
　without loss of generality  it can be assumed that the system has already converged at time 1  i.e. xq = xq t  t = 1 ... t   q （ g . inserting equation  1  into  1  and  after removing all temporal dependencies from the parameters and from the node scores under the assumption of model stationarity and convergence at time 1  the derivative of the instant error for the considered transition parameter is obtained.
　averaging over all the examples  the model parameters can be updated by gradient descent according to equation  1 . the new parameters can be used to compute a new score distribution over the nodes . function optimization and score estimation can be iterated through multiple epochs  till a termination criterion is satisfied.
1 learning the jump parameters
　for a jump parameter w = x ci|cj j   equation  1  shows that it holds that 

where |ci| indicates the number of nodes in the graph g with label ci. the previous equation can be inserted into  1  and  removing all temporal dependencies under the assumption of model convergence and stationarity  yields the derivative of the instant error with respect to a jump parameter.
1 learning the surfer action bias
　following a procedure similar to that shown for the transition and jump parameters  we start computing the derivative  for a single parameter.
in particular  equation  1  shows that 
 1  where the surfer model is assumed to respect the probabilistic constrains at every time step  i.e. x j|ci  t  = 1 x l|ci  t . merging equation  1  and  1  under the assumption of model stationarity and convergence at time 1  yields the derivative of the instant error with respect to a behavior parameter. all parameters can be simultaneously updated as described in section 1  1 and 1 to train the model. after the updating  the jump and behavior parameters must be normalized to respect the probabilistic constrains.
　finally  using an em-style algorithm  dempster et al.  1  the adapted surfer model can re-surf the web graph yielding a new estimate of the scores. the procedure can be repeated till a stop criterion is satisfied. since the parameters are estimated from a few levels deep unfoldings  only a small set of nodes must be considered at each iteration. the main overhead of the proposed learning algorithm is the full pagerank computation that must be completed at each step. our experiments show that usually only very few iterations are needed to converge.
1 experimental results
　the experiments were performed on two datasets  which were collected by focus crawling the web  diligenti et al.  1   on the topics  wine  and  playstation   respectively. the  wine  dataset contains 1.1 documents  while the  playstation  one 1. the documents were clustered using a hierarchical version of the k-means algorithm  duda and hart  1 . in particular  in two different runs  we clustered the pages from the dataset on topic  playstation  into 1 sets  whereas the dataset on topic  wine  was clustered into 1 and 1 sets.
　two topic-specific search engines were built containing the downloaded documents. the rank of each page in the dataset was computed using a random walker producing the standard pagerank scores. we tested the quality of the ranking function by submitting a set of queries to the search engine whose results were sorted by the scores assigned by the random surfer. by browsing the results  we found pages which were authorities on the specific topic but were  incorrectly  not inserted in the top positions of the result list. such pages were selected as examples of pages which should get a higher rank  positive examples . a target score equal to one  the maximum reachable score according to the model  was assigned to the positive examples. on the other hand  we found pages having a high rank which were not authorities for the topic. such pages were selected as negative examples and a target score equal to zero was assigned to those pages. the learning algorithm  as previously described was applied to these datasets  using the selected examples. only a small set of examples  1  was typically labeled as positive or negative in any single experiment.
1 analysis of the effects of learning
　this first group of experiments aims at demonstrating that the surfer model as learned by the proposed training algorithm  could not be generated by a non-adaptive schema as the focused versions of pagerank proposed in the literature.
　in figure 1  we show the values of the transition  jump and behavior parameters for each cluster  resulting from the



figure 1: values of the transition  jump and behavior parameters after learning for the  playstation  dataset. for each cluster  x axis   all the n parameters to the other clusters are sequentially plotted.
training sessions for a surfer on the topic  playstation . the learned parameters represent a complex interaction of the resulting surfer with the page clusters where a few clusters are strongly boosted and a few other strongly demoted. this behavior  expecially targeted demotions  can not be expressed by the focused versions of pagerank  richardson and domingos  1; diligenti et al.  1   which simply boost all the parameters leading to the clusters of relevant pages  according to the simplistic assumption that relevant pages are more likely to point to other relevant pages . this demonstrates that the trained surfer is able to exploit hierarchies of topics to model the complex path leading to good/bad pages.
　in figure 1- a  and 1- b   there are plotted the probabilities that the surfer is located in a page of a specific cluster. in particular we show the results for the dataset on topic  wine  clustered into 1 groups and a random graph with 1 nodes clustered into 1 groups. the learning algorithm is able to increase the likelihood of the random surfer to visit pages belonging to a set of clusters  while decreasing its probability of visiting pages belonging to other clusters.
　figure 1 shows the value of the cost function ep at each iteration. each plot reports the cost function when a specific set of parameters is optimized during training. the behavior parameters are less effective than the jump parameters in optimizing the surfer model. however  neither the behavior or the jump parameters are as effective as the transition parameters. as expected  when all parameters considered during the

 a 

 b 
figure 1: plots of the normalized probability of the surfer of being located in a page of a given cluster before and after learning.  a  results on the crawl for the topic  wine  with 1 clusters.  b  results on a random graph with 1 nodes and 1 clusters.

1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 epoch
figure 1: values of cost function as a function of the iteration number. each plot reports the cost function when a specific set of parameters is considered during training.
learning process  the cost function reaches its minimum.
　in figure 1  there are shown the scores of the pages of cluster 1 of the dataset  playstation  before and after learning. this cluster was manually detected as strongly on the topic  playstation  and 1 pages belonging this cluster were explicitly inserted in learning set as pages that should get an higher score. as expected the rank of the other pages in this cluster was boosted up thanks to the capability of the algorithm to generalize from a small number of training examples.
1 qualitative results
　this experiment compares the quality of the page ranks before and after training. in particular  some authoritative pages on the topic  wine  were provided as positive examples  while some authoritative pages on other topics were provided as negative examples. figure 1 reports the pages which obtained the largest variation in their rank. the pages that ob-

figure 1: the scores of the pages of cluster 1 of the dataset  playstation  before and after learning. in the learning process only 1 pages belonging to the cluster 1 were explicitly inserted in learning set as pages that should get an higher score.
most descending pages on topic  wine 

http://www.macromedia.com/go/getflashplayer http://www.netscape.com http://www.macromedia.com/downloads http://www.yahoo.com http://www.adobe.com/products/acrobat/readstep.html http://help.yahoo.com http://www.nl.placestostay.com/index.html http://home.netscape.com http://www.forgottensoldier.com http://www.inntravels.com
most ascending pages on topic  wine 

http://www.winecountry.com http://webwinery.com http://www.ny-wine.com/wineinfo.asp http://www.wine-searcher.com http://www.insidewine.com http://webwinery.com http://www.viniesaporidipuglia.com/en/index.html http://www.vinitaly.com/home en.asp http://www.wineinstitute.org http://www.winespectator.com
figure 1: urls of the pages that yielded either the largest negative or positive rank changes among the pages that were initially in the top 1 scoring documents.
tained a negative rank variation were either topic-generic authoritative pages  e.g. www.netscape.com  or pages relevant for different topics than  wine   e.g.  www.forgottensoldier.org  . among the pages that got a negative rank variation  only the page  www.yahoo.com  was specified as a negative example. on the other hand  the pages that obtained a positive rank variation were effectively authoritative pages on the topic  wine   e.g.  www.wine-searcher.com  or  webwinery.com  . among the most ascending documents  only the page  www.winespectator.com  was explicitly provided to the system as a positive example  whereas other pages were pushed up thanks to the capability of the system to generalize from a small set of training examples.
1 conclusions
　in this paper we introduced a novel algorithm to learn a score distribution over the nodes of a labeled graph. the learning algorithm adapts the parameters of a random surfer in order to match the target scores assigned on a small subset of the nodes. since the parameters are estimated from the unfoldings of the graph centered on the example nodes  the learning algorithm is efficient and can be applied to graphs with many nodes. the learning algorithm was applied to the problem of learning a generic ranking function on the portion of the web graph.
