 
the use of linear data fusion is a fast developing area in the field of military information and combat systems. however  the use of data fusion in conventional application areas is not as wide spread. to date linear data fusion has been used only in applications in which substantial knowledge of both the problem domain and the sensor devices in use are available. however  in applications such as condition monitoring the problem domain can be very complex  with little or no knowledge about the interactions between measured parameters. this paper describes the use of non-linear self-learning or self-organising systems as a tool for data fusion  since these systems can learn complex interrelationships between a number of parameters  and use this information as a tool for improved classification. 
1 introduction 
the reliable operation of machinery in the industrial sector is of major importance due to the ever-increasing demand for profitability in the competitive marketplace. failure of critical machinery at an unexpected time can devastate a company's ability to maintain production and keep a competitive position. organisations can increase the productivity and reliability of such machines by the use of condition monitoring. 
1 machine 	learning condition monitoring is a well-established term that describes a number of predictive maintenance technologies enabling skilled engineers to determine the current state of a machine and diagnose any problem that might be present. however  the number and availability of these skilled personnel is limited and they are most likely to be external to an organisation  which results in machinery health checks being carried out on a scheduled off-line basis. 
complex machines provide data from multiple sources such as vibration  temperature and pressure  which demonstrate non-linear behavior and are difficult to analyze with normal techniques due to the complexity of the interactions. the process of data fusion allows more information to be gained from the synergy of these multiple sensors than would be gained from one single sensor  alag  1; lou  1 . 
due to the complexity of condition monitoring applications  and the lack of knowledge about the domain  a self-learning system is needed to fuse these complex relationships into a model that can be used to represent normal operating behavior. this model can then be used to identify changes from the normal operating behavior  for example the development of a mechanical fault  and flag this up as a novel condition. 
self-organising neural networks  especially the architecture proposed by kohonen  kohonen  1a  have been used successfully over the years in variety of applications. these self-organizing neural networks have the ability to learn by example the general characteristics of similar items in an unseen data set and to group these into different classifications  or clusters. to classify new input patterns after training has completed  the data is compared for similarity to the learned groupings using a euclidean distance metric. the winning neuron in the network is identified as having the smallest euclidean distance to the new input data  and the classification of the new input is determined to be the same as the winning neuron. 
the kohonen algorithm maps higher dimensional data into a lower dimensional representation and implements a nearest neighbor approach for classification of new patterns. however factors such as outliers and misrepresented data can affect this and give false classifications. kohonen networks are naturally grouped with other algorithms such as lvq and nestor algorithms  which are in fact modified versions of it-nearest neighbor classifiers  reilly et al.  1; kohonen et alt 1b . it is 

known that for a problem where its is assumed that everything is known about the data the best possible classifier for a probabilistic system is a bayes classifier  anderson  1; bishop  1j. 
another method for improved classification for kohonen networks  is the use of thresholds around the neurons within the network to act as a decision boundary to stop false classification occurring. a false classification could be a new input pattern that is a significant distance away from the winner  but is still classified as belonging to that group. alternatively a new pattern may be close to a group of neurons or neuron  but these neurons may represent outliers in the data where the data density is sparse. 
a method is needed to dynamically set these thresholds so as to set a large threshold around tightly grouped neurons and smaller thresholds around loosely connected neurons. this gives an approximation of the probability density of the data by indicating  that it is more likely that closely grouped neurons represent areas of data that are frequently winning  whereas loosely coupled and dispersed neurons  although representative of the data are less likely to give accurate classifications. 
1 	overview 
this paper looks at the implementation of a method for automatically setting thresholds for the problem that has been outlined above  using a kohonen self-organizing map  som . the implementation is based within a small siemmens microprocessor in a hardware device and is used for a modular  real world  on-line conditionmonitoring application. the overall neural-maine project will not be discussed here  but the method of assigning thresholds will be examined in closer detail. the application involves the use of on-line selforganizing maps to learn the normal running state of a machine and identify when a transition from this state occurs  harris  1 . the normal state for this problem is modeled by fusing a number of different sensor types and learning the representation of this state  passing new data through the model and identifying when a significant deviation from the norm has occurred  that is  a novel condition. 
1 proposed method 
previous methods of using threshold values with a kohonen som included the use of a single value  which places a threshold around the entire learned data space  taylor and maclntyre  1a . this method although crude  was effective with low dimensional  normally distributed data. however with more complex higher dimensional data the algorithm failed due to misclassifications. 
another approach uses a static threshold value for each neuron within the kohonen network  so when a winning neuron is identified  the distance of the new pattern is compared to the threshold  taylor and maclntyre  1b . if the distance is greater than the threshold then this is classified as a novelty  as shown in formula 1  where d is the distance form the wining neuron and t is the threshold level for that neuron. 
		 1  
however due to the distribution of neurons relating to the data  outliers for example had the same threshold as tightly grouped neurons  which allows severe misclassifications. this method requires the threshold to be set manually  which is not appropriate for on-line real-time systems. although an improvement  the approach is still too likely to misclassify outlying unseen data. 
the kohonen algorithm roughly estimates the probability distribution of the data by clustering more neurons to a data space region that is well represented by input patterns. for a thresholding algorithm to work efficiently and effectively  it should also model the distribution of the neurons. the distribution of the neurons should have the affect on the thresholds that the closer and more dense a number of neurons  the larger the individual neuron threshold should be. this is because there is a higher probability that any new data point occurring in the same region  or close to the region  is likely to belong to the class represented by those neurons. a group of sparsely distributed neurons should have small thresholds. this will result in new data which is in the area of the sparse representation being less likely to be classified as normal  or belonging to the class represented by those neurons. this method allows small  virtually insignificant thresholds to be assigned to neurons  in effect rendering them inoperable as a classifier. this eliminates the problems mentioned with the previous two approaches. thus a new data point must be very close to the outlier or stretched position to be classified as normal  or belonging to that class. 
the approach used to implement this is to use the lateral distance between the kohonen neurons as a basis to set the thresholds dynamically and in real time  without the need for a user interaction. the algorithm steps through each neuron in the kohonen network and finds the closest of its neighboring neurons using euclidean distance. 
the euclidean distance is normalised to a range of -1 to 
+1  where -1 represents a exact match of new data to an existing neuron  and +1 represents the furthest distance measured within the map. the normalised distance is then passed through a sigmoid function  which gives an output in the range 1 to 1. finally the output of the sigmoid is inverted by subtracting the output value from 1. the result is an activation level which is a normalised 
	taylor  tait  and macintyre 	1 

representation of the euclidean distances  with larger thresholds for densely packed neurons  and lower thresholds for sparse representations. 
the number and type of sensing devices that could be connected to the hardware module can vary greatly  and the output that they generate can vary dramatically. the sensitivity of these parameters must be taken into consideration  as the range of values and the sensitivity of fault types to particular parameters vary considerably. 
for normalization of the data  a technique was needed that would keep the sensitivity correct and scale the incoming data into the same range for the neural network. another problem encountered with our application was that data was being presented in real time that needed to be learned  and due to hardware requirements a pool of data could not be gathered  stored and analyzed for normalization purposes. the technique used in the approach was to use a modified version of vector augmentation  or unit sphere . this allowed the incoming values from the sensors to be scaled as a pattern and not as entire set of data  which has so far proved to be effective and accurate. 
an indication of the sensitivity of faults was determined using the outputs of the kohonen network. this was done by applying a novelty metric to calculate a sensitivity value  as shown in formula 1. where d is the euclidean distance of the new input from the winning neuron and t is the winning neurons threshold. 

previous approaches for applying such a threshold use hard limited values. the new approach gives a better representation of the degree of sensitivity of a developing fault. 
1 data  training and results 
data for testing was provided by leatherhead food research association  who provides technical and support information relating to processes in the food industry for it members. data was collected from a machine that produces uht  ultra heat treated  milk products  over a number of states. the states included a normal running state  two foul  blockage fault  conditions and two unexpected errors. 
the normal running state involved no blockages in the 
uht process as the product was passed through the system. the fouling conditions occurred when the product blocked the heat exchanger plates and caused a fault and the two unexpected errors  occurred during the running of the machine causing the process to be shut 
1 	machine learning 
down. the data sets compromised of eight parameters  including temperature  pressure and flow readings. 
the kohonen based novelty detector was trained on the normal running state only with one pass  i.e. the learning mimicked real-time operation by passing once through the data set gathered during run-up and a short period of steady state operation   as this simulates the normal operating condition of the hardware-based system. to test for novel conditions  new data is passed into the trained network. if the new data is within the threshold  it will be classified as normal. if the new data is not like the learned data and outside of the threshold  then it is classified as a novelty or unknown class. 
as a baseline for comparison  the novelty detector was trained as mentioned previously  but with the dynamic threshold setting  the proposed method  disabled. in its place an arbitrary threshold was assigned by running an unseen normal data set through the novelty detector and using the average winning distance  1  as the threshold level for the network. the normal data was then re-run through the network along with the four fault conditions and the results recorded. 

table 1 - results from using real data with arbitrary 
thresholds 
the average threshold for each pattern set will be exacdy the same as each neuron within the network has the same arbitrary threshold. results from the arbitrary setting are poor as by using just the winning distance information you are not incorporating any information about the clustering of data within the self-organized map. 
the novelty detector was then re-trained in the same way as mentioned previously  but this time with the dynamic thresholding activated. data which had not previously been seen by the network  and represented normal running state  was then passed through the network to validate that it could correctly identify normal states  as shown in table 1. as can be seen in table 1  the average euclidean distance for normal patterns is 1  whereas for the fault conditions there is a substantial difference in the distance. the average threshold value is approximately the same  as the same 

network trained on normal data only was used with the fault data passed through it  the slight fluctuation in the average threshold is due to the firing of the different neurons within the network as each has its own threshold. 

table 1 - results from using real data with dynamic 
thresholds 
the four fault conditions all contain normal data that progresses into a fault condition. the novelty detector correctly identified the normal state as normal  fault conditions as novel  and the severity measure indicated the increasing deterioration of the fault condition as shown in figure 1. by examining the results  the first few patterns of the fault conditions were identified as novel  this was due to the machine having a brief start up period  which was not represented in the normal state training data. 
further testing including brief start up data with the normal data proved to be ineffective as the kohonen was learning on a one pass basis  and the number of patterns representing start up to that of normal state was only a small proportion and varying. the way the kohonen algorithm performs learning  fades  the start up data out  as there is more representational data in the normal state data. this is not really a problem  as the project  at this level  requires an indication that a transition from the normal state has occurred  any other state will be dealt with by a higher level of the neural-maine system. 

figure 1 - results for foul 1 showing severity measure 
by comparing the results of the dynamic thresholding and the arbitrary thresholding techniques  it can be seen that the dynamic method considerably outperforms the arbitrary one. the arbitrary methods could improve  with more complex analysis  but performance will never match that of the dynamic technique since the information contained within the lateral connections of the kohonen network are not used. 
the novelty detector was also tested with a number of synthetic data sets  to test the function of the normalization and sensitivity  and to test the accuracy of the novelty detector. five test sets were generated by neural computer sciences which mimicked a number of sensors connected to a machine component  each having varying sensor outputs  high and low values   and varying sensitivities. the synthetic data sets contained a run-up and normal running state  which progresses into a fault condition. to mimic the real operation of the neural network when embedded in the hardware device  the neural network was trained in a  one pass  mode as with the previous real uht plant data. the novelty detector trained with the first 1 patterns of each data set  which simulated the system being activated when a machine was started  run up and settled into a steady state. after the 1 patterns had been presented  the neural network then examined the rest of the data set and classified the new inputs as either normal or novel. 
the results are shown in table 1 

table 1 - results from the synthetic data set and dynamic 
thresholds 
the pattern set trial 1 mimicked a normal run up for the first 1 patterns with a steady run of the next 1 patterns and the final 1 patterns indicating a slow degradation into a fault condition. trial 1 mimicked a quick run up for the first 1 patterns  then stayed in a steady state for the rest of the data set. however  the steady state was interrupted by a number of single erratic novelties simulating a transient fault. trial 1 had a very quick startup time of the first 1 patterns  and then settled into a steady state for the rest of the data 
	taylor  tait  and mac1ntyre 	1 

set. again transient novelties were detected  which increased in frequency and severity towards the end of the data set. trial 1 had another quick start up of the first 
1 patterns  stabalising into a steady state for another 1 patterns. during the steady state a number of violent single novelties were detected  with a  noisy  steadily increasing fault detected during the last patterns of the data set. trial 1 had a quick startup for the first 1 patterns  followed by a steady state for the next 1 patterns  with a steady fault detected in the last 1 patterns. the novelty and sensitivity readings also showed that in the fault  of trial 1  a repeating violent 
novelty was occurring. 
the novelty detector successfully identified the occurrence of these faults  and the severity could be visualised as the fault progressed by use of the severity metric. 
1 conclusions 
the methods described here were developed with a need for speed  accuracy and automation as the data fusion component is being embedded into a small microprocessor based hardware system. this hardware system will be the basis for the neural-maine project and will be used on a number of real world condition-monitoring applications from small processing machinery to large steam turbines. 
using arbitrary threshold levels is just not feasible  as this is not accurate enough. furthermore the combinations of possible data sets in future applications makes the generic calculation of a threshold value impossible. the dynamic method outperforms the arbitrary method and can be used in general applications. this is because the technique uses the information stored within the lateral connections of the kohonen map  which describes the clustering of the data  as a basis for dynamically setting individual neuron threshold values. 
the methods were tested using synthetic data  which mimicked extreme sensor readings  and real data was used from a uht machine. both data sets were used in testing and the dynamic method performed accurately with no user intervention. 
the sensitivity metric also produced more output information as this gave an indication of the severity of the fault. for the gentle fouling conditions of the uht machine  the severity measure is a gently slow increase. the sudden faults caused violent  powerful sensitivity readings. and the final uht unexpected fault showed an error at startup and a small time of settling into normal state. the severity metric was just on the scale  which indicated that the fault was just on the bounds of normality and did eventually settle into a normal state. 
the initial version of the algorithm looks at only the closest neuron  but future versions will investigate the 
1 	machine learning 
use of k-nearest neurons to see if this improves classification. 
