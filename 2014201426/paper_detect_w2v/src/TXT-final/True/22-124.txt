 
despite the fact that many symbolic and connectionist  neural net  learning algorithms are addressing the same problem of learning from classified examples  very little is known regarding their comparative strengths and weaknesses. this paper presents the results of experiments comparing the id1 symbolic learning algorithm with the perceptron and back-propagation connectionist learning algorithms on several large real-world data sets. the results show that id1 and perceptron run significantly faster than does backpropagation  both during learning and during classification of novel examples. however  the probability of correctly classifying new examples is about the same for the three systems. on noisy data sets there is some indication that backpropagation classifies more accurately. 
1. introduction 
     both symbolic and connectionist  or neural network  learning algorithms have been developed; however  there has been little direct comparison of these two basic approaches to knowledge acquisition. consequently  despite the fact that symbolic and connectionist learning systems frequently address the same general problem  very little is known regarding their comparative strengths and weaknesses. 
     the problem most often addressed by both connectionist and symbolic learning systems is the inductive acquisition of concepts from examples. this problem can be briefly defined as follows: given descriptions of a set of examples each labelled as belonging to a particular class  determine a procedure for correctly assigning examples to these classes.1 within symbolic machine learning  numerous algorithms have been developed for learning decision trees 
　　　* this research was partially supported by the university of texas at austin's department of computer sciences  by a grant from tile university of wisconsin graduate school  and by a graduate fellowship to one of the authors  a.g.  supported by the army research office under grant aro-daag1-k-1. 
　　　*in the connectionist literature  this problem is frequently referred to as supervised or associative learning. 
 quinlan1  or logical definitions  michalski1  from examples  both of which can be used to classify subsequent examples. these algorithms have been tested on problems ranging from soybean disease diagnosis  michalski1  to classifying chess end games  quinlan1 . within connectionism  several algorithms have been developed for training a network to respond correctly to a set of examples by appropriately modifying its connection weights  rosenblatt1  rumelhart1 . after training  a network can be used to classify novel examples. connectionist learning algorithms have been tested on problems ranging from converting text to speech  sejnowski1  to evaluating moves in backgammon  tesauro1 . 
     connectionist and symbolic systems for learning from examples generally require the same input  namely  classified examples described as feature vectors. in addition  their performance is generally evaluated in the same manner  namely  by testing their ability to correcdy classify novel examples. despite this situation  until this conference there were no published experiments that directly compared the two approaches on  real world  data. there have been several experimental comparisons of different symbolic learning systems  e.g.   rendell1  ; however  none of these experiments included connectionist algorithms. 
     this paper presents the results of experiments comparing the performance of the id1 symbolic learning algorithm  quinlan1  with both the perceptron  rosenblatt1  and back-propagation  rumelhart1  connectionist algorithms. all three systems are tested on several large data sets from previous symbolic and connectionist experiments  and their accuracy on novel examples and run-time performance are measured. one surprising result is that the perceptron learning algorithm  which has well-known limitations  performs quite well. in general  its accuracy and run-time are comparable to id1's. although the backpropagation algorithm generally takes one to two orders of magnitude more time to train than the other two algorithms  its accuracy on novel examples is always comparable or superior to that of the other algorithms. 
1. experimental methodology 
     this section motivates the choice of the particular algorithms used in the study and describes the data sets used to test them. next  the versions of the systems used are discussed and the method for choosing appropriate input encodings and parameter settings is described. finally  the 
	mooney  shavlik  towell and gove 	1 
exact experiments conducted and data collected are reported. 
1. choice of algorithms 
     in order to experimentally compare connectionist and symbolic algorithms for learning from examples  the id1  perceptron  and back-propagation algorithms are chosen as representative algorithms. the choice of these particular algorithms is based on several factors. 
     id1 is chosen because it is a simple and popular symbolic algorithm for learning from examples. it has been extensively tested on a number of large data sets and is the basis of several commercial rule induction systems. in addition  id1 has been augmented with techniques for handling noisy data and missing information  quinlan1 . finally  in experimental comparisons with other symbolic learning algorithms  id1 generally performs about as well or better than the competition  rendell1 . 
as is well known  the perceptron learning procedure 
 one of the first  connectionist  learning algorithms  is incapable of learning concepts that are not linearly separable  minsky1 . despite this fact  early results indicated that it performed quite well on data sets used to test other learning systems. for example  it performed well on a popular data set for diagnosing soybean diseases |reinke1   which was found to be linearly separable. consequently  perceptron is included to determine to what extent this simple algorithm can handle real-world data. 
     over the past several years  a few connectionist learning algorithms have been developed that overcome the restrictions of the perceptron. back-propagation  also called the generalized delta rule   rumelhart1  is currently the most popular of these procedures. its ability to train  hidden units  allows it to learn concepts that are not linearly separable. back-propagation has proven more efficient than learning procedures for boltzmann machines and has been tested on several large-scale problems  sejnowski1  tesauro1 . consequently  it is chosen to represent the new connectionist learning algorithms. 
1. data sets used 
     four different data sets are used to test the three algorithms. three have been previously used to test different symbolic learning systems and one has been used to test back-propagation. 
     the three data sets previously used with symbolic learning systems describe soybean diseases  reinke1   chess end games  shapiro1j  and audiological disorders  bareiss1 . these data sets involve  respectively: 1  1  and 1 features; 1  1  and 1 examples; 1  1  and 1 categories. 
     the nettalk data set  sejnowski1  is a training set for converting text to speech. it consists of a 1 word dictionary with corresponding phonetic pronunciation. each letter of each word and the surrounding three letters on each side form a training example and each phoneme/stress pair constitutes a category. a seven letter window is insufficient to uniquely identify the sound/stress pair 
1 	machine learning 
attributable to the central letter. as a result  the data can be considered to be noisy. while this is not a problem for back-propagation  it is a problem for the standard id1 algorithm. to handle noise in id1  the chi-square technique suggested by quinlan  quinlan1  is used with this data set. 
     unfortunately  the full dictionary is too extensive to tractably analyze in a manner equivalent to the other domains. instead  a smaller data set is extracted by looking at the 1 most common english words  as reported in  kuchera1   and keeping those that appear in the nettalk dictionary. this intermediate data set is further pruned by only keeping the examples involving   a   sounds. this produces 1 examples that fall into 1 sound/stress categories. this data set is called nettalk-a. 
     a single test is also done for each algorithm using the full dictionary as a test set. the subset of the 1 most common english words that appear in the dictionary is the training set. this set - called nettalk-full - contains 1 words  which produce 1 examples classified into 1 categories. the test set involves roughly 1 examples. 
1. implementation details 
     both id1 and perceptron are implemented in common lisp and although some attention was paid to efficiency  the code was not carefully optimized. all experiments are run on sun 1's with 1 mbytes of memory. 
     two versions of id1 that learn to discriminate multiple categories are used. for the noisy nettalk data  a chisquare version is used with a confidence level of 1% 
 quinlan1 . in the other domains  a no-noise version of id1 is used. experiments with the noise-handling version of id1 on the noise-free domains indicated that the no-noise version consistently performed better. 
     the version of perceptron used in the experiments includes cycle-detection. once a set of weights repeats  the algorithm stops  indicating that the data is not linearly separable. the perceptron cycling theorem  minsky1 guarantees this will eventually happen if the data is not linearly separable.  however  due to simulation time restrictions  the perceptron is also stopped if it cycles through the training data 1 times.  one perceptron is trained for each possible category to distinguish members of that category from all other categories. a test example is classified by passing it through all the perceptrons and assigning it to the category whose perceptron's output exceeds its threshold by the largest amount. 
     the version of back-propagation used in the experiments is the c code supplied with the third volume of the pdp series fmcclelland1 . the learning rate is set to 1 in an attempt to avoid local minima. the momentum term is set at 1. networks contain one output bit for each category and the number of hidden units is 1% of the total number of input and output units  a number which was empirically found to work well. when testing  an example is assigned to the category whose output unit has the highest value. training terminates when the network correctly classifies at least 1% of the training data or when the number of passes through the data  i.e.  the number of 
     
epochs  reaches 1. 
　　　examples are represented as bit vectors rather than standard feature vectors. for each possible value of each feature there is a a bit which is on or off depending on whether the example has that value for that feature. normally  exactly one bit in the set of bits comprising a single feature will be on. however  if the feature is missing  then all the bits in the set are off  and no special processing for missing features need be done. 
　　　this binary representation was initially used only on the connectionist algorithms which require binary inputs. however  the binary encoding was found to consistently improve the classification performance of id1 as well  and therefore is used for all of the learning algorithms. id1's improved performance with the binary encoding may be due to several factors. first  since every feature has only two values  the binary encoding eliminates the gain criterion's undesirable preference for many-valued features  quinlan1 . second  it allows for more general branching decisions such as red vs. non-red instead of requiring nodes to branch on all values of a feature. this may help overcome the irrelevant values problem  cheng1  and result in more general and therefore better decision trees. however  since the binary encoded examples have more features  id1's run time is increased somewhat under this approach. 
1. description of experiments 
　　　each data set  except for nettalk-full  is separated into a collection of training and testing sets. after each system processes a training set  its performance is measured on the corresponding test set. to reduce statistical fluctuations  results arc averaged over ten different training and testing sets produces by randomly placing two-thirds of the examples for each category in the training set and the others in a corresponding test set. again to reduce statistical fluctuations  each run of back-propagation uses a different seed random number which determines the initial network weights. 
　　　during training  cpu time  the number of cycles through the training data  and the final classification correctness for the training data are measured. id1 only processes the data once  while back-propagation and perceptron repeatedly process the data until one of the stopping criteria described above is met. during testing  cpu time and correctness on the testing data are measured. 
　　　due to computational resource limitations  nettalkfull is not converted into ten randomized data sets. rather  the 1 most common words are processed by each system and the results tested on the full dictionary  minus the 1 training words . because of the size of the training set  training restrictions are required. back-propagation is limited to 1 epochs and perceptron is only allowed to cycle a maximum of 1 times on each category before terminating. 
1. experimental results 
　　　figures 1 and 1 contain the experimental results. the first figure reports the training times of the three systems  normalized to the time taken by id1 . correctness on the test data is reported in figure 1. the actual numbers  their standard deviations  and several other statistics are given in  shavlik1 . the means of all 1 training times  everything but nettalk-full  are 1 seconds  id1   1 seconds  perceptron   and 1 seconds  back-propagation . for correctness  the means are 1%  id1   1%  perceptron   and 1%  back-propagation . for the single run with nettalk-full  the correctness figures are 1%  id1   1%  perceptron   1%  back-propagation  and the training times are 1 seconds  id1   1 seconds  perceptron  and 1 seconds  back-propagation . 
　　　a relevant statistical test for these problem results is a two-way analysis of variance. this test is designed to determine the source of variation in the observed correctness over each of the four domains. the two way analysis of variance returns two values: the likelihood that the variation can be explained by the different training sets  and the likelihood that the variation can be explained by differences between training methods. 
　　　for nettalk-a and soybeans  where backpropagation performs best  and chess  where id1 performs best   it is possible to conclude at the 1% level  i.e.  with 1% confidence  that the variation in observed correctness results from the difference in training methods rather than random error. results are less conclusive for audiology  where back-propagation out-performs the others . there it is only possible to conclude with 1% confidence that the training method is the source of the variation. 

	mooney  shavlik  towell and gove 	1 
     
1. discussion of results 
     the results indicate that the three systems performed remarkably similarly with respect to classification of novel examples. however  id1 and perceptron train much faster than back-propagation. there is some indication that backpropagation works better on the noisy nettalk data. 
1. perceptron performance 
     one surprising result of these experiments is how well the simple perceptron algorithm performs. the perceptron was largely abandoned as a general learning mechanism about twenty years ago because of its inherent limitations  such as its inability to learn non-linearly-separable concepts  minsky1 . nevertheless  it performed quite well in these experiments. except on the nettalk data  the accuracy of the perceptron is hardly distinguishable from the more complicated learning algorithms; and even on the nettalk data  it performs about as well as id1. in addition  it is very efficient. on all but the chess data and nettalkfull  perceptron's training time is less than id1's. 
     these results indicate the presence of a large amount of regularity in the training sets chosen as representative of data previously used to test symbolic learning systems. the categories present in both the soybean and audiology data are linearly separable for all ten randomly chosen training sets. the two categories in the chess data are linearly separable for four of the ten training sets and almost linearly separable on the rest  average correctness on the training set is 1% . despite the fact that the data sets are large and represent real categories  their regularity makes them relatively  easy  for even simple learning algorithms like the perceptron. 
     one possible explanation for the regularity in the data is that it is reflecting regularity in the real-world categories. in other words  members of a real category naturally have a great deal in common and are relatively easy to distinguish from examples of other categories. another possible explanation is that the features present in the data have been very carefully engineered to reflect important differences in the categories. for example  formulating features for chess end games which were appropriate for learning required considerable human effort  quinlan1 . the actual explanation is probably a combination of these two important factors. 
     regardless of the reason  data for many  rear' problems seems to consist of linearly separable categories. since the perceptron is a simple and efficient learning algorithm in this case  using a perceptron as an initial test system is probably a good idea. if a set of categories are not linearly separable  the perceptron cycling theorem  minsky1  guarantees that the algorithm will eventually repeat the same set of weights and can be terminated. in this case  a more complicated algorithm such as id1 or backpropagation can be tried. the perceptron tree error correction procedure  utgoff1a  is an example of such a hybrid approach. this algorithm first tries the perceptron learning procedure and if it fails splits the data into subsets using id1's information-theoretic measure and applies itself recursively to each subset. 
1 	machine learning 
1. equivalence of classification accuracy 
     in general  all three learning systems are remarkably similar with respect to classifying novel examples. despite the obvious differences between decision trees and connectionist networks  their ability to accurately represent concepts and correctly classify novel instances appears to be quite comparable. data from other recent experiments support this general conclusion  fisher1  weiss1 . learning curves presented in  fisher1  suggest that that id1 performs better on relatively small amounts of data; however  this is an artifact of how incremental training was performed in back-propagation. if back-propagation is always run to  convergence  this difference disappears  shavlik1 . fisher and mckusick also found that once back-propagation converges  it consistently attains a correctness a few percentage points above id1. however  they did not use binary input encodings for id1  consequently id1 and back-propagation were given different encodings of the inputs. we found that using the binary encoding for id1 can improve correctness by a few percentage points and may account for some of the difference observed by fisher and mckusick. 
     one possible explanation for the similarity in performance is that most reasonable procedures which correctly classify a particular set of training instances  possibly allowing for some noise  are about equally likely to classify a novel instance correctly. another possible explanation is that the inductive biases inherent in connectionist and symbolic representations and algorithms reflect implicit biases in real categories equally well. for example  all three systems share some form of an  occam's razor  bias and  at least to some degree  prefer simpler hypotheses. however  for perceptron and back-propagation  the complexity of the hypothesis is constrained by the user who must initially select an appropriate network for the problem. unlike most symbolic learning systems which explicitly search for a simple hypothesis  connectionist systems simply search for a correct hypothesis which fits into a user-specified network. although both generally use hill-climbing search to guide learning  connectionist systems hill-climb in  correctness space  while symbolic systems hill-climb in  simplicity space.  an interesting development would be a connectionist learning algorithm which explicitly tries to learn simple networks  for example by eliminating unnecessary hidden units and/or slowly adding hidden units as they are needed  honavar1 . such a system would help ease the burden of having to initially specify an appropriate network for a 
     problem. 
1. performance on the nettalk data 
     in addition to being by far the largest data set  the nettalk data set is the only one that involves noise. on nettalk-a and especially nettalk-full  back-propagation performs better than perceptron and the noise-handling version of id1. although it is not clear actually which properties of the nettalk data lead to back-propagation's superior performance  one hypothesis is that back-propagation's ability to perform in the presence of noisy training data is better than id1's. experiments on the effect of noise on 
     
both id1 and back-propagation tend to support this hypothesis  fisher1  shavlik1 . 
	in 	 sejnowski1   	the 	sound/stress 	outputs 	in 
nettalk-full are encoded for back-propagation as a sequence of 1 bits  as opposed to one bit for each of the 1 categories . the first 1 bits are a distributed encoding of the 1 phonemes contained in the dictionary. each bit is a unary feature describing one of: position of the tongue in the mouth  phoneme type  vowel height  or punctuation. the remaining five bits form a local encoding of the five types of stresses used in the dictionary. a final category is obtained by choosing the one that makes the smallest angle in feature space with the 1 output bits.  this is the  best guess  metric used in  sejnowski1 .  
　　　sejnowski's setup was repeated with the nettalk-full data  using 1 hidden units. running this for 1 epochs  as in  sejnowski1   takes 1 seconds. correctness is 1% on the training set and 1% on the testing set.  these numbers are the best of five runs. due to the random initial weights  twice after 1 training epochs back-propagation's correctness on the both the training and test sets were less than 1%.  this is significantly better than using the original output encoding of one bit per category  which results  after 1 epochs and 1 seconds  in correctnesses of 1% percent on the training set and 1% on the testing set. further investigation has shown that the distributed output encoding can also be used to improve the performance of id1; however  not to same the extent that it improves backpropagation's performance rshavlik1 . 
1. slowness of back-propagation 
　　　although back-propagation performs about as well or better than the other systems at classifying novel examples  it consistently takes a lot more time to train. averaged across all four data sets  back-propagation takes about 1 times as long to train as id1.  testing in back-propagation takes about 1 times longer.  these factors would probably increase if one optimized the id1 code  which is not coded efficiently compared to the c version of back-propagation. 
　　　one obvious way back-propagation can be made faster is to exploit its intrinsic parallelism. the networks used in these experiments contained an average of about 1 units. consequently  assuming one processor per unit and perfect speedup  the training time for back-propagation could possibly be made competitive with id1's. however  id1 is a recursive divide-and-conquer algorithm and therefore also has a great deal of intrinsic parallelism. in addition  in perceptron each output bit is learned independently  one simple source of parallelism for this method. comparing the training time for parallel implementations of all three algorithms would be the only fair way to address this issue. 
1. additional issues 
　　　there are several other differences between the three systems and between symbolic and connectionist approaches in general. for one  given a collection of input/output training pairs  id1 and percepu-on can be directly run. on the other hand  in order to run backpropagation  a network architecture must be chosen  currently much more of an art than a science. not only must the number of hidden units be chosen  but the number of hidden layers must also be specified. in addition  the learning rate and the momentum term must be specified. performance may depend greatly on the initial randomlyselected weights and several runs with different initial weights may be necessary to get a good final result. finally  a criterion for stopping training must be chosen. our experience has shown that if parameters are inappropriately set or if initial weights are unfavorable  back-propagation may fail to converge efficiently. however  it should also be mentioned that many symbolic learning systems have parameters which must be appropriately set to insure good performance  rendell1 . 
　　　another issue is the human interpretability of the acquired rules. symbolic learning can produce interpretable rules while networks of weights are harder to interpret. however  large decision trees can also be very difficult to interpret  shapiro1 . finally  connectionist models are neurally-inspired  while symbolic models are not. hence  connectionist models may shed more light on human neurophysiology. 
1. conclusions 
　　　a current controversy is the relative merits of symbolic and connectionist approaches to artificial intelligence. although  symbolic and connectionist learning systems often address the same task of inductively acquiring concepts from classified examples  their comparative performance has not been adequately investigated. in this paper  the performance of a symbolic learning system  id1  and two connectionist learning systems  perceptron and backpropagation  are compared on four real-world data sets. these data sets have been used in previous experiments. 
three in symbolic learning research  soybeans  chess  and audiology  and one in connectionist research  nettalk . experimental results indicate that id1 and perceptron run significantly faster than does back-propagation  both during learning and during classification of novel examples. the probability of correctly classifying new examples is about the same for the three systems  although there is indication that back-propagation classifies more accurately on noisy complex data sets. 
　　　follow-up experiments reported in  shavlik1  have confirmed that back-propagation handles both noise and missing feature values more reliably than either id1 or perceptron. although on some data sets all three systems degrade equally as noise or missing data is introduced  back-propagation shows significantly less degradation on particular data sets. another issue is how well the algorithms learn as a function of the amount of training. perhaps some methods perform better with relatively little training data while others perform better on large training sets. in  shavlik1   learning curves'' are presented which indicate that relative amount of training is not a distinguishing factor. a third issue is incremental learning without requiring the complete storage of all previous examples. 
	mooney  shavlik  towell and gove 	1 
incremental versions of id1 have been proposed  schlimmer1  utgoff1b  and back-propagation can be performed incrementally by processing each new example some number of times and then discarding it. comparison of various incremental approaches is an area for future research  as is investigation of parallel approaches. further investigation will hopefully lead to a better understanding of the relative strengths and weaknesses of the symbolic and connectionist approaches to machine learning. 
acknowledgements 
　　　the authors would like to thank the following people for supplying data sets: bob stepp and bob reinke for the soybean data; ray bareiss  bruce porter  and craig wier for the audiology data which was collected with the help of professor james jerger of the baylor college of medicine; rob holte and peter clark for alen shapiro's chess data; and terry sejnowski for the nettalk data. elizabeth towell assisted with the analysis of variance. rita duran  wan yik lee  and richard maclin contributed to the implementations. 
