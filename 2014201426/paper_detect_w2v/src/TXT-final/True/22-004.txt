 
in this paper  a distributed heuristic search algorithm is presented. we show that the algorithm is admissible and give an informal analysis of its load balancing  scalability  and speedup. a flow-shop scheduling problem has been implemented on a bbn butterfly multicomputer using up to 1 processors to empirically test this algorithm. from our experiments  this algorithm is capable of achieving almost linear speedup on a large number of processors with a relatively small problem size. 
1 	introduction 
best-first heuristic search algorithms  such as the .a* algorithm  are one of the most important techniques used to solve many problems in artificial intelligence and opera! ions research. a common feature of heuristic search is its high computational complexity  which has significantly limited its application in practical domains such as flexible manufacturing  strategic planning  and management. 
　in the past decade  many parallel architectures have been proposed and some of them are now commercially available. advances in parallel computer technology have offered the potential to greatly speedup the computations in general. due to the combinatorial aspects of heuristic search  a very large scale of parallelism can be potentially explored. however  the efficiency of heuristic search algorithms mainly comes from the intelligent guidance of heuristics. when implementing parallel heuristic search algorithms on commercial multicomputers  researchers often face a tradeoff between the faithfulness to global heuristics and the high communication cost which serializes and slows down the computation. until recently  there was no easy solution to this dilemma. 
　in this paper  we present a distributed best-first heuristic search algorithm  parallel iterative a*  pia* . we show that the algorithm is admissible  and we give an informal analysis of its load balancing  scalability and speedup. to empirically test the pi a* algorithm  a flowshop scheduling problem has been implemented on the bbn butterfly multicomputer  bbn  1  using up to 1 processors. from our experiments  this algorithm is capable of achieving almost linear speedup on a large number of processors with relatively small problem size. 
　we will assume the reader is familiar with the a* algorithm  hart et a/.  1 . 
1 	related work 
research in parallel heuristic search has been very active recently. a* can be parallelized by storing the open list in global storage that is accessible to all processors  mohan  1 . huang and davis  use queueing theory to show that this approach can achieve almost linear speedup to a certain number of processors. however  beyond that point  the speedup levels off suddenly  no matter how many processor are used. rao and kumar  proposed a concurrent heap data structure for organizing the global open list. the new data structure allows processors to interleave operations on open and improves the speedup to some extent; however  congestion near the root of the concurrent heap is still a problem. 
　kumar et al.  propose another approach which substitutes a shared blackboard for the global open list and let each processor maintain its own local open list  unfortunately  the blackboard may eventually become a bottleneck as the number of processors increases. 
　a distributed approach has also been tried by some researchers. early work is represented by wah and eva ma's ma nip . anderson and chen  presented an algorithm to perform distributed best-first search on hypercube multicomputers. to balance the workload  they proposed to exchange a summary of the cost distribution in the local open lists of neighboring processors. quinn  presented four other implementations of the best-first search on hypercubes. these four simple algorithms tried to either improve the useful computation at each processor  or to improve the communication cost  but failed to effectively improve both at the same time. 
1 the pi a* algorithm 
pi a* proceeds by repetitive synchronized iterations. at each iteration  processors are synchronized twice to carry out two different procedures: the node expansion procedure and the node transfer procedure. operations are largely local to the processor in the node expansion 
huang and davis 

procedure and are completely local in the node transfer procedure. data structures in pi a* are distributed to avoid bottlenecks. node selection  node expansion  node ordering and successor distribution operations are fully parallelized by processors. processors performing searches which are not following the current best heuristics are synchronized to stop as soon as possible to reduce search overhead  the increase in the number of nodes that must be expanded owing to the introduction of parallelism . during processor synchronization  speculative computations are contingently performed at each processor  trying to keep processors always productively busy  to reduce synchronization overhead. unnecessary communications are avoided as long as processors are performing worthwhile search work to reduce communication overhead. a symmetric successor node distribution method is used to direct load balancing. finally  the correct termination of pi a* is facilitated by its iterative structure. 
   a general scalable parallel architecture model is used by us to describe p1 a*. the architecture consists of a set of processor-memory pairs which communicate through an unspecified communication channel. the communication channel can be realized using a shared memory or by message parsing. memory referencing through local memory is completed in constant unit time. a remote reference through the communication channel  however  requires    logp  time under normal balanced traffic  where p is the number of processors. it is our belief that this architecture model is general enough to subsume most scalable multicomputers which are currently available commercially  such as the b b n butterfly   b b n   1   the intel hypercube  intel  j1  and the connection machine  llillis  1 . 
   because some forward references are required for us to describe pia*  the reader may need to re-read this section to understand this algorithm. 

1 	t h e n o d e e x p a n s i o n p r o c e d u r e 
sors synchronize for the node transfer procedure which is described in section 1. 
successors generated by a node expansion 
are not considered for expansion until the next iteration; they are added to rl immediately after they are generated. 
　　the node expansion procedure also computes /' + and broadcasts it to all processors. it is set to the m a x i m u m of  a / 1   and  b the m i n i m u m cost of  i all successors 
i
1 	tools  the node expansion procedure at each iteration i operates as follows. a processor j first expands all the nodes from mjl and  by an algorithm to be described in section 1  puts all the successor nodes generated into the reception lists  rl . t h e n   as long as any other processor is expanding a mandatory node  this processor continues to expand the best speculative node from sj'. when all the nodes from m' have been expanded  all proces-
1 	synchronization 
let us digress here to discuss how processor synchronization in the node expansion procedure can be efficiently and correctly implemented. essentially  what we need is a barrier synchronization between processors. all of the processors are required to meet at the barrier before any are allowed to proceed. the barrier in our case is a state in which each processor j has finished expanding mj. but  instead of unproductive waiting at the barrier  a processor j continues to expand the best speculative node from sj while waiting. 
   if a shared memory is available  the barrier synchronization can be implemented by having a global variable which counts the number of processors that are waiting at the barrier. when all of the processors have arrived at the barrier  the barrier can be removed. i his approach requires atomic operations on a global variable and may create a hot spot. 
   a better approach  called the butterfly barrier suggested by brooks iii  1   is to let each processor synchronize with another processor pairwise at each of log p stages in order to synchronize p processors. this approach removes the critical regions and hot spots with desired scalability  and is suitable for message-passing architectures  such as hypercubes  as well. 
1 	t h e successor d i s t r i b u t i o n a l g o r i t h m 
successor nodes generated by a processor are put into rlj  j = 1 	p - 1  in a multiplex round-robin fashion. 
more precisely  suppose that the most recent successor node generated by processor j is added to rlj. then the next successor node generated by processor j will be added to rlk  when* k -  i+ 1 mod p . at each iteration  processor j sends its first generated successor to rlj. 
　the advantage of this approach is that it is simple to implement and its symmetric structure helps pli attain the desired load balancing  see section 1 . since the successors generated are not considered for expansion until the next iteration  some optimization can be made for message-passing architectures. messages for successor distribution can be asynchronous so that computation and communication can be overlapped. for architectures which require large communication setup time  successor nodes generated can be distributed and cached in local memory and not sent until an efficient message size for the underlying architecture is reached. 
1 	t h e node transfer procedure 
after the node expansion procedure  each processor j empties the nodes from rlj and inserts them into ii lj.  to form a new priority queue for the next iteration. note that the node transfer procedure is completely local; no communications between processors are required. 
　the relationship between wli+1 and wli can be expressed as: 
	. 	 eq1  
is the set of speculative nodes which were 
selected for expansion by processors in the node expansion procedure at iteration i to use the otherwise idle time for processor synchronization. 
1 	t e r m i n a t i o n 
when a mandatory node is found to be a goal node by a processor  a message can be broadcast to inform all processors to terminate. if a speculative node is found to be a goal node  this node is simply added to rl because it may not be an optimal goal node. 
   pla* can terminate  failing to reach a goal node  when if and only if  for all j = 
1.... p- 1. this state can be recognized and broadcast to all processors at the end of the node transfer procedure. 
1 	s u m m a r y 
the pi a* algorithm can be summarized below: 

loop until a goal node is reached or wl is empty { start node expansion procedure } for each processor  
expand all mandatory nodes from wlj;  and add successors to rl using the multiplex round-robin successor distribution algorithm: 
while there is a processor expanding a mandatory node 
     expand the best speculative node from wlj and add successors to rl using the multiplex round-robin successor distribution algorithm; { start node transfer procedure } for each processor j 
	insert all nodes from rlj 	to   'lj: 
mandatory nodes and speculative nodes are discriminated by comparing their cost to a threshold t. a node n is a mandatory node if f n    t; otherwise  it is a speculative node. initially  / - h s . successive / values are computed during the node expansion procedure by   fq i  presented in section 1. 

huang and davis 


1 	tools 

attain good load balancing and to obtain almost linear speedup. 
   one of the main features that contribute to the load balancing of the pia* algorithm is its symmetric structure. each processor in pia* maintains the same type of data structures  executes the same type of operations  and interacts with other processors in the same environment. by symmetry principle  probabilistically  the nodes tend to distribute evenly both in number and in cost among processors. in figure 1  we plot  from our experiments  the total number of nodes expanded at each processor  and the number of nodes left in the work list at each processor when 1 processors were used. the distribution is very even among processors. 
   at each iteration of pia*  there are two synchronized procedures: the node expansion procedure and the node transfer procedure. we will study the load balancing of these two procedures respectively. 

1.1 	load balancing in the n o d e expansion procedure* 
   processors are kept almost constantly busy in the node expansion procedure because the speculative computation is performed additionally at each processor to use the waiting time for processor synchronization. this arrangement has been very effective in our experiments. as we can see from figure 1  the total number of nodes expanded was nearly constant at each of the 1 processors . 
   rut  how productive is the speculative computation at each iteration  if mp is evenly distributed among all processors  then very few speculative nodes will be selected for expansion. we have argued above1 that this is generally so probabilistically  owing to the symmetric structure of the pi a* algorithm. a speculative1 node selected for expansion by a processor is the best node available to this processor's work list at that time. at each iteration  any speculative node n with f v    f* *  can become a mandatory node at one of the subsequent  iterations  and will eventually be expanded by a* as well. hence  the speculative computation which expands this type of speculative nodes is also productive. 
   in our experiments  see table i   the total number of speculative nodes expanded tended to increase when more processors were used; but the total number of nodes  mandatory nodes and speculative nodes  expanded did not tend to increase  the zigzagging is attributed to the parallel search anomalies . furthermore  the total number of iterations tended to decrease as more processors were used  see table i . these results empirically show that most of the speculative computation performed was productive in our experiments. 

the multiplex round-robin algorithm for the successor distribution has a deterministic worst case which is independent of the problem size but can grow linearly with the number of processors. the probability of the worse case occurring is very low as the number of processors increases. in fact  when the a priori characteristics of a problem instance are unavailable  because of the symmetry  the expected maximum difference of rlj*s should be close to zero. 
   let us consider figure 1 again. recause the total number of nodes expanded and the final size of the work list at each processor are very uniform  we expect that the size of rlj's before the node transfer procedure should be approximately uniform at each iteration. note that each processor at each iteration only expanded less than 1 nodes on the average  about 1 nodes expanded in over 1 iterations  to obtain this load balancing. 
1 	scalability 
computational requirements for most interesting combinatorial search problems grow very quickly with problem size. it is not difficult to find problems which can use millions of processors for the pia* algorithm   'an the pi a* algorithm scale accordingly provided that the underlying multicomputer is scalable  fxcept for the successor distribution and the processor synchronization  operations in pi a* are completely local. ry using tlu butltrflu barrier suggested by brook iii   proces-
sor synchronization can scale very well with increased numbers of processors. 
   successor distribution is accomplished by the help of a reception list at each processor. since reception lists are accessed by all processors  a legitimate concern would be whether contention for them would lead to significant degradation or a bottleneck. an equal number of reception lists and processors exists  and each processor has equal probability of placing a node on any reception list. recause if is a simple list structure  a reception list can 
huang and davis 

be protected with a very short critical region. we expect that contention for the reception lists l y processors will not be a problem in practice and its seriousness does not increase with the number of processors. in  huang and davis  1   we use elementary queueing theory to support this expectation. 
1 	speedup 
since p1a* proceeds iteratively  we will analyze the speedup in a single iteration to project  the overall speedup. speedup for p processors is normally defined as the ratio of execution time using one processor and that using p processors; i.e.  

let us define 1'  = t1 and wp = p   tp as the total amount of work required when using one processor and p processors respectively. assume that the same set of nodes are expanded in w   and wp . then  to compare ws  and l i p   we note that a fraction of w   f iv   1   /   1  is the work to add successors to rl. when p   i processors are used  the same amount of work is converted to nonlocal successor distribution operations. provided that an assumed scalable multicomputer is used to run pla*  is converted to f.h    c1 log p in hp  where c1 is a constant which depends on the efficiency of the communication network in the underlying multicomputer. in addition   yp also contains the computation cost for processor synchronization. assuming the butterfly barrier is used  each processor needs to run 
log p stages and in each stage communicates with an other processor. the total cost can then be expressed as c 1 plog 1 p  where c1 is another constant. therefore  we can express h'p as: 

and the speedup is: 

   the above equation shows how the communication overhead introduced by successor distribution operations  and the processor synchronization overhead would affect the speedup of pi a*. since only simple operations are required for successor distribution  the fraction f should be very small for most heuristic search problems. the significance of processor synchronization overhead is inversely proportional to  i.e.  the synchronization overhead is less significant if each processor expands more nodes at each iteration. for exponential search problems  we argue that there are normally many mandatory nodes at each iteration. consider the 1-city traveling salesman problem as an example. the size of the search state space is 1!. assuming a 1-bit integer is used to represent the cost of a state  there are at most 1 different- cost values. then  on the average  there are over 1 states assigned the same cost value! the speedup  based on the execution time taken when 1 processors were used  is shown in table 1. on the average  fewer than 1 nodes were expanded at each iteration 
1 	tools 
when more than go processors were used  see  huang and davis  1  . overall  only about 1 nodes were expanded in total in every experiment. the problem size was chosen so that the benchmarking time was reasonable to measure  and it was just barely enough to effectively utilize 1 processors. prom our analysis  pi a*  similar to most other parallel algorithms  will perform better for problems with larger problem sizes. 
   as a comparison  we implemented the same problem using the central queue approach on the same machine. the timing and speedup results are summarized in table 1. the maximum speedup was less than 1 and the execution time had no sign of improvement when we increased the number of processors to 1. 
   because of memory limitations  we were unable to obtain the speedup of pla* based on a* running on a single processor of butterfly in table 1; however  a small problem was tested to compare actual running times of pla* and a*. the result is shown in table 1. the speedup of pla* in table 1 is based on a*. in our implementations  the run time performance of pla* using one processor is very close to a* if both algorithms expand the same number of nodes. the total number of nodes expanded depends on when a goal node is reached  and it can vary widely if there are many nodes with cost equal to the optimal solution cost. also from table 1  the parallelization overhead of p1 a* is not significant for this small problem size.  note that when more than one processor are used  remote memory reference has been reported to be about 1 times more expensive than local memory reference on butterfly.  
table 1: experimental results for the central queue approach   cf. table 1  

 compared to a* 
1 	concluding remarks and future research 
we have presented a distributed best-first heuristic search algorithm  pla*. we proved the algorithm is admissible and gave an informal analysis of its load balancing  scalability and speedup. a  low-shop scheduling problem was chosen to implement the pla* algorithm on the bbn butterfly multicomputer using up to so processors. the experimental results were encouraging. it seems that this algorithm can achieve almost linear speedup on a large number of processors with a relatively small problem size. we expect this algorithm can be efficiently implemented on a large class of scalable mult icomputers and can solve a variety of combinatorial optimization problems. however  because pia* has not been extensively tested on many types of problems and mult icomputers  its actual limitations and advantages have yet to be* more carefully evaluated in future tests. 
　the pla* algorithm uses roughly the same amount of memory as a*. a* often fails to solve an exponential search problem because it runs out of space very quickly. although pla* can effectively use the combined memory of a loosely-coupled multicomputer  it is also vulnerable to the memory shortage problem when trying to 
solve large exponential search problems. we are developing a linear space variant of pi a* and investigating an implementation on the connection machine  further results will be published in a sequel to this paper. 
1 	acknowledgements 
 the authors would like to deeply thank richard korf who reviewed and critiqued early drafts of this paper. vipin kumar's comments on our early versions of pi a* have been very valuable to us. special thanks to perry thorndyke and n. s. sridharan who encouraged and supported this research. 
