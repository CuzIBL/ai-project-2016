
we study the problem of evaluating the goodness of a kernel matrix for a classification task. as kernel matrix evaluation is usually used in other expensive procedures like feature and model selections  the goodness measure must be calculated efficiently. most previous approaches are not efficient  except for kernel target alignment  kta  that can be calculated in o n1  time complexity. although kta is widely used  we show that it has some serious drawbacks. we propose an efficient surrogate measure to evaluate the goodness of a kernel matrix based on the data distributions of classes in the feature space. the measure not only overcomes the limitations of kta  but also possesses other properties like invariance  efficiency and error bound guarantee. comparative experiments show that the measure is a good indication of the goodness of a kernel matrix.
1 introduction
kernel methods  such as support vector machines  gaussian processes  etc.  have delivered extremely high performance in a wide variety of supervised and non-supervised learning tasks  scho¡§lkopf and smola  1 . the key to success is that kernel methods can be modularized into two modules: the first is to map data into a  usually higher dimensional  feature space  which is a powerful framework for many different data types; and the second is to use a linear algorithm in the feature space  which is efficient and has theoretical guarantees  shawe-taylor and cristianini  1 . while many linear algorithms can be kernelized  the major effort in kernel methods is put into designing good mappings defined by kernel functions ¦Õ:
	¦Õ : x ¡ú h.	 1 
x is the original data space and h is the feature space  which is a reproducing kernel hilbert space  rkhs . while h may be a many-dimensional space  the algorithm takes advantage of the kernel trick to operate solely on the kernel matrix induced from data:
	.	 1 
the goodness of kernel function can only be seen from the goodness of the kernel matrix k; therefore  measuring the goodness of k is of primary interest in various contexts.
¡¡there are many ways to measure the goodness of a kernel matrix  kernel function  for a classification task  differently reflecting the expected quality of the kernel function. commonly used measures are regularized risk  scho¡§lkopf and smola  1   negative log-posterior  fine and scheinberg  1  and hyperkernels  ong et al.  1 . these measures do not give a specific value  but only assert certain criteria in form of regularities in certain spaces  for example  rkhs or hyper-rkhs. all of these kernel measures require an optimization procedure for evaluation. therefore  they are prohibitively expensive to be incorporated into other expensive procedures like model and feature selections. other techniques like cross validation  leave-one-out estimators or radius-marginbound can also be consideredas expected quality functionals. like the previously mentioned works  they require the whole learning process.
¡¡to be used in feature and model selection processes  goodness measures of kernel matrices must be efficiently calculated. to the best of our knowledge  the only efficient kernel goodness measure is kernel target alignment  cristianini et al.  1 . due to its simplicity and efficiency  kta has been used in many methodsfor featureand model selections. some notable methods are learning conic combinations of kernels  lanckriet et al.  1   learning idealized kernels  kwok and tsang  1   feature selection  neumann et al.  1  and kernel design using boosting  crammer et al.  1 .
¡¡in this work  we first analyze kta to show that having a high kta is only a sufficient condition to be a good kernel matrix  but not a necessary condition. it is possible for a kernel matrix to have a very good performance even though its kta is still low. we then propose an efficient surrogate measure based on the data distributions in the feature space to relax the strict conditions imposed by kta. the measure is invariant to linear operators in the feature space and retains several properties of kta  such as efficiency and an error bound guarantee. the measure is closely related to some other works on worst-case error bounds and distance metric learning. we show experimentally that the new measure is more closely correlated to the goodness of a kernel matrix  and conclude finally with some future works.
1 kernel target alignment
kernel target alignment is used to measure how well a kernel matrix aligns to a target  or another kernel   cristianini et al.  1 . alignment to the target is defined as the  normalized  frobenius inner product between the kernel matrix and the covariance matrix of the target vector. it is interpreted as cosine distance between these two bi-dimensional vectors. we first introduce some notations.
¡¡training example set {xi}i=1..n   x with the corresponding target vector. suppose that y1 = .. = yn+ = 1 and yn++1 = .. = yn++n  =  1; n+ examples belong to class 1  n  examples belong to class  1  n+ + n  = n. under a feature map ¦Õ  the kernel matrix is defined as:
	.	 1 
frobenius inner product is defined as:
	.	 1 
¡¡the target matrix is defined to be y.yt. kernel target alignment of k is defined as the normalized frobenius inner product:
	.	 1 
¡¡the advantage  which makes kta popular in many feature and model selection methods  is that it satisfies several properties. it is efficient as its computational time is o n1 . it is highly concentrated around its expected value  giving a high probability that a value is close to the true alignment value. it also gives some error bounds  meaning that when kta is high  one can expect that the error rate using the kernel must be limited to a certain amount.
¡¡we claim that having a very high value a k y  is only a sufficient condition  but not a necessary condition  for k to be a good kernel matrix for a given task specified by the target y. as 1   when a k y  = 1  the two bidimensional vectors k and y.yt are linear. up to a constant  the optimal alignment happens when kij = yi.yj. this is equivalent to two conditions:
1. all examples of the same class are mapped into the samevector in the feature space  kij = 1 for xi xj in the same class .
1. the mapped vectors of different classes in the featurespace are additive inverse  kij =  1 for xi xj in different classes .
¡¡it is a sufficient condition that having a high k  one can expect good performance as classes are collapsed into a point in the feature space. violating any of the conditions would mean that kta is penalized. however  it is not a necessary condition as we analyze below.
¡¡the first condition implies that the within-class variance penalizes kta. however  there is a gap between the condition and the concept of margin in support vector machines  svms . varying data  in the featurespace  alongthe separating hyperplane will not affect the margin. if data varies along the perpendicular direction of the separating hyperplane  the margin can be changed. however  kta does not take into account variances in different directions.
¡¡the second condition is too strict  and not applicable in many cases  because it requires the mapped vectors of different classes to be additive inverses of each other. ideally  if the kernel function maps all examples of a class to one vector in the feature space  the kernel matrix should be evaluated as optimal. this is the reason why having high kta is only a sufficient condition  but not a necessary condition. we use some examples to show this limitation quantitatively.
¡¡example 1  best case : the kernel function ¦Õ maps all examples of class 1 into vector ¦Õ+ ¡Ê h and all examples of class  1 into vector ¦Õ  ¡Ê h. assume that ¦Õ+.¦Õ+ = ¦Õ .¦Õ = 1 and. for any ¦Á  the    
kernel matrix k should be evaluated as optimal  as it is induced from an optimal feature map. however  its kta value is:
	.	 1 
kta values of these kernel matrices change as ¦Á varies from 1 to  1  and any value in that range can be the kta value of a kernel matrix of an optimal feature function. as
  kta ranges from 
to 1 in this case.
¡¡example 1  worst case : the kernel function ¦Õ maps half of the examples of each class into vector ¦Õ1 ¡Ê h and the other half into vector¦Õ1 ¡Ê h. assume that ¦Õ1.¦Õ1 = ¦Õ1.¦Õ1 =
1 and 1. for any ¦Á  the kernel matrix k should be evaluated very low  as it is induced from the worst kernel function  which fuses the two classes together and has no generalization ability. however  its kta value is:
	.	 1 
as shown above    which is the same limit as the best case. we can see that kta of this case ranges from . there is a similar caution in  meila  1 . as the best and the worst cases cover the whole range of kta values  any other case would coincide with one of them. therefore  kta may mistake any case to be either the best or the worst.
   example 1  popular case : consider the case when a kernel function maps all examples into an orthant in the feature space  inducing a kernel matrix with non-negativeentries  i.e. . in this case:
	.	 1 
proof can be derived by using the fact that and the
cauchy-schwarz inequality  gradshteyn and ryzhik  1 .
take a special case when. recall that kta of a kernel matrix ranges from 1 to 1  and the

figure 1: data variancein the direction between class centers.
higher the better. this example means that these types of kernel functions will have bounded kta values  no matter how good the kernel functions are. this is a drawback of kta. unfortunately  this type of kernel function is extensively used in practice. gaussian kernels  shawe-taylor and cristianini  1   and many otherkernels defined overdiscrete structures  gartneret al.  1 or distributions  jebara et al.  1   etc. fall into this category.
¡¡the above examples show that kta can mistake any kernel matrix to be the best or the worst. kta is always bound to some numbers for many popular kernel matrices with positive entries  e.g.  gaussian  and many kernels for discrete structures or distributions . kta will disadvantage the use of such kernels.
1 feature space-based kernel matrix evaluation measure
in this section  we introduce a new goodness measure of a kernel matrix for a given  binary classification  task  named feature space-based kernel matrix evaluation measure  fsm . this measure should be computed on the kernel matrix efficiently  and overcome the limitations of kta. our idea is to use the data distributions in the feature space. specifically  two factors are taken into account:  1  the within-class variance in the direction between class centers; and  1  relative positions of the class centers. these factors are depicted in figure 1. the first factor improves the first condition of kta  by allowing data to vary in certain directions. the second factor solves the problem imposed by the second condition of kta. let var be the total within-class variance  for both classes  in the direction between class centers. denote center of a class as the mean of the class data in the feature space: and
. our evaluation measure fsm is defined to be the ratio of the total within-class variance in the direction between the class centers to the distance between the class centers:
	.	 1 
1 fsm calculation
we show that the evaluation measure can be calculated using the kernel matrix efficiently with simple formulas. we first calculate the within-class variance  of both classes  in the direction between class centers. denote as the unit vector in this direction. the total within-class variance of two classes in the direction is:
 1 
.
¡¡we calculate the first term in equation  1   the total within-class variance of the class +1 in the direction between ¦Õ+ and ¦Õ   denoted as var+.
 1 
.
we substitute and
into equation  1   and then we define some auxiliary variables as follows. for i = 1..n+:
 
.
for:
  .
denote and
. therefore   ¦Õ    ¦Õ+ 1 = a + d   b  
plugging them into equation  1   then:
	.	 1 
we can use a similar calculation for the second term in equation  1 :
¡¡the proposed kernel matrix evaluation measure fsm  as defined in formula  1   is calculated as:
	.	 1 
	 	 
  and the smaller fsm k y  is  the better k is.
¡¡one can easily see that ai  bi  ci  di  a  b  c and d can be all calculated together in o n1  time complexity  one pass through the kernel matrix . therefore  the evaluation measure is also efficiently calculated in o n1  time complexity.
1 invariance
there are some properties of fsm k y  regarding linear operations that make it closer to svms than kta is.
¡¡theorem 1: fsm k y  is invariant under translation  rotation and scale in the feature space.
¡¡sketch of proof: fsm k y  is scale invariant owing to its normalization factor a + d   b   c. fsm k y  is rotation and translation invariant  as it is built on the building blocks of ai   bi and di   ci.
¡¡the performance of svms and other kernel methods are unchanged under rotation  translation and scale in the feature space. therefore  it is reasonable to ask for measures to have these properties. however  kta is neither rotation invariant nor translation invariant.
1 error bound
we show that for any kernel matrix  it is possible to obtain a training error rate  which is bounded by some amount proportionate to fsm k y . this means that a low fsm k y  value can guarantee a low training error rate. in this case  we can expect a low generalization error rate.
¡¡theorem 1: there exists a separating hyperplane such that its training error is bounded by:
	.	 1 
¡¡proof: we use a one-tailed version of chebyshev's inequality  feller  1 . consider the data distribution in the direction between class centers. for each class  the data distribution has a mean of either ¦Õ+ or ¦Õ   and the corresponding variance var+ or var . the following inequalities can be derived:

1
.
1 + k1
 1 
¡¡the separating hyperplane  which takes e as its norm vector and intersects the line segment between ¦Õ+ and ¦Õ  at the point h such that  has the formula:
	.	 1 
the error rate of this hyperplanefor each class is bounded using the inequalities in equation  1  with
. therefore  the total training error rate on both classes is also bounded by:
	.	 1 
1 discussion
the measure can be interpreted as the ratio of within-class variance to between-class variance. it indicates how well the two classes are separated. it is advantageous over kta because it takes into account the within-class variances at a finer

figure 1: results on synthetic data with different ¦Â values.
scale  and relaxes the strict conditions of kta by considering relative positions of classes in the feature space. for the examples in section 1  fsm values of the best cases are always 1  those of the worst cases are always ¡Þ  and for the popular case  there are no bounds. this measure also has some similarities with other works.
¡¡the minimax probabilitymachine mpm   lanckrietet al.  1  controls misclassification probability in the worst-case setting. the worst-case misclassification probability is minimized using semidefinite programming. the worst case is determined by making no assumption rather than the mean and the covariance matrix of each class. mpm is similar to our approach that our measure also shows the worst-case misclassification error. the difference is that mpm considers the data variance in all directions  by using the covariance matrix   while our measure takes only the variance in the direction between the class centers. that is the reason why our measure is a lightweight version of mpm and can be calculated efficiently. a special case of mpm is when the direction between class centers contains eigenvectors of both covariance matrices; the objective function that mpm optimizes is equivalent to our measure.
¡¡in the context of nearest neighbor classification  one of the criteria of learning a good distance function is to transform data such that examplesfromthe same class are collapsed into one point  globerson and roweis  1 . our measure also shows quantitatively how much a class collapses. however  the key difference is that in their method  the objective is the whole class collapses to one point  while our measure shows how the whole class collapses to a hyperplane. this difference explains why their method is applied to nearest neighbor classification  while our measure is for kernel methods.
1 experiments
as the purpose of these measures is to predict efficiently how good a kernel matrix is for a given task using kernel methods  we compare the measures to the de facto standard of cross validation error rates of svms. we mimicked the model selection process by choosing different kernels. we monitored

australian	diabetes	fourclass
	poly	rbf
ionosphere
figure 1: results on different uci data sets.

lin.	tanh
lin.	tanh

the error rates  as baselines   kta and fsm  to see if how they reflect the baselines. we used 1-times 1-fold stratified cross validations to estimate the error rates.
¡¡to facilitate visual inspection of the results  we instead showed the following quantities:  a  1 kta   b  fsmerr  defined as error bound based on fsm measure in formula  1   and  c  error  cross validation error rates. the reason is that all of these quantities range from 1 to 1 and relate to the expected error rates of the kernel function in some sense. the smaller their values  the better the kernel. the first two quantities are expected to be correlated to the third one.
1 synthetic data
we generated synthetic data in r1  and used linear kernels to simulate different data distributions by different kernels in a feature space. for each data set parameterized by an angle ¦Â  we used two gaussian distributions  fixed variance in all directions  for two classes centered at ¦Õ+ =  1  ¡Ê r1 for class +1 and at ¦Õ  =  cos ¦Â  sin ¦Â   ¡Ê r1 for class  1. each class contains 1 training examples. the variances of the gaussian distributions are determined to be
. this ensures that for any ¦Â 
any data set can be images of another after a linear operation. therefore  these data sets with linear kernels are equivalent to one data set with different kernel functions. for any ¦Â   1  the problems should have the same level of error rates  or other measures  when using linear kernels  i.e. linear kernels should be evaluated at the same level of goodness. we run experiments with different ¦Â values of 1  1  1  1  1 and 1  degrees  in turn. the results of 1-kta  fsmerr and error are shown in figure 1.
from figure 1  we can observe that error is stable across different ¦Â  as we expected. fsmerr is also rather stable  varying similarly to error. 1-kta changes dramatically from 1 down to 1. it can be concluded that kta is too sensitive to absolute positions of data in the feature space. this confirms our claim about the limitations of kta  and shows that our measure is more robust.
1 real data
we showed the advantage of our proposed measure over kta in real situations by selecting several popular data sets from the uci collection for experiments. data names and experimental results are displayed in figure 1. data was normalized to   1 . classes were groupedto make binary classification problems. we chose four types of kernel for model selection: linear kernels  polynomial kernels  degree 1 with scale 1   rbf  gaussian  kernels  default   and tan-hyperbolic kernels  default 1. as described above  we ran cross validations and displayed the results of different kernels.
¡¡the following conclusions can be observed from the graphs:
  rbf kernels give the  approximately  lowest cross validation error rates in 1 out of 1 data sets. this shows that rbf kernels are usually a good choice. however  kta makes rbf kernels the worst one as values of 1-kta are the highest ones in 1 out of 1 cases. our measure agrees with the error rates in most cases  making it more reliable than kta is.
  similarly  tanh kernels perform worst in most cases  while kta values are among the best ones.
  in case of very low error rates as in breast-cancer data set  fsmerr is highly correlated to error rates  but kta is not.
¡¡we ranked the kernels according to error  kta and fsm separately from 1 to 1. we collected the rank of the best kernel  according to error  for each dataset. on average  kta ranks the best kernel at 1  ¡À1  while fsm ranks it at 1  ¡À1 . a student t-test  with 1% level of confidence  shows that fsm ranks the best kernel closer to 1 than kta. this also confirms the advantage of fsm.
¡¡in summary  we used synthetic and real data sets and compared measures against the de facto standard of cross validation error rates. we showed that our measure correlates more closely to the error rates than kta does. when ranking the best kernel according to the error rates  fsm shows a significantly lower rank  on average. this suggests that our measure is more reliable. this confirms our analysis of the limitations of kta and that our measure can overcome themse limitations. it was also observed that there is still a gap between kernel matrix evaluation measures and error rates  as error rates are collected from many training rounds while measures are calculated with one pass through the kernel matrices. this is the trade off necessary for efficiency.
1 conclusion
the paper shows that kta  an efficient kernel matrix measure  which is popular in many contexts  has fundamental limitations  as it is only a sufficient  not a necessary condition to evaluate the goodness of a kernel matrix. a new measure is proposed to overcome those limitations by using data distributions in the feature space. this new measure follows the conventional wisdom of measuring within-class and between-class variances in an appropriate way. the measure provides the same efficiency as kta  as evaluated in o n1  time complexity  and other properties. it also has links with other methods. this measure is more correlated to the error rates of svms than kta is.
¡¡the implication of this work is vast. one can take into account finer data distribution models in the feature space  to improve the current work. also  there are a large number of applications of this measure for other methods. we can directly apply this measure  similarly to kta  to many feature and model selection problems  such as boosting kernel matrices  multiple kernel learning  feature selection and so on. in general  with efficient kernel evaluation  we can leverage the work of kernel matrix and kernel function learning  which is of central interest in kernel methods.
acknowledgements
we would like to thank japan mext for the first author's financial support  and anonymous reviewers for helpful comments.
