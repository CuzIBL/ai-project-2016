broadcast news navigator  bnn  demonstration 
mark maybury 
information technology division 
the mitre corporation 
1 burlington road 
   bedford  ma 1  usa mavburv a  mitre. org www  mitre. or1/resources/centers/it 

summary 
the broadcast news navigator  bnn  is a fully implemented system that incorporates image  speech  and language processing together with visualization and user preference modeling to support intelligent  personalized access to broadcast news video. the demonstration will illustrate the use of the system's underlying machine learning enabled story segmentation and processing  called the broadcast news editor  bne . a live  scenario-based demonstration will illustrate the use of named entity search  temporal visualization of entities  story clustering and geospatial story visualization  discovery of entity relations  and personalized multimedia summary generation. by transforming access from sequential to direct search and providing hierarchical hyperlinked summaries  we will demonstrate how users can access topics and entity specific news clusters nearly three times as fast as direct search of digital video. in short  we will demonstrate intelligent news on demand enabled by a suite of al technologies. 
1 	intelligent segmentation 
fundamental to bnn is the exploitation of text  audio  and imagery streams and associated cues which are used to detect story and commercial segment boundaries and to select media elements to use for summaries and multimodal displays. we will demonstrate how certain cross media cues such as language content  e.g.  frequent weather or sports terms  funding and/or copyright notices   discourse cues  e.g.   coming up next    music  e.g.  characteristic jingles   silence indicating breaks to commercial  and visual cues  e.g.  logos  anchor booth vs report shots  help signal program start/end  anchor/report shots  commercials  and/or story shifts. underlying bnn is a set of machine learned  time-enhanced finite state automata modeling news structure that take into account the above cues and probabilistic  temporal models of event occurrence  boykin and merlino  1 . 
1 	named entity search and retrieval 
bnn supports the retrieval of stories based on user query  either keyword  named entity  or topics. for example  the user can specify source  date ranges  keywords  subjects  or named entities. as show in figure 1  after selecting news programs  e.g.  cnn newsnight  abc world news tonight  fox news  and indicating date ranges  in this case january 1  1   a user can simply type in keywords in the text box  e.g.   korean weapons of mass destruction  . in addition  if a user is unfamiliar with retrieval terms  they can display an alphabetic listing of all the named entities extracted for the time period and from the programs of interest  such as shown in the person  organization  and location searchers in figure 1. note the evidence of information extraction errors such as  english  under location or  reserve  under organization. we will demonstrate how information extraction  aberdeen et al.  1  on noisy data such as closed caption or speech transcription  as opposed to less errorful newswire text  drops from about 1% precision and recall to about 1%. 
if the user selects  north korea  in figure 1  they are provided access to the 1 stories detected during the week  as presented in the  story skim  display shown in the left of figure 1. in this case the system shows the source and date  the top 1 named entities in each story  and a representative key frame from each selected segment extracted using heuristics based one the type of story segment  e.g.  anchor  reporter  interview . note the stories range from several sources  cnn  cnn money line  cnn newsnight with aaron brown   dates  and times of day. the media elements used in  story skim  and  story details  were selected after careful empirical study of the optimal combination of multimedia elements for video retrieval and extraction tasks  merlino and maybury  1   the current system  maybury 1  allows the user to interactively customize these to their preference. 

intelligent systems demonstrations 	1 


figure 1. text and named entity search menus 

figure 1. january 1 north korean  story skim'* and 
 story details  
1 relevancy feedback 
users can navigate directly from the story skim of figure 1 on the left to a  story details** display as shown on the right.  story details  include the key frame  one line summary  the story line containing the most named entities   all extracted named entities  and pointers to video source  transcript  and relevant stories. the user can either review the story or engage in further query refinement  accomplished with an underlying local context analysis  lca  algorithm. 
1 visualization and discovery 
as shown in figure 1  users can visualize named entity frequencies by collection  source  or date range  visualize temporal occurrences  and display story occurrences associated with geospatial regions  animated over time. finally  users can interactively run data mining algorithms to discover relationships among named entities  e.g.  people associated with locations or organizations   and automatic detection of topic clusters. in addition  users can set profiles that specify their media  e.g.  text summary  image key frames  summary   and navigation preferences  e.g.  overviews or not  links to related stories . we demonstrate live how bnn users can find video stories and answer questions about two to three times as fast as with original sources  with no performance loss. 

figure 1. entity and trend analysis; geospatial display 
acknowledgments 
bnn is the product of numerous contributors including 
stanley boykin  andy merlino  warren grieff  jay ponte  chris clifton  and chad mchenry. 
