 
　　　a folk theorem is developing which suggests that parallel solution of ai programs will not afford a speedup of more than one order of magnitude. we critically review this folk theorem by analyzing some of the problems used to  prove  it  and then cite work that provide examples of better than one order of magnitude improvement for these problems. we examine two representative ai algorithms where parallelism would achieve speedups of two orders of magnitude with a reasonable number of processors. 
i. introduction 
　　　a few years ago the combination of rapidly decreasing prices for microprocessors along with their increasing computational power spawned the belief that massive amounts of parallelism would greatly enhance the power of computational models of intelligence. early experiences with multiprocessor architectures dampened this belief. in particular lenat found that up to four processors could be efficiently used to speedup the execution of his eurisko program  but beyond that additional processors were not useful  l . fennell and lesser's multiprocessor implementation of hearsay ii gave a speedup of 1 with 1 processors  1 with 1 processors  and an estimated 1 with an infinite number of processors . an empirical analysis of a forward chaining rule-based expert systems by forgy suggested an upper limit of a speedup of 1 by parallel rule execution . a generalization of these experiences has led to the emergence of a folk theorem which states that the amount of achievable or effective parallelism in ai programs will be limited to a factor of ten. 
　　　in this paper we show the flaws in arguments supporting the above-mentioned folk theorem. we do this by providing examples from the literature of parallel algorithms for rule-based systems and intelligent search. these algorithms are capable of leading to better than one order of magnitude speedup through the use of a  reasonable  number of processors. by  reasonable  we mean a speedup of a factor of n should be obtained from approximately n log n processors  not n * n processors. 
　　　the folk theorem addresses itself to problems that are dominated by global search  and our critique is based on parallel algorithms for just these sorts of tasks. for other types of ai programs  for example perceptual tasks such as scene analysis and speech recognition  effective parallelism on the order of four orders of magnitude can be achieved  and are essential  see duff  . 
　　　finally  we should point out that the folk theorem puts an upper bound on the asymptotic speedup of ai programs. our response is in the same vein  showing where asymptotic increases in execution time may be obtained through different algorithms. we do not address the engineering problems of communication costs and load balancing that must be taken into account when implementing the algorithms on a multiprocessor. 
ii. folk theorem 
　　　the most expensive portion of most ai programs occurs during some type of search. search is apparently parallel; a branch in the search space is a natural place to apply multiple processors. when tried  however  the speedup was not proportional to the number of added processors  but leveled off after a speedup of a factor of 1 times  regardless of the number of additional processors. 
　　　the argument for this lack of speedup follows: unlike a data base program which  perhaps  must follow all paths of a search  most ai programs involve heuristic search where borne paths are not explored fully. the output of heuristic search is usually neither the optimal solution nor all solutions  but some  satisfactory  solution. in such cases  additional processors  after the first few  tend to increase the size of the search space explored  but fruitlessly. resources are devoted to unimportant paths  increasing the total number of computations performed  but not decreasing  in proportion  the time to find a solution. the argument is that a clever sequential algorithm which selects the most promising paths will perform nearly as well  or better  than a naive parallel algorithm that explores all paths. 
　　　although practical experience supports this argument  both the argument and the experience are seriously misleading. the argument makes implementation assumptions about the way in which parallelism ought to 
　　　
* partial support for this research was provided by hughes ai research center  calabaaas  ca. 
　　　
1 	d. kibler and j. conery 
be achieved  and these assumptions are mirrored in the actual implementation. consequently the flawed argument is merely repeated in the implementation. a correct implementation of parallelism would execute the same operations as the sequential algorithm  only with some computations carried on simultaneously. for a correct analysis of the limits of the potential speedup due to parallelism we need to introduce some concepts. 
　　　let a computation c be specified by a sequence of primitive operations. the nodes of the computation graph associated with c are the primitive operations. a directed arc is drawn between operation f 1 and operation f1 if f 1 enables f1  e.g. it produces some information needed by f 1. to make this analysis automatic it is useful to have a functional language. for logic programs the creation of this computation graph can be done using methods developed by conery and kibler  1   
　　　we can now make a number of simple inferences about computation graphs. the computation graph is always acyclic. the depth of the graph gives the minimum completion time for the computation. the size of the graph  as measured by the number of nodes  gives the completion time for a uniprocessor  which effectively does a depth first traversal  never visiting a node until all of its ancestors have been visited . the maximum effective parallelism  i.e. the maximum number of processors that can be simultaneously working  is a function of the breadth 
of the graph. the maximum speedup is the ratio of the size to the depth. 
　　　the difficulty for achieving a large amount of parallelism in ai programs  without incurring exponentially growing costs  is that the particular computation graph is hidden within a much larger  broader computation graph of all potential operations. in other words  a  trace  of a sequential execution gives one possible graph. consequently it is very difficult to distribute the computation amongst the various processors so that a speedup proportional to the number of processors is achieved  since additional processors are visiting nodes that were not even part of the graph in the sequential computation. indeed very surprising effects of parallelism have been noted: using k processors  one may achieve a speedup of greater than k  or conversely a speedup of less than 1  i.e. a slow-down over uniprocessor speeds. li and wah analyzed necessary conditions for these anomalies . 
　　　in light of these unexpected results one can understand why the first approaches to achieving parallelism were inconclusive. indeed  a simple parallel search  called or parallelism  allows for no parallelism within deterministic subcomputations. research on functional programming languages has shown substantial speedups can be achieved for such tasks as sorting  matrix multiplication  summing a vector  etc. moreover these speedups can be realized without the programmer specifying when and where the parallelism should be instigated  1 . one of our points is that even though the time complexity of an ai program is dominated by search time  there may be substantial speedup from executing deterministic subcomputations in parallel. the execution of these computations in parallel is referred to as and parallelism. 
　　　relying on both and and or parallelism  we examine two representative ai algorithms and show that the amount of speedup is orders of magnitude beyond that suggested by the folk theorem. 
iii. alpha-beta 
　　　consider the standard alpha-beta algorithm that is the basis of most chess programs. note that this algorithm lies within the purview of the argument developed in the folk theorem  so one might be led to expect a maximum speedup of one order of magnitude. finkel and fishburn developed a special parallel algorithm for alpha-beta search which achieves a speedup of k with k processors . although this approach gives unbounded speedup  it takes 
1 processors to yield a speedup of 1. we shall demonstrate a different approach to gain a factor of 1 which uses roughly 1 processors. for concreteness  let us suppose that the terminal evaluation function is the sum of 1 features and that the branching factor is exactly 1. the effective branching factor  due to alpha-beta search  is about 1. 
　　　following a naive approach to achieve parallelism  we would take 1 processors and give each of them one branch of the tree to search. since the effective branching factor is 1  most of the computations performed by this parallel search would not be done by a sequential processor and  in fact  are unimportant to the final result. the overall net gain would be a factor of 1  as predicted by the folk theorem. carrying this process three levels into the tree  we see that 1 processors are used  with only 1 searching useful paths; the effective speedup  as a percentage of the number of processors used  soon levels off. 
　　　now let us achieve the parallelism in another way. we will not allocate processors until the next-to-last expansion in the tree  while growing the tree with the usual sequential algorithm  pruning as we go. when we reach the next to last expansion  we spread 1 processors out amongst the terminal nodes. for each terminal node we allocate an additional 1 processors  one processor for each feature in the evaluation function. we have now allocated a total of 1 processors to the task. the amount of speedup depends on the total number of useful computations that this allocation scheme provides. again the fact that the effective branching factor is 1 implies that only 1 of the processors are doing useful work. in addition the initial tree generation is done sequentially  but once the search has depth more than 1  this has little effect on computation time  as can easily be computed.  we will 
　　　
examine the effect of inherently sequential subcomputation in a later section.  thus we have achieved a speedup of more than two orders of magnitude with a comparable number of processors. 
　　　another parallel alpha-beta algorithm is the  key node  method of lindstrom . no asymptotic performance bounds are given  but simulation results  which take into account message passing and memory contention effects  show a speedup of better than 1 with only 1 processors. 
iv. rule interpreter 
　　　let us suppose we have a standard forward chaining rule-based interpreter with 1 rules. the predicates or conditions for these rules come out of a language with 1 predicates. we further suppose that rules have an average of 1 conditions and 1 actions. a standard rule-interpretation cycle is: 
1. find all rules that can fire 
1. choose  best  rule 
1. apply selected rule 
 now one could achieve a speedup of nearly a factor of 1 over this simple rule interpreter by dedicating a processor to each rule  as step 1 is the time-dominating portion of the computation. roughly  the argument is: step 1 requires about 1 operations  1 rules with 1 conditions ; step 1 requires about 1 operations  scoring each of about 1 rules and finding the  best  one ; and step 1 requires about 1 operations  doing 1 actions . the last two steps take 1 1% of the time  ergo the speedup of about 1. however  it is not necessary to dedicate one processor to each rule in order to achieve a substantial speedup. the rete algorithm  developed by forgy  uses changes to the working memory to determine possible changes to the conflict set . his measurements indicate that each modification to the working memory enabled or disabled about 1 rules. with an average of 1 modifications to memory per rule execution  we should have to check the lefthand sides of roughly 1 rules  not all 1. furthermore  these figures  1 changes  1 rules  appear to be independent of the number of rules in the system. 
　　　these observations have led some to believe that there is minimal effective parallelism in production systems. since not all rules have to be checked on each cycle  adding processors to check rules will not help. beyond the speedup in the execution of rete  at most an order of magnitude  there is no more possible parallelism in production systems. 
　　　however  as ofiazer points out  the analysis is based on production systems written in the ops1 language  and his conclusions about minimal parallelism should be limited to systems written in that language . thus taking a set of rules designed for a sequential production system interpreter  and then looking for parallelism in that set  
d. kibler and j. conery 1 
will lead one to erroneous conclusions about production systems in general. a similar line of thinking for numerical applications would lead one to conclude that since there is minimal parallelism in any given fortran program  the problem solved by that program cannot be solved in parallel. 
　　　as an example of a rule interpreter that is not organized along the lines of the test-select-execute cycle  consider the  suspension interpreter  defined in stammer . this interpreter associates a process with each partially instantiated rule. the conditions of each rule are regarded as a set of tasks to be completed. moreover  associated with each condition is the list of processes  partial rule instantiations  that involve the condition. when new facts are added to working memory  all of the relevant rules  processes  can try to complete their execution. those processes that complete their tasks belong to the conflict resolution set.  processes are also returned to earlier points in their computation if elements of working memory are deleted  but this is a rarer occurrence . depending on the number of processors available  one can distribute suspended processes to processors. the overhead for this activity can be very high  but a speedup for two-orders of magnitude is not out of the question. 
　　　the selection of the best rule from the conflict resolution set involves either finding the maximum or sorting. it is  perhaps  surprising that both finding the maximum 
or sorting n items can be done in 1 lg n   time by n/1 processors. this is the second most costly operation in the rule interpreter and  with parallelism  can be effectively eliminated. the overall increase in efficiency is still dominated by the two-order of magnitude speedup in computing the conflict resolution set. 
　　　another approach towards achieving parallelism for rule interpreters is that suggested by stolfo and shaw  which uses the tree-machine architecture of the dado machine  1 . the rete algorithm was invented to efficiently execute production systems on a standard von neumann machine. two of the limitations assumed by the rete algorithm  that working memory not contain variables and that working memory change slowly  are avoided by the dado architecture. the achievable speedup of the architecture proposed by slolfo and shaw is not yet known . more recently shaw has proposed the nonvon architecture   which mixes simple and complex processors. simulations of non-von on productions systems have yielded speedups of a factor of 1  based on the same examples as examined by forgy and gupta. 
　　　lastly  we should mention that sequential assumptions and constraints have already been built into our abstract formulation of a rule-interpreter. if we have a pure inference system where there is no need to retract deductions  then why should we bother to decide which inference is the most fruitful  if a rule  process  becomes fireable  let 
1 d. kibler and j. conery 
it fire. there is no need to segment execution into finding fireable rules  selecting a rule  and then executing it. with a pure inference system rule instantiation can be tested and fired in parallel. 
　　　the point of these discussions is that there are many ways to implement parallelism and the effectiveness of parallel processing for ai has not yet been determined. in the next section we examine the speedup limitation due to inherently sequential subcomputations. 
v. amdahl's law 
　　　a deep concern for unexcelled computational speed led amdahl to examine the potential speedups arising from parallel computation. amdahl supposed that he had p processors to allocate to a computation which contained an inherently sequential subcomputation. he supposed that the sequential portion of the computation required a fraction / of the total computation. under these assumptions  the upper limit on the amount of speedup s possible is given by the formula: 

in particular we see that even with an unlimited number of processors this computation cannot be spedup by more than a factor of 1 / / . for example  if 1% of a computation is inherently sequential  then the maximum speedup is a factor of 1. thus  even if only a marginal portion of 
the computation is sequential  the maximum achievable speedup is severely limited. 
　　　we must be careful in applying amdahl's law  as demonstrated by the preceding section. it is not enough to show that a particular algorithm has an inherently sequential subcomputation. we must ensure that the algorithm has not already been conceptualized in a sequential manner. 
vi. conclusions 
　　　this papers argues that the apparently small gains due to parallelism are an attribute of a naive approach towards parallelism. several examples were developed which demonstrate that both the conclusion and the argument for the folk theorem are wrong. a means for estimating the effects of sequentiality on parallelism was given. an accurate method for evaluating the speedup and the number of processors required  based on the concept of a computation tree  was developed. to achieve this parallelism is not simple  but holds promise for ai computations. let us avoid ai's history of jumping to conclusions and turning first impressions into strongly held beliefs/prejudices. we are still gathering evidence on the effectiveness of parallel processing for ai. 
acknowledgements 
　　　some of the arguments presented here were finetuned in conversations with gary lindstrom  chip maguire and david shaw. 
