learning discontinuities for switching between local models  
marc toussaint and sethu vijayakumar
institute of perception  action and behavior
school of informatics  university of edinburgh
the king's buildings  mayfield road  edinburgh eh1jz  uk. mtoussai inf.ed.ac.uk  sethu.vijayakumar ed.ac.uk

1 introduction
locally weighted learning techniques  in particular lwpr  vijayakumar et al.  1   have successfully been used for high-dimensional regression problems. their robustness and efficient online versions are crucial in robotic domains where  for instance  an inverse model of an articulated dynamic robot has to be learned in real-time. such models map a highdimensional state  e.g.  joint angles and velocities  and a desired change of state to the required motor signals  torques .
¡¡while typically such mappings are assumed to be smooth  in real world scenarios  there are many interesting cases where the functions of interest are truly discontinuous. some examples include contacts with other objects  in particular the ground   with other parts of the body  or with  joint limits . in fact  many interesting interactions with the environment manifest themselves through discontinuities in the sensorimotor data.
¡¡in this paper  we show how discontinuous switching between local regression models can be learned. the general topic of switching models has been discussed before  e.g.  in the context of state space models  ghahramani and hinton  1; pavlovic et al.  1  or multiple inverse models  wolpert and kawato  1 . generally  the question of which particular model receives responsibilities for a given input can be modeled as a hidden variable i in a generative mixture model. in our case  we assume that the responsibility index i can be predicted from the input  the robot state . thus  inferring a model for i corresponds to classifying the input domain into regions for each sub-model.
¡¡since in robotic domains  local learning is crucial to prevent interference and allow for online adaptation techniques  we propose a model of the responsibility index i which is itself a composition of local classifiers. multiple pairwise classifiers are concatenated to construct a complete model in the form of a product-of-sigmoids  which is capable of learning complex  sharply bounded domains for each local model in lieu of the typical gaussian kernels.
1 learning a family of models
given training data with inputs xk and outputs yk  the first level goal of our algorithm is to learn a family of models {¦Õ1 .. ¦Õn} such that every datum can be explained

 
¡¡¡¡the first author acknowledges support by the german research foundation  dfg   emmy noether fellowship to 1-1.by at least one model. the problem of predicting which particular model ¦Õi is responsible for a given input x is solved on a higher level as explained in the next section. in formal notations  we assume a mixture model
 
where u y  is a uniform distribution accounting for background noise  e.g. outliers  and i is the hidden variable specifying the particular model that generates a datum. since we aim for localized models  we impose a locality constraint already at this level as follows: let ci denote the mean input  center  on which model ¦Õi has been trained on. for a given input x  the ith model is eligible if and only if there does not exist a jth model which has its center  between  ci and x. more precisely 
ploc i|x  = 1     j : hx   cj ci   cji   1   where	h¡¤ ¡¤i	is	the	scalar	product	in	input	space:
i is eligible for x	i is not eligible for x ci
further  ploc i|x  is uniform over all eligible i's. given a current family of models  we can infer a posterior on the responsibility index i for a given datum  x y   using bayes rule:
.
calculating the map assignment  i allows us to associate every training datum with the most likely model. using this  the sufficient statistics of each local model ¦Õi is updated. further  the data that is labeled as  yet unmodeled   which is inferred to have been generated by u y   is used to generate a new family member by the following heuristic  compare ransac : a random datum  x y  is selected from the unmodeled data; the k closest neighbors of  x y   w.r.t.  euclidean input distance  are chosen as initial training data for the new model  where k is a random poisson number with mean 1d  here  d is the input dimensionality . finally  models that receive too few map responsibilities  less than 1d in our experiments  are discarded. this iterative process can be repeated until no new models are generated.
figure 1: kernels that can be represented as a product of sigmoids in 1d and 1d.
¡¡this general scheme of family learning can be realized with any type of models ¦Õi. in the experiments  we will choose ¦Õi to be linear functions  learned with partial least squares  pls  regression. pls  involving an intermediate lower-dimensional projection  has been proven efficient for high-dimensional problems  vijayakumar et al.  1 .
1 products of sigmoids for switching
on the second level of our algorithm  the goal is to learn a predictive model p i|x  of the latent responsibility index i that is more precise than the uninformed prior ploc i|x . given some data  it is easy to decide whether two models are  potentially neighbored -namely whether there exists data for which both models are eligible-based on their centers. for each pair  ij  of neighbored models  we learn a sigmoidal function ¦×ij x   where ¦×ij ¡Ô 1   ¦×ji. the product of such sigmoids around a submodel i defines a coefficient ¦Âi x  that we associate with the submodel for a given input x 
 .
here  z1 normalizes ¦Âi over i. as indicated  we represent sigmoids ¦×ij with a scalar function ¦Õij. fig. 1 illustrates the kind of kernels can be represented as products of sigmoids.
¡¡the sigmoids ¦×ij x  are meant to represent the likelihood that a model i rather than j is responsible for an input x  conditioned on that either i or j is responsible. the product combination is comparable to an and voting. the map labeling  i we introduced in the previous section is now used to train these sigmoids. in the experiments  we consider ¦Õij to be linear functions  again learned with pls.
1 experiments
we tested the algorithm on piecewise linear  discontinuous test functions. a test function has 1 parameters: the input dimension d  the number l of linear pieces it is composed of and the output noise ¦Ò. the localities  slopes and boundaries of the linear pieces are sampled randomly. fig. 1 a b  display learning results from a 1d example in comparison to lwpr. fig. 1 c d  display two error curves on 1-dimensional test functions over the rather large input domain   1 1. the family error is the mse of the best fitting eligible model ¦Õ i  averaged over an independent test data set ; the classification error counts how often the product of sigmoids correctly predicts ¦Õ i to be the best fitting model for a given input  i.e.  argmaxi¦Âi =  i . in the experiments we find that the algorithm reliably generates a family with optimal family error at the noise level  ¦Ò1 = 1 . in 1 dimensions  not displayed here  the classification error rapidly converges to zero while in 1 dimensions  the classification error converges to around 1%. for more results see homepages.inf.ed.ac.uk/mtoussai/projects/1-ijcai.

figure 1:  a  a 1d test function with d=1  l=1  ¦Ò=1. learned switching model after 1 iterations on m=1 training data points.  b  the blended switching model: y x  = pi ¦Âi x ¦Õi x  compared to lwpr.  c  family error  cf. sec. 1  and  d  classification error for 1 runs on random test functions with d=1  l=1  ¦Ò=1  and m=1. the bold line is the average over all curves.
1 discussion
the presented model addresses the problem of handling the discontinuities that naturally arise  e.g.  in sensorimotor data during interaction with a structured environment. our model extends earlier local learning approaches in several ways: the responsibility region associated with each local model  learned with the product of sigmoids  has a much more versatile boundary shape compared to typical gaussian kernels. problems associated with initialization of kernel shapes or widths and the heuristic choice of an ad hoc number of submodels are circumvented by the robust incremental allocation of new models. although we consistently used pls as the underlying regression machinery  the general model allows to utilize any efficient single model learner to represent the local models ¦Õi as well as the classifier functions ¦Õij. future work will in particular investigate non-linear learners for the local models as well as the boundary classifiers.
