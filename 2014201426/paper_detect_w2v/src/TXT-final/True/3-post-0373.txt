
the handling of exceptions in multiclass problems is a tricky issue in inductive logic programming  ilp . in this paper we propose a new formalization of the ilp problem which accounts for default reasoning  and is encoded with first-order possibilistic logic. we show that this formalization allows us to handle rules with exceptions  and to prevent an example to be classified in more than one class. the possibilistic logic view of ilp problem  can be easily handled at the algorithmic level as an optimization problem.
1 introduction
the handling of exceptions is a serious bottleneck in ilp due to the formalization in first-order logic. indeed  when a rule has some exceptions  i.e. some examples are misclassified by it  there is no way to compensate these exceptions by means of another rule. so  a hypothesis accumulates all the exceptions of the rules that appear in it. moreover  when dealing with more than two classes  a rule with some exceptions may prevent another one to perform the right classification. a proper handling of exceptions can be made by adding to the standard ilp setting a logical constraint that expresses that an example can be only classified into one class. thus  having some exceptions may lead to inconsistency. in contrast with first-order logic  default reasoning encoded in possibilistic logic is well-suited for dealing with inconsistency. in this paper  we extend the ilp settings in a possibilistic way in order to handle inconsistency due to exceptions.
1 preliminaries
stated in the general context of first-order logic  the task of ilp is to find a non-trivial set of formulas h such that b ¡È h |= e given a background theory b and a set of examples e of the form c x y  where x denotes the identification key of an example and y a class. e b and h here denote sets of horn clauses. in order to treat multiple classification of an example as inconsistency  we reformulate the ilp problem  by adding the following classification constraint d ¡Ô  x y z c x y   ¡È c x z  ¡ú y = z  as follows : given b  d and e  the goal is to find h such as b ¡È d ¡È h |= e.
¡¡we extend propositional possibilistic logic  dubois et al.  1  to the first-order case. possibilistic logic is sound and complete for refutation  using an extended resolution rule  with respect to a semantics in terms of a complete plausibility preorder on the interpretations  encoded by a possibility distribution   dubois et al.  1 . s denotes a set of herbrand interpretations. a possibility distribution can be defined on herbrand interpretations as well. the possibilistic degree of a formula ¦Õ is the maximum possibility level of its models and is denoted ¦° ¦Õ . necessity is the dual notion of possibility. it refers to the possibility degrees of the counter-models of a formula :n ¦Õ  = 1   ¦°  ¦Õ . a possibilistic first-order formula is a pair  ¦Õ ¦Á  where ¦Õ is a firstorder formula and ¦Á ¡Ê 1 . it is understood as a constraint on an unknown necessity measure of the form n ¦Õ  ¡Ý ¦Á. given a set k of possibilistic formulas  the ¦Á-cut of k is k¦Á = {¦Õ| ¦Õ ¦Â  ¡Ê k ¦Â ¡Ý ¦Á}. given t a set of classical first-order formulas  t is minimal w. r. t. a formula ¦Õ iff t |= ¦Õ and  ¦× ¡Ê t t   ¦× 1|= ¦Õ. the following definition avoids the drowning problem  i.e.  consequences are lost when their implicants are taken in inconsistent sets of formulas  although they don't contribute to inconsistency  by checking if it exists a proof of ¦Õ in k¦Á  which is free from inconsistencies at level ¦Á.
definition 1 given k a set of possibilistic first-order formulas  k |=¦Ð  ¦Õ ¦Á  iff  k¡ä   k¦Á k¡ä 1|= ¡Í such as k¡ä minimal w. r. t. ¦Õ and  k¡ä¡ä minimal w. r. t. ¡Í such that k¡ä   k¡ä¡ä   k¦Á.
in practice  this definition makes sure that  if two logical consequences are inconsistent  the one which has the highest necessity degree is preferred. as already advocated in the introduction  we propose to use possibilistic logic in ilp for a better handling of exceptions.
1 possibilistic ilp
given h a standard ilp hypothesis  let nh denote a function  called priority function  which at each rule in h associates a priority level  to be understood as a necessity degree. this gives birth to a possibilistic hypothesis hp = { h nh h  ;h ¡Ê h}. let bp and dp be composed by all formulas that appear respectively in b and d  with 1 as necessity level. the priority function ne over the examples is deduced from hp as follows :
     1  by convention  if 1  ¦Á   1 such that dp ¡È bp ¡È hp |=¦Ð  e ¦Á 
 
otherwise.
 1 
then  given d  b and e  the goal of possibilistic ilp is to find hp  composed by a classical hypothesis h and an associated priority function nh such that bp ¡È dp ¡È hp |=¦Ð ep with ep = { e ne e  ;e ¡Ê e}. note that  if it exists such hp  this hypothesis will be correct and complete. this enlarges the scope of classical ilp by learning sets of default rules in a framework that handles exceptions  the one of possibilistic logic.
any possibilistic hypothesis  even it contains some rules with exceptions  can be completed in order to be correct and complete by adding the misclassified examples with 1 as necessity level. then  the possibilistic ilp problem can be reformulated as an optimization problem :
given d  b and e  the goal of possibilistic ilp is to find hp that maximizes the accuracy  i.e. the proportion of wellclassified examples . this definition of possibilistic ilp problem is fully in agreement with the paradigm of the minimization of the empirical risk. since here  necessity levels are only used for obtaining an ordering of the formulas in h  this induces equivalence classes of priority functions nh. two priority functions  belong to the same equivalence class  i.e. nh ¡Ô nh¡ä if and only if  h1 h1 ¡Ê h if
nh h1    nh h1  then.
proposition 1 given h  finding the class of equivalence of priority functions such the accuracy is maximal is npcomplete with respect to the number of formulas in h.
it shows that  although using a possibilistic rather than the classical setting is always more effective  finding the best priority function over formulas in h may be computationally very costly. note that choosing a particular ordering based on the confidence or the support degrees of the rules is not optimal in general. it suggests to use heuristics for inducing hypotheses together with their priority function.
1 experimentations
abcdepmaximum111foil111indigo111cilgg111pilp avg1.1.1.1.1.1 1  1  1  1  1 pilp max111in order to learn possibilistic logic rules  we learn a hypothesis directly together with its priority function. the hypotheses are learnt by using a stochastic exploration of the hypothesis space as explained in  serrurier et al.  1 . the priority is found by using a greedy algorithm based on the 1-opt method  switching the order of rules two by two while accuracy increases . our algorithm is denoted as pilp. since pilp is a non deterministic algorithm  the results that are presented are average results on 1 running steps with the same settings. the value in the brackets are standard derivations for the 1 results. for each experiment  two results are shown : pilp avg is the average result for the 1 tests  and pilp max is the best result found in the 1 tests. the experimentation is made with the finite element mesh design dataset because it represents a typical hard ilp multiple class problem. it describes the structure of a mesh with unary and binary predicates. the dataset contains 1 examples that describe 1 classes. the dataset is split in 1 sub datasets denoted by a  b  c  d and e. the test of the accuracy of the algorithm is made by testing on one subset a hypothesis induced from the other sub datasets. results are shown in the previous table. the results for the algorithms other than pilp can be found in  kietz  1 . the results are clearly in the favor of pilp algorithm which increases the number of examples covered by the most effective algorithm up to 1% in average and 1% at maximum.
1 conclusion
in this paper  we have proposed a new ilp formalization for dealing with exceptions in a multiple class problem. in order to do that  we have extended possibilistic propositional logic to a first-order setting. then  by treating exceptions as inconsistency  we have reformulate the ilp problem in firstorder possibilistic logic. in this reformulation  the ilp problem is turned in an optimization problem. this formalization allows our algorithm to learn sets of default rules. in this context  it may exist a correct and complete hypothesis which contains rules that have some exceptions. possibilistic ilp is more flexible and more general than first-order decision lists  mooney and califf  1  and allows us to correctly cope with recursive hypotheses. experiments have proved that an implementation of possibilistic ilp may be very effective for propositional or for relational learning and can compete with the best machine learning algorithms.
