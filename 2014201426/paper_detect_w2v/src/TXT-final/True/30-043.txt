 
computer security depends heavily on the strength of cryptographic algorithms. thus  cryptographic key search is often the search problem for many governments and corporations. 
in the recent years  al search techniques have achieved notable successes in solving  real world  problems. following a recent result which showed that the properties of the u.s. data encryption standard can be encoded in propositional logic  this paper advocates the use of cryptographic key search as a benchmark for propositional reasoning and search. benchmarks based on the encoding of cryptographic algorithms optimally share the features of  real world  and random problems. 
in this paper  two state-of-the-art al search algorithms  walk-sat by kautz & selman and relsat by bayardo & schrag  have been tested on the encoding of the data encryption standard  to see whether they are up the task  and we discuss what lesson can be learned from the analysis on this benchmark to improve sat solvers. 
new challenges in this field conclude the paper. 
1 	introduction 
securing one's data and communication from unauthorized access in large open networks such as the internet is of the main issues for computer science today  anderson & needham  1; g1  1; oecd  1. 
　yet security depends heavily on the strength of cryptographic algorithms: security protocols which have been formally proved correct may be broken by the choice of a bad cipher ryan & schneider  1 . thus  cryptographic key search is often the search problem for many government and large corporations; and the ability of law enforcement officers to perform key search becomes the main concern behind the licensing of encryption technology  oecd  1. 
   * supported by cnr fellowship 1-1. this work has been partly supported by as1  cnr and murst grants. 1 would like to thank l. carlucci aiello  f. liberatore  and l. marraro for useful comments and discussions. 
1 	challenge papers 
　search  although in different settings  has also been a problem at the heart of al research for many years. recently propositional search has received attention for a number of factors  selman et al.  1: 
　　eirst new algorithms were discovered ... based on stochastic local search as well as systematic search  ...|. second  improvements in machine speed  memory size and implementations extended the range of the algorithms. third  researchers began to develop and solve propositional encodings of interesting  real-world problems  ...   between 1 and 1 the size of hard satisfiability problems grew from ones involving less than 1 variables to ones involving over 1 variables. 
following a seminal proposal from  cook and mitchel  
1   an application comes to one's mind: can we encode cryptographic key search as a sat-problem so that ai search techniques can solve it'! 
　a recent result in automated reasoning makes this possible. in  marraro & massacci  1 it has been show that  by combining clever reverse engineering  advanced cad minimization  and propositional simplification  it is possible to encode in propositional logic the properties of the u.s. data encryption standard  des for short   nist  1; 
schneicr  1 . an encoding whose size is within reach of current al search techniques: the encoding of a cryptographic search problem  finding a model is equivalent to finding a key  for the commercial version of des requires slightly more than 1 variables and 1 times many clauses. 
　although des is currently under review  it is still the most widely used cryptographic algorithm within banks  financial institutions  and governments. it is the algorithm on which cryptanalysts tested the final success of their techniques  see  schneicr  1  or sect. 1 for further references . even partial successes with al techniques can be relevant. 
　in this paper we claim that this problem should be one of the reference sat-benchmarks. in particular  it gives the possibility of generating as many random instances as one wants and still each instance is as  real-world  as any instance that can be met in commercial cryptographic applications. it provides a neat answer to the last challenge for propositional reasoning and search proposed by selman  kautz and mcallcster  1 at 1jcai-1. 
　
　to check the potential effectiveness of ai techniques on this problem  two state-of-the-art sat solvers have been tested for cryptographic key search using the propositional encoding. the choices are walk-sat  a local search algorithm proposed in  selman et al  1  as an improvement of gsat  and rel-sat  a combination of the traditional davisputnam algorithm with back-jumping and learning proposed in  bayardo & schrag  1  to solve real-world problems. 
　in the experiments on the data encryption standard  one shouldn't expect to be immediately competitive with twenty years of advanced cryptanalysis techniques  especially because al labs are not equally well funded to afford a specialized hardware machine of 1 usd or the exclusive use of a network of workstations for 1 days which have been used to break des in the last years. still  general purpose search algorithm using off-the-shelf hardware  spares and pentium ii  can crack limited versions of des without being told any problem-dependent information. ad-hoc cryptographic techniques arc still better since the first success with the limited version of des we can solve was obtained in 1  andleman & reeds  1 and modern cryptographic approaches  biham & shamir  1; matsui  1a  obtain the same results with better scaling properties. still  the result is promising and points out at weaknesses of ai search algorithms that we need tackle to solve hard problems. 
　in the next section  we introduce some basic preliminaries on cryptography and the data encryption standard. then we discuss the features of the encoding . this is followed by the experimental analysis with walk-sat and rel-sat . pew lessons for sat solvers we can learn  ′1  and new challenges   conclude the paper. 
1 	cryptography and des 
to make the paper self-contained for the non-security expert we sketch some preliminaries about cryptography and des  for an introduction see  schneier  1  . 
　following ischneicr  1   we denote vector of bits by p  the plaintext   c  the ciphertext   and k  the secret key . at an abstract level  a cryptographic algorithm is simply a function  that transforms a sequence of bits  the plaintext  into another sequence of bits  the ciphertext  with certain  desirable  properties by using some additional  possibly secret  bits k. to decrypt we use another function that maps back c into p using k  or its inverse . 
　the important property of the encryption algorithm is that security of the algorithm must reside in the  secret  key. if one does not know k  it must be difficult to recover p from c  even if the algorithm has been public for years. in the ideal case  the only way to recover the plaintext must be by brute force  generate-and-test : try out all possible keys and see which yields an acceptable plaintext. the need to hinder brute force attacks has therefore generated hot debates on the minimum size of a key ischneicr  1 . 
　exhaustive search is not so impossible as it seems if one can use  and pay for  specialized hardware: last year a machine costing 1 usd broke the data encryption standard finding a 1 bits key in 1 hours i des search  1a . 
search can be cut down if the cryptanalyst knows a suf-

ficient number of blocks of plaintext with the corresponding ciphertext  known plaintext attack . this is a reasonable hypothesis: almost all messages and files have fixed parts. using a network of 1 workstation and 1  randomly generated  plaintexts  matsui  1a  broke des in 1 days. 
　as the reader might now want to know how des works  we start by saying that des is a block cipher  which encipher blocks  sequences  of 1 bits into blocks of 1 bits using a key of 1 bits'. des and almost all symmetric ciphers are built following an architecture which is due to feistel and his group  feistel et al.  1 . after some initial preprocessing  the following operations are executed: 
1. break the plaintext in two halves  
1. combine one half with the key using a clever function  
1. xor the combination with the other half 
1. swap the two parts. 
these 1 operations constitutes a round and are repeated a suitable number of times. figure 1 exemplifies the idea. 
　des has 1 rounds which are almost identical except for the way in which the key is fed into the / function  fig. 1 : for each round a different subset of the 1 keybits is selected and combined with the input of the previous round. the strength of the cipher depends on the number of rounds and on /. its design is  to quote ron rivest   part art  part science . 
　as we have mentioned already  the basic way to break des is by exhaustive search but there are other techniques. 
differential cryptanalysis was introduced by biham and 
shamir . it assumes that the cryptanalyst can choose ciphertext and plaintext pairs presenting particular fixed differences. then  it analyzes the evolution of these differences through the rounds of des. using the differences resulting from ciphertexts  different probabilities are assigned to different keys. by analyzing a large number of ciphertext and plaintext pairs  1 for the commercial version   a key will emerge as the most probable. this attack is only practical for less than 1 rounds. after that  it requires too many resources. 
   'the key is usually expressed as a 1 bits number  in which every eighth bit is a parity bit ignored by the algorithm. 
	massacci 	1 
　
　matsui's linear cryptanalysis  matsui  1a; 1b  works better. this method uses linear approximations  xor  to describe the behavior of the round function /  fig. 1 . by xoring together some plaintext bits with some ciphertext bits  one can get a bit that is the xor of some key bits. this is a linear approximation of a round that is true with a certain probability. again  by analyzing a large number of plain/ciphertext pairs  1 are needed for des at 1 rounds   it is possible to guess the value of some key bits with 1% success rate. a refinement of this method approximates the internal 1-round and then guesses the results of the first and last round. it can find 1 keybits and uses exhaustive search for the rest. 
　a key aspect of cryptanalytic attacks  beside brute force  is that they are probabilistic. no deterministic method is known. 
1 	des as a sat problem 
recently  an encoding of the u.s. data encryption standard in propositional logic has been proposed in  marraro & massacci  1 . before discussing how the encoding can be used to generate random problems  we sketch its functioning: 
  each bit of the ciphertext  the plaintext  and the key is encoded as a propositional variable; 
  the operations corresponding to the round function /  fig. 1  are transformed into boolean formulae and minimized off-line with cad tools; 
  then the encoding algorithm  runs  des at the meta-level  and generates formulae corresponding to each des operation on the way; 
  since the straightforward application of this method would generate a huge formula  clever optimizations are used so that some operations are encoded as formulae and some operations are computed. 
for instance  operations corresponding to permutations of bits are not encoded as formulae; rather the propositional variables describing the inputs are permuted. further details can be found in  marraro & massacci  1 . 
　the outcome  of the algorithm is a formula   p k c  which represent the logical relations between the key bits k  the plaintext bits p and the ciphertext bits c. 
　in a traditional plaintext attack we know the value of some plaintext and ciphertext bits so  if we replace the variables by the corresponding truth value  we have a formula whose structure is shown in fig. 1. the k{ are the key bits while the other variables  are introduced to denote intermediate results and make the formula simpler. we use the superscripts r to denote the results produced at the r-th round and the subscript i to the denote the i-th bit produced at corresponding intermediate stage  i ranges from 1 to 1 . loosely speaking  and looking at fig. i  we may say that each x represents an output of the r-th round and thus an input of the r + 1-th round of the algorithm. the value lastr is the number of rounds of des for which the encoding has been done. the actual formulae have more definitions to ease the subsequent  polynomial  translation into cnf. 
     1 the algorithm in  marraro & massacci  1 takes less than 1 sec  1rounds  up to 1 seconds  1 rounds  to generate the encoding. memory requires a peak of 1m for the full 1 rounds. 
1 	challenge papers 


　table 1  taken from  marraro & massacci  1   shows some quantitative data for the encoding of a single pair constituted by a known block of plaintext and a block of cipher text  for an increasing number of rounds  r . 
　for random formulae  the ratio of c/v is an indicator of the hardness of the formula. in this case  it is not so. for instance  using the data of table 1 for 1 rounds or more  we can sec that if we use one block or an  infinite  number of blocks  the value of c/v changes by less than 1%. this would seem to imply that adding more blocks should not make the problem neither much easier nor much harder. as we shall see  the experimental results contradict this hypothesis. 
　in the introduction  it has been claimed that this encoding can be used to combine the contrasting needs of using  real-world  problems  possibly with lot of structure  and of generating a huge number of instances which can only be  pseudo randomly generated. it might solve the dilemma pointed out in  bayardo & schrag  1 : 
care must be taken when experimenting with real world instances because the number of instances available for experimentation is often limited 
whereas  crawford & auton  1  noted that 
　　 ...   random problems are readily available in any given size and virtually inexhaustible numbers. for example  ...  their experiments  required several million problems and it is hard to imagine collecting that many problems any other way. 
　
　　how do we generate a random sat problem based on cryptography  at first we generate a random key  and a plaintext block vp  just vectors of 1 . then we use the cryptographic algorithm itself to get  vp . finally we substitute in  the corresponding boolean values vp and vc that we have so far generated. then the pair is a solved instance of the sat problem. notice that might contain other variables than 
k but the latter are the only independent variables. if we have n blocks of plaintext  and the corresponding ciphertext  we can constrain the search further by conjoining the corresponding formulae 
　so we have encoded cryptographic key search  a known plaintext attack to des  as a sat problem. since ciphers are designed to be hard to break  this will provide us with the hard solved instances asked for in  cook and mitchel  1 . we can generate generic instances by randomly generating both the plaintext and the ciphertext. 
　the main point here is that by changing the plaintext and the key we can generate an endless stream of different solved instances either with the same solution or with different solutions. at 1 rounds  it would be exactly identical to an actual plaintext and ciphertext used by a bank  financial institution or government department. 
1 walk-sat on des 
the first tested algorithm is a local search one: walk-sat  selman et a/.  1 . it is only briefly recalled: 
  the algorithm starts from a random assignment; 
  then it flips the value of some propositional variables  usually one  trying to increase the value of an objective function  here the number of satisfied clauses ; 
  when a local minimum is reached  the algorithm restart the search with another random assignment. 
variants of this basic approach include the possibility of making random moves from time to time and of continuing the search after a local minimum by using a tabu-list1. for more details see  selman et al.   1; 1 . 
　experiments were run on a pentium ii running linux  with 1mb  and a sun sparc running solaris  with 1m  with qualitatively and quantitatively similar results. to generate an instance we simply follow the recipe above: generate randomly a key  discarding weak keys  schneier  1   and then some hundred blocks of plaintext. for each plaintext block we generate the corresponding ciphertext and then substitute the value of the pair in the formula. an instance is finally created by conjoining the formulae corresponding to the desired number of plain/ciphertext pairs  or blocks . 
　the initial settings of walk-sat were the recommend standard: hill-climbing with some random perturbations. the performance improved by using a random flip every 1 moves 
1
for des we have instances if we consider the encryp-
tion of binary data. if we restrict ourselves to ascii plaintexts  the number of different plaintexts only shifts from 
　　1  a tabu-list is a list of variables which have just been flipped and which cannot be immediately re-flipped 

and the final result is reported in table 1. r denotes the number of rounds on which des has been limited and b the number of blocks which have been conjoined to produce the instance  to get the size of an instance  multiply the values of table 1 for the number of blocks . sec. is the average running time and kbits tells on average how many bits of the solution found by walk-sat coincided with the known solution. for unsuccessful attempts we also report the lowest number of unsatisfied clauses found. 
　walk-sat can crack des up to 1 rounds  and compares favorably with the results of sato and tableau reported in  marraro & massacci  1 . at three rounds walk-sat cannot crack any instance  even with a number of flips hundreds times the number of clauses and a few hundreds tries. moreover  adding more constraints  blocks  makes the search harder and not easier. 
why doesn't walk-sat solve the problem well  
　the first problem has been already pointed out in i selman et al  1 : the difficulty of local search algorithms to run around dependent variables. recall that here almost all variables are dependent. the dagsat approach proposed in  kautz et al  1  might prove to be more successful. 
　the second problem is the presence of wide  rugged  plateaus at the bottom of the search space: the number of unsatisfied clauses goes quickly down from thousands to few tens per block and stays there  with walk-sat flipping  in vain  a lot of dependent variables and moving from a local minima to the next. the lowest number of bad clauses was decreased by re-engineering walk-sat as follows: 
  the first time a local minimum is reached  its value is stored as a reference value and the search continues; 
  after the search visited n local minima with value higher than the reference value  all keybits were randomly flipped  with a probability 1/n- all variables were flipped ; 
  each time a lower minimum was reached  n was reset and that minimum considered the new reference value; 
the idea was to escape the plateaus by exploiting the domain knowledge that the keybits were the only independent variables. in this way  the search converges to a much lower value of bad clauses  usually from 1 bad clauses per block to less than 1   but we are still stuck there. 
	massacci 	1 
　

1 rel-sat on des 
the second algorithm is a systematic one: rel-sat from 
 bayardo & schrag  1. it is a variant of the davis-putnam algorithm  enhanced with conflict directed back-jumping and learning. it works as follows: 
  unit propagation is applied to the clause set; 
  if no contradiction is found a new literal is selected and added either positively or negatively to the clause set; 
  if a contradiction is found then the algorithm backtracks to the literal that caused the contradiction; 
  the clause responsible for the contradiction is resolved with a clause representing the temporary assignment; the resolvent is thus learned as a reason to avoid the corresponding assignment; 
  the procedure is iterated until all literals have been as-signed  sat  or no backtrack is possible  unsat . 
for more details see  bayardo & schrag  1 . 
　the instance generation method  the architecture  and operating systems were the same used for walk-sat. also in this case  the experiment started with the recommend standard: a small learning factor  1   using relevance-based learning. 
　up to 1 rounds  rel-sat cracks des only slightly faster than sato and tableau  see i marram & massaeci  1   or walk-sat. however  it is the c ly algorithm which cracks three round of des in less than ten minutes. its performance is reported in table 1. the success rate is omitted since either all instances could be solved or none could  - . 
　other settings were tried: no learning at all and learning factors larger than 1. the analysis shows that learning is essential if we have few constraints but it might be skipped if enough constraints are around  table 1 . an intuitive explanation could be that with few blocks the algorithm might split on dependent variables and then discover that this was not necessary. with many constraints  standard heuristics select almost only independent variables and therefore learning contribution to performance is diminished. 
　note that the performance of the algorithm improves with the number of blocks composing the instance. adding more 
1 	challenge papers 
constraints makes the search for the only     existing solution easier. this behavior is consistent with standard cryptographic techniques  biham & shamir  1; matsui  1a  where having more data improves the chances of success. 
　given this promising results  the algorithm has been engineered to accept larger formulae with more variables and tried on the full 1-round des  also using 1  1  and 1 blocks. 
the algorithm didn't return within 1 hours. 
　a small re-engineering of the algorithm was carried to exploit in a limited manner the knowledge of the domain. after a first selection of potential branching variables  a threshold is used in the original algorithm to reduce their number. the modified algorithm didn't check the threshold if the selected variable was a keybit. in this way the algorithm gives preferences to dependent variable with very good properties or independent variables with medium properties. however  the running time of the algorithm didn't improved substantially. 
1 lessons for sat solvers 
it is a promising result that sat solvers can crack a limited version of des without using any problem-dependent heuristics but this is not enough. we want to solve the full des. 
　the first temptation is then to dismiss the sat solvers themselves: this problem has a non-clausal structure  so it has to be expected that cnf provers perform badly; the right tool should have been bdds  bryant  1 . surprisingly  an extensive experimentation reported in i ascione  1  shows that the bdds cannot solve key search problems any better than sat-based approaches. 
　the second conclusion might be that the problem is too constrained: at three rounds there is almost only one solution. this makes the problem harder for local search  but should make it easier for complete algorithms. indeed  the very characteristics of des with its avalanche effect  all keybits should affect all ciphertext bits  and a flip in the plaintext should affect all ciphertext bits  etc.  should make this problem easy: if the value of few keybits is wrongly chosen  a cascade of unit propagations should immediately generate an inconsistency. this would implies that formulae encoding more rounds  and more defined variables  should be easier and not harder. since this is not the case  it seems that with more rounds unit propagation is somehow hindered. 
　finding an explanation  and a workaround  for this difficulty is important because the structure of the encoding is common in many hard real problems: the natural formulation of a problem is usually structured in layers  makes use of abbreviations and definitions  and often contains modulo1 arithmetics  xors . for instance see the parity bit problem mentioned in  selman et al  1  and the ifip benchmark for hardware verification. 
　if we look again at the structure of the encoding  we may notice that each round is separated by the next round by a level of xors and that most constraints are in form of xors: a large subpart of the problem is an affine problem  which should be polynomially solvable by schaefcr's theorem. it is precisely this affine subproblem that make the problem hard for current ai techniques. look again at table 1: the problem becomes difficult as soon as xors start to appear. 
　
　in contrast  cryptographic techniques exploits this affine subproblem and even approximate the whole problem into an affine problem  matsui  1a . therefore  to crack dbs or similar real-word problem a sat solver needs the ability to solve affine subproblems. 
1 	conclusions and future challenges 
in this paper we have seen an application of propositional reasoning and search to a key security problem of industrial relevance. we have also discussed how this approach can optimally provide  real-world  problems where many instances can be randomly generated. 
   thus we believe that the whole approach on encoding cryptographic key search as propositional search can be a good answer to the final challenge proposed in  selman et al  1 : 
develop a generator for problem instances that have computational properties that are more similar to real world instances. 
　moreover  the preliminary tests on using sat-solvers to crack the data encryption standard are promising  although sat-solvers must be improved to meet the full challenge provided by this benchmark. thus  a good conclusion of this paper may just be the indication of the future challenges. they are listed in order of feasibility. 
　the first challenge is to find a key for the commercial 1 rounds data encryption standard in less than 1 hours using off-the-shelf h/w and s/w but specialized search heuristics. this might be the simplest and immediately rewarding challenge  assuming that the 1 usd prize of rsa security for breaking its dbs challenges will be there in the year 1. 
　then we may wish to design sat-solvers that work with every feistel-type cipher with data independent rounds like dbs. if we were able to cope with affine subproblems this would not be a big extension. since the operations are data independent  a certain amount of preprocessing for the internal rounds could be done off-line. 
　the third challenge is to find efficient encodings into sat of data-dependent feistel-type ciphers like rc1  schneicr  1. a straightforward encoding is always possible: just translate the cipher into a circuit and this into propositional logic. unfortunately this is already unworkable for dbs. 
　last but not least  we may wish to find efficient encodings into propositional  or any  logic of public-key algorithms such as rsa. this challenge  firstly proposed in i cook and mitchel  1  might prove to be the hardest since number theory is fairly remote from propositional logic. 
　as for all real problems  there might also be a darker side: the final measure of success might well be the  privilege   !   of successful sat algorithms being denied export licenses as dangerous weapons. 
