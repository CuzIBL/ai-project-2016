
this paper deals with learning to classify by using an approximation of the analogical proportion between four objects. these objects are described by binary and nominal attributes. firstly  the paper recalls what is an analogical proportion between four objects  then it introduces a measure called  analogical dissimilarity   reflecting how close four objects are from being in an analogical proportion. secondly  it presents an analogical instance-based learning method and describes a fast algorithm. thirdly  a technique to assign a set of weights to the attributes of the objects is given: a weight is chosen according to the type of the analogical proportion involved. the weights are obtained from the learning sample. then  some results of the method are presented. they compare favorably to standard classification techniques on six benchmarks. finally  the relevance and complexity of the method are discussed.
keywords : instance-based learning  learning by analogy  analogical proportion  analogical dissimilarity.
1 introduction
the aim of this article is to present a new non-parametric classification rule for objects described by binary and nominal attributes. some definitions and original algorithms will be presented to explain this method  and results will be given. the principle of non-parametric  or instance-based  learning is to keep the whole learning set as a base for the decision instead of inducing an explicit classification concept. the best known and simplest instance-based classification rule is the k-nearest neighbors  k-nn  rule  which requires only a metric space. this rule  in its simplest form  computes the distance between the object to classify and all the instances in the learning set and considers the k nearest instances as electors of their class  duda et al.  1 . the class of the new object is the class which has the majority given by the k electors.
learning by analogy  as we present it here  is also an instancebased technique which uses the concept of analogical dissimilarity between four objects  three in the training set  the fourth being the object to classify . one property of the analogical dissimilarity between four objects is that it is null when the objects are in analogical proportion. let us take a toy example to explain what is an analogical proportion and how to learn with it. let tomcat be the object to classify into feline or ruminant. suppose that there are only three instances in the training set: calf  bull and kitten. the animals are described by four binary attributes  1 stands for true and 1 for false  which are has claws hc   is an adult ia   is a male im   feeds by suckling fs .
animalshciaimfsclasscalf11ruminantbull11ruminantkitten11felinetomcat11 for each attribute  we notice that there is an analogical proportion between the three objects  taken in this order  from the training set and the object tomcat  since the four binary values have one of the forms below  we will see later what are all the possible analogical proportions between binary values .
	1 is to 1 as 1 is to 1	 hc 
1 is to 1 as 1 is to 1	 ia and im 
1 is to 1 as 1 is to 1	 fs 
thus  the four objects are in analogical proportion since all their binary attributes are in analogical proportion :
calf is to bull as kitten is to tomcat
we have to emphasize two points here. firstly  taking these three objects  triplet  in a differentorderwill not produceanymore an analogical proportion with tomcat  except when bull and kitten are exchanged. secondly  to make full use of the training set  we must consider also the triplets containing two or three identical objects. consequently  a training set of m objects produces m1 different triplets. in this example  it is easy to verify that only two triplets among the 1 are in analogical proportion with tomcat.
for the triplet  calf  bull  kitten   the resolution of an analogical equation on the classes gives the class of tomcat:
   ruminant is to ruminant as feline is to class tomcat  which solves in : class tomcat  = feline.
the second triplet  calf  kitten  bull  in analogy with tomcat gives the same result. hence  the classification by analogy of tomcat on this learning set is feline.
notice that the 1-nearest neighbor technique  using the hamming distance  concludes in this example that class tomcat  = ruminant. the strength of the analogical classifier stems from the fact that it can use objects in the training set belonging to some class ¦Ø1 to conclude that the unknown class object is ¦Ø1. in terms of distance  an analogical classifier can give importance to objects that are far from the object to be classified. it actually makes a different use of the information in the training set than distance classifiers or decision trees  because it uses a  restricted  form of analogical reasoning  gentner et al.  1 . is the use of analogical proportions actually relevant for classification  the goal of this article is to investigatein this direction  for data sets described by binary and nominal data.
¡¡in section 1  we give a formal definition of the notion of analogical proportion and we introduce that of analogical dissimilarity. in section 1  we present a naive algorithm of learning a classification rule followed by a faster version  which makes use of the properties of analogical dissimilarity. in section 1  we propose to learn from the set of instances three different weights for each attribute  and to use one of them to modify the computation of the analogical dissimilarity  a weight is chosen among the three according to the type of analogical proportion which is involved . we give results on eight data bases from the uci ml repository  newman et al.  1  and compare with various standard classification algorithms in sect. 1. finally  section 1 discusses these results and the complexity of the algorithm.
1 analogical proportion and analogical dissimilarity
1 analogical proportion between four binary objects
generally speaking  an analogical proportion on a set x is a relation between four elements of x  i.e. a subset of x1  which writes a : b :: c : d and reads:
a is to b as c is to d
the elements a  b  c and d are said to be in analogical proportion  delhay and miclet  1 . as defined by lepage  lepage and ando  1  lepage  1   an analogical proportion	a : b :: c : d implies two other analogical proportions:
	symmetry of the  as  relation:	c : d :: a : b
           exchange of the means: a : c :: b : d and the following property is also required :
determinism: if a : a :: b : x then x = b
from the first two properties one can deduce five more equivalent analogical proportions  which gives eight equivalent ways to write that the objects a  b  c and d are in analogical
proportion :c : a :: d : bc : d :: a : bd : b :: c : aa : c :: b : dd : c :: b : aa : b :: c : db : a :: d : c b : d :: a : cwhen x is the boolean set b = {1} there are only six 1tuples in analogicalproportionamong the sixteen possibilities  and two special cases  a  and  b  in the other ten  as we shall see in the next paragraph :
in analogicalnot in analogicalproportionproportion1 : 1 :: 1 : 1 : 1 :: 1 :1 : 1 :: 1 : 1 : 1 :: 1 :1 : 1 :: 1 : 1 : 1 :: 1 :1 : 1 :: 1 : 1 : 1 :: 1 :1 : 1 :: 1 : 1 : 1 :: 1 :1 : 1 :: 1 : 1 : 1 :: 1 :11 : 1 :: 1 :11 : 1 :: 1 :11 : 1 :: 1 :11 : 1 :: 1 :1 a 
 b 
now  if we take objects described by m binary attributes  we can easily construct an analogical proportion on bm.
definition 1 four objects in bm are in analogicalproportion if and only if all their attributes are in analogical proportion:
	a : b :: c : d   aj : bj :: cj : dj	  1 ¡Ü j ¡Ü m
with this definition  it is straightforward to verify that the above properties of analogical proportion are still verified.
1 analogical dissimilarity between binary objects
up to now  four objects are either in analogical proportion or not. here we introduce a new notion to measure  when they are not in analogical proportion  how far are four objects from being in analogical proportion. this measure is called analogical dissimilarity  ad . another measure has already been presented in the literature  for natural langage processing  in the framework of semantic analogy  turney  1  by considering similarity relations between two pairs of words.
in b  we give the value of 1 to every four-tuple which is not in analogical proportion  except for  a  and  b   which take the value of 1. 1 and 1 are the number of binary values to switch to produce an analogical proportion. this definition leads to the following properties:

in bm  we define the analogical dissimilarity ad a b c d  as  and the four properties above still hold true  miclet and delhay  1 .
	h a  : h b  :: h c  : h x 	resolution
¦Ø1 :	¦Ø1 ::	¦Ø1 :	 	h x  = ¦Ø1 ¦Ø1 :	¦Ø1 ::	¦Ø1 :	 
¦Ø1 :	¦Ø1 ::	¦Ø1 :	 	h x  = ¦Ø1 ¦Ø1 :	¦Ø1 ::	¦Ø1 :	 
table 1: possible configurations of a triplets
1 extension to nominal attributes
to cope with nominal data  two approaches are possible:
  the first one  one-per-value encoding  consists in splitting the nominal attribute. as a result  an n valued nominal attribute is replaced by n binary attributes with exactly one at 1.
  the second approach consists in keeping the nominal attribute as a single attribute. this requires to define an analogical proportion on the values of the attribute.
the second approach can be used when there is some knowledge aboutthe nominalvalues  like an order relationor a measure of distance between the values. since it is not the case in the data sets that we have worked on  we have chosen to use the one-per-value encoding to treat the nominal attributes.
1 a classification rule by analogical dissimilarity
1 principle
 be a learning set  where
h oi  is the class of the object oi. the objects are defined by binary attributes. let x be an object not in s. the learning problem is to find the class of x  using the learning set s. to do this  we define a learning rule based on the concept of analogical dissimilarity depending on an integer k  which could be called the k least dissimilar triplets rule. it consists of the following steps:
1. compute the analogical dissimilarity between x and all the n triplets in s which produce a solution for the class of x.
1. sort these n triplets by the increasing value of their ad when associated with x.
1. if the k-th object has the integer value p  then let k be the greatest integer such that the k-th object has the value p.
1. solve the k analogical equations on the label of the class. take the winner of the votes among the k results.
to explain  we firstly consider the case where there are only two classes ¦Ø1 and ¦Ø1. an example with 1 classes will follow. point 1 means that we retain only the triplets which have one of the four1 following configuration for their class  see table 1 . we ignore the triplets that do not lead to an equation with a trivial solution on classes :
h a  : h b  :: h c  : h x 

	¦Ø1 :	¦Ø1 ::	¦Ø1 :	 
	¦Ø1 :	¦Ø1 ::	¦Ø1 :	 

point 1 is comparable to the k nearest neighbors method. since ad takes integer values  there are in general several triplets for which ad = 1  ad = 1  etc. that is why point 1 increases the value of k to take into account all the triplets with the same value for ad. finally  point 1 uses the same voting technique as the k-nn rule.
multiclass problem: we proceed in the same way as in the two classes problem  keeping only the triplets which solve on the class label  for example:

analogical equations

with a solution

¦Ø1 : ¦Ø1 :: ¦Ø1 :     h x  = ¦Ø1 ¦Ø1 : ¦Ø1 :: ¦Ø1 :     h x  = ¦Ø1

with no solution

¦Ø1 : ¦Ø1 :: ¦Ø1 :  	  h x  =  ¦Ø1 : ¦Ø1 :: ¦Ø1 :  	  h x  = 
missing values problem: we consider a missing value as a value for which the distance to each valued nominal attribute is the same. hence  when splitting a nominal attribute  a missing value would take the value 1 in all the split binary attributes instead of taking the value 1 in exactly one of them  sect. 1 .
example
let s = { a ¦Ø1   b ¦Ø1   c ¦Ø1   d ¦Ø1   e ¦Ø1 } be a set of five labelled objects and let be some object to be classified. according to the analogical proportion axioms  there is only 1  =  card s 1 + card s 1 /1  nonequivalent analogical equations among 1 = card s 1  equations that can be formed between three objects from s and x. table  1  shows only the first 1 lines after sorting with regard to some arbitrarily analogical dissimilarity. the following table gives the classification of an object x according to k :
k1 1 1 1k1 1 1 1votes for x1 1     1o1 o1 o1h o1  h o1  h o1 h x adkbad¦Ø1¦Ø1¦Ø1¦Ø11bde¦Ø1¦Ø1¦Ø1¡Í1cde¦Ø1¦Ø1¦Ø1¦Ø11abd¦Ø1¦Ø1¦Ø1¦Ø11cae¦Ø1¦Ø1¦Ø1¡Í1dce¦Ø1¦Ø1¦Ø1¦Ø11dbc¦Ø1¦Ø1¦Ø1¦Ø11ace¦Ø1¦Ø1¦Ø1¡Í1acc¦Ø1¦Ø1¦Ø1¡Í1abe¦Ø1¦Ø1¦Ø1¦Ø11bae¦Ø1¦Ø1¦Ø1¦Ø11bcd¦Ø1¦Ø1¦Ø1¡Í1ccc¦Ø1¦Ø1¦Ø1¦Ø11aac¦Ø1¦Ø1¦Ø1¦Ø11...........................table 1: an example of classification by analogical dissimilarity. analogical proportions whose analogical resolution on classes have no solution  represented by ¡Í  are not taken into account. ad is short for ad o1 o1 o1 x .
we have the more computations we do off line and the less we do on line. to show its efficiency  we give on the spect data base  sect. 1  the performance of fadana*  the naive method would need 1 ad computations .
bp	1	1	1	1nad1 1 1 1bp is the number of base prototypes  and nad is the number of ad computations in the online part.
1 weighting the attributes for an analogical dissimilarity decision rule
the basic idea in weighting is that all the attributes do not have the same importance in the classification. the idea of selecting interesting attributes for analogy is not new. in  turney  1  a discrimination is also done by keeping the most frequent patterns in words. therefore  one should give greater importance to the attributes that are actually discriminant. however  in an analogical classification  there are several ways to find the class of the unknown element. let us take again the preceding two class problem example  1 . we notice that there is two ways to decide between the class ¦Ø1 and the class ¦Ø1  there is also a third possible configuration which is equivalent to the second by exchange of the means  sect. 1  . we therefore have to take into account the equation used to find the class. this is why we define a set of weights for each attribute  according to the number of classes. these sets are stored in an analogical weighting matrix  sect. 1 .
1 weighting matrix
definition 1 an analogical weighting matrix  w  is a three dimensional array. the first dimension is for the attributes  the second one is for the class of the first element in an analogical proportion and the third one is for the class of the last element in an analogical proportion. the analogical proportion weighting matrix is a d ¡Á c ¡Á c matrix  where d is the number of attributes and c is the number of classes.
¡¡for a given attribute ak of rank k  the element wkij of the matrix indicates which weight must be given to ak when encountered in an analogical proportion on classes whose first element is ¦Øi  and for which ¦Øj is computed as the solution.
hence  for the attribute ak:
last element  decision 
class ¦Øiclass ¦Øjclass ¦Øiwkiiwkijclass ¦Øjwkjiwkjjfirst element
since we only take into account the triplets that give a solution on the class decision  all the possible situations are of one of the three patterns:
possible patternsfirstdecisionelementclass¦Øi : ¦Øi :: ¦Øj :¦Øj¦Øi¦Øj¦Øi : ¦Øj :: ¦Øi :¦Øj¦Øi¦Øj¦Øi : ¦Øi :: ¦Øi :¦Øi¦Øi¦Øithis remark gives us a way to compute the values wkij from the learning set.
1 learning the weights from the training sample
the goal is now to fill the three dimensional analogical weighting matrix using the learning set. we estimate wkij by the frequency that the attribute k is in an analogical proportion with the first element class ¦Øi  and solves in class ¦Øj.
¡¡firstly  we tabulate the splitting of every attribute ak on the classes ¦Øi:
... class ¦Øi ...ak = 1...	n1i	...ak = 1...	n1i	...where ak is the attribute k and n1i  resp. n1i  is the number of objects in the class i that have the value 1  resp. 1  for the binary attribute k. hence 

m is the number of objects in the training set.
secondly  we compute wkij by estimating the probability to find a correct analogical proportion on attribute k with first element class ¦Øi which solves in class ¦Øj. in the following table we show all the possible ways of having an analogical proportion on the binary attribute k. 1i  resp. 1i  is the 1  resp. 1  value of the attribute k that has class ¦Øi.
1st1i : 1i :: 1j : 1j
1sd1i : 1i :: 1j : 1j
1rd1i : 1i :: 1j : 1j
1th1i : 1i :: 1j : 1j
1th1i : 1i :: 1j : 1j
1th1i : 1i :: 1j : 1j
pk 1st  estimates the probability that the first analogical proportion in the table above occurs.
	pk 1st 	=	n1in1in1jn1j/m1
...
from wkij = pk 1st  + ... + pk 1th   we compute

the decision algorithm  sect. 1  is only modified at point 1  which turns into weighted analogicalproportion classifier
 wapc :
  given x  find all the n triplets in s which can produce a solution for the class of x. for every triplet among these n  say  a b c   consider the class ¦Øi of the first element a and the class ¦Øj of the solution. compute the analogical dissimilarity between x and this triplet with the weighted ad:

otherwise  if point 1 is not modified  the method is called analogical proportion classifier  apc .
1 experiments and results
1 experiment protocol
we have applied the weighted analogical proportion classifier  wapc  to eight classical data bases  with binary and nominal attributes  of the uci repository  newman et al.  1 .
¡¡monk 1 and 1 problems  mo.1  mo.1 and mo.1   monk1 problem has noise added. spect heart data  sp. . balance-scale  b.s  and hayes roth  h.r  database  both multiclass database. breast-w  br.  and mushroom  mu.   both data sets contain missing values.
in order to measure the efficiency of wapc  we have applied some standard classifiers to the same databases  and we have also applied apc to point out the contribution of the weighting matrix  sect.1 . we give here the parameters used for the comparison method in table 1:
  decision table: the number of non improving decision tables to consider before abandoning the search is 1.
  id1: unpruned decision tree  no missing values allowed.
  part: partial c1 decision tree in each iteration and turns the  best  leaf into a rule  one-per-value encoding.
  multi layer perceptron: back propagation training  one-per-value encoding  one hidden layer with  number of classes+number of attributes /1 nodes.
  lmt  'logistic model trees' : classification trees with logistic regression functions at the leaves  one-per-value encoding.
  ib1: nearest-neighbor classifier with normalized euclidean distance.
  ibk  k=1 : k nearest-neighbor classifier with distance weighting  weight by 1/distance .
  ib1  k=1  guide  1 : k nearest-neighbors classifier with attributes weighting  similarity computed as weighted overlap relevanceweights computed with gain ratio.
1 results
we have worked with the weka package  witten and
frank  1  and with timbl  guide  1   choosing 1 various classification rules on the same data from weka and the last classifier from timbl. some are well fit to binary data  like id1  part  decision table. others  like ib1 or multilayer perceptron  are more adapted to numerical and noisy data. the results are given in table 1. we have arbitrarily taken k = 1 for our two rules. we draw the following conclusions from this preliminary comparative study: firstly  according to the good classification rate of wapc in br. and mu. databases  we can say that wapc handles the missing values well. secondly  wapc seems to belong to the best classifiers for the b.s and h.r databases  which confirms that wapc deals well with multiclass problems. thirdly  as shown by the good classification rate of wapc in the mo.1 problem  wapc handles well noisy data. finally  the results on mo. and b.s database are exactly the same with the weighted decision rule wapc than with apc. this is due to the fact that all ad that are computed up to k = 1 are of null value. but on the other data bases  the weighting is quite effective.
1 discussion and futurework
besides its original point of view  the weighted analogical proportion classifier seems  after these preliminary experiments  to belong to the best classifiers on binary and nominal data  at least for small training sets. the reason is that it can profit from regularities in alternated values in the classes in contrast to decision trees or metric methods which directly correlate the attributes with the classes. this means that an analogical proportion between four objects in the same class is reinforced when taking into account a new attribute with values  1 1  on the four objects but is decreased if the values are  1 1 .
¡¡we believe that there is still ample room for progress in this technique. in particular  we intend to investigate a more precise mode of weighting the attributes according to the type of analogical proportion in which they are involved  and also to test wapc with numerical attributes. obviously  more work has to be done on the computational aspects. even with the fadana* method  sect. 1   the decision process still takes too much time for realistic computation on large data sets.
methodsmo.1mo.1mo.1sp.b.sbr.h.rmu.number of nominal attributes1111number of binary attributes1111number of train instances1111number of test instances1111number of classes1111wapc  k = 1 1%1%1%1%1%1%1%1%apc  k = 1 1%1%1%1%1%1%1%1%decision table1%1%1%1%1%1%1%1%id1%1%1%1%1%mv1%mvpart1%1%1%1%1%1%1%1%multi layer perceptron1%1%1%1%1%1%1%1%lmt1%1%1%1%1%1%1%1%ib1%1%1%1%1%1%1%1%ibk  k = 1 1%1%1%1%1%1%1%1%ib1  k = 1 1%1%1%1%1%1%1%1%table 1: comparison table between wapc and other classical classifiers on eight data sets. the best classification rates ¡¡another interesting question is the limits of this type of classification. we already know how to define an analogical proportion and an analogical dissimilarity on numerical objects  and we have investigated on sequences of symbolic or numerical objects  delhay and miclet  1 . still  we do not know in which cases an analogical proportionbetween classes is related to that of objects that constitute the classes within a significance level = 1%  are in boldface.
in the learning sample. are there data for which an analogical proportion classification is  natural  and data for which it does not make sense 
1 conclusion
in this article  we have shown a new method for binary and nominal data classification. we use the definition of an analogical proportion between four objects. the technique of the classifier is based on the new notion of analogical dissimilarity and on the resolution of analogical equations on the class labels.
¡¡we also have shown the importance of weighting the attributes according to the type of analogical proportion in which they are involved. this has led us to good results in all situations  compared to off the shelf standard classifiers  on six classical data bases.
