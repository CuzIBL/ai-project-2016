filling knowledge gaps in a broad-coverage 
machine translation system* 
k e v i n k n i g h t   i s h w a r c h a n d e r   m a t t h e w h a i n e s   vasileios h a t z i v a s s i l o g l o u   
e d u a r d h o v y   m a s a y o l d a   s t e v e k 	l u k   r i c h a r d w h i t n e y   k e n j i y a m a d a 
usc/infonnation sciences institute 
1 admiralty way 
m a n n a del rey  ca 1 

a b s t r a c t 
knowledge-based machine translation  kbmt  techniques yield high quabty in domuoh with detailed semantic models  limited vocabulary  and controlled input grammar scaling up along these dimensions means acquiring large knowledge resources it also means behaving reasonably when definitive knowledge is not yet available this pa per describes how we can fill various k b m t know  edge gap*  often using robust statistical techniques we describe quantitative and qualitative results from japangloss  a broad-coverage japaneseenglish mt system 
1 	introduction 
knowledge-based machine translation  kbmt  techniques have led to high quality mt systems in circumscribed problem domains  nirenburg et al  1  with limited vocabulary and controlled input grammar  nyberg and mitamura  1  this high quality is delivered by algorithms and resources that permit some access to the meaning of texts but can kbmt be scaled up to unrestricted newspaper articles  we believe it can  provided we address two additional questions 
1 in constructing a kbmt system  how can we acquire knowledge resources  lexical  grammatical  conceptual  on a large scale1 
1 in applying a kbmt system  what do we do when 
definitive knowledge is missing1 
　there are many approaches to these questions our working hypotheses are that  1  a great deal of useful knowledge can be extracted from online dictionaries and text  and  1  statistical methods  properly integrated  can effectively fill knowledge gaps until better knowledge bases or linguistic theories arrive 
　when definitive knowledge is missing m a. kbmt system  we ral' this a knowledge gap a knowledge gap 
   'this work was snpported in part by the advanced research projects agency  order 1  contract mda1c-1  and by the department of defense vasdeios hatzi vassiloglou's address u department of computer science  columbia university  new york ny 1 all authors can be reached at lastnameqibi edu 
1 
may be an unknown word  a missing grammar rule  a missing piece of world knowledge  etc a system can be designed to respond to knowledge gaps in any number of ways it may signal an error it may make a default or random decision it may appeal to a human operator it may give up and turn over processing to a simpler  more robust mt program our strategy is to use programs  sometimes statistical ones  that can operate on more readily available data and effectively address par ticular classes of kbmt knowledge gaps this gives us robust throughput and better quality than that of default decisions it also gives us a platform on which to build and test knowledge bases that will product further improvements in quality 
　our research is driven in large part by the pracli cal problems we encountered while constructing japangloss  a japanese-eng ish newspaper mt system built at usc/isi japangloss is a year-old effort within the pangloss mt project  nirenburg and frederking 1  nmsu/crl et al  1   and it participated in the most recent arpa evaluation of mt quality  white and o'connell  1  
1 	system overview 
this section gives a brief tour of the japangloss system later sections address questions of knowledge gaps on a module-by-module basis 
　our design philosophy has been to chart a middle course between  know-nothing  statistical mt  brown el al   1  and what might be called  know-it-all  know edge-based mt  nirenburg et al  1  our goal has been to produce an mt system that  knows something  and has a place for each piece of new knowledge  whether it be lexical  grammatical  or conceptual the system is always running  and as more knowledge is added  performance improves 
　figure 1 showe the modules and knowledge resources of our current translator with the exceptions of juman and penman  all were constructed new for japangloss the system includes components typ ical of many kbmt systems a syntactic parser driven by a feature-based augmented context-free grammar  a semantic analyzer that turns parse trees into candidate interlinguas  a semantic ranker that prunes away meaningless interpretations  and a flexible generator for ren-


	knight  etal 	1 

dering interhngua expressions into english 
　because written japanese has no inter-word spacing  we added a word segmenter we also added a preparing  chunking  module that performs several lasks  1  dictionary-based word chunking   1  recognition of personal  corporate  and place names   1  number/date recognition  and  1  phrase chunking this last type of chunking inserts new strings into the input sentence  such as begii-kp and eid-ip the syntactic grammar is written to take advantage of these barriers  prohibiting the combination of partial constituents across phrase boundaries vp and s chunking allow us to process the very long sentences characteristic of newspaper text  and to reduce the number of syntactic analyses we also employ a limited inference module between semantic analysis and ranking one of the tasks of this module is to take topic-marked entities and insert them into particular semantic roles 
　the processing modules are themselves independent of particular natural languages-they are driven by language-specific knowledge bases the interhngua expressions are also language independent in theory  it should be possible to translate interhngua to and from any natural language in practice  our interhngua 1 sometimes close to the languages we are translating here is an example interhngua expression produced by our system 

　the source for this expression was a japanese sentence whose literal translation is something like as for new company  then is plan io establish in february 
　the tokens of the expression are drawn from our conceptual knowledge base  or ontology   called sensus sensus is a knowledge base skeleton of about 1 concepts and relations  see  knight and luk  1  for a description of how we created it from online resources note that the interhngua expression above includes a syntactic relation q-hod   quality-modification   while our goal is to replace such relations with real semantic ones  we realize that semantic analysis of adjectives and nominal compounds ib an extremely difficult problem  and so we are willing to pay a performance penalty in the near term 
　in addition to sensus  we have large syntactic lexical resources for japanese and english  roughly 1 stems for each   a large grammar of english inherited from the penman generation system  penman  1   a large semantic lexicon associating english words with any given sensus concept  and hand-built chunking and syntax rules for the analysis of japanese sample rules can be found in  knight el al  1  
　we have not achieved high throughput in our semantic analyzer  primarily because we have not yet completed a large-scale japanese semantic lexicon  l e   mapped tens of thousands of common japanese words onto conceptual expressions for progress on our manual and automatic attacks on this problem  see  okumura and hovy  1  and  knight and luk  1  so we have encountered our first knowledge gap-what to do when a sentence produces no semantic interpretations1 other gaps in elude incomplete dictionaries  incomplete grammar  incomplete target lexical specifications  and incomplete interlingual representations 
1 	incomplete semantic throughput 
while semantic resources are under construction  we want to be able to test our other modules in an end to-end setting  so we have built a shorter path from 
japanese to english called the glossing path 
　we made very minor changes to the semantic inter preter and rechristened it the glosser like the inter preter  the glosser also performs a bottom-up tree walk of the syntactic parse  assigning feature structures lo each node but instead of semantic feature structures it assigns glosses which are potential english translations 
encoded as feature structures a sample gloss looks like 
this 

in this structure  the 1  1  etc features represent sequentially ordered portions of the gloss all syntactic and glossing ambiguities are maintained in disjunctive  *1r*  feature expressions 
　we buijt three new knowledge resources for the glosser a bilingual dictionary  to assign glosses to leaves   a gloss grammar  to assign glosses to higher constituents  keying off syntactic structure   and an english morphological dictionary 
　the output of the glosser is a large english word lattice  stored as a state transition network the typical 

1 

network produced from a twenty-word input sentence has 1 states  1 transitions  and billions of paths any path through the network is a potential english translation  but some are clearly more reasonable than others again we have a knowledge gap  because we do not know which path to choose 
　fortunately  we can draw on the experience of speech recognition and statistical nlp to fill this gap we built a language model for the english language by estimating bigram and trigram probabilities from a large collection of 1 million words of wall street journal material 1 we smoothed these estimates according to class membership for proper names and numbers  and according to an extended version of the enhanced good turing method  church and gale  1  for the remaining words the resulting tables of conditional probabilities guide a statistical extractor  which applies a version of the n-best beam search algorithm  chow and schwartz  1  to identify an ordered set of  best  paths in the word lattice  l e   the set of english sentences that are most likely accordirg to our model  due to heavy computational and memory requirements  we have not yet completed the trigram version of our model but even when only bigrams are used  comparing the statistical extractor to random path extraction reveals the power of statistics to make reasonable decisions  e g 
 random extractor  
' 	planned economy ages is threadbare 	  
 bigraa extractor  
  	planned econoay tiaee axe old 	' 
　the statistical model of english gives us much better glosses a full description of our glossing system  including our use of feature unification  appears in  hatzivassiloglou and knight  1  interestingly  we originally built the statistical model to address knowledge gaps in generating english from interlingua  not glossing we return to these gaps  and the statistical model  in section 1 
1 	incomplete dictionaries 
lexical incompleteness causes problems for both semantics and glossing numbers and dates are typical unknown words  we take a. finite-state approach to recognizing and translating these another important class of unknown words comprises katakana loanwords and foreign proper names  which are represented in japanese with approximate phonetic transliterations for example  kunnton should be translated as clinton  and suttppaa moot a a as stepper motor the knowledge-based approach is to pack our dictionaries with every possible name and technical term  but this approach quickly leads to diminished returns to plug the rest of the gap  we have again applied statistical techniques  this time to 
　　1  available from the acl data collection initiative  cd rom 1 
english spelling given a japanese transliteration like kunnton  we seek an english-looking word likely to have 
been the source of the transliteration  english-looking  is defined in terms of common four-letter sequences to produce candidates  we use another statistical model for katakana translations  computed from an automatically aligned database of 1 katakana-english pairs this model tells us that japanese hi is sometimes a transliteration of english c  especially at the beginning or a word   sometimes of ck  especially at the end  sometimes of cu  etc   with associated probabilities another extractor program delivers a reasonable transliteration  for example  preferring clinton over kulccnin 
1 	incomplete grammatical analysis 
unlike spoken language  newspaper text is general!} grammatical however  it is frequently ungrammatical with respect to our knowledge resources at any given time newspaper sentences are very long  and every new text contains some unusual syntactic construction that we have not yet captured in our formal grammar we also encounter problems with non-standard punctuation as well as wrong segmentation  part-of-speech tagging  and phrase chunking for these reasons  we made an early decision to do all parsing  semantic interpretation  and glossing in a bottom-up fashion  dealing indepen dently with sentence fragments when need be but because japanese and english word orders are so different a fragmentary parse usually leads to a bad translation 
　one method of overcoming these difficulties is to ensure a full parse with statistical context-free parsing low probability rules like adv -  adj guarantee that a full parse will be returned  yamron et al  1  wi avoided this technique because wc wanted to keep our feature-based grammar  in order to have fine-tuned control over the structures we accept and assign inspired by  lavie  1   we turned to word skipping as a grammatical gap-plugger this approach seeks out the largest grammatical subset of words in a sentence  with the hope that the skipped words are peripheral to the core mean ing of the sentence  or perhaps simply strav punctuation marks  rather than port the lr-parsing-based techniques of  lavie  1  to our bottom-up chart parser we developed heuristic techniques for spotting suspicious words and dropping them our most general method is to automatically process large sets of parsed and unparsed sentences  looking for statistical differences be tween the two sets for example  if a part-of-speech bigram appears frequently in unparsed sentences  this is a clue that one of the two words  or both  should l e dropped we also use grammar-specific heuristics sucli as don't drop a single noun from a noun sequence  drop phrasal boundaries only in pairs  and onlj when the internal material is completely parsed   etc skipping raises our full-parse rate from 1% to 1% 
1 	incomplete semantic constraints 
this section and the next return to incompleteness in semantics kbmt systems usually model world knowledge as a collection of  hard  constraints that any m-
	knightetal 	1 

terlingua must satisfy unsatisfactory semantic candidates are pruned away  leaving a single sensible interpretation in many cases  however  semantic constraints are too strong  and all interpretations are ruled out preference semantics  wilks  1  was devised to handle just this problem within the kbmt tradition  role restrictions can be augmented with relaxeable to state ments  carlson and nirenburg  1   as in the agent of a say-event must be a person  or relaxable-to an organization 
　our approach has been to further soften the impact of semantic constraints we assign a score  between 1 and 1  to any interlingue fragment  as follows we first extract all stated relations between entities to each relation  we assign scores based on domain constraints and range constraints in the conceptual knowledge base there are five possible heuristic scores  depending on the suitability of the role filler satisfies basic constraint  1   satisfies relaxed constraint  but is not mutually disjoint from concepts satisfying basic constraint  1   satisfies relaxed constraint but is mutually disjoint from basic constraint  1   satisfies neither basic nor relaxed constraint  1   is mutually disjoint from concepts satisfying basic or relaxed constraint  1  scores for all of the relations are multiplied to yield an overall score 
no interlingua expression is ever assigned a zero score 
　this approach is like language modeling  but here the basic unit is the relational/conceptual n-gram  e g   
 eat  patient  worm  rather than the word n-gram and our scoring is based on hand-built inherited constraints rather than data still  we can view the conceptual knowledge base as a device that assigns a pri on probabilities to tnterlingua fragments  in analogy to how our language model assigns probabilities to english strings 
　this analogy makes it possible to describe kbmt in a statistical framework direct statistical mt systems  brown et al  1  use a noisy channel model in which a human is assumed to be speaking english  but the signal is corrupted  and out comes japanese bayes' theorem is used to retrieve the original  uncorrupted signal 

　that is  the best english translation is the sentence e that maximizes the a priori probability of e times the probability that if e were the original signal  it would have been corrupted into the japanese sentence j estimating the probability distributions p e  and p j | e  allows us to rank translations 
　a noisy channel model of kbmt adopts a different model of a human  one in which he hears english  but by the time it gets into his head  it has been corrupted into interlingua when he speaks  the interlingua signal is further corrupted into japanese we can model this process statistically as 

　now there axe four probability distributions to estimate  one of which is p i   exactly the a priori probability of an interlingua expression described above 
　while the full statistical model of kbmt is not used in japangloss  it is useful to view what we do  and don't do  in this light for example  p i | e  suggests a mode  of english generation that explicitly shies away from ambiguous e's  because they spread probabilities thinly across several i's for example  if i is reit-to  speaker house.'    it is better to say i'm renting my house to someone rather than the correct but ambiguous i'm renting my house most language generation systems focus on accurate renditions rather than unambiguous ones  although as pereira and grosz  1  p 1  remark  this is changing 
 the coextensiveness of natural language perceivers and producers both enables and rrquires the language generator lo reason about the generated language in reflective terms 'how would i react if i heard what i am about to say1' this reflective as pect of language genejatjon is essential at all levels of generation   
1 	i n c o m p l e t e i n t e r l i n g u a e x p r e s s i o n s 
even with full semantic throughput and accurate ranking of interlingua candidates  a full account of text meaning is beyond the state of the art it is easy to record verb tenses  but difficult to make the inferences required to lav out 1tated  and unstated  events on a time line if the bource language is japanese  we have additional problems no articles  a  an  the   no overt singular or plural marks  no agreement constraints  omitted subjects  no marked future tense  and so on in kbmt  we need some semantic representation of these things in order to generate languages like english this leads researchers to envision microtheones  nmsu/crl et al  1s  of time  space  reference  and bo on many such microtheones are not yet available  however  so we have another knowledge gap to fill 
　we can focus on the problem of definite and indefinite reference  which manifests itself as article selection  a  an  the  in japanese-english mt rather than handle this problem during semantics  we postpone the solution-not only until english generation  but until after generation  in an automatic postediting step our posteditor inserts articles into article-free english text  knight and chander  1  it was trained on 1 megabytes of english and performs with an accuracy of 1% the training was done with decision trees rather than n-grams  so that we could flexibly integrate long distance features that typically control the selection of articles human poet editors can achieve 1%  so we still have more features to explore viewing article selection as a postediting step  independent of the japanese source text  means that we can attack the problem statistically without the need for a large parallel corpus of japanese 

1 

and english our automatic posteditor has applications outside of mt  such as improving english text written by native japanese speakers  or chinese  russian  etc   

1 	incompleteness in generation 
we have encountered several classes of knowledge gaps in large-scale generation large-scale in our case means roughly 1 concepts/relations and 1 roots/phrases first  we must anticipate incompleteness in the input specification  as described in the last section second  lexical syntactic specification may be incomplete-does verb v take a nominal direct object or an infinitival complement1 third  collocations may be missing knowledge of collocations has been successfully used in the past to increase the fluency of generated text  smadja and mckeown  1  in particular  such knowledge can be crucial for selecting prepositions  on the field versus in the end zone  and other forms and fourth  dictionaries may not mark rare words and generally may not distinguish between near-synonyms we constantly strive to augment our knowledge bases and lexicons to improve generation  but we also want to plug the gap with something reasonable 
　our approach is one of bottom-up many-paths gen eration  knight and hatzivassiloglou  1   in analogy to bottom-up all-paths parsing if the generator cannot make an informed decision  lexical selection  complement type  spatial preposition  etc    it pursues all possibilities this is in contrast to many industrial strength generators  tomita and nyberg  1  penman  1  that never backtrack and usually default their difficult choices other generators  elhadad  1  do backtrack but still use default or random schemes for making new 
selections 
   our generator packs all paths into efficient word lattices as it goes  and trie fina  output ts a/so a word lattice because the generator produces the same data structure as the glosser  we can select a final output using the same extractor and language model described in section 1 this approach combines the strengths of knowledge-based generation  c g   generally grammatical lattice paths  long-distance agreement  parallel conjoined expressions  with statistical modeling  eg  local dependencies  lexical constraints  common words and collocations  as in section 1  we can compare random path extraction with n-gram extraction 
 random extractor  
 the ilea companies will have as a purpose launching at february   
 bigxaa extractor  
 the lie* coapany plane to establish in february ' 
　as the generator becomes more knowledgeable  its output lattices become leaner  and dependence on automatic statistical selection is reduced however  the statistical component is still useful as long as uncertainty remains  and improvements in language modeling will continue to have a big effect on overall performance 
1 	c o n c l u s i o n 
we have reported progress on aspects of the japangloss newspaper mt system in particular  we have described the integration of statistical and heuristic methods into a kbmt system while these methods are not a panacea  they offer a way to fill knowledge gaps until better knowledge bases become available furthermore  they offer a way of rationally prioritizing manual tasks if a statistical method solves your problem 1% of the time  you may not want to invest in a knowledge base in other cases  statistics may offer only a small benefit over random or default choices  then  a. more careful analysis is called for 
acknowledgments 
we would hke to thank yolanda gil and the ijcai reviewers for helpful comments on a draft of this paper 
a 	j a p a n g l o s s m t o u t p u t 
a 1 	interhngua-based sample 
citizen hatch announced on the eighteenth to establish a j o i n t venture s i t h perkin elmer co a major microcireuit manufacturing device manufacturer  and to start the production of the l i c r o c i x c u i t lanuiacturing device 
the new coapany plane a launching in february 
   the subsidiary of perk in elaer co in japan bears a majority of the stock  and the production of the dry etching device that is used for the manufacturing of the microcircuit chip and the stepper is planned 
a 1 	glosser based sample 
the  gun possession penalties important-national police agency plans of gun sword legal reform 
　the national police agency defend policies that change some of gun esord law that put the brake* on the p r o l i f e r a t i o n of the 1st gun change plans control recovery plan of the wrong owning step currency-make the decision by the cabinet meeting of the middle of this month in three pieces support estimate of the f i l i n g in t h i s parliament 

 random extractor  
 a subsidiary of the perkin elaer co on the japan bears majority of the stock ' 
 blgrem extractor  
 a subsidiary of parkin elaer co in japan bears a majority of the stock ' 