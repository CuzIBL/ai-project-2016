
we consider the problem of policy optimization for a resource-limited agent with multiple timedependent objectives  represented as an mdp with multiple discount factors in the objective function and constraints. we show that limiting search to stationary deterministic policies  coupled with a novel problem reduction to mixed integer programming  yields an algorithm for finding such policies that is computationally feasible  where no such algorithm has heretofore been identified. in the simpler case where the constrained mdp has a single discount factor  our technique provides a new way for finding an optimal deterministic policy  where previous methods could only find randomized policies. we analyze the properties of our approach and describe implementation results.
1 introduction
markov decision processes  bellman  1  provide a simple and elegant framework for constructing optimal policies for agents in stochastic environments. the classical mdp formulations usually maximize a measure of the aggregate reward received by the agent. for instance  in widely-used discounted mdps  the objective is to maximize the expected value of a sum of exponentially discounted scalar rewards received by the agent. such mdps have a number of very nice properties: they are subject to the principle of local optimality  according to which the optimal action for a state is independent of the choice of actions for other states  and the optimal policies for such mdps are stationary  deterministic  and do not depend on the initial state of the system. these properties translate into very efficient dynamic-programming algorithms for constructing optimal policies for such mdps  e.g.   puterman  1    and policies that are easy to implement in standard agent architectures.

this material is based upon work supported by honeywell international  and by the darpa/ipto coordinators program and the air force research laboratory under contract no. fa1-c- 1. the views and conclusions contained in this document are those of the authors  and should not be interpreted as representing the official policies  either expressed or implied  of the defense advanced research projects agency or the u.s. government.¡¡however  there are numerous domains where the classical mdp model proves inadequate  because it can be very difficult to fold all the relevant feedback from the environment  i.e.  rewards the agent receives and costs it incurs  into a single scalar reward function. in particular  the agent's actions  in addition to producing rewards  might also incur costs that might be measured very differently from the rewards  making it hard or impossible to express both on the same scale. for example  a natural problem for a delivery agent is to maximize aggregate reward for making deliveries  subject to constraints on the total time spent en route. problems naturally modeled as constrained mdps also often arise in other domains: for example  in telecommunication applications  e.g.   lazar  1    where it is desirable to maximize throughput subject to delay constraints.
¡¡another situation where the classical mdp model is not expressive enough is where an agent receives multiple reward streams and incurs multiple costs  each with a different discount factor. for example  the delivery agent could face a rush-hour situation where the rewards for making deliveries decrease as a function of time  same delivery action produces lower reward if it is executed at a later time   while the traffic conditions improve with time  same delivery action can be executed faster at a later time . if the rewards decrease and traffic conditions improve on different time scales  the problem can be naturally modeled with two discount factors  allowing the agent to evaluate the tradeoffs between early and late delivery. problems with multiple discount factors also frequently arise in other domains: for example  an agent can be involved in several financial ventures with different risk levels and time scales  where a model with multiple discount factors would allow the decision maker to quantitatively weigh the tradeoffs between shorter- and longer-term investments. feinberg and shwartz  describe more examples and provide further justification for constrained models with several discount factors.
¡¡the price we have to pay for extending the classical model by introducing constraints and several discount factors is that stationary deterministic policies are no longer guaranteed to be optimal  feinberg and shwartz  1; 1 . searching for an optimal policy in the larger class of non-stationary randomized policies can dramatically increase problem complexity; in fact  the complexity of finding optimal policies for this broad class of constrained mdps with multiple costs  rewards  and discount factors is not known  and no solution algorithms exist  aside from some very special cases  feinberg and shwartz  1  . furthermore  even if they could be found  these non-stationary randomized policies might not be reliably executable by basic agent architectures. for example   paruchuri et al.  1  described how executing randomized policies in multiagent systems can be problematic.
¡¡in this paper  therefore  we focus on finding optimal stationary deterministic policies for mdps with multiple rewards  costs  and discount factors. this problem has been studied before and has been proven to be np-complete by feinberg   who formulated it as a non-linear and nonconvex mathematical program. unfortunately  aside from intractable techniques of general non-convex optimization  these problems have heretofore not been practically solvable.
¡¡our contribution in this paper is to present an approach to solving this problem that reduces it to a mixed-integer linear program - a formulation that  while still np-complete  has available a wide variety of very efficient solution algorithms and tools that make it practical to often find optimal stationary deterministic policies. as we will show  moreover  our approach can also be fruitfully employed for the subclass of mdps that have multiple costs  but only a single reward function and discount factor. for these problems  linear programming can  in polynomial time  find optimal stationary randomized policies  kallenberg  1; heyman and sobel  1   but the problem of finding optimal stationary deterministic policies is np-complete  feinberg  1   with no implementable solution algorithms existing previously  aside from the general non-linear optimization techniques . we show that our integer-programming-based approach finds optimal stationary deterministic policies  which can then be compared empirically to optimal randomized policies.
¡¡in the remainder of this paper  we first  in section 1  establish a baseline by briefly reviewing techniques for solving unconstrained mdps. in section 1  we move on to constrained mdps  and present our approach to solving constrained mdps with a single reward  multiple costs  and one discount factor for the rewards and costs. we next expand this to the case with multiple rewards and costs  and several discount factors  in section 1. section 1 provides some empirical evaluations and observations  and section 1 discusses our results and some thoughts about applying the same techniques to other flavors of constrained mdps.
1 background: unconstrained mdps
an unconstrained  stationary  discrete-time  fully-observable mdp can be defined as a 1-tuple hs a p ri  where s = {i} is a finite set of states the agent can be in; a = {a} is a finite set of actions the agent can execute; p : s ¡Á a ¡Á s 1¡ú  1  is the stochastic  pj piaj = 1  transition function  piaj is the probability the agent goes to state j if it executes action a in state i ; r : s ¡Á a 1¡ú r is the bounded reward function  the agent gets a reward of ria for executing action a in state i .
¡¡a solution to an mdp is a policy  a procedure for selecting an action in every state  that maximizes some measure of aggregate reward. in this paper we will focus on mdps with the total expected discounted reward optimization criterion  but our results can be extended to other optimization criteria  as discussed in section 1 . a policy is said to be markovian  or history-independent  if the choice of action does not depend on the history of states and actions encountered in the past  but only on the current state and time. if  in addition to that  the policy does not depend on time  it is called stationary  by definition  a stationary policy is always markovian . a deterministic policy always prescribes the execution of the same action in a state  while a randomized policy chooses actions according to a probability distribution.
¡¡a stationary randomized policy ¦Ð can be described as a mapping of states to probability distributions over actions: ¦Ð : s ¡Á a 1¡ú  1   where ¦Ðia defines the probability that the agent will execute action a when it encounters state i. a deterministic policy can be viewed as a degenerate case of a
¡¡randomized policy for which there is only one action for each state that has a nonzero probability of being executed.
¡¡a policy ¦Ð and the initial conditions ¦Á : s 1¡ú  1  that specify the probability distribution over the state space at time 1  the agent starts in state i with probability ¦Ái  together determine the evolution of the system and the total expected discounted reward the agent will receive:
	 	 1 
where  i t  refers to the probability of being in state i at time t  and ¦Ã ¡Ê  1  is the discount factor.
¡¡it is well-known  e.g.   puterman  1   that  for an unconstrained mdp with the total expected discounted reward optimization criterion  there always exists an optimal policy ¦Ð  that is stationary  deterministic  and uniformly-optimal  where the latter term means that the policy is optimal for all initial probability distributions over the starting state  i.e. 
u¦Ã ¦Ð  ¦Á  ¡Ý u¦Ã ¦Ð ¦Á   ¦Ð ¦Á .
¡¡there are several standard ways of solving such mdps  e.g.   puterman  1  ; some use dynamic programming  value or policy iteration   others  which are much better suited for constrained problems  reduce mdps to linear programs  lps . a discounted mdp can be formulated as the following lp  d'epenoux  1; kallenberg  1   this maximization lp is the dual to the more-commonly used minimization lp in the value function coordinates :

the set of optimization variables xia is often called the occupation measure of a policy  where xia can be interpreted as the total expected discounted number of times action a is executed in state i. then  pa xia gives the total expected discounted flow through state i  and the constraints in the above lp can be interpreted as the conservation of flow through each of the states. an optimal policy can be computed from a solution to the above lp as:
	¦Ðia = xia/xxia.	 1 
a
although this appears to lead to randomized policies  in the absence of external constraints  and if we use strictly positive initial conditions  ¦Ái   1   a basic feasible solution to this lp always maps to a deterministic policy that is uniformlyoptimal  puterman  1; kallenberg  1 . this lp  1  for the unconstrained mdp serves as the basis for solving constrained mdps that we discuss next.
1 constrained mdps
suppose that the agent  besides getting rewards for executing actions  also incurs costs: ck : s ¡Á a 1¡ú r  k ¡Ê  1 k   whereis the cost of type k incurred for executing action a in state i  e.g.  actions might take time and consume energy  in which case we would say that there are two types of costs . then  a natural problem to pose is to maximize the expected discounted reward subject to some upper bounds on the total expected discounted costs. let us label the total expected discounted cost of type k ¡Ê  1 k  as:
¡Þ
	c¦Ãk ¦Ð ¦Á  = xx¦Ãt i t ¦Ðiackia.	 1 
t=1 i a
then  we can abstractly write the optimization problem with cost constraints as
	 	 1 
where ck is the upper bound on the cost of type k. if this probb
lem is feasible  then there always exists an optimal stationary policy  and it can be computed as a solution to the following lp  kallenberg  1; heyman and sobel  1 :
 
 1 
¡¡therefore  constrained mdps of this type can be solved in polynomial time  i.e.  adding constraints with the same discount factor does not increase the complexity of the mdp. however  due to the addition of constraints  the problem  1   in general  will not have uniformly-optimal policies. furthermore  the lp  1  will yield randomized policies  which  as argued in section 1  are often more difficult to implement than deterministic ones.
¡¡thus  it can be desirable to compute optimal solutions to  1  from the class of stationary deterministic policies. this  however  is a much harder problem: feinberg  studied this problem  showed that it is np-complete  using a reduction similar to  filar and krass  1    and reduced it to a mathematical program by augmenting  1  with the following constraint  ensuring that only one xia per state is nonzero:
	|xia   xia1| = xia + xia1	 1 
however  the resulting program  1  is neither linear nor convex  and thus presents significant computational challenges.
¡¡we show how  1  can be reduced to a mixed integer linear program  milp  that is equivalent to  1 . this is beneficial because milps are well-studied  e.g.   wolsey  1    and there exist efficient implemented algorithms for solving them. our reduction uses techniques similar to the ones employed in  dolgov and durfee  1b   where an mdp with limited non-consumable resources is reduced to a milp. the following proposition provides the basis for our reduction.
proposition 1 consider an mdp hs a p r ¦Ái  a policy ¦Ð  its corresponding occupation measure x  given ¦Á   a constant x ¡Ý xia  i ¡Ê s a ¡Ê a  and a set of binary variables  ia = {1}   i ¡Ê s a ¡Ê a.
if x and   satisfy the following conditions
 1 
 1 
then  for all states i that  under ¦Ð and ¦Á  have a nonzero probability of being visited  pa xia   1   ¦Ð is deterministic  and the following holds:
	 ia = 1   xia   1	 1 
proof: consider a state i  that  under policy ¦Ð and initial distribution ¦Á  has a nonzero probability of being visited  i.e.  pa xi a   1. then  since the occupation measure is nonnegative  there must be at least one action in this state that has a non-zero occupation measure:
	 a  ¡Ê a	s.t.	xi a    1.
then   1  forces  i a  = 1  which  due to  1   forces zero values for all other  's for state i :
	 i a = 1	 a 1= a .
given  1   this  in turn  means that the occupation measure for all other actions has to be zero:
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡xi a = 1	 a 1= a   which  per  1   translates into the fact that the policy ¦Ð is deterministic and  ia = 1   xia   1. 
¡¡proposition 1 immediately leads to the following milp whose solution yields optimal stationary deterministic policies for  1 :
 
 1 
where x can be computed in polynomial time by  for example  solving the lp  1  with the objective function replaced by maxpi a xia and setting x to its maximum value.
¡¡the above reduction to an milp is most valuable for domains where it is difficult to implement a randomized stationary policy because of an agent's architectural limitations. it is also of interest for domains where such limitations are not present  as it can be used for evaluating the quality vs. implementation-difficulty tradeoffs between randomized and deterministic policies during the agent-design phase.
1 constrained mdps with multiple discounts
we now turn our attention to the more general case of mdps with multiple streams of rewards and costs  each with its own discount factor ¦Ãn  n ¡Ê  1 n . the total expected reward is a weighted sum of the n discounted reward streams:

where ¦Ân is the weight of the nth reward stream that is defined by the reward function rn : s¡Áa 1¡ú r. similarly  each of the k total expected costs is a weighted sum of n cost streams:

where ¦Âkn is the weight of the nth discounted stream of cost of type k  defined by the cost function ckn : s ¡Á a 1¡ú r.
¡¡notice that in this mdp with multiple discount factors  we have n reward functions and kn cost functions  unlike the constrained mdp from the previous section that had 1 and k reward and cost functions  respectively .
¡¡our goal is to maximize the total weighted discounted reward  subject to constraints on weighted discounted costs:
	.	 1 
¡¡feinberg and shwartz  1; 1  developed a general theory of constrained mdps with multiple discount factors and demonstrated that  in general  optimal policies are neither deterministic nor stationary. however  except for some special cases  feinberg and shwartz  1   there are no implementable algorithms for finding optimal policies for such problems. because of this  and given the complexity of implementing non-stationary randomized policies  even if we could find them   it is worthwhile to consider the problem of constructing optimal stationary deterministic policies for such mdps. feinberg  showed that finding optimal policies that belong to the class of stationary  randomized or deterministic  policies is an np-complete task. he also formulated the problem of finding optimal stationary policies as the following mathematical program  again  based on  eq. 1  :
 
                                              1  this program has an occupation measure xn for each discount factor ¦Ãn  n ¡Ê  1 n  and expresses the total reward and total costs as weighted linear functions of these occupation measures. the first set of constraints contains the conservation of flow constraints for each of the n occupation measures  and the third set of constraints ensures that all occupation measures map to the same policy  recall  1  .
¡¡as in the previous section  we can limit the search to deterministic policies by imposing the following additional constraint on the occupation measures in  1   feinberg  1 :
		 1 
¡¡because of the synchronization of the different occupation measures and the constraint that forces deterministic policies  this program  1  is non-linear and non-convex  and is thus very difficult to solve.
¡¡for finding optimal stationary deterministic policies  we present a reduction of the program  1  to a linear integer program that is equivalent to  1 . just like in the previous section  this reduction to an milp allows us to exploit a wide array of efficient solution techniques and tools. our reduction is based on the following proposition.
proposition 1 consider an mdp hs a p r ¦Ái with several discount factors ¦Ãn  n ¡Ê  1 n   a set of policies ¦Ðn  n ¡Ê  1 n  with their corresponding occupation measures xn  policy ¦Ðn and discount factor ¦Ãn define xn   a constant x ¡Ý xnia  n ¡Ê  1 n  i ¡Ê s a ¡Ê a  and a set of binary variables  ia = {1}.
if xn and   satisfy the following conditions
 1 
	 	 1 
then  the sets of reachable states defined by all occupation measures are the same  i.e.  in =
. furthermore  all ¦Ðn are deterministic on in  and ¦Ðian = ¦Ðian  n n1 ¡Ê  1 n .
¡¡proof: consider an initial state i   i.e.  ¦Ái    1 . following the argument of proposition 1  the policy for that state is deterministic:

this implies that all n occupation measures xn must prescribe the execution of the same deterministic action a  for state i   because all xnia are tied to the same  ia via  1 .
¡¡therefore  all occupation measures xn correspond to the same deterministic policy on the initial states i1 = {i : ¦Ái   1}. we can then expand this statement by induction to all reachable states. indeed  clearly the set of states i1 that are reachable from i1 in one step will be the same for all xn. then  by the same argument as above  all xn map to the same deterministic policy on i1  and so forth. 
¡¡it immediately follows from proposition 1 that the problem of finding optimal stationary deterministic policies for an mdp with weighted discounted rewards and constraints  1  can be formulated as the following milp:
 
 1 
where is a constant  as in proposition 1.

figure 1: value of deterministic and randomized policies  a ; solution time  b  and profile  c ; mdp with two discounts  d .¡¡although this milp produces policies that are only optimal in the class of stationary deterministic ones  at present time there are  to the best of our knowledge  no practical algorithms for finding optimal solutions from any larger policy class for constrained mdps with multiple discount factors.
1 experimental observations
we have implemented the milp algorithm for finding optimal stationary deterministic policies for constrained mdps and empirically evaluated it on a class of test problems. in the following discussion  we focus on the constrained mdps from section 1  because these problems are better studied  and the existing algorithms for finding optimal randomized policies can serve as benchmarks  whereas there are no alternative algorithms for finding policies that are optimal for general constrained mdps with multiple discount factors.
¡¡in our empirical analysis  we tried to answer the following questions: 1  how well do deterministic policies perform  compared to optimal randomized ones  and 1  what is the average-case complexity of the resulting milps. the answers to these questions are obviously domain-dependent  so the following discussion should not be viewed as a comprehensive characterization of the behavior of our algorithms on constrained mdps. however  we believe that our experiments provide some interesting observations about such problems.
¡¡we experimented with a large set of randomly-generated problems and with a more meaningful manually-constructed domain  which we randomly perturbed in several ways. the big picture resulting from the experiments on the randomlygenerated domains was very similar to the one from the manually-constructed example  providing a certain measure of comfort about the stability and validity of our observations. we report the results for the manually-constructed domain.
¡¡for our test domain  we used a simplistic model of an autonomous delivery agent  as mentioned in the introduction  based on the multiagent example from  dolgov and durfee  1b  . in the domain  an agent is operating in a grid world with delivery sites placed randomly throughout the grid. the agent moves around the grid  incurring small negative rewards for every move  and receives positive rewards for making deliveries. the agent's movement is nondeterministic  and the agent has some probability of getting stuck in randomly-placed dangerous locations. the agent also incurs a scalar cost  e.g.  time  per move  and the objective is to maximize the total expected discounted reward subject to an upper bound on the total expected discounted cost.
the results of our experiments are summarized in figure 1.
figure 1a shows the values of randomized and deterministic policies as functions of the constraint level   1    where 1 means that only policies that incur zero cost are feasible  strictest possible constraint   whereas 1 means that the upper bound on cost equals the cost of the optimal unconstrained policy  agent is not constrained at all . the first observation  as illustrated in figure 1a  is that the value of stationary deterministic policies for constrained problems is reasonably close to optimal. we can also observe that the value of deterministic policies changes in a very discrete manner  i.e.  it jumps up at certain constraint levels   whereas the value of randomized policies changes continuously. this is  of course  only natural  given that the space of randomized policies is continuous  and randomized policies can gradually increase the probability of taking  better  actions as cost constraints are relaxed. on the other hand  the space of deterministic policies is discrete  and their quality jumps when the constraints are relaxed to permit the agent to switch to a better action. while the number and the size of these jumps in the value function depends on the dynamics of the mdp  the high-level picture was the same in all of our experiments.
¡¡figure 1b shows the running time of the milp solver as a function of the constraint level  here and in figure 1c the plots contain values averaged over 1 runs  with the error bars showing the standard deviation . the data indicates that our milps  1  have an easy-hard-easy complexity profile  although without a sharp phase transition from hard to easy  i.e.  the problems very quickly become hard  and then gradually get easier as cost constraints are relaxed.
¡¡this complexity profile gives rise to the question regarding the source of the difficulty for solving milps in the  hard  region: is it difficult to find good feasible solutions  or is it time-consuming to prove their optimality  figure 1c suggests that the latter is the case  which can be considered as the more fortunate outcome  since algorithms with such performance profiles can be successfully used in an anytime manner. the figure contains a plot of the quality of the best solution found as a function of the time bound imposed on the milp solver1for problems in the hardest constraint region  constraint level value of 1 . as the graph shows  very good policies are usually produced rather quickly.
¡¡let us conclude with a somewhat intriguing observation about the milp solution time for constrained mdps with multiple discount factors  section 1 . we generated and solved a large number of random mdps with two discount factors and plotted  after cubic smoothing  the average solution time  shown in figure 1d . an interesting observation about this plot is that the problem instances where the two discount factors are equal  or close  appear to be the hardest  notice the contours in the ¦Ã1-¦Ã1 plane . this is counterintuitive  because such mdps are equivalent to standard mdps with one discount factor. a possible explanation might be that when discount factors are far apart  one of the reward functions dominates the other and the problem becomes simpler  while when the discount factors are close  the tradeoffs become more complicated  with the equivalence to a standard mdp hidden in the milp translation . however  this is speculation and the phenomenon deserves a more careful analysis.
1 discussion and conclusions
we have presented algorithms for finding optimal deterministic policies for two classes of constrained mdps  and in both cases we were maximizing a measure of the total expected discounted reward subject to constraints on the total expected discounted costs. however  our technique of finding optimal stationary deterministic policies via mixed integer programming also applies to other classes of mdps.
¡¡in particular  the same methodology applies to mdps with average per-time rewards and constraints  e.g.   puterman  1  . similarly to the constrained total-reward discounted mdp model described in section 1  the mdp with average rewards and constraints can also be formulated as an lp  similar to  1   that yields optimal stationary randomized policies. the problem of finding optimal stationary deterministic policies for such mdps is also known to be np-complete  filar and krass  1 . our milp reduction of section 1 carries through with almost no changes and can thus be used to find optimal stationary deterministic policies for such mdps.
¡¡the techniques of section 1 also apply more generally. for instance  consider an mdp with general utility functions in the optimization criteria and constraints on the probability that the total cost exceeds some upper bound. this class of mdps was discussed in  dolgov and durfee  1a   and the problem of finding approximately-optimal stationary randomized policies was reduced to a non-convex quadratic program in the space of the higher-order moments of the state visits. the non-convex quadratic constraints resulted from the requirement that the moments of different orders had to correspond to the same policy  and were almost identical to the quadratic constraints in  1  that synchronized the occupation measures for different discount factors. in fact  the two are so similar that our milp reduction from section 1 can be used to approximate optimal stationary deterministic policies for mdps in  dolgov and durfee  1a .
¡¡to summarize  we have presented a general integer programming method for finding optimal stationary deterministic policies in constrained mdps. we have demonstrated the method on two classes of mdps:  i  a constrained discounted mdp and  ii  a constrained mdp with multiple discount factors. for  i   our methodology is of most value for domains where randomized policies  which work better and are easier to compute  are undesirable or difficult to implement because of an agent's architectural limitations. however  even in the absence of such limitations  the approach is useful in situations where it is desirable to compare the quality of randomized and deterministic policies  such as when an agent is being designed for a particular task and it is necessary to weigh the cost of implementing a more complex policy-execution mechanism against the gain in expected performance. for problem  ii   to the best of our knowledge  no feasible algorithms have been reported for finding optimal solutions in any interesting policy class  and thus our milp approach for finding optimal stationary deterministic policies provides the first practical approach to dealing with constrained mdps with multiple discount factors.
