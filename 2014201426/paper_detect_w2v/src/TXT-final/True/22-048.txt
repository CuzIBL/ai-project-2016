 
a problem with a* is that it fails to guarantee optimal solutions when its heuristic  h  overestimates. since optimal solutions are often desired and an underestimating h is not always available  we seek to remedy this. from a nonadmissible h an admissible one is generated using h's statistical properties. the new heuristic  hm  is obtained by inverting h with respect to its own least upper bound function. the set of nodes expanded when a* uses g + hm as an evaluator is compared with the set of nodes expanded using other approaches which have been suggested in the literature. a considerable potential savings in node expansion when using hm is indicated. in 1-puzzle experiments a* using g +hm expands one fifth as many nodes as does the best alternative approach. 
1. introduction 
a problem with a* is that it fails to guarantee optimal solutions when its heuristic  h  overestimates. since optimal solutions are often desired and an underestimating h is not always available  we seek to remedy this. 
davis  bramanti-gregor and chen 　　two approaches to this problem have been suggested in the literature. in one  chakrabarti et al  1; pearl  1  p. 1   an upper bound p on the worst case overestimation of h is obtained; ie  p = max h n /h* n : h* n    1   where h* returns the optimal distance to goal. now set hp = h/p so that hp underestimates h*. a* with evaluator g + hp  denoted a* hp   is admissible. a problem with this is that when p is large  hp is weak; in the extreme case hp returns values less than the minimum edge length of the state space graph and its effect is only to break ties in a breadth-first search. the other approach is a two phase variation of bagchi and mahanti's c-algorithm  1  1 . it requires additional information  such as p  above  and is 
called here c' p . it is described in appendix b. 
　　the new approach described here uses a statistical sampling to learn more precise information about h's overestimation. namely  one estimates the maximum possible value of h n  as a function of n 's true distance from goal. we denote this statistically learned function by maxh: 
maxh x  = max{ h n : h* n  = x }  x   1 . 
　　if p may be obtained from samplings  then the same measurements taken may be used to estimate maxh. if p can be obtained from domain specific theoretical considerations  then these same considerations might enable the upper bound to be a function of distance to goal  ie  they may be used to ascertain maxh. from h an underestimating heuristic hm is built using maxh by defining hm n  = min x: h n    maxh x } 
for all nodes n. the heuristic hm is no less informed than hp in the sense that hp  hm   h*. if we had hp   hm on all non-goal nodes  then we could conclude that all nodes expanded by a* hm  are expanded by a* hp   nilsson  1 . unfortunately the inequality is not strict. a simplified search model  due to huyn  pearl and dechter  huyn et al  1   when applied to this situation  implies that the non-strict inequality is adequate to assure that the expected number of nodes expanded by a* hp  is greater than or equal to the expected number expanded by a* hm . however  we would like a more detailed statement of which nodes may be expanded by a* hp   a* hm  in order to better compare the two algorithms with each other and with c' p . 
　　in sections 1  1 we described the nodes expanded by a* hp   a* hm  and c' p  by specifying lower and upper bounds for them  ie sets of nodes which are surely expanded  se  and possibly expanded  pe  by each algorithm. these se and pe sets are defined in terms of whether or not certain  constrained  paths 

have their nodes expanded. the path constraint for each of the three algorithms is expressed in terms of the original non-admissible heuristic h; this allows a direct comparison of the three node-expansion sets. the paths explored by the three algorithms are plotted on a single graph. it is seen that  when maxh is non-linear  the potential savings in node expansion when using a* hm  instead of a* hp  is considerable. it increases as distance between start and goal increases. c' p  is seen to be substantially slower than the other two algorithms. we also show how se and pe set containment may be used in a direct way to rank the speed of the algorithms. 
　　all three algorithms were run on the 1-puzzle using the enhanced manhattan distance for an overestimating heuristic. on average a* hp  expanded 1 times as many nodes as did a* hm . as problem difficulty increased the comparative behavior of a* hm  improved. as expected  c' p  ran considerably slower than a* hp . details are in section 1. section 1 discusses methods for building hm and section 1 concludes. 
1. notation 
s start node g* n  length of cheapest path from s to n h* n  length of cheapest path from n to a goal gin  length of cheapest path found so far from s to n by a search algorithm; also length along a particular specified path in section 1 h n  estimate of h* n ; assume h goal  = 1 c* h* s  p max{ h n /h* n : h* n    1 } hp h/p maxh x  max  h n : h* n  = x   hm n  min{ x: h n    maxh x    a* h  	a* algorithm using g + h as evaluation 
function 
　　the state space graph is assumed to be a locally finite directed graph with arc length bounded below by a positive number. we assume a solution path exists. a heuristic function h is called admissible 
 =underestimating  	if 
1. basic properties of hm 
let h be a non-admissible heuristic function. in order to build an admissible function from it we first obtain maxh: 

this may be learned by doing a statistical sample of the values returned by h on nodes a known distance from the goal. for example  figure 1. shows maxh when h is the enhanced manhattan distance in the 1-
1 	search 

figure 1. 
puzzle. the latter is simply the manhattan distance plus a rotational term see nilsson . the data were gathered by gaschnig.  see gaschnig  where the sampling techniques and confidence levels are discussed.  
define hm on nodes n via 

since h n    maxhih*in    one of the x-values on the right side of  1  is h* n . thus hm n   h* n  so hm is admissible. hm is no less informed than hp in the sense that  to see this  notice that maxh x  px and that the right side of  1  defines hp when maxh x  is replaced by px. 
let maxh x  = max maxhit : t   x  so that 
maxh is like maxh except that when maxh values decrease those of maxh remain constant; ie  maxh is non-decreasing. if maxh is replaced by maxh in  1   the values of hm are unchanged. hence we may assume  without loss of generality  that maxh is nondecreasing and defined for all x  1 c* . this is done in the sequel. 
1. nodes expanded: 
let p be a path emanating from the start node. let r be some constraint on the nodes of p. for example  r might be the requirement that nodes n satisfy gin    c*  where g is understood to mean distance from start along p and c* denotes optimal distance from start to goal1 . we say p is r-constrained if every node on p satisfies r. the set of all nodes n such that there is 
1. as used here  g is a function of p and n. the more precise symbol  used in the appendix   is c p n  instead of gin . 


davis  bramanti-gregor and chen 

versa . one then says that a dominates b. proving dominance for large classes of problems is difficult and not always achievable. however  in theorem 1 the upper and lower bounds placed on e a  by pe a  and se a  differ only by an '=' or an '♀' within a path constraint. this considerably restricts possible values for e a . therefore we propose using pe and se set containment to get an approximate idea of which of several competing algorithms is fastest. algorithm a is said to have bounded dominance over b  written 
a   b  if we always have se b     se a  and pe b     pe a . by the theorem  a* hm    a* hp    c p . accordingly  we expect a* hm  to expand the least number of nodes and c' p  the most. 
1. experimental data 
all three algorithms were run on the 1-puzzle using the enhanced manhattan distance for an overestimating heuristic. gaschnig's statistics  figure 1 and  gashnig  1   were used for p and maxh. accordingly  p was set to 1. table 1 shows comparative data for nodes expanded by a* hp  and a* hm . in this sample there were 1 problems with start-goal distances between 1 and 1. on average a* hp  expanded 1 times as many nodes as did a* hm . as problem difficulty increased  the comparative behavior of a* h m   improved. for example  when start-to-goal distances were 1  a* hp  expanded 1 times as many nodes as did 
a* h m   while  for start-goal distances of 1  the ratio was 1. these distances can be as large as 1 in the 1-puzzle and the most frequent distance is 1. however  we ceased sampling when a* hp  required more than one day per problem on our facility  ibm 1 . 
　　c' p  ran considerably slower than a* hp  so statistics were not collected for it. both a* hp   a* h m   returned only optimal solutions in this sample. however  since hp  hm are built from statistically gathered data  this need not always happen. 
d  no. of problems nodes expanded  avg.  
	a* hp  	a* hm  ratio1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 1 
1 1 
1 1 
1 1 	1 	1 	1 	1 
table 1. d = distance from s to goal. 
1 	search 
1. building hp  hm 
the techniques described in this paper all require information about the behavior of the non-admissible heuristic ft. 
　　a* hp  and c' p  require p and a* hm  requires maxh. the problem of how to obtain such information has never been addressed: chakrabarti  says that 'if the proportional error h/h* is bounded above by e  then ...*; bagchi  says 'suppose q1pt 
  aq for some given constant   1  ...'; and pearl  writes: 'when a heuristic ft  is known to overestimate ft* cosistently  ... then the use of h1 = aft  with a  1 may be justified ...' however no-one has suggested how to obtain the critically needed constants  e  a or p . in some cases theoretical insight about a problem domain may reveal p  maxh. the only general methods of which we are aware involve statistical sampling. in this case p  maxh are known with imperfect confidence. 
　　to estimate p  maxh one needs to sample a large number of ft n -values for each of many possible ft* n -values. even when the ft*ffl -values are known with certainty  the critical nodes n may not have been examined  consequently the estimates of p  maxh may be too low. the corresponding heuristics  may overestimate. confidence in their admissibility increases with confidence in the estimates of p  maxh. 
　　there are several possible approaches  some of which we mention below. 
 1  a breadth-first expansion of the state space from several possible goals provides good information  but only for small ft* n -values due to likely combinatorial explosion. however such limited data may suggest that maxh is essentially concave down  as in figure 1   causing p to occur early.  it occurs at h* n  = 1 in figure 1.  to the extent that this is believed  one may wish to build hp from such limited data. 
 1  if a weak admissible heuristic is known  then it may be used with a* to find optimal paths between randomly generated start-goal pairs. from these paths  desired statistics may be obtaiained . ida*  korf  1  rather than a* could be used. the problem here is that a weak heuristic may not enable the discovery of long optimal paths within reasonable computer resources.  this method raises another question: will the 'admissibilized' heuristic expand fewer nodes on average than the admissible heuristic used to generate the statistics  in the 1puzzle we have created hm 's stronger than this-outof place  a weak admissible heuristic   but not stronger than the manhattan distance. had the former been used to generate our statistics  then the answer would be 'yes'. other domains need to be 
studied.  

 1  goals are selected randomly from the state space. random walks are made from each of these goals into the state space; from the nodes reached  data are collected regarding maximum h-values corresponding to apparent h*-values  as measured along the walk . the result is an even lower estimate of p or maxh than would be the case if accurate h*-values were known1. however  one can build up confidence in such estimates by taking a sufficiently large sample. this method has been suggested in a different context by politowski . 
　　a combination of the above methods may be used to build better and better estimates of maxh. if a domain is to be repeatedly searched  then this searching may itself be combined with learning better estimates of maxh. 
　　the problem requires further study. another interesting problem is that of relating statistical confidence in the estimate of maxh to statistical confidence in the admissibility of a* hm . 
1. conclusions 
we have shown that an overestimating heuristic h may be made admissible by using a statisticallylearned non-linear transformation. when used with a*  the new heuristic enables optimal goals to be found while expanding fewer nodes than does any previously suggested technique which is also based on 
h. 
　　all previous methods use some additional information about h's behavior  namely p. the method described here uses more detailed information  namely maxh; but the same measurements taken to statistically estimate p may be used to estimate maxh. the initial estimation cost may pay off if a* is to be run repeatedly in the same domain and an acceptable admissible heuristic is not available. in all these methods confidence in admissibility is based on confidence in the statistical estimates of p or maxh. 
appendix a. notation for appendices b  c 
open 	nodes which are candidates for expansion in 
a* and similar algorithms 
c p n  length of path p from s to n 
m p  max{c p n  + h n : n is on p  where p is some solution path 
q 	min{m p : p is a solution path   	called the first 	discriminant 
qopt min m p : p is an optimal solution path   called the second discriminant 
the last four terms are from  bagchi 1 . 
1. to see this recall from section 1 that maxh = maxh. 
appendix b. a two-phase admissible search algorithm 
bagchi and mahanti  describe an algorithm  c  which yelds high solution quality and reexpands fewer nodes than a*. they point out that a 1-phase variation  which we call c' p   may be used to find optimal solutions when heuristics overestimate. 
the essential control part of c' p   is shown in 
figure 1. we use the symbol open t  to denote  n: n e open  fin    t} and call this the focus; t is called the focus bound. f in line 1  1 is the largest value yet of min  fin : n e open }. in phase 1 c is run to completion obtaining a possibly non-optimal goal  say n. this phase ends when the test at line 1 is positive. at this point the value of f is q  the first discriminant. the whole point of phase 1 is to find q. 
　　in the second phase an upper bound for qoptlq is required  p works  lemma ci  in appendix c  in lines 1 1 n is returned to open and the focus bound is set larger than q1pt  namely pq    where it remains for the duration of the search. focused nodes and their focused descendants are now breadth-first expanded until a  second  goal is found. this goal will be optimal because the focus bound is large enough to assure that nodes on some optimal path can be 'focused'  and a breadth-first selection finds this solution path first. if n was optimal  then it will be rediscovered  but now with knowledge of its optimality. the idea of the algorithm is that presumably breadth-first expanding only nodes which qualify for focus and have g-values less than gin  is faster than conducting a blind breadth-first search from start considering all of open  

davis  bramanti-gregor and chen 


1 	search 
1 

1 

1 



1 











1 

1 

