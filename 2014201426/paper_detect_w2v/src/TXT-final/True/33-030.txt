 
this paper outlines a theory of analogical reasoning based on a process-model of problem solving by analogy and the hypothesis that problem solving and learning are inalienable  concurrent processes in the human cognitive system. the analogical problem solver exploits prior experience in solving similar problems  and  in the process  augments a hierarchically-structured epsiodic long term memory. an analogical transformation process is developed based on a modified version of means-ends analysis in order to map past solutions from similar problems into solutions satisfying the requirements of the new problem.1 
1. introduction 
　analogical reasoning has been a sparsely-investigated phenomenon in artificial intelligence  1 1  1 . nonetheless  analogy promises to be a central inference method in human cognition as well as a powerful computational mechanism. in this paper  i discuss a computational model of problem solving by analogy based on an extension of means-ends analysis  mea . my central hypothesis  based in part on schanks theory of memory organization  1   is the following: when encountering a new problem situation  a person is reminded of past situations that bear strong similarities  at different levels of abstraction  to the present situation. this type of reminding experience serves to retrieve behaviors that were appropriate in earlier situations  whereupon past behavior is adapted to meet the demands of the current situation. 
　commonalities among previous and current situations  as well as successful applications of modified plans can serve as the basis for generalization. similarly  performing an inappropriate behavior in a new situation can lead to discrimination  among the ways in which situations are organized in memory and/or the mechanisms that adapted an existing plan to the new situation . however  a reactive environment that informs the problem solver of success  failure  or partial success is an absolute requirement for any generalization or discrimination process to apply. i consider problem solving by analogy and experiential learning to be inalienable components of a unified cognitive model. analogical problem solving exploits knowledge of plans indexed under situations similar to the current one  generating new purposive behavior potentially relevant to future problem solving. and  the learning component creates the memory structures to encode experiential knowledge  generalizing where appropriate  
   this research was sponsored in pan by the office of naval research  onr  under grant number n1 c 1 the author would like to thank allen newell  david kishi and monica lam for their useful suggestions in various discussions. 
that enable the problem solver to retrieve and compare relevant situations and plans from memory. the bulk of this paper focuses on developing a computationally-effective mechanism for problem solving by analogy. a detailed discussion of experiential learning processes is beyond the scope of this presentation  but see  . 
1. problem solving by analogy 
   traditional al models of problem solving  e.g.  gps   strips 1   and noah   approach every problem almost without benefit of prior experience in solving other problems in the same or similar problem spaces. hence  gps will perform the same means-ends analysis  mea  process to solve the monkeyand bananas problem  where a monkey needs to place a box underneath some bananas suspended from the ceiling in order to climb on the box  reach the bananas and eat them  and to solve the experimenter-andbananas problem  where the experimenter must move the same box enabling him to reach the hook in the ceiling where he will hang the bananas   then he must move the box away to create a problem for the monkey . however  an intelligent monkey observing the experimenter hang the bananas  may directly conclude which parts of the experimenter's behavior he should replicate in order to reach the bananas. similarly  an experimenter who becomes hungry after watching an unenlightened monkey repeatedly fail in his attempts to reach the bananas  should know how to reach the bananas himself without planning from ground zero. 
　clearly  the bulk of human problem solving takes place in problem spaces that are either well known or vary only slightly from familiar situations. it is rare for a person to encounter a problem that does not remind him of potentially applicable solutions to similar problems solved or observed in past experience. new puzzles  such as rubik's magic cube  are such exceptional problems  where initially the only tractable solution procedure is the application of weak methods  without benefit of  nonexistent  past experience. therefore  my investigations center on simplified versions of real-world problems  rather than more abstract mathematical puzzles. 
　now  let us turn to problem solving in familiar problem spaces. what makes a problem space  familiar   clearly  knowledge of solved problems in that space is a major aspect. this knowledge must have been acquired  and must be brought to bear in the problem solving process. there is no other way to account for the tact that humans solve problems in familiar situations much faster  and with more self-assurance. a computer model should exploit the same skill acquisition process; i.e.  it should learn to adapt its problem-solving behavior to known problem spaces   falling back on the application of weak methods when more direct recall-endmodification of existing solutions fails to provide an answer. how might a problem solver be augmented to exhibit such adaptive 
1 

behavior  below  i illustrate a tractable mechanism for problem solving by analogy. 
1. the plan-transformation problem space 
　first  consider a traditional means-ends analysis  mea  problem space   populated by: 
  a set of possible problem states. 
  one state designated as the initial state 
  one or more state s  designated as goal states    for simplicity  assume there is only one goal state. 
  a set of operators with known preconditions that transform one state into another state in the space. 
  a difference function that computes differences between two states  typically applied to compute the difference between the current state and the goal state . 
  a method for indexing operators as a function of the difference s  they reduce  e.g.  the table of differences in gps . 
  a set of global path constraints that must be satisfied in order for a solution to be viable.  e.g.  a path constraint may disallow particular subsequences of operators  or prevent an operator that consumes k amount of a resource from applying more than n times  if there is only nxk amount of the resource available to the problem solver. 1 
problem solving in this space consists of standard mea: 
compare the current state to the goal state; choose an operator that reduces the difference; apply the operator if possible    if not recurse on the subgoal s  of establishing the unsatisfied precondition s  of that operator. 
   how can we exploit knowledge of solutions to previous problems in this problem space  first  consider the simplest case; knowledge consists only of solutions to previous problems. each solution consists of a sequence of operators and intermediate states  including the initial and final states  together with the path constraints that the solution was designed to satisfy. one rather simple idea is to create  macro operators  from sequences and sub-sequences of atomic operators that have proven useful as solutions to earlier problems. for instance  strips with macrops exploited this idea  using its  triangle table  to store all partial sequences of operators encountered in a solution to a previous problem. however  the simple creation of macrooperators suffers two serious shortcomings. first  the combinatorics involved in storing and searching all possible subsequences of all solutions ever encountered becomes rapidly unmanageable. searching for applicable macro-operators can become a more costly process than applying mea to the original problem. second  path constraints are ignored in this process. if the new problem must satisfy a different set of path constraints  most previous macro-operators may prove invalid. therefore  let us think not in terms of creating more and more powerful operators that apply to fewer and fewer situations  but rather think in terms of gradually transforming an existing solution into one that satisfies the requirements of the new problem. 
　consider a reminding process  a search for solutions to problems similar to the one at hand  that compares differences among the following: 
1. the initial state of the new problem and the initial state of previously-solved problems 
1. the final state of the new problem and the final state of previously-solved problems 
1. the path constraints under which the new problem must be solved and path constraints present when previous similar problems were solved. 
1. the differential applicability of the retrieved operator sequence in the old and the new problem situations. 
   the difference function used in comparing initial and final states may be the very same function used for difference reduction in standard mea. here  i advocate using the difference function as a similarity metric to retrieve the solution of a previously-solved problem closely resembling the present problem. the difference function applied to path constraints is a 
   generalization of the problem state difference function  as it must address operator-sequence differences in addition to state information. hence  reminding in our problem-solving context consists of recalling a previously solved problem whose solution may transfer to the new problem under consideration. a more sophisticated method of computing similarities among episodic memory structures is based on a relativeinvariance hierarchy among different components of recalled problem solutions  as discussed in  1 . 
reminding is only the first phase in analogical problem solving. 
the second phase consists of transforming the old solution sequence into one satisfying the criteria for the new problem. how does this transformation process proceed  i submit that it is equivalent to problem solving in the space of solutions.1 

1    here i apply my previous definition of a solution to be a sequence of operators and intermediate states together with the set of path constraints that sequence is known to satisfy thus. i advocate applying mea to the space of potential solution sequences rather than the original problem space. however  the reminding process should generate an initial solution sequence close to the goal solution sequence  where closeness is determined by the difletenco metric above 
　finding an appropriate analogical transformation is itself a problem solving process  but in a different problem space. the states of the transform problem space are solutions to problems in the original problem space  where the initial state is the retrieved solution to a similar problem  and the goal state is a solution satisfying the criteria for the new problem. the operators in the transform problem space are the atomic components of all solution transformations  e.g.  substitute an operator in the solution sequence for another operator that reduces the same difference  but requires a different set of preconditions or entails different side effects  etc.    see below . the differences that the problem solver attempts to reduce in the new problem space are precisely those computed by the similarity metric in the reminding process. in other words  progress towards a goal is determined by transitions in the solution space towards  solution sequences  corresponding to problems increasingly similar to the original problem. intermediate states in the transform space need not correspond to viable solutions in the original  object  space  in that intermediate solution sequences may not be executable due to unsatisfied operator preconditions. the diagram in figure 1 gives an intuitive flavor of this problem-solving process. more precisely  the analogy transform problem space  t-space  is defined as follows: 
estates in the transform space are potential solutions to problems in the original problem space  i.e.  sequences of states and operators including the initial and final states  plus the path constraints under which those solutions were computed.  
  the initial state in the transform space is the solution to a similar problem retrieved by the reminding process. 
  a goal state in the transform space is the specification of a 
solution that solves the new problem  satisfying its path constraints. 
  an operator in the transform space  labeled a  toperator  to avoid confusion  maps one solution sequence into another potential solution sequence in the untransformed problem space. the following is a partial list of useful t operators: 
o general insertion. insert a new operator into the solution sequence. 
o general deletion. delete an operator from the solution sequence. 
o subsequence splicing. splice a solution to a new subproblem into the larger established solution sequence. this toperator is useful in the following situation: if an operator in the original problem sequence cannot be applied under the new problem specification because one of its preconditions is not satisfied  problem solve in the original  object  space to find a means of satisfying this precondition. if successful  splice the preconditionfulfilling subsequence into the original solution sequence. 
o subgoal-preserving substitution. substitute an operator in the original solution sequence by another operator  or sequence of operators  that reduces the same difference. this toperator is particularly useful if either a precondition of an operator in the original sequence cannot be satisfied  or if the presence of a particular operator in the solution sequence violates a path constraint.1 
   note that a aubgoal-preserving substitution is much more restrictive than a general delete toperator followed by a general insert toperator. therefore  thia t-operator is more apt to yield useful transformations  a fact reflected in the ordering of operators under each appropriate entry in the difference table. 
o final-segment concatenation. treat the solution sequence as a macro-operator in the original problem space and apply mea to reduce the difference between the old final state and the new final state. if successful  concatenate the solution to this subproblem at the end of the original solution sequence. 
o initial-segment concatenation. apply the process above to find a path in the original problem space from the new initial state to the old initial state. if successful  concatenate at the beginning of the original solution.  note that in this case we start with the initial state for the new problem and seek a path to the initial state for the retrieved solution  whereas in the final segmentconcatenation operator the inverse process applies.  
o sequence meshing. if two solutions to previous problems were retrieved in the reminding process  each solution differing from the new problem specification in non-identical aspects  merge their operator sequences. the resultant solution sequence should differ from a solution to the new problem by the intersection of the differences between each retrieved solution and the new problem specification.1 if the differences between the two retrieved solutions and the new problem specification form disjoint sets  sequence meshing yields a complete solution. 
o operator reordering. often a path constraint in the new problem specification can be met by simple reordering of operators  when allowed by their preconditions  in the retrieved solution. 
o parameter substitution. the objects to which operators were applied in the retrieved solution may be substituted for objects in the new problem  that do not violate the preconditions of the operators . 
o solution-sequence truncation. if the final state of an operator subsequence of the retrieved solution exhibits a smaller difference with the new problem specification  use this subsequence as the new basis for mapping into the desired solution sequence. 
e the difference function in the transform space  d   is a 
combination of the difference measures between initial states  of the retrieved and desired solution sequences   final states  path constraints  and degree of applicability of the retrieved solution in the new problem scenario. hence  the values of dt are 1- vectors  with the interpretation that all four component differences must be reduced  independently or jointly  in the transform space  t-space  problem solving process. 
	dt= d1 s1 s1 . 	d1 sf 1 sf 1 . 
	dp pc1 pc1   	d sol solj   
d1 is the difference function between states in the original space. dp computes differences between path constraints  pc's . da measures the applicability of the old solution in the new scenario by determining the fraction of operators in the initial solution sequence  sol   whose preconditions are not satisfied under the new problem specification. s1 denotes an initial state  and sf denotes a final state. the subscript 1 indexes the retrieved solution  and 1 indexes the specifications on the desired solution to the new problem. dt is reduced when any of its four components is independently reduced. 
1 　*merging two partial operator sequences is an interesting and potentially complex problem in itself procedural networks  developed in the noah system  1 . facilitate computations of operator interactions when meshing two plans. it it not always the case that two partial solution sequences can be merged effectively  eg  each subsequence may violate necessary preconditions for the other subsequence  nonalgorrthmic t-operatora such aa sequence meshing.define their own internal problem apace. 
the problem-solving process in tspace succeeds when 
dt =  nil  nil  nil  nil . interesting search problems occur when  in order to reduce one component in the difference vector  one or both of the other components must be increased. for example  the insertion of new operators into the solution sequence may have the unfortunate side-effect of violating an established precondition of an operator in the original sequence. in this case reducing do i  or dq f  results in increasing da. our first-pass solution is to define a  linear  combination of the four components and choose the operator that maximally reduces this value  backtracking when necessary. fortunately  it is often the case that differences in the 1-vector can be reduced in a componentwise independent manner. moreover  a modified version of the a min method  may apply  focusing the backtracking process when backtracking proves necessary. 
  a difference table for indexing the t-operators is needed. entries in the difference table take the form  to reduce 
	 difference . 	apply 	a mesiber 	of 	 t-1perat1r-
set  . the operators in the applicable set are usually ordered as a function of the heuristic measure of their utility in reducing the given difference. a sample difference table entry would be: 
o if the preconditions to an operator in sol1 are not satisfied  i.e. da is non-null   try  first  subgoahproservlng substitution on the inapplicable operator  or try solution-sequence splicing to satisfy the violated preconditions. 
  there are no path constraints in the transform space. since we are mapping from one solution sequence to another  the intermediate states and toperators do not necessarily correspond to actual operations performed on an external world  and therefore are not subject to its restrictions. this simplification is offset by the more complex difference metric in tspace. 
1. an example 
　reconsider the monkey and bananas and experimenter and* bananas problem  in light of the analogical problem-solving model. 
a monkey watches a behavioral psychologist  i.e.  the experimenter  pick up a wooden box and place it under a hook in the ceiling. next  the experimenter climbs on the box  places some bananas on the hook  climbs off the box  and returns the box to its original location. then  the experimenter releases the  hungry  monkey and leaves the room. how does the monkey plan to reach the bananas  can he benefit from having observed the experimenter  
　as we mentioned earlier  a  smart monkey  ought to learn from his observations of the experimenter. let us see how analogical problem solving applies here. for simplicity  assume the monkey does not have prior experience solving similar problems beyond his recent observation of the experimenter. the monkey's problem is: initial state - monkey on the floor  bananas on the ceiling  box in the room; final state - monkey in possession of the bananas; path constraints = physical abilities of the monkey. however  the solution to the experimenter's problem cannot be applied directly.  his problem was initial state ＊ possession of the bananas  box in the room  experimenter on the floor; final state   bananas on the ceiling  box nor under the bananas; path constraints = physical abilities of the experimenter.  
　assuming the path constraints match  the differences between the initial states  and the differences between the final states  are so large as to preclude any reasonable attempt at direct analogical transformation. therefore  the monkey must resort to standard me a  in the object problem space . he selects the operator get object  applied to bananas . this operator suffers an unsatisfied precondition: the monkey cannot reach the bananas. therefore  the active subgoal becomes: reach the ceiling where the bananas are located. how may the monkey proceed at this juncture  
the entire problem can  of course  be solved by standard mea. 
however  there is a more direct solution method. if the monkey recalls his observation of the experimenter  he may realize that the problem of reaching the ceiling has already been solved  by the experimenter  as a subgoal to placing the bananas there   although the monkey need not understand the experimenter's higher-level goals . the monkey can apply the parametersubstitution t-operator  substituting  monkey  for  experimenter'   and optionally the solution-sequence truncation t-operator  eliminating the need to return the box to its original location after having used it . this problem-solving process in t-space results in a plan that the monkey can apply directly to reach the bananas  and thus achieve his original goal of having  and presumably eating  the bananas. 
　the significant aspect of the experimenter monkey-andbananas example is that standard mea and tspace mea were combined into a uniform problem-solving process where standard mea calls on analogical problem solving to solve a subproblem more directly. the converse process is also possible  and potentially significant. hence  analogical reasoning adds a 
powerful dimension to standard problem solving when prior experience can be brought to bear  but remains largely unobstrusive when no relevant prior knowledge suggests itself. 
　additionally  it would be useful for the problem solver to remember his observations and problem solving experiences to use as a basis for future analogical reasoning. these could be remembered directly or abstracted into episodic traces  much like schank and abelson's scripts  1   and hierarchically organized as a function of the goals they fulfill. 
1. evaluating the analogical reasoning process 
　in an informal experiment  not meant to withstand statistical significance tests  i gave the following problem to -five undergraduate historyandart students: 
prove that the product of two even numbers is even. 
somewhat to my surprise  none of the five was able to solve this simple algebraic problem  although all five made serious attempts. later i explained the solution carefully enough to insure that all five understood it: 
first  recall the definition of an even number: a number that is divisible by 1. 
second  write down an expression that represents an even number: you may write  1n  where n is any integer  to represent a number divisible by 1. 
next  multiply two even numbers  writing: 1n x 1m  where m is also any integer. multiplying we get 1nm. 
	now  	recall 	the 	representation 	of 	an 	even 	number: 
1 x any integer. therefore you can write 1nm ＊ 1 x 1nm  which by closure of integers under multiplication matches the representation of an even number. hence  the product of two even numbers is even. 

1 

　at this point  all five students claimed they understood the proof  and moreover expressed some feeling of embarrassment for not having derived such an  obvious  proof themselves. then  i suggested they try the following problem: 
prove that the product of two odd numbers is odd. 
with a grim determination to redeem their previous poor performance all five attempted the problem and three of them succeeded. briefly: 
odd numbers can be represented as  even + 1  = 1n + 1 for any integer n. 
	the 	product  	therefore: 	 1n + 1  	x 	 1m + 1  = 
 1nm + 1n + 1m + 1 - 1nm + n + m  + 1  which is the representation of an odd number.1 
this informal experiement strongly indicates that the second problem was solved by analogy from the solution of the first problem. the scratch papers collected from the students suggest direct attempts at transfering and modifying steps of the first solution. the insertion of an extra algebraic step1 illustrates an application of the subsequence splicing t-operator. moreover  the mere fact that three of five students were able to solve a problem more complex that the one where all five failed previously  argues very convincingly for an analogical process exploiting the previous solution  or some abstraction thereof . however  it should be noted that this type of experiment does not in itself demonstrate dominance of analogical reasoning in human problem solving  but rather it provides strong evidence for the existence of analogical processes in cognitive activities. demonstrating the conjecture that analogy is the central inference mechanism for human problem solving would require a much more thorough  and perhaps more controlled  set of psychological observations. 
　as a test of the computational feasibility of the analogical problem solving process  a simple version of mea was programmed to operate on the transform space  and given a subset of the t-operators with a corresponding difference table. it solved the product of-two-odds problem starting from the solution for two even numbers.1 the initial program is not too interesting it demonstrates that the analogical problem solving process  actually works   but does little else. the truly interesting issues will arise when: 
  a much fuller implementation is available allowing comparisons among different problem solving methods over a representative corpus of problems  
  issues of learning from experience are further investigated  
  and the analogical problem solver is integrated with a 
dynamically-changing long term memory. 
   interestingly  one student chose to represent odd numbers as 1n   1  which is correct but requires a bit of additional algebraic manipulation of the two students who did not present an adequate proof  one erred in an algebraic manipulation step  the other was unable to represent odd numbers correctly. 
   i.e.. distributing the product of the two odd numbers is required to fulfill a precondition of the factormg-a constant operator that factors 1 from three of the four terms in; 1nm + 1n + 1m + 1. 
   it used n 1 to represent an odd number  since the su1 operator was inadvertently listed before ad1 in the object space difference table  and therefore had to splice in an additional algebraic step in the solution. 
1. learning as part of the problem solving process 
　let us examine briefly the learning processes inherent in analogical problem solving. my central hypothesis is that human learning occurs as an integral  inalienable aspect of problem solving and comprehension. this does not imply that disembodied concept acquisition and pure grammatical inference are ineffective processes. such processes are computationally valid but psychologically implausible. since i am primarily interested in cognitive modeling  i find the creation of a computational model that incorporates both learning and performance as inseparable processes to be an intellectually appealing endeavor. analogical reasoning provides the basis for such an integrated model. the fact that humans learn in realworld tasks  by practice  observation  trial and error  etc.  with little or no discernible cognitive effort lends credence to my integration-oflearningandperformance hypothesis. moreover  there is evidence that for cognitive as well as. physical motorcoordination tasks humans cannot help but learn in the process of repeated practice . this clearly suggests that at least some forms of human learning cannot be separated from cognitiveperformance or motor-coordination systems. 
　learning can occur in both phases of analogical problem solving: 1  the reminding process that organizes and searches past experience  and 1  the analogical transformation process itself. the first phase is discussed briefly below  and at greater length by schank and lebowitz . the second phase is analyzed in . environmental feedback  such as success or failure of the planned solution  can trigger solution analysis  where solutions to several similar problems can be considered exemplars for the task of synthesizing a general plan to deal with future problems of the 
same general type. simon and others  1  discuss this form of reducing a learningbydoing problem to a learning-fromexamples task. 
1. episodic memory organization 
　memory of solutions to previous problems  whether observed or directly experienced  must be organized by similarities in goal states  initial states  and means available  or path constraints present . otherwise  there can be no reasonable reminding process when solving future problems of a similar nature. hence  a hierarchical indexing structure on an episodic memory must be dynamically constructed and extended as the system gradually accumulates new experience. thus  continuously expanding and structuring episodic memory is a relatively simple  but absolutely essential  aspect of learning that proceeds concurrent with analogical reasoning. 
1. concluding remark 
　the objective of this paper has been to lay a uniform framework for analogical problem solving capable of incorporating skill refinement and acquisition processes. most work in machine learning does not attempt to integrate learning and problem solving into a unified process.  however  mitchell  and lenat  are partial counterexamples.  past and present investigations of analogical reasoning have focused on disjoint aspects of the problem. no past investigation has resulted in a unified analogical problem solving method. for instance winston   investigated analogy as powerful mechanism for classifying and structuring episodic descriptions. kling  studied analogy as a means of reducing the set of axioms and formulae that a theorem prover must consider when deriving new proofs to 

1 

theorems similar to those encountered previously. in his own words  his system  ...derives the analogical relationship between two  given  problems and outputs the kind of information that can be usefully employed by a problem solving system to expedite its search.  however  analogy takes no direct part in the problemsolving process itself. hence  the extension of means-ends analysis to an analogy transform space is  in itself  a new  potentially-significant problem-solving method  independent of the learning mechanisms that it can support. 
