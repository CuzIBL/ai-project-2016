
many problems in nlp require solving a cascade of subtasks. traditional pipeline approaches yield to error propagation and prohibit joint training/decoding between subtasks. existing solutions to this problem do not guarantee non-violation of hard-constraints imposed by subtasks and thus give rise to inconsistent results  especially in cases where segmentation task precedes labeling task. we present a method that performs joint decoding of separately trained conditional random field  crf  models  while guarding against violations of hard-constraints. evaluated on chinese word segmentation and part-of-speech  pos  tagging tasks  our proposed method achieved state-of-the-art performance on both the penn chinese treebank and first sighan bakeoff datasets. on both segmentation and pos tagging tasks  the proposed method consistently improves over baseline methods that do not perform joint decoding.
1 introduction
there exists a class of problems which involves solving a cascade of segmentation and labeling subtasks in natural language processing  nlp  and computational biology. for instance  a semantic role labeling  srl  system relies heavily on syntactic parsing or noun-phrase chunking  np-chunking  to segment  or group  words into constituents. based on the constituent structure  it then identifies semantic arguments and assigns labels to them  xue and palmer  1; pradhan et al.  1 . and for asian languages such as japanese and chinese that do not delimit words by space  solving word segmentation problem is prerequisite for solving part-of-speech  pos  labeling problem. another example in the computational biology field is the dna coding region detection task followed by sequence similarity based gene function annotation  burge and karlin  1 .
　most previousapproachestreat cascaded tasks as processes chained in a pipeline. a common shortcoming of those approaches is that errors introduced in upstream tasks propagate through the pipeline and cannot be easily recovered. moreover  the pipeline structure prohibits the use of predictions of tasks later in the chain to help making better prediction of earlier tasks  sutton and mccallum  1a . several new techniques have been proposed recently to address these problems. sutton et al.  introduced the dynamic conditional random fields  dcrfs  to perform joint training/decoding of subtasks. one disadvantage of this model is that exact inference is generally intractable and can become prohibitively expensive for large datasets. sutton and mccallum  1a  presented an alternative model by decoupling the joint training and only performing joint decoding. kudo et al.  presented another conditional random field  crf   lafferty et al.  1  based model that performs japanese word segmentation and pos tagging jointly. another popular approach of joint decoding for cascaded tasks is to combine multiple predicting tasks into a single tagging or labeling task  luo  1; ng and low  1; yi and palmer  1; miller et al.  1 .
　however  all aforementioned approaches do not work well for cases where a segmentation task comes before a labeling task. that is because in such cases  the segmentation task imposes hard-constraints that cannot be violated in successive tasks. for example  if a chinese pos tagger assigns different pos labels to characters within the same word  as defined by a word segmenter  the word will not get a single consistent pos labels. similarly  the constituent constraints imposed by syntactic parsing and np-chunkingtasks disallow argument overlaps in semantic role labeling  pradhan et al.  1 . from a graphical modeling's perspective  those models all assign nodes to the smallest units in the base segmentation task  e.g.  in the case of chinese word segmentation  the smallest unit is one chinese character . as a result  those models cannot ensure consistency between the segmentation and labeling tasks. for instance   ng and low  1 can only evaluate pos tagging results on a per-character basis  instead of a per-word basis. hindered by the same problem   kudo et al.  1  only considered words predefined in a lexicon for constructing possible japanese word segmentation paths  which puts limit on the generality of their model.
　to tackle this problem  we propose a dual-layer crfs based method that exploits joint decoding of cascaded sequence segmentation and labeling tasks that guards against violations of those hard-constraints imposed by segmentation task. in this method  we model the segmentation and labeling tasks by dual-layer crfs. at decoding time  we first perform individual decoding in each layer. upon these individual decodings  a probabilistic framework is constructed in order to find the most probable joint decodings for both subtasks. at training time  we trained a cascade of individual crfs for the subtasks  for given our application's scale  joint training is much more expensive  sutton and mccallum  1a . evaluated on chinese word segmentation and part-of-speech pos  tagging tasks  our proposed method achieved state-of-the-art performance on both the penn chinese treebank  xue et al.  1  and first sighan bakeoff datasets  sproat and emerson  1 . on both segmentation and pos tagging tasks  the proposed method consistently improves over baseline methods that do not perform joint decoding. in particular  we report the best published performance on the as open track of the first sighan bakeoff dataset and also the best average performance on the four open tracks.
　to facilitate our discussion  in later sections we will use chinese segmentation and pos tagging as a working example to illustrate the proposed approach  though it should be clear that the model is applicable to any cascaded sequence labeling problem.
1 joint decoding for cascaded sequence segmentation and labeling tasks
in this section  using chinese sentence segmentation and pos tagging as an example  we present a joint decoding method applicable for cascaded segmentation and labeling tasks.
1 a unified framework to combine chinese sentence segmentation and pos tagging
let c = {c1 c1 ... cn} denote the observed chinese sentence where ci is the ith chinese character in the sentence  s = {s1 s1 ... sn} denote a segmentation sequence over c where si （ {b i} represents segmentation tags  begin and inside of a word   t = {t1 t1 ... tm} denote a pos tagging sequence where m ＋ n and tj （
{the set of possible pos labels}. our goal is to find a segmentation sequence and a pos tagging sequence that maximize the joint probability p s t|c 1. let s  and t  denote the most likely segmentation and pos tagging sequences for a given chinese sentence  respectively. by applying chain rule  s  and t  can be obtained as follows:
  s  t    = argmaxp s t|c 
s t
	= argmaxp t|s c p s|c 	 1 
s t
	= argmaxp t|w c s  p s|c 	 1 
s t
equation 1 can be rewritten as equation 1  since given a sequence of characters c = {c1 c1 ... cn} and a segmentation s over it  a sentence can be interpreted as a sequence of words w c s  = {w1 w1 ... wm}.
　note that the joint probability p s t|c  is factorized into two terms  p t|w c s   and p s|c . the first term represents the probability of a pos tagging sequence t built upon the segmentation s over sentence c  while the second term represents the probability of the segmentation sequence s. maximizing the product of these two terms can be viewed as a reranking process. for a particular sentence c  we maintain a list of all possible segmentations over this sentence sorted by the their probability p s|c . for each segmentation s in this list  we can find a pos tagging sequence t over s that maximizes the probability p t|w c s  . using the product of these two probabilities  we can then rerank the segmentation sequences. the segmentation sequence s that is reranked to be the top of the list of all possible segmentations is the final segmentation sequence output  and the most probable pos tagging sequence along with this segmentation is our final pos tagging output. such a final pair always maximizes the joint probability p s t|c .
　intuitively  given a segmentation over a sentence  if the maximum of probabilities of all pos tagging sequences built upon this segmentation is very small  it can be a signal that tells us there is high chance that the segmentation is incorrect. in this case  we may be able to find another segmentation that does not have a probability as high as the first one  but the best pos tagging sequence built upon this segmentation has a much more reasonable probability  so that the joint probability p s t|c  is increased.
1 n-best list approximation for decoding
to find the most probable segmentation and pos tagging sequence pair  exact inference by enumerating over all possible segmentation is generally intractable  since the number of possible segmentations over a sentence is exponential to the number of characters in the sentence.
　to overcome this problem  we propose a n-best list approximation method. instead of exhaustively computing the list of all possible segmentations  we restrict our reranking targets to the n-best list s = {s1 s1 ... sn}  where {s1 s1 ... sn} is ranked by the probability p s|c . then  the approximated solution that maximizes the joint probability p s t|c  can be formally described as:
  s  t    = argmaxp s t|c 
s（s t
	= argmaxp t|w c s  p s|c 	 1 
s（s t
　comparing to other similar work that uses n-best lists and svm for reranking  daume and marcu  1; asahara et al.  1   or perform rule-based post-processing for error correction  xue and shen  1; gao et al.  1; ng and low  1   our method has a unique advantage that it outputs not just the best segmentation and pos sequence but also a joint probability estimate. this probability estimate allows more natural integration with higher level nlp applications that are also based on probabilistic models  and even reserves room for further joint inference.
1 dual-layer conditional random fields
in section 1  we have already factorized the joint probability into two terms p s|c  and p t|w c s  . notice that both terms are probabilities of a whole label sequence given some observed sequences. thus  we use conditional random fields  crfs   lafferty et al.  1  to define these two probability terms.
　crfs define conditional probability  p z|x   by markov random fields. in the case of chinese segmentation and pos tagging  the markov random fields in crfs are chain structure  where x is the sequence of characters or words  and z is the segmentation tags for characters  b or i  used to indicate word boundaries  or the pos labels for words  nn  vv  jj  etc. . the conditional probability is defined as:

where n x  is a normalization term to guarantee that the summation of the probability of all label sequences is one. fk z x t  is the kth localfeaturefunctionat sequence position t. it maps a pair of x and z and an index t to {1}.  λ1 ... λk  is a weight vector to be learned from training set.
　we model separately the two probability terms defined in our model  p s|c  and p t|w c s    using the dual-layer crfs  figure 1 . the probability p s t|c  that we want to maximize can be written as:
	p s t|c  = p t|w c s  p s|c 	 1 
	1

 w c s  j  
		 1 
where m and n are the number of words and characters in the sentence  respectively  nt w c s   and ns c  are the normalizing terms to ensure the sum of the probabilities over all possible s and t is one. {λk} and {μk} are the parameters for the first and the second layer crfs  respectively  fk and gk are the localfeaturefunctions for the first and the second layer crfs  respectively. their properties and functions are the same as common crfs described before.
　the n-best list of segmentation sequences s and the value of their corresponding probabilities p s|c   s （ s  can be obtained using modified viterbi algorithm and a* search  schwartz and chow  1  in the first layer crf. given a particular sentence segmentation s  the most probable pos tagging sequencet and its probabilityp t|w c s   can be inferred by the viterbi algorithm  lafferty et al.  1  in the second layer crf. having the n-best list of segmentation sequences and their corresponding most probable pos tagging sequences  we can use the joint decoding method proposed in section 1 to find the optimal pair of segmentation and its pos tagging defined by equation 1.
　we divide the learning process into two: one for learning the first layer segmentation crf  and one for learning the second layer pos tagging crf. first we learn the parameters μ = {μ1 ... μks} using algorithm based on improved iterative

figure 1: dual-layer crfs. p s t|c   the joint probability of a segmentation sequence s and a pos tagging sequence t given sentence c is modeled by the dual-layer crfs. in the first layer crf  the observed nodes are the characters in the sentence and the hidden nodes are segmentation tags for these characters. in the second layer crf  given the segmentation results from the first layer  characters combine to form  supernodes   words . these words are the observed variables  and pos tagging labels for them are the hidden variables.
scaling algorithm  iis   della pietra et al.  1  to maximize the log-likelihood of the first layer crf. then we learn the parameters λ = {λ1 ... λkt} also using iis  to maximize the log-likelihood of the second layer crf. a detailed derivation of this learning algorithm for each learning step can be found in  lafferty et al.  1 .
1 features for crfs
features for word segmentation
the features we used for word segmentation are listed in the top half of table 1. feature  1 - 1 are the basic segmentation features we adopted from  ng and low  1 . in  1   lbegin c1   lend c1  and lmid c1  represent the maximum length of words found in a lexicon that contain the current character as either the first  last or middle character  respectively. in  1   single c1  indicates whether the current character can be found as a single word in the lexicon.
　besides the adopted basic features mentioned above  we also experimented with additional semantic features  table 1   1 - 1  . in these features  sem c1  refers to the semantic class of current character  and sem c 1   sem c1  represent the semantic class of characters one position to the left and right of the current character  respectively. we obtained a character's semantic class from hownet  dong and dong  1 . since many characters have multiple semantic classes defined by hownet  it is a non-trivial task to choose among the different semantic classes. we performed contextual disambiguation of characters' semantic classes by calculating semantic class similarities. for example  let us assume the current character is  look read  in a word context of
　 read newspaper . the character  look  has two semantic classes in hownet  i.e.  read  and  doctor . to determine which class is more appropriate  we check the example words illustrating the meanings of the two semantic classes given by hownet. for  read   the example word is  read book ; for  doctor   the example word is
 see a doctor . we then calculated the semantic class

segmentation features

 1  cn n （   1 
 1  cncn+1 n （   1 
 1  c 1
 1  pu c1 
 1  t c 1 t c 1 t c1 t c1 t c1 
 1  lbegin c1   lend c1 
 1  single c1 
 1  sem c1 
 1  sem cn sem cn+1  n （  1

pos tagging features

 1  wn n （   1 
 1  wnwn+1 n （   1 
 1  w 1
 1  wn 1wnwn+1 n （   1 
 1  cn w1  n （   1 
 1  len w1 
 1  other morphological features
table 1: feature templates list
similarity scores between  newspaper  and  book   and  newspaper  and  illness   using hownet's built-in similarity measure function. since  newspaper  and  book  both have semantic class  document   their maximum similarity score is 1  where the maximum similarity score between  newspaper  and  illness  is 1. therefore  sem c1 sem c1  =  read  document . similarly  we can figure out sem c 1 sem c1 . for sem c1   we simply picked the top four frequent semantic classes ranked by hownet  and used  none  for absent values.
features for pos tagging
the bottom half of table 1 summarizes the feature templates we employed for pos tagging. w1 denotes the current word. w n and wn refer to the words n positions to the left and right of the current word  respectively. cn w1  is the nth character in current word. if the number of characters in the word is less than 1  we use  none  for absent characters. len w1  is the number of characters in the current word. we also used a group of binary features for each word  which are used to represent the morphological properties of current word  e.g. whether the current word is punctuation  number  foreign name  etc.
1 an illustrating example of the joint decoding method
in this section  we use an illustrating example to motivate our proposed method. this example found in the real output of our system gives suggestive evidences that pos tagging helps predicting the right segmentation  and the right segmentation is more likely to get a better pos tagging sequence. we are only showing a snippet of the full sentence due to space limit:
the production and sales situation of foreign owned companies is relatively good.
the segmentation that has the highest probability  1  is:
	 foreign owned   company 	 production 
	 situation 	 situation   relatively good 
the second best segmentation which has probability 1 is:
	 foreign owned   company 	 production 
	 situation 	 situation   relatively   good 
the	only	difference	from	the	first	sequence	is	that
　 relatively good  was segmented into two words  relatively   good . despite the lower probability  the second segmentation is more appropriate  since the two characters that compose the word  relatively good  carry their own meanings as individual words.
　the traditional methods would have stopped here and use the first segmentation as the final output  though it is actually incorrect according to the gold-standard. our joint decoding method further performs pos tagging based on each of the segmentation sequences. the pos tagging sequence with the highest probability  1  assigned to the first segmentation is:
 nn	   nn    nn	   nn	   nn	 
 vv	   pu	 
where nn represents noun  vv represents other verb  and pu represents punctuation. the second segmentation was assigned the following pos label sequence with the highest probability 1:
 nn	   nn    nn	   nn	   nn	 
 ad	   va    pu  
where ad represents adverb  va represents predicative adjective.
　the best pos sequence arising from the second segmentation is more discriminative than the best sequence based on the first segmentation  which indicates the second segmentation is more informative for pos tagging. the joint probability of the second segmentation and pos tagging sequence  1  is higher than the joint probability of the first one  1   and therefore our method reranks the second one as the best output. according to the gold-standard  the second segmentation and pos tagging sequences are indeed the correct sequences.
1 results
we evaluate our model using the penn chinese treebank  ctb   xue et al.  1  and open tracks from the first international sighan chinese word segmentation bakeoff datasets  sproat and emerson  1 .
　using a linear-cascade of crfs with the same set of features listed in table 1 as baseline  we compared the performance of our proposed method. the accuracies of both word segmentation and pos tagging are measured by recall  r   precision  p   and f-measure which equals to . for segmentation  recall is the percentage of correct words that were produced by the segmenter  and precision is the percentage of automatically segmented words that are correct. for pos tagging  recall is the percentage of all gold-standard words that are correctly segmented and labeled by our system  and precision is percentage of words returned by our system that are correctly segmented and labeled. we chose a n value of 1 for using in the n-best list  based on cross-validation results.
1 results of segmentation
for segmentation  we first evaluated our joint decoding method on the ctb corpus. 1-fold cross-validation was performed on this corpus. results are summarized in table 1.
111baseline1%1%1%1%1%1%joint decoding1%1%1%1%1%1%11averagebaseline1%1%1%1 %1%joint decoding1%1%1%1%1%table 1: comparison of 1-fold cross-validation segmentation results on ctb corpus. each column represents one out of the 1-fold cross-validation results. the last column is the average result over the 1 folds.
　as can been seen in table 1  the results in all of the 1fold tests improved with our joint decoding method. we conducted pairwise t-test and our joint decoding method was found to be statistically significantly better than the baseline method under confidence level 1 1  p-values .
　we also evaluated our proposed method on the open tracks of the sighan bakeoffdatasets. these datasets are designed only for evaluation of segmentation results  and no pos tagging information were provided in the training corpus. however  since the learning of the pos tagging model and the segmentation model is decoupled  we can use a separate training corpus to learn the second layer pos tagging crf model  and still be able to take advantage of the proposed joint decoding method. the results comparing to the baseline method are summarized in table 1.
asctbprf1prf1baseline1%1%1%1%1%1%joint decoding1%1%1%1%1%1%pkhkprf1prf1baseline1%1%1%1%1%1%joint decoding1%1%1%1%1%1%table 1: overall results on first sighan bakeoff open tracks. p stands for precision  r stands for recall  f1 stands for the f1 measure.
　for comparison of our results against previous work and other systems  we summarize the results on the four open tracks in table 1. we adopted the table used in  peng et al.  1  for consistency and ease of comparison. there were 1 participating teams  sites  in the official runs of the first international sighan bakeoff  here we only show the 1 teams that participated in at least one of the four open tracks. each row represents a site  and each cell gives the f1 score of a site on one of the open tracks. the second to fifth columns contain results on the 1 open tracks  where a bold entry indicates the best performance on that track. column s-avg contains the average performance of the site over the tracks it participated  where a bold entry indicates that this site on average performs better than our system; column o-avg is the average of our system over the same runs  where a bolded entry indicates our system performs better on average than the other site. our results are shown in the last row of the table.
　in the official runs  no team achieved best results on more than one open track. we achieved the best runs on as open  aso  track with a f1 score of 1%  1% higher than the second best system  peng et al.  1 . comparing to peng et al.   whose crfs based chinese segmenter were also evaluated on all four open tracks  we achieved higher performance on three out of the four tracks. our average f1 score over all four tracks is 1%  1% higher than that of peng et al.'s system. comparing with other sites using the average measures in the right-most two columns  we outperformed seven out of the nine sites. and the two sites that have higher average performance than us both did significantly better on the ctb open  ctbo  track. the official results showed that almost all systems obtained the worst performance on the ctbo track  due to inconsistent segmentation style in the training and testing sets  sproat and emerson  1 .
asoctbohkopkos-avgo-avgs11%1%1%1%s11%1%1%s1.1%1%1%1%1%1%s11%1%1%s11%1%1%s11%1%1%1%s11%1%1%1%s1.1%1%1%1%1%1%peng et al. '1.1%1%1%1%1%1%our system1%1%1%1%1%table 1: comparisons against other systems  this table is adopted from peng et al. 1 
1 results of pos tagging
since the bakeoffcompetitiondoes not providegold-standard pos tagging outputs  we only used ctb corpus to compare the pos tagging results of our joint decoding method with the baseline method. we performed 1-fold cross-validation on the ctb corpus. the results are summarized in table 1.
111baseline1%1%1%1%1%1%joint decoding1%1%1%1%1%1%11averagebaseline1%1%1%1 %1%joint decoding1%1%1%1%1%table 1: comparison of 1-fold cross-validation pos tagging results on ctb corpus. each column represents one out of the 1-fold cross-validation results. the last column is the average result over the 1 folds.
　from table 1 we can see that our joint decoding method has higher accuracy in each one of the 1 fold tests than the baseline method. pairwise t-test showed that our method was found to be significantly better than the baseline method under the significance level of 1 1  p-value . this improvement on pos tagging accuracy can be understoodas the result of the improved segmentation accuracy through joint decoding  as shown in table 1 . therefore  these results showed that our joint decoding method not only helps to improve segmentation results  it also benefits pos tagging results.
1 discussion on reranking
reranking technique has been successfully applied in many nlp applications before  such as speech recognition  stolcke et al.  1   np-bracketing  daume and marcu  1  and semantic role labeling  srl   toutanova et al.  1 . however  it is worth pointing out that the contexts in which they applied reranking method differ from ours in that we use reranking as an approximation technique for joint decoding. one similar work which also used reranking as approximation to joint decoding is  sutton and mccallum  1b . nevertheless  their experimentsshowed negativeresults when reranking was applied to the task of joint parsing and srl. one possible explanation is that the maximum entropy classifier they used is based on a local log-linear model  while crfs employed by our method model the joint probability of the entire sequence  and therefore are more natural for our proposed joint decoding method.
1 conclusion
we introduced a unified framework to integrate cascaded segmentation and labeling tasks by joint decoding based on duallayer crfs. we applied our method to chinese segmentation and pos tagging tasks  and demonstrated the effectiveness of our method. our proposed method not only enhances both segmentation and pos tagging accuracy  but it also offers an insight to improving performance of a task by learning from its related tasks.
