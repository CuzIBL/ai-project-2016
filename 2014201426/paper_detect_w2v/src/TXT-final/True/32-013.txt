 
we present a provably efficient and near-optimal algorithm for reinforcement learning in markov decision processes  mdps  whose transition model can be factored as a dynamic bayesian network  dbn . our algorithm generalizes the recent  algorithm of kearns and singh  and assumes that we are given both an algorithm for approximate planning  and the graphical structure  but not the parameters  of the dbn. unlike the original algorithm  our new algorithm exploits the dbn structure to achieve a running time that scales polynomially in the number of parameters of the dbn  which may be exponentially smaller than the number of global states. 
1 	introduction 
kearns and singh  1  recently presented a new algorithm for reinforcement learning in markov decision processes  mdps . their  algorithm  for explicit explore or exploit  achieves near-optimal performance in a running time and a number of actions which are polynomial in the number of states and a parameter t  which is the horizon time in the case of discounted return  and the mixing time of the optimal policy in the case of infinite-horizon average return. the  algorithm makes no assumptions on the structure of the unknown mdp  and the resulting polynomial dependence on the number of states makes  impractical in the case of very large mdps. in particular  it cannot be easily applied to mdps in which the transition probabilities are represented in the factored form of a dynamic bayesian network  dbn . mdps with very large state spaces  and such dbn-mdps in particular  are becoming increasingly important as reinforcement learning methods are applied to problems of growing difficulty {boutilierer al  1 . 
　in this paper  we extend the algorithm to the case of dbn-mdps. the original algorithm relies on the ability to find optimal strategies in a given mdp - that is  to perform planning. this ability is readily provided by algorithms such as value iteration in the case of small state spaces. while the general planning problem is intractable in large mdps  significant progress has been made recently on approximate solution algorithms for both dbn-mdps in 
1 
particular  boutilier et al.   1   and for large state spaces in general  kearns et al  1; koller and parr  1 . our new dbn-e1 algorithm therefore assumes the existence of a procedure for finding approximately optimal policies in any given dbn-mdp. our algorithm also assumes that the qualitative structure of the transition model is known  i.e.  the underlying graphical structure of the dbn. this assumption is often reasonable  as the qualitative properties of a domain are often understood. 
　using the planning procedure as a subroutine  dbn-  explores the state space  learning the parameters it considers relevant. it achieves near-optimal performance in a running time and a number of actions that are polynomial in t and the number of parameters in the dbn-mdp  which in general is exponentially smaller than the number of global states. we further examine conditions under which the mixing time t of a policy in a dbn-mdp is polynomial in the number of parameters of the dbn-mdp. the  anytime  nature of dbn-e1 allows it to compete with such policies in total running time that is bounded by a polynomial in the number of parameters. 
1 	preliminaries 
we begin by introducing some of the basic concepts of mdps and factored mdps. a markov decision process  mdp  is defined as a tuple  s a   r  p  where: s is a set of states; a is a set of actions; r is a reward function such that r s  represents the reward obtained by the agent in state $ l; p is a transition model. such that  represents the probability of landing in state s' if 
the agent takes action a in state s. 
　most simply  mdps are described explicitiy  by writing down a set of transition matrices and reward vectors - one for each action a. however  this approach is impractical for describing cornplex processes. here  the set of states is typically described via a set of random variables x = whore each  takes on values in some finite 
　　　　　　. in general  for a set of variables y x   an instantiation y assigns a value  for every 
we use val  y  to denote the set of possible instantiations to 
　　1 a reward function is sometimes associated with  state action  pairs rather than with states. our assumption that the reward depends only cm the state is made purely to simplify the presentation; it has no effect on our results. 

y. a stale in this mdp is an assignment x val x ; the total number of states is therefore exponentially large in the number of variables. thus  it is impractical to represent the transition model explicitly using transition matrices. 
　the framework of dynamic bayesian networks  dbns  allows us to describe a certain important class of such mdps in a compact way. processes whose state is described via a set of variables typically exhibit a weak form of decoupling - not all of the variables at time t directly influence the transition of a variable from time t to time t + 1. for example  in a simple robotics domain  the location of the robot at time t + 1 
　may depend on its position  velocity  and orientation at time t  but not on what it is carrying  or on the amount of paper in the printer. dbns are designed to represent such processes compactly. 
　let a a be an action. we first want to specify the transition model let denote the variable 
xi at the current time and denote the variable at the next time step. the transition model for action a will consist of two parts- an underlying transition graph associated with a  and parameters associated with that graph. the transition graph is a 1-layer directed acyclic graph whose nodes are f. all edges in this graph are directed from nodes in to nodes in 
           ; note that we are assuming that there are no edges between variables within a time slice. we denote the parents of in the graph by . intuitively  the transition graph for a specifies the qualitative nature of probabilistic dependencies in a single time step - namely  the new setting of depends only on the current setting of the variables in . t o make this dependence quantitative  each node xi is associated with a conditional probability table . the transition probability     i s then defined t o b e   where i s 
the setting in x of the variables in 
　we also need to provide a compact representation of the reward function. as in the transition model  explicitly specifying a reward for each of the exponentially many states is impractical. again  we use the idea of factoring the representation of the reward function into a set of localized reward functions  each of which only depends on a small set of variables. in our robot example  our reward might be composed of several subrewards: for example  one associated with location  for getting too close to a wall   one associated with the printer status  for letting paper run out   and so on. more precisely  let r be a set of functions each function is associated with a cluster of variables ci 
	  such that 	is a function 	from 	to r. 
abusing notation  we will use to denote the value that ri takes for the part of the state vector corresponding to c . 
the reward function associated with the dbn-mdp at a state x is then defined to be 
　the following definitions for finite-length paths in mdps will be of repeated technical use in the analysis. let m be a markov decision process  and let  be a policy in m. a t-path in m is a sequence p of t +1 states  that is  t transitions  of m:   . . .   . t h e probability that pis 
traversed in m upon starting in state 	and executing policy 
 is denoted 
　there are three standard notions of the expected return enjoyed by a policy in an mdp: the asymptotic discounted return  the asymptotic average return  and the finite-time average return. like the original e1 algorithm  our new generalization will apply to all three cases  and to convey the main ideas it suffices for the most part to concentrate on the finitetime average return. this is because our finite-time average return result can be applied to the asymptotic returns through either the horizon time  for the discounted case  or the mixing time of the optimal policy in the average case.  we examine the properties of mixing times in a dbn-mdp in section 1.  
let m be a markov decision process  let Π be a policy in 
m  and let p be a t-path in m. the average return along p in m is 

　an important problem in mdps is planning: finding the policy  that achieves optimal return in a given mdp. in our case  we are interested in achieving the optimal t-step average return. the complexity of all exact mdp planning algorithms depends polynomially on the number of states; this property renders all of these algorithms impractical for dbnmdps  where the number of states grows exponentially in the size of the representation. however  there has been recent progress on algorithms for approximately solving mdps with large state spaces  kearns et al.  1   particularly on ones represented in a factored way as an mdp  boutilier et al  1; koller and parr  1 . the focus of our work is on the reinforcement learning task  so we simply assume that we have access to a  black box  that performs approximate planning for a dbn-mdp. 
definition 1: a -approximation t-step planning algorithm for a dbn-mdp is one that  given a dbn-mdp   produces a  compactly represented  policy n such that 

we will charge our learning algorithm a single step of computation for each call to the assumed approximate planning algorithm. one way of thinking about our result is as a reduction of the problem of efficient learning in dbn-mdps to the problem of efficient planning in dbn-mdps. 
　our goal is to perform model-based reinforcement learning. thus  we wish to learn an approximate model from experience  and then exploit it  or explore it  by planning given the approximate model. in this paper  we focus on the problem of learning the model parameters  the cpts   assuming that the model structure  the transition graphs  is given to us. it is therefore useful to consider the set of parameters that we wish to estimate. as we assumed that the rewards are deterministic  we can focus on the probabilistic parameters. 
 our results easily extend to the case of stochastic rewards.  
we define a transition component of the dbn-mdp to be a 
	kearns 	and 	koller 1 

distribution for some action and some particular instantiation u to the parents in the transition model. note that the number of transition components is at most   but may be much lower when a variable's behavior is identical for several actions. 
1 	overview of the original 
since our algorithm for learning in dbn-mdps will be a direct generalization of the e1 algorithm of kearns and singh - hereafter abbreviated ks - we begin with an overview of that algorithm and its analysis. it is important to bear in mind that the original algorithm is designed only for the case where the total number of states n is small  and the algorithm runs in time polynomial in n. 
　e1 is what is commonly referred to as an indirect or modelbased algorithm: rather than maintaining only a current policy or value function  the algorithm maintains a model for the transition probabilities and the rewards for some subset of the states of the unknown mdp m. although the algorithm maintains a partial model of m  it may choose to never build a complete model of m  if doing so is not necessary to achieve high return. 
　the algorithm starts off by doing balanced wandering: the algorithm  upon arriving in a state  takes the action it has tried the fewest times from that state  breaking ties randomly . at each state it visits  the algorithm maintains the obvious statistics: the reward received at that state  and for each action  the empirical distribution of next states reached  that is  the estimated transition probabilities . 
　a crucial notion is that of a known state - a state that the algorithm has visited  so many  times that the transition probabilities for that state are  very close  to their true values in m. this definition is carefully balanced so that  so many  times is still poly normally bounded  yet  very close  suffices to meet the simulation requirements below. an important observation is that we cannot do balanced wandering indefinitely before at least one state becomes known: by the pigeonhole principle  we will soon start to accumulate accurate statistics at some state. 
　the most important construction of the analysis is the known-state mdp. if s is the set of currently known states  the known-state mdp is simply an mdp ms that is naturally induced on s by the fall mdp m. briefly  all transitions in m between states in s are preserved in ms  while all other transitions in m are ''redirected  in ms to lead to a single new  absorbing state that intuitively represents all of the unknown and unvisited states. although e1 does not have direct access to ms by virtue of the definition of the known states  it does have a good approximation ms   
　the ks analysis hinges on two central technical lemmas. the first is called the simulation lemma  and it establishes that ms has good simulation accuracy: that is  the expected t-step return of any policy in ms is close to its expected tstep return in ms. thus  at any time  ms is a useful partial model of m  for that part of m that the algorithm  knows  very well. 
　the second central technical lemma is the  explore or exploit  lemma. it states that either the optimal  t-step  policy 
1 
in m achieves its high return by staying  with high probability  in the set s of currently known states  or the optimal policy has significant probability of leaving s within t steps. most importantly  the algorithm can detect which of these two is the case; in the first case  it can simulate the behavior of the optimal policy by finding a high-return exploitation policy in the partial model ms and in the second case  it can replicate the behavior of the optimal policy by finding an exploration policy that quickly reaches the additional absorbing state of the partial model ms   thus  by performing two off-line planning computations on ms   the algorithm is guaranteed to find either a way to get near-optimal return for the next t steps  or a way to improve the statistics at an unknown or unvisited state within the next t steps. ks show that this algorithm ensures near-optimal return in time polynomial in n. 
1 	the dbn-e1 algorithm 
our goal is to derive a generalization of for dbn-mdps  and to prove for it a result analogous to that of ks - but with a polynomial dependence not on the number of states n  but on the number of cpt parameters l in the dbn model. our analysis closely mirrors the original  but requires a significant generalization of the simulation lemma that exploits the structure of a dbn-mdp  a modified construction of ms that can be represented as a dbn-mdp  and a number of alterations of the details. 
　like the original e1 algorithm  dbn-  will build a model of the unknown dbn-mdp on the basis of its experience  but now the model will be represented in a compact  factorized form. more precisely  suppose that our algorithm is in state x  executes action a  and arrives in state x'. this experience will be used to update all the appropriate cpt entries of our model - namely  all the estimates are updated in 
the obvious way  where as usual u  is the setting of 
in x. we will also maintain counts of the number of times  has been updated. 
　recall that a crucial element of the original e1 analysis was the notion of a known state. in the original analysis  it was observed that if n is the total number of states  then after o n  experiences some state must become known by the pigeonhole principle. we cannot hope to use the same logic here  as we are now in a dbn-mdp with an exponentially large number of states. rather  we must  pigeonhole  not on the number of states  but on the number of parameters required to specify the dbn-mdp. towards this goal  we will say that the cpt entry  is known if it has been visited 
 enough  times to'ensure that  with high probability 

we now would like to establish that if  for an appropriate choice of   all cpt entries are known  then our approximate dbn-mdp can be used to accurately estimate the expected return of any policy in the true dbn-mdp. this is the desired generalization of the original simulation lemma. as in the original analysis  we will eventually apply it to a generalization of the induced mdp ms  in which we deliberately restrict attention to only the known cpt entries. 

1 	the dbn-mdp simulation lemma 
let m and m be two dbn-mdps over the same state spaee with the same transition graphs for every action a  and with the same reward functions. then we say that m is an approximation of m if for every action a and node   in the transition graphs  for every setting u of   and for every possible value 

where  are the cpts of m and m  respectively. 
lemma 1: let m be any dbn-mdp over n state variables with ♀ cpt entries in the transition model and let m be an a-approximation of m  where 
then for any policy -
proof:  sketch  let us fix a policy  and state x. recall that for any next state x' and any action a  the transition probability factorizes via the cpts as 
where 	is the setting of 	. let us say that 
　　　　　contains a small factor if any of its cpt factors   is smaller than  note that a transition 
probability may actually be quite small itself  exponentially small in n  without necessarily containing a -small factor. 
　our first goal is to show that trajectories in m and m that cross transitions containing a -small cpt factor can be  thrown away  without much error. consider a random trajectory of t steps in m from state x following policy  it can be shown that the probability that such a trajectory will cross at least one transition . that contains a small factor is at most essentially  the probability that at any step  any particular  -small transition  cpt factor  will be taken by any particular variable is at most a simple union argument over the cpt entries and the t time steps gives the desired bound. therefore  the total contribution to 
ignore such trajectories for now. 
　the key advantage of eliminating -small factors is that we can convert additive approximation guarantees into multiplicative ones. let p be any path of length t. if all the relevant cpt factors are greater than and we let it can be shown that 
in other words  ignoring -small cpt factors  the distributions on paths induced by in m and m are quite similar. 
from this it follows that  for the upper bound 1 
the 
1 the lower bound argument is entirely symmetric. 
　returning to the main development  we can now give a precise definition of a known cpt entry. it is a simple application of chernoff bounds to show that provided the count i exceeds  has additive error at most a with probability at least 1 - 1. we thus say that this cpt entry is known if its count exceeds the given bound for the choice  specified by 
the dbn-mdp simulation lemma. the dbn-mdp simulation lemma shows that if alt cpt entries are known  then our approximate model m can be used to find a near-optimal policy in the true dbn-mdp m. 
　note that we can identify which cpt entries are known via the counts . thus  if we are at a state x for which at least one of the associated cpt entries 
is unknown  by taking action a we then obtain an experience that will increase the corresponding count . thus  in analogy with the original   as long as we are encountering unknown cpt entries  we can continue taking actions that increase the quality of our model - but now rather than increasing counts on a per-state basis  the dbn-mdp simulation lemma shows why it suffices to increase the counts on a per-cpt entry basis  which is crucial for obtaining the running time we desire. we can thus show that if we encounter unknown cpt entries for a number of steps that is polynomial in the total number ♀ of cpt entries and   there can no longer be any unknown cpt entries  and we know the true dbn-mdp well enough to solve for a near-optimal policy. 
　however  similar to the original algorithm  the real difficulty arises when we are in a state with no unknown cpt entries  yet there do remain unknown cpt entries elsewhere. then we have no guarantee that we can improve our model at the next step. in the original algorithm  this was solved by defining the known-state mdp ms  and proving the aforementioned  explore or exploit  lemma. duplicating this step for dbn-mdps will require another new idea. 
1 	the dbn-mdp  explore or exploit  lemma 
in our context  when we construct a known-state mdp  we must satisfy the additional requirement that the known-state mdp preserve the dbn structure of the original problem  so that if we have a planning algorithm for dbn-mdps that exploits the structure  we can then apply it to the known-state mdp1. therefore  we cannot just introduce a new  sink state  to represent that part of m that is unknown to us; we must also show how this  sink state  can be represented as a setting of the state variables of a dbn-mdp. 
　we present a new construction  which extends the idea of  known states  to the idea of  known transitions . we say that a transition component  is known if all of its cpt entries are known. the basic idea is that  while it is impossible to check locally whether a state is known  it is easy to check locally whether a transition component is known. 
　let t be the set of known transition components. we define the known-transition dbn-mdp mt as follows. the 
　　1  certain approaches to approximate planning in large mdps do not require any structural assumptions  kearns et al  1   but we anticipate that the most effective dbn-mdp planning algorithms eventually will. 
	kearns and poller 	us 

model behaves identically to m as long as only known transitions are taken. as soon as an unknown transition is taken for some variable xfi the variable xi takes on a new wandering value w  which we introduce into the model. the transition model is defined so that  once a variable takes on the value w  its value never changes. the reward function is defined so that  once at least one variable takes on the wandering value  the total reward is nonpositive. these two properties give us the same overall behavior that ks got by making a sink state for the set of unknown states. 
definition 1:let m be a dbn-mdp and let t be any subset of the transition components in the model. the induced dbn-mdp on t  denoted mt  is defined as follows: 

　with this definition  we can prove the analogue to the  explore or exploit  lemma  details omitted . 
lemma 1:let m be any dbn-mdp  let t be any subset of the transition components of m  ami let mr be the induced 

the probability that a walk of t steps following π will take at least one transition not in 
this lemma essentially asserts that either there exists a policy that already achieves near-optimal  global  return by staying only in the local model m r  or there exists a policy that quickly exits the local model. 
1 	putting it all together 
we now have all the pieces to finish the description and analysis of the dbn-e1 algorithm. the algorithm initially executes balanced wandering for some period of time. after some number of steps  by the pigeonhole principle one or more transition components become known. when the algorithm reaches a known state x - one where all the transition components are known-it can no longer perform balanced wandering. at that point  the algorithm performs approximate off-line policy computations for two different dbn-mdps. the first corresponds to attempted exploitation  and the second to attempted exploration. 
　let t be the set of known transitions at this step. in the attempted exploitation computation  the dbn-e1 algorithm would like to find the optimal policy on the induced dbnmdp mr. clearly  this dbn-mdp is not known to the algorithm. thus  we use its approximation m r   where the true 
1 
transition probabilities are replaced with their current approximation in the model. the definition of mr uses only the cpt entries of known transition components. the simulation lemma now tells us that  for an appropriate choice of a - a choice that will result in a definition of known transition that requires the corresponding count to be only polynomial in - the return of any p o l i c y i n mr is within e of its return in mr- we will specify a choice for e later  which in turn sets the choice of a and the definition of known state . 
　let us now consider the two cases in the  explore or exploit  lemma. in the exploitation case  there exists a policy  in mr such that   again  we will discuss the choice of r below.  from the simulation 
lemma  we have that 	 . our approximate planning algorithm returns a policy ' whose value in is guaranteed to be a multiplicative factor of at most 1 - away from the optimal policy in mr   thus  we are guaranteed that 
therefore  in the exploitation case  our approximate planner is guaranteed to return a policy whose value is close to the optimal value. 
　in the exploration case  there exists a policy   and therefore in  that is guaranteed to take an unknown transition within t steps with some minimum probability. our goal now is to use our approximate planner to find such a policy. in order to do that  we need use a slightly different construction . the transition structure of  is identical to that of m r . however  the rewards are now different. 
here  for each i = 1   . . .   k and 	  we have that i for other vectors c  we have that 
now let ' be die policy returned by our approximate planner on the dbn-mdp . it can be shown that the probability that a t-step walk following will take at least one unknown transition is at least 
　to summarize: our approximate planner either finds an exploitation policy in mr that enjoys actual return from our cur-
rent state x  or it finds an exploitation policy in mf that has probability at least 
of improving our statistics at an unknown transition in the next t steps. appropriate choices for e and r yield our main theorem  which we are now finally ready to describe. 
　recall that for expository purposes we have concentrated on the case of t-step average return. however  as for the original e1  our main result can be stated in terms of the asymptotic discounted and average return cases. we omit the details of this translation  but it is a simple matter of arguing that it suffices to set t to be either   discounted  or the mixing time of the optimal policy  average . 
theorem 1a:  main theorem  let m be a dbn-mdp with 
♀ total entries in the cpts  
    undiscounted case  let t be the mixing time of the policy achieving the optimal average asymptotic return u* in m. there exists an algorithm dbn-e  that  given access to a  -approximation planningalgorithm fordbn-

mdps  and given inputs   takes a number of actions mid computation time bounded by a polynomial tm    . . . and rmax  and with 
probability at least 1 - achieves total actual return exceeding u* -
   discounted case  let v* denote the value function for the policy with the optimal expected discounted return in m. there exists an algorithm dbn-e1 that  given access to a u-approximation planning algorithm for dbn-mdps  and given inputs and v*  takes a number of actions and computation time bounded by a 
some remarks: 
  the loss in policy quality induced by the approximate planning subroutine translates into degradation in the running time of our algorithm. 
  as with the original e1  we can eliminate knowledge of the optimal returns in both cases via search techniques. 
  although we have stated our asymptotic undiscounted average return result in terms of the mixing time of the optimal policy  we can instead give an  anytime  algorithm that  competes  against policies with longer and longer mixing times the longer it is run.  we omit details  but the analysis is analogous to the original e1 analysis.  this extension is especially important in light of the results of the following section  where we examine properties of mixing times in dbn-mdps. 
1 mixing time bounds for dbn-mdps 
as in the original e1 paper  our average case result depends on the amount of time t that it takes the target policy to mix. this dependence is unavoidable. if some of the probabilities are very small  so that the optimal policy cannot easily reach the high-reward parts of the space  it is unrealistic to expect the reinforcement learning algorithm to do any better. 
　in the context of a dbn-mdp  however  this dependence is more troubling. the size of the state space is exponentially large  and virtually all of the probabilities for transitioning from one state to the next will be exponentially small  because a transition probability is the product of n numbers that are   1 . indeed  one can construct very reasonable dbnmdps that have an exponentially long mixing time. for example  a dbn representing the markov chain of an ising model  jerrum and sinclair  1  has small parent sets  at most four parents per node   and cpt entries that are reasonably large. nevertheless  the mixing time of such a dbn can be exponentially large in n. 
　given that even  reasonable  dbns such as this can have exponential mixing times  one might think that this is the typical situation - that is  that most dbn-mdps have an exponentially long mixing time  reintroducing the exponential dependence on n that we have been trying so hard to avoid. we now show that this is not always the case. we provide a tool for analyzing the mixing time of a policy in a dbn-mdp  which can give us much better bounds on the mixing time. in particular  we demonstrate a ekes of dbn-mdps and associated policies fen* which we can guarantee rapid mixing. 
　note that any fixed policy in a dbn-mdp defines a markov chain whose transition model is represented as a 
dbn. we therefore begin by considering the mixing time of a pure dbn  with no actions. we then extend that analysis to the mixing rate for a fixed policy in a dbn-mdp. 
　our bounds on mixing times make use of the coupling method  lindvall  1 . the idea of the coupling method is as follows: we run two copies of the markov chain in parallel  from different starting points. our goal is to make the states of the two processes coalesce. intuitively  the first time the states of the two copies are the same  the initial states have been  forgotten   which corresponds to the processes having mixed. 
　more precisely  consider a transition matrix q over some state space 1. let q* be a transition matrix over the state space sxs  such that if is the markov 
chain for q*  then the separated markov chains and  both evolve according to q. let r be the random variable that represents the coupling time-the smallest m for which . the following lemma establishes the correspondence between mixing and coupling times. 

　thus  to show that a markov chain is e-mixed by some time m  we need only construct a coupled chain and show that the probability that this chain has not coupled by time m decreases very rapidly in m. 
　the coupling method allows us to construct the joint chain over  in any way that we want  as long as each of the two chains in isolation has the same dynamics as the original markov chain q. in particular  we can correlate the transitions of the two processes  so as to make their states coincide faster than they would if each was picked independently of the other. that is  we choose to be equal to each other whenever possible  subject to the constraints on the transition probabilities. more precisely  let 

of this event if the two processes were independent  which is the product of these two numbers rather than their minimum. overall  by correlating the two processes as much as possible  and considering the worst case over the current state 
	kearns 	amd 	k1uer 1 

of the process  we can guarantee that  at every step  the two processes couple with probability at least 
this quantity represents the amount of probability mass that any two transition distributions are guaranteed to have in common. it is called the dobrushin coefficient  and is the contraction rate for -norm  dobrushin  1  in markov chains. 
now  consider a dbn over the state variables x = 
           . as above  we create two copies of the process  letting denote the variables in the first component of the coupled markov chain  and denote those in the second component. our goal is to construct a markov chain over y  z such that both y and z separately have the same dynamics as x in the original dbn. 
　our construction of the joint markov chain is very similar to the one used above  except that will now choose the transition of each variable pair and so as to maximize the probability that they couple  assume the same value . as above  we can guarantee that 
this coefficient was defined by  boyen and koller  1  in their analysis of the contraction rate of dbns. note that fa depends only on the numbers in a single cpt of the dbn. assuming that the transition probabilities in each cpt are not too extreme  the probability that any single variable couples will be reasonably high. 
　unfortunately  this bound is not enough to show that all of the variable pairs couple within a short time. the problem is that it is not enough for two variables ' and to couple  as process dynamics may force us to decouple them at subsequent time slices. to understand this issue  

our sampling process may be forced to give them different values  decoupling them again. 
　as this example clearly illustrates  it is not enough for a variable pair to couple momentarily. in order to eventually couple the two processes as a whole  we need to make each variable pair a stable pair - i.e.  we need to guarantee that our sampling process can keep them coupled from then on. in our example  the pair is stable as soon as it first couples. and once is stable  then will also be stable as soon as it couples. however  if couples while is not yet stable  then the sampling process cannot guarantee stability. 
1 	machine learning 
　in general  a variable pair can only be stable if their parents are also stable. so what happens if we add the edge . 
to our transition model  in this case  neither can stabilize in isolation. they can only stabilize if 
they couple simultaneously. 
this discussion leads to the following definition. 
definition 1:consider a dbn over the state variables 
 the dependency graph v for the dbn is a directed cyclic graph whose nodes are x1 ...  xn and where there is a directed edge from to if there is an edge in the transition graph of the dbn from x to 
hence  there is a directed path from - in v iff influences .  for some . we assume that the transition graph of the dbn always has arcs so that the every node in v has a self-loop. 
　let   ...  be the maximal strongly connected components in v  sorted so that if t   j  there are no directed edges from . our analysis will be based on stabilizing the r 's in succession.  we note that we provide only a rough 
bound; a more refined analysis is possible.  let 
	. assume that 	t1 ... have all stabi-
lized by time t. in order for t  to stabilize  all of the variables need to couple at exactly the same time. this event happens at time t with probability as soon as t stabilizes  we can move on to stabilizing . when all the ti have stabilized  we are done. 
theorem sa:for any  1  the markov chain corresponding to a dbn as described above is e-mixed at time m provided 

thus  the mixing time of a dbn grows exponentially with the size of the largest component in the dependency graph  which may be significantly smaller than the total number of variables in a dbn. indeed  in two real-life dbns - bat  pprbes et al  1  with ten state variables  and water  jensen et al.  1  with eight - the maximal cluster size is 1. 
　it remains only to extend this analysis to dbn-mdps  where we have a policy  our stochastic coupling scheme must now deal with the fact that the actions taken at time t in the two copies of the process may be different. the difficulty is that different actions at time t correspond to different transition models. if a variable has a different transition model in different transition graphs .   it will use a different transition distribution if the action is not the same. hence cannot stabilize until we are guaranteed that the same action is taken in both copies. that is  the action must also stabilize. the action is only guaranteed to have stabilized when all of the variables on which the choice of action can possibly depend have stabilized. otherwise  we might encounter a pair of states in which we are forced to use different actions in the two copies. 
　we can analyze this behavior by extending the dependency graph to include a new node corresponding to the choice of action. we then see what assumptions allow us to bound the set of incoming and outgoing edges. we can then use 

the same analysis described above to bound the mixing time. the outgoing edges correspond to the effect of an action. in many processes  the action only directly affects the transition model of a small number of state variables in the process. in other words  for many variables   we have that 
and .  are the same for all a. in this case  
the new action node will only have outgoing edges to the remaining variables  those for which the transition model might differ . we note that such localized influence models have a long history both for influence diagram  howard and matheson  1  and for dbn-mdps  boutilier et al  1 . 
　now  consider outgoing edges. in general  the optimal policy might well be such that the action depends on every variable. however  the mere representation of such a policy may be very complex  rendering its use impractical in a 
　dbn-mdp with many variables. therefore  we often want to restrict attention to a simpler class of policies  such as a small finite state machine or a small decision tree. if our target policy is such that the choice of action only depends on a small number of variables  then there will only be a small number of incoming edges into the action node in the dependency graph. 
　having integrated the action node into the dependency graph  our analysis above holds unchanged. the only difference from a random variable is that we do not have to include the action node when computing the size of the ti- that contains it  as we do not have to stochastically make it couple; rather  it couples immediately once its parents have coupled. 
　finally  we note that this analysis easily accommodates dbn-mdps where the decision about the action is also decomposed into several independent decisions  e.g.  as in  meuleau et al  1  . different component decisions can influence different subsets of variables  and the choice of action in each one can depend on different subsets of variables. each decision forms a separate node in the dependency graph  and can stabilize independently of the other decisions. 
　the analysis above gives us techniques for estimating the mixing rate of policies in dbn-mdps. in particular  if we want to focus on getting a good steady-state return from dbn-e1 in a reasonable amount of time  this analysis shows us how to restrict attention to policies that are guaranteed to mix rapidly given the structure of the given dbn-mdp. 
1 	conclusions 
structured probabilistic models  and particularly bayesian networks  have revolutionized the field of reasoning under uncertainty by allowing compact representations of complex domains. their success is built on the fact that this structure can be exploited effectively by inference and learning algorithms. this success leads one to hope that similar structure can be exploited in the context of planning and reinforcement learning under uncertainty. this papa:  together with the recent work on representing and reasoning with factored mdps  boutilier et al  1   demonstrate that substantial computational gains can indeed be obtained from these compact  structured representations. 
　this paper leaves many interesting problems unaddressed. of these  the most intriguing one is to allow the algorithm to learn the model structure as well as the parameters. the recent body of work on learning bayesian networks from data  heckerman  1  lays much of the foundation  but the integration of these ideas with the problems of exploration/exploitation is far from trivial. 
acknowledgements 
we are grateful to the members of the dags group for useful discussions  and particularly to brian milch for pointing out a problem in an earlier version of this paper. the work of daphne koller was supported by the aro under the muri program  integrated approach to intelligent systems   by onr contract n1-c-1 under darpa's hpkb program  and by the generosity of the powell foundation and the sloan foundation. 
