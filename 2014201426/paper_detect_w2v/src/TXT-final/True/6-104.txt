 
in multiagent environments  forms of social learning such as teaching and imitation have been shown to aid the transfer of knowledge from experts to learners in reinforcement learning  rl . we recast the problem of imitation in a bayesian framework. our bayesian imitation model allows a learner to smoothly pool prior knowledge  data obtained through interaction with the environment  and information inferred from observations of expert agent behaviors. our model integrates well with recent bayesian exploration techniques  and can be readily generalized to new settings. 
1 introduction 
reinforcement learning is a flexible  yet computationally challenging paradigm. recent results demonstrating that under certain assumptions the sample complexity of reinforcement learning is polynomial in the number of problem states  kearns and singh  1  are tempered by the sober fact that the number of states is generally exponential in the number of the attributes defining a learning problem. with recent interest in building interacting autonomous agents  reinforcement learning is increasingly applied to multiagent tasks  a development which only adds to the complexity of learning  littman  1; hu and wellman  1 . in this paper  we examine multi-agent reinforcement learning under the assumption that other agents in the environment are not merely arbitrary actors  but actors  like me . that is  the other agents may have similar action capabilities and similar objectives. this assumption radically changes the optimal learning strategy. information about other agents  like me  can give the learning agent additional information about its own capabilities and how these capabilities relate to its own objectives. a number of techniques have been developed to exploit this  including imitation  demiris and hayes  1; mataric  1   learning by watching  kuniyoshi et al.  1   teaching or 
programming by demonstration  atkeson and schaal  1  behavioral cloning  sammut et al.  1   and inverse rein-
forcement learning  ng and russell  1 . 
　learning by observation of other agents has intuitive appeal; however  explicit communication about action capabil-
ities between agents requires considerable infrastructure: a 
1 
craig boutilier university of toronto toronto  on  canada m1s 1 cebly cs. toronto. edu 
communication channel  a sufficiently expressive representation language  a transformation between possibly different agent bodies  and an incentive to communicate. in dynamic  competitive domains  such as web-based trading  it is unrealistic to expect all agents to be designed with compatible representations and altruistic intentions. observation-based techniques  in which the learning agent observes only the outward behaviors of another agent  can reduce the need for explicit communication. implicit communication through passive observations has been implemented as implicit imitation  price and boutilier  1; 1 . in this model  the effects of other agents' action choices on the state of the environment can be observed  but the internal state of other agents and their action control signals are not observable. independent exploration on the part of the observer is used to adapt knowledge implicit in observations of other agents to the learning agent's own needs. unlike classic imitation models  the learner is not required to explicitly duplicate the behavior of other agents. 
　in this paper  we recast implicit imitation in a bayesian framework. this new formulation offers several advantages over existing models. first it provides a more principled  and more elegant approach to the smooth pooling of information from the agent's prior beliefs  its own experience and the observations of other agents  e.g.  it eliminates the need for certain ad hoc tuning parameters in current imitation models . second  it integrates well with state-of-the-art exploration techniques  such as bayesian exploration. finally  the bayesian imitation model can be extended readily to partiallyobservable domains  though the derivation and implementation are considerably more complex and are not reported here. 
1 	background 
we assume a reinforcement learning  rl  agent is learning to control a markov decision processes  mdp  with finite state and action sets s ao  reward function s  r  and dynamics d. the dynamics d refers to a set of transition distributions  and rewards  are subscripted to distinguish them from those of other agents  see below . we assume throughout that the agent knows  but not the dynamics d of the mdp  thus we adopt the  automatic programming  perspective   and has the objective of maximizing discounted reward over an infinite horizon. any of a number of rl techniques can be used to learn an optimal policy we focus here on model-
multiagent systems 

based rl methods  in which the observer maintains an estimated mdp 	based on the set of experiences 
obtained so far. at each stage  or at suitable inter-
vals  this mdp can be solved exactly  or approximately using techniques such as prioritized sweeping  moore and atkeson  1 . since r1 is known  we focus on learning dynamics. 
　bayesian methods in model-based rl allow agents to incorporate priors and explore optimally. in general  we employ a prior density p over possible dynamics d  and update it with each data point  s  a  t . letting h1  = denote the  current  state history of the observer  and a1 = be the action history  we use the poste-
to update the action q-values  which are 
used in turn to select actions. the formulation of dearden et al 1 renders this update tractable by assuming a convenient prior: p is the product of local independent densities for each transition distribution and each density is a dirichlet with parameters to model we require one parameter for each possible successor state . update of a dirichlet is straightforward: given prior and data vector  where is the number of observed transitions from s to t under a   the posterior is given by parameters thus the posterior in eq. 1 can be factored into posteriors over local families: 
 1  
where is the subset of history composed of transitions from state s due to action a  and the updates themselves are simple dirichlet parameter updates. 
　the bayesian approach has several advantages over other approaches to model-based rl. first  it allows the natural incorporation of priors over transition and reward parameters. second  approximations to optimal bayesian exploration can take advantage of this approach  and the specific structural assumptions on the prior discussed above  dearden et al.  1 . 
1 	bayesian imitation 
in multiagent settings  observations of other agents can be used in addition to prior beliefs and personal experience to improve an agent's model of its environment. these observations can have enormous impact when they provide information to an agent about parts of the state space it has not yet visited. the information can be used to bias exploration towards the most promising regions of state space and thereby reduce exploration costs and speed convergence dramatically. 
　the flexibility of the bayesian formulation leads to an elegant and principled mechanism for incorporating these observations into the agent's model updates. following price and boutilier 1  we assume two agents  a knowledgeable mentor 1i and a naive observer o  acting simultaneously  but independently  in a fixed environment.1 like the observer  the mentor too is controlling an mdp with the same underlying state space and dynamics  that is  for any action  the dynamics are identical . the assumption that the two agents have the same state space is not critical: more important is that there is some analogical mapping 

we assume that the agents are performing non-interacting tasks. 
multiagent systems 

figure 1: dependencies among model and evidence sources 
between the two  nehaniv and dautenhahn  1 . wc assume full observability of the mentor's state space; but we do not assume the observer can identify the actions taken by the mentor-it simply observes state transitions. 
　we make two additional assumptions regarding the mentor's dynamics: the mentor implements a stationary policy which induces a markov chain = 
pr ; and for each action taken by the mentor  there exists an action such that the distributions and are the same. this latter assumption is the homogeneous action assumption and implies that the observer can duplicate the mentor's policy.1 as a consequence we can treat the dynamics d as the same for both agents. note that we do not assume that the learner knows a priori which of its actions duplicates the mentor's  for any given state s   nor that the observer wants to duplicate this policy  as the agents may have different objectives . 
since the learner can observe the mentor's transitions 
 though not its actions directly   it can form estimates of the mentor's markov chain  along with estimates of its own mdp 
 transition probabilities and reward function . in  price and boutilier  1   this estimate is used to augment the normal 
bellman backup  treating the observed distribution pr s .  as a model of an action available to the observer. imitators using augmented backups based on their observations of a mentor often learn much more quickly  especially if the mentor's reward function or parts of its policy overlap with that of the observer. techniques like interval estimation  kaelbling  1  can be used to suppress augmented backups where their value has low  confidence.  
　in the bayesian approach  the observer incorporates observations of the mentor directly into an augmented model of its environment. let hm denote the history of mentor state transitions observed by the learner. as above  h1 and a1 represents the observer's state and action history respectively. figure 1 illustrates the sources of information available to the imitator with which to constrain its beliefs about z   and their probabilistic dependence. while the observer knows its own action history  a1 it has no direct knowledge of the actions taken by the mentor: at best it may have  often weak  prior knowledge about the mentor's policy  the learner's beliefs over d can then be updated w.r.t. the joint observations: 

1
　　the homogeneous action assumption can be relaxed  price and boutilier  1 . essentially  the observer hypothesizes that violations can be  repaired  using a local search for a short sequence of actions that roughly duplicates a short subsequence of the mentor's actions. if a repair cannot be found  the observer discards the mentor influence  at this point in state space . 
1 
　we assume that the prior p d  has the factored dirichlet form described above. without mentor influence  a learner can maintain its posterior in the same factored form  updating each component of the model p  independently. unfor-
tunately  complications arise due to the unobservability of the mentor's actions. we show  however  that the model update in eq. 1 can still be factored into convenient terms. 
　we derive a factored update model for p  describing the dynamics at state s under action a by considering two cases. in case one  the mentor's unknown action could be different than the action a. in this case  the model factor would be independent of the mentor's history  and we can employ the standard bayesian update eq. 1 without regard for the mentor. in case two  the mentor action is in fact the same as the observer's action a. then the mentor observations are relevant to the update of p  
let be the prior parameter vector for p   and denote the counts of observer transitions from state s 
via action a  and the counts of the mentor transitions from state s. the posterior augmented model factor density 
p is then a dirichlet with parameters that is  we simply update with the sum of the observer and mentor counts: 

　since the observer does not know the mentor's action we compute the expectation w.r.t. these two cases: 

this allows a factored update of the usual conjugate form  but where the mentor counts are distributed across all actions  weighted by the posterior probability that the mentor's policy chooses that action at state  
　with a mechanism to calculate the posterior over the mentor's policy  eq. 1 provides a complete factored update rule for incorporating evidence from observed mentors by a bayesian model-based rl agent. to tackle this last problem-that of updating our beliefs about the mentor's policy-we have: 

　if we assume that the prior over the mentor's policy is factored in the same way as the prior over models-that is  we 
　　1 this assumes that at least one of the observer's actions is equivalent to the mentor's  but our model can be generalized to the heterogeneous case. an additional term is required to represent  none of the above . 1 
have independent distributions over am for each s- this update can be factored as well  with history elements at state s being the only ones relevant to computing the posterior over   we still have the difficulty of evaluating the integral over models. following dearden et al. 1  we tackle this by sampling models to estimate this quantity. specifically  we sample models from the factored dirichlet p 
of 
　we can combine the expression for expected model factor probability in eq. 1 with our expression for mentor policy likelihood in eq. 1 to obtain a tractable algorithm for updating the observer's beliefs about the dynamics model d based on its own experience  and observations of the mentor.1. 
　a bayesian imitator thus proceeds as follows. at each stage  it observes its own state transition and that of the mentor  using each to update its density over models as just described. efficient methods are used to update the agent's value function. using this updated value function  it selects a suitable action  executes it  and repeats the cycle. 
　like any rl agent  an imitator requires a suitable exploration mechanism. in the bayesian exploration model  dearden et al  1   the uncertainty about the effects of actions is captured by a dirichlet  and is used to estimate a distribution over possible q-values for each state-action pair.1 notions such as value of information can then be used to approximate the optimal exploration policy. this method is computationally demanding  but total reward including reward captured during training is usually much better than that provided by heuristic techniques. bayesian exploration also eliminates the parameter tuning required by methods like -greedy  and adapts locally and instantly to evidence. these facts makes it a good candidate to combine with imitation. 
1 experiments 
in this section we attempt to empirically characterize the applicability and expected benefits of bayesian imitation through several experiments. using domains from the literature and two unique domains  we compare bayesian imitation to non-bayesian imitation  price and boutilier  1   and to several standard model-based rl  non-imitating  techniques  including bayesian exploration  prioritized sweeping and complete bellman backups. we also investigate how bayesian exploration combines with imitation. 
first  we describe the agents used in our experiments. the 
oracle employs a fixed policy optimized for each domain  
   1 sampling is efficient as only one local model needs to be resampled at any time step. 
   1 scaling techniques such as those used in hmm's may be required to prevent underflow in the term  in eq. 1. 
   1 the q-value distribution changes very little with each update and can be repaired efficiently using prioritized sweeping. in fact  the bayesian learner is cheaper to run than a full bellman backup over all states. 
multiagent systems 

providing both a baseline and a source of expert behavior for the observers. the egbs agent combines greedy exploration  eg  with a full bellman backup  i.e.  sweep  at each time step. it provides an example of a generic modelbased approach to learning. the egps agent is a model-based rl agent  using e-greedy  eg  exploration with prioritized sweeping  ps . egps use fewer backups  but applies them where they are predicted to do the most good. egps does not have a fixed backup policy  so it can propagate value multiple steps across the state space in situations where egbs would not. the be agent employs bayesian exploration  be  with prioritized sweeping for backups. bebi combines bayesian exploration  be  with bayesian imitation  bi . egbi combines c-greedy exploration  eg  with bayesian imitation  bl . 
	figure 1: flagworld domain 	the egnb1 agent combines e-greedy exploration with non-
bayesian imitation. 
　in each experiment  agents begin at the start state. the agents do not interact within the state space. when an agent achieves the goal  it is reset to the beginning. the other agents continue unaffected. each agent has a fixed number of steps  which may be spread over varying numbers of runs  in each experiment. in each domain  agents are given locally uniform priors  i.e.  every action has an equal probability of resulting in any of the local neighbouring states; e.g.  in a grid world 
     there are 1 neighbours . imitators observe the expert oracle agent concurrently with their own exploration. results are reported as the total reward collected in the last 1 steps. this sliding window integrates the rewards obtained by the agent making it easier to compare performance of various agents. during the first 1 steps  the integration window starts off empty causing the oracle's plot to jump from zero to optimal in the first 1 steps. the bayesian agents use 1 sampled mdps for estimating q-value distributions and 1 samples for estimating the mentor policy from the dirichlet distribution. exploration rates for e-greedy agents were tuned for each experfigure 1: flag world results  1 runs  imental domain. 
　our first test of the agents was on the  loop  and  chain  examples  designed to show the benefits of bayesian exploration   taken from  dearden et al  1 . in these experiments  the imitation agents performed more or less identically to the optimal oracle agent and no separation could be seen amongst the imitators. 
　using the more challenging  flagworld  domain  dearden et al.  1   we see meaningful differences in performance amongst the agents. in flagworld  shown in figure 1  the agent starts at state s and searches for the goal state gl. the agent may pick up any of three flags by visiting states f l   
f1 and f1. upon reaching the goal state  the agent receives 
1 point for each flag collected. each action  n e s w  succeeds with probability 1 if the corresponding direction is clear  and with probability 1 moves the agent perpendicular to the desired direction. figure 1 shows the reward collected in over the preceding 1 steps for each agent. the oracle demonstrates optimal performance. the bayesian imitator using bayesian exploration  bebi  achieves the quickest convergence to the optimal solution. the e-greedy bayesian 
figure 1: flag world moved goal  1 runs  imitator  egbi  is next  but is not able to exploit information locally as well as bebi. the non-bayesian imitator  eg-
nbi  does better than the unassisted agents early on but fails 
multiagent systems 	1 

to find the optimal policy in this domain. a slower exploration rate decay would allow the agent to find the optimal policy  but would also hurt its early performance. the non-imitating bayesian explorer fares poorly compared to the bayesian imitators  but outperforms the remaining agents  as it exploits prior knowledge about the connectivity of the do-
main. the other agents show poor performance  though with high enough exploration rates they would converge eventually . we conclude that bayesian imitation makes the best use of the information available to the agents  particularly when combined with bayesian exploration. 
　we altered the flag world domain so that the mentor and the learners had different objectives. the goal of the expert oracle remained at location g1  while the learners had goal location g1  figure 1 . figure 1 shows that transfer due to imitation is qualitatively similar to the case with identical rewards. we see that imitation transfer is robust to modest differences in mentor and imitator objectives. this is readily explained by 
	figure 1: tutoring domain results  1 runs  	the fact that the mentor's policy provides model information 
over most states in the domain  which can be employed by the observer to achieve its own goals. 
　the tutoring domain requires agents to schedule the presentation of simple patterns to human learners in order to minimize training time. to simplify our experiments  we have the agents teach a simulated student. the student's performance is modeled by independent  discretely approximated  exponential forgetting curves for each concept. the agent's action will be its choice of concept to present. the agent receives a reward when the student's forgetting rate has been reduced below a predefined threshold for all concepts. presenting a concept lowers its forgetting rate  leaving it unpresented increases its forgetting rate. our model is too simple to serve as a realistic cognitive model of a student  but provides a qualitatively different problem to tackle. we note that the action space grows linearly with the number of concepts  and the state space exponentially. 
	figure 1: no-south domain 	the results presented in figure 1 are based on the presen-
tation of 1 concepts to a student.  egbs has been left out as it is time-consuming and generally fares poorly.  we see that all of the imitators learn quickly  but with the bayesian imitators bebi and egbi outperforming egnbi  which converges to a suboptimal policy .1 the generic bayesian agent  be  also chooses a suboptimal solution  which often occurs in be agents if its priors prevent adequate exploration . thus  we see that imitation mitigates one of the drawbacks of bayesian exploration: mentor observations can be used to overcome misleading priors. we see also that bayesian imitation can also be applied to practical problems with factored state and action spaces and non-geometric structure. 
　the next domain provides further insight into the combination of bayesian imitation and bayesian exploration. in this grid world  figure 1   agents can move south only in the first column. in this domain  the optimal oracle agent proceeds due south to the bottom corner and then east across to the goal. the bayesian explorer  be  chooses a path based on its prior beliefs that the space is completely connected. the agent can 
figure 1: no south results  1 runs  
　　increasing exploration allows egnbi to find the optimal policy  but further depresses short term performance. 
1 	multiagent systems 

easily be guided down one of the long  tubes  in this scenario  only to have to retrace it steps. the results in this domain  shown in figure 1  clearly differentiate the early performance of the imitation agents  bebi  egb1 and egnb1  from the bayesian explorer  be  and other independent learners. the initial value function constructed from the learner's prior beliefs about the connectivity in the grid world lead it to over-value many of the states that lead to a dead end. this results in a costly misdirection of exploration and poor performance. we see that the ability of the bayesian imitator bebi to adapt to the local quality of information allows it to exploit the additional information provided by the mentor more quickly than agents using generic exploration strategies like e-greedy. again  mentor information is used to great effect to overcome misleading priors. 
1 conclusions 
bayesian imitation  like the non-bayesian implementation of implicit imitation  accelerates reinforcement learning in the 
presence of other agents with relevant knowledge without requiring either explicit communication with or the cooperation of these other agents. the bayesian formulation is built on an elegant pooling mechanism which optimally combines prior knowledge  model observations from the imitator's own experience and model observations derived from other agents. the combination of bayesian imitation with bayesian exploration eliminates parameter tuning and yields an agent that rapidly exploits mentor observations to reduce exploration and increase exploitation. in addition  imitation often overcomes one of the drawbacks of bayesian exploration  the possibility of converging to a suboptimal policy due to misleading priors. bayesian imitation can easily be extended to multiple mentors  and though we did not present the derivation here  it can also be extended to partially observable environments with known state spaces. though the bayesian formulation is difficult to implement directly  we have shown that reasonable approximations exist that result in tractable algorithms. 
   there are several very promising areas of future research that can benefit from the current formulation of bayesian imitation. one obvious need is to extend the model to the heterogeneous action setting by incorporating the notions of feasibility testing and repair described in  price and boutilier  1  . we are particularly excited by the prospects of its generalization to richer environmental and interaction models. we have also derived one possible mechanism for using the bayesian approach in domains with continuous attributes. we hope to extend this work to include methods for discovering correspondences between the state and action spaces of various agents. we also plan to introduce game-theoretic considerations into imitation so that agents can learn solutions to interacting tasks from experts and reason about both the rewardoriented aspects of their action choices as well as the information it reveals to others. 
acknowledgements 
this research was supported by the natural sciences and engineering research council and iris. 
multiagent systems 
