 
we introduce our research approach to investigating real world intelligence by building 'remote-brained robots'. the key idea is that of interfacing ai systems with real-world behaviors through wireless technology. in this approach the robot system is designed to have the brain and body separate  both conceptually and physically. it allows us to tie ai directly to the world  enabling the verification of high-level ai techniques which could previously only be used in simulation. for robotics research  this approach opens the way to the use of large-scale powerful parallel computers. for ai  this approach allows experiments with realistic agents  an essential step to the application of ai in the real world. in this presentation we introduce the remote-brained approach  describe some remote-brained robots and discuss experiments. 
1 	introduction 
robots of the future will act in the real world. to realize robots with the common sense to do this will  it is generally believed  require massively parallel processing. this is a problem for those of us who want to do experiments with robots in the real world today - it is hard to build active  limber  situated robots when they have to carry along heavy brains. our answer is  remote-brained robots   inaba  1 . 
　a remote-brained robot does not bring its own brain with the body. it leaves the brain in the mother environment  by which we mean the environment in which the brain's software is developed  and talks with it by wireless links. in this 'remote-brained approach'  we can design and implement a robot with both a large-scale powerful brain and a lightweight limber body. this opens up the possibility of working with intelligent behaviors of a multi-limbed robot  such as a whole-bodied humanoid  as yet not much studied. 
　in this framework  a body is defined as the real interface for a brain to the outer world. the channel from the body to the brain defines all the input to the brain. 
the brain is defined as a program which receives the input containing all the sensor information and sends the output for actuation to the body. the research task regarding intelligent behaviors with remote-brained robots is to design and implement a brain program based on a well defined interface between the brain and the body. the remote-brained approach divides the timeconsuming work of building an integrated robot for intelligence research into interdependent work on body  interface  and brain. 
　to put the brain inside the mother environment provides us with convenience and important advantages. it allows us to observe the internal states of the brain during experiments. if the robot is a mobile robot  this facility allowing us to probe the brain behaviors is indispensable for effective debugging of brain software. the brain software is developed in the mother environment which is inherited over generations. it can benefit directly from the mother's 'evolution'  meaning that the software gains power easily when the mother is upgraded to a more powerful computer. 
　the research topics with remote-brained robots include the mother environment for raising brains  brain software based on different architectures  brain implementation on parallel computers  manipulation in handling flexible objects  multi-robot coordination  reconfigurable robot system with distributed modular bodies  adaptive behaviors of quadruped robots  apelike robots and humanoids  and so on. figure 1 shows the idea of the remote-brained robots. this paper introduces some of them with real experiments. 
1 	the inherited mother environment 
in 1 we built a hand-eye system  cosmos  cognitive sensor motor operation systems/studies   which could perform rope handling tasks  such as insertrope-into-ring and tie-knot  using a single arm and a camera with a stereo viewer  inoue and inaba  1; inoue  1 . our rope handling experiments allowed us to study some of the essential functions for the integrated hand-eye system. our computational environment was based on a 1 bit mini-computer with a frame memory and gpib interface. the visual guidance program was written in terms of look-and-move sequences. while the 
	inaba 	1 


figure 1: the idea of remote-brained robot 

figure 1: the environment of our remote-brained robots 
invited speakers 


figure 1: hardware configuration of our remote-brained robotic environment 

robot arm moved  the vision system could not watch the scene because of the limited power of the mini-computer. this forced us to develop attention control strategies based on local image processing - by projecting the predicted situation resulting from arm motions  we could determine in advance where vision should watch  making it much more efficient. 
　as the successor to the cosmos system  we built cosmos-ii  using networked workstations and a vision server. the vision server  inaba and inoue  1  provides the workstations with visual functions such as multiple parallel-processed visual windows  inoue and mizoguchi  1  and high-speed line segment detectors 
 moribe and others  1   the programming language was the object-oriented lisp  euslisp  matsui and inaba  1   which was designed specifically for the needs of robot programming on workstations. this allowed us to demonstrate a new  language-free  robot programming approach   teaching-by-showing   kuniyoshi et al.  1 . these experiments also showed that multiple flexible visual trackers are essential for observing flexible structured objects  such as the human hand  and for implementing ongoing action recognition  kuniyoshi and inoue  1 . 
　we found  however  that a system based on unix workstations was not adequate for intensive sensor interaction. after cosmos-ii  we have extended the real world interface of the workstation with multiple real-time processors  in the form of a transputer. a transputer is a micro-processor which can communicate with other transputers through four serial high speed communication links. it supports programming facilities for parallel process execution. we have developed a new tracking vision system  inoue et a/.  1  implemented it using the transputer and a correlation chip  and a general-purpose transputer board for motion control. all the workstations of the cosmos-ii system now have access to such a 'real world interface'. we can connect such vision systems and motion controllers to every workstation and build flexible configurations with transputer networks. this programming environment has allowed us a variety of vision-based behaviors  inoue  1 . 
　the remote-brained approach aims not only to keep such extensibility for both the robot system and the research environment but also to provide real robots with mobility free from the workstations. the remote-brained robot is intended to freely support growing the research platform and to utilize the software inherited from previous versions and to be developed in future extensions of cosmos. 
　the research environment for the remote-brained approach inherits the software developed in the cosmosii environment  with many improvements. the robot vision server which includes high speed line-segment finders and multiple visual trackers for attention control is replaced with multiple transputer vision systems. euslisp  matsui and inaba  1  now includes powerful facilities for robot programming: modeling robot body structures  model-based object recognition  network interface over workstations  transputer interface  windowbased programming  parallel programming with multithreads  matsui and sekiguchi  1  and so on. 
1 	the remote-brained robot system 
the advantage of the remote-brained robot approach is that we can use any computer as the brain hardware and apply any software platform for developing the brain software. the design and the performance of the inter-
	inaba 	1 
face between brain and body is the key in the remotebrained approach. our current implementation adopts a fully remote brained approach  which means the body has no computer onboard. the key to simplifing the configuration for multisensor integration in the remotebrained approach is the method for multiplexing different kinds of sensor signals on the wireless link. a method for integrating multisensor signals into a video signal is to use an image superimposer which multiplexes several sensor signals into a two dimensional sensor image lnaba et a/.  1 . the sensor image is defined as all the input 
from the outer world to the brain. 
　figure 1 shows our environment when we started the remote-brained robot project in 1. nine different robots are presented in this figure. the tower in the center is the brain interface including a parallel vision system and a motion controller. the cubic box to the left of the tower is the box of transputers which includes 1 t1 transputers. it is connected to the tower through transputer links. the tower and the transputer box are connected to the workstations. 
　figure 1 shows the hardware configuration of the system built in our laboratory. the brain base consists of three blocks of vision subsystems and the motion control system. each block of the vision system can receive video signals from cameras on robot bodies through the wireless video link. two of the vision subsystems are parallel sets each consisting of eight vision boards. the vision board consists of a transputer augmented with a special lsi chip which performs local image block matching  inoue et a/.  1 . it can provide a robot body with real time image processing capabilities such as pattern matching  motion detection  and object tracking. the motion control subsystem can handle up to 1 actuators through 1 wave bands and send the reference values to all the actuators every 1msec. 
　in remote-brained robot approach  the body design is a question of designing mechanical hardware using modular components. the key idea in body development is to use broadly available components to build robot body. when we try to acheive new behaviors with a real robot and it breaks  we can repair it by just replacing the broken part. the main electric components of the body of a remote-brained robot are joint servo modules  a control signal receiver  a battery for actuators  a camera  a video transmitter  and the battery for the vision systems. the servo module  receiver  transmitter are available for radio hobby toys. the servo module is a geared module which includes an analog servo circuit and receives a position reference value from the motion receiver. 
　as the brain base system consists of workstation and transputer networks  we have to coordinate the non realtime system software on the workstation with the realtime system software on transputers. the system requires a flexible connection between the workstation and the transputer network. we have designed and implemented the software platform where we can test any type of connections between them kagami et a/.  1 . 

figure 1: reconflgurable robot environment  a  overview   b  a view of an eye module   c  the simulator in the mother environment 
1 	multi-robots applications 
the wireless connection of a remote-brained robot allows us to build an environment of multi-robots. 
1 a m u l t i robot application on a super parallel computer 
in the japanese national project on fifth generation computer systems  logic programming languages on parallel computers have been investigated intensively. an aim of remote-brained robotics is to allow such socalled advanced computer technology to be incorporated into and allow the development of advanced robots. 
　we have used one of the committed choice languages  fleng  nilsson and tanaka  1   which arose from the debate and discussion of the fifth generation project. this language was designed as a kernel language for a fine-grained parallel computer  the pie1  koike and tanaka  1 . 
　what we first wanted to use a parallel computer for was brain software for multiple robots at work. we took soccer as the example. each robot could have its own brain sofware  and each change in the situation was handled as an event for each robot body  m.inaba et al.  1 . 
　a major advantage of remote-brained robots is that things based on simulation of the natural world can be ported to real robots' brain software without change. 


figure 1: the hand-eye mobile robots handling a flexible rope 
thus the brain  operating in the real-world-simulating mother-environment  can do well. 
1 coordination of hand modules and eye modules 
　one of the advantages of the remote-brained approach is the ability to use a mobile body with many dof. in order to organize so many actuators and degrees of freedom of actuators without decreasing flexibility  the body should be designed to be physically reconfigurable to perform many kinds of tasks. 
　our first design was to build a distributed modular robot which has eye modules and arm modules. the concept of the robot is that it can grasp where it wants to grasp and it can watch where it wants to look. figure 1 shows two eye modules and two arm modules and a view of an eye module observing a hand module. eye modules have a camera set with pan-tilt mounted on a mobile base with two wheels. arm modules have a gripper at the end of a 1dof arm mounted on the same mobile base. the total number of actuators is twenty. 
　working with reconfigurable robots raises many problems such as identification between eye and hand modules  navigation of the hand modules by eye modules  coordination between hand modules  and so on. as a first step  we have started with a model and vision based approach without complete calibration between modules. the model works as a memory to control visual atten-

figure 1: vision-based ballboy robot with recovery actions in locomotion 
tions. each eye module can verify the memory and update it. during visual guidance of a gripper  each eye module can guide it by sending relative motion command such as left/right or up/down to the control process of the hand module. 
　figure  c  of 1 is a view of the mother software which shows the inside of the brain. the view at the left lower shows the predicted image which the eye module is looking at such as in  b . the brain of this reconfigurable robot has to update the memory according to the visual information. the tracking vision can use the memory to recover from failures of tracking. the identification process of the vision for updating the memory requires such memory based tracking processes. 
　the software for the brain of the reconfigurable robot requires such memory management. it is hard for each module to have memory storage and maintain it in its own body. during the development of software for reconfiguarable robots  the environment can show the memories of every mobile module and also the result detected by each eye module to the developer  enabling faster in software development. this is another advantage of the remote-brained approach. 
1 	c o o r d i n a t i o n s i n r o p e h a n d l i n g 
handling flexible objects requires a robot where sensing and action are tightly coordinated. we have developed two sets of mobile hand-eye robots. each robot has a camera  an 1dof arm with a gripper and two wheels mounted on a base. the robot is very compact  with the size of the base being 1cm by 1cm. a team of two of these robots can succeed at rope handling  inaba et al.  1a . 
the remote-brained approach affects the design of the 
	inaba 	1 
arms. the gripper is small enough that vision can observe the target object without occlusion by the gripper. as the wires making up the gripping effector are flexible  the picked object may change orientation. in general  such a gripper is not considered to be an appropriate robot hand  because the robot can't estimate the location of the grasped object. however  a vision-based robot can tolerate such flexibility because vision allows it to know the actual location of the object  not just the gripper location. thus  an approach based on intensive powerful vision allows us to design simpler robot bodies. 
1 	vision-based behaviors 
the framework of remote-brained robot allows us to carry out different kinds of robotics research especially vision-based experiments. so far  we have developed the realtime tracking vision and applied it for many experiments with our robots  inaba et al.  1d . 
1 vision-based action selection in a quadruped robot 
another advantage of remote-brained robots is that the bodies can be lightweight. this opens up the possibility of working with legged mobile robots. as with animals  if a robot has 1 legs it can walk. we are focusing on vision-based adaptive behavirors of 1-legged robots  experimenting in a field as yet not much studied. 
　moreover  we are considering a quadruped robot where two of the legs can be used as arms. rather than assigning distinct functions from the start  we want a robot to decide to use legs as arms if required by the situation  as kind of an emergent property  the development of intelligence ; this is another problem for futher research. 
　a quadruped robot is designed to interact with human by catching a ball. it can chase a ball and grab it. figure 1 shows a sequence of motions and the image which the robot sees. vision is the only sensor in this interaction. it detects the target ball by template matching  analyzes the relationship between its body and the target ball  and verifies the picking up action. 
　the robot can show several vision-based adaptive behaviors. when the robot chases a moving ball  it controls the stride of both sides of legs to change its walking speed and direction. in order to keep tracking the ball in the middle of the view  the robot continuously controls the neck orientation in parallel with walking motion. when the target detection fails  a looking-for-action is performed and the robot tries to find the target by block matching by moving its head around. 
　when the robot tracks the target  it keeps watching the target and measures the distance to the target. if the distance to a static target does not change when walking  it means some obstacles are in the way. if the walking direction does not point to the goal  it means that one of the legs is probably being hindered  thus avoid and climb actions emerge to give adaptive capability in chasing the target. 

figure 1: keeping balance  as done by humanoid robot 

figure 1: vision-based dynamic motion in swing 
1 	vision-based balancing in humanoid robot 
figure 1 shows an experiment in balancing. in this experiment  a human tilts the ground board on which the robot is standing. the robot vision tracks the scene in the front view. it remembers the vertical orientation of an object as the reference for visual tracking and generates several rotated images of the reference image. the rotational visual tracker  inaba et a/.  1b  can track the image at video rate. if the vision tracks the reference object using the rotated images  it can measure the body rotation. in order to keep the body balance  the robot feedback controls its body rotation to control the center of the body gravity. 
erated by a genetic algorithm. this is a first step to automatic action acquisition for mechanical animals. as the ga method requires long time trials  we developed a simulation system for the swing world. the simulation program is written in euslisp and simulates not only the physical motion of the robot and the swing but also visual processing with time-base filters. generally  it is hard to generate accurate optical flows. we adopted a butterworth-filter to get the motion velocity of the robot body from the optical flows. the visual process with a time-based filter is indispensable to get stable motion control in dynamic actions. 



figure 1: experiment of sumo game with human controlled robot 
1 	vision-based swing control 
when a robot sits on a swing and swings  it requires a fast visual tracker and a free body. the remote-brained design allows us to make an apelike robot perform visionbased dynamic control of a swing  inaba et al.  1e . figure 1 shows the experiment. the robot vision can measure the frequency and the phase of the swing using optical flow generation. in order to accelerate swinging  the robot controls the height of the center of gravity of its body using the visual measurement. although a human can write a control program based on the theory of parametric oscillation  we have succeeded in automatic generation of swing motions on a neural-net based controller where the weights of the neural network are genfigure 1: guiding knuckle walk by visual instructions 
1 	human robot interactions 
the remote-brained approach encourages us to try building whole-bodied robots which interact with humans. we present the principal experiments on interactions done in our platform. 
1 improving robot through human r o b o t c o m p e t i t i o n 
we have done an application for improving robots through interactions with human-controlled robots. the example is the computer-human sumo wrestling. two robots fight until one of them is pushed out of  dohyo  ring or falls down. in this experiment  as shown in figure 1  we control one robot by a computer for fighting against a human controlled robot. it is an experiment of human-computer competition for a reactive game in the real world  inaba et al.  1b . 
	inaba 	1 

　in such a mutually reactive process  a human operator observes the behavior of the computer robot and reacts against it by means of adequate fighting strategies. at the same time  the computer also reacts against the behavior of human controlled one. as a human develops his skill  the computer program must be improved so as to compete with the opponent. when the computer enhances its skill  a human operator must try to find better strategy. in such a way  both sides improve their own strategy through the exercise of mutual reactions. the remote-brained approach helps the developing cycles as the brain of the robot is inside the mother development environment. 
1 	visual guidance 
interaction with a human through vision is an essential task for a vision-based robot. the robot has to recognize the human intention by seeing and performing an action. figure 1 shows a experiment using visual tracking function. the robot is programmed to follow the target object which the human shows. the actions the robot can select from are waiting  standing up  go forward/backward  and turn left/right. one of the action is selected by observation of the target object. as the prepared actions include primitive locomotion in any direction  the human can navigate the robot to anywhere by visual guidance  inaba et al.  1b . 
1 	learning instructions 

figure 1: experiment on learning mapping between visual signs and gestures 
　another project is the interactive teaching of gesture learning. figure 1 shows some gestures and the scheme of learning a mapping between visual patterns shown by a human and the gestures. as the interface with a human is only vision  it is assumed that the robot knows 
1 

figure 1: developed apoids and functional developments 
the predefined meta signs  yes and no  to know whether the requested gesture is right or not. 
　in the experiment  a human shows a gesture sign to request a gesture. when the robot detects a gesture sign  it starts performing a gesture. after the robot performs a gesture corresponding to the presented sign  the human verifies the correspondence. if it is bad  the human indicates no by waving the sign horizontally. 
1 	behavior development in humanoids 
across the ages  one dream of intelligent robotics has been humanoid robots  kato et al.  1; sugano et al.  1; brooks and stein  1; inoue and sato  1 . the remote-brained approach allows us to develop a whole-bodied humanoid. our approach towards humanoid robots will be via apelike robots   apoids . how to evolve from an ape's use of arms and legs to bipedal locomotion  taking our clue for our 'mother environment for software development' from development over long time spans in the natural world  is another goal of our research. 
　figure 1 shows apelike robots. they have different body structures. the apoids in the bottom picture corresponds to  a  ..  d  from left. 
　the apoid  a  has 1dof  1 in leg  1 in arm and gripper  1 in head . the apoid  b  has the same number 


figure 1: general relation between an agent and its mother 

of total dof of the body but the freedom of the leg is extended  each leg has four dof. the extended degree is added at the ankle and it gives the robot the ability to tilt the body to left and right while standing  which allows the robot to move the center of the gravity of the body over each foot. thus the robot can perform biped walking in a static way. the apoid  a  can not do such static biped walking. it just performs static locomotion by knuckle walking. 
　the apoid  c  has 1dof  1 in arm  1 in gripper  1 in leg  1 in head . it can change walking direction during biped walking because of the extended dof of the legs. as the arms have more dof  it can carry an object by holding it horizontally  kanehiro et al.  1 . 
　the apoid  d  has 1dof  1 in arm  1 in gripper  1 in leg  1 in head   inaba et al.  1b . as the number of dof in each arm and leg is six  it can control its hand and foot in any position and orientation in three dimensional space  even if it can't change the orientation of the body orientation. this robot can move on a chair and manipulate objects on a desk. one of the goal tasks of this robot is to retreive an object that has fallen off of the dask. 
　the goal here is to know how the brain for apoid  a  should be raised to accept the body of apoid  b  and so on. another goal is to know whether there is a brain which accepts all body structures  and whether there is a general method to build such brain. in order to approach these question  we have tried to share the software environment in the mother environment. 
1 	operations study for development 
once we can have a rich platform to investigate various robotics themes by developing real robots  we can do research on the operations for development of a robot  inaba et al.  1c . 
　figure 1 is shown to explain the relationship between a robot and a mother and the operations done in the mother. preview simulates the situation of the real environment and body behaviors according to action values given from brain. it shows simulated results to brain. observe monitors behaviors of body through real sensors of the mother. evaluate receives observed behaviors from preview and observe and selects appropriate operations from teach  bear  build. 
1 	bearing a brain through preview operation 
one example of preview operation is a case of building a brain to perform vision-based dynamic action of an agent. in the swing experiment as shown in figure 1  the swing and the virtual body are modeled in a three dimensional geometric model. the preview ticks away the states of virtual body and environment according to the trajectory given from the brain. the previewed result is given to the brain as sensing data. 
　at the first stage  the builder of the apoid body evaluate the previewed data and bear the brain based on the model of parametric oscilator. the brain structure is based on a feedback loop architecture. the attention selection is done in both space and time. the watching region is given by the last position of the visual tracker. the apoid controls the angle of neck to view the target successfully. the monitoring time to generate the reference value for action is defined by the frequency of the swing. the frequency is also measured by vision. 
　as the next stage  the builder changes the evaluate process and the bear operation with a ga system. evaluate sets the fitness of all population of the genes. bear operation is operation on all population of genes. brain receives the map from the perception to the action. the perception data is optical flow. this stage closes the loop of preview  evaluate and bear without a human operator. 
1 	developing getting up motions 
research on biped walking robots has focused on the dynamics of walking  treating it as an advanced problem in 
	inaba 	1 


figure 1: sequence of standing up:  a  from face down   b  rolling over from face up   c  using table for support 
1 	invited speakers 


figure 1: sequence of walking:  a  knuckle walking   b  biped walking   c  carrying an object 

control  miura and shimoyama  1 . however  focusing attention on intelligent reactivity in the real world  it is more important to build  not a robot that won't fall down  but a robot that can get up if it does fall down  inaba et al.  1a . the behaviors of getting up are indispensable for robots. the sequences shown in figure 1 are examples. 
　those behaviors are developed through preview with a virtual body and environment simulator. preview can calculate the balance of the robot body. as shown in the figure  the brain can estimate the contact points between the body and the ground by calculating the center of its body gravity and the configuration of the body lims. 
　in the case that the effect of the difference between the virtual body and the real body is large  the bear operation through the observe process is indispensable. the task here is to build a good observer that can recognizes the robot body. this flow through observe to bear is a good way to investigate emerging properties for real embodied agents. the remote-brained approach with a multi-degreed body such as an apoid will help with this task. 
1 	developing walking motions 
in order to focus attention on flexible mobility in the real world  it is important to build  not only a robot that can run  but also a robot that can stand and walk even in statical way. figure 1 shows experiments of walking done by the apoids   a   b  and  c  respectively . a sequence of walking is generated through observe and bear loop done by a human at the first stage. if the mother has only the virtual body simulator inside  it is able to build a teaching loop through the preview. this makes the bear operation interactive. the next stages are to make methods for teach  to make observe by computer vision and to make preview by computer program. 
　in order to think about a loop through the build operation  the mother can make use of the apoids explained above. that is  if the mother has computer vision-based observe  it can change the body structure by selecting one from already developed apoids. as the remotebrained system does not place the brain inside the body  the mother system can continue raising the same brain after switching the body. this is another feature of the remote-brained system. in this case  the system structure becomes a variation of a system with multiple bodies. the software of apoid  d  is designed using the software developed for apoids  a   b   c . 
1 i n t e r a c t i o n t h r o u g h w h o l e - b o d i e d tactile sensor suit 
the remote-brained approach based on the integrated sensor images allows us to do experiment not only in vision-based but also multi-sensor based interactions. if we can get a general methodology to give a large number of distributed tactile sensors to any robots  it encourages the remote-brained approach in developing a variety of robot bodies with tactile sensors. our approach is the 'sensor suit' approach which covers the whole body of the robot lnaba  1 . we have developed a method to build a sensor suit using electrically conductive fabric ecf  inaba et al.  1aj. the ecf is soft fabric electrically plated with ni and cu which is usable as a soft and flexible metalic sheet. 
　the key issue in building a suit with a large number of sensitive regions is how to wire the cable from the sensing segments to the processing subsystem. our approach is 
	inaba 	1 


figure 1: experiment of sensor-image based interactions 

based on two methods: sewing with ecs and the video multiplexer to pass on all signals. as wiring is done by electrical conductive string ecs   it keeps the suit flexible enough to allow movement of the body. 
　figure 1 shows an experiment in sensor suit based interactions with humans   a  is the tactile sensing region for the right arm.  b  is the real sensor image of the robot sensor suit when the human holds the right upper arm of the robot. in this experiment  if a human touches a left or a right arm  the robot looks in the corresponding direction to start interactions with the human. the program for this behavior monitors the sensing regions and detects pattern changes in these regions. if the change is larger than a threshold  it decides whether to move its head to look at the human. 
tions. we are now witnessing the dawn of an age where wireless communications will surround everyone  expanding each individual's scope of activity  and changing the way people think. it will have the same effect on robots. 
　wireless permits robot bodies free movement  but it also changes the way we conceptualize robotics. in our laboratory it has enabled the development of a new research environment  better suited to robotics and realworld ai. 
there has been a missing link in research  between 
' ai which couldn't survive if embodied in the real world' and 'robots with feeble intelligence'. remote-brained robotics opens the way for engineering advances which will bridge the gap. 

1 	concluding remarks 
this paper has presented a new framework  the 'remotebrained approach'  as a step towards future robotics  and illustrated it with real examples. the wireless connection between a brain and a body allows us to do a variety of vision-based experiments. software for visionbased behaviors can be trained by playing games with humans; this and other software can be kept in the mother environment and inherited by the next generation of robots. further  this approach allows us to design new types of robot body  such as the apelike body. 
　regarding the question of whether the inference and representation techniques of ai are useful for robots in the real world  it seems the answer will depend on how ai is interfaced to the real world. the remote brained approach allows us to tie ai directly to the world  enabling the verification of high-level ai techniques which could previously only be used in simulation. for robot research  this approach opens the way to the use of powerful parallel computers. for ai  this approach allows experiments with realistic  indeed  real  agents  an essential step to the application of ai in the real world. 
　the technology underlying this approach is of course wireless communications. the driving force behind this technology has been the needs of human communica-
1 	invited speakers 
