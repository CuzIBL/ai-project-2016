 
this article presents a high-level discussion of some problems that are unique to web search engines. the goal is to raise awareness and stimulate research in these areas. 
1 introduction 
web search engines are faced with a number of difficult problems in maintaining or enhancing the quality of their performance. these problems are either unique to this domain  or novel variants of problems that have been studied in the literature. our goal in writing this article is to raise awareness of several problems that we believe could benefit from increased study by the research community. we deliberately ignore interesting and difficult problems that are already the subject of active research. an earlier version of this paper appeared in  henzinger et al  1 . 
　we begin with a high-level description of the problems that we describe in further detail in the subsequent sections. 
spam. users of web search engines tend to examine only the first page of search results. silverstein et al.  silverstein et al.  1  showed that for 1% of the queries only the first result screen is requested. thus  inclusion in the first result screen  which usually shows the top 1 results  can lead to an increase in traffic to a web site  while exclusion means that only a small fraction of the users will actually see a link to the web site. for commercially-oriented web sites  whose income depends on their traffic  it is in their interest to be ranked within the top 1 results for a query relevant to the content of the web site. 
　to achieve this goal  some web authors try to deliberately manipulate their placement in the ranking order of various search engines. the result of this process is commonly called search engine spam. in this paper we will simply refer to it as spam. to achieve high rankings  authors either use a textbased approach  a link-based approach  a cloaking approach  
　*part of this work was done while the author was visiting google inc. work also supported in part by nsf grant iis-1  and research grants from the okawa foundation and veritas. 
or a combination thereof. there are web ranking optimization services which  for a fee  claim to place a given web site highly on a given search engine. 
　unfortunately  spamming has become so prevalent that every commercial search engine has had to take measures to identify and remove spam. without such measures  the quality of the rankings suffers severely. 
　traditional research in information retrieval has not had to deal with this problem of  malicious  content in the corpora. quite certainly  this problem is not present in the benchmark document collections used by researchers in the past; indeed  those collections consist exclusively of high-quality content such as newspaper or scientific articles. similarly  the spam problem is not present in the context of intranets  the web that exists within a corporation. 
　one approach to deal with the spam problem is to construct a spam classifier that tries to label pages as spam or not-spam. this is a challenging problem  which to the best of our knowledge has not been addressed to date. 
content quality. even if the spam problem did not exist  there are many troubling issues concerned with the quality of the content on the web. the web is full of noisy  low-quality  unreliable  and indeed contradictory content. a reasonable approach for relatively high-quality content would be to assume that every document in a collection is authoritative and accurate  design techniques for this context  and then tweak the techniques to incorporate the possibility of low-quality content. however  the democratic nature of content creation on the web leads to a corpus that is fundamentally noisy and of poor quality  and useful information emerges only in a statistical sense. in designing a high-quality search engine  one has to start with the assumption that a typical document cannot be  trusted  in isolation; rather  it is the synthesis of a large number of low-quality documents that provides the best set of results. 
　as a first step in the direction outlined above  it would be extremely helpful for web search engines to be able to identify the quality of web pages independent of a given user request. there have been link-based approaches  for instance pagerank  brin and page  1   for estimating the quality of web pages. however  pagerank only uses the link structure of the web to estimate page quality. it seems to us that a better estimate of the quality of a page requires additional 
　
sources of information  both within a page  e.g.  the readinglevel of a page  and across different pages  e.g.  correlation of content . 
quality evaluation. evaluating the quality of different ranking algorithms is a notoriously difficult problem. commercial search engines have the benefit of large amounts of user-behavior data they can use to help evaluate ranking. users usually will not make the effort to give explicit feedback but nonetheless leave implicit feedback information such as the results on which they clicked. the research issue is to exploit the implicit feedback to evaluate different ranking strategies. 
web conventions. most creators of web pages seem to follow simple  rules  without anybody imposing these rules on them. for example  they use the anchor text of a link to provide a succinct description of the target page. since most authors behave this way  we will refer to these rules as web conventions  even though there has been no formalization or standardization of such rules. 
　search engines rely on these web conventions to improve the quality of their results. consequently  when webmasters violate these conventions they can confuse search engines. the main issue here is to identify the various conventions that have evolved organically and to develop techniques for accurately determining when the conventions are being violated. 
duplicate hosts. web search engines try to avoid crawling and indexing duplicate and near-duplicate pages  as they do not add new information to the search results and clutter up the results. the problem of identifying duplicates within a set of crawled pages is well studied. however  if a search engine can avoid crawling the duplicate content in the first place  the gain is even larger. in general  predicting whether a page will end up being a duplicate of an already-crawled page is chancy work  but the problem becomes more tractable if we limit it to finding duplicate hosts  that is  two hostnames that serve the same content. one of the ways that duplicate hosts can arise is via an artifact of the domain name system  dns  where two hostnames can resolve to the same physical machine. there has only been some preliminary work on the duplicate hosts 
problem  bharat et al.  1 . 
vaguely-structured data. the degree of structure present in data has had a strong influence on techniques used for search and retrieval. at one extreme  the database community has focused on highly-structured  relational data  while at the other the information retrieval community has been more concerned with essentially unstructured text documents. of late  there has been some movement toward the middle with the database literature considering the imposition of structure over almost-structured data. in a similar vein  document management systems use accumulated meta-information to introduce more structure. the emergence of xml has led to a flurry of research involving extraction  imposition  or maintenance of partially-structured data. 
　web pages in html fall into the middle of this continuum of structure in documents  being neither close to free text nor to well-structured data. instead html markup provides limited structural information  typically used to control layout but providing clues about semantic information. layout information in html may seem of limited utility  especially compared to information contained in languages like xml that can be used to tag content  but in fact it is a particularly valuable source of meta-data in unreliable corpora such as the web. the value in layout information stems from the fact that it is visible to the user: most meta-data which is not user-visible and therefore is particularly susceptible to spam techniques  but layout information is more difficult to use for spam without affecting the user experience. there has only been some initial  partly related work in this vein  nestorov et al  1; chakrabarti et al  1; chakrabarti  1 . we believe that the exploitation of layout information can lead to direct and dramatic improvement in web search results. 
1 spam 
some web authors try to deliberately manipulate their placement in the rankings of various search engine. the resulting pages are called spam. traditional information retrieval collections did not contain spam. as a result  there has not been much research into making search algorithms resistant to spam techniques. web search engines  on the other hand  have been consistently developing and improving techniques for detecting and fighting spam. as search engine techniques have developed  new spam techniques have developed in response. search engines do not publish their anti-spam techniques to avoid helping spammers to circumvent them. 
　historical trends indicate that the use and variety of spam will continue to increase. there are challenging research issues involved in both detecting spam and in developing ranking algorithms that are resistant to spam. current spam falls into following three broad categories: text spam  link spam  and cloaking. a spammer might use one or some combination of them. 
1 	text spam 
all search engines evaluate the content of a document to determine its ranking for a search query. text spam techniques are used to modify the text in such a way that the search engine rates the page as being particularly relevant  even though the modifications do not increase perceived relevance to a human reader of a document. 
　there are two ways to try to improve ranking. one is to concentrate on a small set of keywords and try to improve perceived relevance for that set of keywords. for instance  the document author might repeat those keywords often at the bottom of the document  which it is hoped will not disturb the user. sometimes the text is presented in small type  or even rendered invisible  e.g.  by being written in the page's background color  to accomplish this. 
　another technique is to try to increase the number of keywords for which the document is perceived relevant by a search engine. a naive approach is to include  some subset 
　
of  a dictionary at the bottom of the web page  to increase the chances that the page is returned for obscure queries. a less naive approach is to add text on a different topic to the page to make it appear that this is the main topic of the page. for example  porn sites sometimes add the names of famous personalities to their pages in order to make these pages appear when a user searches for such personalities. 
1 	link spam 
the advent of link analysis by search engines has been accompanied by an effort by spammers to manipulate link analysis systems. a common approach is for an author to put a link farm at the bottom of every page in a site  where a link farm is a collection of links that points to every other page in that site  or indeed to any site the author controls. the goal is to manipulate systems that use raw counts of incoming links to determine a web page's importance. since a completelylinked link farm is easy to spot  more sophisticated techniques like pseudo web-rings and random linkage within a member group are now being used. 
　a problem with link farms is that they distract the reader because they are on pages that also have legitimate content. a more sophisticated form of link farms has been developed  called doorway pages. doorway pages are web pages that consist entirely of links. they are not intended to be viewed by humans; rather  they are constructed in a way that makes it very likely that search engines will discover them. doorway pages often have thousands of links  often including multiple links to the same page.  there is no text-spam equivalent of doorway pages because text  unlike links  is analyzed by search engines on a per-page basis.  
　both link farms and doorway pages are most effective when the link analysis is sensitive to the absolute number of links. techniques that concentrate instead on the quality of links  such as pagerank  brin and page  1; brin et al  1   are not particularly vulnerable to these techniques. 
1 cloaking 
cloaking involves serving entirely different content to a search engine crawler than to other users.1 as a result  the search engine is deceived as to the content of the page and scores the page in ways that  to a human observer  seem rather arbitrary. 
　sometimes cloaking is used with the intent to  help  search engines  for instance by giving them an easily digestible  textonly version of a page that is otherwise heavy with multimedia content  or to provide link-based access to a database which is normally only accessible via forms  which search engines cannot yet navigate . typically  however  cloaking is used to deceive search engines  allowing the author to achieve 
　　1 a search engine crawler is a program that downloads web pages for the purpose of including them in the search engine results. typically a search engine will download a number of pages using the crawler  then process the pages to create the data structures used to service search requests. these two steps are repeated continuously to ensure the search engine is searching over the most up-to-date content possible. 
the benefits of link and text spam without inconveniencing human readers of the web page. 
1 defending against spam 
in general  text spam is defended against in a heuristic fashion. for instance  it was once common for sites to ''hide  text by writing it in white text on a white background  ensuring that human readers were not affected while search engines were misled about the content. as a result  search engine companies detected such text and ignored it. such reactive approaches are  obviously  not optimal. can pro-active approaches succeed  perhaps these approaches could be combined; it might be possible for the search engine to notice what pages change in response to the launch of a new antispam heuristic  and to consider those pages as potential spam pages. 
　typically  link-spam sites have certain patterns of links that are easy to detect  but these patterns can mutate in much the same way as link spam detection techniques. a less heuristic approach to discovering link spam is required. one possibility is  as in the case of text spam  to use a more global analysis of the web instead of merely local page-level or sitelevel analysis. for example  a cluster of sites that suddenly sprout thousands of new and interlinked webpages is a candidate link-spam site. the work by  kumar et al  1  on finding small bipartite clusters in the web is a first step in this direction. 
　cloaking can only be discovered by crawling a website twice  once using an http client the cloaker believes is a search engine  and once from a client the cloaker believes is not a search engine. even this is not good enough  since web pages typically differ between downloads for legitimate reasons  such as changing news headlines. 
　an interesting challenge is to build a spam classifier that reliably detects a large fraction of the currently existing spam categories. 
1 content quality 
while spams are attempts to deliberately mislead search engines  the web is replete with text that - intentionally or not - misleads its human readers as well. as an example  there is a webpage which claims  falsely!  that thomas jefferson was the first president of the united states. many websites  purposefully or not  contain misleading medical information.1 other sites contain information that was once correct but is now out of date; for example  sites giving names of elected officials. 
　while there has been a great deal of research on determining the relevance of documents  the issue of document quality or accuracy has not been received much attention  whether in web search or other forms of information retrieval. for instance  the trec conference explicitly states rules for when it considers a document to be relevant  but does not mention the accuracy or reliability of the document at all. this is understandable  since typical research corpora  including the 
　　1  one study showed many reputable medical sites contain contradictory information on different pages of their site  berland et ai  1  - a particularly difficult content-quality problem! 
　
ones used by trec and found in corporate intranets  consist of document sources that are deemed both reliable and authoritative. the web  of course  is not such a corpus  so techniques forjudging document quality is essential for generating good search results. perhaps the one successful approach to  heuristically  approximating quality on the web is based on link analysis  for instance pagerank  brin and page  1; brin et al.  1  and hits  kleinberg  1 . these techniques are a good start and work well in practice  but there is still ample room for improvement. 
　one interesting aspect of the problem of document quality is specific to hypertext corpora such as the web: evaluating the quality of anchor text. anchor text is the text  typically displayed underlined and in blue by the web browser  that is used to annotate a hypertext link. typically  web-based search engines benefit from including anchor-text analysis in their scoring function  craswell et al  1 . however  there has been little research into the perils of anchor-text analysis e.g. due to spam and on methodologies for avoiding the pitfalls. 
　for instance  for what kinds of low-quality pages might the anchor text still be of high quality  is it possible to judge the quality of anchor text independently of the quality of the rest of the page  is it possible to detect anchor text that is intended to be editorial rather than purely descriptive  in addition  many fundamental issues remain open in the application of anchor text to determination of document quality and content. in case of documents with multiple topics  can anchor text analysis be used to identify the themes  
　another promising area of research is to combine established link-analysis quality judgments with text-based judgments. a text-based analysis  for instance  could judge the quality of the thomas jefferson page by noting that most references to the first president of the united states in the web corpus attribute the role to george washington. 
1 quality evaluation 
search engines cannot easily improve their ranking algorithms without running tests to compare the quality of the new ranking technique with the old. performing such comparisons with human evaluators is quite work-intensive and runs the danger of not correctly reflecting user needs. thus  it would be best to have end users perform the evaluation task  as they know their own needs the best. 
　users  typically  are very reluctant to give direct feedback. however  web search engines can collect implicit user feedback using log data such as the position of clicks for a search and the time spent on each click. this data is still incomplete. for instance  once the user clicks on a search result  the search engine does not know which pages the user visits until the user returns to the search engine. also  it is hard to tell whether a user clicking on a page actually ends up finding that page relevant or useful. 
　given the incomplete nature of the information  the experimental setup used to college implicit user data becomes particularly important. that is: how should click-through and other data be collected  what metrics should be computed from the data  
　one approach is to simply collect the click-through data from a subset of the users - or all users - for two ranking algorithm. the experimenter can then compute metrics such as the percentage of clicks on the top 1 results and the number of clicks per search. 
　recently  joachims  suggested another experimental technique which involves merging the results of the two ranking algorithms into a single result set. in this way each user performs a comparison of the two algorithms. joachims proposes to use the number of clicks as quality metric and shows that  under some weak assumptions  the clickthrough for ranking a is higher than the clickthrough for b if and only if a retrieves more relevant links than b. 
1 web conventions 
as the web has grown and developed  there has been an evolution of conventions for authoring web pages. search engines assume adherence to these conventions to improve search results. in particular  there are three conventions that are assumed relating to anchor text  hyperlinks  and meta tags. 
  as discussed in section 1  the fact that anchor text is meant to be descriptive is a web convention  and this can be exploited in the scoring function of a search engine. 
  search engines typically assume that if a web page au-thor includes a link to another page  it is because the author believes that readers of the source page will find the destination page interesting and relevant. because of the way people usually construct web pages  this assumption is usually valid. however  there are prominent exceptions: for instance  link exchange programs  in which web page authors agree to reciprocally link in order to improve their connectivity and rankings  and advertisement links. humans are adept at distinguishing links included primarily for commercial purposes from those included primarily for editorial purposes. search engines are less so. 
to further complicate matters  the utility of a link is not a binary function. for instance  many pages have links allowing you to download the latest version of adobe's acrobat reader. for visitors that do not have acrobat 
reader  this link is indeed useful  certainly more useful than for those those who have already downloaded the program. similarly  most sites have a terms of service link at the bottom of every page. when the user first enters the site  this link might well be very useful  but as the user browses other webpages on the site  the link's usefulness immediately decreases. 
  a third web convention concerns the use of meta tags. these tags are currently the primary way to include metadata within html. in theory meta tags can include arbitrary content  but conventions have arisen for meaningful content. a meta tag of particular importance to search engines is the so-called content meta tag  which web page authors use to describe the content of the document. convention dictates that the content meta tag contains either a short textual summary of the page or a brief list of keywords pertaining to the content of the page. 
abuse of this meta tags is common  but even when there is no attempt to deceive  there are those who break the convention  either out of ignorance or overzealousness. for instance  a webpage author might include a summary of their entire site within the meta tag  rather than just the individual page. or  the author might include keywords that are more general than the page warrants  using a meta description of  cars for sale  on a web page that only sells a particular model of car. in general  the correctness of meta tags is difficult for search engines to analyze because they are not visible to users and thus are not constrained to being useful to visitors. however  there are many web page authors that use meta tags correctly. thus  if web search engines could correctly judge the usefulness of the text in a given meta tag  the search results could potentially be improved significantly. the same applies to other content not normally displayed  such as alt text associated with the image tag. 
　while link analysis has become increasingly important as a technique for web-based information retrieval  there has not been as much research into the different types of links on the web. such research might try to distinguish commercial from editorial links  or links that relate to meta-information about the site   this site best viewed with  start link browser x cnd link    from links that relate to the actual content of the site. 
　to some extent  existing research on link analysis is helpful  since authors of highly visible web pages are less likely to contravene established web conventions. but clearly this is not sufficient. for instance  highly visible pages are more  rather than less  likely to include advertisements than the average page. 
　understanding the nature of links is valuable not only for itself  but also because it enables a more sophisticated treatment of the associated anchor text. a potential approach would be to use text analysis of anchor text  perhaps com-
bined with meta-information such as the url of the link  in conjunction with information obtained from the web graph. 
1 duplicate hosts 
web search engines try to avoid crawling and indexing duplicate and near-duplicate pages  since such pages increase the time to crawl and do not contribute new information to the search results. the problem of finding duplicate or nearduplicate pages in a set of crawled pages is well studied  brin et al.  1; broder  1   there has also been some research on identifying duplicate or near-duplicate directory trees  cho et al.  1   called mirrors. 
　while mirror detection and individual-page detection try to provide a complete solution to the problem of duplicate pages  a simpler variant can reap most of the benefits while requiring less computational resources. this simpler problem is called duplicate host detection. duplicate hosts   duphosts   are the single largest source of duplicate pages on the web  so solving the duplicate hosts problem can result in a significantly improved web crawler. 
　a host is merely a name in the domain name system  dns   and duphosts arise from the fact that two dns names can resolve to the same ip address.1 companies typically reserve more than one name in dns  both to increase visibility and to protect against domain name  squatters.  for instance  currently both b i k e s p o r t . com and b i k e s p o r t w o r l d . com resolve to the same ip address  and as a result the sites http://www.bikesport.com/ and http://www.bikesportworld.com/ display identical content. 
　unfortunately  duplicate ip addresses are neither necessary nor sufficient to identify duplicate hosts. virtual hosting can result in different sites sharing an ip address  while roundrobin dns can result in a single site having multiple ip addresses. 
　merely looking at the content of a small part of the site  such as the homepage  is equally ineffective. even if two domain names resolve to the same website  their homepages could be different on the two viewings  if for instance the page includes an advertisement or other dynamic content. on the other hand  there are many unrelated sites on the web that have an identical  under construction  home page. 
　while there has been some work on the duphosts problem  bharat et al.t 1   it is by no means a solved problem. one difficulty is that the solution needs to be much less expensive than the brute-force approach that compares every pair of hosts. for instance  one approach might be to download every page on two hosts  and then look for a graph isomorphism. however  this defeats the purpose of the project  which is to not have to download pages from both of two sites that are duphosts. 
　furthermore  web crawls are never complete  so any linkstructure approach would have to be robust against missing pages. specifically  a transient network problem problem  or server downtime  may keep the crawler from crawling a page in one host of a duphost pair  but not the other. likewise  due to the increasing amount of dynamic content on the web  text-based approaches cannot check for exact duplicates. 
　on the other hand  the duphosts problem is simpler than the more general problem of detecting mirrors. duphosts algorithms can take advantage of the fact that the urls between duphosts are very similar  differing only in the hostname component. furthermore  they need not worry about content reformatting  which is a common problem with mirror sites. 
　finally - and this is not a trivial matter - duphost analysis can benefit from semantic knowledge of dns. for instance  candidate duphosts http://foo.com and http://foo.co.uk are  all other things being equal  likely to be duphosts  while candidates h t t p : / / f o o . com and ht t p : / / b a r . com are not as likely to be duphosts. 
     1 in fact  it's not necessary that they resolve to the same ip address to be duphosts  just that they resolve to the same webserver. technically even that is not necessary; the minimum requirement is that they resolve to computers that serve the same content for the two hostnames in question. 
1 vaguely-structured data 
while information retrieval corpora has tended to be very low in structure  database content is very well structured. this has obviously led to a major difference in how the two fields have evolved over the years. for instance  a practical consequence of this difference is that databases permit a much richer and complex set of queries  while text-based query languages are in general much more restricted. 
　as database content  or more generally structured data  started being exposed through web interfaces  there developed a third class of data called semi-structured data. in the web context  semi-structured data is typically the content of a webpage  or part of a webpage  that contains structured data but no longer contains unambiguous markup explicating the structure or schema. there has been considerable research on recovering the full structure of semi-structured data  for example   ahonen et al.1  and  nestorov et al  1 . 
　the three examples above cover three points on the continuum of structured data. however  most web pages do not fall into any of these categories  but instead fall into a fourth category we call vaguely-structured data. the information on these web page is not structured in a database sense - typically it's much closer to prose than to data - but it does have some structure  often unintentional  exhibited through the use of html markup. 
　we say that html markup provides unintentional structure because it is not typically the intent of the webpage author to describe the document's semantics. rather  the author uses html to control the document's layout  the way the document appears to readers.  it is interesting to note that this subverts the original purpose of html  which was meant to be a document description language rather than a page description language.  to give one example  html has a tag that is intended to be used to mark up glossary entries. in common browsers  this caused the text to be indented in a particular way  and now the glossary tag is used in any context where the author wants text indented in that manner. only rarely does this context involve an actual glossary. 
　of course  often markup serves both a layout and a semantic purpose. the html header tags  for instance  produce large-font  bold text useful for breaking up text  but at the same time they indicate that the text so marked is probably a summary or description of the smaller-font text which follows. 
　even when markup provides no reliable semantic information  it can prove valuable to a search engine. to give just one example  users have grown accustomed to ignoring text on the periphery of a web page  faraday  1   which in many cases consists of navigational elements or advertisements. search engines could use positional information  as expressed in the layout code  to adjust the weight given to various sections of text in the document. 
　in addition  layout can be used to classify pages. for instance  pages with an image in the upper-left of the page are often personal homepages. pages with a regular markup structure are likely to be lists  which search engines may wish to analyze differently than pages with running text. 
markup can be meta-analyzed as well. it is plausible that pages with many mistakes in the markup are more likely to be of lower quality than pages with no mistakes. patterns in the markup used may allow a search engine to identify the web authoring tool used to create the page  which in turn might be useful for recovering some amount of structure from the page. markup might be particularly useful for clustering web pages by author  as authors often use the same template for most of the pages they write. 
　and  of course  html tags can be analyzed for what semantic information can be inferred. in addition to the header tags mentioned above  there are tags that control the font face  bold  italic   size  and color. these can be analyzed to determine which words in the document the author thinks are particularly important. 
　one advantage of html  or any markup language that maps very closely to how the content is displayed  is that there is less opportunity for abuse: it is difficult to use html markup in a way that encourages search engines to think the marked text is important  while to users it appears unimportant. for instance  the fixed meaning of the  h1   tag means that any text in an hi context will appear prominently on the rendered web page  so it is safe for search engines to weigh this text highly. however  the reliability of html markup is decreased by cascading style sheets  world wide web consortium     which separate the names of tags from their representation. 
　there has been research in extracting information from what structure html does possess. for instance   chakrabarti etal  1; chakrabarti  1  created a dom tree of an html page and used this information to increase the accuracy of topic distillation  a link-based analysis technique. 
　however  there has been less research addressing the fact html markup is primarily descriptive  that is  that it is usually inserted to affect the way a document appears to a viewer. such research could benefit from studies of human perception: how people view changes in font size and face as affecting the perceived importance of text  how much more likely people are to pay attention to text at the top of a page than the bottom  and so forth. as newspaper publishers have long known  layout conveys semantic information  but it's not trivial to extract it. 
　turning html into its markup is also a challenge. it is possible to render the page  of course  but this is computationally expensive. is there any way to figure out  say  if a given piece of html text is in the  middle  of a rendered html page without actually rendering it  
　of course  html text is only one example of vaguely structured data. what other kinds of content exists that is somewhere between unstructured data and semi-structured data in terms of quantity of annotation  how does it differ from html text  for that matter  the continuum of structure is not well-mapped. what techniques appropriate for unstructured data work equally well with vaguely structured data  what techniques work for semi-structured data  how can these techniques be improved as data gets more structured  and is there any way to map the improvements down to less structured forms of data  perhaps by imputing something  structural  to the data  even if that doesn't correspond to any 
　
intuitive idea of structure   
1 conclusions 
in this paper we presented some challenging problems faced by current web search engines. there are other fruitful areas of research related to web search engines we did not touch on. for instance  there are challenging systems issues that arise when hundreds of millions of queries over billions of web pages have to be serviced every day without any downtime and as inexpensively as possible. furthermore  there are interesting user interface issues: what user interface does not confuse novice users  does not clutter the screen  but still fully empowers the experienced user  finally  are there other ways to mine the collection of web pages so as to provide a useful service to the public at large  
1 resources 
here are two resources for the research community: 
　stanford's webbase project  http://wwwdiglib.stanford.edu/~testbed/doc1/webbase/  distributes its content of web pages. 
　web term document frequency is available at berkeley's web term document frequency and rank site   h t t p : / / e l i b . cs . b e r k e l e y . edu/docf req/ . 
