 
this paper addresses the problem of concept sampling. in many real-world applications  a large collection of mixed concepts is available for decision making. however  the collection is often so large that it is difficult if not unrealistic to utilize those concepts directly  due to the domain-specific limitations of available space or time. this naturally yields the need for concept reduction. in this paper  we introduce the novel problem of concept sampling: to find the optimal subset of a large collection of mixed concepts in advance so that the performance of future decision making can be best preserved by selectively combining the concepts remained in the subset. the problem is formulized as an optimization process based on our derivation of a target function  which ties a clear connection between the composition of the concept subset and the expected error of future decision making upon the subset. then  based on this target function  a sampling algorithm is developed and its effectiveness is discussed. extensive empirical studies suggest that  the proposed concept sampling method well preserves the performance of decision making while dramatically reduces the number of concepts maintained and thus justify its usefulness in handling large-scale mixed concepts. 
1 introduction 
in many real-world applications people in machine learning community are confronted with large numbers of mixed concepts1  upon which the final decision is made. the inherent reason leading to this situation is that  in many cases  the concept underlying the data evolves due to the changes of hidden contexts  widmer et al.  1 . 
 example 1. concepts related to stock trading strategies are influenced by many time-related factors which are hardly accessible  such as macro economic environments and political events  harries et al.  1 . on this occasion  one has to break training data into segments over short time intervals in order to extract stable concept from each interval  and then decision making is made upon the resulting large collection of diverse concepts. this strategy is widely used in on-line learning over data streams  street et al.  1; wang et al. 1  and result in unlimited numbers of mixed concepts. 
　example 1. consider the problem of detecting credit card frauds. the characteristics of the fraud events depend on some hidden contexts  such as the policies of the specific bank branch  the economic conditions  and the new law in the local area. thus the concepts  i.e. the fraud patterns underlying the data  from different branches or even from different periods of the same branch may differ. therefore  it is highly desirable to systematically analyze all these records. however  for privacy preserving  different branches can not share their records. in this case  one possible solution is that each branch periodically contributes the concept describing its recent records  and the models for decision making can be constructed upon the large collection of available concepts.  the typical way to utilize mixed concepts  when there are some data to be classified  is to select some  suitable  concept s  and combine them by some strategies  which will involve searching within the large concept collection. however  such solutions are usually inefficient or even infeasible  because firstly  sometimes the collection is too large to be held in main memory  and secondly  the time complexity of decision making among large numbers of concepts is unacceptable for many efficiency-critical applications  such as online prediction of network intrusion.  
 this yields the need for concept reduction  the main focus of this paper. but is it possible to remove a large part of concepts in the collection while preserve the performance of decision making  the theory of ensemble learning  i.e. combining multiple concepts   kuncheva et al.  1; ruta et al.  1  suggests that a concept can be approximated by an ensemble of similar concepts. thus  the removed concepts can be estimated by selectively combining the remains.  while some literatures addressed the problem that seems similar to this topic  e.g. data sampling  to the best of our knowledge  concept sampling has not been explored. the main contributions of this paper are:  1  we propose the problem of concept sampling: to find the optimal subset of a 
large collection of mixed concepts so that the performance of future decision making can be preserved.  1  we formally derive a target function that ties a clear connection between the composition of the subset and the expected error of the decision making upon this subset. using this function  we formulize the problem of concept sampling as an optimization process.  1  we design a sampling procedure to determine the concept subset. the empirical results suggest that the proposed method well preserves the performance of decision making while dramatically reduces the number of concepts maintained  and thus justify its usefulness in handling large-scale mixed concepts. 
1 related work 
　the existence of mixed concepts has long been accepted in machine learning  such as in the on-line learning problem over changing hidden contexts  widmer et al.  1   or in the discussion of extracting stable concepts when hidden contexts evolve  harries et al.  1 . recently  on-line learning over data streams with evolving concept becomes an active field. many algorithms in this field extract stable concepts from short time intervals and engage various forgetting policies to emphasize the recent concepts  street et al.  1; wang et al. 1 . in fact  all this work is to deal with the mixed concepts. in this paper  we propose concept sampling  which plays an active role in managing large-scale mixed concepts. even in data streams scenario  our method can act as an offline component to extract useful information from huge collection of historical concepts and thus complements the existing on-line learning styles. 
 existing sampling techniques in machine learning mainly focus on instance space  which is to reduce the number of instances. general sampling methods often engage some empirical criteria on data distribution  e.g. density condensation  mitra et al.  1   entropy-based data reduction  huang et al.  1 . similarly  specific methods  such as those for instance-based learning  wilson et al.  1   also rely on some empirical criteria  e.g. assuming that object with the same label as its k neighbors is redundant. 
　different from data sampling  this paper proposes a new problem of concept sampling  which is to preserve the quality of decision making upon the reduced concept set. more importantly  we derive a target function that ties a clear connection between the composition of the reduced set and the performance of decision making  and formulize the sampling as an optimization rather than relying on empirical criteria.  one of the foundations of our work is the theory of combining multiple concepts. since different concepts exist  the diversity between concepts  kuncheva et al.  1  must be considered when pursuing the consensus. further  theoretical analysis about the performance of the ensemble classifiers  ruta et al.  1  is engaged in our paper. 
1 general framework of concept sampling 
　consider a large collection of mixed concepts s = {c1  c1  ...  cn}  where each concept ci is represented by a classifier  and q denotes the unknown set of instances to be classified in the future. essentially  s contains all the concepts so far observed  and we assume that for each future instanceqq  the correct concept cq can be found in s1. 
 however  s is often too large to be directly used due to the limitations of resources. thus only a subset r whose size is much smaller is permitted. since many concepts in s have to be removed  the right concepts cq for many potential qq are not in r  and thus the performance of decision making declines. this calls for concept sampling  which is to reduce the number of concepts maintained but preserve the performance of decision making. this idea is possible because the theory of ensemble learning  kuncheva et al.  1; ruta et al.  1  suggests that a concept c can be approximated by an ensemble composed of its similar concepts. 
 	the problem of concept sampling is formally defined as to find the optimal subset r with predefined size v1 that satisfies 
	r	argmin e r q   	 1  
r s |r| v1
where the target function e r q  is the expected error rates of using concept set r to classify instances in q. note that for many q in q  the best fitted concepts cq in the original set s have been removed  and we want to approximate these concepts by selectively combining the remained ones. therefore  the target function e r  q  can be further defined as: 
	e r q 	e   r q  q p q  	 1  
here p q  is the probability of observing q in q   r q  is the set of concepts in r that are selected to classify q  i.e. the ensemble for classifying q   and e  r q  q  is the expected error of this classification. 
1 concept sampling method 
this section presents our concept sampling method. to minimize the target function mentioned in  1   the following key problems should be solved. first  the objective function  1  involves q  the set of unlabeled instances in future  which is unseen in the time of sampling. thus  the relationship between the composition of the subset r and the expected error defined in  1  is not clear. second  sampling should be efficient to handle large concept collection. in section 1  we derivate an equivalent target function of  1  to deal with the first problem. in section 1  we design an efficient method to optimize this target function and discuss its effectiveness. 
1 the target function 
in this section  we derivate an equivalent target function for  1   which is computable given the subset r and the entire set s  and thus ties a clear connection between the composition of r and the expected error of future decision making upon r.  to handle the problem that q is unknown  we define qc as the set of instances q in q whose inherent concept cq is c in s: 
	qc	{q q | cq c} c s 	 1  
　recall from section 1  the correct concept cq of an unlabeled instance q in q can be found in s. thus {qc | c s}is a partition of q. then it holds that:  
	e r q 	e   r q  q p q 
	 	 1  
e   r q  q p q 
 according to the definition of qc  the inherent concept cq for each instance q in qc is the concept c in s. thus  the ensemble of concepts that are selected by  r q for classifying q should be the set of concepts that are chosen for approximating concept c. moreover  the expected error of classifying q  termede   r q  q    can be represented by the expected error of the approximation on concept c. therefore  given that  r c  is the set of concepts in r that are selected to approximate concept c  and    r c  c  refers to the expected error of this approximation  it holds that: 
	e r q 	e   r q  q p q 
c s q qc
   r c  c p q 
c s q qc
	   r c  c  p q  	 1  
	c s	q qc
      r c  c p c  c s
　　　　　　　e' r s  where three notations need investigation: p c    r c  and    r c  c  . 
 first  concepts in s are collected independently and assumed to be equally important. thus  p c is calculated as: 
1
	p c   	 1  
| s |
 second   r c  defines the ensemble of concepts that can be used to approximate concept c. since concept c can be approximated by combining multiple concepts similar to c given that these similar concepts make different mistakes  kuncheva et al.  1; ruta et al.  1    r c  is defined as the concepts in r that are   by and large   similar to c: 
	 r c 	{c' r |	 c' c  d1} 	 1  
here threshold d1 controls the strictness level of allowing a concept to be used as the ensemble member for approximating c. it is set as 1 in our empirical studies. also note that  c' c    the expected error of using concept c' to approximate concept c  will be defined later. according to  1   concepts obviously inconsistent with c will be excluded  while concepts with slight deviation from c are permitted  in order to ensure the diversity in the ensemble. 
　third  for    r c  c   we start the discussion with the case that  r c  returns a single concept c'. thus   c' c  is the estimated error of using c' to approximate c and can be obtained from empirical test. given d  a set of instances that represents the instance distribution  labels are ignored   we use concept c to label the instances in d  and denote the labeled dataset as dc . then   c' c is naturally defined as: 
	 c' c 	1	 c' dc  	 1  
where  c'  dc   measures the consistency  e.g. accuracy  of concept c' on dc . note that d can be obtained by random sampling on all the available instances  and can be incrementally maintained in dynamic situations  vitter  1 .  however  if  r c  returns a group of concepts c*  how to compute the expected error  c* c    in this paper we use the majority voting as the combination strategy since it is both theoretically sound and practically powerful  kuncheva et al.  1; ruta et al.  1 . it is true that  c* c  can be tested on dc as in  1 . but it is very time consuming: consider when searching the optimal r that minimizes  1   large numbers of possible combinations of  r c  will be examined  and for each possible combination  we need compute    r c  c . fortunately  if the empirical error of each single concept c' on c has been calculated as  1   the exhaustive testing of  c* c  for all possible c* can be replaced by theoretical estimation  ruta et al.  1 : consider using an ensemble c* of m concepts to approximate concept c. and ei is the probability that the ith concept in c* gives the incorrect label in each voting  which is obtained by  1 . according to  ruta et al.  1   the distribution of the normalized incorrect rates of these m concepts in each voting  defined as the number of incorrect votes divided by m  can be approximated by the probability density function of the normal distribution f  x  whose mean and variance are: 
	1	m	1	m
	e	ei  	1	ei  1 ei   	 1  
	m i 1	m i 1
based on the above notions   c* c    the expected error of the ensemble c* via majority voting  is the probability that more than half of the m votes are incorrect: 
	 c* c 	f  x dx 	 1 
x 1
 finally  by combining  1    1    1    1  and  1   we get a computable target function which is equivalent to  1 :  
	e r q  e' r  s  r c  c  	 1 
where  r c  is defined as  1      r c  c  is computed as  1  when  r c  returns a single concept and is estimated as  1  when  r c  is an ensemble c*. 
　function  1  indicates that e r q  can be computed given the entire concept set s and a subset r. in this sense  it ties a clear connection between the composition of r and the performance of future decision making based on r. as a result  the sampling problem in  1  can be solved as an optimization process. 
input:   the concept collection s             the desired size of the reduced set  denoted by k             the empirical error between concepts as in  1              the threshold d1 in  1  output: the subset r 
 
algorithm: 
1. r 	 randomly sampling k concepts from s 
1. compute 	 	 r c  c  for each c in s 
1. repeat until r is invariant 
1 find the concept c' in s  that the reduction of  1  is       largest if inserting c' into r 
1 insert c' into r and update    r c  c  for each       c in s 
1 find the concept r in r that the increase of  1  is        smallest if removing r from r 
1 remove r from r and update    r c  c  for 
each c in s 
1 the removed r is labeled so that it will not be inserted into r again. figure 1: concept sampling algorithm  
 
　intuitively speaking  which concepts should be in r  for each concept c in s   c* c  computed as  1  and  1  reveals the relationship between the approximation error on c and the size m of the ensemble c* to approximate c: if  largely remains constant and is smaller than 1  the larger the m  the lower the   and thus the lower the approximation error in  1 . in this sense  we want that each concept c in s has an ensemble c* with enough qualified members from r. 
　on the one hand  the size of r is limited. thus  the more ensembles that a concept r can join simultaneously  the more likely r should be in r. note that whether a concept r can join the ensemble of a concept c is determined according to  1 . 
　on the other hand  based on  1  and  1   the smaller the size m of the ensemble c* of a concept c  the more dramatic the decrease of  when inserting a new member to c*  and thus the more substantial the decrease of the approximation error on c. thus  the concept that can enter the ensembles that few other concepts can enter should be deemed valuable  and the concept that mainly joins the ensembles that many other concepts can also join is more or less trivial. 
1 the sampling procedure 
clearly  it is very difficult to directly solve the optimization problem in  1  due to its nature of combinational optimization: there are almost infinite possible combinations of r given s. in this section  we proposed a feasible approach that iteratively improves the subset r  which is similarly in general to the methods designed for similar optimization problem  e.g.  as in  huang et al.  1 . more specifically  this method begins with a randomly selected subset r  and then successively improves it by firstly inserting into r an outside concept whose insertion maximizes the reduction of  1   and secondly  removing from r an inside concept whose deletion minimize the increase of  1 . this process is repeated until the subset is invariant  e.g. invariant in 1 continuous iterations . the detailed algorithm is shown in figure 1. 
 according to the discussion at the end of section 1  the value of a concept r is determined by  firstly  whether it can join many ensembles for concepts in s  and secondly  whether there exist many other concepts in r that can also join these ensembles. in this sense  the step 1-1 of the algorithm repeatedly insert the concepts that join many ensembles that are in short of members  and remove the concepts in r that can enter few ensembles or that mainly enter the ensembles that already have sufficient members. thus  the quality of r is continuously improved. note that step 1 is used to avoid being trapped in the local minima. 
　before formally analyzing the time complexity  we mention two points. firstly  we assume that for each concept r  the notion nb r  {c s |  r c  d1} can be efficiently accessed: it can be computed before the sampling  and thus each time concept sampling is executed  nb r  can be accessed directly. in fact  an equivalent notion of nb r  is 
that nb r  {c s | r  r c } : the concepts whose ensembles admit r. thus  when r is inserted into or removed from r  only concepts in nb r  will have their ensemble changed. this will dramatically facilitate the evaluation of the change of  1  for possible insertions or deletions in r. 
 secondly  the time-consuming integral computation in  1  depends on two variables  mean  and variance of normal distribution f  x  . thus we can discretize these two variables and then produce an integral table beforehand  from which  1  can be accessed directly. 
 accordingly  time complexity of the proposed method is o nn   where n is number of iterations  and n is the size of s:  step 1: o n . 
 step 1: o n . firstly  for each r in r  update and for concepts in nb r ; then for each c in s  compute    r c  c  as  1  or  1 ; at last  compute  1 . so the time complexity is o n . 
 step 1: o n . for each concept c' in s  inserting it to r only affects the concepts in nb c'   thus the reduction of  1  can be estimated quickly. 
 step 1: o |nb c' | .  
 step 1: o k . for each r in r  the increase of  1  is computed rapidly based upon nb r . k is the size of r. 
 step 1: o |nb r | . 
 step 1: o 1 . 
1 empirical results 
in this section  we present our empirical results. the goals of our experiments are:  1  to demonstrate the ability of our concept sampling method to preserve the performance of decision making while reducing the number of concepts maintained.  1  to justify the superiority of the proposed method over the straightforward selection method in terms of both performance and stability. we compared three methods: decision making upon the entire concept set s  es   upon the reduced set obtained by concept sampling  cs ; and upon the reduced set from random sampling  rs . 
 	 

	1.1 
1.1	1	1.1	1	1	1 x	subset size
figure 1: distinct concepts figure 1: accuracy in scescenario  scenario 1  nario 1 
 

	1.1 
1.1	1	1.1	1	1	1 x	subset size
figure 1: miscellaneous con- figure 1: accuracy in scecepts scenario  scenario 1  nario 1 

　given a concept set  the decision making strategy was:  1  for each instance q to be classified  a few evaluation data de  1 instances in our experiments  corresponding to the inherent concept cq was given1;  1  based on de  the following  suitable concepts  were selected: the concept c* that had the highest  c*  dc   see  1   plus all the concepts c satisfying  c  de   1  c*  de   .  1  the selected concepts were combined by majority voting. in our experiments  each concept was represented by a c1 tree  quinlan  1 . 
　to comprehensively examine the performance of our concept sampling method in various applications  three typical scenarios were included in our empirical studies:  1   distinct concepts scenario  and  1   miscellaneous concepts scenario  are two boundary cases. many real-world large collections of mixed concepts can be deemed as the  interpolations  of these two synthetic cases. then  in  1   real-world scenario   the real-world  adult  dataset was tested  in order to evaluate the effectiveness of the proposed method in real-life applications. 
　distinct concepts scenario: as in figure 1  1 circle centers were produced. a concept was generated based on one of the centers: 1d points  x and y coordinates were both in  1  1   that fell into the circle around the center  with radius 1  were positive examples and otherwise negative.  distinct concepts scenario  means that large numbers of concepts in the entire concept set s could be divided into distinct classes: concepts in the same class were similar  while concepts in different classes were distinct. each of the 1 circle centers in figure 1 indicated a distinct class. for each class  1 concepts were produced  each of which was trained from 1 random 1d points  and 1% noise on labels was added when training each concept. thus  even concepts in the same class  i.e. determined by the same circle center  would be slightly different. finally  1 concepts in 1 distinct classes were generated  which formed the complete concept set s.  
　the entire set s  reduced sets r   generated by cs  and r'  generated by rs  were tested on 1 testing datasets  each containing 1d points corresponding to one distinct concept class. we focused on the average classification accuracies over these 1 datasets. for totally 1 concepts in s  diverse sampling rates were tested for both cs and rs  which resulted in concept subsets with 1  1  1  1  1 concepts  respectively. the final results were averaged over 1 independent runs. 
　clearly  the ideal sampling algorithms should sensibly determine the number of remained concepts for each class. the results are shown in figure 1. it can be observed that:  1  cs method outperformed rs  on all the sampling rates  in term of the performance of decision making upon the reduced concept set.  1  the lower the sampling rate  the more obvious was the superiority of cs over rs. this is because when the number of concepts that can be retained is quite limited  the effectiveness of the sampling strategy becomes crucial: even the unsuitable allocation of one position will lead to remarkable performance degradation.  1  cs method with a sampling rate at 1% could provide largely the same classification accuracy as the original set s without sampling. miscellaneous concepts scenario: as in figure 1  1 concepts were generated. but different from figure 1  no distinct class existed and the 1 concepts were miscellaneous  each of which corresponded to a random center circle. the entire set s  reduced set r and r' were tested on 1 testing datasets  each containing 1 examples corresponding to one concept. we compared the average classification accuracy over these 1 datasets. also  different sampling rates were tested and the final results were averaged over 1 independent runs. from figure 1 we can observe the similar results as those in figure 1  i.e. in distinct concepts scenario . and this justifies the superiorities of cs method over rs when concepts in the complete set s are miscellaneous. 
　real-world scenario: in this scenario  we tested the three methods on the real-world  adult  dataset from uci repository. we divided both the training and the testing datasets into eight groups  based on the value of  workclass  attribute. two groups with very few examples were omitted. we extracted concepts from each of the remained six groups in the training dataset. since each concept was trained from 1 examples in a group  groups with more examples produced more concepts. totally 1 concepts were generated from the six groups  and formed the complete concept set s. 
　testing dataset without the two omitted groups was directly used for test. this is reasonable because the proportion among the size of six remained groups is similar between the training dataset and the testing dataset. thus  groups generating more concepts would have more instances in the testing dataset  which is consistent with the assumption that concepts in the complete set s should have the same probability to be useful  in future   i.e. in testing dataset . 
  	x 1	 	x 1	 	x 1	 
	1 	1 	1 	1 
1	1	1	1	1	1	1	1	1	1	1	1 subset size	subset size	subset size	subset size
figure 1: accuracy on adult  a  scenario 1  b  scenario 1  c  scenario 1 dataset  scenario 1  figure 1: variance of performance 　since label distribution in  adult  dataset is unbalanced  1% positive examples   the  accuracy  was defined as: the average of the accuracy on the positive examples and the accuracy on the negative examples. please note that this new measure should also be engaged as  c'  dc   in  1 . 
　the final results averaged over 1 independent runs are shown in figure 1. clearly  cs method obviously outperformed rs method  especially when the sampling rate is low. secondly  using cs method  a subset of 1 concepts offered competent performance compared with the complete set of 1 concepts. this justified the usefulness of our approach in real world applications: well preserving the performance of the decision making while dramatically reducing the number of concepts maintained. 
　performance variance: stability is very important for sampling methods. while figure 1  1 and 1 showed the average performance of cs and rs in 1 independent runs  figure 1 a   1 b  and 1 c  focus on the variances of the performance in 1 runs. these results demonstrate that cs method is much more reliable than rs method. it is because the optimization of the proposed target function always guarantees the high quality of the reduced concept set. 
1 conclusion 
in this paper  we introduced the novel problem of concept sampling: to retain an optimal subset from a large collection of mixed concepts to ensure efficient usage while guarantee that the performance of future decision making can be preserved by selectively combining the remained concepts. we provided a general framework of the problem  a target function that ties a clear connection between the composition of the concept subset and the error of future decision making  and an efficient sampling method based on the target function. the effectiveness and efficiency of the proposed method were discussed. extensive empirical studies suggested that  1  the proposed method can well preserves the performance of decision making while dramatically reduce the number of concepts maintained;  1  it has superiorities over straightforward method in terms of both performance and stability. through these studies  we demonstrated the usefulness of the proposed concept sampling method in handling large-scale mixed concepts. 
acknowledgments 
　the work was supported by natural science foundation of china 1. 
