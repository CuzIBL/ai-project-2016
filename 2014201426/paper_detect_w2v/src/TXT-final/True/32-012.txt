 
we present a new algorithm for polynomial time learning of near optimal behavior in stochastic games. this algorithm incorporates and integrates important recent results of kearns and singh   1  in reinforcement learning and of monderer and tennenholtz  in repeated games. in stochastic games we face an exploration vs. exploitation dilemma more complex than in markov decision processes. namely  given information about particular parts of a game matrix  how much effort should the agent invest in learning its unknown parts. we explain and address these issues within the class of single controller stochastic games. this solution can be extended to stochastic games in general. 
1 	introduction 
stochastic games  sgs  extend markov decision processes  mdps  to a multi-agent environment. in classical stochastic games  shapley  1   two players  the agent and the adversary  engage in a series of competitive interactions. thus  each state in an sg is associated with a game between the agent and the adversary. following each game  each of the players obtains some reward and both end up in a new state  i.e.  game . the reward obtained by the players is a function of the current state and their actions; the new state is a stochastic function of the current game and the players* actions. 
　much like in mdps  the agent's goal is to find an optimal  or near-optimal  policy  i.e.  a mapping from states  i.e.  games  to actions. however  unlike in mdps  such optimal policies are typically mixed  i.e.  each game is mapped to a probability distribution over actions rather than to a particular action. the optimization criteria for sgs are similar to those used in mdps  and include cumulative discounted reward  cumulative undiscounted reward  where the number of steps is finite but unbounded   average discounted reward  and average undiscounted reward; we concentrate on this last criterion. unfortunately  it is not known whether stationary optimal policies for infinite horizon stochastic games exist when the average undiscounted reward criterion is used. moreover  there are no known polynomial time algorithms for computing solutions for such games. however  there 
1 	machine learning 
moshe tennenholtz 
faculty of industrial eng. and management 
technion 
　haifa  1 israel moshet ie.technion.ac.il 
is an important class of stochastic games in which stationary optimal policies in the infinite horizon average undiscounted reward case exist  and they can be computed in polynomial time. this type of games is called single-controller stochastic games  scsgs   horijk and kallenberg  1; parthasarathy and raghavan  1; vrieze  1   a name which derives from the fact that the state  or game  transitions depend on the action of the agent alone. hence  the adversary's action influences the rewards only. in this paper  we concentrate on learning in scsgs containing zero-sum games. the extension of these results into zero-sum stochastic games is discussed in the full version of this paper  brafman and tennenholtz  1 . 
　the learning algorithm we present is based on the e1 algorithm  kearns and singh  1   a model-based learning algorithm  i.e  one in which a partial model of the mdp is formed  that has introduced a number of new concepts and ideas in the area of reinforcement learning. the extension of these ideas into sgs raises a number of issues that stem from the existence of an adversary whose behavior is unknown. in particular  this adversary can  at will  hide information from the agent by refraining from taking particular actions or by rarely playing such actions. therefore  certain aspects of the model may never be known to the agent or may take an unbounded time to learn  unlike in mdps. therefore  we cannot emulate the two phase approach of the e1 algorithm. there  die agent first attempts to learn enough about the model to obtain near-optimal return  after which it enters an exploitation phase. instead  we will have to allow for the possibility of continuous learning. 
　indeed  in stochastic games  a more complicated form of the exploration vs. exploitation problem arises. recall that the exploration vs. exploitation dilemma refers to the question of whether to play optimally given the current knowledge  or to attempt to increase knowledge at the risk of unknown losses. kearns and singh  ks   kearns and singh  1  solve this problem in the context of mdps by using the fact that  if we know the value of the optimal policy of the mdp  we can  at each stage  examine whether we have learned enough to guarantee ourselves this value. once this is the case  the agent needs no longer explore. unfortunately  in sgs  this is not die case. because of the ability of the adversary to hide parts of the game matrix  in many cases  we lack the information to calculate the value of a given policy. 

to overcome this  we employ techniques introduced by mon* derer and tennenholtz  in the context of learning in repeated games. namely  we explore at two levels. first  as in e1  we perform global exploration. that is  we attempt to learn some facts about different games. in addition  we perform local exploration in order to extend our knowledge about particular games. because it depends on the behavior of the adversary  this local exploration part cannot be a-priori bounded. this is to be contrasted with the initial exploration phase of 1  which takes polynomial time. 
　the algorithm presented in this paper addresses these and other issues and yields near optimal performance for the agent in time polynomial in the basic problem parameters. it is  to the best of our knowledge  the first such result in the context of stochastic games. previous algorithms for learning in sgs  littman  1; hu and wellman  1  were not concerned with analytic treatment and proof of efficiency  nor dealt explicitly with the exploration vs. exploitation issue in an efficient manner. however  littman does provide asymptotic convergence results. 
　in the following section we discuss single-controller stochastic games. in section 1 we present our measure of complexity. in section 1 we present our main theorem  which makes use of two basic ideas. a discussion of the first idea  based on a recent algorithm by kearns and singh  appears in section 1. a discussion of the second idea  which is in the spirit of work on learning in repeated games  and in particular follows recent work by monderer and tennenholtz   is presented in section 1. the synthesis of these ideas into a complete algorithm is presented in section 1. we conclude in section 1. the full version of this papa:   brafman and tennenholtz  1   contains all proofs  explains the issues that general stochastic games pose  and explains how the results presented here can be extended to the general case. 
1 preliminaries 
first  	we 	define 	single-controller-stochastic-games  scsg : 
definition 1 a single-controller-stochastic-game m on states consists of: 
  stage games: each state  is associated with a zero-sum game in strategic form  where the action set of each player is a. the first player is termed agent and the second player is termed adversary. 
  probabilistic transition function:  is the probability of a tmnsitionfivm stot given that the first player  termed agent  plays a. 
　for ease of exposition we normalize the payoffs of each state game to be non-negative real numbers between 1 and a 
　constant.  i.e. the sum of the players' payoffs for any 
joint action is always we will also take the number of actions to be constant. the set of possible histories of length t is   and the set of possible histories  /f  is the union of the sets of possible histories for all t  1  where the set of possible histories of length 1 is s. given an scsg  a policy for the agent is a mapping fromh to the set of possible probability distributions over a. hence  a policy determines the probability of choosing each particular action for each possible history. a stationary policy depends only on s instead of on h. such a policy associates with each state a probability distribution on the actions. 
　given an scsg m and a natural number t  we denote the expected t-step undiscounted average reward of a policy when the adversary follows a policy p  and 
　in the sequel we will assume that the scsg is ergodic in the sense that given any stationary policy of the agent  the probability of transition between each pair of states is greater than 1 regardless of the adversary behavior. this makes the value of each stationary policy well-defined  i.e.  it is independent of the initial state . in particular  the value of m is the value i of an optimal policy   and we know that for scsgs  there is no loss of generality in assuming this policy is stationary . 
　the ergodicity assumption is consistent with the treatment of  kearns and singh  1   and is quite natural for the following reasons. any markov chain defined by a policy has one or more absorbing subsets of states. that is  subsets of the state space such that once the agent enters them  he will remain in them. in the initial stages of learning  the agent cannot be expected to know which policies will lead to which sets of absorbing states  and so we cannot really influence the choice of an absorbing state set. however  once we are within such a set  we would like to quickly learn how to behave. this is basically what we  and ks  offer. 
1 our measure of complexity 
one of ks's contributions is the identification of the central parameter upon which the analysis of algorithms for learning in mdps must be based  namely  the mixing time. ks argue that it is unreasonable to refer to the efficiency of learning algorithms without referring to the efficiency of convergence to a desired value. they defined the e-neturn mixing time of a 
stationary policy π to be the smallest value of t after which guarantees an expected payoff of at least  more formally  in the context of scsgs we say that a policy n belongs to the set  of stationary policies whose e-ieturn mixing time is at most t  if after time t  returns an expected  average  undiscounted  payoff of at least for every possible adversary behavior. that is  on the average  we have to employ t steps of policy until our average accumulated reward is sufficiently close to the value of notice an agent that already knows an optimal policy whose c-return mixing time is t  will need this much time  on the average  to obtain a value of  almost  v. clearly  one cannot expect an agent lacking this information to perform better. 
brafman and tennenholtz 1 

that  1  the probability of failure of learning all columns in at 
least one set  from among 
deviations  i.e. selections of unknown columns  is smaller than  times the probability offailing to learn in one such set of deviations  and that  1  there are at most n entries to learn. 
we run the the algorithm for y stages  such that 
 is polynomial in the problem pa-
rameters. this will guarantee that the proportion of stages in which we do not follow is smaller than hence  
i 
these inequalities  we can indeed choose  polynomial  x and y that satisfy these conditions. 
　to complete the proof we need to show that we obtain the desired expected value. this follows from the fact that after y stages  with the corresponding probability  only at most of the stages correspond to adversary deviations  while in ofthe stages an expected pay off of is obtained. | 
　hence  once we have reached a situation where we have a policy that can obtain the desired value if the adversary behaves  nicely   we can modify this policy to a policy which obtains almost the desired value or learns a new fact about the stales  with overwhelming probability . thus  we tradeoff some exploitation for exploration in a manner that guarantees that if the adversary plays an unknown column polynomialy many tunes  we will learn this column after a polynomial number of steps. if the adversary rarely plays that column  we will rarely encounter it  and so the possible losses stemming from the randomization effect are almost surely insignificant given a sufficiently long  but polynomial  number of steps. 
1 	the algorithm 
the lsg algorithm can be executed for any desired number of steps . for sufficiently large values of t  polynomial in the problem parameters  a near optimal average return is guaranteed  as staled in theorem 1. 
1. initialize the set lof known states to be empty. 
1. if the current state is not in l: 
 a  randomly sample an action and execute it. 
 b  if foil owing this sampling the current state has been visited enough times  see definition 1  lemma 1  and the discussion after lemma 1   and at least one column of the game associated with it is known  add it to the set of known states l. 
1. if the current state is in l  i.e.  a known state   perform an off-line computation on ml in order to check whether a value of at least  can be guaranteed  assuming the adversary uses only actions that correspond to fully known columns. 
1 	machine learning 
1. if such a value can be guaranteed by a policy π  then the policy πm is executed.1 the run of πm is halted whenever a deviation of the adversary from the actions associated with fully known columns is observed  when the agent deviates from π when we have readied an unknown state  or when a new column in a state in l becomes fully known. 
1. otherwise  a payoff of can not be obtained  and a  global  exploration policy is executed  see section 1  for t steps or until an unknown state is reached. this policy is guaranteed by lemma 1 to reach a state outside l with probability of at l e a s t w i t h i n steps. 
1. in all cases  whenever an entry in a state game is learned  the value of it is kept in memory. 
　it is clear  from lemma 1  that the above algorithm is polynomial  i.e. leads to near optimal average return after polynomial time  in the appropriate parameters. it remains to be shown that this algorithm will yield the desired return with probability of at least for a g i v e n t o show this  we have to consider the four sources of failure of the algorithm. the first three appear in the context of the  algorithm  while the fourth stems from the need to perform local exploration. 
1. in some states the algorithm may have a poor estimate of the true next-state distribution. using standard chernoff bound analysis  alon et al  1   as was applied by ks in the case of mdps  we can show that  if the number of times an entry was explored is sufficiently large  kit still polynomial   the probability of an error larger than we wish for is small. notice that  for this analysis  our definition of known states enables us to ignore the fact the rewards in some columns are only partially known. 
1. repeated attempted explorations may fail to expose new information. this can be either because of failure to reach an unknown state and failure to sample a new entry in an unknown state. we can view a global exploration step  followed by random wandering  as a bernoulli trial  with a constant positive probability of success of at least  for reaching an unknown state and exploring a new entry in that state. the number of such trials which might be executed before all states become known can therefore be taken  since all of the trials can be treated as independent trials  to be polynomial  with a failure probability of at most . notice that in general  not all states need to become known. 
1. when we perform t-step exploitation with no lo-cal exploration we reach an expected return of 
  but the actual return may be lower. this point is handled by the fact that after polynomially local exploitations are carried out  can be obtained with a probability of failure of at most 
 this is obtained by standard chernoff bounds  and 
   'recall that  performs the optimal policy with respect to known states with some amount of local exploration. 

makes use of the fact that the standard deviation of the expected reward in a t-step policy is bounded. 
1. the agent may get a low payoff because it does not know the entries in some column and does not learn new entries in unknown columns. this is handled by 
lemma 1  where we can choose the failure probability   as needed. 
　by making the failure probability less than  at each of the above stages  we are able to get the desired result 
　finally  we remove the assumptions that both the value and its e-return mixing time are known. this is straightforward and almost identical to the treatment given by  kearns and singh  1 . first  as to knowledge of the value  this is needed when we have to decide whether to explore or exploit. lemma 1 states that we can either get enough return or we have a sufficiently high probability of reaching a new state quickly. one can calculate this probability without knowledge of the value and perform exploration whenever this probability exceeds the desired bound. notice that by employing this technique  with overwhelming probability we remain with known states only after polynomial time. at this point  we can compute an optimal policy. hence  we can safely apply this exploration bias. 
　next  we must deal with the lack of knowledge of t. the idea is as follows: from the proofs of the algorithm's properties  one can deduce some polynomial p in the problem parameters such that if t is the mixing-time  then after p t  steps we are guaranteed  with probability 1 -  the desirable return. hence  we can simply attempt to run this algorithm for t = 1 1 .... for each value of t  we run the algorithm p t  time. suppose that  is the mixing time  then a f t e r s t e p s   we will obtain the desirable return. 
　one thing to notice is that this algorithm does not have a final halting time and will be applied continuously as long as the agent is functioning in its environment. the only caveat is that at some point our current mixing time candidate t will be exponential in the actual mixing time to  at which point each step of the algorithm will require an exponential calculation. however  this will occur only after an exponential number of steps. this is true for the  algorithm too. 
　another point worth mentioning is that in scsgs  the agent may never know some of the columns. consequently  if π is the optimal policy given full information about the game  the agent may actually converge to a policy  that differs from π  but which yields the best return given the adversary *s actual behavior. this return will be no smaller than the return guaranteed by π. the mixing time of will  in general  differ from the mixing time of . however  we are guaranteed that if to is the e-return mixing time of   and v is its value  after time polynomial in   the agent's actual return will be at least v  subject to the deviations afforded by the theorem . 
1 	conclusion 
we described an algorithm for learning in a restricted class of stochastic games. this algorithm extends earlier work of kearns mid singh {1  on learning in mdps using the techniques of monderer and tennenholtz  for learning in repeated games. these results can be extended to stochastic games in general  as explained in the full paper  brafman and tennenholtz  1 . unfortunately  die adversary's ability to influence transitions can lead to very large mixing times and slower  though still polynomial  convergence. 
　in describing the algorithm we aimed for clarity rather than efficiency  with the sole constraint of providing a polynomial time algorithm. a more careful analysis will lead to reduced running time. it is worth noting a simple  but interesting  corollary of our results. if the algorithm is run concurrently by the agent and the adversary  where we consider stochastic games with stationary equilibrium  they are both guaranteed to attain near optimal performance  i.e.  the value of the game  within the allowed error bounds . finally  we remark that our algorithm would seem a natural candidate for learning in non-stochastic environment  although additional assumptions about the nature of the environment could be used to improve its efficiency. 
acknowledgement: we thank the anonymous reviewers for their useful comments. the first author was partially funded by the paul ivanier center for robotics research and production management. 
