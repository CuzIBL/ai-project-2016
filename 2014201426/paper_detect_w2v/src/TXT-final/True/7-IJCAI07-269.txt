
we are developing a corpus-based approach for the prediction of help-desk responses from features in customers' emails  where responses are represented at two levels of granularity: document and sentence. we present an automatic and humanbased evaluation of our system's responses. the automatic evaluation involves textual comparisons between generated responses and responses composed by help-desk operators. our results show that both levels of granularity produce good responses  addressing inquiries of different kinds. the humanbased evaluation measures response informativeness  and confirms our conclusion that both levels of granularity produce useful responses.
1 introduction
email inquiries sent to help desks often  revolve around a small set of common questions and issues .1 this means that help-desk operators spend most of their time dealing with problems that have been previously addressed. further  a significant proportion of help-desk responses contain a low level of technical content  corresponding  for example  to inquiries addressed to the wrong group  or insufficient detail provided by the customer. organizations and clients would benefit if the efforts of human operatorswere focused on difficult  atypical problems  and an automated process was employed to deal with the easier problems.
　in this paper  we report on our experiments with corpusbased approaches for the automation of help-desk responses. our study was based on a log of 1 email dialogues between users and help-desk operators at hewlett-packard. however  to focus our work  we used a sub-corpus of 1 email dialogues  which consisted of two-turn dialogueswhere the answers were reasonablyconcise  1 lines at most . these dialogues deal with a variety of user requests  which include requests for technical assistance  inquiries about products  and queries about how to return faulty products or parts.
　analysis of our corpus reveals that requests containing precise information  such as product names or part specifications  sometimes elicit helpful  precise answers referring to
is there a way to disable the nat firewall on the cp-1w so i don't get a private ip address through the wireless network 
 unfortunately  you have reached the incorrect eresponse queue for your unit. your device is supported at the following link  or at 1-phone-number. we apologize for the inconvenience.
figure 1: a sample request-response pair.
this information  while other times they elicit answers that do not refer to the query terms  but contain generic information  as seen in the examplein figure 1 our previousexperiments show that a standard document retrieval approach  where a new request is matched in its entirety with previous requests or responses  is successful only in very few cases  zukerman and marom  1 . we posit that this is because  1  many requests raise multiple issues  and hence do not match well any one document;  1  the language variability in the requests is very high; and  1  as seen in figure 1  the replies to many technical requests are largely non-technical  and hence do not match technical terms in the requests.
　these observations lead us to consider a predictive approach  which uses correlations between features of requests and responses to guide response generation. in principle  correlations could be modelled directly between terms in the requests and responses  berger and mittal  1 . however  we have observed that responses in our corpus exhibit strong regularities  mainly due to the fact that operators are equipped with in-house manuals containing prescribed answers. for example  figure 1 shows two responses that contain parts that are almost identical  the two italicized sentences . the existence of these regularities motivates us to generate abstractions of responses  rather than deal with low-level response terms. in contrast  similar regularities do not exist in the requests so we choose to represent them at a term-based level. the request-response correlations are then modeled at these two levels. as seen in the examples in figures 1 and 1  the desirable granularity for representing responses can vary: it can be as fine as sentences  figure 1   or as coarse as complete documents  figure 1 . the investigations reported here involve these two levels of granularity  leading to the sent-pred and doc-pred methods respectively  section 1 .
 if you are able to see the internet then it sounds like it is working  you may want to get in touch with your it department to see if you need to make any changes to your settings to get it to work. try performing a soft reset  by pressing the stylus pen in the small hole on the bottom left hand side of the ipaq and then release.
 i would recommend doing a soft reset by pressing the stylus pen in the small hole on the left hand side of the ipaq and then release. then charge the unit overnight to make sure it has been long enough and then see what happens. if the battery is not charging then the unit will need to be sent in for repair.
figure 1: responses that share a sentence.
　as for any learning task  building prediction models for responses at an abstracted level of representation has advantages and drawbacks. the advantages are that the learning is more focused  and it deals with data of reduced sparsity. the drawback is that there is some loss of information when somewhat dissimilar response units  sentences or documents  are grouped together. in order to overcome this disadvantage  we have developed a prediction-retrieval hybrid approach  which predicts groups of responses  and then selects between dissimilar response units by matching them with request terms. we investigate this approach at the sentence level  leading to the sent-hybrid method  section 1 .
　note that the doc-pred method essentially re-uses an existing response in the corpus to address a new request. in contrast  the two sentence-based methods combine sentences from multiple response documents to produce a new response  as is done in multi-document summarization  filatova and hatzivassiloglou  1 . hence  unlike doc-pred  sent-pred and sent-hybrid may produce partial responses. in this paper  we investigate when the different methods are applicable  and whether individual methods are uniquely successful in certain situations. specifically  we consider the trade off between partial  high-precision responses  and complete responses that may contain irrelevant information.
　the rest of this paper is organized as follows. in the next section  we describe our three prediction methods  followed by the evaluation of their results in section 1. in section 1  we discuss related research  and then present our conclusions and plans for future work in section 1.
1 methods
in this section  we present the implementation details of our three prediction methods. note that some of the methods were implemented independently of each other at different stages of our project. hence  there are minor implementational variations  such as choice of machine learning algorithms and some discrepancies regarding features. we plan to bridge over these differences in the near future  but are confident that they do not impede the aim of the current study: evaluating a predictive approach to response generation.
1 document prediction  doc-pred 
this prediction method first groups similar response documents  emails  in the corpus into response clusters. for each request it then predicts a response cluster on the basis of the request features  and selects the response that is most representative of the cluster  closest to the centroid . this method predicts a group of responses similar to the response in figure 1 from the input term  cp-1w .
　the clustering is performed in advance of the prediction process by the clustering program snob  which performs mixture modelling combined with model selection based on the minimum message length criterion  wallace and boulton  1 . we chose this program because one does not have to specify in advance the number of clusters. we use a binary representation whereby the lemmatized content words in the corpus make up the components of an input vector  and its values correspond to the absence or presence of each word in the response  this representation is known as bag-of-words .
　the predictive model is a decision graph  oliver  1  trained on  1  input features: unigram and bigram lemmas in the request 1 and  1  target feature: the identifier of the response cluster that contains the actual response for the request. the model provides a prediction of which response cluster is most suitable for a given request  as well as a level of confidence in this prediction. if the confidence is not sufficiently high  we do not attempt to produce a response. 1 sentence prediction  sent-pred 
as for the doc-pred method  the sent-pred method starts by abstracting the responses. it uses the same clustering program  snob  to cluster sentences into sentence clusters  scs   using the representation used for doc-pred.1 unlike the doc-pred method  where only a single response cluster is predicted  resulting in a single response document being selected  in the sent-pred method several scs are predicted. this is because here we are trying to collate multiple sentences into one response. each request is used to predict promising scs  and a response is composed by extracting one sentence from each such sc. because the sentences in each sc originate from different response documents  the process of selecting them for a new response corresponds to multidocument summarization. in fact  our selection mechanism is based on a multi-document summarization formulation proposed by filatova and hatzivassiloglou .
　to illustrate these ideas  consider the fictitious example in figure 1. three small scs are shown in the example  in practice the scs can have tens and hundreds of sentences . the thick arrows correspond to high-confidence predictions  while the thin arrows correspond to sentence selection. the other components of the diagram demonstrate the sent-hybrid approach  section 1 . in this example  three of the request terms -  repair    faulty  and  monitor  - result in a confident prediction of two scs: sc1 and sc1. the sentences in sc1 are identical  so we can arbitrarily select a sentence to include in the generated response. in contrast  although the sentences in sc1 are rather similar  we are less confident in arbitrarily selecting a sentence from it.
the predictive model is a support vector machine  svm .

figure 1: a fictitious example demonstrating the sent-pred and sent-hybrid methods.a separate svm is trained for each sc  with unigram and bigramlemmas in a requestas input features  and a binarytarget feature specifying whether the sc contains a sentence from the response to this request. during the prediction stage  the svms predict zero or more scs for each request  as shown in figure 1. the sentence closest to the centroid is then extracted from each highly cohesive sc predicted with high confidence. a high-confidence prediction indicates that the sentence is relevant to many requests that share certain regularities. a cluster is cohesive if the sentences in it are similar to each other  which means that it is possible to obtain a sentence that represents the cluster adequately. these stringent requirements placed on confidence and cohesion mean that the sent-pred method often yields partial responses. the cohesion of an sc is calculated as

where n is the number of content lemmas  pr wk （ sc  is the probability that lemma wk is used in the sc  obtained from the centroid   and α is a parameter that represents how strict we are when judging the similarity between sentences. we have used this parameter  instead of raw probabilities  because strictness in judging sentence similarity is a subjective matter that should be decided by the users of the system. the sensitivity analysis we performed for this parameter is discussed in  marom and zukerman  1 .
　our formula implements the idea that a cohesive group of sentences should agree on both the words that are included in these sentences and the words that are omitted. for instance  the italicized sentences in figure 1 belong to an sc with cohesion 1. for values of α close to zero  our formula behaves like entropy  favouring very strong agreement on word usage and omission. the latter is necessary because just knowing that certain words appear in a high percentage of the sentences in a cluster is not sufficient to determine how similar are these sentences. one also must know that these sentences exclude most other words. 1 sentence prediction-retrieval  sent-hybrid 
as we can see in cluster sc1 in figure 1  it is possible for an sc to be strongly predicted without being sufficiently cohesive for a confident selection of a representative sentence. however  sometimes the ambiguity can be resolved through cues in the request. in this example  one of the sentences matches the request terms better than the other sentences  as it contains the word  monitor . the sent-hybrid method complements the prediction component of sent-pred with a retrieval component  and thus forms a hybrid.
　this retrieval component implements the traditional information retrieval paradigm  salton and mcgill  1   where a  query  is represented by its content terms  and the system retrieves a set of documents that best matches this query. in the help-desk domain  a good candidate for sentence retrieval contains terms that appear in the request  but also contains other terms that hopefully provide the requested information  in the example in figure 1  the  best  sentence in sc1 shares only one term with the request . therefore  we perform a recall-based retrieval  where we find sentences that match as many terms as possible in the request  but are allowed to contain additional terms. recall is calculated as
Σ tf.idf of lemmas in request sent & response sent
recall = 
Σ tf.idf of lemmas in request sentence
　we have decided to treat the individual sentences in a request email as separate  queries   rather than treat the complete email as a single query  because a response sentence is more likely to have a high recall when matched against a single request sentence as opposed to a whole document.
　for highly cohesive scs predicted with high confidence  we select a representative sentence as before.
　for scs with medium cohesion predicted with high confidence  we attempt to match its sentences with a request sentence. here we use a liberal  low  recall threshold  because the high prediction confidence guarantees that the sentences in the cluster are suitable for the request. the role of retrieval in this situation is to select the sentence whose content lemmas best match the request  regardlessof how well they match  the non-content lemmas  also known as function words or stop words  are excluded from this account .
　for uncohesive clusters or clusters predicted with low confidence  we can rely only on retrieval. now we must use a more conservative recall threshold to ensure that only very highly-matching sentences are included in the response. sc1 in figure 1 is an example of an sc for which there is insufficient evidence to form strong correlations between it and request terms. however  we can see that one of its sentences matches very well the second sentence in the request. in fact  all the content words in that request sentence are matched  resulting in a perfect recall score of 1.
　once we have the set of candidate response sentences that satisfy the appropriate recall thresholds  we remove redundant sentences. since sentences that belong to the same medium-cohesivesc are quite similar to each other  it is sufficient to select a single sentence - that with the highest recall. sentences from uncohesive clusters are deemed sufficiently different to each other  so they can all be retained. all the retained sentences will appear in the generated response  at present  these sentences are treated as a set  and are not organized into a coherent reply .
1 evaluation
in this section  we examine the performance of our three prediction methods. our experimental setup involves a standard 1-fold validation  where we repeatedly train on 1% of a dataset and test on the remaining 1%. we present each test split as the set of new cases to be addressed by the system.
1 performance measures
we are interested in two performance indicators: coverage and quality.
　coverage is the proportion of requests for which a response can be generated. we wish to determine whether the three methods presented in the previous section are applicable in different situations  i.e.  how exclusively they address or  cover  different requests. each of these methods specifies a requisite level of confidence for the generation of a response. if the response planned by a method fails to meet this confidence level for a request  then the request is not covered by this method. since the sentence-based methods generate partial responses  we say that their responses cover a request if they contain at least one sentence generated with high confidence. non-informative sentences  such as  thank you for contacting hp   which are often produced by these methods  have been excluded from our calculations  in order to have a useful comparison between our methods.
　quality is a subjective measure  best judged by users of a deployed system. here we approximate a quality assessment by means of two experiments: a preliminary human-based study where people evaluate a small subset of the responses generated by our system  section 1 ; and a comprehensive automatic evaluation that treats the responses generated by the help-desk operators as model responses  and performs text-based comparisons between these responses and the generated ones  section 1 . we are interested in the correctness and completeness of a generated response. the former measures how much of its information is correct  and the latter measures its overall similarity with the model response. we consider correctness separately because it does not penalize missing information  enabling us to better assess our sentence-based methods. these measures are approximated by means of two measures from informationretrieval  salton table 1: results of the automatic evaluation
methodcoveragequality average  stdev. precisionf-scoredoc-pred1%1  1 1  1 sent-pred1%1  1 1  1 sent-hybrid1%1  1 1  1 and mcgill  1 : precision approximates correctness  and f-score approximates completeness and correctness in combination. precision and f-score are calculated as follows using a word-by-word comparison  stop-words are excluded .1
           # words in both model and generated response precision = 
               # of words in generated response # words in both model and generated response recall = 
# of words in model response
　　　　1 〜 precision 〜 recall f-score = 
precision + recall
1 results
the combined coverage of the three methods is 1%. this means that for 1% of the requests  a partial or complete response can be generated by at least one of the methods. table 1 shows the individual results obtained by the different methods. the doc-pred method can address 1% of the requests. only an overall 1% of the requests are uniquely addressed by this method  but the generated responses are of a fairly high quality  with an average precision and f-score of 1. notice the large standard deviation of these averages  suggesting a somewhat inconsistent behaviour. this is due to the fact that this method gives good results only when it predicts a complete generic response that is very similar to the model response. however  when this is not the case  performance degrades substantially. this means that doc-pred is suitable when requests that share some regularity receive a complete template response.
　the sent-pred method can find regularities at the subdocument level  and therefore deal with cases where partial responses can be generated. it produces responses for 1% of the requests  and does so with a consistently high precision  average 1  standard deviation 1 . only an overall 1% of the requests are uniquely addressed by this method. however  for the cases that are shared between this method and other ones  it is useful to compare the actual quality of the generated responses. for example  with respect to doc-pred  for 1% of the cases sent-predeither uniquely addresses requests  or jointly addresses requests but has a higher f-score. this means that in some cases a partial response has a higher quality than a complete one. note that since sent-pred has a higher average precision than doc-pred  its lower average f-score must be due to a lower average recall. this confirms that sent-pred produces partial responses.
　the sent-hybrid method extends the sent-pred method by employing sentence retrieval  and thus has a higher coverage
my screen is coming up reversed  mirrored . there must be something loose electronically because if i put the stylus in it's hole and move it back and forth  i can get the screen to display properly momentarily. please advise where to send for repairs.
 to get the ipaq serviced  you can call 1-phone-number  options 1  1  enter a 1 digit phone number   1. enter your phone number twice and then wait for the routing center to put you through to a technician with technical support. they can get the unit picked up and brought to our service center.
　to get the ipaq repaired  battery  stylus lock and screen   please call 1-phone-number  options 1  1  enter a 1 digit phone number   1.
figure 1: example demonstrating the sent-hybrid method.
 1% . this is because the retrieval component can often include sentences from scs with medium and low cohesion  which might otherwise be excluded. however  this is at the expense of precision. retrieval selects sentences that match closely a given request  but this selection can differ from the  selections  made by the operator in the model response. precision  and hence f-score  penalizes such sentences  even when they are more appropriate than those in the model response. for example  consider the request-response pair at the top of figure 1. the response is quite generic  and is used almost identically for several other requests. the sent-hybrid method almost reproduces this response  replacing the first sentence with the one shown at the bottom of figure 1. this sentence  which matches more request words than the first sentence in the model response  was selected from an sc that is not highly cohesive  and contains sentences that describe different reasons for setting up a repair  the matching word is  screen  . overall  the sent-hybrid method outperforms the other methods in about 1% of the cases  where it either uniquely addresses requests  or addresses them jointly with other methods but produces responses with a higher f-score.
1 human judgements
the purpose of this part of the evaluation is twofold. first  we want to compare the quality of responses generated at different levels of granularity. second  we want to evaluate cases where only the sentence-based methods can produce a response  and therefore establish whether such responses  which are often partial  provide a good alternative to a non-response. hence  we constructed two evaluation sets: one containing responses generated by doc-pred and senthybrid  and one containing responses generated by sent-pred and sent-hybrid. the latter evaluation set enables us to further examine the contribution of the retrieval component of the hybrid approach. each evaluation set comprises 1 cases  and each case contains a request email  the model response email  and the two system-generated responses. we asked four judges to rate the generated responses on several criteria. owing to space limitations  we report here only on one criterion: informativeness. we used a scale from 1 to 1  where 1 corresponds to  not at all informative  and 1 corresponds to  very informative . the judges were instructed to position themselves as users of the system  who know that they are receiving an automated response  which is likely to arrive faster

figure 1: human judgements of informativeness.
than a response composed by an operator.
　we maximized the coverage of this study by allocating different cases to each judge  thus avoiding a situation where a particularly good or bad set of cases is evaluated by all the judges. since the judges do not evaluate the same cases  we cannot employ standard inter-tagger agreement measures. still  it is necessary to have some measure of agreement between judges  and control for bias from specific judges or specific cases. we do this separately for each prediction method by performingpairwise significance testing  using the wilcoxon rank sum test for equal medians   where the data from two judges are treated as independent samples. we then remove the data from a particular judge if he or she has a significant disagreement with other judges. this happened with one judge  who was significantly more lenient than the others on the sent-pred method. since there are four judges  we have an overall maximum of 1 cases in each evaluation set.
　figure 1 shows the results for the two evaluation sets. the top part  which is for the first set  shows that when both docpred and sent-hybrid are applicable  the former receives an overall preference  rarely receiving a zero informativeness judgement. since the two methods are evaluated together for the same set of cases  we can perform a paired significance test for differences between them. using a wilcoxon signed rank test for a zero median difference  we obtain a p-value  indicating that the differences in judgements between the two methods are statistically significant.
　the bottom part of figure 1 is for the second evaluation set  comparing the two sentence-based methods. recall that this evaluation set comprises cases that were addressed only by these two methods. thus  the first important observation from this chart is that when a complete response cannot be reused  a response collated from individual sentences is often judged to contain some level of informativeness. the second observation from this chart is that there does not seem to be a difference between the two methods. in fact  the wilcoxon signed rank test produces a p-value of 1 for the second evaluation set  thus confirming that the differencesare notstatistically significant. it is encouraging that the performance of sent-hybrid is at least as good as that of sent-pred  because we saw in the automatic evaluation that sent-hybrid has a higher coverage. however  it is somewhat surprising that sent-hybrid did not perform better than sent-pred. it is worth noting that there were a few cases where judges commented that a generated response contained additional useful information not appearing in the model response  as seen in the example in figure 1. we confirmed that these responses were generated by the sent-hybrid method  but this did not occur sufficiently to show up in the results  and requires further investigation.
1 related work
there are very few reported attempts at help-desk response automation using purely corpus-based approaches  where the corpus is made up of request-response pairs. the retrieval system eresponder  carmel et al.  1  retrieves a list of request-response pairs and presents a ranked list of responses to the user. bichel and scheffer  also implement this kind of document retrieval approach  as well as an approach similar to our doc-pred. berger and mittal  present a summarization approach more akin to our sent-pred method  but where a prediction model is learned directly from terms in requests and responses. the contribution of our work lies in the investigation of predictive approaches at different levels of granularity and the consideration of a hybrid predictionretrieval approach.
1 conclusion and future work
we have presented a predictive approach to the automation of help-desk responses  applied at two levels of granularity. our methods take advantage of the strong regularities that exist in help-desk responses by abstracting them either at the document level or at the sentence level. they then find correlations between requests and responses to build predictive models for addressing new requests. our hybrid method was designed to overcome the loss of information resulting from abstracting response sentences. the use of sentence retrieval in combination with prediction was shown to be useful for better tailoring a response to a request. in future work  we intend to investigate a more focused retrieval approach that utilizes syntactic matching of sentences. for example  it may be beneficial to favour sentences that match on verbs. as another extension of our work we would like to improve the representation used for clustering  prediction and retrieval by using features that incorporate word-based similarity metrics  pedersen et al.  1 .
　our results show that each of the prediction methods can address a significant portion of the requests  and that when the re-use of a complete response is not possible  the collation of sentences into a partial response can be useful. a future avenue of research is thus to characterize situations where the different methods are applicable  in order to derive decision procedures that determine the best method automatically.
　our results also suggest that the automatic evaluation method requires further consideration. precision  and hence f-score  penalize good responses that are more informative than the model response. our human judgements provide a subjective indication of the quality of a generated response. however  a more extensive user study would provide a more conclusive evaluation of the system  and could also be used to determine preferences regarding partial responses.
acknowledgments
this research was supported in part by grant lp1from the australian research council and by an endowment from hewlett-packard. the authors also thank hewlett-packardfor the extensive anonymized help-desk data.
