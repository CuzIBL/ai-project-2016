 
　　this paper examines the relationship between probability theory and reasoning in uncertainty  and argues that  contra opposing views  probability theory does have a place in reasoning in uncertainty  but that its place is more restricted than many of its advocates claim. two major theses are argued for.  1  reasoning from probabilities works well in domains which permit a clear analysis in terms of events over outcome spaces and for which either large bodies of evidence or long periods of  training  are available; but such domains are relatively rare  and even there  care must be taken in interpreting probability results.  1  some generalizations with which ai applications must concern themselves are not statistical in nature  in the sense that statistical generalizations neither capture their meanings nor even preserve their truth values. for these contexts  different models will be needed. 
i introduction 
　　probability estimates have been used to aid decision making in ai systems for over ten years now  shortliffe and buchanan 1; duda  hart  and nilsson 1   and have been under fire even longer  mccarthy and hayes 1 . recently  the increased attention devoted to common sense reasoning and reasoning in contexts of uncertainty has fueled the debate  and clearly defined battle lines have emerged. supporters point to a well-developed rigorous formalism for dealing with uncertainty  cheeseman 1; ginsberg 1 . opponents continue to object that applying bayesian methods to calculate probabilities requires information that is not generally available   where do the numbers come from     and that  insofar as it is available  is usually tainted  kahneman  slovic  and tversky 1 . in addition  they charge that using probabilites suppresses important information  cohen and grinberg 1; sullivan and cohen 1 ; that statistical analyses fail to distinguish uncertainty from inherent  fuzziness   zadeh 1 ; that judgements of typicality and normic generalizations underlie much reasoning from uncertainty and are not probabilistic  rosch 1; rosch and 
mervis 1; rosch  mervis  gray  johnson and boyes-braem 1; scriven 1; scriven 1; nutter 1 ; and so on. 
　　even supporters of statistical approaches separate into those who prefer straightforward bayesian analysis  cheeseman 1   those who prefer the dempster-shafer  dempster 1; shafer 1  approach  yen 1; ginsberg 1; ginsberg 1; strat 1; yu and stephanou 1 ; and those who prefer some other variant of the bayesian approach  snow 1 . others are developing theones to permit logic-style inferences about probabilites  especially in the realm of reasoning about independence  pearl 1   while continuing to argue that probability provides an epistemically adequate and effective framework for the general problem of reasoning in uncertainty  henrion 1 . 
　　both sides present their positions forcibly and plausibly  but not always with attention either to their opponents' points or to independent substantiation. as a consequence  arguments on both sides of the fence have tended to produce more heat than light. this paper aims to lower the temperature while illuminating the terrain. in the end  this paper argues neither for nor against statistical reasoning in ai. rather it argues that statistical methods do apply  but only in some cases  and that care must be taken to identify those cases correctly  to fulfill requirements for reliable results  and to make sure that what is treated as a probability is indeed probabilistic in nature. 
　　the body of this paper consists of three sections. the first describes elementary aspects of probability theory  to form a basis for discussion. the second characterizes some of the features an ai application and its domain must have for statistical analysis to be useful. the final section describes cases of uncertainty that cannot be represented as probability. 
ii preliminary remarks on probability 
a. what arc probabilities  
　　the philosophical nature of probabilities matters less for ai purposes than what kinds of phenomena classical and bayesian probability analyses model. however  given the vehement disputes on this issue  a few observations may be useful. statistics begins investigating probabilities in any particular instance by defining  at least loosely  a space of outcomes  that is  mutually exclusive observations of test results. events are sets of outcomes from that space. when probability theorists refer to probabilities  they typically mean event probabilities  that is  the likelihood that the outcome of a particular test will belong to the set which defines the event. this likelihood is traditionally defined in terms of frequency: given a  sufficiently large  number of tests  what proportion of all outcomes fall in the event set  
　　the frequency view has been attacked for centuries; a recent criticism can be found in  cheeseman  1 . probably the most pursuasive argument against the frequency view from an ai standpoint is that on that view  each event has exactly one correct probability. but for ai purposes  such a 
　　probability is neither attainable nor in some cases even interesting. rather  we are interested in the probability of an 
	nutter 	1 

hypothesis given the current evidence. critics further object that the frequency theory  restricts probability to domains where repeated experiments  e.g. sampling  are possible  or at least conceivable   cheeseman  1 . in addition  the concept of  long run frequency  has bothered people for centuries. how long  how do you know  why should 
 large numbers   how large   have special properties  
   these objections can be met without deserting a frequency-based approach. the probability of any hypothesis on the basis of the current evidence can be - and in normal statistical practice is - interpreted as the conditional probability of the hypothesis given the conjunction of events which that evidence reflects. in other words  in addition to a single  well-defined probability for every event over the space  the frequency view also provides a way to represent precisely the relativized probabilities we are most interested in  and these are exactly the probabilities that statisticians investigate . 
   classical statistics texts also contain chapters on game theory and decision theory which describe techniques for estimating probabilities on the basis of very small samples  see e.g. freund and walpole 1  chapter 1  or almost any other freshman text . so not only does classical statistics recognize that this can be done  the theory instructs the interested in how to do it; only  it also warns not to place great faith in the accuracy of such estimates. 
   the hardest question to meet is the philosophical question of the significance of the law of large numbers: what does it mean to talk about  long run  frequencies  classical statistics provides some tests for whether an actual sample is large enough; but that cannot answer the philosophical question. the best that can be said here is that other approaches have their own philosophical questions that they cannot answer  but none of these philosophical questions seem to affect ai. 
   the classical alternative to the frequency view is the subjective probabilities view  which derives from the views of the 1th century english clergyman thomas bayes. on this approach  probabilities measure certainty levels. two options here should be distinguished. the first is well-defined  and clearly subjective  as philosophers use the term : the probability of an event given the current evidence is the measure of the degree to which a particular specific  real live  individual believes that the event will occur on the basis of that evidence. the problem here is evident: people will believe all sorts of things  and different things at different times  for different reasons or none at all. there is no reason to suppose that one person's  probability  in this sense will match another's  and no grounds for a science of probability at all. 
   it is unlikely that many supporters of subjective probabilities ever meant that  though they often seem to say it: 
... the following definition is put forward as one that withstands all previous criticisms: the  conditional  
probability of a proposition given particular evidence is a real number between zero and one  that is a measure of an entity's belief in that proposition  given the evidence. 
 cheeseman 1; emphasis in original  
the alternative  and the view that is actually held  is that probabilities measure how much an ideal rational subject ought to believe that an event will occur  given the evidence. this option makes probabilities relative  to evidence   but not really 
1 	knowledge representation 
subjective: no actual subjects are involved any more. this approach has two difficulties  both as obvious and as pressing as the problem the frequency theory has with understanding the long run. first  what makes someone an ideal rational subject  probability cannot be considered well-defined on this view until that is spelled out. second  how other than by measured frequencies can we establish the degree to which such a subject ought to believe that a given event will occur  
b. htm do probabilities behave  
   the mathematics for measuring probabilities is the same on both these competing definitions: bayes's theorem is a theorem of classical statistics  for example. the significant differences come in questions of when it is legitimate to apply the formulas  and what they can be taken as establishing. in this regard  it seems hat the frequency analysis has an advantage: designers of ai systems generally care less whether their systems  ought  to believe their answers than how often those answers are right. for systems whose judgements have practical consequences  we should measure and maximize that if we measure anything. but whatever philosophical view of probabilities we take  the mathematics always agrees with long run frequency expectations in all situations in which we can make sense of them. 
   several of the mathematical features of event probabilities and their measurement are counterintuitive enough to be worth mentioning. for example  experiments structured by statisticians always assume more than is known. statistical experiments define a hypothesis about the likelihood of an event  and then compare actual observations against the predictions of the hypothesis. this fact has implications frequently overlooked in ai debates. one such implication relates to the controversy over the so-called assumption of maximum entropy: the policy of assuming all events independent unless a connection has been found  see cheeseman 1 . opponents claim that this practice involves assuming more than is known  since the events in question may be dependent; supporters respond that the assumption of maximum entropy provides  a neutral background against which any systematic  non-random  patterns can be observed....  w ithout this prediction  it is difficult to detect if the current information is incomplete  and thus to discover new information   cheeseman 1 . the truth is that any hypothesis provides a background for detecting deviation; and no experiment can be run without some hypothesis. the real question  then  is which hypotheses yield the best results without extensive  training ; this question must be answered by experiment. 
   another  and for ai more serious  implication is that the outcome of an appropriate experiment must be observable independent of the statistical prediction. this is a problem for medical expert systems  for instance. the outcomes predicted by a system trying to solve problems at the level of  diagnose infectious disease  can not be straightforwardly observed: if they could  we wouldn't need the systems. this has serious consequences concerning the  trainability  of such systems; we will return to this later. 
   finally  some simple properties of probabilities should be noted. for independent events the joint probability  probability that all events will occur  is the product of the probabilities of the events.  events are independent provided that whether an outcome belongs to one does not affect how likely it is to 

belong to another.  since all probabilities lie between zero and one  the joint probability of several independent events is always smaller than the probability of any one of them  unless all but one have probability one or at least one has probability zero. for dependent events  the joint probability is at most the maximum of the individual event probabilities  and it is that only if the corresponding event entails all the others. this has important consequences which we will return to later. notice that the joint probability for dependent events may be zero even though none of the individual probabilities is  and it will always be so if at least two of the events are mutually exclusive. more subdy  the joint probability of  say  six events may be zero even though no two of them are mutually exclusive  if  say  five of them together exclude the sixth. 
   similarly  the probability that an outcome will fall into at least one of several independent events is the sum of the probabilities of the events in question. if they are dependent  it is at least the maximum of the individual probabilities  and at most their sum. notice that a false assumption of independence underestimates the probability of disjunctions and overestimates the probability of conjunctions. in a long chain of reasoning involving both  it is not at all clear that these offsetting errors would be easy to detect and isolate. 
i l l 	requirements for probability-based reasoning 
   where decisions or predictions must be made on the basis of partial information  and where there is enough information to tell what outcomes are most likely given what is known  probability theory can be used to make these decisions and predictions accurately and responsibly. the mechanism is available  it is well-defined and well-understood  it gives good results  and it is the only mechanism we have with those properties. these facts alone show that probability has a place in reasoning in uncertainty  and henceforth i will take that as established. but probability-based reasoning takes more than arithmetic. this section looks at some of those needs and their consequences for ai systems. 
a. where do the numbers come from  
   any application must consider where the system gets its data. there are two possiblities: a system may use known probability values and distributions  or it may begin with initial probability estimates that are refined in the light of further evidence  in bayesian terminology  these are called  prior probabilities  or  priors  . the first choice gives better results  and is easier to implement. unfortunately  it requires a depth of knowledge in the application domain that is almost never attainable  and so the second approach is more often taken. 
    prior probabilities  are not probabilities: they are guesses.  the only interpretation under which priors qualify as probabilities is the extreme subjective view above  on which anyone's level of commitment is a fortiori a probability  but not an interesting one  since on this view a science of probabilities is impossible and in any case would have nothing to do with what is or is not likely to happen.  if the priors are bad guesses  results based on them will be bad results  even if all other assumptions hold. designers can deal with this problem two ways: use data and experiments to start with good prior probability estimates  or validate or train the system to improve bad ones. 
   basing priors on data and experiments is straightforward and the most reliable course  when it can be done. where data from reasonably representative samples already exist  those data are used. where data dp not exist  classical experiments based on the outcome distribution and the predictions of a testable hypothesis are designed. for some ai domains  this procedure is feasible. the domain for prospector  for instance  is small and reasonably well understood. we have fairly good information on the occurrence rates of different minerals  and it is easy to imagine  at least  what it would be like to have reliable estimates of joint and conditional probabilities over many of the relevant events. since knowledge of conditional probabilities entails knowledge of which events are independent of each other  it also largely eliminates the need for assumptions like maximum entropy. 
   unfortunately  domains in which this kind of information is available are rare. by contrast  if we consider a medical domain  the number of possible outcomes is huge  their distributions are less well known  their interactions are frequently unknown  and reliable data are notoriously hard to come by. the number and scope of experiments needed to establish such data are overwhelming. in cases such as this  some other course must be taken. the usual approach relies on expert opinions elicited formally or informally from individuals or from panels by any of a variety of strategies. 
   there are many reasons to doubt the accuracy of such estimates. first  people in general and trained scientists in particular are lousy at estimating probabilities. the classic studies showing this were published over a decade ago  tversky and kahneman 1   tversky and kahneman 1  and many more have followed confirming and extending the results  kahneman  slovik  and tversky 1 . second  even supposing the experts on the panel avoid the most common kinds of error  they lack the information they would need to make accurate estimates.  the problem isn't that the experts refuse to give us this information; they don't have it cither.  the question is what to do about it. 
   option one: do nothing. it is a theorem that repeated application of bayesian analysis to a given event yields a sequence of priors which converges on the frequency probability  however abysmal the original guess. so if we start with whatever priors we have and let the system refine them as it goes  its results will improve in the course of nature. 
   there are three problems with this   a  the system can only improve its estimates if it has independent confirmation of the outcome. for a medical system  this independent information is often unavailable.  that a patient got well after treatment does not confirm the diagnosis  for instance  since many treatments help a broad range of problems  and anyhow most patients get well no matter what.  without independent information on the outcome the system's estimates will not improve. worse  they may seem confirmed because disconfirming instances  although present  go undetected   b  even if the system improves over time  it starts badly. if we care what answers we get  we may not want to tolerate this.  it is good if a medical expert system learns ro recommend the right treatment; it is less good if it learns by recommending wrong treatments that kill half of new york.   c  although the mathematics guarantees that estimates will converge  it doesn't guarantee that they will do so rapidly. as a general rule  
	nutter 	1 

estimates converge fairly slowly  and the more inaccurate one there are  and the more inaccurate they are   the slower the convergence. if the initial priors are bad enough  it may be a very long time before the system's predictions get much better. 
   option one a: do nothing  but report ignorance. many researchers propose ranges instead of point probabilities to reflect  second order  uncertainty: wide ranges reflect shaky estimates  while narrow ranges reflect more solid ones. dempster-shafer theory gives way to calculate probabilities from these ranges. this approach has the advantage that a user who gets an answer with an attached probability of  say   1  1  can tell that the system really doesn't know  whereas a one who gets the same answer with an attached probability of 1 can not. but we have no more reliable guide for setting ranges than for setting priors; and a bad answer with a warning is still a bad answer. this approach can be combined with the steps below for improving the quality of the priors; whether and how much improvement will result remains to be seen. 
   option two: validate the system's predictions. it may not be possible to check a system's judgements against actual outcomes  they can be checked against human judgements. comparison against human performance may not give results as good as  true  priors would  but it meets any standard that could reasonably be expected. but in this context as in any other  care must be taken to ensure that validation is not tainted. 
   in particular  the system's performance should be compared with the performance of several experts  the more the better  on the same cases  not just ones that look similar . in addition  the experts in question should not know the system's design  its basic assumptions  including its priors   what questions it asked  what conclusions it reached  or how it reached them. in practice  this means that systems cannot be validated in the environments in which they would be used: if the considered opinions of several experts were routinely available  nobody would need the expert system. finally  since the point of validation is to tune the system's priors  this phase must be pursued with the attitude that in case of disagreement  and in the absence of overwhelming evidence to the contrary  the experts are right and the system is wrong. 
   option three: train the system. if outcomes are independently observable  they can be used to put a system through a training phase. this amounts to the  do nothing  approach  but pursued  off-line  and with careful supervision until there are signs of convergence. given observable outcomes  this lets the mathematics work for the designers  while allowing intervention if thrashing values show that a particular estimate was so bad that it will take a long time to converge. it should be noted  however  that if many factors are involved  or if original estimates are very bad  this course may require very large amounts of data before responses become reliable enough for the system to  go on-line . 
b. what can the numbers tell you  
   the most likely hypothesis is usually not the best explanation. the reason for this has to do with the intuitively paradoxical fact that a hypothesis which covers half the information and ignores  is indifferent to  the rest is more probable than one which gives exactly the same explanation of the first half of the data and then goes on to explain the rest. recall that the probability of a conjunction is at most equal to the maximum of the probabilities of its conjuncts  and only 
1 	knowledge representation 
reaches that limit when the conjunct with the highest probability entails all the others. it follows that more specific hypotheses are mathematically guaranteed to have lower probabilities than any less specific hypotheses they entail: adding information to a hypothesis reduces its probability. always. ten out of ten. 
   so  consider the following example  once again in the medical domain. suppose  our data  that a patient has a fever  a sore throat  white spots on the tonsils  nausea  diarrhoea  and vomiting. now consider two possible hypotheses:  a  the patient has strep throat;  b  the patient has strep throat and a gastro-intestinal virus. for the above reasons  a is necessarily the more probable hypothesis; but b is the better explanation. 
   in many cases  we really want the best explanation. this means something like  we want the hypothesis that best covers the facts while maintaining a reasonable probability. if the  answer space  has more than one level of granularity  this is different from the most probable hypothesis  because more specific explanations are better than more general ones  so long as they do not become intolerably unlikely. this is not to say that a system which is looking for the best explanation cannot use probability-based reasoning to advantage. but it needs a more sophisticated answer selection mechanism than  most probable hypothesis : whatever else goes on  at the end of the calculations  some reasoning takes place - to select the best explanation as opposed to most likely hypothesis - which is not probabilistic in nature. hence  pace cheeseman  even in situations which admit of a clear statistical analysis  it takes more than just probabilities to reason in uncertainty. 
c. what do the numbers cost  
　　 ginsberg 1  proposes to use probabilities as a limiting threshhold to reduce the cost of inference. in particular  he suggests setting a threshhold so that once a conclusion's probability reaches that limit  it may be taken as established. for the sake of argument  say that the limit in question is 1. suppose we know the following: anything which is a has a probability of 1 of also being b: x is a; anything which is c 
　　has a probability of 1 of also being b; x is c. now: is x ib1 the answer  of course  is no. but if the system first finds the rule about as and the fact that x is a  giving a probability of 1 that x is b  it will cut off inference there. so the argument that using probabilities allows early termination should be taken with a grain of salt. the  best   highest hit rate  hypothesis for any rare event is always the hypothesis that it never happens. that isn't useful if we are trying to predict  detect  and reason about rare events. non-numerical inference mechanisms can stop inference early  for instance using resource limitation  see e.g. donlon 1 ; only  the system may miss an answer it would otherwise get. likewise a system that threshholds on probabilities can stop early; but it may get the wrong answer. for more on threshholding problems  especially with regard to the dempster-shafer approach  see  dubois and prade 1 . 
　　in addition  the training process may prove more expensive than it appears.  ginsberg 1  actually goes into detail on the process of getting tests to improve the quality of probability judgements  so the following remarks will be made in terms of his system's behavior. but these costs result from necessary steps if the system trains  on-line : any system that counts on this must either sacrifice accuracy or pay these prices. each time an inference establishes a probability for a proposition  ginsberg's system records the evidence that was used. then every time new evidence changes a proposition's probability  

the system retraces every inference that proposition was used in to update the conclusion's probability. unfortunately  while the number of tests is small  none of the system's probabilities are reliable. so for most of its early life  the system has to recompute the probability for virtually all its inferences every time it sees anything. using cut-offs makes this worse  by the way  since then the system really cannot just retrace the proofs that went through; it should also recheck those that were terminated early  of which no record exists. in effect  this means that rather than retracing a known path  it must perform the entire inference again from scratch. 
   in any case  ai systems rarely perform controlled statistical experiments  testing selected observable variables against the predictions of a hypothesis. instead  they face specific situations  get information  and reason from it. this means that to get the full benefit of their  tests   systems that train must analyze every new piece of information to find out which of the events they know about this datum may fit. consider such a system when it first meets fred the female flamingo. not only is it meeting a bird that flies  it is meeting something pink that flies  something over three feet tall that is pink  a female named fred  and so on and on and on. the computational cost of extracting events from data and of retracing past inferences should be clear. 
iv 	where probabilities don't belong 
   pace cheescman and many  many others  not all that is not universal is probabilistic. for instance: if  as cheeseman claims  the by now tormented example  birds fly  really means  most birds fly   then birds don't fly in the spring. in nesting season  baby birds outnumber adults. baby birds don't fly. hence in nesting season   most birds fly  is false. by the way  we can do even better with  birds lay eggs   which is out-and-out false year round of at least half the population  none of the males do . so if cheeseman is right  anyone who says in the spring that birds fly or at any time that birds lay eggs is mistaken. this is nonsense. 
　　 birds fly  must be decoded with respect to typicallity. if typicallity can be modeled by any statistical concept  it is category cue validity  not probability  rosen 1  rosch and mervis 1; rosch  mervis  gray  johnson  and 
boyes-braem 1 .  birds lay eggs   on the other hand  is not statistical at all. it is shorthand for a genuine  accept-nosubstitutes universal - but not for  for all x  if x is a bird  then x lays eggs . instead  it is in a class with the non-universal generalizations  mammals bear young alive   duck-billed platypi lay eggs  and  reptiles and fish lay eggs   garter snakes and sharks bear live young . by the way  these generalizations cannot be translated straightforwardly into probability claims counting over species instead of individuals: no species either bears live young or lays eggs; only  female  individuals belonging to species do. 
   the typicality-based uncertainty involved in generalizations like  birds fly  centers on whether an individual has a property typical of things of its kind. another kind of uncertainty  also related to typicality  centers instead on the extent to which a given property applies to an individual. this is the issue of vagueness  and the kind of inferences justified on the basis of degree-of-applicability are different from the kinds based on either typicality or probability. the difference between measure-of-membership and typicality is subtle but real. typical birds fly. but how typical a bird tweety is does not measure how well tweety flies or how even how likely tweety is to fly  hummingbirds are atypical in many ways  but spectacularly good fliers . 
   more importantly  because more often confused  degree-of-applicability does not work like probability. consider the following two claims about oscar the ostrich: 
 i  oscar is a  typical  bird at 1 
 ii  oscar is male at 1 
claim  i  says that oscar is not very birdlike  ostriches aren't . it is the sort of claim fuzzy set theory was originally developed to handle; it tries to measure the extent to which an individual falls in the bounds established by a fuzzy concept. claim  ii  is a probability claim  reflecting that the system doesn't know whether oscar is male but does know that oscar is a bird  and that half of all birds are male  making the chances that oscar is male 1. that is not to say that oscar is half male: the system can consistently hold  ii  and also hold that any given bird is either completely male or not at all. the claims look superficially alike  but they cannot be taken the same way:  i  says that oscar is not a very typical bird;  ii  docs not say that oscar is not a very typical male   i  does not really reflect incomplete information at all: it reflects a fundamental fact about how oscar relates to a vague concept. in case  ii   the information is incomplete and can be completed by a single experiment  look at oscar and see . if no difference in representation reflects this basic difference in content  the system will reason incorrectly a good part of the time. 
   translations of common generalizations into probabilities do not preserve truth values  and translations of degree-ofapplicability claims do not preserve inferences. so neither preserves meaning. hence wherever else they can be used  probabilities cannot be used to understand generalizations and expressions of uncertainty in understanding natural language or in any system which gets its data in natural language. natural language understanding requires inference in contexts of uncertainty all over the place  including inference from previously processed non-statistical generalizations. hence there are instances of inference in contexts of uncertainty which are not amenable to analysis as probabilities. 
	v 	conclusions 
   probability theory is an important tool for many ai applications involving uncertainty. where outcome likelihood is at stake  and where the necessary data are available  it is the best known tool. but it is also hard. it requires a detailed analysis and understanding of the domain  and either a great deal of data or an extensive validation procedure  if the answers obtained are to be reliable. there are no short cuts. 
   in any case  statistics cannot provide a panacea for all problems of uncertainty  generality  vagueness  and ignorance. no mathematical model however rigorous can be expected to give reasonable answers unless the situations to which it is applied conform to its underlying assumptions. the existence and prevalence of non-statistical generalizations shows that non- statistical models uncertainty must also be investigated. 
the existence of a well-defined  long-studied  clearly 
	nutter 	1 
articulated theory argues strongly for its use - but not for its extension  willy-nilly  into other fields. sometimes one theory is developed before another because the first makes sense and the second doesn't; other times  as for instance with physics and biology  we make progress on the easier problem first. 
　　ultimately  ai systems which reason in uncertainty need to combine these modes of uncertainty. in particular  we need to distinguish representations of probabilities  fuzzy membership  and generalizations based on typicality. one such approach would represent the first two as functions which take properties and yield either numbers or second order relations  and represent the third using some form of default reasoning  i have argued elsewhere for a simple monotonic extension to first order logic; see nutter 1 . axioms and rules can then make use of information when and as it is available  without misrepresenting that information and so making wrong inferences from it. obviously this scheme is utopian; so what can we do meanwhile  some application domains are particularly amenable to one of these forms of inference  and can do without the others; in these cases  we make choices  hopefully understanding the limitations and trade-offs. on the science front  we can develop as many models as we can  with as close attention to the phenomena to be modeled as possible. and meanwhile  we can remember that given the diverse kinds of reasoning involved  anyone who claims to have the one and only key to reasoning in uncertainty is almost certainly wrong. 
acknowledgements 
my thanks to dr. janet c. rice of the department of 
biostatistics and epidemiology  school of tropical medicine and public health  tulane university  for her patience in looking over early versions of this paper to check for gross statistical bloomers. as always  credit goes to her only for accuracy; any errors lie solely at the author's door. 
