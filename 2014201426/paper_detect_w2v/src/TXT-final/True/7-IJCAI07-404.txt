
automatic tools for finding software errors require knowledge of the rules a program must obey  or  specifications   before they can identify bugs. we present a method that combines factor graphs and static program analysis to automatically infer specifications directly from programs. we illustrate the approach on inferring functions in c programs that allocate and release resources  and evaluate the approach on three codebases: sdl  openssh  and the os kernel for mac os x  xnu . the inferred specifications are highly accurate and with them we have discovered numerous bugs.
1 introduction
software bugs are as pervasive as software itself  with the rising cost of software errors recently estimated to cost the united states economy $1 billion/year  rti  1 . fortunately  there has been a recent surge of research into developing automated and practical bug-finding tools. while tools differ in many respects  they are identical in one: if they do not know what properties to check  they cannot find bugs. fundamentally  all tools essentially employ specifications that encode what a program should do in order to distinguish good program behavior from bad.
　a specification is a set of rules that outlines the acceptable behavior of a program. a  bug  is a violation of the rules. for example  oneuniversalspecification is that a program should not crash.  because crashes are fail-stop errors  i.e.  the program halts  they are easy to detect  but because many factors can lead to a crash they are equally difficult to diagnose.1moreover  while software crashes are colorful symptoms of a program behaving badly  many bugs are not fail-stop. memory leaks  or more generally leaks of application and system resources  lead through attrition to the gradual death of a program and often induce erratic behavior along the way. data corruption errors can lead to unpleasant results such as loss of sensitive data. further  most security-related bugs  such as those allowing a system to be compromised  are not fail-stop.
properly locating and fixing such bugs requires knowledge of the violated program invariants.
　traditionally  discovering such bugs was the responsibility of software developers and testers. fortunately  automated bug-finding tools  such as those based on static program analysis  have become adroit at finding many such errors. operationally  a static analysis tool analyzes a program without running it  similar to a compiler  and reasons about the possible paths of execution through the program. conceptually  the tool checks that every analyzed path obeys the program invariants in a specification. when a rule can be violated  the tool flags a warning. static tools have achieved significant success in recent years  with research tools finding thousands of bugs in widely used open-source software such as the linux kernel  engler et al.  1 . unfortunately there is no free lunch. like human testers  these tools require knowledge of a program's specification in order to find bugs.
　the key issue is possessing adequate specifications. unfortunately many important programproperties that we could check with automated tools are domain-specific and tied to a particular api or program. to make matters worse  the manual labor needed to specify high-level invariant properties can be overwhelming even for small programs  flanagan et al.  1 . further  in large evolving codebases the interfaces may quickly change  which further complicates the problem of keeping a specification current. consequently  many bugs that could have been found with current tools are rendered invisible by ignorance of the necessary specifications.
　this paper describes a technique that combines factor graphs with static program analysis to automatically infer specifications directly from programs. we illustrate the kind of specifications that can be inferred with an example specification inference task. this paper formalizes and extends the model informally introduced in our earlier work  kremenek et al.  1 ; we also describe algorithms for inference and parameter learning. these changes result in significantly improved performance of the model. we also apply these ideas to finding a number of bugs  many serious  in sdl  openssh  postgresql  wine and mac os x  xnu .
1 specifications of resource ownership
almost all programs make use of dynamically allocated resources. examples include memory allocated by functions like malloc  file handles opened by calls to fopen  sockets 
1. file * fp1 = fopen   myfile.txt    r   ;
1. file * fp1 = fdopen  fd   w   ;
1. fread  buffer  n  1  fp1  ;
1. fwrite  buffer  n  1  fp1  ;
1. fclose  fp1  ;
1. fclose  fp1  ;

figure 1: example use of standard c i/o functions.

figure 1: dfa for a static analysis checker to find resource errors. shaded final states represent error states  bugs .
database connections  and so on. functions that  allocate  resources  or allocators  typically have a matching deallocator function  such as free and fclose  that releases the resource. even if a language supports garbage collection  programmers usually must enforce manual discipline in managing arbitrary allocated resources in order to avoid resourcerelated bugs such as leaks or  use-after-release  errors.
　numerous tools have been developedto find resource bugs  with the majority focusing on finding bugs for uses of wellknown allocators such as malloc  heine and lam  1 . many systems  however  define a host of allocators and deallocators to manage domain-specific resources. because the program analysis required to find resource bugs is generally the same for all allocators and deallocators  current tools could readily be extended to find resource bugs for domainspecific allocators if they were made aware of such functions.
　a more general concept  however  that subsumes knowing allocators and deallocators is knowing what functions return or claim ownership of resources. to manage resources  many programs employ the ownership idiom: a resource has at any time exactly one owning pointer  or handle  which must eventually release the resource. ownership can be transferred from a pointer by storing it into a data structure or by passing it to a function that claims it  e.g.  a deallocator . although allocators and deallocatorsrespectively return and claim ownership  many functions that return ownership have a contract similar to an allocator but do not directly allocate resources; e.g.  a function that dequeues an object from a linked list and returns it to the caller. once the object is removed from the list  the caller must ensure that the object is fully processed. a similar narrative applies to functions that claim ownership. by knowing all functions that return and claim ownership  we can detect a wider range of resource bugs.
　this paper explores the problem of inferring domainspecific functions in c programs that return and claim ownership. our formulation uses an encoding of this set of functions that is easily consumed by a simple static analysis tool  or checker  which we briefly describe. figure 1 depicts a contrived code fragment illustrating the use of several standard i/o functions in c. for the return values of fopen and fdopen  we can associate the label ro  returns ownership  or  ro. for the input arguments  with a pointer type  of fwrite  fread  and fclose we can associate labels co  claims ownership or  co. these labels can be used by a simple checker that operates by tracing the possible paths within the function where fp1 and fp1 are used  and  along those paths  simulate for each pointer the propertydfa in figure 1. every time a pointer is passed as an argument to a function call or returned from a function the corresponding label is consulted and the appropriate transition is taken. an  endof-path  indicates that the end of the function was reached. there are five final states. the states leak and invaliduse are error states  shaded  and indicate buggy behavior  invalid use captures both  use-after-release  errors as well as claiming a non-owned pointer . the other final states indicate a pointer was used correctly  and are discussed later in further detail. further details regarding the implementation of the checker can be found in kremenek et al. .
1 our approach
while specifications conceivably come in arbitrary forms  we focus on inferring specifications where  1  the entire set of specifications is discrete and finite and  1  a given specification for a program can be decomposed into elements that describe the behavior of one aspect of the program. for example  in the ownership problem if there are m functions whose return value can be labeled ro and n function arguments that can be labeled co then there are 1n possible combined labellings. in practice  there are many reasonable bug-finding problems whose specifications map to similar domains.
　our primary lever for inferring specifications is that programs contain latent information  often in the form of  behavioral signatures   that indirectly documents their high-level properties. recall that the role of specifications is to outline acceptable program behavior. if we assume that programs for the most part do what their creators intended  or at least in a relative sense  bugs are rare   then a likely specification is one that closely matches the program's behavior. thus  if such a specification was fed to a static analysis checker  the checker should flag only a few cases of errant behavior in the program. finally  latent information may come in a myriad of other forms  such as naming conventions for functions  e.g.  alloc   that provide hints about the program's specification.
　this motivates an approach based on probabilistic reasoning  which is capable of handling this myriad of information that is coupled with uncertainty. our solution employs factor graphs  yedidia et al.  1   where a set of random variables in a factor graph represent the specifications we desire to infer  and factors represent constraints implied by behavioral signatures. the factor graph is constructed by analyzing a program's source code  and represents a joint probability distribution over the space of possible specifications. once the factor graph is constructed  we employ gibbs sampling to infer the most likely specifications.
1 factor graph model
we now present our factor graph model for inferring specifications. we illustrate it in terms of the ownership problem  and make general statements as key concepts are introduced.
　we begin by mapping the space of possible specifications to random variables. for each element of the specification with discrete possible values we have a random variable ai with the same domain. for example  in the ownership problem for each return value of a function  foo  in the codebase we have a random variable afoo:ret with domain {ro  ro}. further  for the ith argument of a function  baz  we have a random variable abaz:i with domain {co  co}. we denote this collection of variables as a  and a compound assignment a = a represents one complete specification from the set of possible specifications.
1 preliminaries
our goal is to define a joint distribution for a with a factor graph. we now review key definitions pertaining to factor graphs  yedidia et al.  1 .
definition 1  factor  a factor f for a set of random variables c is a mapping from val c  to r+.
definition 1  gibbs distribution  a gibbs distribution p over a set of random variables x = {x1 ... xn} is defined in terms of a set of factors {fj}jj=1  with associated random variables  such that:
j
	p.	 1 
j=1
the normalizing constant z is the partition function.
definition 1  factor graph  a factor graph is a bipartite graph that represents a gibbs distribution. nodes correspond to variables in x and to the factors. edges connect variables and factors  with an undirected edge between xi and fj if xi （ cj.
1 overview of model components
we now define the factors in our model. maintaining terminology consistent with our previous work  we call the factor graphs constructed for specification inference annotation factor graphs  afgs . the name follows from that the specifications we infer  e.g. ro and co  serve to  annotate  the behavior of program components. while often the only random variables in an afg will be a  i.e.  x = a   other variables  such as hidden variables  can be introduced as needed.
　there are two categories of factors in an afg that are used to capture different forms of information for specification inference. the first set of factors  called check factors  are used to extract information from observed program behavior. a given specification a = a that we assign to the functions in a programwill determine  for each trackedpointer  the outcome of the checker described in section 1. these outcomes reflect behaviors the checker observed in the program given the provided specification  e.g.  resource leaks  a pointer being properly claimed  and so on . our insight is that  1  some behaviors are more likely than others  e.g.  errors should occur rarely  and that  1  some behaviors are harder for a program to  coincidentally  exhibit; thus when we observe such behaviors in a program they may provide strong evidence that a given specification is likely to be true. check factors incorporate into the afg both our beliefs about such behaviors and the mechanism  the checker  used to determine what behaviors a program exhibits.
　the second set of factors are used to model arbitrary domain-specific knowledge. this includes prior beliefs about the relative frequency of certain specifications  knowledge about suggestive naming conventions  e.g  the presence of  alloc  in a function's name implies it is an ro   and so on.
we now discuss both classes of factors in turn.
1 check factors
we now describe check factors  which incorporate our beliefs about the possible behaviors a program may exhibit and the specifications they imply.
　each final state in our checker corresponds to a separate behavioral signature observed in the program given a specification a = a. the checker observes five behavioral signatures  two of which indicate different kinds of bugs  leaks and everything else   and three which identify different kinds of  good  behavior. by distinguishing between different behaviors  we can elevate the probability of values of a that induce behaviors that are more consistent with our beliefs.
　first  we observe that  in general  bugs occur rarely in programs. although not a perfect oracle  the checker can be employed to define an error as a case where the dfa in figure 1 ends in an error state. thus an assignment to a that causes the checker to flag many errors is less likely than an assignment that leads to few flagged errors. note that we should not treat errors as being impossible  i.e.  only consider specifications that cause the checker to flag no errors  because  1  real programs contain bugs and  1  the checker may flag some false errors even for a bug-free program.
further  not all kinds of errors occur with equal frequency.
in practice invaliduse errors occur far less frequently than
leaks. thus  for two different specifications that induce the same number of errors  the one that induces more leaks than invaliduse errors is the more likely specification.
　finally  errors aside  we should not equally weight observations of different kinds of good program behavior. for example  the deallocator signature recognizes the pattern that once an owned pointer is claimed it is never subsequently used  while ownership matches behavior that allows a claimed pointer to be used after it is claimed. the former is a behavioral signature that is much harder for a set of functions to fit by chance. consequently when we observe the deallocator pattern we could potentially weight it as stronger evidence for a given specification than if a code fragment could only obey the ownershippattern. finally  the contra-ownership pattern  which recognizes all correct use of a non-owned pointer  is the easiest pattern to fit: all functions can be labeled  ro and  co and the checker will never flag an error. such a specification is useless  however  because we wish to infer ro and co functions! thus we should potentially  reward  observations of the ownership or deallocator signatures more than the contra-ownership pattern. in other words  we are willing to tolerate some errors if a set

figure 1: factor graph model for the code in figure 1. circular nodes correspond to variables and square nodes to factors. the shaded factors indicate check factors  while the top row depicts factors modeling prior beliefs.
of functions appear to consistently fit either the deallocator or ownership signatures.
　we now discuss how these ideas are modeled using factors. we first atomize the output of the checker into checks. a check is a distinct instance in the program where the specification could be obeyed or disobeyed. for the ownership problem  we have a check for every statement of the form  p = foo    where a pointer value is returned from a called function. for the code in figure 1 we have one check for fp1 and another for fp1. in general  the actual definition of a check will depend on the specifications we are trying to infer  but essentially each check represents a distinct observation point of a program's behavior.
　once we define the set of checks for a codebase  for each check we create a corresponding check factor  denoted fcheck i   in the afg. check factors represent  1  the analysis result of the checker at each check when running the checker using a provided set of values for a and  1  our preferences over the possible outcomes of each check. the variables in a associated with a given fcheck i   denoted acheck i   are those whose values could be consulted by the checker to determine the check's outcome. for example  figure 1 depicts the factor graph for the code example in figure 1. we have two check factors  shaded   one for fp1 and fp1 respectively. because for fp1 the checker needs only consult the specifications represented by the variables afopen:ret  afread:1 and
afclose:1  these variables are those associated with fcheck fp1 .
　check factors have a simple mathematical definition. if ci acheck i   represents the output of the checker for check i when acheck i  = acheck i   then fcheck i  is defined as:
		:	if ci acheck i   = c
thus a check factor is encoded with a set of real-valued parameters  θc （ r   one for each distinct behavior observed by the checker. these parameters are shared between all check factors that observe the same set of behaviors 1 and are used to encode our intuitions about programbehaviorand the specifications they imply. for example  we expect that the parameters for error states  θleak and θinvaliduse  will have lower values than the remaining parameters  i.e.  errors are rare . while parameters can be specified by hand  kremenek et al.  1   in this paper we focus on learning them from partially known specifications and observing if the learned parameters both  1  match with our intuitions and  1  compare in quality to the specifications inferred using hand-tuned parameters.
　multiple execution paths. note that the value of the check is a summary of all the analyzed paths within the function for that pointer. each analyzed path may end in a different state in the dfa. instead of reporting results for all analyzed paths  we summarize them by reporting the final state from the analyzed paths that appears earliest in the following partial order:
invaliduse   leak  contra-ownership
  ownership  deallocator
for example  if on any path the analysis encounters aninvalid use state  it reports invaliduse for that check regardless of the final states on the other paths. the idea is to report bad behavior over good behavior.
1 further modeling: domain knowledge
beyond exploiting the information provided by a checker  the factor graph allows us to incorporate useful domain knowledge. we discuss two examples for the ownership problem.
　prior beliefs. often we have prior knowledge about the relative frequency of different specifications. for example  most functions do not claim ownership of their argumentsand should be labeled  co. such hints are easily modeled as a single factor attached to each ai variable. we attach to each afoo:i a factor f afoo:i = x  = eθx. the two parameters  θco and θ co  are shared between all factors created by this construction. analogously we define similar factors for each
afoo:ret. these factors are depicted at the top of figure 1.
　suggestive naming. naming conventions for functions  e.g.  a function name containing  alloc  implies the return value is ro  can be exploited in a similar fashion. we selected a small set of well-known keywords k  |k| = 1  containingwords such as  alloc    free  and  new.  to model keyword correlation with ro specifications  for each afoo:ret whose functions contains the keyword kw we construct a single factor associated with afoo:ret:
	f afoo:ret = x  = eθkw:x	 1 
since x （ {ro  ro} this factor is represented by two parameters  per keyword . these parameters are shared between all factors created by this construction. note that the factor is present only if the function has the keyword as a substring of it's name; while the presence of a keyword may be suggestive of a function's role  we have observed the absence of a keyword is usually uninformative.
　keyword correlation for co specifications is similarly modeled  except since a function may contain multiple arguments  each of which may be labeled co  we construct one  keyword factor  over all the afoo:i variables  denoted a  for a function foo:

thus  if any of foo's arguments has the specification co then the factor has value eθkw:co  and eθkw: co otherwise . for clarity  keyword factors have been omitted in figure 1.
1 inference
once the factor graph is constructed  we employ gibbs sampling to sample from the joint distribution. for each ai we estimate the probability it has a given specification  e.g.  p ai = ro   and rank inferred specifications by their probabilities. analogously  we estimate for each check factor fcheck i  the probability that the values of acheck i  cause the checker to flag an error. this allows us to also rank possible errors by their probabilities.
　when updating a value for a given aj （ a  we must recompute the value of each check factor fcheck i  where
aj （ acheck i . this requires actually running the checker. because at any one time gibbs sampling has a complete assignment to all random variables  the checker simply consults the current values of a to determine the outcome of the check. this clean interface with the checker is the primary reason we employed gibbs sampling.
　while our checker is relatively simple  the analysis is still very expensive when run repeatedly. to compensate  we cache analysis results by monitoring which values of a are consulted by the checkerto determinethe outcomeof a check. this results in a speedup of two orders of magnitude.
　we experienced serious issues with mixing. this is a byproduct of the check factors  since values of several ai variables may need to be flipped before the outcome of a check changes. we explored various strategies to improve mixing  and converged to a simple solution that provided consistently acceptable results. we run 1 chains for n = 1 iterations and at the end of each chain record a single sample. moreover  for each chain  we apply the following annealing schedule so that each factor fi has the following definition on the kth gibbs iteration:
		 1 
this simple strategy significantly improved the quality of our samples. while satisfied by the empirical results of this procedure  we continue to explore faster alternatives.
1 learning
we now discuss our procedure for parameter learning. the factors we have discussed take the exponential form of f cj = cj  = eθcj  with θcj （ r . the set of parameters θ for these factors can be learned from  partially  observed data  denoted d = d  by using gradient ascent to maximize the log-likelihood of d. generally d   a  representing partially known specifications. we omit the derivation of the gradient  as it is fairly standard. for the case where a single parameter θcj appears in a single factor fj  the corresponding term of the gradient is:
epclamped i{cj=cj}    epunclamped i{cj=cj} 
 1 
here pclamped represents the conditional distribution over all variables in the factor graph when d is observed  while
punclamped represents the distribution with no observed data. if a parameter appears in multiple factors  the gradient term for θcj is summed over all factors in which it appears. 1 implementation: heuristics and optimizations
we now briefly describe a few key features of our implementation of gradient ascent for our domain.
	afg size	manually classified specifications
codebase lines  1  |a| # checks total	sdl	1	1 1	1 1 1 1openssh	1	1 1	1 1 1 1	xnu	1 1 1	1 1 1 1table 1: breakdown by project of codebase size  number of manually classified specifications  and afg size.
　seeding parameters. all parameters  excluding θleak and θinvaliduse  were initialized to a value of 1  i.e.  no initial bias . θleak and θinvaliduse were initialized to  1 to provide a slight bias against specifications that induce buggy behavior.
　estimating the gradient. for each step of gradient ascent  the expectationsin equation 1 are estimated using gibbs sampling  but each with only two chains  thus relying on properties of stochastic optimization for convergence . consequently  our estimate of the gradient may be highly noisy. to help mitigate such noise  samples are drawn frompclamped and punclamped in a manner similar to contrastive divergence  hinton  1 . first  each sample from pclamped is sampled as described in section 1. to generate a sample from punclamped  we continue running the markov chain that was used to sample from pclamped by  1  unclamping the observed variables d and then  1  running the chain for 1 more iterations. this noticeablyreduces much of the variation between the samples generated from pclamped and punclamped.
　because the term for θcj in the gradient is additive in the number of factors that share θcj  its value is in the range   numfactors θcj  numfactors θcj  . this causes the magnitude of the gradient to grow with the size of the analyzed codebase. to compensate  we scale each θcj term of the gradient by numfactors θcj   leaving each term of the modified gradient in the range   1 . this transformation along with a modest learning rate  worked extremely well. we experimented with alternate means to specify learning rates for gradient ascent  and none met with the same empirical success.
　finally  since an afg typically consists of multiple connected components  if a connected component contains no observed variables  then equation 1 is trivially 1 for all factors in the component. we thus prune such components from the factor graph prior to learning.
1 evaluation
we evaluate our model by inferring ro and co functions in three codebases: sdl  openssh  and the os kernel for mac os x  xnu . sdl is a cross-platform graphics library for game programming. openssh consists of a network client and server for encrypted remote logins. both manage many custom resources  and sdl uses infrequently called memory management routines from xlib. like all os kernels  xnu defines a host of domain-specific routines for managing resources. for each project we randomly selected and manually classified 1 specifications for the return values  ro or  ro  and arguments  co or  co  of functions. table 1 shows the size of each codebase  the number of manual classifications  and afg sizes.
1 specification accuracy
our hypothesis is that many codebases will exhibit similarities in code structure and style  allowing a model trained on one codebase to be applied to another. we evaluate this hypothesis with two experiments.
　first  for each project we randomly divide our known specifications  table 1  into training and test sets  1% . we train the model on the training set and then use the trained model to infer the specifications in the test set. because the strictest test of our model is to apply it to a codebase with no known specifications  when inferring specifications for the test set  none of the variables in a are observed  including those in the training set . this simulates applying a model to a codebase that has practically identical code characteristics to the codebase on which the model was trained. we repeat this experiment 1 times.
figure 1 depicts averaged roc curves for each project.
each figure depicts five lines. the base model  afg  is an
afg that includes check factors and factors to model prior beliefs over ro/co labels. the second line  afg-keywords  is afg augmented with keyword factors. hand-tuned is the afg model using parameters that were tuned by hand over time by inspecting inference results on all codebases.
　the remaining two lines represent an ablative analysis  where we test simpler systems that use only a subset of the features of the full system. one strength of the model is that it captures the inter-correlation between specifications across the codebase. afg-rename is constructed from afg by weakening the correlation between variables: each variable ai （ a is replicated for each associated check factor  this is equivalent to renaming each function call in the codebase to refer to a distinct function . for example  for the afg in figure 1  we split afopen:ret into two random variables  one for each of the two check factors for which afopen:ret shares an edge. these two variables then serve as substitutes to afopen:ret for the respective check factors. specification probabilities are then estimated by averaging the probabilities of the replicated variables. the remaining model  keywords only  is an afg that includes only keyword and prior belief factors. all models  with the exception of hand-tuned  had their parameters separately learned on the same data.
　the roc curves illustrate that our model generally performs very well. for sdl  afg  afg-keywords  and handtuned achieve between 1% true positive rate  tpr  for both ro and co specifications with a 1%  or less  false positive rate. it is encouraging our trained models perform as well or better ashand-tuned which essentially had access to both training and test set data for all codebases   with afgkeywords slightly edging out all other models. we observe similar results on openssh and xnu. on xnu  both afg and afg-keywords significantly outperforms hand-tuned for ro accuracy  with hand-tuned achieving higher co accuracy with the trade-off of lower ro accuracy.
　our ablated models perform significantly worse. for sdl and openssh  afg-renamehas noticeably degraded ro accuracy compared to afg  but maintains decent co accuracy  the reverse being the case on xnu . we believe this is due to the richer models capturing relationships such as several



	 e  xnu: ro accuracy	 f  xnu: co accuracy
figure 1: roc curves depicting inferred specification accuracy.
allocators being paired with a common dealloactor function  thus information about one propagates to the others . note that its performance is still significantly better than random guessing. this suggests that when the ownership idiom fits at a  local  level in the code it is still strongly suggestive of a program's specification. for keywords-only  we observe excellent co accuracy on openssh because of the small number of co functions with very suggestive names  while for similar reasons it has decent co accuracy on sdl  up to the 1% tpr level  at which point accuracy falls off . on xnu  co accuracy is worse than random. on all codebases its ro accuracy is modest to poor; a more careful analysis suggests that some naming conventions are used inconsistently and that many ro functions do not have suggestive names.
　our second experiment directly evaluates training the model parameters on one codebase and applying them to inferring specifications on another. figure 1 depicts specification inference results for xnu. the sdl and openssh parameters are trained using our full set of known specifications for those projects and then are tested on our full set of known
	.	.1	.1	.1	.	1.	.	.1	.1	.1	.	1.
	 a  xnu: ro accuracy	 b  xnu: co accuracy
figure 1: specification accuracy on xnu when using model parameters trained on sdl and openssh.
specifications for xnu  while xnu avg  is the afg line from the previous experiment. all models are afg  without keywords . graphs for the other codebases are similar. we observe in this figure that all the lines are very close to each other. we believe this strongly supports the generality of the model and its applicability across codebases.
　interpreting parameters. upon inspection  in most case learned parameters matched well with our intuitions  section 1 . for all codebases  the parameters for error states  θleak and θinvaliduse  were less than the remaining parameters for check factors  non-errors . on some codebases  however  their relative values were higher to  we believe  compensate for increased codebase-specific noise from the checker. consequently  our afg model can compensate for some deficiencies in the checker as long as the checker can identify informative behavioral patterns. we also observed that θdeallocator was always greater than θownership and θcontra-ownership  which matches with our intuition that observations of the deallocatorpattern should be  rewarded  higher than other behaviors.
1 software bugs
as discussed in section 1  we can use the results from gibbs sampling to rank possible bugs by their probabilities before examining any inferred specifications. this enabled us to quickly find errors in each codebase that are based on the specifications inferred with the highest confidence. we observed about a 1% false positive rate for flagged errors  a rate consistent with current static checking tools . most false positives were due to static analysis imprecision  a source of noise that our model appears to handle well when inferring specifications   with a few due to misclassified specifications.
　in practice  we may feed the inferred specifications into a more precise  and expensive  static analysis  e.g.  xie et al.   when actually diagnosing bugs. nevertheless  even with our simple checker we discovered 1 bugs in sdl and 1 bugs in openssh. for xnu  many bugs were still pending confirmation  but 1 bugs were confirmed by developers  including one serious error  discussed below . we also casually applied our model to other projects including postgresql  a relational database engine  and wine  an implementation of the win1 api for linux  and quickly found several bugs in all of them. most errors are leaks  and involve custom allocators and deallocators not checked by current tools.
string allocated by proccorename.
　figure 1 illustrates an example bug found in the xnu. the function coredump is invoked within the kernel to process a core dump of a user process. the function proccorename is called to construct a freshly allocated string that indicates the location of the core dump file. this string is always leaked after the call to vnodeopen  which leads to the kernel leaking a small amount of memory every time a process core dumps. this is a serious error  as a renegade process can cause the kernel to leak an arbitrary amount of memory and eventually cripple the os  this bug has been fixed for the next release of mac os x . the bug was found by inferring that proccorename is an ro because it calls a commonly invoked allocator function that was also inferred to be an ro.
1 conclusion
we presented a general method that combines factor graphs and static program analysis to infer specifications directly from programs. we believe the technique shows significant promise for inferring a wide range of specifications using probabilistic analysis. this includes applications in computer security  where many security exploits could be fixed by correctly identifying  tainted  input data  such as from a web form  that is exploitable by an attacker  or by inferring possible bounds for arrays to detect buffer overruns when conventional analysis fails. these and other problems represent promising and exciting future directions for this work. 