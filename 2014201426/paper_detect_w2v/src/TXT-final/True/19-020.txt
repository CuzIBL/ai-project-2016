surprjsingness and expectation failure: whats the difference  
andrew ortony 
university of illinois at urbana-champaign and 
derek partridge* 
university of exeter  uk 
　　
　　intelligent systems operate in the midst of a superabundance of information lacking the tags that indicate which few aspects are significant to the particular problems at hand at any given time and place. given this wealth of information coupled with real-time processing constraints  selective attention is fundamental to any chance of success. in much of cognitive science  attentional focus is linked with the concept of expectation generation and failure. unfortunately  research in ai has to finesse the problem of selective attention  although sometimes recognizing it as an important component of the frame problem  e.g.  schank  1; 1 . 
　　we think that surprisingness  which is closely related to expectations and expectation failures  is important for focusing attention. however  we shall argue that that there is much more to surprisingness than expectation failure  and that the difference between the two is critical for ai. the standard view of the role of expectations in ai suffers from a reliance on underspecified concepts of  expectation  and  expectation failure   both of which appear to be oversimplifications within which a range of rather different phenomena are confounded. we shall attempt to provide a more general preliminary analysis of surprisingness. 
　　to start  we need to introduce some constructs that we use in our analysis. first  we shall suppose that a system is presented with an input proposition. second  we shall assume that at any point in time the current task results in some parts of the data base being activated. third  we assume that the data base comprises both episodic and semantic knowledge  an important aspect of which is that each propositionally distinct element has an immutability index associated with it. in other words  some elements are believed  perhaps erroneously  to be virtually immutable  while others are believed to be typically true  and yet others merely sometimes true. whereas we view immutability as a continuous variable  for simplicity of exposition it will be sufficient to distinguish cases that are immutable from those that are merely typical. this is because when surprisingness results from a conflict between an input proposition and propositions explicitly represented or readily deducible from the data base  there has to be a reasonable degree of conflict. it is generally surprising to encounter situations that conflict with propositions that are held to be immutable  but it is not generally surprising to encounter propositions that conflict with things that are only sometimes true. sometimes dogs are black  but this fact alone does not give rise to surprise on encountering a brown dog. 
　　we assume that with the help of a few simple inference rules  the knowledge base has the potential to produce a wealth of knowledge not explicitly represented. the set of propositions that are explicitly represented in the data base  or that are readily deducible from those that are  we call the practically deducible propositions. thus  the class of practically deducible propositions includes explicitly represented propositions as limiting cases. practically deducible propositions are contrasted with what we call 
* this paper was prepared while derek partridge was at the computing research laboratory of new mexico state university whose support we gratefully acknowledge. 
1 	cognitive modeling 
practically non-dtduciblc propositions. these are possible propositions that are neither explicitly represented nor readily deducible from those that are. not all propositions that are formally deducible count as practically deducible. in general  we view a proposition as being practically deducible to the extent that it does not require many and complex inferences. examples of practically deducible propositions might include propositions to the effect that  a  restaurants are establishments in which meals are served  which might be explicitly represented and virtually immutable   or  b  that in restaurants one pays after eating  which might be deducible from a script and represented as being typically true   or  c  that 1 is not a prime number  which is deducible and immutable . a non-deducible proposition might be that a rock will come hurtling through one's office window in five minutes  or indeed  the proposition that a rock will not come hurtling through one's window in five minutes. 
　　although a proposition may be practically deducible  it does not follow that it will in fact have been deduced on any particular occasion in which it is implicated in some reaction of surprise. in principle  it might have been  but in practice it may not have been. when it has been deduced  we can say that the corresponding event is actively expected  in which case we have what might be called an active prediction. when it has not  one might refer to the situation as one of passive expectation  or passive assumption. this amounts to proposing that expectations  active or passive  can only pertain to practically deducible propositions. so  for example  if one actively expecte that the democratic candidate will win the next presidential election  this prediction is rooted in propositions such as that there is to be an election  and that there is a democratic candidate. if the republican candidate wins  one would be surprised as a result of the failure of an active expectation  in the sense that the expected outcome was  at some point  actively inferred as a prediction . however  if  while having the same data base  one had never made the inference to the conclusion of an expected democratic victory  the surprise would be the result of a passive expectation failure. such failures are better thought of as failures of assumptions in that they derive from input propositions that are inconsistent with what one knows or believes but that were not actively predicted. some deviations from normalcy  kahneman & miller  1  should probably also be included in this category. there remains  however  an important third source of surprise  namely cases that do not arise from conflicts at all. in such cases the surprise results not from an input proposition violating an  active  expectation or a  passive  assumption  but from the input proposition being not practically deducible from the data base. in such cases there is no logical contradiction between the input proposition and a practically deducible proposition. rather  there is a conflict between the input proposition and what  after the fact  may be judged to be normal or usual. such retrospective judgments of unexpectedness are the subject of kahneman and miller's  1  norm theory. 
　　so we have the following important distinctions: first  states of the world corresponding to practically deducible propositions are in principle predictable  while states of the world corresponding to practically non-deducible propositions are not. 
　　
second  for states of the world corresponding to deducible propositions  there can be expectations if the related propositions were activated. these expectations are active  predictions  if they have been consciously entertained  and they are passive  assumptions  if they have not. third  for states of the world corresponding to non-deducible propositions  there cannot be expectations  active or passive   but only ex post facto evaluations of surprisingness on the basis of deviations from norms. it is our suspicion that in the real world  a great many surprising situations are of this latter variety  involving events whose prospects were never entertained but that just arose  out of the blue.  it is for this reason that one needs to be cautious about equating surprisingness with expectation failures. 
　　the table below presents the major sources of surprise that this kind of analysis yields. 
| status of input 
proposition nature of 
affected 
propositions 	related 	1
cognition active 	passive deducible immutable ml 
s
a   
prediction 1 
sp-i 
mbumptoin typical    l l o   sa   . prediction  1  
s	 s
p   a 
assumption not deducible immutable h 
no entry l l 
sp i none typical m 
no entry  1| 
1   sp   1 none consider first what happens when an input proposition conflicts with a  deducible  active prediction or a passive assumption. recall that an active expectation is one that is being or has been consciously entertained. such a conflict can legitimately be called an expectation failure in that there was an explicit expectation  rooted in a prediction  that failed. an example might be a case in which one chooses to go t/  a french restaurant because one feels like eating french food. on receiving the menu  one discovers that all the entrees are greek. such a situation would be highly surprising because it violates a virtually immutable constituent of the french restaurant schema which was active in the sense that it was a causal component of a decision to choose the restaurant. this source of surprisingness is shown in the table in cell   in which the degree of surprisingness is represented by s   where the subscript  a  indicates that the surprisingness results from an active prediction. in this particular case  s - 1  indicating a maximum degree of surprise. in fact  however  the surprisingness of such an event is independent of whether the violated expectation was active  i.e.  predicted  or passive  i.e.  merely assumed . the case of a violated assumption is shown in cell   where the surprisingness  sp  is written with a subscript  p  to indicate that it results from a passive expectation. an example would be a case in which one came across a green dog. given a reasonable representation of dog  in which  of course  the proposition that dogs are not green would not be explicitly represented   it is not difficult to infer that dogs are not green. however  in the normal course of events  such an inference would not be made although  to the degree that it can be readily inferred  it is a practically deducible proposition. thus  an input proposition to the effect that there is a green dog would conflict with a passive expectation for dogs not to be green. the high degree of surprise indicated in cells  l  and  suggests that the surprise results from the fact that it was an immutable proposition that was violated. 
　　in the case in which the violated proposition is only typically true as opposed to immutable  the level of surprise tends to be somewhat less high. for example  if  having chosen a french restaurant because one wanted to eat frogs' legs  the waiter explains that frogs legs are not a menu item in this particular restaurant  this might be somewhat surprising because typically  but by hypothesis  not necessarily  frogs legs can be ordered in french restaurants. this would be an example of the failure of an active expectation of something that is typically true. in such a case we might suppose 1   sa   1  cell  1  . what this indicates is that violations of propositions that are only typically true are less surprising than violations of immutable ones  which are at the limit of surprisingness. in fact  the same analysis applies to failures of passive expectations  i.e  assumptions  about things that arc typically true  although here we indicate a slightly reduced level of surprise because the expectation was not an active prediction  cell  . 
　　we move now to a discussion of cases in which surprise results from non-deducible propositions. there are two general classes of cases here  rather than the four in the practically deducible cases. this is because  by definition  one cannot actively predict something that is not realistically derivable from the data base  which means that the input proposition cannot  in principle  conflict with an active expectation. this is why there is no entry in cells  and . the kind of examples we have in mind in connection with input propositions that are not practically deducible are ones in which something happens that was not and could not  realistically  have been entertained. when a rock flies through one's office window  one will certainly be surprised  but the surprise cannot be attributed to a conflict with some practically deducible proposition. this is the case numbered  in the table. what we are claiming here is that the proposition that a rock will not fly through one's office window is not practically deducible. before we elaborate this example  we need to make a 
　　couple of preliminary observations. first  we are assuming a normal context  so that we would not want to make this claim for a person who looks out of his window and sees a riot taking place  with people hurling rocks in all directions. in such a situation  it would be quite possible to entertain the possibility. the proposition would then be practically deducible because new and relevant propositions would have been added to the knowledge-
'base. second  as mentioned earlier in explaining what counts as a practically deducible proposition  in normal contexts  the connection between what one knows and propositions about rocks flying through windows is so remote that it cannot be deduced. furthermore  there is nothing to motivate an attempt to deduce it. it is not realistic to suppose that a system with goals and tasks to perform is at the same time randomly spawning inferences about unrelated and improbable possibilities. this is what distinguishes the case of the rock from that of  say  tossing a coin. when a coin is tossed  the propositions that it might come up heads and that it might come up tails are  presumably  generated with more or less equal confidence  and their generation is motivated by the context. 
　　so  when the input proposition is a practically nondeducible proposition it cannot conflict with an expectation. it can  however  be inconsistent with practically deducible propositions that it suggests  and this inconsistency might be with relatively immutable propositions or only with ones that are typically true. if it is inconsistent with relatively immutable propositions the situation is very surprising  shown as sp = 1 in cell  . such a case might arise were one to suddenly see a person take off and fly with no apparent mechanical aids  superman style . if  on the other hand  the conflict is with a typical rather than an immutable proposition such as finding a restaurant in which one does not sit down to eat  or having a rock fly through one's window  we have the kind of surprisingness indicated in cell  1|. 
　　schank  1  offers an account of learning and reminding in terms of expectation failures that derive from predictions encoded in scripts. an active script generates active expectations  
	ortony and partridge 	1 
　　
all other scripted propositions  presumably  account for  what we term  passive expectations. schank briefly acknowledges the difficulties of  what for us are  type 1 expectation failures  but only to say that they are not  failed expectations in the straightforward sense of that term . thus  when schank talks about  expectations  he is referring to what we have identified as active expectations. yet  if one restricts expectation failures to the failures of active expectations  and postulates nothing else  it is difficult to avoid the conclusion that there can be situations in which one cannot possibly be surprised even though one has no idea what to expect. the solution to this problem is to allow that input propositions that are practically non-deducible can give rise to surprise. the omission of this class of sources of surprise  cells  1  and  of our table  from schank's discussion is probably the result of focusing too closely on the scripts and their capacity to generate active expectations at the expense of a more global view of the problem of expectations and surprise. surprisingness resulting from a nondeducible input proposition cannot  by definition  be accounted for in terms of script conflict. 
　　the term  expectations violation  is used by hayes-roth  1  to label a component of his description of  five heuristic methods for repairing flawed beliefs . the  beliefs  described are strategies for playing a card game  and his usage of the  expectations violation  concept is minimal. input data may d is confirm a prediction from a strategy for playing the card game. he considers only the case in which an expectation  that is  a prediction  turns out to be wrong  cell  in our table . 
　　partridge  1  has explored the scope and various manifestations of an in put-expectation discrepancy reduction mechanism. the numerous examples discussed illustrate the importance of this mechanism at a number of different levels of interpretation of intelligent behavior: cognitive  subsymbolic  and neuronal. the examples are also used to expose the range of differing limited perspectives from which input-expectation discrepancy is viewed. the paper argues for the importance of this discrepancy both as an attention-focusing mechanism and as the instigator of learning behaviors  with some  but by no means total  guidance as to what needs to be learned . 
　　dennett  1  notes that a midnightrsnack-making robot should be surprised by the fact that its beer glass has been glued to a shelf. he observes that a startle response means we must have expected something else  but that things are not that simple. one can be startled by something one didn't expect without having to expect something else. in terms of our analysis  the glued-down beer glass is an example of a cell  situation in which sp is perhaps closer to 1 than to 1. the robot's expectation that the glass would not be glued to the shelf is not a practically deducible one  and glasses are not typically glued to shelves but there is no reason why one could not be so glued. our contention that for this example sp will be at the high end of its range is due to the fact that the glued-down glass directly thwarts the robot's active plan to pick up the glass. by way of contrast  in the absence of some exceptional circumstance  the occurrence of say  alphabetically parked cars  e.g. an alpha romeo  abuick  and a chevrolet parked in line  again falls into cell  1  of our table. but this time sp will be at the low end of its range. it may even be zero if the mechanism of selective attention fails to register either the parked cars themselves or the alphabeticity of the parked threesome. dennett appears to have purposely selected cases characterized by cells  and  in our table  presumably precisely because such examples are particularly difficult to explain under any simplistic interpretation of surprise as expectation failure. 
　　as is clear from our introductory remarks  we believe that the registration of and reaction to failed expectations  failed assumptions  and unanticipated incongruities  are a crucial component of general intelligence. these sources of surprisingness can provide the basis for an attention-focusing mechanism  and a cue for learning. they are nothing like a complete answer to either problem  but they do constitute an important piece of the 
1 	cognitive modeling 
solution  a piece that has yet to be carefully explored in ai. thus we would expect to find considerations of surprisingness in a problem solver that is not pre-set to solve only one very specific problem  and as a precursor to machine learning where the raw input data are not highly preprocessed to remove all information that is irrelevant to the specific rule or concept being learned. 
dennett  d.  1 . cognitive wheels: the frame problem of ai. 
in c. hookway  ed. . mind; machines and evolution. new york: cambridge university press. 
hayes-roth  f.  1  using proofs and refutations to learn from experience. in r. s. michalski  j. g. carbonell  & t. m. mitchell  eds. . machine learning. palo alto  ca: tioga. 
kahneman  d. & miller  d. t.  1 . norm theory: comparing reality to its alternatives. psychological review  1  1. 
partridge  d.  1  inpuuexpectation discrepancy reduction: a ubiquitous mechanism  proc. 1th ijcai  los angeles  ca  1. 
schank  r. c.  1 . interestingness: controlling inferences. artificial intelligence  it  1. 
schank  r. c.  1 . dynamic memory. new york: cambridge university press. 
