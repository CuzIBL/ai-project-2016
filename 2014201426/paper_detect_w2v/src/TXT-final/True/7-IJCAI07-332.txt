
in this paper we present a discriminativeframework based on conditional random fields for stochastic modeling of images in a hierarchical fashion. the main advantage of the proposed framework is its ability to incorporate a rich set of interactions among the image sites. we achieve this by inducing a hierarchy of hidden variables over the given label field. the proposed tree like structure of our model eliminates the need for a huge parameter space and at the same time permits the use of exact and efficient inference procedures based on belief propagation. we demonstrate the generality of our approach by applying it to two important computer vision tasks  namely image labeling and object detection. the model parameters are trained using the contrastive divergence algorithm. we report the performance on real world images and compare it with the existing approaches.
1 introduction
probabilistic modeling techniques have been used effectively in several computer vision related tasks such as image segmentation  classification  object detection etc. these tasks typically require processing information available at different levels of granularity. in this work we propose a probabilistic framework that can incorporate image information at various scales in a meaningful and tractable model. our framework  like many of the existing image modeling approaches  is built on random field theory. random field theory models the uncertain information by encoding it in terms of a set s of random variables and an underlying graph structure g. the graph structure represents the dependencerelationship among these variables. we are interested in the problem of inferencing which can be stated as: given the values taken by some subset x （ s  estimate the most probable values for another subset y （ s. conventionally  we define the variables in the set x as observations and the variables in the set y as labels.
　the information relevant for constructing an accurate model of the image can be broadly classified into two categories widely known as bottom-up cues and top-down cues.
bottom-up cues refer to the statistical properties of individual pixels or a group of pixels derived from the outputs of various filters. top-down cues refer to the contextual features present in the image. this information represents the interactionsthat exist among the labels in the set y. these top-down cues need to be captured at different levels of scale for different tasks as described in  kumar and hebert  1 . bottom-up cues such as color and texture features of the image are insufficient and ambiguous for discriminating the pixels with similar spectral signatures. with reference to figure 1 taken from the corel image database  it can be observed that {rhino  land} and {sea  sky} pixels are closely related in terms of their spectral properties. however  an expanded context window can resolve the ambiguity. it also obviates the improbable configurations like the occurrence of a boundary between snow and water pixels. further enlarging the context window captures expected global relationships such as 1  polar bear surrounded by snow and 1  sky at the top of the image while water at the bottom. any system that fails to capture them is likely to make a large number of errors on difficult test images. in this work we try to model these relationships by proposing a discriminative hierarchical framework.

figure 1: contextual features at multiple scales play a vital role in discrimination when the use of local feature fails to give a conclusive answer.
1 related work
in the past   geman and geman  1  have proposed markov random field  mrf  based models for joint modeling of the variables x and y. the generative nature of mrfs restricts the underlying graph structure to a simple two dimensional lattice which can capture only local relationships among neighboring pixels  e.g. within a 1 〜 1 or a 1 〜 1 window. the computational complexity of the training and inference procedures for mrfs rules out the possibility of using a larger window size. multiscale random fields  msrfs   bouman and shapiro  1  try to remove this limitation by capturing the interactions among labels in a hierarchical framework. similar approach has been proposed by  feng et al.  1  using tree structured bayesian networks  tsbns . these approaches have the ability to capture long range label relationships. in-spite of the advantage offered by the hierarchical nature  these methods suffer from the same limitations of the generative models  namely 
  strong independence assumptions among the observations x.
  introduction of a large number of parameters by modeling the joint probability p y x .
  need for large training data.
　recently  with the introduction of conditional random fields  crfs   the use of discriminative models for classification tasks has become popular  lafferty et al.  1 . crfs offer a lot of advantages overthe generativeapproachesby directly modeling the conditional probability p y |x . since no assumption is made about the underlying structure of the observations x  the model is able to incorporate a rich set of non-independent overlapping features of the observations. in the context of images  various authors have successfully applied crfs for classification tasks and have reported significant improvementoverthe mrf based generativemodels he et al.  1; kumar and hebert  1 .
　discriminative random fields  drfs  which closely resemble crfs have been applied for the task of detecting manmade structures in real world images  kumar and hebert  1 . though the model outperforms traditional mrfs  it is not strong enough to capture long range correlations among the labels due to the rigid lattice based structure which allows for only pairwise interactions.  he et al.  1  propose multiscale conditional random fields  mcrfs  which are capable of modeling interactions among the labels over multiple scales. their work consists of a combination of three different classifiers operating at local  regional and global contexts respectively. however  it suffers from two main drawbacks
  including additional classifiers operating at different scales into the mcrf framework introduces a large number of model parameters.
  the model assumes conditional independence of hidden variables given the label field.
　though a number of generative hierarchical models have been proposed  similar work involving discriminative frameworks is limited  kumar and hebert  1 . in this paper we propose a tree structured conditional random field  tcrf  for modeling images. our results demonstrate that tcrf is able to capture long range label relationships and is robust enough to be applied to different classification tasks. the primary contribution of our work is a method which combines the advantages offered by hierarchical models and discriminative approaches in a single framework. the rest of the paper is organized as follows: we formally present our framework in section 1. in the section we also discuss the graph structure and the form of the probability distribution. training and inference for a tcrf are discussed in sections 1 and 1 respectively. we present the results of our model on two different tasks in section 1. a few possible extensions to our framework are discussed in section 1. we finally conclude in section 1.
1 tree structured conditional random field
let x be the observations and y the corresponding labels. before presenting our framework we first state the definition of conditional random fields as given by  lafferty et al.  1 .
1 crf definition
let g =  v e  be a graph such that y is indexed by the vertices of g. then  x y  is a conditional random field if  when conditioned on x  the random variables yi （ y obey the markov property with respect to the graph:
. here i ゛ j implies
that yi and yj are neighbors in g.
　by applying the hammersley-clifford theorem  li  1   the conditional probability p y |x  factors into a product of potential functions. these real valued functions are defined over the cliques of the graph g.
		 1 
　in the above equation c is the set of cliques of the graph g  yc is the set of variables in y which belong to the clique c and z is a normalization factor.
1 tcrf graph structure
consider a 1〜1 neighborhoodof pixels as shown in figure 1. one way to model the association between the labels of these pixels is to introduce a weight vector for every edge  u v  which represents the compatibility between the labels of the nodes u and v  figure 1a . an alternative is to introduce a hidden variable h which is connected to all the 1 nodes. for every value which variable h takes  it induces a probability distribution over the labels of the nodes connected to it  figure 1b .

figure 1: two different ways of modeling interactions among neighboring pixels.
　dividing the whole image into regions of size m 〜 m and introducing a hidden variable for each of them gives a layer of hidden variables over the given label field. in such a configuration each label node is associated with a hidden variable in the layer above. the main advantage of following such an approachis that long range correlationsamong non-neighboring pixels can be easily modeled as associations among the hidden variables in the layer above. another attractive feature is that the resulting graph structure induced is a tree which allows inference to be carried out in a time linear in the number of pixels  felzenszwalb and huttenlocher  1 . motivated by this  we introduce multiple layers of hidden variables over the given label field. each layer of hidden variables tries to capture label relationships at a different level of scale by affecting the values of the hidden variables in the layer below. in our work we take the value of m to be equal to 1. the quadtree structure of our model is shown in figure 1.

figure 1: a tree structured conditional random field.
　formally  let y be the set of label nodes  x the set of observations and h be the set of hidden variables. we call  {y h} x  a tcrf if the following conditions hold:
1. there exists a connected acyclic graph g =  v   e  whose vertices are in one-to-one correspondence with the variables in y “ h  and whose number of edges |e| = |v |   1.
1. the node set v	can be partitioned into subsets
 v1  v1  ... vl  such that and the vertices in vl correspond to the variables in y.
1. deg v  = 1 	 v （ vl.
　similar to crfs  the conditional probability p y h|x  of a tcrf factors into a product of potential functions. we next describe the form of these functions.
1 potential functions
since the underlying graph for a tcrf is a tree  the set of cliques c corresponds to nodes and edges of this graph. we define two kinds of potentials over these cliques:
local potential
this is intended to represent the influence of the observations x on the label nodes y. for every yi （ y this function takes the form exp Γi yi x  . Γi yi x  can be defined as
	Γi yi x  = yitwfi x 	 1 
　let l be the number of classes and f be the number of image statistics for each block. then yi is a vector of length l  w is a matrix of weights of size l 〜 f estimated from the training data and f .  is a transformation  possibly nonlinear  applied to the observations. another approach is to take Γi yi x  = logp yi |x   where p yi |x  is the probability estimate output by a separately trained local classifier  he et al.  1 .
edge potential
this is defined over the edges of the tree. let φt b be a matrix of weights of size l 〜 l which represents the compatibility between the values of a hidden variable at level t and its bth neighbor at level t+1. for our quad-tree structure b = 1 1 or 1. let l denote the number of levels in the tree  with the root node at level 1 and the image sites at level l and ht denote the set of hidden variables present at level t. then for every edge  hi yj  such that hi （ hl 1 and yj （ y the edge potential can be defined as exp hti φl 1 byj . in a similar mannerthe edgepotential between the nodehi at level t and its bth neighbor hj at level t + 1 can be represented as exp hti φt bhj .
　the overall joint probability distribution which is a product of the potentials defined above can be written as
		 1 
yi（y

yj（y hi（hl 1
（e
1 parameter estimation
given a set of training images t = { y1 x1 ... yn xn } we want to estimate the parameters Θ = {w Φ}. a standard way to do this is to maximize the conditional log-likelihood of the training data. this is equivalent to minimizing the kullback-leibler  kl  divergence between the model distribution p y|x Θ  and the empirical distribution q y|x  defined by the training set. the above approach  requires calculating expectations under the model distribution. these expectations are approximated using markov chain monte carlo  mcmc  methods. as pointed by  hinton  1   these methods suffer from two main drawbacks
1. the markov chain takes a long time to reach the equilibrium distribution.
1. the estimated gradients tend to be noisy  i.e. they havea high variance.
　contrastive divergence  cd  is an approximate learning method which overcomes this problem by minimizing a different objective function  hinton  1 . in this work we use cd learning for estimating the parameters.
1 contrastive divergence learning
let p be the model distribution and q1 be the empirical distribution of label variables. cd learning tries to minimize the contrastive divergence which is defined as
		 1 
where is the kullback-
leibler divergence between q & p. q1 is the distribution defined by the one step reconstruction of the training data vectors  hinton  1 . for any weight wij 
 d
Δwij =  η	 1   wij
　where η is the learning rate. let y1 be the one step reconstruction of training set y1. the weight update equations specific to our model are
	i（y1	yi（y1
yj（y1 hi（hl 1
 yj hi （e
yj（y
 yj hi （e

hi（ht hj（ht+1
 hi hj （e

 hi hj （e
1 inference
the problem of inference is to find the optimal label field y  = argmaxyp y|x . this can be done using the maximum posterior marginals  mpm  criterion which minimizes the expected number of incorrectly labeled sites. according to the mpm criterion  the optimal label for pixel i  y i is given as
	y i = argmaxyip yi |x  	 yi （ y	 1 
　due to the tree structure of our model the probability value p yi|x  can be exactly calculated using belief propagation  pearl  1 .
1 experiments and results
in order to evaluate the performance of our model  we applied it to two different classification tasks  namely  object detection and image labeling. our main aim was to demonstrate the ability of tcrf in capturing the label relationships. we have also compared our approach with the existing techniques both qualitatively and quantitatively. the predicted label for each block is assigned to all the pixels contained within it. this is done in order to compare the performance against models like mcrf which operate directly at the pixel level. we use a common set of local features as described next.
1 feature extraction
visual contents of an image such as color  texture  and spatial layout are vital discriminatory features when considering real world images. for extracting the color information we calculate the first three color moments of an image patch in the ciel a b  color space.
　we incorporate the entropy of a region to account for its randomness. a uniform image patch has a significantly lower entropy value as compared to a non-uniform patch. entropy is calculated as shown in equation 1
		 1 
where  p contains the histogram counts. texture is extracted using the discrete wavelet transform  mallat  1 . spatial layout is accounted for by including the coordinates of every image site. this in all accounts for a total of 1 statistics corresponding to one image region.
1 object detection
we apply our model to the task of detecting man-made structures in a set of natural images from the corel image database. each image is 1 〜 1 pixels. we divide each image into blocks of size 1 〜 1. each block is labeled as either structured or unstructured. these blocks correspond to the leaf nodes of the tcrf. in order to compare our performancewith drfs  we use the same set of 1 training and 1 testing images as used by  kumar and hebert  1 . we also implement a simple logistic regression  lr  based classifier in order to demonstrate the performance improvement obtained by capturing label relationships. as shown in table 1  tcrf achieves significant improvement over the lr based classifier which considers only local image statistics into account. figure 1 shows some the results obtained by our model.
table 1: classification accuracy for detecting man-made structures calculated for the test set containing 1 images.
modelclassification accuracy  % lr1drf1tcrf11 image labeling
we now demonstrate the performance of our model on labeling real world images. we took a set of 1 images consisting of wildlife scenes from the corel database. the dataset contains a total of 1 class labels: rhino/hippo  polar bear  vegetation  sky  water  snow and ground. this is a data set with high variability among images. each image is 1〜1 pixels. we divide each image into blocks of size 1〜1. a total of
1 images were chosen at randomfor training. the remaining 1 images were used for testing. we compare our results with that of a logistic classifier and our own implementation of the mcrf  he et al.  1 . it can be observed from table 1 that tcrf significantly improves over the logistic classifier by modeling the label interactions. our model also outperforms the mcrf on the given test set. qualitative results on some of the test images are shown in figure 1.
table 1: classification accuracy for the task of image labeling on the corel data set. a total of 1 testing images were present each of size 1 〜 1.
modelclassification accuracy  % lr1mcrf1tcrf11 discussion
the framework proposed in this paper falls into the category of multiscale models which have been successfully applied by various authors  bouman and shapiro  1; feng et al.  1 . the novelty of our work is that we extend earlier hierarchical approaches to incorporate them into a discriminative framework based on conditional random fields. there are several immediate extensions possible to the basic model presented in this work. one of them concerns with the  blocky  nature of the labelings obtained. the reason for this is the fixed region size which is used in our experiments. this behavior is typical of tree based models like tsbns. one solution to this problem is to directly model image pixels as the leaves of the tcrf. this would require increasing the number of levels of the tree. although the number of model parameters increase only linearly with the number of levels  training and inference times can increase exponentially with the number of levels. recently  he et al. have reported good results by applying their model over a super-pixelized representation of the image  he et al.  1 . this approach reduces the number of image sites and at the same time does not define rigid region sizes. we are currently exploring the possibility of incorporating super-pixelization into the tcrf framework.
1 conclusions
we have presented tcrf  a novel hierarchical model based on crfs for incorporating contextual features at several levels of granularity. the discriminative nature of crfs combined with the tree like structure gives tcrfs several advantages over other proposed multiscale models. tcrfs yield a general framework which can be applied to a variety of image classification tasks. we have empirically demonstrated that a tcrf outperforms existing approaches on two such tasks. in future we wish to explore other training methods which can handle larger numberof parameters. further we need to study how our framework can be adapted for other image classification tasks.
