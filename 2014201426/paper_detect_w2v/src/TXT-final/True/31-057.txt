 
incremental concept learning algorithms using backtracking have to store previous data. these data can be ordered by the  is more specific than  relation. using this order only the most informative data have to be stored  and the less informative data can be discarded. moreover  under certain conditions some data can be replaced by automatically generated  more informative data. 
we investigate some conditions for data to be discarded  independently of the chosen concept learning algorithm or concept representation language. then an algorithm for discarding data is presented in the framework of iterative versionspaces  which is a depth-first algorithm computing versionspaces as introduced by mitchell. we update the datastructures used in the iterative versionspaces algorithm  while preserving its most important properties. 
1 	introduction 
incremental concept learning algorithms maintaining a hypothesis consistent with all data  usually called examples or instances  have to store all previous data 
as soon as any backtracking is involved. exceptions are  e.g.  the candidate elimination algorithm  mitchell  1   because it searches bi-directionally  i.e.  specific-to-general and general-to-specific  and breadthfirst  or algorithms searching specific-to-general in a conjunctive tree-structure language  as incremental nonbacktracking focusing  smith and rosenbloom  1 . 
 hirsh  1  even prefers a representation storing all negative examples together with s over storing s and g in case g can grow exponentially or can be infinite.  bundy et a/.  1  argues that for learning disjunctive concepts all data will have to be stored anyway. 
　one of the goals of concept learning is compaction of the information provided to the algorithm. therefore  in cases where all instances have to be memorized  preferably no redundant information should be stored. in this paper  we remove redundant instances in a language independent way by partially ordering them  according to 
1 	cognitive modelling 
their information contents. in  sebag  1  and  sebag and rouveirol  1  this is done for negative examples in a conjunctive tree-structure  resp. first order logic language. according to the partial order  we only have to store minimal and maximal instances  while forgetting the ones with less information content. however  we have to take care that the search algorithm does not lose any solutions  does not search previously discarded parts of the search space again  and retains its most interesting properties. 
　we develop this idea in the framework of the iterative versionspaces algorithm  itvs   sablon et a/.  1 . nevertheless  we argue that it has a much wider application potential. the theory is formulated independently from any concept learning algorithm or search strategy and independently from the chosen concept representation language. the partial order on instances is defined solely in terms of the  is more specific than  relation. identifying and removing redundant instances can be used in any incremental algorithm that stores all instances  and even in a preprocessing phase of a non-incremental concept learning algorithm  to reduce its actual processing time. the reason for studying this problem in the context of itvs  is that we believe the datastructures and complexity measures of itvs contribute to understanding the nature and the complexity of concept learning. 
　we ensure that the main properties of itvs are retained: a worst case space complexity linear in the number of instances  and a worst case time complexity of testing a candidate hypothesis for maximal generality or maximal specificness also linear in the number of instances. the cost of extending the itvs algorithm is a global increase in time complexity quadratic in the number of instances. the gain is twofold: firstly storing less instances will reduce the memory needed by the algorithm. secondly  in case the size of s or g is exponential in the number of instances  the worst case time complexity of the search is exponential in the number of instances. with a branching factor 1  reducing the number of instances with a factor k  would then reduce the time complexity with a factor bk. 
　the paper is organized as follows: section 1 briefly reviews the definitions and invariants of the datastructures used in itvs. section 1 describes the theoretical background for discarding instances. in section 1 we 


	sablon and de raedt 	1 


1 	cognitive modelling 


	sablon and de raedt 	1 


1 	cognitive modelling 

	sablon and de raedt 	1 

although the worst case time complexity has increased 
w.r.t. itvs's  it has only increased with a quadratic term  while  if the size of s or g is exponential  reducing the number of 1-bounds  resp. g-bounds  would also reduce bearch time with an exponential factor. 
1 	related work 
the ideas extend the work of  sebag  1    sebag and 
rouveirol  1  and  smith and rosenbloom  1  in a language independent framework. in  sebag  1   which is restricted to conjunctive tree-structure languages  negative lowerbounds are converted into positive upperbounds  and only those nearest to the target concept  i.e.  the most specific ones  are stored. in 
 sebag and rouveirol  1  this is extended to negative lowerbounds in ilp  which are represented by integrity constraints and ordered by 1-subsumption. in our framework we can generalize the notion of a nearest miss  which is introduced in  sebag  1  and defined as a negative lowerbound which is not 1-prunable  to all negative instances neither 1-prunable nor g-prunable. 
　two aspects of inbf  smith and rosenbloom  1  can be compared to ours. in the specific-to-general search inbf drops all positive examples  because no backtracking is needed in searching specific-to-general in a conjunctive tree-structure language. using theorem 1  our approach would also drop all positive lowerbounds  except one  which would then coincide with s   because any two positive examples will have only one least upperbound. in the general-to-specific search inbf processes and then forgets all near-misses w.r.t. s. its maximally general hypothesis upper is only kept consistent with all positive examples and all near-misses  so no backtracking is needed. our notion of a near-miss generalizes this approach  by converting all negative lowerbounds to positive upperbounds  and considering their greatest lowerbound  i.e.  upper  as a positive upperbound. 
　 hirsh  1  informally describes a technique of  skipping data that do not change the versionspace  in the context of the incremental versionspace merging algorithm. intersecting a versionspace vs1 with vs1  and then with a subset vs1' of vs1 will always yield the latter intersection. consequently the first intersection operation was not necessary. in our framework  we more formally describe the approach using the notions 1-prunable and g-prunable  we relate these notions to the concept of near-misses and to inbf  and provide a framework to automatically generate new information elements. 
1 	conclusion 
we have introduced the notions of 1-prunable and gprunable instances and a generalized notion of near-miss. using these we identified redundant instances. we also introduced automatically created instances  that made other instances redundant without any loss of information. this resulted in data compaction. furthermore  as in  smith and rosenbloom  1   this paper shows that 
1 	cognitive modelling 
near-misses  and their dual counterpart  play a very important role in converging towards the target concept. 
　we have also shown how 1-prunable and g-prunable instances can be discarded in the framework of iterative versionspaces  without losing the most important properties of the itvs algorithm. 
acknowledgements 
luc de raedt is supported by the belgian national fund for scientific research and by the esprit project nr. 1 on ilp  and is co-financed by the flemish government under contract no. 1/1. 
