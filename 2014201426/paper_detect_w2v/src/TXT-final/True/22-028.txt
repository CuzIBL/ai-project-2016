 
in this paper  we explore the computational potential and limitations of the multi-layered connectionist models  minsky and papert  1 . we found that the number of layers and the width are two crucial parameters for the multilayered connectionist models. if each layer has the same size n and we increment the number of layers by 1  then the number of problems solved will increase 1{n1   times. on the other hand  suppose the number of layers is equal to 1. if we increment the width by 1  then the number of problem solved will increase  d nn   times  where n is the input size. hence  we can extend a 1-layered connectionist model by adding layers or increasing width. our conclusion is that increasing width is better than adding layers. 
1 introduction 
for studying learning in the multi-layered connectionist models  it is important to understand the computational potential and limitations of the multi-layered connectionist models. the single layered connectionist models have limited computational ability. for example  there is no single layered connectionist machine which can compute the 'exclusive or' of two bits. but the 'exclusive or' problem can be solved by a 1-layer connectionist machine. however  the computational ability of 1-layer connectionist models depends on the width of the models. we define the width of a connectionist machine to be the maximum number of neurons in one layer of the connectionist machine. the number of layers and the width are two crucial parameters for the multi-layered connectionist models. in this paper  we examine two extreme cases of the multi-layered connectionist models. in the first case  we fix the width of the multi-layered connectionist models and see what happen when the number of layers is increased. in the second case  we fix the number of layers to be two and see what happen when the width is increased. our result is that in the first case  if each layer has the same size n and we increment the number of layers by 1  then the number of problems solved will increase 1 nn   times; in the second case  when we increment the width by 1  the number of problem solved will increase 1 nn   times  where n is the input size. 
1 	parallel and distributed processing 
1 layered connectionist models 
a layered connectionist machine is a special case of connectionist models. it has t layers and one input layer  layer zero . the input  bottom  layer contains n input neurons. the last  top  layer contains m output neurons. each of the remainder layers contains w neurons. for i = 1 ...  t - 1  there are links connecting the neurons in layer i to the neurons in layer  i + 1 ; no other links exist. the following is a formal definition of layered connectionist machines. 


	ho 	1 

1 	parallel and distributed processing 


	ho 	1 
