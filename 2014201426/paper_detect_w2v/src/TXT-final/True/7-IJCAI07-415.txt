
many dynamic systems involve a number of entities that are largely independent of each other but interact with each other via a subset of state variables. we present global/local dynamic models  gldms  to capture these kinds of systems. in a gldm  the state of an entity is decomposed into a globally influenced state that depends on other entities  and a locally influenced state that depends only on the entity itself. we present an inference algorithm for gldms called global/local particle filtering  that introduces the principle of reasoning globally about global dynamics and locally about local dynamics. we have applied gldms to an asymmetric urban warfare environment  in which enemy units form teams to attack important targets  and the task is to detect such teams as they form. experimental results for this application show that global/local particle filtering outperforms ordinary particle filtering and factored particle filtering.
1 introduction
many systems involve a number of entities that interact with each other. these entities may be largely independent of each other  and only interact with other entities via a subset of state variables. an example is a collection of companies  that interact via market conditions. each company may have internal state that is conditionally independent of other companies given the market conditions. another example is the spread of an infectious disease through a population. each person may have an individual state  which corresponds to his or her symptoms  and people will interact via their infectious states. we present a type of model called global/local dynamic models  gldms  that allows the representation of these kinds of systems. in a gldm  the state of an entity is divided into two subsets: a locally influenced state that only depends on the state of the entity itself  and a globally influenced state that depends globally on the states of all entities. in this way  interactions between the entities are allowed  but much of the dynamic model is isolated within individual entities. observations are restricted to depend only on the state of individual entities.
　one would hope that in such a system  it will be possible to exploit the largely independent nature of the entities for efficient inference. we are concerned with the inference task of monitoring  i.e. the computation of the probability distribution over the state of the system at each point in time given the history of observations up to that time point. one popular approach to monitoring is particle filtering  pf   isard and blake  1   in which the state is estimated by a set of particles. however in pf all the inference is performed globally  and the largely local structure is not exploited. in particular  when there are many entities the global state is high-dimensional  and pf will not perform well. factored particle filtering  ng et al.  1  is an approach that attempts to address this issue by decomposing particles into factors. however  all the dynamics propagation and conditioning on observations is still performed globally. as a result  factored pf still has a hard time inferring the correct local state from observations.
　in global/local particle filtering  we introduce a simple inference principle: reason globally about global dynamics  and locally about local dynamics. like in factored pf  particles are decomposed into factors. unlike factored pf  however  only the dynamics propagation for globally influenced state is performed globally. dynamics propagation for locally influenced state and conditioning on observations are performed locally. this allows global/local pf to more accurately take into accountthe observationswhen inferringthe posteriordistribution over the local state of each entity.
　we present an application of our ideas to the task of monitoring goal formation of enemy units in an asymmetric urban warfare environment. in this task  there are a number of small  mobile enemy units moving around an urban environment. sometimes the enemy units adopt the goal of attacking one of a number of possible targets. the units communicate with each other to form teams to attack a target. our task is to detect when such teams have been formed  given the movements of the units and their communication.
　we present experimental results for our application. our results show that global/local pf considerably outperforms ordinary pf and factored pf at the task  and also outperforms a method that performs all its inference locally. our method scales reasonably well both to situations with twenty units and to those with twenty target locations.
1 global/local dynamic models
we are concerned with a dynamic system that evolves over time. we begin by describing a hidden markov model  hmm  rabiner and juang  1 . the state of the system at time t is represented by a variable xt. at each time t there is an observation represented by the variable ot. the dynamic system is defined by a transition model pt xt|xt 1 ; an observation model pt ot|xt ; and a distribution overthe initial state p1 x1 . we do not assume that the system is homogeneous  i.e. the transition and observation models may vary from one time point to the next. the joint probability distribution over a sequence of states and observations is given by
.
　in a global/local dynamic model  gldm   we let e1 ... en be a set of entities. we assume that the state of the system can be decomposed into a product of states of the individual entities. each entity ei has a locally influenced state uit and a globally influenced state vit. we denote the pair  and call it the local state of entity i at time t. also the tuple will be denoted by ut and  by vt  and the variable representing the entire state by xt. note that even the globally influenced state is called local state  because it pertains to a single entity. to summarize  the state space decomposes as
x.
　in the transition model  the locally influenced state is constrained to depend only on the previous state of the individual entity  and the current globally influenced state of the entity. no restriction is made on the globally influenced state  and no decomposition of the distribution of that part of the state is assumed. thus the transition model decomposes into
.
note that pit uit|xit 1 vit  may be different for different entities ei.
　the observation decomposes into a local observation oit of each entity ei that depends only on the local state of that entity. thus the observation model decomposes into

　just as the state of a hmm can be factored into the product of variables to produce a dynamic bayesian network  dbn   dean and kanazawa  1   so all the parts of a gldm can be factored into variables. thus the locally influenced state uit is factored into variables ui 1 ... ui m  and similarly for the globally influenced state  and the local observation. the factoring may be different for different entities. the probabilistic models are factored into the product of conditional probabilities of variables given their parents in the usual way. the restrictions on the model are extended naturally from the non-factored case. the parents of a locally influenced variable may only be local variables of the same entity at the previous or current time step. the parents of a globally influenced variable may be any variable at the previous time step  and any globally influenced variable at the current time step. the parents of a local observation variable may be any locally or globally influenced variable of the same entity at the current time step.

figure 1: dbn representation of two-entity gldm.
　a gldm can itself be viewed as a dbn  which is factored into variables representing the local states of individual entities  and the observations. this is the case even if the local states and observations are not themselves factored  as discussed in the previous paragraph. such a dbn  for two entities  is shown in figure 1. note that there is an edge from to. no assumption is made that the vit are conditionally independent.
　for an example of a gldm  consider a stock tracking and prediction application  where each entity may be a particular company. the locally influenced state may be the internal state of the company  while the globally influenced state may be the market conditions faced by the company. at each time point  the internal state of the company depends only on its previous internal state and on the market conditions it faces  whereas the market conditions are all dependent on all the previous market conditions. the observations may be the stock prices of individual companies.
　for another example  consider an application of tracking the spread of an infectious disease through a population. here the entities may be people  the locally influenced state may be the symptoms of the person  while the globally influenced state may be the stage of infection  if any  of the person. the transition model for the globally influenced state may specify that the infection stage of a person at time t depends on the infection stages at time t   1 of the people with whom the first person comes into contact at time t.
　for a third example  which will be expanded on in section 1  consider the task of monitoring enemy units moving around an urban environment. from time to time the units communicate and adopt goals of attacking possible targets. here the locally influenced state of an entity may consist of its current position  while the globally influenced state is its goal. when a unit communicates with another unit  its goal becomes dependent on the goal and position of the other unit as well as itself. a unit's position  however  depends only on its previous position and its current goal.
　let us consider the expressive power of gldms. trivially they can capture any hmm or dbn  since we can have a system with only one entity. it is more interesting to ask what can naturally be captured using the structure of the model. one question arises regarding the representation of a global state that applies to all entities  since each state in our representation is a local state of a particular entity. global state can be captured in a gldm by introducing an additional entity representing the global state  and making its state globally influenced. in the stocks example  there may be an entity representing general economic conditions  that influences the market conditions faced by each company. however  this is only legal if the global state does not influence locally influenced states  e.g. the internal states of the companies . another question involves the representation of observations. there are no observations that depend on the state of more than one entity. we will see a way to get around this restriction in our application in section 1. the reason for this restriction is that it will allow us  when performing inference as described in section 1  to condition on observations locally.
　gldms bear a superficial resemblance to factorial hmms  ghahramani and jordan  1 . in a factorial hmm  there are a number of hidden state sequences  each of which evolves independently. the observation depends jointly on all the hidden states. in fact gldms and factorial hmms are quite different. factorial hmms cannot easily be modeled as gldms  because the observation depends on the joint state of all entities and not on the hidden state of a single entity. on the other hand  in factorial hmms all sequences evolve independently  so there is no globally influenced state; the state of each entity is completely locally influenced.
1 inference
there are several inference tasks on dynamicsystems  including diagnosing the past  predicting the future  and keeping track of the current state. in this paper  we will focus on the latter task  known variously as monitoring  filtering and state estimation. the filtering task is to compute  at each time point t  p xt|o1 ... ot   where o1 ... ot is the sequence of observations obtained up to time t. the quantity p xt|o1 ... ot  is known as the belief state at time t. in principle  this can be computed simply using bayesian updating:
.
in practice this is very difficult because the state space may be very large. even if the transition and observation models are represented in factored form as in a dbn  the belief state cannot be decomposed and must be represented as an explicit joint distribution over the state variables  which is exponential in the number of variables. the same holds for gldms. after a certain amount of time  the local states of all the entities become dependent on each other  and performing the filtering exactly requires a belief state which is a joint probability distribution over the local states of all entities. this is exponential in the number of entities.
　therefore approximate filtering algorithms are used. one standard algorithm for dbns is the boyen-koller algorithm  bk   boyen and koller  1 . in this algorithm  the variables of the dbn are partitioned into clusters. in a gldm  each cluster will be the locally and globally influenced state of one entity. an approximate belief state p  is maintained as a product of distributions p i over the clusters  i.e.
.
in principle  the method works by beginning with the factored distributions p i xit 1|o1 ... ot 1   multiplying them to obtain p  xt 1|o1 ... ot 1   propagating through the dynamics and conditioning on the observation to obtain p  xt|o1 ... ot   and then marginalizng onto the factors to obtain p i xit|o1 ... ot . the joint distributions p  xt 1|o1 ... ot 1  and p  xt|o1 ... ot  are not represented explicitly. instead  the factored distributions
p i xit|o1 ... ot  are computed more efficiently. one way to do that is to create a junction tree representing two time slices of the dbn  in which each factor is contained in a clique at both the previous and current time points. in practice  even though this method often results in more efficient inference than the exact method  sometimes the cliques of the junction tree are too large and the method is still too expensive to be practical. this may particularly be a problem with some gldms  because no restrictions are placed on the way globally influenced variables evolve.
　an alternative approach to approximate inference is to use particle filtering  pf   isard and blake  1 . in pf  the joint distribution over the state variables is approximated by a set of samples  or particles as they are called. each particle contains an assignment of values to the state variables. the probability of any property of the state is the fraction of particles that have that property. the basic steps of pf for a gldm are as follows.
begin with m particles xt 1  ... xt 1 m.
for m = 1 to m:
propagate:
sample v t m from pt vt|xt 1 m . for each entity ei:
	sample u t mi	from pit uit|xti 1 m v it m .
condition:
.
resample:
for:
choose xt  from x t 1 ... x t m  with probability that x t m is chosen being proportional to wm.
　the difficulty with pf for this problem is that the variance of the method is high and the number of particles required for a good approximation generally grows exponentially with the dimensionality of the problem. therefore this approach does not scale well with the number of entities. an observation is that the different entities are somewhat independent of each other. the different entities interact with each other only through the globally influenced state. if these interactions are relatively weak  we might be able to take advantage of that fact. we might expect that instead of maintaining particles that assign values to all variables for all units  we can maintain local particles that only assign values to variables belonging to a single unit. this is the idea behind factored particle filtering  ng et al.  1 . in factored particle filtering  the state variables are divided into factors. the joint distribution over all state variables is approximated by the product of marginal distributions over the factors  as in bk. furthermore  the marginal factor distributions are approximated by a set of factored particles. factored pf introduces two new steps into the pf process described above. the first joins factored particles together to produce global particles. the second projects global particles back down onto the factors. in between these two steps  all the usual steps of pf are performed. in particular  propagating through the dynamics and conditioning on the observations are done with global particles.
　for this reason  ordinary factored pf is also not ideal for our situation. the problem is that in any global particle  it is highly likely that there will be some entities whose local state is far from the truth. therefore  it will often be the case that for all global particles in the set of particles  the probability of the observation will be extremely low. even if one entity's local state in the particle is good  other entities' states may be bad and so the observation will not confirm the first entity's state. as a result  inference about entities' true local states based on the observations will be poor. 1 global/local particle filtering
in order to allow observations about an entity to more effectively condition its local state  we introduce global/local particle filtering. global/local pf is based on the principle of reasoning globally about global dynamics and locally about local dynamics. the method involves a simple change to factored pf  but one that makes a big difference. instead of performing all the dynamics propagation globally  and conditioning on observations globally  and only then projecting down onto the individual factors  we project immediately after propagating the dynamics for the globally influenced variables. propagation of dynamics for locally influenced variables and conditioning on observations are performed locally. the global/local pf process is as follows:
begin with m factored particles for each entity ei.
join the factored particles for different entities together to produce m global particles.  for details on the join process see  ng et al.  1  .
for m = 1 to m:
propagate globally:
sample v t m from pt vt|xt 1 m .
project:
projectdown to
for each entity ei.
for each entity ei: for m = 1 to m: propagate locally:
	sample u t mi	from pit uit|xti 1 m v it m .
condition:
.
resample:
for:
choosefrom x t i 1 ... x t mi	  with probability that x t mi	is chosen being proportional to wm.
　why does this method work  the key point is that in order for local propagation and conditioning to be successful  we don't need to have exactly the right joint distribution over all the globally influenced states. it is enough that the marginal distributions over individual entities' globally influenced states is approximately correct. if this happens  when we condition the local state of each entity on the local observation  we will produce an approximately correct posterior distribution over the local state. this is much easier to acheive than producing an approximately correct joint posterior distribution.
　it is important to note that something is lost by propagating and conditioning locally. we lose the ability to reason from observations of one entity to another entity. for example  in the domain of detecting goals of enemy units  we are unable to reason about the fact that since unit 1 is moving towards target 1  and we have some reason to believe that unit 1 and unit 1 are on a team  then it is likely that unit 1 has a goal of attacking target 1. we do on the other hand successfully reason about the interaction between the units when reasoning globally  so we may infer that since unit 1 and unit 1 appear to have formed a team  they are both a priori likely to be attacking target 1. we are just unable to use the observation about unit 1 to confirm our beliefs about unit 1. the hope is that the inability to perform this type of reasoning is outweighed by the fact that inferences about individual units from their own observations are more accurate. the success of the global/local reasoning method will depend on this tradeoff  and how important this type of reasoning is in a particular application.
1 application: monitoring dynamic goals of enemy units
we have applied gldms to monitoring the dynamic goals of enemy units in an asymmetric urban warfare environment. in this scenario  units move about on a streetmap  and adopt goals of attacking one of a number of target locations. the motion of units depends on their goals; a unit with a goal of attacking a target will generally move in the direction of the target.
　the goals of units can change dynamically. a unit may adopt a new goal in one of three ways: two units communicate and jointly agree to adopt a new goal; two units communicate and one invites the other to adopt its goal; or one unit spontaneously decides to adopt a goal. in all cases  the goal adoption decision is influenced by the proximity of the units to the goal; a unit will prefer to choose to attack a closer target.
　our task is to detect threats such as ambushes to targets  as soon as possible after they are formed. two sources of evidence can be used. the first is a noisy sensor of the current position of each unit. the second is a noisy indication of whether or not a unit communicates. communication provides some indication that two units are forming a team. this is a weak inference  however. even when a unit communicates  usually the communication will not be related to goal adoption.
　we model this scenario with a gldm in which each entity corresponds to a unit. since a unit's movement depends only on its own goal  the position of an entity is a locally influenced state. units interact with other units in adopting goals  therefore the goal of a unit is a globally influenced state. in the dynamic model  the new goal of a unit depends on its previous goal and position  and the previous goal and position of a unit with which it communicates. since this could be any unit  it depends on all the units' goals and positions.
　the positional observation of each unit is a local observation. whether or not two units communicate  however  depends on the goals and positions of both units  so the communication observations cannot be adequately captured in our framework as local observations. to get around this issue  we use the technique of evidence reversal  kanazawa et al.  1 . instead of having the observation depend on the state  we condition the transition model on the observation. that is  for each possible configuration of the communication sensors  we have a different transition model. this is allowed since we have not assumed that the dynamic model is homogeneous. in the inference process  we can sample from this conditioned dynamic model as follows: we first sample a configuration of actual communications given the noisy observations. we then sample which pairs of units communicate from the list of communicating units. for each pair of communicating units  we determine whether they jointly adopt a new goal or one joins the other's team  based on their previous goals and their proximity to the targets. then  for each unit whose goal has not been determined by communication  we determine whether it spontaneously adopts a new goal.
1 experiments
we tested the global/local particle filtering algorithm on simulated data generated from the model  and compared its performanceto ordinary pf and factored pf. we also constructed an algorithm in which all interactions between units are ignored and all inference is performed locally  and compared our algorithm to that.
　each experiment consisted of one run of the system  with units moving about and choosing targets. each run of the system lasted 1 time steps. a threat  which was defined to be four units sharing a common goal  was considered to be successfully detected if it was discovered within 1 time steps of its development. this was enough time for each unit to reach two intersections on average  and was considered to be the minimum amount of time in which our system could reasonably be expected to detect goal-directed behavior. if the threat was not detected within that time  the result was a false negative. if a threat was reported when none was present  the result was a false positive. for each experiment  we ran 1 runs and counted the number of true positives  tp   false positives  fp   and false negatives  fn . our metrics are precision  which is  i.e the fraction of threats reported by the algorithm that were really threats  and recall  which is tptp+fn   i.e. the fraction of real threats caught by the algorithm. one parameter of the algorithm is the threshold of probability at which a threat is reported. we varied this probability in each experiment  thereby trading off precision for recall. in all experiments  we adjusted the number of particles allocated to each algorithm so that they all had approximately the same running time.
　figure 1 a  shows the precision-recall curves for each method for experiments with ten units and six target locations. the graph shows the recall that could be achieved for different levels of precision. also shown for reference is the performance of random guessing. while all methods do better than random guessing  our method does best  getting much higher precision while still achieving high recall. at one point it achieves 1% precision with 1% recall. this is quite good performance considering the difficulty of the task. when there are a number of units moving about the map  it is quite likely that at some point in time several units will appear to be moving towards a target  even though in actual fact they have no intent of attacking it. thus if we wish to achieve a high recall  we cannot avoid having a relatively low precision. interestingly  factored pf performs very poorly  indicating that it is not simply the factoring that leads to the good performance of our method  but reasoning locally about unit positions. also  note the relatively poor performance of the method that does all the reasoning locally. this shows that the global part of global/local pf is important.
　figure 1 b  shows how the algorithms scale up to a situation with 1 units. the task is more difficult  because there is more opportunity for units to appear to be heading towards a goal in the course of their business. again our method does best  at one point achieving 1% precision with 1% recall. figure 1 c  shows the performance when the number of targets is increased to 1. this is a much harder task  because some targets are close to each other and it is difficult to identify a unit's goals. nevertheless  our method is able to achieve relatively good performance getting 1% precision with 1% recall.

figure 1: comparison of performance without different evidence.

	 a 	 b 	 c 
figure 1: comparison of methods:  a  1 units  1 targets;  b  1 units  1 targets;  c  1 units  1 targets.　figure 1 assesses the relative importance of each of the two sources of evidence  observations about the position of units  and about communications. we see that evidence from positional observations is more important  but taking communications into account is also useful. surprisingly  the method that does not take into account communication evidence performs better than the method that performs all the inference locally. the reason may be that although it does not take into account evidence pertaining to unit interactions  it still reasons about them and accounts for their possibility.
1 conclusion
many situations involve a number of entities that are largely independent of each other and only interact via a portion of their state. we have presented global/local dynamic models to represent these kinds of situation. we have presented global/local particle filtering  a monitoring algorithm based on the principle of reasoning globally about global dynamics and locally about local dynamics. we have applied our ideas to monitoring the goals of units in an urban warfare environment. we have shown experimentally that glpf performs better than ordinary pf and other competitors on this application.
　in future  it is important to explore whether global/local pf has benefits in other applications. we wish to explore ways in which gldms can be exploited for other types of inference algorithms. we would also like to extend our application to allow the number of units to change  and units to split or merge over time.
acknowledgements
this work was funded by onr contract n1-c-1  with thanks to dr wendy martinez.
