 
this paper introduces the point-based value iteration  pbvi  algorithm for pomdp planning. pbvi approximates an exact value iteration solution by selecting a small set of representative belief points and then tracking the value and its derivative for those points only. by using stochastic trajectories to choose belief points  and by maintaining only one value hyper-plane per point  pbvi successfully solves large problems: we present results on a robotic laser tag problem as well as three test domains from the literature. 
1 introduction 
the value iteration algorithm for planning in partially observable markov decision processes  pomdps  was introduced in the 1s  sondik  1 . since its introduction numerous authors have refined it  cassandra et al  1; kaelbling et a/.  1; zhang and zhang  1  so that it can solve harder problems. but  as the situation currently stands  pomdp value iteration algorithms are widely believed not to be able to scale to real-world-sized problems. 
　there are two distinct but interdependent reasons for the limited scalability of pomdp value iteration algorithms. the more widely-known reason is the so-called curse of dimensionality  kaelbling et al.% 1 : in a problem with n physical states  pomdp planners must reason about belief states in an  n - l -dimensional continuous space. so  naive approaches like discretizing the belief space scale exponentially with the number of states. 
　the less-well-known reason for poor scaling behavior is what we will call the curse of history: pomdp value iteration is in many ways like breadth-first search in the space of belief states. starting from the empty history  it grows a set of histories  each corresponding to a reachable belief  by simulating the pomdp. so  the number of distinct actionobservation histories considered grows exponentially with the planning horizon. various clever pruning strategies  littman et al  1; cassandra et al% 1  have been proposed to whittle down the set of histories considered  but the pruning steps are usually expensive and seem to make a difference only in the constant factors rather than the order of growth. 
　the two curses  history and dimensionality  are related: the higher the dimension of a belief space  the more room it has for distinct histories. but  they can act independently: planning complexity can grow exponentially with horizon even in problems with only a few states  and problems with a large number of physical states may still only have a small number of relevant histories. in most domains  the curse of history affects pomdp value iteration far more strongly than the curse of dimensionality  kaelbling et al.1; zhou and hansen  1 . that is  the number of distinct histories which the algorithm maintains is a far better predictor of running time than is the number of states. the main claim of this paper is that  if we can avoid the curse of history  there are many real-world pomdps where the curse of dimensionality is not a problem. 
　building on this insight  we present point-based value iteration  pbvi   a new approximate pomdp planning algorithm. pbvi selects a small set of representative belief points and iteratively applies value updates to those points. the point-based update is significantly more efficient than an exact update  quadratic vs. exponential   and because it updates both value and value gradient  it generalizes better to unexplored beliefs than interpolation-type grid-based approximations which only update the value  lovejoy  1; brafman  1; hauskrecht  1; zhou and hansen  1; bonet  1  . in addition  exploiting an insight from policy search methods and mdp exploration  ng and jordan  1; thrun  1   pbvi uses explorative stochastic trajectories to select belief points  thus reducing the number of belief points necessary to find a good solution compared to earlier approaches. finally  the theoretical analysis of pbvi included in this paper shows that it is guaranteed to have bounded error. this paper presents empirical results demonstrating the successful performance of the algorithm on a large  1 states  robot domain called tag  inspired by the game of lasertag. this is an order of magnitude larger than other problems commonly used to test scalable pomdp algorithms. in addition  we include results for three well-known pomdps  where pbvi is able to match  in control quality  but with fewer belief points  the performance of earlier algorithms. 
1 an overview of pomdps 
the pomdp framework is a generalized model for planning under uncertainty  kaelbling etal  1; sondik  1 . a pomdp can be represented using the following n-tuple: where 1 is a  finite  set of discrete 

probabilistic planning 	1 

 a number of algorithms have been proposed to implement this backup by directly manipulating a-vectors  using a com-

bination of set projection and pruning operations fsondik  1; cassandra et ai  1; zhang and zhang  1 . we now describe the most straight-forward version of exact pomdp value iteration. 
figure 1: pomdp value function representation using pbvi  on the left  and a grid  on the right . 
　the complete pbvi algorithm is designed as an anytime algorithm  interleaving steps of value iteration and steps of belief set expansion. it starts with an initial set of belief points for which it applies a first series of backup operations. it then grows the set of belief points  and finds a new solution for the expanded set. by interleaving value backup iterations with expansions of the belief set  pb vi offers a range of solutions  gradually trading off computation time and solution quality. we now describe how we can efficiently perform point-based value backups and how we select belief points. 
1 	point-based value backup 
to plan for a finite set of belief points we modify the backup operator such that only one vector per belief point is maintained. for a point-based update we start by creating projections  exactly as in eqn 
 next  the cross-sum operation  eqn 1  is much simplified by the fact that we are now operating over a finite set of points. 
we construct 
 1  
finally  we find the best action for each belief point  step 1 : 
		 1  
when performing point-based updates  the backup creates projections as in exact vi. however the final so-
lution is limited to containing only components  in time thus a full point-based value up-
date takes only polynomial time  and even more crucial  the size of the solution set remains constant. as a result  the pruning of a vectors  and solving of linear programs   so crucial in exact pomdp algorithms  is now unnecessary. the only pruning step is to refrain from adding to v any vector already included  which arises when two nearby belief points support the same vector 
　in problems with a finite horizon /i  we run h value backups before expanding the set of belief points. in infinite-horizon problems  we select the horizon so that 

1 	belief point set expansion 
as explained above  pbvi focuses its planning on relevant beliefs. more specifically  our error bound below suggests that pbvi performs best when its belief set is uniformly dense in the set of reachable beliefs. so  we initialize the set b to contain the initial belief and expand b by greedily choosing new reachable beliefs that improve the worst-case density as rapidly as possible. 
 the last inequality holds because each a-vector represents the reward achievable starting from some state and following some sequence of actions and observations. 
     1thc actual choice of norm doesn't appear to matter in practice; some of our experiments below used euclidean distance  instead of l   and the results appear identical. 
     1we experimented with other strategies such as adding a fixed number of new beliefs  but since value iteration is much more ex-

pensive than belief computation the above algorithm worked best. if desired  we can impose a maximum size on b based on time constraints or performance requirements. 
   1if not all beliefs are reachable  we don't need to sample all of densely  but replace by the set of reachable beliefs 	below. the error bounds and convergence results hold on 
probabilistic planning 	1 

the domain of tag is based on the popular game of lasertag. the goal is to search for and tag a moving opponent  rosencrantz et al  1 . figure 1a shows the live robot as it moves in to capture an opponent. in our pomdp formulation  the opponent moves stochastically according to a fixed policy. the spatial configuration of the domain used for planning is illustrated in figure 1b. this domain is an order of magnitude larger  1 states  than mosr other pomdp problems considered thus far in the literature  cassandra  1   and is proposed as a new challenge for fast  scalable  pomdp algorithms. a single iteration of optimal value iteration on a problem of this size could produce over 1 a-vectors before pruning. 

figure 1: tag domain  1 states  1 actions  1 observations  
the state space is described by the cross-product of 
two features  robot 	=	a	n	d opponent 	- 
bom agenis siart in independently-
selected random positions  and the game finishes when opponent  the robot can select from five actions: 
{north  south  east  west  lag}. a reward of -1 is imposed for each motion action; the tag action results in a +1 reward if robot = opponent  or -1 otherwise. throughout the game  the robot's position is fully observable  and the effect of a move action has the predictable deterministic effect  e.g.: 

the position of the opponent is completely unobservable unless both agents are in the same cell. at each step  the opponent  with omniscient knowledge  moves away from the robot with pr = 1 and stays in place with pr = 1  e.g.: 

domains. in the tag domain  however  it lacks the representational power to compute a good policy. 
1 	additional experiments 
1 	comparison on well-known problems 
to further analyze the performance of pbvi  we applied it to three well-known problems from the pomdp literature. we selected maze1  hallway and hallway 1 because they arc commonly used to test scalable pomdp algorithms  littman etal  1; brafman  1; poon  1 j. figure 1 presents results for each domain. replicating earlier experiments  results for maze1 arc averaged over 1 runs  reset after goal  terminate after 1 steps ; results for hallway and hallway1 are averaged over 1 runs  terminate at goal  max 1 steps . in all cases  pbvi is able to find a good policy. table 1 compares pbvl's performance with previously published results  comparing goal completion rates  sum of rewards  policy computation time  and number of required belief points. in all domains  pbvi achieves competitive performance  but with fewer samples. 
1 	validation of the belief set expansion 
to further investigate the validity of our approach for generating new belief states  section 1   we compared our approach with three other techniques which might appear promising. in all cases  we assume that the initial belief b1  given as part of the model  is the sole point in the initial set  and consider four expansion methods: 
1. random  ra  
1. stochastic simulation with random action  ssra  
1. stochastic simulation with greedy action  ssga  
1. stochastic simulation with explorative action  ssea  
the ra method consists of sampling a belief point from a uniform distribution over the entire belief simplex. ssea is the standard pbvi expansion heuristic  section 1 . ssra similarly uses single-step forward simulation  but rather than try all actions  it randomly selects one and automatically accepts the posterior belief unless it was already in b. finally  ssga uses the most recent value function solution to pick the greedy action at the given belief 1  and performs a single-step simulation to get a new belief 
　we revisited the hallway  hallway1  and tag problems from sections 1 and 1 to compare the performance of these 


figure 1: pbvi performance for four problems: tag left   maze1 center-left   hallway center-right  and hallway1 right  

table 1: results for pomdp domains. those marked were computed by us; other results were likely computed on different platforms  and therefore time comparisons may be approximate at best. all results assume a standard  not lookahead  controller. 
four heuristics. for each problem we apply pbvi using each of the belief-point selection heuristics  and include the qmdp approximation as a baseline comparison. figure 1 shows the computation time versus the reward performance for each domain. 
　the key result from figure 1 is the rightmost panel  which shows performance on the largest  most complicated domain. in this domain our ssea rule clearly performs best. in smaller domains  left two panels  the choice of heuristic matters less: all heuristics except random exploration  ra  perform equivalently well. 
1 related work 
significant work has been done in recent years to improve the tractability of pomdp solutions. a number of increasingly efficient exact value iteration algorithms have been proposed  cassandra et al.  1; kaelbling et al.  1; zhang and zhang  1 . they are successful in finding optimal solutions  however are generally limited to very small problems  a dozen states  since they plan optimally for all beliefs. pbvi avoids the exponential growth in plan size by restricting value updates to a finite set of  reachable  beliefs. 
　there are several approximate value iteration algorithms which are related to pbvi. for example  there are many gridbased methods which iteratively update the values of discrete belief points. these methods differ in how they partition the belief space into a grid  brafman  1; zhou and hansen  1 . 
　more similar to pbvi are those approaches which update both the value and gradient at each grid point  lovejoy  1; hauskrecht  1; poon  1 . while the actual point-based update is essentially the same between all of these  the overall algorithms differ in a few important aspects. whereas poon only accepts updates that increase the value at a grid point  requiring special initialization of the value function   and hauskrecht always keeps earlier a-vectors  causing the set to grow too quickly   pbvi requires no such assumptions. a more important benefit of pbvi is the theoretical guarantees it provides: our guarantees are more widely applicable and provide stronger error bounds than those for other pointbased updates. 
　in addition  pbvi is significantly smarter than previous algorithms about how it selects belief points. pbvi selects only reachable beliefs; other algorithms use random beliefs  or  like poon's and lovejoy's  require the inclusion of a large number of fixed beliefs such as the corners of the probability simplex. moreover  pbvi selects belief points which improve its error bounds as quickly as possible. in practice  our experiments on the large domain of lasertag demonstrate that pbvfs belief-selection rule handily outperforms several alternate methods.  both hauskrecht and poon did consider using stochastic simulation to generate new points  but neither found simulation to be superior to random point placements. we attribute this result to the smaller size of their test domains. we believe that as more pomdp research moves to larger planning domains  newer and smarter belief selection rules will become more and more important.  
　gradient-based policy search methods have also been used to optimize pomdp solutions  baxter and bartlett  1; kearns et al  1; ng and jordan  1   successfully solving multi-dimensional  continuous-state problems. in our view  one of the strengths of these methods lies in the fact that they restrict optimization to reachable beliefs  as does pbvi . unfortunately  policy search techniques can be hampered by low-gradient plateaus and poor local minima  and typically require the selection of a restricted policy class. 

probabilistic planning 	1 


figure 1: belief expansion results for three problems: hallway left   hallway1 center  and tag right  

1 	conclusion 
this paper presents pbvi  a scalable anytime algorithm for approximately solving pomdps. we applied pbvi to a robotic version of lasertag  where it successfully developed a policy for capturing a moving opponent. other pomdp solvers had trouble computing useful policies for this domain. pbvi also compared favorably with other solvers on three well-known smaller test problems. we attribute pbvi's success to two features  both of which directly target the curse of history. first  by using a trajectory-based approach to select belief points  pbvi focuses planning on reachable beliefs. second  because it uses a fixed set of belief points  it can perform fast value backups. 
   in experiments  pbvi beats back the curse of history far enough that we can solve pomdps an order of magnitude larger than most previous algorithms. with this success  we can now identify the next hurdle for pomdp research: contrary to our expectation  it turns out to be the old-fashioned mdp problem of having too many distinct physical states. this problem hits us in the cost of updating the point-based value function vectors.  this cost is quadratic in the number of physical states.  while this problem is not necessarily easy to overcome  we believe that sparse matrix computations  together with other approaches from the existing literature  poupart and boutilier  1; roy and gordon  1   will allow us to to scale pbvi to problems which are at least another order of magnitude larger. so  pbvi represents a considerable step towards making pomdps usable for realworld problems. 
