automatic thesaurus construction based on grammatical relations 
t o k u n a g a t a k e n o b u i w a y a m a m a k o t o t a n a k a h o z u m i dept 	of computer science advanced 	research lab dept 	of computer science tokyo institute of technology hitachi l t d tokyo institute of technology 1-1 o k a   a m a meguro 1 akanuma hato ama 1-1 ookayama meguro tokyo 1 japan hiki 1 japan tokyo 1 japan 
a b s t r a c t 
we propose a method to build thesauri on the basis of grammatical relations the proposed method constructs thesaun by using a hierarchical clustering algorithm an important point in this paper is the claim that thesauri in order to be efficient need to take  surface  case information into account we refer to the thesauri as ' relation-based thesaurus  rbt    in the experiment four rbts of japanese nouns were constructed from 1 verb-noun cooccurrences  and each rbt was evaluated fry objective criteria the experiment has shown that the rbts have better properties for selectional restriction of case frames than conventional ones 
1 	i n t r o d u c t i o n 
for most natural language processing  nlp  systems thesaun are one of the basic ingredients in particular coupled with case frames  they are useful to guide corrert analysis  allen  1  in the examplebased frameworks thesauri are also used to compensate for insufficient example data  sato and nagao  1  nagao and kurohashi 1  roget s international thesaurus  chapman  1  bunruigoihyo  hayashi 1  and wordnet  miller et al  1  are typical thesaun which have been used in the past nlp research all of them are handcrafted  machine-read able and have farely broad coverage however  since these thesaun are originally compiled for human use they are not always suitable for natural language processing by computers their classification of words is sometimes too coarse and does not provide sufficient distinctions between words 
　one of the reasons for this is that these thesauri aim for broad coverage  rather than for dealing with a par ticular domain experience has shown that restricting the target domain appropriately is the key to building successful nlp systems this fact has been discussed by researchers working on  sublanguage   gnshman and sterling  1  sekine et al 1  or  register   halliday and hassan  1 biber  1  another problem with handcrafted thesauri is the fact that their classification is based on the intuition of lexicographers  with their cnteria of classification not being alwavs clear furthermore crafting thesauri by hand is very expensive even in restricted domains 
　therefore building thesauri automatically from corpora has received a large attention in recent years  hirschman et al 1  hindle 1 hatzivassiloglou and mckeown  1  pereira et al 1  these attempts basically take the following steps  charniak 1  
 1  extract co-occurrences 
 1  define similarities  distance  between words on the basis of co-occurrence data 
 1  cluster words on the basis of similarity 
at each step  we have several options in this paper we will focus on step  1  the properties of co-occurences as for step  1  and  1  we will use the method proposed by iwayama and tokunaga  lwayama and tokunaga  1   which is bnefl  described in section 1 
　co-occurrenres are usually gathered on the basis of some relations such as predicate-argument modifiermodified  adjacency or mixture of them for example hmdle used verb-subject and verb-object relations to classify nouns  hindle  1  hirschman et al also used verb-subject and verb-object relations as well as prepositions and adjective-noun relations  hirschman tt al  1  hatzivassiloglon and mckeown suggested to 
use as many relations as possible in order to classify ad-
jectives  hatzivassiloglou and mckeown  1  
　all these attempts assume a distribution hypothesis that is words appearing in a similar context are similar hence they should be classified into the same class  grishman et al  1  hindle  1  as far as we concerned  we consider co-occurrences of words as a kind of context the more specific the context is  the more precise our classification will be in this respect we should use as specific relations as possible in order to obtain better thesauri unlike previous research on this topic  we suggest to build a thesaurus for each grammatical relation in particular  we will use surface cases therefore we would have a thesaurus for each surface case this is what we call ''relation-based thesaurus  rbt    
　another aspect that seems to be lacking in the past research is an objective evaluation of the automatically built thesauri all the previous attempts except  pereira 

et al   1  evaluate their results on the basis of subjec tive cntena to what extent is the result consistent with human intuition in this paper we propose an objective evaluation method for automatically built thesauri 
　in the following  we will introduce relation-based thesauri  section 1  and describe the clustering algorithm  section 1  section 1 describes an experiment in which we compared with relation-based thesauri to conventional ones finally section 1 concludes the paper and gives some future research directions 
1 	r e l a t i o n - b a s e d t h e s a u r i 
this paper focuses on building thesauri of nones based on verb-noun relations following the research mentioned in the previous section co-occurrence data is represented b  tuples as shown in the left column of figure 1 where n1  and v1 denote nouns and verbs respectivelv while t1 denotes grammatical relations such as subject 
object and so forth 
	fig 1 	thesaurus construction from tuples 
　past research has not focused on using grammatical relations  t1  for example hindle used subject and object relations but did not distinguish between them when calculating the distance between nouns  hindle  1  hirschman et al used other grammatical relations than subject and object in order to build word classes actually they used various relations simulta neously  hirschman tt al 1  on the other hand 
pereira et al used only the object relation  pcreira et al 1  unlike all these attempts  we will focus on difference of relations and propose to build a thesaurus for each relation this approach is based on the fact that a noun behaves differently depending on its grammatical role take the following examples 
 a  john studied english at the university 
 b  mary worked till late at her office 
 c  the university stated that they would raise the tu it ion fee 
 d  the mavor stated that he would raise taxes 
with regard to taking a locatije role  derived from ' at  phrase in  a  and  b    'university  and 'office   behave similarly  hence thev would be classified into the same word class on the other hand with regard to being a subject of verb  state   in  c  and  d     university  behaves like  mayor  with this respect   university  and  mayor  would be classified into the same class it should be noted that the transitivity does not always hold beyond the relations in the above example  it is questionable if we could classify 'office  and  mavor  into the same class the bases of the similarity between 'university  and  office' and that between  university  and  mayor  are different 
　in conventional thesauri  university and  mayor' would be placed in the different classes university  would be some kind of organization and mayor  some kind of human however they could be put in the same class  namely as being a subject of a certain set of verbs 
　figure 1 shows our approach while figure 1 illustrates the conventional ones the tuples arc divided into the subsets with respect to their tt latum   thesaurus is built from each set of these tuples 
1 	h i e r a r c h i c a l b a y e s i a n c l u s t e r i n g 
wc adopt a hierarchical clustering algorithm that attempts to maximize the bavesian posterior probability at each step of merge this algorithm has been introduced b  iyvayama and toknnaga  iwavama and toku naga 1  and is referred to as hi  rarchical bay at an clustering  hbc  in this set lion wc briefly icview the outline of the algorithm 
　given a set of training data d hbc constructs the set of clusters c that has the locally maximum value of the posterior probability p{c d  this is a general form of the well known maximum likelihood estimation  estimating the most likely model  i e   set of clusters  for a given set of training data 
	fig 1 	hierarchical bavesian clustering 
　like most agglomerative clustering algorithms  cor mack  1 anderberg  1  griffiths et al 1 willett 1  hbc constructs a cluster hierarchy  also called   dendrogram'  from bottom up by merging two clusters at a time at the beginning  the bottom level m a dendrogram  each datum belongs to a cluster whose only member is the datum itself for even pair of clusteis  hbc calculates the posterior probability after merging the pair  selecting the pair with the highest probability; to see the details of this merge process con sider a merge step k+1  1   k   v- 1  bv the step k + 
takenobu makoto  and h1umi 



	 1  
since each p{d  appears in ever} estimation of p c d  onlv once  this can be excluded for maximization purpose other probabilities p   =v d  p   =v c   and p{  = v  are estimated from given data by using the simplest estimation as below 
  p v  = t d  relative frequency of a verb v cooccurnng with a noun d 
  p{  = v c  relative frequency. of a verb v cooccurring with nouns in cluster c 
  p v  = v  relative frequent; of a verb v appearing in the whole training data 
1 evaluation 
this section describes an experiment to evaluate rbts compared with a thesaurus constructed without consul ering grammatical relations 
1 	d a t a and preprocessing 
the data we used for evaluation is a subset of the edr collocation dictionary of japanese  edr 1  this dictionary contains 1 1 tuples of words with various relations the tuples are extracted from newspa per articles and magazines the words in the tuples are tagged with concept identifiers which are the pointers to the edr concept dictionary this dictionary describes thus collocations of word senses this is a nice feature for clustering words because we can avoid the problems caused bv polvsemy  fukumoto and tsujii 1 
　from the dictionary we extracted the tuples that fulfilled the following three conditions 
  describing verb and noun relations 
  the surface case of the nouns are either qa  nom    wo   ace  m  dat/loc  or  de   inst/loc  1 
  both verb and noun are tagged with concept identi-fiers that is words are semantically disambiguated 
we excluded the tuples in which the surface cases changed because of the passive or causative constructions as a result we obtained 1 tuples due to the scarceness of the data and the limitation of our computational resources we chose 1 nouns on the basis of their frequencies and used only those tuples containing them these 1 nouns were used for clustering ta ble 1 shows the number of tuples which include these 1 nouns for each surface case 
	table 1 	number of tuples 

　we conducted 1-fold cross validation with this data namely  one half of the data was used as training set for building clusters and the other half was held out as test data  and vice versa since we are considering these four surface cases we built four rbts from the training data  and one thesaurus from all the training data without taking into account surface cases we refer to the last one as ''relation-neglected thesaurus  rnt '' the rnt is the baseline of the rbts 
1 	evaluation m e t h o d 
the thesauri are evaluated bv the following procedure 
for a each verb in the test data  a set of nouns that cooccur with the verb is associated with the verb this set of nouns is referred to as answers set of the verb we use a threshold of the number of nouns in an answer set onlv the verbs that have more nouns than the threshold in their answer set are used as test cases in the exper ment  the threshold was set to 1 the number  1  does not have anv special meaning  it is- simply chosen as a compromise between accuracy and reliability of the evaluation greater threshold decreases the number of test cases therefore it degrades the reliablity of the eval nation on the other hand lower threshold spuriously decreases the accuracy of each test case 
　each verb has an answer set for each surface case thus we have four test set of verbs corresponding to each surface case as we can see from the algorithm described in section 1  the hbc algorithm generates a binary tree  dendrogram  in which each leaf is a noun we traverse the tree from top to bottom for each verb in the test data  and at each node we calculate recall and precision from the answer set of the verb and the set of nouns under the current node we have an option at each nonterminal node the child node which dominates more nouns that are in the answer set is chosen figure 1 is an example of such a tree traversal 

	fig 1 	traverse of the thesaurus 
　in figure 1 ' o  denotes a noun that is included in the answer set of the verb while 'x ' denotes a noun that is not we call the former correct noun and the latter incorrect noun recall and precision at each node are calculated as follows 
		 1  
taken1bu  mak1  and hozumi 


in the above example  the answer set of the verb includes four  correct  nouns recall and precision of this verb are calculated as shown in the right column of figure 1 as we move down the tree the recall decreases monotonicallv since the number of the nouns dominated by the current node decrease on the other hand  the precision increases as we move down the tree if we aggregate the nouns having a similar tendency to co-occur with verbs  the recall will remain at the high level therefore  we can evaluate the quality of the thesaurus in terms of the recall-precision curve for example  suppose we use the thesaurus for the constraints of selectional restriction for this purpose we also need case frames of verbs in which a node  or a set of nodes of the thesaurus is described as the case fillers1 if the thesaurus has the desirable property described above  the number of nodes to be described as a case filler would decrease this is precisely what we want because unlike the examplebased framework one of the motivations of using thesaurus is to minimize the description of knowledge in the example-based framework all the individual words that co-occur with a  erb would be desmbed as case fillers  kurohashi and nagao 1  
1 	r e s u l t a n d discussion 
for all combinations of the four test bets corresponding to each surface case and the five thesauri  four rbts and one rnt   the recall-precision curves were calcu iated as mentioned before  recall and precision have mutual exclusive properties in order to summarize their balance  we used a breakeven point which is defined as the point at which the recall and precision become equal on a recall-precision curve  lewis 1  the greater breakeven value means the better the recall-precision curves for each test case the breakeven point was calculated bv linear interpolation  and for each combination of the test set and the thesaurus  the mean average of breakeven points was also calculated table 1 summarizes the mean breakeven points of every combination 
	table 1 	breakeven point  %  

   table 1 shows that for all surface cases  the r b t marks the best breakeven value with the test set of the 
     assigning thesaurus nodes to a case filler is also an important issue and several attempts have been made  grishman et al 1  grishman and sterling  1  the automatic method for acquiring case frames should be discussed together with the automatic thesaurus construction however  this issue is beyond the scope of this paper we are currently working on a paper that deals with this problem corresponding surface case the diagonal values in the table are the best in the columns they are also superior to rnts this result supports our claim that we would be able to obtain better thesauri bv considering surface cases 
   the breakeven values in the table are all verv poor in the absolute sense the main reason for this is that we derived the answer set only from the co-occurrence data there might be many nouns that would actually be a case filler of a verb but do not belong to the answer set of the verb in. order to solve this problem  we need to check manually which noun can really be the case filler of the verb for all nouns in the thesaurus however  this is time consuming and introduces subjective critena  therefore we used onlv the observed data thus the values in table 1 should be interpreted in the relative sense not in the absolute one 
   as for the surface cases ''wo  and   m   the difference between r b t and r n t is not really significant the reason for this is that for these two surface cases the distribution of noun frequencies in the tuples for r b t is very similar to that for r n t in other words  many frequently occurring nouns in the tuples of these two surface cases do not appear in the tuples of other surface cases note that the tuples used for creating each r b t is a proper subset of the tuples used for creating the r n t we would not suffer from this problem if more data were available  or we chose the target nouns- based on the frequencv in the tuples of each surface case in the latter case however  we would be able to compare a r b t to the r n t   but not to the other rbts because the set of nouns to be clustered would be different for each surface case 
1 	c o n c l u d i n g r e m a r k s 
we have proposed to build thesauri on the basis of gram matical relation we have conducted a preliminary experiment with 1 tuples of verb  noun and surface cases of japanese the results are quite promising we have also proposed a method that allows to evaluate thesauri objectively 
　we started from the assumption that surface cases are independent from each other however such an assumption is questionable we also need to evaluate rbts in the context of real world settings  such as parsing  gr ishman et al 1  for this purpose  we need case frames whose case fillers are described in terms of the r b t nodes we should explore methods that can automatically acquire case frames  gnshman et al 1 grishman and sterling  1  as well 
   furthermore  we have used e d r collocation dictionary  in which the words are already semantically disambiguated obviouslv we can not expect to find such pure data if we work on large scale last but not least  we have to evaluate the quality of r b t that are built from raw data  text  

r e f e r e n c e s 
 acl  1  acl '1 proceedings of the slst annual meeting of the association for computational linguis tics  1 
 allen  1  j allen natural language understanding 
the benjamin/cumnungs publishing company  inc 
1 
 anderberg  1  m r anderberg cluster analysis for applications academic press  1 
 biber  1  d biber using register-diversified corpora for general language studies computational lin gutstics  1  1 1 
 chapman  1  l r chapman roget's international thesaurus  fourth edition  harper & row  1 
 charniak 1  e charniak statistical language learning the mit press  1 
 col 1  coling 1 proceedings of the 1th international conference on computational linguistics  
1 
 cormack 1  r m cormack   review of classification journal of the royal statistical society 1-
1 
 edr 1  edr collocation dictionary technical report tr-1  japan electronic dictionarv research institute  1 
 fukumoto and tsujn 1  f fukumoto and j tsujn automatic recognition of verbal pohysemy in 
proceedings of the 1th international conference on computational linguistics volume 1 pages 1 coling '1  1 
 gnffiths et al 1  a griffiths  l a. robinson  and p willett hierarchic agglomerative clustering methods for automatic document classification journal of documentation  1  1 1 
 gnshman and sterling  1  r grishman and j sterling acquisition of selection patterns in proceed ings of the 1th international conference on compu tational linguistic  pages 1 
 gnshman et al 1g  r grishman l hirschman and n t nhan discover} procedures for sublanguage selectional patterns initial experiments computational linguistics  1  1  1 
 halhday and hassan 1  m a r halhdav and r hassan language context and text aspects of language in a social semiotic perspective deakin universitv press  1 
 hatzivassiloglou and mcreown  1  
v hatzivassiloglou and r r mcreown towards the automatic identification of adjectnal according to meaning in proceedings of the slst annual meeting of 
the association for computational linguistics   pages 1 
 hayaahi  1  o hayashi 	bunruigoihyo syueisyuppan 1 
 hindle  1  d hindle noun classification from predicate-argument structures in proceedings of the 1th annual meeting of the association for computational linguistics  pages 1 acl '1  1 
 hirschman et al  1  l hirschman r gnshman and n sager grammatically based automatic word class formation information processing & management  1-1 
 iwavama and tokunaga 1  m iwajama and t tokunaga a probabilistic model for text categorization baaed on a single random variable with mul tiple values in proceedings of 1 th conference on ap plied natural language processing  anlp 1  pages 
1 1 
 iwavama and tokunaga 1  m iwavama and t tokunaga hierarchical bayesian clustering for automatic text classification in proceedings of the in-
ternational joint conference on artificial intelligence 
 to appear  1 
 rurohashi and nagao 1  s kurohashi and m nagao structural disambiguation in japanese by eval uating case structures based on examples in a case frame dictionary in proceedings of the int  rnattonal workshop on parsing technologies pages 1 
iwpt '1  1 
 lewis 1  d d lew is an evaluation of phrasal and clustered representations of a text categorization task in annual international acm sigir conference on research and development in information retrieval pages 1 1 
 miller et al  1  g a miller r bechwith c fellbaum  d gross  r miller  and r tengi five papers on wordnet technical report csl report 1  cognitive science laboratory princeton university 1 revised version 
 nagao and kurohashi 1  m nagao and s rurohashi dvnamic programming method for analyzing conjunctive structures in japanese in proceedings of the 1th international conference on computational linguistics  pages 1 
 pereira et al  1  f pereira  n tishbv and l lee distributional clustering of english words in proceed ings of the 1st annual meettinq of tht association for computational linguistics  pages 1 
 sato and nagao  1  s sato and m nagao toward memory-based translation in proceedings of the 1th international conference on computational linguistics volume 1  pages 1 coling 1 
 sekine et al  1  s sekine j j carroll s ananiadou  and j tsujn automatic leaning for semantic collocation in proceedings of the 1rd conference on applied natural language processing  anlp 1  pages 1 1 
 willett  1  p willett recent trends in hierarchic document clustering a critical review information 
processing & management  1  1 1 
taken1bu  mak1  and hozumi 
1 	natural language 

1 	natural language 

1 	natural language 

1 

1 

1 

	1 	natural language 

	1 	natural language 

	1 	natural language 

1 

1 

1 

1 	natural language 

1 	natural language 

1 	natural language 

1 

1 

1 

