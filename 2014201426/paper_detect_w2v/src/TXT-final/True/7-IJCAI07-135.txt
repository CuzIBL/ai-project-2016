
in the past ten years  boosting has become a major field of machine learning and classification. this paper brings contributions to its theory and algorithms. we first unify a well-known top-down decision tree induction algorithm due to  kearns and mansour  1   and discrete adaboost  freund and schapire  1   as two versions of a same higher-level boosting algorithm. it may be used as the basic building block to devise simple provable boosting algorithms for complex classifiers. we provide one example: the first boosting algorithm for oblique decision trees  an algorithm which turns out to be simpler  faster and significantly more accurate than previous approaches.
1 introduction
loosely speaking  a boosting algorithm repeatedly trains moderately accurate learners from which it gets its weak hypotheses  and combines them to output a strong classifier which boosts the accuracy to arbitrary high levels  kearns and valiant  1 . a pioneering paper  schapire  1  proved the existence of such boosting algorithms  and another one drew the roots of their most popular representative: adaboost  that builds a linear combination of the predictions of the weak hypotheses  freund and schapire  1 . another paper due to  kearns and mansour  1  proved that some of the most popular top-down decision tree induction algorithms are also boosting algorithmsin disguise  breimanet al.  1; quinlan  1 . these two kinds of algorithms are outwardly different from each other: while adaboost repeatedly modifies weights on the training examples and directly minimizes a convex exponential loss  top-down decision tree induction algorithms do not modify weights on the examples  and they minimize the expectation of a concave  so-called permissible function  kearns and mansour  1 .
¡¡this explains why the starting point of this paper is a quite surprising result  as we prove that these two kinds of algorithms are in fact the same algorithm  whose induction is performed on two differentclassifier  graphs . the apparentperceptual differences in the algorithms stem only from different structural properties of the classifiers. this suggests a generic induction scheme which gives  for many classifiers that meet a simple assumption  a corresponding boosting algorithm. using a recent result due to  nock and nielsen  1   this algorithm uses real-valued weak hypotheses  but it does not face the repetitive minimization of the exponential loss faced by previous real boosting algorithms  friedman et al.  1; schapire and singer  1 . when the classifier is an oblique decision tree  the algorithm obtained has three key features: it is fast  simple  and the first provable boosting algorithm that fully exploits this class. this is important  as the problem of inducing oblique decision trees has a longstanding history  and the algorithms available so far are either time consuming or complex to handle formally  breiman et al.  1; cantu¡ä-paz and kamath  1; heath et al.  1; murthy et al.  1; shawe-taylor and cristianini  1 . the following section presents definitions; it is followed by a section on our theoretical results  and a section that discusses them and gives our boosting algorithm for oblique decision trees; two last sections discuss experiments  and conclude. for the sake of clarity  proofs have been postponed to an appendix.
1 definitions and problem statement
bold-faced variables such as w and x  represent vectors. unless otherwise stated  sets are represented by calligraphic upper-case alphabets  e.g. x  and  unless explicitely stated  enumerated following their lower-case  such as {xi : i = 1 ...} for vector sets  and {xi : i = 1 ...} for other sets. consider the following supervised learning setting. let x denote a domain of observations of dimension n  such as rn  {1}n  etc. . we suppose a set s of |s| = m examples  with |.| the cardinal notation  defined as
x ¡Á { 1 +1} : i = 1 ... m}  onto which a discrete distribution w1 is known.  +1  is called the positive class  and  -1  the negative class. our primary objective is related to building an accurate classifier  or hypothesis  h : x ¡ú r. the goodness of fit of h on s may be evaluated by two quantities:
	 	 1 
 1 
input: s w1; for t = 1 ... t ht ¡û tree s wt ; ¦Át ¡û argmin¦Á¡Êr ewt exp  y¦Áht x   ; for i = 1 ... m wt+1 i ¡û wt i exp  ¦Átyiht xi  /zt;input: s w1; for t = 1 ... t let  ¡Ê leaves ht 1  containing examples of both classes; ht ¡û stump  ;
;input: s w1; for t = 1 ... t let  ¡Ê leaves ht 1  containing examples of both classes; ht ¡û stumpls  ;
;adaboost + treestopdown dt  cart  c1 topdown odt  oc1  sadt figure 1: an abstraction of 1 core greedy procedures of some popular induction algorithms  see text for references and details .here  sign a  = +1 iff a ¡Ý 1 and  1 otherwise  1 .  is the 1 indicator variable  and e.   is the expectation.  1  is the conventional empirical risk  and  1  is an upperbound  the exponential loss  schapire and singer  1 . we are interested in the stagewise  greedy fitting of h to s  from scratch. a large number of classes of classifiers have popular induction algorithms whose core procedure follows this scheme  including decision trees  dt   breiman et al.  1; quinlan  1   branching programs  bp   mansour and mcallester  1  and linear separators  ls   freund and schapire  1; nock and nielsen  1 . virtually all these procedures share another common-point: the components that are used to grow h are also classifiers. these are the dts for adaboost with trees  the internal node splits for the dts and bps  akin to decision stumps   etc.
¡¡now  given some fixed class of classifiers in which we fit h  here is the problem statement. a weak learner  wl  is assumed to be available  which can be queried with a sample s and  discrete  distribution w on s; the assumption of its existence is called a weak learning assumption  wla . it returns in polynomial time1 a classifier h on which only the following can be assumed: 
1   ¦Ã for some ¦Ã   1  schapire and singer  1 . given   is it possible to build in polynomial time some h with  after having queried t times wl for classifiers h1 h1 ... ht  for some t   1   provided additional assumptions are made for its generalization abilities  see section  discussion  below   this algorithm is called a boosting algorithm  freund and schapire  1; kearns and valiant  1 .
1 unifying boosting properties
for the sake of clarity  we now plug t in subscript on
h; an element of ls is a classifier ht with ht x  =
  where ¦Át ¡Ê r is a leveraging coefficient that may be interpreted as a confidence in ht. an element of dt is a rooted directed tree in which each internal node supports a single boolean test over some observation variable  and each leaf is labeled by a real. the classification of some observation proceeds by following  from the root  the path whose tests it satisfies  until it reaches a leaf which gives its class. figure 1  left  presents an example of dt  n = 1   where v1 v1 are boolean description variables. finally  oblique decision trees  odt  generalizes dt  as linear combinations of variables are authorized for the internal nodes  which allows splits that are oblique instead of just axis-parallel  breiman et al.  1 .
¡¡figure 1 gives an abstract view of some of the most popular induction algorithms  or at least of their core procedure which induces a large classifier: adaboost with trees  freund and schapire  1; schapire and singer  1   c1  quinlan  1   cart  breiman et al.  1   oc1  murthy et al.  1   sadt  heath et al.  1 ; the first algorithm for the induction of odt dates back to the mid-eighties; it was a descendant of cart  breiman et al.  1 . most of the induction algorithms for dt or odt integrate a pruning stage; here  we are only interested in the greedy induction scheme. wl has various forms: it induces a decision tree on the whole sample s and on a distribution which is repeatedly modified for adaboost; it induces a decision tree with a single internal node  a stump  on the subset of s that reaches leaf   noted s  and on a distribution which is not modified for cart and c1; it replaces this ordinary  axis-parallel stump  by a linear separator stump  also called stump for the sake of simplicity  for oc1 and sadt. the choice of a split for dt and odt relies on the repetitive minimization of an  impurity  criterion which is the expectation  over the leaves of the current tree h  of a so-called permissible function  kearns and mansour  1 :
leaves 
here  is the total weight of s in w1 and w.+ is its weight further restricted to the positive class. in the expectation  the weight of a leaf. the permissible function f :  1  ¡ú  1  has to be concave  symmetric around 1  and with f 1  = f 1  = 1 and f 1  = 1. examples of permissible functions are f z  = 1z 1   z  for gini index  breiman et al.  1   f z  =  z logz    1   z log 1 z  for the entropy  quinlan  1   log base-1   or even for the optimal choice of  kearns and mansour  1 . remark that we have ¦Åimp h w1 f  ¡Ý ¦Åimp h w1min{z 1   z}  = 1¦Å h w1   for any permissible f  and so minimizing  1  amount to minimizing the empirical risk of h as well. the reason why this is a better choice than focusing directly on the empirical risk comes from the concavity of the permissible function  kearns and mansour  1 . though seemingly very different from each other  adaboost and dt induction algorithms from the cart-family were independently proven to be boosting algorithms  freund and schapire  1; kearns and mansour  1; schapire and singer  1 . so far  no such formal

figure 1: a dt with 1 leaves and 1 internal nodes  left  and an equivalent representation which fits to  a   right .
boosting algorithm or proof exist for odt that would take full advantage of their structure combining oblique stumps and decision trees. for this objective  let us shift to a more general standpoint on our problem  and address the following question: what kind of classifiers may be used to solve it   consider the following assumption  that ht represents some kind of local linear classifier:
 assumption a   t   1  denote ht = {h1 h1 ... ht} the set of outputs of wl; then  there exists a function which maps any observation to a non-empty subset of ht  ght : x ¡ú 1ht { }.  x ¡Ê x  ght x  is required to be computable polynomially in n and the size of ht; it describes the classification function of ht  in the following way:
 .
by means of words  the classification of ht is obtained by summing the outputs of some elements of ht. many classifiers satisfy  a   such as decisition trees  decision lists  linear separators  etc.
definition 1 the decisiongraph of ht  on x  is an oriented graph g =  ht e ; an arc and there
exists some x ¡Ê x such that and no with.
remark that g is acyclic  and ght maps each observation of x to some path of g. we also define p as representing the set of paths of g that is mapped from x by ght :
	p	=	{p   ht :  x ¡Ê x p = ght x } .
 the simplest case of g is obtained when ght x  = ht  x ¡Ê x: ht is a linear separator  and g a single path from h1 to ht. examples of classes of classifiers different from linear separators and that fit to  a  include dt and odt. in this case  g is a tree and weak hypotheses are in fact constant predictions on the tree nodes. figure 1  right  displays the way we can represent a dt so as to meet  a . remark that there exists many possible equivalent transformations of a dt; the way we induce ht shall favor a single one out of all  as follows. we define two types of subsets of s  with p ¡Ê p and 1 ¡Ü t ¡Ü t + 1 :
	 	 1 
 1 
input: s w1; for t = 1 ... t
compute st;
ht ¡û wl st wt ;
for;1si ¡Ê s stfor t = 1 ... t;genericgreedy  g1 figure 1: the generic greedy induction of ht. the computation of st depends on the decision graph of ht.
we also extend the weight notation  and let 
 and  with p ¡Ê p and
 . we also define
r  the maximal absolute value of ht over st. after  nock and nielsen  1   we define two notions of margins: the first is the normalized margin ht over s:

with the help of this definition  figure 1 presents the generic greedy induction of ht. remark that when ht is a linear separator  g1 is the adaboostr boosting algorithm of  nock and nielsen  1 . the second margin definition is the margin of ht on example  nock and nielsen  1 :

this margin comes to mind from the relationships between boosting and the logistic prediction ht x  = log pr y = +1|x /pr y =  1|x    friedman et al.  1 . in this case   1  becomes
 1|x    whose expectation is the normalized margin  1  of the bayesian prediction in   1 +1   also called gentle logistic approximation  friedman et al.  1  . following
 nock and nielsen  1   we define the margin error of ht   ¦È ¡Ê   1 +1  :
           ¦Íw1 ht ¦È	=	ew1 ¦Ít s ¡Ü¦È    	 1  and we have ¦Å ht w1  ¡Ü ¦Íw1 ht 1.	thus  minimizing ¦Íw1 ht 1 would amount to minimize ¦Å ht w1  as well. the following theorem shows much more  as under some weak conditions  we can show that ¦Íw1 ht ¦È is minimized for all values of ¦È ¡Ê   1 +1   and not only for ¦È = 1 .
theorem 1 after t ¡Ý 1 queries of wl  the classifier obtained  ht  satisfies:

for any ¦È ¡Ê   1 +1 .
 proof in appendix . to read theorem 1  consider the following wla for real-valued weak hypotheses  borrowed from  nock and nielsen  1 :
 wla  |¦Ìt| ¡Ý ¦Ã  for some ¦Ã   1   t ¡Ý 1 .
under the wla  theorem 1 states that ¦Íw1 ht ¦È ¡Ü k¦È exp  minp¡Êp |p|¦Ã1   with k¦È constant whenever ¦È is a constant ¡Ê   1 +1 . in other words  provided the induction in g1 is performed so as to keep paths with roughly equal size  such as by a breadth-first induction of the decision graph  the margin error is guaranteed to decrease exponentially fast. to see how g1 fits to tree-shaped decision graphs  consider the following assumption:
 assumption b   i  each ht ¡Ê ht is a constant  and  ii  g is a rooted tree.
assumption  b  basically restricts ht to be a tree of any arity  still in which any kind of classifier may be used to split the internal nodes. as in  1   we use notation w.+/  as the index' weight for class  +1  or  -1   respectively.
theorem 1 suppose that  a  and  b  hold. then:
	  	 1 
 x ¡Ê x and ht is the leaf of ght x . furthermore   1  simplifies as:
 proof in appendix .
1 discussion
1 kearns and mansour's algorithm  adaboost and g1
the similarity between  1  and  1  with is immediate  and quite surprising as it shows the identity between a convex loss and the expectation of a concave loss. however  this is not a coincidence. indeed  theorem 1 shows a rather surprising result: the choice of the weak hypotheses does not impact at all on ht  see  1  . when  a  and  b  hold  the only way to modify ht is thus through its decision graph  i.e. on the choice of the splits of the tree. there is a simple way to choose them  which is to do the same thing as the most popular ls boosting algorithms  friedman et al.  1; schapire and singer  1 : repeatedly minimize the exponential loss in  1 . because of theorem 1  this amounts to the minimization of the impurity criterion in  1  with . this is exactly the dt induction algorithm proposed by  kearns and mansour  1  that meets the representation optimal bound.
¡¡on the other hand  when ht is a linear separator  there is no influence of the decision graph on the induction of ht as it is a single path from h1 to ht. the only way to modify ht is thus through the choice of the weak hypotheses. suppose that each weak hypothesis has output restricted to the set of classes  { 1 +1}. in this case  ¦Át =  1 ln  1   ¦Å ht wt  /¦Å ht wt   = argminr ew1 exp  yht x     figure 1   and g1 matches exactly discrete adaboost  freund and schapire  1 .
¡¡thus  decision trees and linear separators are somehow extremal classifiers with respect to g1. finally  when ht is a linear separator without restriction on the weak hypotheses  g1 specializes to adaboostr  nock and nielsen  1 .
1 all boosting algorithms
 in the original boosting setting  the examples are drawn independently according to some unknown but fixed distribution d over x  and the goal is to minimize the true risk ¦Å ht d  with high probability  i.e. we basically wish that with probability ¡Ý 1   ¦Ä over the sampling of s  freund and schapire  1; kearns and valiant  1 . two sufficient conditions for a polynomial time induction algorithm to satisfy this constraint are  i  return ht with ¦Å ht w1  = 1  and  ii  prove that structural parameters of the class of classifiers to which ht belongs satisfy particular bounds  freund and schapire  1; shawe-taylor and cristianini  1 . theorem 1 is enough to prove that  i  holds under fairly general conditions for algorithm g1 in figure 1 provided wla holds. for example  iterations for ls and for dt are enough to have from theorem 1. fixingeasily yields  i . the bound for ls is the same as adaboost  discrete or real   schapire and singer  1   while that for dt improves upon the exponent constant of  kearns and mansour  1 . finally   ii  is either immediate  or follows from mild assumptions on g and wl. as a simple matter of fact   i  and  ii  also hold when inducing odt with g1.
1 recursive boosting and oblique decision trees
the preceeding subsections suggest that g1 could be used not only to build ht  but also as the core procedure for wl. for example  it comes from theorem 1 that adaboost + trees without pruning  figure 1  is equivalent to g1 growing linear separators  in which wl is g1 growing a decision tree  in which wl returns any constant weak hypothesis. in the general case  we would obtain a recursive/composite design of the  master  g1  via g1 itself  and the recursion would end until we reach a wl that can afford exhaustive search for simple classifiers  e.g. axis-parallel stumps  constant classifiers  etc.   instead of calling again g1. however  g1 can also be used to build the decision graph of ht  in the same recursive fashion. consider for example the class odt. the internal nodes' splits are local classifiers from ls that decide the path based on the sign of their output  or equivalently  on the class they would give. this suggest to build the tree with
g1 on both the linear separators in the internal nodes of ht  to split leaf   where the linear separator uses ordinary decision stumps   and on the tree shape as well. call boostodt this odt induction algorithm. it turns out that it brings a boosting algorithm  that takes full advantage of the odt structure. however  this time  it is enough to assume the wla one level deeper  i.e. only for the stumps of the linear separators  and not for the splits of the oblique decision tree.
theorem 1 boostodt is a boosting algorithm.

figure 1: margin distributions for boostodt  t1 = 1  on domain xd1  with 1%  left   1%  center  and 1% class noise  right . stair-shaped  bold plain  curves are the theoretical margins for the logistic model  see text .
the proof  omitted due to the lack of space  builds upon theorem  1  plus lemma 1 in  mansour and mcallester  1   and structural arguments in  freund and schapire  1; shawe-taylor and cristianini  1 . the proof emphasizes the relative importance of sizes: suppose that each linear separator contains the same number of weak hypotheses  t1   and the tree is complete with t1 internal nodes; then  having

is enough to have. from an experimental standpoint  this suggests to build trees with.
1 experiments
we have performed comparisons on a testbed of 1 domains with two classes  most of which can be found on the uci repository of ml databases  blake et al.  1 . comparisons are performed via ten-fold stratified cross-validation  against oc1 and adaboost in which the weak learner is c1 unpruned. boostodt was ran for t1 ¡Ê {1}  the weak hypotheses of the linear separators are decision stumps  and t1 = 1. to make fair comparisons  we ran adaboost for a total number of t = 1 boosting iterations. this brings fair comparisons  as an observation is classified by 1 nodes  including leaves  in boostodt  and 1 unpruned trees in adaboost. before looking at the results  boostodt has proven to be much faster than oc1 in practice  ten to hundred times faster . oc1's time complexity is o nm1 logm   murthy et al.  1   without guarantee on its result  while boostodt's is o nm  under the wla  with the guarantee to reach empirical consistency in this case. complexity reductions have almost the same order of magnitude with respect to svm based induction of odt  shawe-taylorand cristianini  1 . table 1 summarizes the results obtained. with rejection probabilities p ranging from less than .1 to less than .1 for the hypothesis h1 that boostodt does not perform better  the four sign tests comparing both runs of boostodt to its opponentsare enough to display its better performances  and this is confirmed by student paired t-tests. there is more: we can tell from simulated domains that boostodt performs as better as the domain gets harder. it is the case for the monks domains  and the ledeven domains. boostodt is indeed beaten by both opponents on ledeven  while it beats both on ledeven+1  =ledeven +1 irrelevant variables .
¡¡looking at these simulated domains  we have drilled down the results of boostodt. using  1   we haveplotted on figure
domainboostodtoc1adaboostt1 =1t1 =1+c1adult-strech1111breast-cancer1111breast-cancer-w.1111bupa1111colic1111colic.orig1111credit-a1111credit-g1111diabetes1111hepatitis1111ionosphere1111kr-vs-kp1111labor1111ledeven1111ledeven+1.1.1.1.1monks1.1.1.1.1monks1.1.1.1.1monks1.1.1.1.1mushroom1111parity1111sick1111sonar1111vote1111xd1.1.1.1.1yellow-small1111#wins  t1 =1 1/1 1 1 #wins  t1 =1 1/1 1 1 table 1: results on 1 domains. for each domain  bold faces indicate the lowest errors. in each row  #wins  t1 = z    bold faces denote the number of times the corresponding algorithm in column is the best over three columns: boostodt t1 = z   oc1 and adaboost+c1  z ¡Ê {1} . furthermore  the four numbers in parentheses in each row are the number of significant wins  student paired t-test  p = .1   for boostodt vs oc1  boostodt vs adaboost+c1  and oc1 vs boostodt  adaboost+c1 vs boostodt  from left to right .
1 its margin error curves on domain xd1 with variable class noise  see e.g.  nock and nielsen  1  for a description of the domain   averaged over the test folds  nock and nielsen  1 . the margin curve obtained is compared to that of the logistic prediction of  friedman et al.  1   which can be computed exactly. the approximation of the logistic model by boostodt is quite remarkable. indeed  its margin curves display the single stair-shape of a theoretical logistic model for a domain xd1 with 1% additional class noise  uniformly distributed among the odt leaves.
1 conclusion
perhaps one main contribution of this paper is to show that formal boosting is within reach using the same unified algorithm  for a wide variety of formalisms not restricted to the most popular included in this paper  such as decision lists  rivest  1   simple rules  nock  1   etc.  . another contribution  quite surprising  is to show that a boosting algorithm follows immediately even for complex combinations of these formalisms  such as linear combinations of oblique decision trees  decision trees in which splits are decided by decision lists  etc. this is crucial  as our last contribution  the first boosting algorithm for the class of oblique decision trees  contrasts in simplicity with respect to previous approaches on inducing oblique decision trees. in future works  we plan to evaluate the experimental and theoretical potentials of these boosting algorithms for these other formalisms.
acknowledgments and code availability
r. nock gratefully thanks sony cs labs tokyo for a visiting grant during which this work was started. resources related to boostodt  including related java classes for the weka platform  can be obtained from the authors.
