
fully decentralized algorithms for distributed constraint optimization often require excessive amounts of communication when applied to complex problems. the optapo algorithm of  mailler and lesser  1  uses a strategy of partial centralization to mitigate this problem.
we introduce pc-dpop  a new partial centralization technique  based on the dpop algorithm of  petcu and faltings  1 . pcdpop provides better control over what parts of the problem are centralized and allows this centralization to be optimal with respect to the chosen communication structure.
unlike optapo  pc-dpop allows for a priory  exact predictions about privacy loss  communication  memory and computational requirements on all nodes and links in the network. upper bounds on communication and memory requirements can be specified.
we also report strong efficiency gains over optapo in experiments on three problem domains.
1 introduction
constraint satisfaction and optimization are powerful paradigms that model a large range of tasks like scheduling  planning  optimal process control  etc. traditionally  such problems were gathered into a single place  and a centralized algorithm was applied in order to find a solution. however  problems are sometimes naturally distributed  so distributed constraint satisfaction  discsp  was formalized by yokoo et al. in  yokoo et al.  1 . here  the problem is divided between a set of agents  which have to communicate among themselves to solve it. to address distributed optimization  complete algorithms like adopt  optapo and dpop have been recently introduced.
　adopt  modi et al.  1  is a backtracking based bound propagation mechanism. it operates completely decentralized  and asynchronously. its downside is that it may require a very large number of small messages  thus producing important communication overheads.
　optapo is a hybrid between decentralized and centralized methods. it operates as a cooperative mediation process  where agents designated as mediators centralize parts of the problem in dynamic and asynchronous mediation sessions. message complexity is significantly smaller than adopt's. however  it is possible that several mediators solve overlapping problems  thus needlessly duplicating effort. furthermore  the asynchronous and dynamic nature of the mediation sessions make it impossible to predict what will be centralized where  how much of the problem will be eventually centralized  or how big a computationalburden the mediators have to carry. it has been suggested in  davin and modi  1  that often a handful of nodes centralize most of the problem  and therefore carry out most of the computation.
　dpop  petcu and faltings  1  is a complete algorithm based on dynamic programming which generates only a linear number of messages. however  dpop may produce large messages and require large amounts of memory  up to space exponential in the induced width of the problem. therefore  dpop may be infeasible for problems with high induced width.
　we present pc-dpop  a new hybrid algorithm that is controlled by a parameter k which characterizes the size of the largest message  and the amount of available memory. the algorithm proceeds as normal dpop  except in the dense parts of the problem  width greater than k . these are clustered and centralized in the root of the cluster  which solves them with a centralized algorithm. communication requirements over any link in the network are limited thus to exp k . the linear number of messages is preserved.
　the rest of this paper is structured as follows: section 1 formally describes the optimization problem  and introduces the dpop and optapo algorithms. section 1 introduces the pc-dpop hybrid algorithm  which is evaluated empirically in section 1. section 1 relates pc-dpop to existing work. section 1 briefly discusses privacy  and section 1 concludes.
1 preliminaries
definition 1 a discrete distributed constraint optimization problem  dcop  is a tuple   x d r   such that:
  x = {x1 ... xn} is a set of variables
  d = {d1 ... dn} is a set of finite variable domains
  r = {r1 ... rm} is a set of relations  where a relation ri is any function with the scope  xi1 ，，， xik   ri : di1 〜 .. 〜 dik ★ r  which denotes how much utility is assigned to each possible combination of values of the involved variables. negative amounts mean costs. 1
　in a dcop  each variable and constraint is owned by an agent. a simplifying assumption  yokoo et al.  1  is that each agent controls a virtual agent for each one of the variables xi that it owns. to simplify the notation  we use xi to denote either the variable itself  or its  virtual  agent.
　this is a multiagent instance of the valued csp framework as defined in  schiex et al.  1 . the goal is to find a complete instantiation x  for the variables xi that maximizes the sum of utilities of all individual relations.
　we assume here only unary and binary relations. however  dpop and pc-dpop can easily extend to non-binary constraints   petcu et al.  1  .
1 depth-first search trees  dfs 
pc-dpop works on a dfs traversal of the problem graph.
definition 1  dfs tree  a dfs arrangement of a graph g is a rooted tree with the same nodes and edges as g and the property that adjacent nodes from the original graph fall in the same branch of the tree  e.g. x1 and x1 in figure 1 .
　figure 1 shows an example dfs tree that we shall refer to in the rest of this paper. we distinguish between tree edges  shown as solid lines  e.g. x1   x1   and back edges  shown as dashed lines  e.g. x1   x1  x1   x1 . definition 1  dfs concepts  given a node xi  we define:
  parent pi / children ci: these are the obvious definitions  p1 = x1  c1 = {x1 x1} .
  pseudo-parents ppi	are xi's	ancestors	directly connected to xi through back-edges  pp1 = {x1} .
  pseudo-children pci are xi's descendants directly connected to xi through back-edges  pc1 = {x1} .
  sepi is the separator of xi: ancestors of xi which are directly connected with xi or with descendants of xi  e.g. sep1 = {x1 x1}  sep1 = {x1 x1} . removing the nodes in sepi completely disconnects the subtree rooted at xi from the rest of the problem.
1 optimal asynchronous partial overlay
optimal asynchronous partial overlay  optapo  mailler and lesser  1   is a sound and optimal algorithm for solving dcops that uses dynamic  partial centralization  dpc . conceptually  dpc is a technique that discovers difficult portions of a shared problem through trial and error and centralizes these sub-problems into a mediating agent in order to take advantage of a fast  centralized solver. overall  the protocol exhibits an early  very parallel hill climbing behaviorwhich progressivelytransitions into a more deliberate  controlled search for an optimal solution. in the limit  depending on the difficulty of the problem and the tightness of the interdependences between the variables  one or more agents may end up centralizing the entire problem in order to guarantee that an optimal solution has been found.
1 dpop: dynamic programming optimization
dpop is a distributed version of the bucket elimination scheme from  dechter  1   which works on a dfs. dpop has 1 phases.
　phase 1 - a dfs traversal of the graph is done using a distributed dfs algorithm like in  petcu et al.  1 . the outcome of this step is that all nodes consistently label each other as parent/child or pseudoparent/pseudochild  and edges are identified as tree/back edges. this can be achieved for any graph with a linear number of messages.
　phase 1 - util propagation is a bottom-up process  which starts from the leaves and propagates upwards only through tree edges. the agents send util messages to their parents. the subtree of a node xi can influence the rest of the problem only through xi's separator  sepi. therefore  a message contains the optimal utility obtained in the subtree for each instantiation of sepi. thus  messages are exponential in the separator size  bounded by the induced width .
　phase 1 - value propagation top-down  initiated by the root  when phase 1 has finished. each node determines its optimal value based on the messages from phase 1 and the value message it has received from its parent. then  it sends this value to its children through value messages.
1 pc-dpop k  - partial centralization hybrid
to overcome the space problems of dpop  we introduce the control parameter k that bounds the message dimensionality. this parameter should be chosen s.t. the available memory at each node and the capacity of its link with its parent is greater than dk  where d is the maximal domain size.
　as with dpop  pc-dpop also has 1 phases: a dfs construction phase  an util phase  and a value phase.
1 pc-dpop - util phase
this is where the algorithm is different from dpop. the propagation proceeds as in dpop where possible  and reverts to partial centralization where the width exceeds k:
1. the util propagation starts bottom-up exactly like in dpop  and proceeds as long as the dimensionality of the outgoing util messages is below the threshold k.
1. as soon as a node's outgoing util message has more than k dimensions  centralization begins  see section 1 . the node does not compute its util message  but sends to its parent a relation message that contains the set of relations  arity at most k  that the node would have used as an input for computing the util message. it annotates this message with its id.
1. upon receiving such a relation message  a node tests to see if its separator is smaller than k. the separator is determined as the intersection between the nodes on the root path of the current node and the set of nodes encountered as dimensions in the messages received from its children.

figure 1: a problem graph  a  and a dfs tree  b . in low-width areas the normal util propagation is performed. in high width areas  shaded clusters c1  c1 and c1 in  b   bounded util propagation is used. all messages are of size at most dk. clusters are centralized and solved in the cluster roots  the bold nodes x1 x1 x1 .  if this is the case  the node becomes a cluster root  reconstructs the subproblem from the incoming relation messages and then solves it  see section 1 . then it continues the util propagation as in dpop.
later on  during the value phase  when the node receives the value message from its parent  it retrieves the solution from its local cache and informs nodes in the cluster of their optimal values via value messages.
  otherwise  it cannot become cluster root and has to pass on to its parent all the relevant relations  the ones received from its children and its own   that it would otherwise use to compute its util message.
for details  see section 1.
pc-dpop - centralization
this process occurs in high-width areas of the problem  e.g. the shaded areas in figure 1 . it is initiated by any node that cannot compute and send its util messages because that would exceed the dimensionality limit imposed by k. any such node produces the union of the relations and util messages received from children  and its own relations with its parent/pseudoparents. all these relations can be sent to its parent in a relation message  annotated by appending the node's id to the path.
　on one hand  this ensures the dimensionality limit k is observed  as no relation with arity larger than k is produced or sent over the network. on the other hand  it will allow the cluster root to reconstruct the subproblem that has to be centralized  and enable the use of structure sensitive algorithms like dpop  aobb  etc.
　alternatively  to save bandwidth  avoid overload on cluster root nodes  and also improve privacy  a node can selectively join subsets of these relations/util messages  s.t. the dimensionality of each of the resulting relations is less than k.
the resulting set of relations is then packaged as a relation message  annotated with the node's id  and sent to the parent  see section 1 . this happens as follows:
1. node xi receives all util/relation messages from its children  if any
1. xi forms	the	union ui of	all	relations	in	the
util/relation messages and the relations it has with its parent and pseudoparents
1. xi matches pairs of relations in ui s.t. by joining them the resulting relation will have k dimensions or less  the dimensionality of the resulting relation is the union of the dimensions of the inputs . if the join was successful  remove both inputs from ui  and add the result instead. try until no more joins are possible between relations in ui. this process is linear in the size of ui.
1. the resulting ui set is sent to xi's parent in a relation message
　this process proceeds bottom-up until a node xr with a separator smaller than k is reached. xr declares itself a cluster root and executes the procedures from section 1.
　the result of this phase is that minimal  difficult  highwidth  areas of the problem are identified in a distributed fashion. in these areas the algorithm reverts to partial centralization  by having nodes send to their parents not high dimensional util messages  but lower arity inputs that could be used to generate those util messages.
subproblem reconstruction
let us assume a cluster root node xi has received a set of relations rci from its children. each relation ri （ rci has a set of variables that it involves  scope ri    and the path that it traveled through until it reached xi. xi creates an internal copy of all the nodes found in the scopes of the relations received. then  xi reconstructs the subtree by placing each internal variable xk in its position as follows:
  if xk is an ancestor of xi  then it is identified as such  because xi knows the path from root to itself.
  if xk is a child of xi  then it is identified as such.
  otherwise  it is a descendant of xi. in this case xi deduces its place in the subtree by analyzing backwards the path recorded in the relations involving xk. for example  in figure 1  x1 receives r1   r1 and r1   all annotated with the path x1  x1  x1  x1. x1 is already a child of x1  so it is then easy to see that x1 is x1's child and x1 is x1's child. thus  the whole branch x1   x1   x1   x1 is reconstructed.
　it is interesting to note that this technique allows for identifying different branches as well. consider again x1 that also receives and  all annotated with the path x1   x1   x1   x1. following the same reasoning as before  x1 can infer that x1 is also a child of x1  like x1. therefore  x1 and x1 lie in different branches.
　this makes it possible for a cluster node to reconstruct the subproblem while preserving structural information. this is important because it enables the use of high-performance optimization algorithms that also take advantage of problem structure. examples include aobb  marinescu and dechter  1   a centralized dpop  petcu and faltings  1   etc.
solving centralized subproblems
the centralized solving occurs in the roots of the clusters determined by the high-width areas. in the example of figure 1  such a cluster is the shaded area containing
x1 x1 x1 x1 x1.
　the root of the cluster  e.g. x1  maintains a cache table that has as many locations as there are possible assignments for its separator  in this case dk = d1 locations . as a normal node in dpop  the root also creates a table for the outgoing util message  with as many dimensions as the size of the separator. each location in the cache table directly corresponds to a location in the util message that is associated with a certain instantiation of the separator. the cache table stores the best assignments of the variables in the centralized subproblem that correspond to each instantiation of the separator.
then the process proceeds as follows:
  for each instantiation of sepi  the cluster root solves the corresponding centralized subproblem. the resulting utility and optimal solution are stored in the location of the util message  cache table location  respectively  that correspond to this instantiation.
  when all sepi instantiations have been tried  the util message for the parent contains the optimal utilities for each instantiation of the separator  exactly as in dpop   and the cache table contains the corresponding solutions of the centralized subproblem that yield these optimal utilities.
  the cluster root sends its util message to its parent  and the process continues just like in normal dpop.
　there are multiple possibilities for choosing an optimization algorithm for solving subproblems: a centralized version of dpop  petcu and faltings  1   aobb  marinescu and dechter  1   etc.

algorithm 1: pc-dpop - partial centralization dpop.

pc-dpop x d r k : each agent xi does:
util propagation protocol
1 wait for util/relation messages from all children
1 if sepi ＋ k  can send outgoing utilpi i  then
1 if incoming are all util  normal dpop node  then compute utilpi i as in dpop and send it to pi
1 else  this means xi is the root of a cluster 
1 reconstruct subproblem from received relations
1 solve subproblem for each and store utility inand solution in local cache
1 send utilpi i to pi
1 else  this means xi is part of a cluster 
1 join subsets of incoming util/relation and relations with  p parent with same dimension s.t. for each join  dim join  ＋ k
1 send joins to pi
value propagation xi gets sepi ○ sep i from pi 
1 if xi is cluster root then
1find in cache sol  that corresponds to sep i
1assign self according to sol 
1send sol  to nodes in my cluster via value msgs
1 else continue value phase as in dpop

1 pc-dpop - value phase
the labeling phase has determined the areas where bounded inference must be applied due to excessive width. we will describe in the following the processing to be done in these areas; outside of these  the original value propagation from dpop applies.
　the value message that the root xi of a cluster receives from its parent contains the optimal assignment of all the variables in the separator sepi of xi  and its cluster . then xi can simply retrieve from its cache table the optimal assignment corresponding to this particular instantiation of the separator. this assignment contains its own value  and the values of all the nodes in the cluster. xi can thus inform all the nodes in the cluster what their optimal values are  via value messages . subsequently  the value propagation proceeds as in dpop.
1 pc-dpop - complexity
in low-width areas of the problem pc-dpop behaves exactly as dpop: it generates a linear number of messages that are at most dk in size. in areas where the width exceeds k  the clusters are formed.
theorem 1 pc   dpop k  requires communication o exp k . memory requirements vary from o exp k   to o exp w   depending on the algorithm chosen for solving centralized subproblems  w is the width of the graph .
proof. section 1 shows that whenever the separator of a node is larger than k  that node is included in a cluster. it also shows that within a cluster  util messages with more than k dimensions are never computed or stored  but their input components sent out instead. it can be shown recursively that these components have always less than k dimensions  which proves the first part of the claim.
　assuming that w   k  memory requirements are at least o exp k  . this can easily be seen in the roots of the clusters: they have to store the util messages and the cache tables  both of which are o exp sep = k  .
　within a cluster root  the least memory expensive algorithm would be a search algorithm  e.g. aobb 1   that uses linear memory. the exponential size of the cache table and util message dominates this  so memory overall is o exp k  .
　the most memory intensive option would be to use a centralized version of dpop  that is proved to be exponential in the induced width of the subtree induced by the cluster. overall  this means memory is exponential in the maximal width of any cluster  which is the overall induced width.

1 experimental evaluation
we performed experiments on 1 different problem domains: distributed sensor networks  dsn   graph coloring1  gc   and meeting scheduling  ms . our versions of optapo and pc-dpop used different centralized solvers  so in the interest of fairness  we did not compare their runtimes. instead  we compared the effectiveness of the centralization protocols themselves  using 1 metrics: communication required  see figure 1   and amount of centralization  see figure 1 . figures 1 and 1 present the results on the gc problems.
　the bound k has to be at least as large as the maximal arity of the constraints in the problem; since these problems contain only binary constraints  we ran pc-dpop k  with k between 1 and the width of the problem. as expected  the larger the bound k  the less centralization occurs. however  message size and memory requirements increase.
　dsn the dsn instances are very sparse  and the induced width is 1  so pc   dpop k − 1  always runs as dpop: no centralization  message size is d1 = 1. in contrast  in optapo almost all agents centralize some part of the problem. additionally  in the larger instances some agents centralize up to half the problem.
　meeting scheduling we generated a set of relatively large distributed meeting scheduling problems. the model is as in  maheswaran et al.  1 . briefly  an optimal schedule has to be found for a set of meetings between a set of agents. the problems were large: 1 to 1 agents  and 1 to 1 meetings  yielding large problems with 1 to 1 variables.
 a  maximal size of a centralized problem vs. total problem size

figure 1: graph coloring: centralization.  a  in optapo there is always an agent which centralizes all the problem.  b  in optapo all agents centralize some subproblem.
 b  how many agents centralize subproblems vs. total problem size

figure 1: graph coloring: message exchange.  a  all pcdpop variants use a linear # of messages. ;  b  total information exchange  bytes  is much lower for pc-dpops.
the larger problems were also denser  therefore even more difficult  induced width from 1 to 1 .
　optapo managed to terminate successfully only on the smallest instances  1 variables   and timeout on all larger instances. we believe this is due to optapo's excessive centralization  which overloads its centralized solver. indeed  optapo centralized almost all the problem in at least one node  consistent with  davin and modi  1 .
　in contrast  pc-dpop managed to keep the centralized subproblems to a minimum  therefore successfully terminating on even the most difficult instances. pcdpop 1   smallest memory usage  centralized at most 1% of the problem in a single node  and pc-dpop 1   maximal k  centralized at most 1% in a single node. pc-dpop 1  is equivalent to dpop on these problems  no centralization .
　overall  our results show that both optapo and pc-dpop centralize more in dense problems; however  pc-dpop's structure-based strategy performs much better.
1 related work
tree clustering methods  e.g.  kask et al.  1   have been proposed for a long time for time-space tradeoffs. pcdpop uses the concept loosely and in many parts of the problem transparently. specifically  in areas where the width is low  there is no clustering involved  the nodes following the regular dpop protocols. in high-width areas  pc-dpop creates clusters based on the context size of the outgoing util messages and bounds the sizes of the clusters to a minimum using the specified separator size.
1 a note on privacy
 maheswaran et al.  1  show that in some settings and according to some metrics  complete  centralization is not worse  privacy-wise  than some distributed algorithms.
　even though the nodes in a cluster send relations to the cluster root  these relations may very well be the result of aggregations  and not the original relations. for example  in figure 1  x1 sends x1  via x1 and x1  1 relations:    and. notice that that is sent to x1 like this is not the real  but the result of the aggregation resulting from the partial join performed with the util message that x1 has received from x1. therefore  inferring true valuations may be impossible even in this scenario.
1 conclusions and future work
we have presented an optimal  hybrid algorithm that uses a customizable message size and amount of memory. pc-dpop allows for a priory  exact predictions about privacy loss  communication  memory and computational requirements on all nodes and links in the network.
　the algorithm explores loose parts of the problem without any centralization  like dpop   and only small  tightly connected clusters are centralized and solved by the respective cluster roots. this means that the privacy concerns associated with a centralized approach can be avoided in most parts of the problem. we will investigate more thoroughly the privacy loss of this approach in further work.
　experimental results show that pc-dpop is particularly efficient for large optimization problems of low width. the intuition that dense problems tend to require more centralization is confirmed by experiments.
