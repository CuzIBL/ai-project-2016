
a key issue in artificial intelligence lies in finding the amount of input detail needed to do successful learning. too much detail causes overhead and makes learning prone to over-fitting. too little detail and it may not be possible to learn anything at all. the issue is particularly relevant when the inputs are relational case descriptions  and a very expressive vocabulary may also lead to inconsistent representations. for example  in the whodunit problem  the task is to form hypotheses about the identity of the perpetrator of an event described using relational propositions. the training data consists of arbitrary relational descriptions of many other similar cases. in this paper  we examine the possibility of translating the case descriptions into an alternative vocabulary which has a reduced number of predicates and therefore produces more consistent case descriptions. we compare how the reduced vocabulary affects three different learning algorithms: exemplar-based analogy  prototypebased analogy  and association rule learning. we find that it has a positive effect on some algorithms and a negative effect on others  which gives us insight into all three algorithms and indicates when reduced vocabularies might be appropriate.
1 introduction
one problem that is consistent across nearly every application of machine learning is identifying the appropriate amount of detail in the input data. as tempting as it is to learn from all of the available data  in a real-world application most of it will be irrelevant or redundant. including such extraneous detail in an analysis will not only slow down the process  but may also lead to over-fitting and hence learning of an incorrect model. clearly though  a balance must be found  since with too little data it becomes unlikely that anything can be learned at all.
　the problem is perhaps even worse when dealing with relational data. since most relational learning algorithms operate by comparing across instances of the relation itself  redundant relations become particularly dangerous. the more expressive a vocabulary  the more ways there may be to express the same information. unless all such language redundancies are detected ahead of time  relational learning algorithms will suffer.
　the issue is also particularly relevant whenever a learning system is to be deployed in any sort of real-world environment. such environments tend to be brimming with unrelated observations. robotic systems for example  will wisely choose to focus on only a few sensory inputs  which they can analyze carefully  rather than a cursory analysis of many inputs which would only confuse the picture. a similar application is the automatic extraction of knowledge from text. this is becoming more popular as the internet grows  and it is crucial to identify which relationships are useful to know  and which convey practically the same information.
　this paper  which was motivated by just such an attempt to improve knowledge extraction from text  is an analysis of the contrasting effects between using a large  very detailed but often redundant vocabulary  and a small  consistent  but extremely simplified one. the two different vocabularies are applied to the whodunit problem  which tries to learn how to predict the perpetrator of a terrorist event  given many descriptions of similar events expressed by propositional assertions. the descriptions are currently entered by hand but will eventually be extracted from text  which makes the choice of vocabulary especially relevant. each vocabulary is used by three different learning algorithms  in order to better understand the effects of the vocabulary size and to find the best overall combination.
　section 1 begins by introducing the whodunit problem. following that  each of the three learning algorithms for solving the problem are explained in section 1. section 1 describes how the original  large vocabulary was reduced  and section 1 presents our results. finally  we conclude with a discussion of related work.
1 the whodunit problem
an important task for analysts is coming up with plausible hypotheses about who performed an event. recall the preelection bombing in madrid  spain. while the spanish government originally claimed that the basque separatist group eta was the most likely suspect  evidence quickly mounted that al qaeda was very likely responsible. multiple  highly coordinated attacks  for example  are more similar to al qaeda's modus operandi than previous eta actions. this is an example of what we call the whodunit problem.
　stated more formally  given some event e whose perpetrator is unknown  the whodunit problem is to construct a small set of hypotheses {hp} about the identity of the perpetrator of e. these hypotheses should include explanations as to why these are the likely ones  and be able to explain on demand why others are less likely.
　we define a more restricted class of whodunit problems to begin with:
　formal inputs. we assume that the input information is encoded in the form of structured descriptions  including relational information  expressed in a formal knowledge representation system. note that we do not require uniform representations in each input; that is  we treat each case as simply a collection of arbitrary predicate calculus statements rather than as an object with predefined slots that may or may not be filled.
　accurate inputs. we assume that the input information is completely accurate  i.e.  that there is no noise.
　one-shot operation. once the outputs are produced for a given e  the system can be queried for explanations  but it does not automatically update its hypotheses incrementally given new information about e.
　passive operation. the hypotheses are not processed to generate differential diagnosis information  i.e.   tells  that could be sought in order to discriminate between the small set of likely hypotheses.
　supervised learning. we allow the system to train on a set of pre-classified examples {d}. for some algorithms  this involves forming non-overlapping generalizations {g} over those examples.
　the assumption of formal inputs is reasonable  given that producing such representations from news sources is the focus of considerable research in the natural language community these days. the assumptions of accurate inputs  of one-shot  passive operation  and of supervised learning are good starting points  because if we cannot solve this restricted version of the problem  it makes no sense to try to solve harder versions.
table 1. example of a whodunit case description

 isa attack-1 terroristattack  
 eventoccuredat attack-1 fallujah 
 cityincountry fallujah iraq 
 thereexistatleast 1  x
 and  citizenof  x iraq   wasinjured attack-1  x   

　the corpus we use in our experiments is an independently-developed knowledge base of terrorist incidents  provided to us by cycorp. the version they provided consists of 1 descriptions of different terrorist attacks  each hand-entered and checked by domain experts. these attacks were all expressed using the symbolic representation vocabulary of the cyc kb  which  in our subset  consists of over 1 concepts  over 1 relationships and over 1 functions  all constrained by 1 million facts. the descriptions of terrorist attacks ranged in size from 1 to 1 propositions  with the average being 1 propositions.
　the whodunit problem is an excellent domain for exploring relationships between similarity and probability. the input data consists entirely of arbitrarily high order symbolic relations with arbitrary structure between them. this means we will have to pay careful attention to structure in order to get probabilities over the correct statements  i.e. those which uniformly correspond to the same concept within each case . there is a very large number of records of terrorist attacks on which to train  but there is also a large number of possible perpetrators  1  to choose from during testing.
1 learning algorithms
we used three different algorithms to try to solve the whodunit problem. since the contribution of this paper is on the effects of vocabulary reduction and not the learners themselves  we have only selected algorithms which have been previously published. here then  we present only a terse description of each learner and the implementation details particular to the domain.
　all three algorithms utilized structural analogy in some fashion  in order to make comparisons between and across cases.
　our approach to analogy is based on gentner's  structure-mapping theory of analogy and similarity. in structure-mapping  analogy and similarity are defined in terms of structural alignment processes operating over structured representations. the output of this comparison process is one or more mappings  constituting a construal of how the two entities  situations  or concepts  called base and target  can be aligned. for the purposes of this paper  a mapping consists of a set of correspondences and a structural evaluation score. a correspondence maps an item  e.g. an entity or expression  from the base to an item in the target. the structural evaluation score indicates the overall quality of the match.
　we used the structure-mapping engine  sme  to implement this theory of analogical mapping  falkenhainer  forbus  & gentner  1 . sme uses a greedy algorithm to compute approximately optimal mappings in polynomial time  forbus & oblinger  1 .
　formally  the task of each learning algorithm is  given descriptions to train on {d}  and input event e  to produce n hypotheses about the identity of the perpetrator of event e.
1 exemplar-based analogy: mac/fac
the first algorithm operates purely on exemplar retrieval. that is  it is designed to find a small number of input cases which are most similar to the probe case e. for each such case  it hypothesizes that the perpetrator of e is the same as the perpetrator of the similar case. the process can be iterated until n unique hypotheses are generated. mac/fac  forbus  gentner  & law  1  is an algorithm which performs this similarity-based retrieval in two stages.
 the first stage uses a special kind of feature vector  called a content vector  which is automatically computed from each structural description. a content vector consists simply of the counts of each predicate in the corresponding description. their dot product then is an estimate of how many correspondences sme will generate when considering possible mappings between two descriptions  and therefore an estimate of the quality of the match. content vector computations are used to rapidly select a few  typically three  candidates from a large memory.
　in the second stage  sme is used to do an analogical comparison between the subset of {d} which was returned by the first stage and the probe description e. it returns the one  or more  if very close  most similar of the cases in memory as what the probe reminded it of.
　as deployed performance systems  both sme and mac/fac have been used successfully in a variety of different domains  and as cognitive models  both have been used to account for a variety of psychological results  forbus  1 .
1 prototype-based analogy: seql
the second algorithm is designed to also use analogy. however  it first builds generalizations of the cases to serve as prototypes. each generalization is constructed from the cases of only one perpetrator at a time. this way  each generalization serves as a prototypical description of the events associated with that single perpetrator. then instead of comparing e to every case in {d}  as the first algorithm does   this algorithm compares e to every prototype.
　the generalizations are built by using analogy to determine which concepts in one case best correspond to the concepts in another case. we use a probabilistic implementation of the seql algorithm to do this. seql  kuehne  forbus  et al.  1   which stands for sequential learner  is designed to produce generalizations incrementally from a stream of examples. it uses sme to compare each new example to a pool of prior generalizations and exemplars. if the new example is sufficiently similar to an existing generalization  it is assimilated into that generalization. otherwise  if it is sufficiently similar to one of the prior exemplars  it is combined with it to form a new generalization.
　a generalization of two cases is done by taking the union of the expressions in the two descriptions  and adjusting the probability of each expression according to whether or not it was in both descriptions. matching entities that are identical are kept in the generalization  and non-identical entities are replaced by new entities that are still constrained by all of the statements about them in the union.
　seql provides an interesting tradeoff between traditional symbolic generalization algorithms like ebl  dejong & mooney  1  and statistical generalization algorithms  such as connectionist systems. like ebl  it operates with structured  relational descriptions. unlike ebl  it does not require a complete or correct domain theory  nor does it produce a generalization from only a single example. like most connectionist learning algorithms  it is conservative  only producing generalizations when there is significant overlap. however  seql has been shown to be substantially faster than connectionist algorithms when compared head-to-head  kuehne et al.  1 . moreover  this was done using the same input representations as the connectionist models  and the seql-based simulation continued to work when given noisy versions of the data.
　one potential drawback with seql is that generalizations - the union of many case descriptions - can grow quickly in size  but sme has polynomial memory requirements. therefore seql must often pare down the facts in a generalization  it culls those with lowest probability   throwing away information to preserve a higher level of abstraction within a reasonable space requirement.
1 rule learning
the third method that we applied was more statistical in nature. it learns a list of association rules for predicting each possible perpetrator. in order to do this  it must convert the structured relational data of {d} into a feature-value representation. we used a case flattening approach introduced by halstead & forbus  to accomplish this. namely  seql was applied to all of the input cases at once in order to build one very large generalization. this generalization was then used as the framework for building a feature set  with each assertion in the generalization corresponding to one feature. the generalization and feature set  taken together  form an invertible mapping from relational case descriptions to features and back again.
　for interpretation of the results  it is important to note here that features come in two types. the first  an existential feature  is the default type. it simply takes the value true or false depending on whether the assertion it represents is present in a given case or not. a characteristic feature on the other hand  can be used when more is known about the structure of the assertion  and so more information can be conveyed. for example  the english sentences  the attack killed someone  and  it was an ambush  are existential features - they are either true or false. the sentences  the attack killed three people  and  it happened in baghdad  are characteristic features  which have the values 1 and baghdad  respectively. as we will show  the conciseness of a reduced relational vocabulary makes it easier to extract characteristic feature values from the data.
　for rule learning  we use the definition of association rule introduced by agrawal  et al. . hence  each association rule is a conjunction of feature-value pairs  a.k.a. literals  such as  location . iraq   which implies another such conjunction.
　the actual rule learning is done using an ad-tree to cache the joint probabilities of the input data  anderson & moore  1 . adopting anderson & moore's algorithm  we start with a list of candidate hypotheses  antecedents  h  initially empty  and perform a breadth-first search with a beamwidth of 1 over the space of possible hypotheses. on each iteration of the search  we further specify each hypothesis in h by adding literals from the set of possible literals l. we then re-evaluate each hypothesis  choose the best 1 again  and re-iterate until h is empty or the rules are 1 terms long. one example of a rule learned looks like:
 if  and  location  attack philippines 
 agentcaptured  attack  agent  
 perpetrator  attack moroislamicliberationfront  
　finally  for every possibly perpetrator  the algorithm actually learns a rule-list by recursively calling the rulelearner. for any perpetrator p  it begins by learning a single rule h1   p. on the next iteration  it tries to learn a rule for h1   p  h1. this process continues until it reaches a maximum of 1 rules or no more rules can be found.
　once a list of rules rp is learned for every perpetrator p  then the input case e is applied to every rp  to see which rules fire. the n rules which fired with the highest confidence have their consequents returned as the hypothesis to the identity of the perpetrator.
　whereas the other two learners are limited to reductive learning  the rule learner learns simple  higher-level patterns.
1 reduced vocabulary
the original human-encoded input data consisted of 1 predicates  drawn from a vocabulary of over 1. this was translated into only 1 predicates  a 1% reduction in vocabulary size and knowledge compression ratio of 1.
　the reduced vocabulary that we used was a subset of the vocabulary designed by language computer corporation for their polaris system  bixler  moldovan  & fowler  1 . their vocabulary consisted of 1 predicates  chosen for their usefulness in natural language processing  the feasibility of their automatic extraction from text  and of particular importance to this research  the broadest semantic coverage with the least amount of overlap. lcc explains:
while no list  of predicates  will ever be perfect... this list strikes a good balance between being too specific  too many relations making reasoning difficult  and too general  not enough information to be useful .
　the subset of 1 predicates which we selected from this vocabulary was based simply on those polaris predicates which were needed to represent the information already present in the whodunit training cases. the final list of predicates is provided in table 1.
table 1. all relations in the reduced vocabulary
agentgoalresultthemepredicatepurposelocationtimemeasurepropertypartcauseinstrumenttopicbeliefassociatedreasonsourceexperiencerrecipientpossible  entails　each new predicate corresponds to any of a set of old predicates. these relationships were easily hand-coded into a list of 1 translation rules  which were used for the actual data reduction. for example  the following rule handles intentional actions:
 translate
 or  doneby  event  agent 
 eventplannedby  event  agent  
 and  agent  event  agent   goal  agent  event   
　the above rule fires on any facts in the original cyc representation whose predicates are either doneby or eventplannedby  or are specializations of doneby or eventplannedby. each such fact is translated into two new facts: the first describing that the agent played some causal role in the event  and the second describing that the event was in fact a goal of the agent. some facts are actually translated by the application of more than one rule. an example of this translation process can be seen in table 1.
table 1. translation example
cyc thereexistexactly 1  agent
 agentcaptured attack  agent  rule 1 translate
 thereexistexactly  number  variable  fact 
 and  measure  variable  number   fact  rule 1 translate
 objectactedon  event  object 
 and  theme  event  object 
 predicate  object  predicate 
 result  event  predicate   polaris measure  agent 1 
 theme attack  agent 
 predicate  agent agentcaptured 
 result attack agentcaptured 　note that since the original cyc vocabulary is much richer  many of the facts in the original data must be represented by more than one fact upon translation. specifically  the average translation rule turns one fact into 1 new facts. the average number of facts in a case increased by 1%.
1 results
we used three criteria for bounding the size of the set of hypotheses n. the most restrictive is producing only a single perpetrator  i.e.  guessing directly who did it. the least restrictive is a  top 1  list  rank ordered by estimated likelihood. the middle ground is the  top 1  list  which has the virtue of providing both the best and some  hopefully mindjogging  alternatives.

　the results turn out to be very different for each algorithm. chart 1 shows that under the conditions of the original vocabulary  the rule learner performs the best. it is able to return the correct answer on its first guess more than 1% of the time. seql finds as many correct answers in the long run  but is less certain in the beginning  providing the correct answer in its first guess only 1% of the time. finally  mac/fac does a little better than seql on its first guess. interestingly though  continuing to construct hypotheses from mac/fac beyond that point proved useless.
　under the new vocabulary  the exemplar-based algorithm improved  p-value .1 . seql though  performed worse  p-value .1 . perhaps stranger still  the rule learner  which depends on the generalization algorithm provided by seql  performed even better than it had before  p-value .1 . it gets the correct answer on its first guess 1% of the time.
　closer examination reveals that the seql algorithm was hard-pressed. in the original vocabulary  seql generalized from case descriptions which contained an average of 1 facts each. however  1% of those facts had to be discarded to preserve memory as dimensionality  case description size  increased with abstraction. under the reduced vocabulary  which is already an abstraction of the original data  this information loss is compounded. when the average description size increases by 1%  so does the number of facts discarded by seql during generalization  which rises to 1%. furthermore  the facts which remain carry less information than they did under the original vocabulary  the average reduced predicate corresponds to 1 different predicates from the original vocabulary .
　so  how did the rule-learner improve in performance  one possible explanation may be because of the leap it takes from reductive to higher-order learning. however  this seems to be only part of the story. more important is that the rule learner is able to take advantage of the conciseness in the reduced vocabulary. this conciseness allows the flattening process to generate many more characteristic values for the features than before: the average arity increases by 1%. this plethora of feature values gives the rule learner more grist by having more relevant options to consider than before. further analysis shows that when features are treated as existential and allowed only two values again  as 1% of them had under the original vocabulary   the rulelearner reverts to almost seql-like levels of performance.
　we were surprised by how well all three strategies performed  even the non-statistical ones  given the difficulty of the problem. consider that although each case contains an average of only 1 facts  there are over 1 features in the dataset. this means that for any given record  over 1% of the features will be missing. this makes for a very sparse dataset. fortunately  the closed world assumption seems to have held up. yet  when we consider that the arity of the output attribute is 1  it seems that those 1 features may not be enough. a random algorithm would select the correct perpetrator 1% of the time  and would get it right with ten guesses only 1% of the time. therefore  we believe that success rates of 1% are quite good.
　in conclusion  given that researchers tend to use larger relational vocabularies  it is extremely interesting that a reduced relational vocabulary can improve two out of three learners tested. certainly  since these experiments only test one reduced vocabulary in one domain  some caution in interpreting the results is warranted. it also seems that reduced vocabularies are dangerous to use in a learning algorithm that already relies heavily on abstraction  e.g. seql   since it may lead to too much data and/or too much loss of information. however  vocabulary reduction does trade away smaller case descriptions and some information for added conciseness. learning algorithms which are known to do well with large amounts of data  high dimensionality  and which can take advantage of this extra conciseness  such as the rule learner  which pays attention to the greater number of extractable feature values  appear likely to do better under a well-chosen reduced relational vocabulary.
1 related work
at first glance  this work appears to tie in strongly to forms of dimension reduction  such as feature selection. however  the task in feature selection is to automatically select a subset of concepts  a form of filtering. reduced relational vocabularies abstract and transform more than they filter. other forms of dimension reduction do perform a transformation on the data  such as principal component analysis. however  the result of this transformation has often lost any real-world meaning. furthermore  all of these techniques are typically automated within a given domain. this particular reduced vocabulary  in contrast  was manually constructed  by other researchers   based on needs of natural language processing  to work under any domain. finally  although the number of relations in the data decreases under vocabulary reduction  the dimensionality of the data actually increases  since the simpler relations must be used in many different sentences to convey the same information.
　many different ontologies have also been developed in an effort to balance language expressiveness with computability. owl is a common example targeted for use with the world wide web. work has also been done on computing the expressiveness of a given ontology. for example  corcho and gomez-perez  establish a common framework for comparing the expressiveness and reasoning capbility between languages  and apply this methodology to compare and contrast between ontologies. also  golbreich  dameron  et al.  attempt to establish some minimal requirements that a web-based ontology should meet.
　a number of alternative cognitive simulations of analogical mapping and retrieval have been developed. some are domain-specific  mitchell  1   and thus inapplicable to this problem. others are based on connectionist architectures  hummel & holyoak  1; eliasmith & thagard  1   and are known to not be able to scale up to the size of examples used in these experiments. while cbr systems have been used to tackle similar structured representation problems  leake  1   they also tend to be built as special-purpose systems for each domain and task. by contrast  the simulations used here are applicable to a broad variety of representations and tasks.
　also  there are many other approaches for doing learning from high-order relational data which do not use analogy at all. one of the most notable in recent years may be the building of probabilistic relational models  or prm's  getoor et al.  1 . a prm is a bayesian dependence model of the uncertainty across properties of objects of certain classes and the relations between those objects. other approaches include blockeel and uwents   who present a method for building a neural network from relational data  and dayanik and nevill-manning   who discuss clustering relational data through a graph-partitioning approach. recent work in link analysis also provides a means for tying probability in with relational data to do learning. cohn and hofmann  actually create joint probability tables of both properties and links  in this case  terms and citations in document analysis  through a probabilistic decomposition process related to lsa  and then use it to perform classification. variants on ilps such as bayesian logic programs  blps   kersting  de raedt  & kramer  1  have also been suggested for this sort of application.
