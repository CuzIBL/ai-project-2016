 
we give a general overview of cunt  a userfriendly interactive concept-learner  which can be used as a module for learning apprentice systems. clint combines several interesting features : it uses domain-knowledge  generates examples  copes with indirect relevance  shifts its bias  recovers from errors and identifies concepts in the limit. 
1 introduction. 
one of the achievements of early work on machine learning  michalski et al  1  was the development of concept-learners which operated in isolation  relied heavily on the user and addressed only well defined and restricted tasks. today  a new generation of learning systems  named learning apprentice systems  las   is being developed  e.g. disciple  tecuci  1  learning preconditions of actions and object models  blip  morik  1  modelling the world by observation  prodigy  minton  1  and leap  mitchell et al  1  improving their problem-solving performance  ... concept-learning is often a subtask in this type of systems; it has to use the available knowledge and keep the user's involvement to a minimum. 
　　this paper gives a general overview of clint  concept-learning in an interactive way   a user-friendly concept-learner. it uses knowledge  generates examples  copes with indirect relevance  shifts its bias  recovers from errors and identifies concepts in the limit. all these properties make it well suited as a module in a las. a more thorough discussion of clint w.r.t. example generation and assimilation can be found in  de raedt and bruynooghe  1a  and w.r.t. bias  language and explanation in  de raedt and bruynooghe  1b . the contribution of this paper is that it shows how the techniques of  de raedt and bruynooghe  1ab  can be integrated in one system. 
　　the paper is organized as follows : in section 1  the learning task is formulated in a logical framework; the following section contains a full description of the learning algorithms in clint; an example illustrating the technique is given in section 1 and finally in section 1  we briefly touch upon related work. the appendix contains a summary of clint's algorithms. 
1 problem-specification. 
concept-learning refers to the process of generating a concept-description from examples. we use a logical framework  genesereth and nilsson  1 . a concept is a 
predicate. a concept-description is a set of horn-clauses defining the predicate and examples are ground instances of a predicate. we restrict ourselves to function free predicates. moreover  literals are of the form p x1   ...   x n   with the x. distinct variables. this is not an additional restriction because constants are allowed in the knowledge-base and an equality predicate is also available e.g. q a x x  rewrites to q x y z  :- a x   y=z where a a  is in the knowledge-base. 
　　in practice the concept-learning process is not only driven by training-examples but also by bias  utgoff and mitchell  1   which is -following utgoff- anything which influences how the concept-learner draws inductive inferences based on the training-examples. the significance of bias for concept-learning is widely recognized  utgoff and mitchell  1    utgoff  1 . although bias may be incorporated in a concept-learning system in a variety of ways  we will only consider bias in the form of restrictions on the description language. 
　　in our framework  a bias is correct for learning a predicate p if there exists a set of horn-clauses s for the predicate p in the description language associated with the bias  such that all positive and no negative examples are covered by s. a concept is conjunctive if it can be defined with one horn-clause. otherwise it is disjunctive. 
　　the knowledge-base  employed by the learner  contains two different kinds of predicates. basic predicates are defined by the user and assumed to be correct. they are specified by arbitrary horn-clauses. learned predicates are defined by the learner and derived from opportunities. an opportunity occurs when the user signals that there is an error in the learner's knowledge-base. in our framewoik  an opportunity takes the form of an uncovered positive or a covered negative example. horn-clauses for learned predicates satisfy one of the languageschema's in the learner's knowledge-base. a languageschema describes a set of syntactic restrictions  i.e. a 
	de raedt and bruynooghe 	1 
bias  which must be satisfied by a hom-clause in order to belong to the language associated with the schema. our concept-learner has an ordered series of languageschema's l1 l 1  ...  ln ... 
　　for each opportunity  the learner analyzes and augments its knowledge by asking membership questions to the user. each question asks for the classification of an example for a learned predicate. by processing the opportunities  the learner's knowledge base gradually converges to the desired one. 
　　by now we can define our notion of concept-learning more precisely : 
given : 
     a knowledge-base of basic and learned predicates 
     for learned predicates  a set of positive and negative examples 
     a series of language-schema's  l1 l 1  ...  ln ... 
     an opportunity to learn  indicated by the oracle 
     an oracle  willing to answer membership questions find: 
     an adapted knowledge-base  such that all positive examples and no negative examples are covered 1 processing an opportunity. 
the two classes of opportunities  uncovered positive examples and covered negative examples  are processed in a different way. 
1 processing a positive example. 
when the learner receives an uncovered positive example  it constructs a new clause covering the example and adds it to the knowledge-base. to compute the clause  the learner constructs an explanation  cf. later  in order to obtain a starting clause  and generalizes it by asking questions. the algorithm to process an uncovered positive example is shown in handle uncovered of the appendix. 
　　as examples are ground facts  they only specify the objects which are directly involved and not the relevant relations between these objects with respect to the concept. all relations in the knowledge-base are potentially relevant. however  considering all these relations simultaneously is clearly not feasible since this would very rapidly lead to combinatorial problems. therefore  our learner divides its knowledge-base into different regions and at any time only relations belonging to one region are considered. the regions are computed from the example  the knowledge-base and the language-schema's. given an example  a knowledge-base and a language-schema  a maximal ' set of relations holding for the example 
 as such a set may indicate why an example belongs to a concept  we call it an explanation however  in contrast to ebg  mitchell et al.  1   our notion of explanation does not mean that the conditions are sufficient. 1 
for simplicity  we will assume that the explanation is unique. in  de raedt and bruynooghe 1b   we show that the explanation is unique for some interesting languages and show how to extend the method if explanations are not unique. 
1 	machine learning 
and satisfying the schema  is computed. this set constitutes the explanation. from the explanation and the example  a starting clause is derived by turning all constants into distinct variables in the clause with as head the example and as body the explanation. note that the starting clause for an example is true  provided that the concept can be described in terms of the predicates in the knowledge-base and the language-schema. the reason is that a starting clause is the most specific one covering the example with respect to the knowledge-base and the language-schema. since the language-schema's are ordered according to growing expressiveness of the languages they describe  we can order the starting clauses according to the language they belong to. within this order  the first starting clause  which does not cover any negative example  is passed to the generalization procedure. 
　　in the next step  the starting clause is carefully generalized using a specific to general search which drops multiple conditons  this part constitutes the w h i l e loop . the generalization procedure computes a new clause obtained from the old clause  the starting clause   by deleting a subset s of the body of the old clause  such that the new clause covers an example e which was not covered by the old clause  the new clause does not cover negative examples and s is minimal. the example e is then presented to the user for classification. if it is positive the old clause is replaced by the new one  and the process is repeated. otherwise  the learner backtracks in order to construct another subset. if there are no more subsets fulfilling the conditions  the generalization process terminates and the clause is added to the knowledgebase. during generalization  the system may have collected new negative examples. the algorithm tests whether these are covered by previously learned rales  and if necessary  it takes appropriate action in order to recover from possible errors. 
1 processing negative examples. 
when the learner receives a covered negative example  there must be an incorrect clause in its knowledge-base. the algorithm to handle covered negative examples is shown in h a n d l e   c o v e r e d in the appendix. a clause c is incorrect if there exists a substitution 1 such that head c 1=false and body c 1 is true. to locate incorrect clauses  the learner constructs the proof tree explaining why the negative example is covered and analyses it. if necessary it asks intelligent questions to the user. these questions are also membership-questions. they ask for the classification of examples which correspond to nodes in the proof tree. the method to locate incorrect clauses is very similar to shapiro's debugging method used in the model inference system  shapiro 1 . when an incorrect clause is located  it is retracted from the knowledgebase. of course  in order to maintain consistency the learner must verify whether the positive examples which were covered by the incorrect clause  are still covered. if not  it will generate and process an opportunity for each uncovered positive example. also  if a predicate was learned after the predicate for the incorrect clause and if the latter one was involved in learning the former  then 
we must also verify its positive and/or negative examples. in order to avoid these interactions between different predicates as much as possible  it is advised that predicates arc thoroughly tested before other ones are learned. the complete algorithm is shown in the appendix. 
1 about the algorithm. 
at this point we want to stress that the subsets s in handle uncovered can efficiently be computed using a breadth-first search  pruned by knowledge and guided by heuristics. for more details see  de raedt and bruynooghe  1a . 
　　the efficiency of the algorithm depends on the redundancy in the starting clause s  and the size of the knowledge-base. for a lot of interesting problems  the needed resources are acceptable  cf.  de raedt and bruynooghe  1a  . the number of questions generated by the algorithm  depends on the number of conditions and the redundancy in the starting clause s . in general  the number of questions needed is small  cf.  de raedt andbruynooghe  1a  . 
　　because of space-limitations we do not discuss the language-schema's and the way they are implemented here. for more details see  de raedt and bruynooghe  1bj. instead  we give a detailed example. 
clint's algorithm has several interesting features : 
  it copes with indirect relevance  michalski  1  because the user only has to supply the directly involved objects in an example and not the important relations holding between them. this is a more natural and less demanding way to describe examples. 
  disjunctive as well as conjunctive predicates can be learned. 
  the system can shift its bias when it discovers that its language is not sufficient to describe the predicate. this situation occurs whenever the starting clause covering a positive example in a language covers negative examples  see also the example . in that case the language of the clause will be shifted. so  cunt shifts the bias at the clause-level. 
  the system generates most of the examples it needs. the user has to specify at most one positive example to learn a clause. 
  the system is a closed-loop one. this means that newly learned predicates are integrated  assimilated  in the knowledge-base and that once learned  they can be used like any other predicate. 
  the system provides a kind of error-recovery. if an incorrect clause is assimilated  the system will retract it when there is an opportunity to do so. 
  clint identifies concepts in the limit  provided that the predicate can be described in one of the languages and the current knowledge. this limiting behaviour is proved in  de raedt and bruynooghe  1ab . for conjunctive predicates and simple languages there is even finite identification. 
there are still some remaining problems with the algorithm : 
  recovering from an error in an assimilated predicate may also affect other predicates. therefore  recovering from such errors may involve a lot of work. 
  the order in which predicates are learned is impor-tant. ideally  the easier predicates should be learned first  and the more difficult ones  which use the easier ones in their definition should only be learned afterwards. however  if the user presents the predicates in a different order  this does not necessarily lead to problems  because of clint's error-recovery ability. in fact  problems only arise when there is a positive and a negative example such that given the knowledge and the languages  there is no clause which covers the positive example and which does not cover the negative one. in that case  clint will keep on shifting its bias. 
  the learned clauses depend very much on the knowl-edge clint possesses. if clint knows all relevant predicates w.r.t. the concept to be learned  then clint will learn a small number of clauses in easy languages. on the other hand  if clint knows only predicates that are not so relevant  clint will learn a larger number of clauses or the bias of the clauses will be more complex. this problem seems to be inherent to concept-learners. 
  clint does not take into account the relationship between different clauses for the same concept. as a consequence  clint does not always learn the concept-description whith the minimal number of clauses. this problem also arises in marvin  sammut and banerji  1  and mis  shapiro  1 . 
  for some language-schema's  knowledge-bases and examples there may be more than one explanation. if the number of possible explanations is large  the algorithm may require a lot of computation. for a discussion of these issues and some possible solutions we refer to  de raedt and bruynooghe  1b . 
1 	an example session. 
the knowledge-base consists of the following definitions in prolog : 

	de raedt and bruynooghe 	1 
object x :- block x . less x y  :- x   y. 
comments are in italic  user input in bold and clints output in roman. 
 -learn. 
positive example of new predicate   
  same coior t1 b1 . 
an explanation for the example is computed using the first language schema : table t1   block b1   object t1   object b1 . the starting clause is derived from the explanation. notice that the starting-clauses contain only relations which hold for the example. the first languageschema requires that all variables in the body of the starting-clause also occur in the head of the startingclause. although the system knows that object is more general than block and table  it keeps these predicates in the body in order to facilitate the generalization procedure. this knowledge is however used by the system during the generalization process and in order to simpli-
fy final clauses. 
starting clause : 
same  color  x  y  :t a b l e   x     b l o c k   y     o b j e c t   x     o b j e c t   y   . 
the system tries to generalize and after considering one candidate generalization  it constructs an example and a subset fulfilling the necessary conditions  so it asks the user to classify the example. the example is choosen randomly from the set of examples which can be constructed using the subset. 
is same color tl tl  positive    y/n  
 y 
after searching 1 clauses  next clause : same  color  x  y  :-
	t a b l e   x   	  	o b j e c t   x   	  	o b j e c t   y   . 
as the user classifies it as positive  the system generalizes. 
is same color bl tl  positive    y/n  
 y. 
after searching 1 clauses  next clause : same color x y  	: -
	o b j e c t   x   	  	o b j e c t   y   	. 
the system is unable to find further generalizations  so the clause is asserted in the knowledge-base. no more valid generalizations after searching 1 clauses generalized clause : 
same  color x y  	:o b j e c t   x   	  	o b j e c t   y   	. 
 -error. 
example which is incorrectly handled   
  same color t1 b1 . 
the user signals that an error has occurred. the system locates and retracts the incorrect clause by analyzing the proof for the example. if object was a basic predicate  then the two verification questions would not be asked since it is assumed that basic predicates are correct. 
verify whether object b1  is true    y/n  
1 	machine learning 
 y. 
verify whether object t1  is true    y/n  
 y. 
retracting wrong clause: 
	same color x y  	: -
	o b j e c t   x   	  	o b j e c t  y  	. 
same color t1 b1  is positive and uncovered starting clause for same color t1 b1  covers negative example same color t1 b1  
as the system discovers that the starting clause covers a negative example  it decides to shift the bias of this clause. the new language-schema includes clauses where each condition can have one variable not occurring in the head. no relations between these existentially quantified variables are allowed. shifting bias  new starting clause : same  color  x  y  : -
t a b l e   x     b l o c k   y     weight x xw   w e i g h t   y   y w     composition x xc    c o m p o s i t i o n   y   y c     c o l o r   x   c x     c o l o r   y   c y     o b j e c t   x     o b j e c t   y   . 
starting clause for same color t1 b1  covers negative example same color t1 b1  
the new language-schema is the same as the old one except that it allows for relations between the existentially quantified variables. 
shifting bias  new starting clause : same  color  x  y  : -
t a b l e   x     	b l o c k   y     	weight x xw   w e i g h t   y   y w     	c o m p o s i t i o n   x   x c     c o m p o s i t i o n   y   yc   	c o l o r   x   c x     c o l o r   y   c y     	o b j e c t   x     	o b j e c t   y     cx = cy  less xw yw . 
ls same color bl b1  positive    y/n  
after searching 1 clauses  next clause : same  color {x  y  :-
b l o c k   y     	w e i g h t   x   x w     w e i g h t   y   y w     	c o m p o s i t i o n  x xc   c o m p o s i t i o n   y   y c     	c o l o r   x   c x     c o l o r   y   c y     	o b j e c t   x     	o b j e c t   y     cx = cy  	less xw yw . 
i know same color bl tl  is positive  so i generalize ! 
after searching 1 clauses  next clause : same color x y  	: -
w e i g h t   x   x w     w e i g h t   y   y w     	c o m p o s i t i o n   x   x c     composition y yc    	c o l o r  x cx    c o l o r  y cy   	o b j e c t  x   	o b j e c t  y   cx = cy  	less xw yw . 
is same  color bl b1  positive    y/n  
 n. 
ls same color bl bl positive    y/n  
after searching 1 clauses  next clause : same  color  x  y  :-
w e i g h t   x   x w     weight y yw    c o m p o s i t i o n   x   x c     c o m p o s i t i o n   y   y c     c o l o r   x   c x     c o l o r   y   c y     o b j e c t   x     o b j e c t   y     c x = cy. 
is same color b1 p1  positive    y/n  

after searching 1 clauses  next clause : same  color  x  y  :-
w e i g h t   x   x w     c o m p o s i t i o n   x   x c     c o m p o s i t i o n   y   y c     c o l o r   x   c x     c o l o r   y   c y     o b j e c t   x     o b j e c t   y     c x = cy. 
is same color pl pl  positive    y/n  
 y. 
after searching 1 clauses  next clause : same  color  x  y  :-
composition x xc  composition y yc    color x cx  color y  cy   object x   o b j e c t   y     c x = cy. 
no more valid generalizations  after searching 1 clauses generalized and simplified clause : same color x y  	: -
c o l o r   x   c x     c o l o r   y   c y     c x = cy. 
the final clause can be simplified using information gathered during the while-loop in the appendix. this session with cunt  implemented in bimprolog took about 1 seconds of cpu-time on a sun 1. 
1 	related work. 
our approach is related to the work on version-spaces 
 mitchell  1   learning apprentices  tecuci  1   ebg 
 mitchell et al  1   experimentation  subramanian and feigenbaum  1   sammut and banerji  1    krawchuk and witten  1   indirect relevance  buntine  1   identification in the limit and debugging  shapiro  1  and shift of bias  utgoff and mitchell  1   utgoff  1 . 
　　clint's basic algorithm is very similar to versionspaces  except that clint stores the negative examples instead of the g-set. for rich description languages this is often more efficient  de raedt and bruynooghe  1a . 
　　as clint builds plausible explanations for examples in an inductive way it attempts to overcome the strong theory requirement imposed by ebg. therefore it also needs more than one example to find a new definition. 
　　clint asks membership questions to the user  just like factoring  subramanian and feigenbaum  1   marvin  sammut and banerji  1   alvin  krawchuk andwitten  1 . however  none of these techniques copes with indirect relevance  because they all require that the relevant relations in an example are specified. subramanian uses some kind of propositional logic  while we use subsets of first order logic. factoring is more powerful but less general than clint because it requires that the space is factorable. for marvin and alvin it is less clear which concepts can be learned. alvin is less efficient and more powerful because it always tries to construct crucial objects  while clint constructs just significant objects. also  marvin and alvin cannot shift their bias and only cope with a more limited form of indirect relevance  from some given relations it is possible to induce other ones . 
　　coping with indirect relevance in clint is very similar to the technique in pga  buntine  1 . only  pga is unable to shift its bias or to generate examples. 
　　clint is related to the learning component of disciple  tecuci  1 . however  it does not suffer from some of the problems with disciple  tecuci  1  like the use of a restricted versionspace approach  limited explanations and no means for error-recovery. 
　　the debugging method used in clint is adapted from  shapiro  1   and optimized not to ask questions about basic predicates. 
　　previous work on shifting the bias concentrated on propositional logic  utgoff  1 . one of the main contributions of clint is the introduction of a series of languages  which are subsets of first order logic and which can be used to shift the bias. these languages are computed dynamically from one example and the knowledgebase. 
1 conclusions. 
we presented an original approach to concept-learning combining several interesting features  example-generation  shift of bias  use of knowledge  indirect relevance  into one system. we believe that the resulting system clint  is more friendly than other concept-learners. 
hence  it seems very promising for use in learning apprentice systems. clint is currently being integrated in an expert system shell in order to obtain a learning apprentice. 
acknowledgements. 
　　we would like to thank y. kodratoff  g. tecuci  g. sablon and j.f. puget for suggestions on clint and g. 
sablon  d. de schreye  m. vermaut and the referees for their comments on earlier versions of this paper. part of the work was done while luc was a visitor in the inference and learning group of y.kodratoff at the universite de paris-sud. maurice bruynooghe is supported by the belgian national fund for scientific research and luc de raedt's visit was partially supported by the belgian national fund for scientific research by means of an ibm-travel grant. 
