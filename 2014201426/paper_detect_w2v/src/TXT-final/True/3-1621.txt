
it has recently been proposed that it is advantageous to have models of dynamical systems be based solely on observable quantities. predictive state representations  psrs  are a type of model that uses predictions about future observations to capture the state of a dynamical system. however  psrs do not use memory of past observations. we propose a model called memory-psrs that uses both memories of the past  and predictions of the future. we show that the use of memories provides a number of potential advantages. it can reduce the size of the model  in comparison to a psr model . in addition many dynamical systems have memories that can serve as landmarks that completely determine the current state. the detection and recognition of landmarks is advantageous because they can serve to reset a model that has gotten off-track  as often happens when the model is learned from samples. this paper develops both memory-psrs and the use and detection of landmarks.
1 introduction
this paper explores the use of two types of observable quantities - the history of past observations and predictions about future observations - in creating models for complex dynamical systems. models that use only predictions of future observations to capture state  called predictive state representations or psrs  littman et al.  1   have been shown  surprisingly  to be at least as expressive and compact  singh et al.  1  as classical models such as partially observable markov decision processes  pomdps  that use hidden state variables. models that only use observation history  such as k-order markov models  are known to be not as expressive and hence not as widely applicable as psrs or pomdps. on the other hand  history-based models are easier to learn from data than either psrs and pomdps. in this paper  we propose an extension to psrs  called memory-psrs  mpsrs   that combines past observations  memories  with predictions about the future to define the state of a dynamical system.1our goal is to provide a model class that is as expressive as psrs but that has some of the efficient learning properties of history-based models.
¡¡in addition to accelerating learning  mpsrs can also exploit landmarks or observations that capture the state of the system uniquely  by  resetting  the approximate state of the learned model during prediction whenever these landmarkobservations are encountered. we present a method for finding landmarks in mpsrs  develop the basic theory of mpsrs and provide preliminary empirical results exploring the relative efficiency of learning mpsrs and psrs.
1 psrs and memory-psrs
psrs depart from other models of dynamical systems in that their representation of state is a vector of predictions of the outcomes of tests that may be performed on the dynamical system. a test t = a1 ...akok is a sequence of alternating actions ai ¡Ê a and observations oj ¡Ê o  and its prediction  p t   is the conditional probability that the observation sequence occurs  given that the action sequence is taken  and so p t  = prob o1 ...ok|a1 ...ak . of course  the prediction of a test is dependent on the actions and observations that have occurred so far  called the history. the prediction of a test t at history h is p t|h  = prob o1 ...ok|ha1 ...ak .
¡¡a psr's state representation consists of predictions of a special set q of tests  called the core tests. the core tests are special because at any history  the predictions for any test can be computed as a linear function of the predictions of the core tests. the prediction vector p q|h  is the  n = |q| ¡Á 1  vector of predictions for the tests in q at history h. thus  p q|h  is the psr's representation of state and is the counterpart to belief-states in pomdps and the last k observations in k-order markov models.
¡¡in addition to the set of core tests  a psr has model parameters: a set of  n¡Án  matrices mao  and  n¡Á1  vectors mao  for all a o. the model parameters allow linear computation of the prediction for any test t = {a1 ...akok} as follows:
	p t|h  = p q|h tma1...mak 1ok 1makok.	 1 
updating the current state  or prediction vector  when action a is taken in history h and o is observed  is accomplished by
	.	 1 
the matrices mao and vectors mao have a special form. the ith column of mao is the  n ¡Á 1  constant vector that computes the prediction of the  ao  one-step extension of the ith core test qi ¡Ê q  i.e.  of test aoqi. the vector mao is the constant  n ¡Á 1  vector that computes the prediction for the one-step test t = ao. the fact that the model parameters have these meanings is the foundation of existing psr learning algorithms  james and singh  1; rosencrantz et al.  1  as well as the new mpsr learning algorithm presented here.

figure 1: the system dynamics matrix.
system dynamics matrix
the theoretical development of mpsrs is best explained using the system dynamics matrix  d  that was developed in singh et al. . the matrix d  shown in figure 1  has all possible histories as rows and all possible tests as columns. the columns  rows  are arranged by test  history  length and within the same length in lexicographic ordering. the entry dij = p tj|hi  is the prediction for the jth test at the ith history. there is a one to one correspondence between system dynamics matrices and dynamical systems. the rank of d is the dimension of the corresponding dynamical system and is the number of core tests needed by the psr model. the core tests of a psr model correspond to a maximal set of linearly independent columns. similarly  we can define a notion of core histories that correspond to a maximal set of linearly independent rows. we will only be interested in finite dimensional systems. this assumption is not terribly restrictive because all pomdps with n underlying or nominal states are dynamical systems of dimension at most n. a full derivation of psrs through use of the system dynamics matrix is presented in singh et al. .
memory-psrs
the central idea behind mpsrs is to partition d into a set of m submatrices d1 ...dm by partitioning the set of histories  rows  but not the tests  columns   i.e.  each submatrix di contains all the columns but only a subset of the histories. how do memories enter this picture  we will use memory of past observations to partition histories  and every memory will correspond to a partition; we elaborate on this later. for now  note that each submatrix will have its own rank and its own core tests and core histories as well as model-update parameters. the set of memories and core tests for each partition form the basis of mpsr models. it is straightforward that the rank of each submatrix is at most the rank of the full matrix d. in the worst case  the ranks of all the submatrices are the same as the rank of d  in which case the resulting mpsr model may have  many  more parameters than the psr model. but in other cases the ranks of the submatrices may be much smaller than the rank of d and then the resulting mpsr model may be more compact than the psr model. we provide examples of both cases in section 1. the size of the model is important because a model with fewer parameters should in general be more efficient  i.e.  require less data  to learn. we also test this empirically in section 1.
¡¡in this paper we do not address the question of automatically discovering useful ways of partitioning histories and instead assume that partitions correspond to history suffixes of some fixed length.
¡¡definition of memory: for this paper  given an mpsr using length k suffixes  a memory is a specific sequence of length k that identifies a partition. so  when considering a history h and memories of length one  the memory at h is just the last  most recent  observation in h  and there will be |o| memories in the mpsr. for memories of length two  the memory at h is the last  most recent  action-observation pair in h  and there will be |a|   |o| memories in the mpsr. in general  the set of possible memories in an mpsr that uses length-k suffixes is the set of all length-k sequences of alternating actions and observations that end in an observation; we will denote the size of such a set as mk.
¡¡let ¦Ì1 ...¦Ìmk represent all the distinct memories in an mpsr model. also  let the memory at history h be denoted ¦Ì h . each memory ¦Ìi has a corresponding submatrix d¦Ìi created by the partition of histories corresponding to ¦Ìi. the core tests for partition d¦Ì are referred to as ¦Ì-core tests to distinguish them from the core tests of the psr model of the same system. thus  the ¦Ì-core tests for the submatrices d¦Ì1 ...d¦Ìmk are denoted q¦Ì1 ...q¦Ìmk respectively. the prediction vector p q¦Ì h |h  contains the predictions for the ¦Ì-core tests q¦Ì h  at memory ¦Ìh.
¡¡definition of mpsr state: the mpsr state at history h is denoted by the concatenation of the memory at h and the prediction vector for that memory at h  i.e.  as  ¦Ì h  p q¦Ì h |h  . thus  the mpsr state contains both a memory of the past as well as predictions about the future while a psr state contains only predictions about the future.
¡¡definition of mpsr update parameters: for each memory  ¦Ìi  we will keep a set of matrices mao¦Ìi and vectors m¦Ìaoi for all a  o. there is a subtle issue in defining these parameters. it is not the case that each submatrix is a separate dynamical system. indeed if the memory corresponding to current history h is say ¦Ìi and we take action a and observe o  the memory corresponding to the next history hao could be different from ¦Ìi. lets say it is ¦Ìj. thus the update parameters mao¦Ìi must transform the current prediction vector that makes prediction for ¦Ì-core tests q¦Ìi in history h to the prediction vector for ¦Ì-core tests q¦Ìj in history hao. note that all histories belonging to memory ¦Ìi will transition to the same memory ¦Ìj for action-observation pair ao  i.e.  j is uniquely determined by i and the pair ao; and so the model parameter mao¦Ìi need not explicitly reference to the next memory ¦Ìj. thus one can define the state update for mpsrs as follows: upon taking action a in history h and observing o

where ¦Ì h  = ¦Ìi and ¦Ì hao  = ¦Ìj. the matrix mao¦Ìi is of size  |q¦Ìi|¡Á|q¦Ìj|  and the vector m¦Ìaoi is of size  |q¦Ìi|¡Á1 . as in psrs the update parameters in mpsrs have meaning. for instance  the kth column of mao¦Ìi is the  |q¦Ìi| ¡Á 1 -sized constant parameter vector that computes the prediction for the test that is the kth ¦Ì-core test for memory ¦Ìj.
¡¡the mpsr model parameters allow linear computation of the prediction for any test t = {a1 ...akok} at history h as follows:
	.	 1 
where ¦Ì1 is the memory corresponding to history h  ¦Ì1 is the memory corresponding to history ha1 and so on.
¡¡definition of mpsr: an mpsr model is defined by the tuple ha o ¦Ì1 ...¦Ìmk q¦Ì1 ...q¦Ìmk m  ¦Ì1 p q¦Ì1|   i where a is the set of actions; o is the set of observations; ¦Ì1 ...¦Ìmk are the possible memories; q¦Ìi is the set of ¦Ìcore tests for memory ¦Ìi; m is the set of update parameters mao¦Ìi and m¦Ìaoi for all a  o  ¦Ìi; ¦Ì1 is the initial memory; and p q¦Ì1|   is the initial prediction vector.
¡¡a special and interesting case arises when a memory by itself serves as state and no predictions are needed.
¡¡definition of landmark: given an mpsr  a landmark is a memory that serves as state  i.e.  is a sufficient statistic of history. landmarks can be quite beneficial for making accurate predictions. we will discuss the identification of landmarks and exploiting them for prediction in section 1.
1 basic theory of mpsrs
our first result is to show that for any dynamical system it is possible to find ¦Ì-core tests for all memories in an mpsr model from the set q of core tests for the corresponding psr model.
lemma 1. let a dynamical system be modeled as a psr with core tests q. then there always exists an mpsr model for the system such that for each memory ¦Ìi the corresponding ¦Ìcore tests q¦Ìi satisfy q¦Ìi   q.
proof. we provide a constructive proof which shows how to derive the subset q¦Ìi from q. recall that all columns of d are linear combinations of the columns corresponding to q in d. consider the submatrix d¦Ìi for memory ¦Ìi. it must be the case that all columns of d¦Ìi are linear combinations of the columns corresponding to q in d¦Ìi. thus  there exists q¦Ìi   q.	
¡¡in this paper  we don't exploit lemma 1 for two reasons: 1  the core tests of psrs are not unique  and 1  in any case when learning a model of a dynamical system from experience data we do not have a psr model and thus its core tests q to begin with.
lemma 1. for any dynamical system of finite dimension and any choice of  fixed-length suffix  memories  the size of the resulting mpsr model for that system is at most the number of memories times the size of a psr model for that system.
proof. in the worst case  the rank of each of the submatrices in the mpsr is exactly the rank of the full system dynamics matrix. in such a case  if we happen to find different ¦Ì-core tests for each submatrix  then the model for each submatrix will be exactly as large as the psr model. 
¡¡the above lemma holds no matter how one chooses the memories. but what if we can choose the memories to use in constructing an mpsr judiciously  by which we mean to minimize the size of the resulting model  
theorem 1. with a judicious choice of memories  the size of the corresponding mpsr for any dynamical system is at least as compact as the size of the psr for that dynamical system. furthermore  there exist dynamical systems for which some choice of memories leads to an mpsr model that is more compact than the psr model of the same dynamical system.
proof. the proof of the first statement follows from the fact that a psr is also an mpsr with the null set as memories. we prove the second statement by constructing such a dynamical system. table 1 compares the size of mpsrs and psrs for various standard test dynamical systems  based on pomdps . in all instances  a suffix of length one was used to partition the histories. for at least three problems  cheese  shuttle and four-three  the mpsr is more compact than the psr. 
1 landmarks
we show that landmarks are equivalent to memories for which there is only one ¦Ì-core test and furthermore that the prediction of any test at a landmark is the same at all histories that map to that landmark.
lemma 1. for an mpsr model of a dynamical system 
  any memory that is a landmark has only one ¦Ì-core test  and every memory that has a ¦Ì-core set of size one is a landmark.
  for all histories that map to a landmark the prediction vector is a constant  i.e.  independent of history.
  for all histories that map to a landmark the prediction of any test t is a constant  i.e.  independent of history.
proof. if ¦Ì is a landmark  then the predictions for all tests are fully determined when ¦Ì is known. therefore  at any two histories that have memory ¦Ì  the prediction of every test is the same at both histories. this means that every row of d¦Ì must be identical  so the rank of d¦Ì is one  and only one ¦Ì-core test exists for ¦Ì.
¡¡for a memory ¦Ì that has a single ¦Ì-core test  q¦Ì  let h¦Ì denote the histories that map to memory ¦Ì. for any h ¡Ê h¦Ì  and any action a  it must hold that po p ao|h  = 1 which implies that po p q¦Ì|h tm¦Ìao = 1  which in turn implies that p q¦Ì|h t = 1/po m¦Ìao. recall that m¦Ìao are independent of history  and thus the prediction of q¦Ì must be independent of table 1: comparison of the size of psrs and mpsrs
problem# core tests# param. entriespsrmpsrpsrmpsrtiger1 11paint1 11cheese1 1 1 11network1  11bridge1 1 11shuttle1 1 11four three1 1 1 11float reset1 11the history. this calculation exploits the fact that the prediction vector and update parameters are scalars.
¡¡furthermore  because the prediction of q¦Ì is independent of history  and the update vector m¦Ìt for any test t is independent of history  the prediction p t|h  = p q¦Ì|h tm¦Ìt of t at history h must also be independent of history. so  all rows of d¦Ì must be identical  and ¦Ì is therefore a landmark. 
¡¡landmarks come in handy because they can be used to keep a learned - and therefore only approximately correct - model from progressively drifting farther and farther away from reality as we make longer and longer term predictions from such a model. every time we observe a landmark memory  we can reset the prediction vector to the constant corresponding to the landmark irrespective of the actual observed history. this can keep the error in the prediction for very long tests from growing with the length of the test. we exploit this ability in our empirical results below.
1 learning mpsr models from data
in this section we present an algorithm for learning mpsr models from data under the assumption that the algorithm has the ability to reset the dynamical system being modeled to an initial state. we present the algorithm in two stages. in the first stage  we will assume that the algorithm has access to an oracle that if given a history h and test t will return the prediction p t|h  for the dynamical system. in the second stage we will show how the oracle can be replaced by the use of sample data from the dynamical system with reset. because we wish to minimize the amount of sample data needed by our algorithm  we will attempt to minimize the number of calls to the oracle in the first stage. our algorithm is a relatively straightforward adaptation to mpsrs of the algorithm presented by james and singh  for psrs.
1 oracle based algorithm
recall that there are two components to an mpsr: the ¦Ì-core tests for the different memories and the model-update parameters for the memories. in our empirical work below  we always use memories of length one. clearly the model parameters for a memory depend on the choice of ¦Ì-core tests for that memory  recall that the ¦Ì-core tests are not unique . the process of computing ¦Ì-core tests and update parameters for each memory is identical and so we can just discuss how to do this for any one memory  say ¦Ìi.
determining ¦Ì-core tests and histories
the algorithm proceeds in iterations. at iteration t  the algorithm has a current set of linearly independent or ¦Ì-core tests and histories for each memory. let ht¦Ìi  tt¦Ìi  be the set of ¦Ì-core histories  ¦Ì-core tests  found by iteration t for memory ¦Ìi. these ¦Ì-core tests  histories  start off as the empty set at the first iteration. we also keep a set of candidate tests and histories at each iteration. the set of candidate tests for memory ¦Ìi is the set of one-step tests; and for every ao pair  the set of ao-extensions for all the current ¦Ì-core tests for memory ¦Ìj  where ¦Ìj is the next memory when action a is taken in memory ¦Ìi and o is observed. the set of candidate histories is similarly generated. we ask the oracle for the predictions of all the candidate tests at all the candidate histories. we compute the rank of this oracle-generated matrix  denoted xt. the linearly independent rows and columns of xt become the new and respectively. in selecting the new ¦Ì-core tests and histories we always include the previous ¦Ì-core tests and histories. we stop at iteration s if the rank of xs is the same as the rank of xs 1 for all memories. the above algorithm is an adaptation of the similar algorithm for determining core test and histories for psrs by james and singh . just like for psrs this algorithm is not guaranteed to find all ¦Ì-core tests and histories  but also just like for psrs it seems to work for all empirical work done so far.
computing the model-update parameters now we discuss how to compute the model-update matrix mao¦Ì for any ao pair and any memory ¦Ì. we define a matrix a for memory ¦Ì that contains the predictions for all the ¦Ì-core tests at all the ¦Ì-core histories for memory ¦Ì. this matrix will have full rank and is computed in the algorithm above. let ¦Ìj be the memory achieved when action a is taken in memory ¦Ì and o is observed. the kth column of mao¦Ì   denoted x  is the constant parameter vector that computes the prediction for the ao-extension of the kth ¦Ì-core test  denoted y  of memory ¦Ìj. let b be the column vector of predictions of test y for the ¦Ìcore histories of memory ¦Ì. we ask the oracle for the entries of b. then from equation 1 above  ax = b. since a is full rank  it is invertible and hence x = a 1b. we can use the same idea to compute the model-update parameter vector m¦Ìao.
¡¡in the next section we show how the oracle needed for the above algorithm can be replaced by using data from the dynamical system with reset.
1 learning mpsrs from sample data


	g	h

figure 1: results of running mpsr learning algorithms on a suite of test problems. see text for details.we show how to obtain an estimate for the prediction p t|h   something we went to the oracle for in the previous algorithm. we generate samples from the distribution p t|h  by first taking the action sequence in h after a reset. if the observation sequence in h happens  then we have succeeded in generating history h. if we don't succeed in generating the history  we reset the system and try again. if we do succeed in generating the history  we take the action sequence in test t. if the observation sequence in test t happens  the test succeeds  else the test fails. we get an estimate of p t|h  from the empirical success rate of the test t at history h. of course  this is extremely wasteful in the number of data samples needed. we implement this basic idea much more efficiently by executing the action sequence in ht regardless of the observations obtained and then mining the data generated for all possible history and test pairs that can be extracted from it.
¡¡one problem that occurs with this estimation technique is that the sampled entries of the system dynamics matrix will almost certainly result in inaccurate calculations of rank  needed in the above oracle-based algorithm . we use a more robust rank calculation procedure developed in the psr learning algorithm by james and singh . the central idea is that we can use the number of samples that went into each entry of the matrix we wish to find the rank of to generate a threshold on singular values obtained via singular value decomposition. this threshold is more conservative when we have fewer samples and generally leads to a smaller rank than otherwise. as more samples are included  the threshold becomes less conservative and the calculated rank is closer to the straightforward calculation of rank.
1 empirical results
we conducted experiments with a view towards validating four major ideas presented above. the first experiments are meant to compare the relative size of psrs and mpsrs. the second is to test whether the oracle-based algorithm for computing mpsrs finds exact mpsr models. the third is to test the efficacy of the sample-data based learning algorithm for mpsrs. finally  we are also interested in whether using landmarks  where available  to reset prediction vectors during the process of computing predictions for long test sequences provides a measurable benefit. all of these experiments were on a suite of standard pomdp-based dynamical systems  cassandra  1  that have been used for both pomdp-learning as well for psr-learning.
¡¡again  for all of the results presented below  the memories used for the mpsr models were the most recent observation.
1 comparing psrs and mpsrs
for psrs and mpsrs  the size of the model can be measured both in terms of the number of core tests  ¦Ì-core tests   and in the number of entries in the model parameters. table 1 compares the sizes of psr and mpsr models for the suite of test problems. for mpsrs  the number of ¦Ì-core tests at each memory is listed. fully half of the test problems have landmarks  memories with only one ¦Ì-core test   indicating that this may be a fairly common situation.
¡¡the number of entries in the model parameters are listed for both psrs and mpsrs. on three of the problems  cheese  shuttle  and four-three  there are significantly fewer entries needed for the mpsr model than for the psr model; on three other problems  tiger  bridge  and paint  there are significantly more; and on the two remaining problems there are approximately an equal number needed. this proves that mpsrs can be more compact than psrs. this also illustrates that the unless we choose memories wisely  mpsrs can also be less compact than psrs.
1 constructing mpsrs from exact predictions
we tested the oracle algorithm for computing mpsrs by providing access to the true predictions computed analytically for the test problems. as we mentioned in section 1  this algorithm is not guaranteed to find all ¦Ì-core tests for every memory. we found that in fact for all the test problems  the algorithm does indeed find all the ¦Ì-core tests and histories for all the models. we also verified that the resulting mpsr models were perfect  in the sense that any prediction error was due to machine round-off error. furthermore  because correct sets of ¦Ì-core tests were discovered  all landmarks were identified.
1 learning mpsrs from samples
here we present the results of learning mpsr models for all our dynamical systems. as mentioned above we assumed a reset. this makes our results directly comparable to the results of james and singh  on the same set of problems under the same reset assumption. we applied the algorithm of section 1  stopping every so often during learning to test the model learned so far. for each test of the learned model  we asked the model to make predictions while tracking the system for 1 steps  with randomly chosen actions . the test error reported is the average error in the one-step predictions of the model relative to the true predictions. this is the same error measure as used in james and singh .
¡¡the results of this testing are presented in figure 1. we compare our results to the results for psr learning for the same dynamical systems from james and singh . plots a to d present the results for the systems without any landmarks  and show the results of psr and mpsr learning. in two of these systems  psr learning is more efficient while in the other two they are roughly equivalent. we note that in all of these problems the psr models are more compact or equal in size than the mpsr models. plots e to h are for systems with landmarks. here we plot three results: psr learning  mpsr learning and mpsr learning with landmarks. the last algorithm  mpsr learning with landmarks  uses the mpsr learning algorithm of section 1 but during testing resets the prediction vector when a landmark memory is encountered to the constant prediction for that memory. in three of the systems corresponding to graphs e  g and h  the size of the mpsr model is smaller than the psr model. as we surmised in section 1 this leads to mpsr learning being more efficient than psr learning. in the system for plot f the two are roughly equivalent as are their sizes. the effect of using landmarks is most easily seen in plots g and h. here mpsr learning without using landmarks is very noisy for if the system gets off-track in its predictions the error accumulates rapidly as the test sequence lengthens  while using the landmarks stabilizes the error and gives the best results.
1 conclusion
in this paper we proposed a new class of models  mpsrs  that combines memory of past observations and predictions of future observations into its state representation. we showed that mpsrs are as expressive as and can be more compact than psrs that solely use predictions of future observations in its state representation. our preliminary empirical results showed that in dynamical systems where the mpsr model of the system is more compact than the psr model  learning the mpsr model is more efficient than learning the psr model. finally  we formalized the notion of landmark memories and demonstrated how to find them and how to exploit them in making long-term predictions.
¡¡as future work  we will explore the interesting challenge of automatically finding good memories to combine with predictions of tests to make compact and efficiently learnable mpsr models.
acknowledgements the research reported in this paper was supported by nsf grant iis-1. the authors also thank rich sutton for helpful comments.
