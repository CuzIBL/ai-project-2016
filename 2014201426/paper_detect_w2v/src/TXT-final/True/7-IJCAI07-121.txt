
most methods for classifier design assume that the training samples are drawn independentlyand identically from an unknown data generating distribution  although this assumption is violated in several real life problems. relaxing this i.i.d. assumption  we consider algorithms from the statistics literature for the more realistic situation where batches or sub-groups of training samples may have internal correlations  although the samples from different batches may be considered to be uncorrelated. next  we propose simpler  more efficient  variants that scale well to large datasets; theoretical results from the literature are provided to support their validity. experimental results from real-life computer aided diagnosis  cad  problems indicate that relaxing the i.i.d. assumption leads to statistically significant improvements in the accuracy of the learned classifier. surprisingly  the simpler algorithm proposed here is experimentally found to be even more accurate than the original version.
1 introduction
most classifier-learning algorithms assume that the training data is independently and identically distributed. for example  support vector machine  svm   back-propagation for neural networks  and many other common algorithms implicitly make this assumption as part of their derivation. nevertheless  this assumption is commonly violated in many reallife problems where sub-groups of samples exhibit a high degree of correlation amongst both features and labels.
¡¡in this paper we:  a  experimentally demonstrate that accounting for the correlations in real-world training data leads to statistically significant improvements in accuracy;  b  propose simpler algorithms that are computationally faster than previous statistical methods and  c  provide links to theoretical analysis to establish the validity of our algorithm.
1 motivating example: cad
although overlookedbecause of the dominance of algorithms that learn from i.i.d. data  sample correlations are ubiquitous in the real world. the machine learning community frequently ignores the non-i.i.d. nature of data  simply because we do not appreciate the benefits of modeling these correlations  and the ease with which this can be accomplished algorithmically. for motivation  consider computer aided diagnosis  cad  applications where the goal is to detect structures of interest to physicians in medical images: e.g. to identify potentially malignant tumors in ct scans  x-ray images  etc. in an almost universal paradigm for cad algorithms  this problem is addressed by a 1 stage system: identification of potentially unhealthy candidate regions of interest  roi  from a medical image  computation of descriptive features for each candidate  and classification of each candidate  e.g. normal or diseased  based on its features. often many candidate roi point to the same underlying anatomical structure at slightly different spatial locations.
¡¡under this paradigm  correlations clearly exist among both the features and the labels of candidates that refer to the same underlying structure  image  patient  imaging system  doctor/nurse  hospital etc. clearly  the candidate roi acquired from a set of patient images cannot be assumed to be iid. multiple levels of hierarchical correlations are commonly observed in most real world datasets  see figure 1 .
1 relationship to previous work
largely ignored in the machine learning and data mining literature  the statistics and epidemiology communities have developed a rich literature to account for the effect of correlated samples. perhaps the most well known and relevant models for our purposes are the random effects model  rem    and the generalized linear mixed effects models  glmm  . these models have been mainly studied from the point of view of explanatory data analysis  e.g. what is the effect of smoking on the risk of lung cancer   not from the point of view of predictive modeling  i.e. accurate classification of unseen test samples. further  these algorithms tend to be computationally impractical for large scale datasets that are commonly encountered in commercial data mining applications. in this paper  we propose a simple modification of existing algorithms that is computationally cheap  yet our experiments indicate that our approach is as effective as glmms in terms of improving classification accuracy.
1 intuition: impact of sample correlations

figure 1: a mixed effects model showing random and fixed effectssimplified thought experiment consider the estimation of the odds of heads for a biased coin  based on a set of observations. if every observation is an independent flip of a coin  then this correspondsto an i.i.d. assumption for the data; based on this assumption one can easily build estimators for our binomial data. this would work fine so long as the observations reported to the statistician are really true to the underlying coin flips  and provided the underlying data generating mechanism was truly binomial  and not from some other distribution that generates  outliers  as per the binomial model . however  suppose that the experimenterperformingthe experiment reports something else to the statistician. after certain coin flips  the experimenter reports the result of one coin flip observation as if it occurred many times  but only if he observed  heads  on those occasions . for other observations he reports the occurrences exactly once. then the simple binomial estimator  designed using the i.i.d. assumption  would not be appropriate. on the other hand  if every occurrence of an i.i.d. sample is repeated the same number of times  then we are essentially immune to this effect. although this example may seem simplistic  one should bear in mind that logistic regression  gaussian processes and most other classifiers essentially rely on the same binomial distribution to derive the likelihood that they maximize in the training step.
¡¡implications for classifier learning the implicit assumption in the machine learning communityseems to be that even if iid assumptions are violated  the algorithms would work well in practice. when would this not be the case 
¡¡the first intuition is that outliers  e.g. mis-labeled samples   do not systematically bias the estimation of the classifier during training  provided they are truly i.i.d. and drawn from a fairly symmetric distribution. these outliers introduce a larger variance in the estimation process  but this can be largely overcomesimply by increasing the sample sizes of the training set. on the other hand  if outliers  and samples  are systematically correlated  they do introduce a systemic bias in the estimation process  and their effect remains even if we have a large amount of training data.
¡¡for explaining the second intuition  let us first consider a practical situation occurring in cad problems. due to the way the candidate generation  cg  algorithms for identifying roi are designed  some diseased structures  e.g. wall attached nodules in a lung  may be identified by many candidates that are spatially close in the image; i.e.  all these candidates will refer to the same underlying physiological region  e.g. the same lung nodule . moreover  some other types of structures  e.g. non-wall attached nodules  may be associated with only one or two candidates  again due to the fundamental properties and biases of the candidate generation  cg  algorithm. the occurrence frequency & other statistical properties of the underlying structural causes of the disease are systematically altered if we naively treated all data as being produced from i.i.d. data sources. as a result  a systematic bias is introduced into the statistical classifier estimation algorithm that learns to diagnosediseases  and this bias remains even when we have large amounts of training data.
¡¡when does the violation of i.i.d. assumptions not matter  clearly  if the correlations between samples is very weak  we can effectively ignore them  treating the data as i.i.d. . further  even if the correlations are not weak  if we do not much care for outlier immunity  and if each sub-type or subpopulation occurs with similar or almost identical frequency  then we should be able to ignore these effects.
1 random & mixed effects models
consider the problem shown in figure 1. we are given a training dataset d = { xijk yijk }  where is the feature vector for the ith candidate in the jth patient of the kth hospital and yijk ¡Ê { 1} are class labels. let us also assume without loss of generality that the indexing variable follow.
in generalized linear models  glm   such as logistic or probit regression  one uses a nonlinear link function-e.g. the logistic sigmoid function ¦Ò r  = 1/ 1 + exp  r   -to express the posterior probability of class membership as:
	.	 1 
¡¡however  this mathematically expresses conditional independence between the classification of samples and ignores the correlations between samples from the same patient  j k  or from the same hospital k. this limitation is overcome in a generalizedlinear mixed effects model  glmm  by postulating the existence of a pair of random variables that explain the patient specific effect ¦Äj k and a hospital specific effect ¦Äk. during training  one not only estimates the fixed effect parameters of the classifier ¦Á  but also the distribution of the random effects p ¦Äk ¦Äj k|d . the class prediction becomes:

¡¡in bayesian terms  classification of a new test sample x with a previously trained glm involves marginalization over the posterior of the classifier's fixed effect parameters:

the extension to find the classification decision on a test sample in a glmm is quite straight-forward:
d¦Ád¦Á1d¦Äkd¦Äj k
notice that the classification predictions of sets of samples from the same patient  j k  or from the same hospital k are no longer independent. during the testing phase  if one wants to classify samples from a new patient or a new hospital not seen during training  so the appropriate random effects are not available   one may still use the glm approach  but rely on a marginalized version of the posterior distributions learnt for the glmm:

a note of explanation may be useful to explain the above equation. the integral in  1  is over ¦Äk and ¦Äj k  for all j and k  i.e. we are marginalizing over all the random effects in order to obtain p ¦Á ¦Á1|d  and then relying on  1 . clearly  training a glmm amounts to the estimation of the posterior distribution  p ¦Á ¦Á1 ¦Äk ¦Äj k|d .
1 avoiding bayesian integrals
since the exact posterior can not be determined in analytic form  the calculation of the above integral for classifying every test sample can prove computationally difficult. in practice  we can use markov chain monte carlo  mcmc  methods but they tend to be too slow for data mining applications. we can also use approximate strategies for computing bayesian posteriors such as the laplace approximation  variational methods or expectation propagation  ep . however  as we see using the following lemma  adapted from a different context    if this posterior is approximated by a symmetric distribution e.g. gaussian  and we are only interested in the strict classification decision rather than the posterior class membership probability  then a remarkable result holds: the glmm classification involving a bayesian integration can be replaced by a point classifier.
¡¡first we define some convenient notation  which will use bold font to distinguish it from the rest of the paper . let us denote the vector combining ¦Á  ¦Á1 and all the random effect variables as
w.
let us define the augmented vector combining the feature vector x  the unit scalar 1 and a vector 1 of zeros- whose length corresponds to the number of random effect variables-as x . next  observe that if one only requires a hard classification  as in the svm literature  the sign    function is to be used for the link. finally  note that the classification decision in  1  can be expressed as:
e p y = 1.
lemma 1 for any sample x  the hard classification decision of a point classifierx  is identical to that of a bayesian voting classification
		 1 
 is a symmetric distribution with respect to
symmetric reflection of ww ww . w w w w  that is  if q  |   = q  |    where ¡Ô 1   is the
in the domain of the integral. since ww  we see proof: for every w in the domain of the integral  is also that w  x. three cases have to be considered:
case 1: when   or 
	and	to the
	1	w
.
case 1: when x  it follows that 
. thus 
		 	
x .
case 1: 
x .
case 1 or 1 will occur at least some times. hence proved.w	 unless = 1  in which case the classifier in undefined  
¡¡assumptions & limitations of this approach: this lemma suggests that one may simply use point classifiers and avoid bayesian integration for linear hyper-plane classifiers. however  this is only true if our objective is only to obtain the classification: if we used some other link function to obtain a soft-classification then the above lemma can not be extended for our purposes exactly  it may still approximate the result .
¡¡secondly  the lemma only holds for symmetric posterior distributions p ¦Á ¦Á1 ¦Äk ¦Äj k|d . however  in practice  most approximate bayesian methods would also use a gaussian approximation in any case  so it is not a huge limitation. finally  although any symmetric distribution may be approximated by a point classifier at its mean  the estimation of the mean is still required. however  for computing a laplace approximation to the posterior  one simply chooses to approximate the mean by the mode of the posterior distribution: the mode can be obtained simply by maximizing the posterior duringmaximum a posteriori  map  estimation. this is computationally expedient & works reasonably well in practice.
1 proposed algorithm
1 augmenting feature vectors  afv 
in the previous section we showed that full bayesian integration is not required and a suitably chosen point classifier would achieve an identical classification decision. in this section  we make practical recommendations about how to obtain these equivalent point classifiers with very little effort  using already existing algorithms in an original way.
¡¡we propose the following strategy for building linear classifiers in a way that uses the known  or postulated  correlations between the samples. first  for all the training samples  construct augmented feature vectors with additional features correspondingto the indicator functionsfor the patient & hospital identity. in other words  to the original feature vector of a sample x  append a vector that is composed of zero in all locations except the location of the patient-id  and another corresponding to the location of the hospital-id  and augment the feature vector with the auxiliary features by .
¡¡next use this augmented feature vector to train a classifier using any standard training algorithm such as fisher's discriminant or svms. this classifier will classify samples based not only on their features  but also based on the id of the hospital and the patient. viewed differently  the classifier in this augmented feature space not only attempts to explain how the original features predict the class membership  but it also simultaneously assigns a patient and hospital specific  random-effect  explanation to de-correlate the training data.
¡¡during the test phase  for new patients/hospitals  the random effects may simply be ignored  implicitly this is what we used in the lemma in the previous section . in this paper we implement the proposed approach for fisher's discriminant.
1 fisher's discriminant
in this section we adopt the convex programmingformulation of fisher's discriminant fd  presented in  

	s.t.		 1 
where l z  ¡Ô  logp z  be the negative log likelihood associated with the probability density p. the constraint  where  is the total
number of labeled samples  pulls the output for each sample to its class label while the constraints ensure that the average output for each class is the label  i.e. without loss of generality the between class scattering is fixed to be two. the first term in the objective function minimizes the within class scattering whereas the second term penalizes models ¦Á that are a priori unlikely.
¡¡setting  i.e. assuming a zero mean gaussian density model with unit variance for p ¦Î|c¦Î  and p ¦Á|c¦Î  and using the augmented feature vectors we obtain the following optimization problem. fisher's discriminant with augmented feature vectors  fd-afv :

where for the first set of constraints  k runs from 1 to k  j from 1 to pk for each k  i from 1 to jk for each pair of  j k   ¦Îc is a vector of ¦Îijk corresponding to class c and ¦Ä is the vector of model parameters for the auxiliary features.
1 parameter estimation
depending on the sensitivity of the candidate generation mechanism  a representative training dataset might have on the order of few thousand candidates of which only few are positive making the training data very large and unbalanced between classes. to account for the unbalanced nature of the data we used different tuning parameters  c¡¥¦Î+  c¡¥¦Î  for the positive and negative classes. to estimate these and c¡¥¦Á  c¡¥¦Ä  we first coarsely tune each parameter independently and determine a range of values for that parameter. then for each parameter we consider a discrete set of three values. we use 1-fold cross validation on the training set as a performance measure to find the optimum set of parameters.
1 experimental studies
for the experiments in this section  we compare three techniques: naive fisher's discriminant  fd   fd-afv  and glmm  implemented using approximate expectation propagation inference . we compare fd-afv and glmm against fd to see if these algorithms yield statistically significant improvements in the accuracy of the classifier. we also study if the computationally much less expensive fd-afv is comparable to glmm in terms of classifier sensitivity.
¡¡for most cad problems  it is important to keep the number of false positives per volume at a reasonable level  since each candidate that is marked as positive by the classifier will then be visually inspected by a physician. for the projects described below  we focus our attention on the region of the receiver operating characteristics  roc  curve with fewer than 1 false positives per volume. this roughly corresponds to 1% specificity for both datasets.
1 experiment 1: colon cancer
problem description: colorectal cancer is the third most common cancer in both men and women. it is estimated that in 1  nearly 1 cases of colon and rectal cancer will be diagnosed in the us  and more than 1 people would die from colon cancer . while there is wide consensus that screening patients is effectivein decreasing advanced disease  only 1% of the eligible population undergoes any colorectal cancer screening. there are many factors for this  key being: patient comfort  bowel preparation and cost.
¡¡non-invasive virtual colonoscopy derived from computer tomographic  ct  images of the colon holds great promise as a screening method for colorectal cancer  particularly if cad tools are developed to facilitate the efficiency of radiologists' efforts in detecting lesions. in over 1% of the cases colon cancer progressed rapidly is from local  polyp adenomas  to advanced stages  colorectal cancer   which has very poor survival rates. however  identifying  and removing  lesions  polyp  when still in a local stage of the disease  has very high survival rates   hence early diagnosis is critical.
¡¡this is a challenging learning problem that requires the use of a random effects model due to two reasons. first  the sizes of polyps  positive examples  vary from 1 mm to all the way up to 1 mm: as the size of a polyp gets larger  the number of candidates identifying it increases  these candidates are highly correlated . second  the data is collected from 1 patients across seven different sites. factors such as patient anatomy/preparation  physician practice and scanner type vary across different patients and hospitals-the data from the same patient/hospital is correlated.
¡¡dataset: the database of high-resolution ct images used in this study were obtained fromnyu medicalcenter  cleveland clinic foundation  and five eu sites in vienna  belgium  notre dame  muenster and rome. the 1 patients were randomly partitioned into training  n=1 patients  with 1 polyps among 1 candidates  and test  n=1 patients  with 1 polyps among 1 candidates  groups. the test group was sequestered and only used to evaluate the performance of the final system. a combined total of 1 features are extracted for each candidate.
¡¡experimental results: the roc curves obtained for the three techniques on the test data are shown in figure 1. to better visualize the differences among the curves  the enlarged views corresponding to regions of clinical significance are plotted. we performed pair-wise analysis of the three curves to see if the difference between each pair for the area underthe roc-curveis statistically significant  p valuescomputed using the technique described in  . statistical analysis indicates that fd-afv is more accurate than fd with a p-value of 1  glmm will be more accurate than fd with a p-value of 1 and fd-afv will be more accurate than glmm with a p-value of 1. the run times for each algorithm are shown in table 1.
1 experiment 1: pulmonary embolism
data sources and domain description
pulmonary embolism  pe   a potentially life-threatening condition  is a result of underlying venous thromboembolic disease. an early and accurate diagnosis is the key to survival. computed tomography angiography  cta  has merged as an accurate diagnostic tool for pe. however  there are hundreds of ct slices in each cta study. manual reading is laborious  time consuming and complicated by various pe look-alikes  false positives  including respiratory motion artifact  flowrelated artifact  streak artifact  partial volume artifact  stair step artifact  lymph nodes  vascular bifurcation among many others   . several computer-aided detection cad  systems are developed to assist radiologists in this process by

figure 1: comparing fd  fd-afv on the colon testing data 
.
table 1: comparison of training times in seconds for fd  fdafv and glmm for pe and colon training data
algorithmtime  colon time  pe fd1fd-afv1glmm1helping them detect and characterize emboli in an accurate  efficient and reproducible way   .
¡¡an embolus forms with complex shape characteristics in the lung making the automated detection very challenging. the candidate generation  cg  algorithm searches for intensity minima. since each embolus is usually broken into several smaller units  cg picks up several points of interest in the close neighborhood of an embolus from which features are extracted. thus multiple candidates are generated while characterizing an embolus by the cg algorithm.
¡¡in addition to usual patient and hospital level random effects  here we observe a more concrete example of random effects in cad; the samples within the close neighborhood of an embolus in a patient are more strongly correlated than those far from the embolus. this constitutes a special case of patient-level random effects. all the samples pointing to the same pe are labeled as positive in the ground truth and are assigned the same pe id. we have collected 1 cases with 1 pes marked by expert chest radiologists at two different institutions. they are randomly divided into two sets: training  1 cases with 1 clots generatinga total of 1 candidates  and testing  1 cases with 1 clots generating a total of 1 candidates . the test group was sequestered and only used to evaluate the performance of the final system. a combined total of 1 features are extracted for each candidate.
experimental design and results:
the roc curves obtained for the three techniques on the test data are shown in figure 1. the statistical analysis indicate

figure 1: comparing fd  fd-afv on the pe testing data 
.
that fd-afv will be more sensitive than fd with a p-value of 1  glmm will be more sensitive than fd with a p-value of 1 and fd-afv will be more sensitive than glmm with a p-value of 1. even though statistical significance can not be proved here  a p-value less than 1 is required   a pvalue of 1 clearly favors fd-afv over fd. note also that the roc curve corresponding to fd-afv clearly dominates that of fd in the region of clinical interest. when the roc curves for fd-afv and glmm are compared we see that the difference is not significant. however  using the proposed augmented feature vector formulation we were able account for the random effects with much less of computational effort than is required for glmm.
1 conclusion
summary: the basic message of this paper is: there is a standard i.i.d. assumption that is implicit in the derivation of most classifier-training algortihms. by allowing the classifier to explain the data based on both the randomeffects  common to many samples  and the fixed effects specific to a each sample  the learning algorithm can achieve better performance. if training data is limited  fully bayesian integration via mcmc algorithms can help improve accuracy slightly  but significiant gains can be reaped without even that effort. all that a practitioner needs to do is to create a categorical indicator variable for each random effect.
¡¡contributions: generalized linear mixed effects models have been extensively studied in the statistics and epidemiology communities. the main contribution of this paper is to highlight the problem and solutions to the machine learning & data mining community: in our experiments  both glmms and the proposed approach improve the classification accuracy as compared to ignoring the inter-sample correlations. our secondary contribution is to propose a very simple and extremely efficient solution that can scale well to very large data mining problems  unlike the current statistics approach to glmms that are computationally infeasible for the large datasets. despite the computational speedup  the classification accuracy of the proposed method was no worse than that of glmms in our real-life experiments.
¡¡applicability: although we describes our approach in a medical setting in order to make it more concrete  the algorithm is completely general and is capable of handling any hierarchical correlations structure among the training samples. though our experiments focussed on fisher's discriminants in this study  the proposed model can be incorporatedinto any linear discriminant function based classifier.
