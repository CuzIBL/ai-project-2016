 
in order to control the motion of a mobile robot  it is necessary to have accurate egomotion parameters. in addition  egomotion parameters are useful in determining environmental depth and structure. we present a computationally inexpensive method that rapidly and robustly determines both the translational vector and rotational component of robot motion through the use of an active camera. we employ gaze control consisting of two types of camera motion. first  the camera fixates on an item in the environment  while measuring motion parallax. based on the measured motion parallax  the camera then rapidly saccades to a different fixation point. the algorithm iteratively seeks out fixation points that are closer to the translational direction of motion  rapidly converging so that the active camera will always point in the instantaneous direction of motion. at that point  the tracking motion of the camera is equal but opposite in sign to the robot's rotational component of motion. experiments are carried out both in simulation and in the real world  giving results that are close to the actual motion parameters of the robot 
1 	introduction 
when a mobile robot moves within and interacts with its environment  accurate egomotion parameters are required for tasks such as short-term control of the vehicle  as well as being useful in determining environmental depth and structure. egomotion based on inertial navigation systems and/or wheel encoders accumulate positional error and are unreliable for long distance navigation. visual information  on the other hand  can provide accurate motion parameters. 
　much work in visual motion analysis has concentrated primarily on recovering 1-d motion and scene structure from passively acquired 1-d image sequences  review given in  . the majority of the approaches are based on either determining dense optical flow accurately or matching a smaller set of discrete points between images. the computations involved are usually based on solving systems of non-linear  sometimes linear  equations. unfortunately  these algorithms are not too successful in the real world since they have high noise sensitivity  are sometimes unstable  and are computationally complex making them inadequate for real-time processing. 
　since determining general 1-d motion via vision is problematic  there has been much research concerning only translational motion  1 . when a visual sensor translates without rotation  all of the flow vectors in the time-varying images emanate from a single point known as the focus of 
   *this work was sponsored by the japan society for the promotion of science. expansion  foe . this point can be determined from the flow vectors  and allows us to determine the translational motion vector of the visual sensor. many algorithms have been developed that use the foe  such as determining depth to scene points via the time-to-adjacency relationship  1 . in short  the foe can provide reliable motion information as long as the motion is purely translational. 
　if even a small amount of rotation occurs during translational motion  the accuracy of the foe  if one can be found  deteriorates. effects of rotation on the foe have been given elsewhere  the general conclusion being that translational motion determined from an foe is highly inaccurate when rotation occurs . even still  since the foe is quite useful in determining translational motion  methodologies have been developed that first 'derotate' any effects of rotation  and then determine the foe as normal in the derotated images  1 1 . several methods have been proposed for determining the rotational component of motion  such as first finding vanishing points in the image  which are invariant to translational motion  and then monitoring their motion in order to determine the rotation  1 . another method derotates an image by first selecting different foe candidates  after which inverse rotational mappings are applied. the resulting image that is most radial  i.e.  has flow vectors emanating from the smallest area  contains the best foe candidate. using this method  a concept of a 'fuzzy foe' has been developed that indicates a highly probable area for the foe  rather than a single point  1 . this method not only provides a reasonable foe location  and thus translational information   but also the rotational component as well. however  the method requires a large number of steps that take a good deal of processing time. 
　in order to robustly determine both the translational direction and rotational component of robot motion in a rapid fashion  we present a computationally inexpensive method that uses an active camera mounted on a mobile platform. our method exploits the constraints imposed by actively controlling the camera's motion independently of the robot's motion. based on the animate vision paradigm   we control the gaze of the camera in order to fixate on items in the environment while the robot is moving. during fixation  we measure any motion parallax that occurs along the line of sight. we then perform saccadic movement  i.e.  rapid camera movement or jumping'  in order to select a new fixation point based on the previously measured motion parallax. the algorithm operates in an iterative fashion  repeatedly selecting and tracking fixation points and saccading in the direction of robot motion. the algorithm quickly converges so that the active camera always points in the instantaneous direction of robot motion. at that point  the tracking motion of the camera is equal but opposite in sign to the robot's rotational component of motion. this algorithm operate continuously and provides accurate egomotion parameters while the robot moves in its environment. 
	barth  ishiguro  and tsuji 	1 
　our strategy is very similar to the optokinetic nystagmus behavior in humans. this reflex in human vision consists primarily of two parts. first  as we move in our environment  our eyes tend to select a point in the scene and track that point as we move forward. through the use of our field-holding reflex  we are able to fixate rather accurately. as the fixation point reaches a point where mechanically we can not continue tracking  we perform a saccadic movement which moves the eyes rapidly in the direction opposite of the original tracking motion. a new fixation point is then selected and tracked as before. humans perform this behavior regardless of whether the path is linear or curvilinear. it was suggested by cutting  that motion parallax is used by humans during fixation to determine egomotion. we expound on this idea  and apply it to egomotion determination for mobile robots. 
1 	motion equations and associated flow 
we first review some mathematical preliminaries that have been developed elsewhere . starting with the general six degree of freedom motion equations  which can be found in several references  see    we consider the coordinate system fixed to a camera as shown in figure 1. in this system  the optical axis is aligned along the z-axis  and z   1 for points in front of the camera. we then consider point p whose coordinates in space are r =  x y z . if the camera moves with an instantaneous translation of t =  u v w   and instantaneous rotation w =  a b c   then the velocity vector of point p is given as: 
		 1  
             figure 1. camera coordinate system. further  we see in figure 1 that 

where lr is the length of vector r. we wish to determine the optical flow of point p in these spherical coordinates  note: it is possible to determine the optical flow for an image plane using common projection equations  however spherical coordinates are more natural for an active camera system . by differentiating the above equations  we obtain the optical flow given as: 

　at this point  we shall consider motion only in the x-z plane  and not allow the camera to rotate around the x- or z-axes. the mathematics and algorithms that follow can easily be extended to less restricted motion  but for illustrative purposes  we only consider the motion given by the vectors: 

　　　　 1  therefore  
 1  
and thus 
 1  
we now want to consider all the points that have equal flow in 
the x-z plane. if we set constant and equal to zero  we obtain the solutions: 
 1  
where k is a constant. 
　these solutions are shown in figure 1. with a fixed k  the first solution is an equation for a circle in the x-z plane. the radius of the circle is given by: 
 1   1  
figure 1. points with equivalent flow when translating by t and rotating by w =  a  b  c . 
　it is important to note that this circle is tangent to the camera's instantaneous translation vector. the second solution is an equation for a straight line perpendicular to the x-z plane and intersecting the plane at the point: 
                 1  the meaning of these solutions is as follows: for a given k  all points that he on the circle and line described in the above equations will have equivalent optical flow. since all of the points on the circle have equivalent optical flow  we shall refer to the circle as an iso-flow circle. when we vary k  we obtain a 

1 	robotics 

set of circles which all intersect at the camera's focal point  as shown in figure 1. we can consider this set of iso-flow circles as iso-flow contours. the values of optical flow corresponding to the iso-flow contours depend on the values of k  the direction of gaze  and the amount of camera rotation. however  for any 


figure 1a. iso-flow contours while gazing straight ahead for translation only in the x-z plane; t =  u 1 w   note k is some constant . 
direction of gaze and any rotation  the circular shape of the contours remains the same. 
　in figure 1a  we consider the case when the camera undergoes only translational motion while gazing straight ahead. we see that when k = 1  i.e.  when the optical flow is zero  all of the environmental points with zero optical flow lie on a straight line pointed in the direction of motion. this is true for any gaze direction. this makes sense from an foe standpoint  since this line will intersect the image plane at a single point  i.e.  the foe  or focus of contraction  foc  if the camera is pointed backwards . 
　in figure 1b  the camera undergoes both translation and rotation while looking along the instantaneous direction of motion  given by the vectors t =  u 1 w  and to =  1 b 1 . we now see that all of the points with zero optical flow now lie on a circle  referred to as the zero flow circle  zfc  . from this we can see that since the points on the zfc will project onto the image plane at many different points  a single foe point will not exist. it is also important to note that all of the points inside of the zfc will have optical flow values opposite in sign from the optical flow values outside the zfc. also  points that fall on the line of instantaneous translation will have optical flow equal but opposite in sign from the rotational component of the camera. 
1 	motion parallax 
motion parallax  or sometimes called kinetic depth  is the sensation of visual depth obtained by a moving observer while fixating on a point in the visual scene. objects in front of the fixation point move in the direction opposite of the observer movement  while objects behind the fixation point move in the same direction. the apparent velocity of each object near the fixation point is proportional to the distance from the fixation point . experiments have been carried out in the real-time computation of kinetic depth using an active camera  showing that such calculations are easy to compute when constraining the camera motion . 
　motion parallax is easily understood when considering the iso-flow circles developed in section 1. when tracking  the fixation point must lie on a zero-flow circle that also intersects with the moving observer . objects inside this zfc will move in the opposite direction of the observer  and objects outside of it will move in the same direction as the observer. motion parallax will occur anytime the direction of the fixation point is not directed along the instantaneous translational vector of observer motion  i.e.  when the line of sight crosses more 

figure 1b. iso-flow contours while gazing straight ahead for translation and rotation in the x-z plane; t =  ut1 w   to =  1 b 1 -
than one iso-flow contour. this holds true when the camera undergoes any amount of rotation. 
　a general case of motion parallax is illustrated in figure 1. we consider the camera at two positions along a line of translation  position p1 and p1. the optical axis of the camera always intersects at the fixation point z during motion. we also consider a point a and a point b that lie in front and behind the fixation point z respectively. the image motion of point a is given by angle a  and of point b by angle  1. what we wish to show is how a and β change as the angle 1  formed by the line of sight through the fixation point and the line of translation  changes. if we define z1 and z1 as the distances from the fixation point to the robot at positions p1 and p1 respectively  then we can write z1 in terms of zj using the law of cosines: 

 1  from the law of sines  we know that: 
		 1  
 
combining these two equations  we can express angle z in terms of d  1  and z1: 
             1  in a similar fashion  we can determine equations for the angles a and b: 
	barth  ishiguro  and tsuji 	1 

 1   1  
　we set the values of a1  z1  b1 and d constant  and plot a  β  and their difference  x-p for 1 ranging from 1＜ to 1＜  shown in figure 1. from this we can verify that the motion parallax of points a and b given by a and β is zero when   = 1＜ or 1＜  i.e.  when the line of sight is directed along the line of translation. further  both a and go through a maximum near 
1＜. the functions a 1  and p 1  act very much like the sine function  and indeed  when d is relatively small compared to a1  z1  and b1  a 1  and p 1  can be approximated by: 

　a key thing to notice in figure 1 and equations 1 and 1  is that the amplitude of a is greater than the amplitude of β for all values of 1. that is  points nearer to the observer than the fixation point will flow in the opposite direction of the observer motion  and because they are closer  they will have greater magnitude than the flow of points behind the fixation point. cutting suggested that humans use this information in determining the direction of motion . by grouping the flow vectors moving in one direction from a group flowing in the opposite direction  and by calculating the average magnitude of flow of each group  it is possible to determine the direction of observer motion. if a group of flow vectors has greater magnitude than the other group  then the observer movement is in the direction opposite to the motion of the group of greater magnitude. however  there are cases where this concept fails. if the closer group of points are close to the fixation point  and the far group of points are far from the fixation point  then the magnitude of the closer group shall be smaller than the distant group. this problem can be rectified through an intelligent choice of fixation points. by choosing a fixation point so that there are ample scene points between the observer and fixation point  and more specifically  that there are points closer to the observer than the halfway mark to the fixation point  then the problem is eliminated. 
1 egomotion determination algorithm 
1 	general strategy 
in order to determine the direction and rotational component of robot motion  we use two different kinds of camera motion. we first choose a point in the environment and fixate on it while we move a short distance. during fixation  we compute any motion parallax that occurs from points that lie on or near the line of sight. based on the measure of motion parallax  we then saccade the camera  selecting a new view direction. a new fixation point is then selected  and the process repeats. the algorithm converges so that the camera is always pointed along the direction of instantaneous translation  where no motion parallax occurs. 
the assumptions of this method are: 
1  the robot motion to be measured must change slowly enough so that the saccadic algorithm is allowed to converge. 
1  there must exist objects in the environment that are closer than half the distance between the camera and the fixation point. this insures that the average motion of objects behind the fixation point moves less than the average motion of objects in front of the fixation point  this is easily realized in the case of a mobile robot by directing the view slightly downwards. since there are usually points seen on the ground that are much closer than a chosen fixation point  the assumption is satisfied . 
the steps of the algorithm are: 
1  regardless of where the camera is pointed  we choose a fixation point in the center of the field of view  in the case of a mobile robot  the camera is tilted slightly downward with respect to the mobile platform  so that the fixation point usually lies on the ground . 
1  the fixation point is tracked as the robot moves for a fixed distance along the path. during tracking  we measure the flow of scene points that are imaged near the vertical line passing through the fixation point. we group the flow vectors into a left-moving group and a right-moving group. we then calculate the average magnitude of flow  only in the 
x-direction  within each group. we call the resulting value of the left-moving group aml  average magnitude left  and the value of the right-moving group amr  average magnitude right . 
1  a new view direction is calculated  given by: 
		 1  
where  i  is the previous view direction  and k is a control constant that determines how quickly the the algorithm converges to choosing a fixation point along the instantaneous direction of motion. the value of k depends on the length of the tracking step  the focal length  and a limit determining how close objects may come to the camera  the choice of k is discussed later when we consider convergence and stability . 
1  the camera saccades to the new view direction  and the process repeats from step 1. 
　as the choice of fixation points approaches the line of translation  the k aml - amr  term in equation 1 goes to zero  and the selected view direction will converge so that the camera always points along the line of translation. while tracking fixation points that lie on the line of instantaneous translation  the tracking velocity will be equal but opposite in sign to the robot's rotational velocity. this fact is confirmed in figure 1b  where the optical flow value of the line of 

1 	robotics 

instantaneous translation is always -b. we thus know both the direction and rotational component of robot motion. 
1 	convergence and stability 
considering the aml and amr values as single points in front and behind the fixation point  we can model the  aml - amr  term in equation 1 simply as 	where 	and 	are given in equation 1. therefore  equation 1 takes the form of: 
		 1  
where k1 is a cumulative constant made up of the constant k in equation 1  along with the terms a  z  b  and d from equation 
1. it can be shown that equation 1 converges to zero when 
k1 for all initial values of 1 in the range from  as an example  the convergence is shown when k1 = 1 in figure 
1. note the convergence is slower when 	is close to 1＜. even though at 1＜ the gaze direction is along the line of instantaneous translation  the algorithm will tend to diverge away from that point due to any small disturbing rotational value. 
　since we do not know a priori the values a  z  and b for each selected fixation point  we can not immediately choose an optimum constant k in equation 1 for convergence. however  we can insure convergence and stability by choosing k with knowledge of the maximum possible length of any flow vector used in determining aml or amr. knowing the geometry of the moving platform and attached camera  it is possible to determine how close environmental objects can get to the camera. by knowing the focal length of the camera and the closest possible distance of an object in terms of the translational motion during the tracking period  with corresponding movement d   we can determine the maximum possible flow that object will have when imaged. we consider the extreme case when the fixation point is at infinity and we are viewing perpendicularly to the direction of motion. k then can be chosen so the saccadic algorithm will always converge: 
		 1  
where maxflow is the maximum largest flow of any object. k 
should be chosen close to this value for rapid convergence  but should remain less than the value in order to insure stability. 
1 	experiments and results 
both simulated and real world experiments were carried out to show the effectiveness of our method: 
1 	simulation 
the movement of a robot with an active camera was simulated on a sun workstation in various environments such as a flat floor with objects  an approaching wall  and objects randomly placed in 1-d space. both linear and curvilinear paths with different degrees of rotation were simulated. in the simulation  the focal length of the camera was set to 1 pixels and the image plane consisted of a 1 x 1 array. the robot moved in one-step units  and the camera's focal center was 1 units above the floor. in the flat floor simulation  the camera tilt angle was set to a constant 1 ＜ down from the horizontal robot axis  
therefore the fixation point generally was selected 1 units away. we allowed no imaged object point to come closer than 1 units to the robot  therefore  the maximum flow of an object point was no greater than 1 pixels. for this reason  k was chosen to be equal to one in the simulation. 
 the algorithm was executed using many different rotational values  including the case of zero rotation resulting in pure translation. in all cases the algorithm converged to within 1% of the final correct rotational value in 1 steps or less. we show the flat floor case where the robot rotates 1＜ counter-clockwise for each unit of forward translation in figure 1. the initial camera angle starts at 1＜ to the left of the direction of motion. 
　figure 1a gives the camera angle relative to the instantaneous direction of translation after each saccadic step. we see that the algorithm rapidly converges to a final camera angle slightly less than 1＜. therefore  during the tracking step  the camera's zaxis crosses the line of translation  resulting in equal angles at the start and end of tracking. figure 1a also shows the tracking delta theta value  which should be equal to the robot rotation component after convergence. we see that convergence occurs after 1 steps  with a final value of 1＜. figure 1b shows the values of aml and amr for each algorithm iteration. we see that the aml dominates until convergence when they are roughly equal. figure 1c shows the simulated view from the camera with the associated motion parallax when the camera tracks at the initial angle of 1＜. 

figure 1b. average magnitude left  aml  and average magnitude right  amr  of flow versus tracking steps for t =  1 1   =  1＜ 1 . 
1 	real-world 
we have carried out the same algorithm in a real world experiment. an active camera was attached to a robot  which 
	barth  ishiguro  and 	tsuji 	1 


figure 1c. flow simulation as the robot moves and tracks a fixation point. the camera is oriented 1＜ to the direction of translation  and the robot rotates 1＜ counter-clockwise. the left frame shows the flow vectors and the motion parallax of the center vertical line. the right frame shows the robot and camera position in the environment. 

moved along the perimeter of a circle whose radius was 1 millimeters. the camera only rotated around the y-axis  and had a focal length of 1 pixels. the camera was tilted down so that the fixation points were chosen on the floor at around 1 meters. objects were not allowed to come close enough to the camera so as to generate flow larger than 1 pixels. therefore  we set k = 1 in the saccadic control equation so that the convergence of the algorithm was guaranteed. 
　the robot moved 1o for each tracking step  1 mm translation . we had initially pointed the camera at -1＜ from the direction of translation. the flow of objects points near the center vertical line was measured  and aml and amr values were determined. figure 1 shows the scene observed during the step when the camera was oriented at -1＜ from the direction of translation. also in the figure are the flow vectors associated with object points. the fixation point is near the center of the image  identified by a small circle. we see that the near objects have large flow moving right  and far objects have small flow moving left. 
　the results of the convergence are shown in figure 1. we see that the algorithm converged to a measure of robot rotation of 1＜. the measured rotation remained within ＼1＜ of this value. in figure 1b  we see the relative values of aml and amr. at the fourth step  we can see that there was a very close 


　figure 1. view of the scene with flow vectors when camera is oriented at -1＜ with robot rotation at 1＜. the fixation point is seen in the middle of the image indicated by a small circle. 
1 	robotics 

object so as to give a high a m r value. the resulting 1% error is due to an accumulation of error in the measure of the tracking delta theta  tracking error of the fixation point  and error in the measure of the flow vectors in determining a m l and amr. 

figure 1a. real experiment- camera angle and calculated rotation for t =  1 1  w =  1＜ 1 . 

figure 1b. average magnitude left  aml  and average magnitude right  amr  of flow versus tracking steps for t =  1 1   w =  1＜ 1 . 
1 	discussion a n d conclusions 
we have developed a computationally inexpensive methodology for determining robot rotation and translation by using an active camera that tracks fixation points and subsequently performs saccadic motion. through the use of controlled saccadic motion  the rotational and translational values of motion can be determined after a few iterations of the algorithm. if the motion of the robot changes smoothly along its trajectory  the algorithm will 'track' the instantaneous direction of motion  and will provide continuous egomotion values for any type of path. it is important to note that our method simultaneously provides rotation and translation information  rather than the usual method which first determines rotation and then derotates an image in order to find the foe and thus the direction of translation. 
　note that this method does not assume that the forward pointing axis of the vehicle coincides with the instantaneous direction of motion. such an assumption can be made for vehicles with conventional forward wheel steering with no wheel slippage. in this case  a similar but somewhat simpler algorithm can be used when the camera's rotational angle is known with respect to the forward direction of the vehicle . 
　we have encountered a small percentage of error in our experiments. this error could be reduced by improving the angular resolution of our active camera. also  the algorithm depends highly on accurate tracking  so error in the tracking algorithm needs to be minimized. accurate flow measurements are also important  albeit only near the center vertical line   so we must employ an optical flow method which is precise and robust. for speed considerations  we are currently developing a method that determines the optical flow only along the center vertical line. we intend to continue experimenting with our method  and eventually wish to perform the egomotion calculations in real-time. 
