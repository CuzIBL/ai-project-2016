 
this panel is a celebration of artificial intelligence  ai . basing its claims to interest on the past accomplishments of ai  it highlights some of the new exciting concepts and technologies that compete for the title the next big thing. 
1 	introduction 
it is well-known among artificial intelligence  ai  researchers that ai has made significant contributions to computing research and practice. this fact is not always recognized in the rest of the computing community. indeed  many people believe that  what works isn't ai.  this is only half true. ai has traditionally focused on challenging problems  and when a problem is about to be understood  ai researchers tend to move to something more interesting. indeed  topics that were identified by ai researchers  but are considered mainstream today include  among others 
  functional programming   object-oriented programming 
  agents. 
　this panel seeks to identify the next major ai contributions to the rest of computing  while they are still considered ai! the goals of this panel are twofold: 
  to raise awareness within ai of applications that will be the most amenable to ai techniques 
  to identify results coming down the pike that may be used to raise awareness of ai contributions. 
　the panelists were asked to address the following questions. the first two questions define the essence of this panel. the next two help relate ai research with the rest of computing. the last question is the most speculative. 
1. what major contributions might we expect from your area in about a decade  
  what external technical factors might affect these  
  what are their main applications  
1. what is the likely next major contribution from your area  
  what external technical factors might affect it  
  what are their main applications  
1. from which part of computing are we learning the most  
1. which part of computing will we affect the most  
1. what advice can you offer researchers and funders  
　in the context of the above questions  this panel includes commentaries by distinguished researchers representing major technology areas of ai. the area of ai programming is covered by bobrow; distributed ai by huhns; language technology & interfaces by king; learning & evolutionary computing by kitano; and 
knowledge representation by reiter. the moderator's contribution may be classified under cooperative information systems. 
singh  etal. 

1 	bobrow: concurrent constraint models: a basis for intelligent computation 
models have been at the heart of much of what has been done in artificial intelligence. we have built models of problems  of places  of things. we have built programs to reason about them  diagnose them when they exhibit faulty behavior  and explain how they behave. for each of these reasoners  we have often built different kinds of models. we are now starting to understand how to build simple composable models based on a hybrid concurrent constraint programming language  saraswat et al  1  augmented with hybrid-time semantics. the  hybrid  nature of the language enables the description of both continuous changes over time that might be described by a differential equation  and discrete changes of values and regimes of operations  gupta et a/.  1 . the support for  concurrency  enables compositionality; individual components and processes can be defined separately; simple composition of the separately defined components  and the strong semantic definition underlying the constraint system enables the composed system to run as a simulation with appropriate  real  parallelism  not just choosing one of the possible orderings of events . 
　as a complete programming language  it enables both environments of a system  and a task for the system to do to be expressed in the same language. as a constraint-based system  after composition of the model  the sense of locality can change. in the usual object oriented systems  information between composed objects defined separately is communicated through a protocol of messages between the objects  but all constraints on the object's behavior stay implicit  and local to the ob-
ject. with constraint-based models  partial evaluation and constraint propagation allow information to flow to where it might be needed. we will describe a simple example with respect to a scheduling problem shortly. this style of programming  where the domain is the focus  constraints are the way of capturing the domain  and reasoners are the interpreter will be a very strong influence on computer science over the next decade. similar systems will be used to control autonomous robots and immobots  williams & nayak  1 . 
   as an example of a use of this technology  xerox has been building a constraint-based modeling framework to enable construction of software to schedule plug-and lay xerographic engines used for copying and printing !fromherz & saraswat  1 . the software is intended to schedule the machine to produce paper at the fastest possible rate  given the description of the input stream  and the desired output. what makes this particularly difficult is that the composition of the machine is not known at the time the software is constructed  since different customers want different configurations of feeders  sorters  finishers  etc. these are plugged together in the field. this is a problem because the timing of feeding sheets in the beginning of the machine can be affected by delays that a finisher might impose because  for example  it needs time to move a stack of paper to a stapler  and no sheets should be moved to the stacker at that time. because each module that can be connected together has a constraint-based description  this information can be propagated at the time these are plugged together  and taken into account by the generic scheduling software  a form of anytime optimizing planner. 
　the same types of models are being used to evaluate new design features to determine whether they increase productivity. i expect to see these kind of applications much more ubiquitous in the near future. taking into account a wider range of model-based tasks  one can think of a generic device model as one that has five interfaces that: define its inputs; its outputs; its task/goal  set its internal parameters/behavioral modes  and provide temporal control. given such a model in an appropriate declarative form  one can provide some of these interfaces  and compute the plausible values for the others. for example  control code for a device to achieve a certain task can be created  and compiled to low level machine code . 
　what is liable to affect the use of this most is whether the tools we are using can be put into the context of widely used systems and languages such as java  and integrated into larger frameworks. this requires us to build these systems as component software so that they can be added easily to any preexisting architecture  framework  or algorithm. one thing we have learned from computer science is that there is no silver bullet. no single good idea  or system is enough to solve all problems. and systems need to be tuned to the work practice of the individuals and communities who use them. sometimes solutions are more to be found in changing or leveraging the social environment in which a system is embedded than in clever software techniques  o'day et a/.  1 . and what we have to teach computer sci-
ence is the power of symbolic reasoning  deep modeling  explicit task description and task independent reasoners. 
1 	huhns: a new dai-based software paradigm 
the field of software engineering has exhibited almost glacial progress over the last twenty years  and appears to be in a malaise. programmers still produce approximately the same number of lines of tested and debugged code per day as they did in 1  in spite of several  silver bullets   such as structured programming  declarative specifications  object-oriented programming  formal methods  and visual languages. 
this should not be surprising  for three reasons: 
1. software systems are the most complicated artifacts people have ever attempted to construct 
1. software systems are  supposedly  guaranteed to work correctly only when all errors have been detected and removed  which is infeasible in light of the above complexity 
1. the effect of an error is unrelated to its size  i.e.  a single misplaced character out of millions can render a system useless or  worse  harmful. 
　software engineering attempts to deal with these problems by considering both the process of producing software and the software that is produced. the major goal for the software is that it be correct  and the major goal for the process is that it be conducted efficiently. one fundamental approach to meeting these goals is to exploit modularity and reuse of code. the expectations are that small modules are easier to test  debug  and verify  and therefore more likely to be correct  that small modules will be more likely to be reused  and that reusing debugged modules is more efficient than coding them afresh. this is the major result of the series of programming paradigms that have evolved from the machine language of the 1's: 
1. imperative paradigm: procedure-oriented programming 
1. declarative paradigm: functional and logic programming 
1. interactive paradigm: object-based and distributed programming. 
　however  software has not kept pace with the increased rate of performance for processors  communication infrastructure  and the computing industry in general  lewis  1 . whereas processor performance has been increasing at a 1% annual rate and network capacity at a 1% annual rate  software productivity has been growing at a 1% annual rate and the power of programming languages and tools has been growing at an 1% annual rate. case tools  meant to formalize and promote reuse  have not been widely adopted  livari  1 . in addition to these sluggish rates for software  there is a legacy of approximately 1 billion1 lines of cobol  representing roughly 1% of all software written since 1. it is unlikely that we can replace it anytime soon  even though maintaining it is a $1 billion annual expense. 
the current  hot  computing paradigm is based on 
java  and the ability it provides for users to download the specific functionality they want at the moment they request that functionality. this is leading to the rise of a software-component industry  which will produce and then distribute on demand the components that have the users' unique desired functionality  yourdon  1 . however  because of this uniqueness  how can component providers be confident that their components will behave properly  this is a problem that can be solved by agent-based components that actively cooperate with other components to realize the user's goals. 
1 	a new software paradigm 
therefore  it is time to consider a completely different approach to software systems. we propose one based on the  intentionally provocative  recognition that 
	1
1 . 
  errors will always be a part of complex systems 
  error-free code can at times be a disadvantage 
  where systems interact with the complexities of the physical world  there is a concomitant power that can be exploited. 
　we suggest an open architecture consisting of multiple  redundant  agent-based modules interacting via a verified kernel. the appropriate analogy is that of a large  robust  natural system. 
1 	requirements for new applications 
there are a new class of applications evolving thanks to ongoing advances in computer systems. the applications have characteristics that lead naturally to an agentbased approach to their development  woelk et al  1 : 
  they solve a specific business problem by providing a user with seamless interaction with remote information  application  and human resources. 
  the identities of the resources to be used are mostly unknown at the time the application is developed. 
  the pattern of interaction  workflow  among the re-sources is a critical part of the application  but the pattern might be unknown at the time the application is developed and might vary over time. 
　the development of these new applications requires improved programming languages and improved system services. these are not alternatives to such capabilities 
as omg corba and microsoft dcom  but rather advanced features implemented at a higher level of abstraction and useful across multiple heterogeneous distributed computing environments. 
　because each application executes as a set of geographically distributed parts  a distributed active-object architecture is required. it is likely that the objects taking part in the application were developed in various languages and execute on various hardware platforms. a simple  powerful paradigm is needed for communications among these heterogeneous objects. due to the distributed nature of the application  an object might not always be available when it is needed. for example  an object executing on a pda might be out of physical communications with the rest of the application. 
　because the identities of the resources are not known when the application is developed  there must be an infrastructure to enable the discovery of pertinent objects. once an object has been discovered  the infrastructure must facilitate the establishment of constructive communication between the new object and existing objects in the application. 
　because the pattern of interaction among the objects is a critical part of the application and may vary over time  it is important that this pattern  workflow  be explicitly represented and available to both the application and the user. when an object has been discovered to be relevant to an application  the language for interaction and the pattern of interaction with the object must be determined. this interaction becomes part of the larger set of object interactions that make up the application. the objects collaborate with each other to carry out the application task. 
　fundamentally  most business software modules are intended to be models of some real object within the business. a problem is that these modules are passive  unlike most of the objects they represent. 
1 	multiagent-based development 
multiagent-based interoperation is a new paradigm distinguished by the features that requests are specified in terms of  what  and not  how   agents can take an active role  monitoring conditions in their environment and reacting accordingly  and agents may be seen as holding beliefs about the world. 
　we have initiated development of this new paradigm- a cooperative paradigm-based on interacting agents  active objects  and active wrappers of legacy components. the resultant methodology and language  interaction-oriented programming  represent a fundamental extension of the earlier paradigms  with greater expressive power  different conceptual foundations  such as beliefs held by components  and new modeling techniques. 
1 	k i n g : language technology and interfaces 
i take the language technology and interfaces area to cover all of speech technology  written language technology and aspects of interfaces such as the inclusion of images and sound. my own expertise is in written language technology  and my knowledge of the other areas involved scanty  being especially weak on the nonlanguage areas. consequently  my intervention here will concentrate mainly on language technology. i hope that this will not be interpreted to mean that 1 believe work outside that subdomain to be less important. similarly  i ask my speech colleagues to forgive me any unintentional misrepresentation of their field. 
major contributions in about a decade 
before looking at the critical developments 1 see coming in language technology  i want to be negative  or realistic  depending on one's point of view  and say what i do not see coming. i see no signs of a breakthrough which would allow us finally to conquer the semantic barrier by direct frontal attack. by this i mean that i see no developing technology which would allow us  in the general case  to resolve the problem of rampant ambiguity in natural language. thus  whilst i can easily imagine succesful applications in constrained domains  i cannot  for example  imagine the development of a machine translation system which will produce high quality translation of arbitrary input  or a question answering system that relies on being able to simulate human understanding of an arbitrary text. 
   what i do see is the development of less ambitious technologies that will radically change the lives of very many people. 
　on the written language side  many research workers are already involved in finding ways round the semantic barrier  often taking as their starting point the recently developed capacity to process very large amounts of preexisting text  armstrong  1 . thus we see  for example  suggestions that it might be possible to spot the spelling mistakes in  sea the dove sore  by relying on probabilities determined through part of speech tagging of large quantities of text  or in  i bought the mink in the super market  through probability of collocation. i do not mean to suggest that statistics-based techniques on their own are the magic solution to all problems: we know from the past decade or so that this is not so. but they do provide a new weapon  which  combined in certain cases with rule-based approaches  can give us a better succes rate in solving old problems. and perhaps even more importantly  they encourage a way of thinking in which being inventive about how to deploy the technology available is as important as tackling intractable fundamental problems directly. 
　on the spoken language side  i think a major contribution will be the development of systems which allow reliable multispeaker recognition of continuous speech  albeit perhaps in limited domains. quite impressive prototypes already exist  and i believe we can foresee the incorporation of the technology into products in the relatively near future. 
external factors that may affect this. one factor which will affect these developments strongly is  quite simply  market forces. some products of language technology have become parts of everyday life: few secretaries would give up their spelling checkers  anybody who uses the web uses a search engine which incorporates a modest use of language technology. even more ambitious products are becoming more widely known and used: compuserve  amongst others  offers machine translation on the web  translators' workbenches are selling more and more  dictation systems are doing well. as the general public becomes increasingly aware of what can already be done  an appetite is created for yet more  and the manufacturers thereby encouraged to create and market more advanced language technology based products. 
　another external factor is the development of the www: communication links across geographically dispersed communities and across different language communities creates a need for tools which facilitate communication. the need is accentuated by the existence of users communicating in a language which is not their own  or needing to communicate across languages. 
　one major technological issue will be the develpoment of robust systems: systems for both written and spoken language that can operate in noisy environments with nonexpert users. 
main applications. a hint at some applications is already contained in the above: improved spelling and grammar checkers  more user friendly and more robust dictation systems  voice input to information retrieval systems  improved information management tools  as well as more futuristic applications based on  for example  multilingual information management and retrieval. the list is  of course  far from being exclusive! 
the likely next major contribution 
the next major contribution is the integration of speech and language work. funding agencies have been pushing in this direction for the last few years  as witnessed by the german national project  verbmobil  which aims at interpretation carried out over the telephone  an earlier similar japanese effort and parts of the european union fourth framework programme  alexandersson et al.  1; gibbon  1; mctear  1 . as a result of these and other initiatives  there are already a few interesting prototypes around  but no widely available products  and the full impact of integrated speech and language technology has not as yet been realised. 
　integration is not  of course  limited to speech and written language. i think we will see an explosion of multimedia systems incorporating imagery and sounds as well as speech and written language. 
external factors that might affect this. two major factors are likely to affect the development  of integrated speech and language technology  favourably or unfavourably. 
　the first is the obvious factor of funding agencies forming and maintaining a consistent policy favouring work in this direction. the classical danger here is that of agencies expecting concrete and commercialisable results very quickly. when the results do not appear within a short time-frame  the agencies have a tendency to become disppointed and switch their attention somewhere else. especially when this is coupled with a policy of funding for the short term only  four years or so being the maximum that a project can expect  and two years being typical  the result can easily be stop-andstart encouragement for work in a particular area  with all the associated problems of multiplying overheads in repeated start-up time and effort  and diminution of return on investment towards the end of a project  when the participants are investing much of their effort in writing convincing reports and prospecting for continuation funding. 
　this problem is particularly acute in the european setting  where funding is normally given only to consortia including groups from at least three different countries  and consortia are encouraged to change their membership from one project to the next. it is consequently difficult to produce medium or long-term consistent effort devoted to tackling specific problems or problem areas. however  the problem of acquiring funding for more than short term efforts is not particular to the language technology area. 
　the second factor  more specifically tied to that area  is that of initiating and maintaining creative dialogue between the speech community and the written language community. the problem here is not primarily one of good will. it is more a question of learning to reach across established habits of thought and ingrained ways of talking. an example may be useful. when a speech person talks about a corpus  he typically means a relatively small  but extensively annotated collection of items. when a written language person talks about a corpus  he typically means an extremely large collection of items  not necessarily including more than minimal annotation. put the two of them together to talk about problems of corpus collection  storage and management  and cross-purposes discussion is almost inevitable.  note that even the interpretation of  small  and  large  is influenced by the community one comes form: small here means several hundred  maybe a few thousand items  large means one or several million items . it would not be too difficult to find an extensive list of such areas of potential noncommunication. 
main applications. the range of potential applications is enormous. it starts with fairly unexciting but extremely important applications like dictation systems allowing for multispeaker input and natural speech  backed up by intelligent spelling checkers and grammar checkers  through to multilingual document retrieval and management and multilingual communication systems  passing through such a wide range of other applications on the way that the imagination boggles. indeed  it quite seriously seems to me that we are limited in our imagination by what we know of the current state of technology. once a breech is created by enabling integration of different means of communication  i foresee a flood of applications which we cannot yet even envisage. this is reinforced by experience with the www: there can be very few of us who would have imagined fifteen years ago that if we wanted to track down a rare book or find a modest hotel in new york or find out where a friend we last saw twenty years ago was now living  we would start with a web search. 
the part of computing from which we are learning the most 
if this is rephrased as  are we benefitting from   the answer must surely be hardware developments. only increased memory capacity and processing power has made the enabling technology on which both written and spoken language rely possible. 
the part of computing we will affect the most 
i am very unsure of my answer here  but i suspect that the answer may once again be hardware. if we really think in terms of multimedia information and communication systems  it is not hard to imagine that we are influencing a push towards the development of more sophisticated sound and vision equipment incorporated into standard computer technology. 
advice for researchers and funders 
my advice to researchers follows from all the above: be inventive about ways to use the technologies that are becoming available  about ways to combine new technologies with what is already there  and look at the interfaces between speech and language. we also need to fuse the written language and the speech communities  in order to create a language technology community. 
　my advice to funders is also clear: adopt a funding strategy which allows at least medium term research to be undertaken by relatively stable groups  and at the same time try to encourage research aimed at the integration of different modalities of communication. 
1 kltano: building and understanding adaptive complex systems 
fifty years have passed since the invention of computers  and over forty years have passed since the birth of artificial intelligence. assuming a 1 year economic cycle-based on the lead time between the conceptualization of a new technology and its maturity-computer science and ai will face a major leap in a decade. the explosive use of the internet across the globe signifies the first major wave of the penetration of computer science into society  which impacts global economy as well as personal life. in the ai community  the success of the computer chess challenge epitomizes state-of-theart ai technologies that can be immediately recognized by a general audience. however  we need to move forward. 
　in recent years  there have been several attempts to elucidate future direction of ai research  including panel sessions at ijcai-1   grand challenge ai applications   kitano et a/.  1  and at aaai-1   challenge problems for artificial intelligence   selman et a/.  1 . 
　looking back at the history of ai  success has been limited to domains where the world can be described by symbolic representations  and complete information is accessible. computer chess is a typical example. most expert systems also fall within this category. i have classified problems that ai systems may face into four classes  kitano  1 : linear decomposition  linear approximation  nonlinear  and nonequilibrium problems. if we are to engage in the collective and organized effort to bring about the next big thing  task domains need to be carefully chosen. scientifically  the next big thing is going to be adaptive complex systems. the challenge is how to build such systems and understand them. 
1 	building a d a p t i v e c o m p l e x systems 
contextual-thickness as a foundation of integrated systems 
one of the dreams of ai is to build an autonomous agent which can handle a broad range of tasks intelligently. this involves the capabilities of understanding and using language and behaving in the physical real world for specific tasks. what is an essential principle  or a concept which is salient in a complete agent  together with my colleagues  i organized a closed workshop in 1 at sony computer science laboratory titled  grand breakthrough.  the participants were leading ai researchers in japan. the discussion highlighted a concept of  contextually-thick system.  a contextually-thick system is a robust system which can behave reasonably well for tasks with different contexts. a natural language system which can carry out dialogue with humans and translate written texts can be viewed as contextuallythicker than a traditional machine translation system. an autonomous system which can translate language and play soccer game is very contextually-thick. with the growing interests in building integrated systems  the concept of contextual-thickness will be highlighted. however  it is a nontrivial task to build such a system. 
thus  a systematic and comprehensive approach needs to be taken from both engineering and science. 
multiaspect learning 
one example of contextual-thickness can be illustrated by taking an example of how players should  learn  in the robocup. to be successful in the robocup  individual players' skills  teamwork  and coach's coaching ability should all be learned. the players need to learn before the match  during the match  and after the match. learning of teamwork play before the match is allowed a substantial amount of time. however  if the team needs to adapt to the opponent's strategy  this type of learning must be done within 1 minutes. also  the result of the match needs to be reflected to the next match against different opponents as well as the same opponents. in addition  the strategy needs to be changed if one of the players or an opponent player was involved in an accident and is not able to take part in the match anymore-thus  the system must learn to detect anomalies and how to switch strategies. this example from the robocup makes it clear that there are multiple aspects in learning when we try to build an integrated system for a comprehensive task. while robocup itself restricts a context to soccer games  it still has multiple aspects which naive use of current learning theories cannot cope with. variations and constraints imposed on learning in various situations has not been systematically investigated. thorough investigation on aspects of learning would be a possible first step towards a contextuallythick system. 
evolvable systems 
another potential breakthrough can be found in evolutionary computation. first  the increasing availability of reconfigurable hardware such as field programmable gate array enables genetic search and optimization technique to be used for real-time applications  such as telecommunication traffic optimization and real-time control of mobile robots. second  increasing demands for evolving more complex structures will promote research into sophisticated genotype-phenotype mapping. both avenue of biologically-inspired methods and formal methematical methods will be explored. 
chess robocup environment static dynamic state change turn taking real time info  accessibility complete incomplete sensor readings symbolic nonsymbolic-control central distributed table 1: comparison of chess and robocup 
infrastructure 
in order to promote research for the next big thing  an infrastructure needs to be implemented. examples follow: 
standardized robot components. as the need for real world applications of ai increases  ai research platforms will increasingly emphasize robots and systems that interact with the physical world. however  the lack of standard components  which can be purchased at affordable prices hampers many researchers from actually going into the physical world. a collective effort shall have to be made to define a standard for reliable  flexible  scalable  and affordable physical robot components. such a component set should provide a mother board with a real-time operating system  and a basic development environment. 
　the central focus of current molecular biology is ♀o identify genes and their functions. most papers published in nature  science  and genes and development report identification of new genes with significant functions. in addition  major efforts are begin made on genome sequencing  such as the human genome project  c. elegans genome project  and e. colt genome project. the problem  however  is that completion of dna sequencing and identification of genes is not sufficient to really understand biological systems. it is crucial to identify how these genes interact and what kinds of regulatory mechanisms control the whole process. however  this is an extremely difficult task  due to the involvement of a large number of interacting components. 
　the opportunities for ai and computer science are abundant. there is a pressing need to understand and possibly predict complex interactions among a large number of components. components are interconnected with certain nonlinear functions with noise and delay. the identification of a set of parameters would significantly help biologists in further investigating the molecular mechanisms of biologically significant phenomena. thus  the integration of simulation technology  search in a very large parameter space  logical deduction and induction of possible genetic networks from gene cloning and mutant analysis data  and intelligent visualization of highly complex information spaces would be extremely helpful. 

robocup. the robocup  for example  is one of few attempts to provide a long-range challenge for the ai community  kitano et al.  1; 1 . it can be a significant testbed for ai and robotics research  as well as a showcase for a wide range of ai technologies  such as motion reconstruction for three-dimensional visualization of simulator  automatic commentary system  and intelligent studio systems. since the robocup itself has been extensively publicized  here i only provide a table which contrasts chess and robocup in terms of their domain characteristics  table 1 . 
1 understanding adaptive complex systems: c o m p u t e r - a i d e d biology and neuroscience 
in the area of basic research  computer science and biology can establish a stronger relationship in the long run. computer-based simulation and analysis systems  empowered by various ai and simulation techniques  will revolutionalize the way biology research is conducted. at the same time  ai will benefit a lot from the recent achievements in biology  particularly in the area of molecular neurobiology and functional brain mapping. 
from computer science to biology 
already there have been many contributions from computer science to biology concerning instrumentation systems  databases  and protein structure predictions. however  i would argue that much more can be achieved. 
from biology to computer science 
recent progress in molecular biology  particularly molecular neurobiology  and functional mri technologies enables us to directly observe and manipulate the activities of the brain at both the neural  or subneural  level and the macroscopic level. in the past  much of the functionalities of the brain and its cognitive role have been discussed largely based on speculation. now  we will be able to actually measure activities at various grain-sizes. 
　for example  a mouse's place memory formed at ca-1 region of hippocampus was directly measured by microscopic electrobes  and the effects of specific signal transaction channels was identified using an induced knockout mouse  tsien et a/.  1; mchugh et al.  1 . the knockout mouse is a transgenic mouse whose specific gene has been knocked out. induced knockout is a means to knockout a specific target gene epigenetically. such experiments resolve disputes over the internal representation of memory and learning. from more computational aspects  the relationship between biological processes and reinforcement learning was reported in  schultz et al.  1 . 
　these studies greatly expand our knowledge of the brain and neural systems  which is the foundation of intelligence. i believe a massive inflow of knowledge from the rapidly developing areas of molecular neuroscience  functional brain mapping  and molecular biology in general will drastically change the directions and quality of research in the ai community. 

1 	reiter: knowledge representation 
ai practitioners tend to view knowledge representation  kr  as a rarefied  theoretical side of their field  and they do not normally look to it for a source of applied research. but in the past  foundational work in kr has led quite directly to important practical benefits. before splitting off into their respective subcommunities  logic programming  deductive databases  description logics for taxonomic reasoning  and agent programming all had intellectual and methodological origins in kr. 
　i have two candidates for the next big thing to emerge in the coming decade from research in knowledge representation  both closely related to each other. 
dynamical systems 
a good case can be made that one of the most pressing needs for ai these days is a solid theoretical and computational account of actions. it is a challenge to capture  in a single formal and computational framework  the full range of characteristics associated with dynamical systems: the frame  ramification and qualification problems  exogenous and natural events  chance events and the unpredictability of action effects  complex actions and procedures and the ability of an agent to perform such actions  time  concurrency  planning  hypothetical and counterfactual reasoning about action occurrences and time  perceptual actions and their effects on an agent's mental state  the complex relationships among reasoning  perception and action  belief revision in the presence of conflicting observations  etc. almost every ai system needs such a story because  virtually without exception  the phenomena ai wishes to model have dynamic components. consider: robotics  planning and scheduling  natural language communication and speech acts  qualitative physics  database transaction processing  diagnosis and repair of time-varying systems  simulation  etc. 
　the good news is that  largely as a result of substantial foundational work by the kr community over the past few years  we are well along in the development of such a theory. the frame  ramification and qualification problems have largely been solved  e.g.   kartha & lifschitz  1    reiter  1    shanahan  1    sandewall  1   and  thielscher  1  . this means that dynamical systems can now be given purely logical characterizations  thereby eliminating the need for the often ad hoc  procedural and hybrid approaches to modeling dynamics that ai systems have employed in the past. i think it's safe to predict that the next 1 years will produce  off-the-shelf general logics and implementations suitable for incorporation into specifications and programs for domain dependent applications. there are already signs of this in the well-developed axiomatizations for the temporal action languages of  sandewall  1  and kowalski-sergot as extended by  shanahan  1   for the family of a languages  gelfond & lifschitz  1   and for the situation calculus incorporating time  concurrency and continuous events  pinto  1 . moreover  various action-centered logic programming languages are now in use  for example the temporal logic-based metatem  fisher  1   and the situation calculus-based 
golog family of languages  levesque et al.  1; de giacomo et al.  1 . 
　with respect to  the next big thing  to emerge from these developments  i believe the most significant outcome is that they provide action-centered  purely logical programming languages. the computations of such programs are determined through their interactions with an axiomatic knowledge base describing actions  their preconditions and effects   as well the initial state of the world being modeled. this feature makes these languages of some interest beyond ai. in computer science  for example  software engineering  database transaction processing  discrete event simulation  computer animation  systems programming and programming language semantics are all areas where dynamics is central  and where  knowledge-based  specifications and programming can be profitably exploited. further afield  the situation calculus can be seen as a generalization of classical discrete event control theory  providing for control systems with an explicitly represented knowledge base. this means that situation calculus-based programming languages  like golog  are well suited to implementing control systems and their simulators  including  hybrid  control systems involving both discrete and continuous time and events. 
agent programming 
there are many perspectives on what counts as agent programming. for the purposes of this presentation  i shall take it to concern the design of high level programming languages for  possibly distributed  possibly multiagent  systems in which each agent is encapsulated in a program  communication among agents is possible  and each agent program respects suitable logical specifications involving the intensional states  knowledge  belief  desires  intentions  of itself and of other agents. here   knowledge and beliefs  are what you would expect   desires  are the agent's goals  and  intentions  are those goals that the agent is currently committed to achieve. but of course  other intensional attitudes are possible  and sometimes desirable  permissions  obligations  etc . the key problem here is to capture in suitable programming languages  agent behaviors that emerge from the interactions among these intensional states  the agent's environment  and the agent's repertoire of basic actions that it can perform in its world  including perceptual actions that change its knowledge and belief states. notice that these agents ultimately act in the world  so that all of the issues raised in my previous discussion of dynamical systems arise here as well  the frame  ramification and qualification problems  concurrency  temporal reasoning  etc . moreover  they arise with a vengeance because they must be adapted to deal with intensional attitudes as well as ordinary actions  scherl 1: levesque  1 . 
for many years now  providing formal and compu-

tational foundations for intensional attitudes and belief change has been meat-and-potatoes research for kr  so it should come as no surprise that much current activity in agent programming is informed by this work. the basic logical framework for intentions stems from  cohen k levesque  1  and the influential bdi  beliefs  desires and intentions  framework for multiagent systems is due to  rao k georgeff  1 . during the past several years  ch. meyer and his group at utrecht have been sytematically developing formal accounts of rational agents  van der hoek  et al.  1 . there have been a few implementations of multiagent programming languages that were motivated by the above considerations  for example the procedural reasoning system  georgeff & in!;rand  1  and its successors  and shoham's agento shoham  1   but currently there remains a large gap between the logical ideals characterized by the foundational research in kr and actual implementations. this situation will change  of course  and in the next 1 years we can expect extremely rich programming languages that represent  at a very high level of abstraction  the logical and procedural properties of agents with mental states. 
　the consequences of such developments for computer science  especially programming language theory  software engineering and databases are obvious  and i won't repeat the standard arguments. nor will i rapture on about how the world will be a better place what with smart web browsers  personal assistants and so on. certainly  someone will make a lot of money from agent programming  but alas  it probably won't be me. 
1 	singh: interaction-oriented programming and social constructs 
open information environments are heterogeneous  distributed  dynamic  large  and frequently comprise autonomous components. for these reasons  they require solutions that marry artificial intelligence  ai  and traditional techniques to yield extensibility and flexibility. agents are a result of this marriage. unfortunately  many current agent approaches are  autistic  with all of the attendant limitations of centralization in open environments. 
　multiagent systems require  potentially autonomous  agents to behave in a coordinated manner. therefore  the designer of a multiagent system must handle not only the application-specific aspects of the various agents  but also their interactions with one another. however  constructing multiagent systems manually can lead to unnecessarily rigid or suboptimal designs  wasted development effort  and sometimes to the autonomy of the agents being violated. it is these difficulties that lead many to the centralized approaches. 
　we propose interaction-oriented programming  iop  as a class of languages  techniques  and tools to develop multiagent systems. briefly  iop focuses on what is between  rather than within  agents. interactions may conveniently be classified into three layers  lower to upper : 
  coordination  which enables the agents to operate in a shared environment  hewitt  1  
  commitment  which add coherence to the agents' actions  castelfranchi  1; singh  1  
  collaboration  which includes knowledge-level protocols on commitments and communications  grosz k kraus  1; singh  1 . 
informal concepts  such as competition  may be classified into different layers: auctions require only coordination  whereas commerce involves commitments  and negotiation involves sophisticated protocols. 
tenets. our key tenets are as follows. we describe and defend these in some of our other work. 
1. the openness  autonomy  and heterogeneity of modern systems are often sacrosanct. 
1. correctness or data integrity may be difficult to characterize  but coherence is still crucial. 
1. complex interactions greatly exacerbate the difficulties in developing robust multiagent systems. 
1. customizable approaches can yield productivity gains that far outweigh any performance penalties. 
　interaction has been studied  albeit fragmentarily  in distributed computing  dc   agha et a/.  1; milner  1   databases  db   gray & reuter  1   and distributed al  dai . the db and dc work focuses on narrower problems  and eschews high-level concepts. thus it is less flexible  but more robust  than the dai work. the challenge is in achieving rigor and flexibility. 
　it is instructive to evaluate the autonomy allowed by different db approaches. transactions publish results cautiously  gray k reuter  1 : they preserve autonomy of the consumers of those results  but violate autonomy of the subtransactions producing the results. extended transaction models release results more liberally  and restrict the autonomy of their components in executing compensating subtransactions as needed  elmagarmid  1 . both of the above leave it to the application to handle errors or discrepancies that arise after a given transaction has completed. by contrast  spheres of control publish their results early  but require control over the activities consuming the results  davies  1 . 
toward an ontology of commitments. we believe social commitment is the key abstraction for supporting coherent interactions  while preserving autonomy. the db approaches deal with passive objects  and view commitments as depending solely on the computation that  commits   not on the interplay between the interacting computations. agents give us a handle on persistent activity  but without commitments they are quite limited. in order to formalize commitments  we observe that 
1. agents can be structured  and are recursively composed of heterogeneous individuals or groups of agents  singh  1  
1. agents are autonomous  but constrained by commitments-or we would have chaos! 
1. social commitments cannot be reduced to internal commitments  which apply within an agent-the relationships among these concepts cannot be definitional 
1. commitments are  in general  revocable; the clauses for revoking them are no less important than the conditions for satisfying them! 
1. commitments arise  exist  are satisfied or revoked  all in a social context; commitments not only rely on the social structure of the groups in which they exist  but also help create that structure. 
we believe that these observations  especially 1 and 1  have not received the attention they merit. taking them seriously leads to a view of commitments as relating a debtor  a creditor  and a context a number of natu-
ral operations can be defined on commitments  along with social policies that agents acquire when they  autonomously in some cases  adopt a role. 
　the above gives a flavor of iop  and its social component. we invite the reader to join in its investigation. 
questions. we would suggest that the next result of iop would be the osmosis of social constructs into mainstream computing  with tools for iop being realized within a decade or so  for open applications. the key external factors will be advances in dc and the emergence 
of standards for agent interaction and  society management.  iop will benefit the most from dc and have the greatest influence on db. if we may venture some advice to researchers it would be not to lose sight of principles when playing with the enticing technologies of today; our advice to funders would be the same! 
