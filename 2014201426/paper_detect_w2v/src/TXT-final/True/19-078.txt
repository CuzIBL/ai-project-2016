 
　　　winner-take-all  wta  structures are currently used in massively parallel  connectionist  networks to represent competitive behavior among sets of alternative hypotheses. however  this form of competition might be too rigid and not be appropriate for certain applications. for example  applications that involve noisy and erroneous inputs might mislead wta structures into selecting a wrong outcome. in addition  for networks that continuously process input data  the outcome must dynamically change with changing inputs; wta structures might  lock-in  on a previous outcome. this paper offers an alternative competition model for these applications. the model is based upon a meta-network representation scheme called network regions that are analogous to net spaces in partitioned semantic networks. network regions can be used in many ways to clarify the representational structure in massively parallel networks. this paper focuses on how they are used to provide a flexible and adaptive competition model. regions can be considered as representational units that represent the conceptual abstraction of a collection of nodes  or hypotheses . through this higher-level abstraction  regions can better influence the collective behavior of nodes within the region. several ai applications were used to test and evaluate this model. 
i. introduction 
　　　winner-take-all  wta  structures  l  represent competitive behavior among sets of alternative hypotheses in massively parallel networks  1  1  1 . these networks consist of large numbers of simple processing elements that give rise to emergent collective properties. the behavior of such networks has been shown to closely match human cognition in many tasks  such as natural language understanding and parsing  learning  speech perception and recognition  speech generation  physical skill modeling  vision and others. 
　　　in a wta structure  whenever there is any activation  the structure forces only the node  which may represent a hypothesis  with the highest output level to remain activated  while the other nodes die out. this mechanism allows one to define  decision points  that 


figure 1. wta structure that represents competing weak fricatives. 
　　　in the example  the link weights for inhibition and activation links are arbitrarily set to be -1 and 1 respectively and the decay factors are set to 1. figure 1 is the final state after the inputs -voiced  1-labial  and -ffricative were activated and the network was relaxed. as the figure shows  the wta structure enabled / / to compete and suppress the other candidates. 

output suppress nodes with lower output. there are two parameters that influence this competitive behavior - the inhibition link weights and the nodes' decay factor. the link weights define the degree of competition. 
lower link weights represent a milder  slower form of competition. higher link weights represent stronger  quicker competition. the decay factor  on the other hand  controls how well a network retains the decision made by the wta structure and allows the network to reset itself in the absence of inputs. in this paper  these two parameters are assumed to be uniform. 
	n . 	i s s u e s 
     although wta structures have been used successfully in many ai applications  this model of competition might not always be appropriate. for example  applications that involve noisy and erroneous inputs might mislead wta structures into forming a wrong decision. in addition  for networks which continuously process input data  the network must be able to dynamically change decisions based upon changing inputs. wta structures might  lock-in  on a particular decision. the following outlines the key problems when wta structures are used in these applications. 
     overly rigid competition - since the mutual inhibition link weights are fixed  the degree of competition is also fixed. if the input data is not at a consistent activation level or if new hypotheses are encoded into the structure  the competition may be too weak or too strong. competition that is too weak may result in multiple activations or decisions  some of which may be conflicting. excessive competition may result in a structure that is very sensitive to the output levels of the nodes. this may mislead a wta structure into prematurely forming an incorrect decision. 
decisions are not flexible - once a wta structure 
 locks-in  on one interpretation  it is difficult to shift to another interpretation as the input changes. in other words  newly activated nodes will have little chance of competing with the current  winner . flexible decision making is essential for dynamic systems where the network continuously processes different inputs. the decay factor does alleviate the  lock-in  effect by allowing the winning node to gradually decay  but may not be in time to accommodate changes in the input. 
     no likelihood information - the output level of a node may sometimes be considered a  likelihood  or  certainty  measure . wta structures  by definition  allow only one  winner   thus the output levels of other nodes are suppressed and this  likelihood  information is lost. in certain applications  it is more desirable to have several outcomes with varying output levels to reflect the likelihood of these outcomes. 
     in general  wta structures are useful in applications where only one outcome is desired and where the input data is not noisy and can be clearly categorized. in these well-defined applications  some of the above mentioned problems can be solved by using building blocks or combination rules in constructing the wta structure  l . however  for applications involving real world data with large variance  a more flexible and adaptive competition is needed. this paper presents an alternative model of competition for these applications. 
h i . n e t w o r k r e g i o n s 
     network region is a meta-network structure that represents the conceptual abstraction of a collection of nodes or hypotheses. intuitively  it represents a  chunk  of network where well-behaved properties can be defined. graphically  a region is displayed as a solid-lined rectangular box around the collection of nodes it represents. 
     the following figure shows the previous wta structure enclosed in a network region  labeled wf  which represents the category of weak fricatives. 

figure 1. a region representing the category of weak fricatives. 
     conceptually  there are similarities between network regions and partitioned semantic networks . in semantic networks  nodes and arcs are partitioned into net spaces  while in our massively parallel networks  nodes and arcs are partitioned into network regions. the net spaces are used mainly to delimit the scopes of quantified variables. in this paper  network regions are used to delimit sets of competing hypotheses. in addition  besides being conceptual units  network regions are computational entities just as nodes and links are. network regions can also activate or inhibit other nodes or regions  similar to spaces that can be linked to other parts of a semantic network. 
     regions are in effect higher-order nodes with internal parameters  that have node counterparts  i.e. potential level  output level  and inputs  where: 
s - sum of all the nodes' output within region. 
p - average potential of activated nodes in region. 
v - average output of activated nodes in region. 
ir - average input level activated nodes in region. 
these parameters act as indicators of the collective state of the nodes within a region and are updated during each 
	chun  bookman  and afshartoua 	1 

relaxation cycle. this provides an abstract view of a set of nodes that can be used either to influence the network system outside the region  or to better control the collective behavior of nodes within the region. 
     in terms of influencing the network system outside a given region  regions can be used to encode partition of concerns . in the above example  even though the net-
work may still be determining the correct weak fricative  the state of the wf region  or more precisely  the value of vr  can immediately provide information to other network structures which may only need to know that a weak fricative is present and not necessarily which particular weak fricative. this allows phonological rules  that apply to all weak fricatives  to trigger whenever there is any activity in the wf region. 
     regions also simplify the task of knowledge encoding by permitting knowledge to be encoded at appropriate levels of abstraction. in speech recognition  phonological rules are used at various levels of abstraction. through the use of regions  rules can be encoded into connectionist networks at the same level they are expressed. an example rule is: if a final voiced fricative is actually voiced  then the following sound is voiced. this may be encoded with a region that represents the set of all voiced fricatives. if this region is active and is followed by a silence  then the system should expect a +voiced sound to follow. without the regions representation  this knowledge has to be encoded for each voiced fricative separately. with regions  knowledge is expressed at a more appropriate level of abstraction  that allows networks to be more modular and comprehensible. 
     the interaction of regions with other network structures is discussed in  bookman and chun  1 . the current paper focuses on the use of network regions to influence the collective competitive behavior of nodes within a region. 
a. n-region 
     the n-region  normalizing region  provides activation stability for competing hypotheses. in addition this mechanism allows  likelihood  information to be maintained and permits competition that is more tolerant to variations in input levels. 
     in certain applications  it is useful to have more than one outcome from a set of alternative hypotheses. in these cases  the output level of a node may be considered as a  likelihood  or  certainty  of a particular hypothesis . since wta structures only allow one  winner   the output levels of the other nodes are suppressed and the  likelihood  information is lost. the n-region  without the mutual inhibition links  maintains this likelihood information within a set of alternative hypotheses. if we consider a hypothesis by itself  the output level would indicate the  likelihood  of this hypothesis as determined by the current low-level inputs. for example  if only half of the inputs to a particular hypothesis are activated  the hypothesis may have an output level of 1. this information is only useful if we are considering how well a particular hypothesis matches the current input. however  when hypotheses are in competition  it is more informative to have the output level indicate the likelihood of a particular hypothesis among all other competing hypotheses. 
     the n-region accomplishes this by a normalizing computation that limits the maximum total output within a region  the value of sr of the n-region  to 
1. in other words  the total  likelihood  within the nregion will not exceed 1. the general idea of normalization is of course not new  however with network regions the normalization is integrated into the relaxation process itself. figure 1 and table 1 shows the output levels of the four weak fricatives with and without the n-region  after the inputs -voiced  -labial and +fricative were activated for 1 cycle. 

     the output level of /v/ with the wta structure indicates that there is a 1 likelihood that /v/ is matched just by considering only the inputs  only twothirds of the features for /v/ were activated  . this output level is misleading  since /f/  /v/ and /o/ are all highly activated. this would misinform other network structures that all three were closely matched. however  with the n-region  the output level of /v/ is adjusted to 1  indicating the likelihood of /v/ among the four alternatives is only 1. this mechanism puts a hypothesis in perspective among all the other competing hypotheses. 
     this approach is similar to the ub  upper-bound  parameter proposed in  feldman and ballard  1 . 
in our example  the activation link weighty are uniformly set to 1. in evi-
dential reasoning systems  this weight would be e   hypothesis 1 input . however  the utility of the n-region is the same. 

however  instead of inhibiting all the nodes equally  the 
n-region proportionally adjusts the nodes output level to maintain the  likelihood  information. 
the virtual lateral inhibition proposed by  reggia  
1  also maintains a normalized activation in each layer when only a single input node is activated at a time. since the effect of this competition model is similar to wta structures  the network might also  lock-in  on a premature outcome. 
     since there are no inhibition links between competing nodes in the n-region  it does not encode active competition or contour enhancement   filter out noise by suppressing nodes with low output level . however  there is implicit competition through the conservation of activation within the n-region. in addition  by this lack of active competition  n-regions will not  lockin  on a particular decision. if active competition is required  the n-region can be used in conjunction with a wta structure. 
     the amount of competitive inhibition received by a node in a wta structure varies with the total output level in the structure. after the wta structure is finetuned for a particular input activation level  the performance varies when the input level increases or decreases. when n-regions are used on top of wta structures  the n-regions maintain the total output level to be constant. hence  a more uniform competition results since the total amount of competition will not vary with input levels. this also eliminates the problem of too weak a competition where several hypotheses get fully activated. with n-regions  the activation of these hypotheses will be normalized. 
     to summarize  the most significant advantages of n-regions are that they maintain  likelihood  information  permit uniform competition  eliminate excessive competition  allow multiple outcomes  and will not  lock-in  on a single  winner . 
b. c-region 
     the c-region  competitive region  provides an adaptive and flexible competition that permits graceful shifting of decisions made when inputs change. 
     the wta competition is rigid because the weights on the inhibition links are fixed. when input levels are not consistent  as in the case of noisy data  competition may either be too weak or too strong. for example  in speech recognition  the clarity in which certain phonemes are spoken varies greatly within each word. the cregion avoids this problem by having the inhibition link weights be sensitive to the current inputs to the region  i.e. the value of ir in the region . as the cregion relaxes  the inhibition link weights are adjusted to adapt to the current input level. 
competition in the c-region is defined as: competition. = ki lr wi e vi  i = j  
where kj is a constant  indicating the sensitivity to the input. in contrast to the original wta competition  the  effective inhibition weight  is now  ki ir wi. . the parameter ki is set so that the  effective inhibition weight  for the expected input levels is the same as the original wta inhibition weight. 
the following experiments show how the c-
region adapts to varying input levels. in the examples  the activation link weights from the inputs are 1. the inhibition link weights in both the wta structure and the c-region are -1. the proportionality constant kj is set to 1 so that the  effective inhibition weight of the c-region is approximately equal to the inhibition weight  i.e. -1  of the wta structure when inputs nodes are fully activated. 
     in experiment 1  when the input nodes were fully activated  the outcome   winner   will shift when inputs change for both the wta and c-region. however  when the average input level  /  drops to 1  the wta structure  locks in  on the previous outcome  while the c-region will shift to the correct decision. table 1 shows that the wta locked onto /$/  while the cregion correctly shifted to /s/. figure 1 shows the final state of the network at the end of 1 cycles. 

     in experiment 1  see table 1   the average input level  /  for the first 1 cycles was lower than expected  only 1 . however  both network structures still settled on the correct outcome of /f/. in the previous experiment  the wta structure failed because it  locked in  on a previous outcome. here  there is no previous outcome to lock onto. however  when the input was changed and the average input level dropped further to 1  the wta structure failed while the c-region still chun  bookman  and afshartous 1 

     the main advantage of the c-region is that it provides a flexible competition which adapts to the current input level. the decision made by the structure is less dependent on previous decisions  i.e. will not  lock in  on previous results . this ability to graceful change decisions is crucial in networks which continuously process different inputs at varying levels. when the input level is higher than the expected value  the competition in both the wta structure and c-region might be too strong. consequently  more negative competition activation will be spread. if this activation is higher than the nodes1 output level  nodes in both structures may toggle between active and inactive states. the cn-region eliminates this problem. 
c. cn-region 
     cn-regions  used on top of wta structures  combine the advantages of both c-regions and nregions. the n-region maintains uniform competition with possibly multiple outcomes  while the cregion adapts competition to varying input levels. 
when average input levels are lower than expected  the 
c-region can still permit the inputs to influence the competition. when average input levels are higher than expected  the n-region prevents excessive competition. the net effect of combining these two is that competition is highly flexible and uniform. the possibility of too strong or too weak competition is greatly reduced. 
     the effects of the cn-region are similar to the competition in grossberg's on-center off-surround network . groasberg's network uses a quenching threshold to limit which nodes will be activated. in essence  the adaptive competition in the cn-region is similar to a self-adjusting quenching threshold that changes with the current input level. 
i v . a p p l i c a t i o n s 
     the network region model of competition was tested in three ai applications - high-level vision  isolated word recognition  and microfeature-based natural language understanding. these examples illustrate the effectiveness of wta structures and network regions in different classes of applications. the input to the labeling problem of high-level vision is well-defined without noise  whereas the input to the word recognition example is highly noisy and ambiguous. the natural language example shows the importance of flexible decision making. all examples were implemented using the alnet-1 system   a massively parallel network simulator and development environment  which runs on symbolics lisp machines. 
a. high-level vision 
     this section investigates the line drawing labeling problem  of high-level vision. in our line drawing labeling system  each junction in an object is represented by a wta structure with nodes that represent the possible labels for that junction. the label of a junction is rigidly constrained; it must agree with the labels of adjacent junctions. these constraints are represented as activation links between adjacent junctions. this method of constraint propagation by spreading activation mimics the effects of the waltz filtering algorithm. 
     an example object and its correct labeling is shown in figure 1. the three types of junctions that occur in this object are the l's  forks  and arrows  see figure 1 . 

     all nodes were initially active  to indicate all labels are equally possible. as the network relaxed  nodes which did not satisfy the defined constraints died out. eventually  the network converges to the correct labeling as shown by the darkened nodes in figure 1. 
     in our experiments  networks performed correctly as long as there was a balance between the inhibition and activation link weights. both excessive inhibition and weak activation caused the network to converge incorrectly. weak inhibition and excessive activation resulted in multiple labels being selected at some of the junctions. 

figure 1. network with wta structures to perform labeling. 
     wta structures are effective for the line drawing labeling problem and other domains where constraints are rigidly defined. the problem of  locking in  on a decision is not relevant here since evidence for a particular hypothesis  label  cannot vary with time. however  as we shall see in the following sections  this problem becomes more significant in applications where evidence 
for a hypothesis can vary with time. 
b. isolated word recognition 
an isolated word recognition system  called seco 
 1  1   was used as an application with highly noisy and inconsistent inputs. this system recognizes spoken letter-names and digits. it partially evolved from a mas-
sively parallel model of word perception called cohort . the structure of seco is similar to cohort with the addition of a temporal constraint layer. 
     the word recognition network consists of five layers - phonetic feature  e.g. vowels  stops  etc.  layer that is activated by an lpc-based front-end processor  phonetic segment  e.g. /f/  /v/  etc.  layer  phonetic segment token layer  temporal constraint layer  and a word layer which is the output of this system. nodes in the 
	chun  bookman  and afahartout 	1 

     when a word is spoken  the duration of the speech varies with the speaker and the current context. this variation in the duration of input data causes problems for wta structures. if an utterance is longer than expected  word level nodes will be highly activated before the utterance is complete and prematurely settle on a ''winner.  the normalizing effect of the cn-region prevents this  premature  recognition by adjusting the output levels to represent the current  likelihood  of a word at any point during the recognition process. 
     when competition is too weak  the wta structure may fully activate more than one word node. the outcomes may also be activated before utterances is completed. with cn-region  normalization reduces the nodes' output to better indicate their  likelihoods.  figure 1 shows an example where the input utterance is  three.  this particular utterance is somewhat noisy and the network confuses this with  four  while slightly favoring  three . the plot shows the wta structure activates both  three  and  four.  this would misinform other network structures that both words were recognized. in the case of cn-region  figure 1    three  and  four  are only partially activated. 
the top layer is a  local  connectionist model  the bottom layer  a distributed layer of  microfeatures.  the microfeatures are used as a basis for defining nodes  i.e. concepts  hypotheses  in the top layer  at least partially  and to associate the node with others that share its microfeatures. each node in the top layer  is connected via bi-directional links to only those microfeatures that describe it. conceptually related nodes have common microfeatures. 
     the following is one of the sentences used to contrast the competition found in wta structures with cn-regions. 
 john went driving with five bucks in his truck.  
the network that models this sentence is shown in figure 1  only the top layer is shown . the rectangular nodes in the center of the figure represent the input words. the structure above this is the syntactic parse tree for the sentence. the elliptical nodes below the input words represent the competing word senses. in addition  semantic constraints associated with the inputs  are encoded but not displayed. there is also a microfeature memory system that represents the currently active and inactive microfeatures  these are not shown in 

the figure . 
1 	knowledge representation 

in wta-1  the correct sense of truck is not chosen. 
here  initially the input driving activates the transport sense of truck  but later inputs activate the vehicle sense of the word. however  they are too late to enter competition. this was not the case with the cn-region  which correctly shifted to the vehicle sense of the word. 
     results from our experiments indicate several problems that wta structures pose for natural language processing. first  the semantic knowledge of the network and its microfeatures varies with one's experience. there is no one correct set. thus  slight variations in the network's microfeature set will cause problems for wta networks which are sensitive to transiently higher input. this can lead to premature stabilization of activation. on the other hand  the cn-regions allow smooth competitions among the microfeatures  the syntactic structures  the semantic constraints  and the input words  thus enabling subtle differences in meaning to exist. secondly  the processing of a sentence is sequential over time. as a result  one's understanding of the sentence and the sentences that follow are gradually refined as one processes the inputs. a wta may prematurely  lock in  on a word sense and ignore later information. whereas  cn-region permits a dynamic flexibility in shifting from one interpretation to another. in addition  wta does not provide a good representation for ambiguous meanings  it always choose the one that is transiently higher. cn-region allows multiple outcomes each associated with a ''likelihood  measure. 
v . r e s e a r c h d i r e c t i o n s 
     the research documented here represents the first step in utilizing network regions to provide higher-level representation in massively parallel networks. as regions can provide a means of creating hierarchies  as well as shared structures  it may be possible to use current learning techniques to create networks that can generalize from existing concepts to form new regions that represent these generalized concepts. this is a topic for future research. currently we are investigating the effects of interaction among regions   that is  the computational mechanisms needed to represent the interaction of higher-order concepts. 
a c k n o w l e d g e m e n t s 
     we would like to thank dave waltz and the members of the brandeis ai group for their comments. we would also like to thank the members of the knowledge engineering center at honeywell bull. 
r e f e r e n c e s 
 l  feldman  j.a. and d.h. ballard   connectionist models and their properties   cognitive science  1  1  pp.1. 
 hinton  g.e. and j.a. anderson  eds.   parallel models of associative memory  hillsdale: lawrence erlbaum associates  1. 
 rumelhart  d.e. & mcclelland  j.l.  parallel distributed processing: explorations in the microstructure of cognition volume 1  mit press  1. 
1  mcclelland  j.l. & d.e. rumelhart  parallel distributed processing: explorations in the microstructure of cognition volume 1  mit press  1. 
1  shastri  l. and j.a. feldman  ''semantic networks and neural nets   computer science department  the university of rochester  tr1  june  1. 
1  hendrix  g.g.   expanding the utility of semantic 
networks through partitioning   in proceedings of 
ijcai-1  1  pp.1 
1  brachman  r.j. and h.j. levesque   on the partitioning of concerns in knowledge representation   in proceedings of aaa1  1. 
1  bookman  l.a. and ii.w. chun  ''a model of higher-
order concept interaction using network regions   
computer science department  brandeis university  technical report  cs-1  massachusetts  1. 
1  reggia  j.a.   virtual lateral inhibition in parallel activation models of associative memory   in proceedings of ijcai-1  1  pp.1. 
1  grossberg  s.   contour enhancement  short term memory  and constancies in reverberating neural networks   studies in applied mathematics  vol. lii  no. 1  september  1  pp.1. 
1  chun  h.w.   ainet-1 user's manual   computer science department  brandeis university  technical report  cs-1  massachusetts  1. 
1  waltz  d. l.   understanding line drawings of scenes with shadows   in the psychology of computer vision  p. winston  ed   mcgraw-hill  1. 
1  chun  h.w.   a representation for temporal sequence and duration in massively parallel networks: exploiting link interactions   in proceedings of aaai-1  august 1. 
1  wong  m.k. and h.w. chun   toward a massively parallel system for word recognition   1 ieee international conference on acoustics  speech  and signal processing  tokyo  japan  april 1. 
1  elman  j.l. and j.l. mcclelland   speech perception as a cognitive process: the interactive activation model   in n. lass  ed.   speech and language: vol. x.  orlando  florida  academic  1. 
1  bookman  l.a.   a microfeature based scheme for modelling semantics   in proceedings of ijcai-1  1. 
1  cottrell  g.w.  a connectionist approach to word sense disambiguation    phd thesis  tr 1  u. of rochester cs dept.  1. 
1  waltz  d.l. and j.b. pollack   massively parallel parsing: a strongly interactive model of natural 
language interpretation   cognitive science  volume 1  number 1  january-march  1. 
	chun  bookman  and afshartous 	1 
1 	knowledge representation 



1 	knowledge representation 

1 	knowledge representation 

1 	knowledge representation 

1 	knowledge representation 

1 	knowledge representation 























