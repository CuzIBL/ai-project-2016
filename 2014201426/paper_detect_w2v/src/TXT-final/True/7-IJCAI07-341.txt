
human object recognition in a physical 1-d environment is still far superior to that of any robotic vision system. we believe that one reason  out of many  for this-one that has not heretofore been significantly exploited in the artificial vision literature-is that humans use a fovea to fixate on  or near an object  thus obtaining a very high resolution image of the object and rendering it easy to recognize. in this paper  we present a novel method for identifying and tracking objects in multi-resolution digital video of partially cluttered environments. our method is motivated by biological vision systems and uses a learned  attentive  interest map on a low resolution data stream to direct a high resolution  fovea.  objects that are recognized in the fovea can then be tracked using peripheral vision. because object recognition is run only on a small foveal image  our system achieves performance in real-time object recognition and tracking that is well beyond simpler systems.
1 introduction
the human visual system far outperforms any robotic system in terms of object recognition. there are many reasons for this  and in this paper  we focus only on one hypothesis- which has heretofore been little exploited in the computer vision literature-that the division of the retina into the foveal and peripheral regions is of key importance to improving performance of computer vision systems on continuous video sequences. briefly  the density of photoreceptor rod and cone cells varies across the retina. the small central fovea  with a high density of color sensitive cone cells  is responsible for detailed vision  while the surrounding periphery  containing a significantly lower density of cones and a large number of monochromatic rod cells  is responsible for  among other things  detecting motion. estimates of the equivalent number of  pixels  in the human eye vary  but based on spatial acuity of 1 degrees  edelman and weiss  1;
fahle and poggio  1   appears to be on the order of 1〜1 pixels. for a more in-depth treatment of human visual perception  we refer the reader to one of the many excellent textbooks in the literature  for example  wandell  1 .
　because very little changes in a visual stream from one frame to the next  one expects that it is possible to identify objects or portions of a scene one at a time using the high resolution fovea  while tracking previously identified objects in the peripheral region. the result is that it is possible to use computationally expensive classification algorithms on the relatively limited portion of the scene on which the fovea is fixated  while accumulating successful classifications over a series of frames in real-time.
　a great deal has happened in recent years in the area of location and identification of specific objects or object classes in images. much of this work  however  has concentrated on single frames with the object of interest taking up a significant proportion of the field-of-view. this allows for accurate and robust object recognition  but implies that if we wish to be able to find very small objects  we must go to much higher image resolutions.1 in the naive approach  there is significant computational cost of going to this higher resolution.
　for continuous video sequences  the standard technique is simply to treat each frame as a still image  run a classification algorithm over the frame  and then move onto the next frame  typically using overlap to indicate that an object found in frame n + 1 is the same object found in frame n  and using a kalman filter to stabilize these measurements over time. the primary difficulty that this simple method presents is that a tremendous amount of effort is misdirected at the vast majority of the scene which does not contain the objects we are attempting to locate. even if the kalman filter is used to predict the new location of an object of interest  there is no way to detect the appearance of new objects which either enter the scene  or which are approached by a camera in motion.
　the solution we propose is to introduce a peripheral-foveal model in which attention is directed to a small portion of the visual field. we propose using a low-resolution  wide-angle video camera for peripheral vision and a pan-tilt-zoom  ptz  camera for foveal vision. the ptz allows very high resolution on the small portion of the scene in which we are interested at any given time. we refer to the image of the scene supplied by the low-resolution  wide-angle camera as the peripheral view and the image supplied by the pan-tilt-zoom camera as the foveal view or simply fovea.
　we use an attentive model to determine regions from the peripheral view on which we wish to perform object classification. the attentive model is learned from labeled data  and can be interpreted as a map indicating the probability that any particular pixel is part of an unidentified object. the fovea is then repeatedly directed by choosing a region to examine based on the expected reduction in our uncertainty about the location of objects in the scene. identification of objects in the foveal view can be performed using any of the state-of-the-art object classification technologies  for example see  viola and jones  1; serre et al.  1; brubaker et al.  1  .
　the ptz camera used in real-world robotic applications can be modeled for study  with less peak magnification  using a high-definition video  hdv  camera  with the  foveal  region selected from the video stream synthetically by extracting a small region of the hdv image. the advantage of this second method is that the input data is exactly reproducible. thus we are able to evaluate different foveal camera motions on the same recorded video stream. figure 1 shows our robot mounted with our two different camera setups.

figure 1: stair platform  left  includes a low-resolution peripheral camera and high-resolution ptz camera  top-right   and alternative setup with hdv camera replacing the ptz  bottom-right . in our experiments we mount the cameras on a standard tripod instead of using the robotic platform.
　the robot is part of the stair  stanford artificial intelligence robot  project  which has the long-term goal of building an intelligent home/office assistant that can carry out tasks such as cleaning a room after a dinner party  and finding and fetching items from around the home or office. the ability to visually scan its environment and quickly identify objects is of one of the key elements needed for a robot to accomplish such tasks.
the rest of this paper is organized as follows. section 1 outlines related work. in section 1 we present the probabilistic framework for our attention model which directs the foveal gaze. experimental results from a sample hdv video stream as well as real dual-camera hardware are presented in section 1. the video streams are of a typical office environment and contain distractors  as well as objects of various types for which we had previously trained classifiers. we show that the performance of our attention driven system is significantly better than naive scanning approaches.
1 related work
an early computational architecture for explaining the visual attention system was proposed by koch and ullman  1  . in their model  a scene is analyzed to produce multiple feature maps which are combined to form a saliency map. the single saliency map is then used to bias attention to regions of highest activity. many other researchers  for example  hu et al.  1; privitera and stark  1; itti et al.  1   have suggested adjustments and improvements to koch and ullman's model. a review of the various techniques is presented in  itti and koch  1 .
　a number of systems  inspired by both physiological models and available technologies  have been proposed for finding objects in a scene based on the idea of visual attention  for example  tagare et al.  1  propose a maximum-likelihood decision strategy for finding a known object in an image.
　more recently  active vision systems have been proposed for robotic platforms. instead of viewing a static image of the world  these systems actively change the field-of-view to obtain more accurate results. a humanoid system that detects and then follows a single known object using peripheral and foveal vision is presented in  ude et al.  1 . a similar hardware system to theirs is proposed in  bjorkman and： kragic  1  for the task of object recognition and pose estimation. and in  orabona et al.  1  an attention driven system is described that directs visual exploration for the most salient object in a static scene.
　an analysis of the relationship between corresponding points in the peripheral and foveal views is presented in  ude et al.  1   which also describes how to physically control foveal gaze for rigidly connected binocular cameras.
1 visual attention model
our visual system comprises two separate video cameras. a fixed wide-angle camera provides a continuous lowresolution video stream that constitutes the robot's peripheral view of a scene  and a controllable pan-tilt-zoom  ptz  camera provides the robot's foveal view. the ptz camera can be commanded to focus on any region within the peripheral view to obtain a detailed high-resolution image of that area of the scene. as outlined above  we conduct some of our experiments on recorded high-resolution video streams to allow for repeatability in comparing different algorithms.
　figure 1 shows an example of a peripheral and foveal view from a typical office scene. the attention system selects a foveal window from the peripheral view. the corresponding region from the high-resolution video stream is then used for

figure 1: illustration of the peripheral  middle  and foveal  right  views of a scene in our system with attentive map showing regions of high interest  left . in our system it takes approximately 1 seconds for the ptz camera to move to a new location and acquire an image.classification. the attentive interest map  generated from features extracted from the peripheral view  is used to determine where to direct the foveal gaze  as we will discuss below.
　the primary goal of our system is to identify and track objects over time. in particular  we would like to minimize our uncertainty in the location of all identifiable objects in the scene in a computationally efficient way. therefore  the attention system should select regions of the scene which are most informative for understanding the robot's visual environment.
　our system is able to track previously identified objects over consecutive frames using peripheral vision  but can only classify new objects when they appear in the  high-resolution  fovea  f. our uncertainty in a tracked object's position grows with time. directing the fovea over the expected position of the object and re-classifying allows us to update our estimate of its location. alternatively  directing the fovea to a different part of the scene allows us to find new objects. thus  since the fovea cannot move instantaneously  the attention system needs to periodically decide between the following actions:
a1. confirmation of a tracked object by fixating the fovea over the predicted location of the object to confirm its presence and update our estimate of its position;
a1. search for unidentified objects by moving the fovea to some new part of the scene and running the object classifiers over that region.
　once the decision is made  it takes approximately 1 seconds-limited by the speed of the ptz camera-for the fovea to move to and acquire an image of the new region.1during this time we track already identified objects using peripheral vision. after the fovea is in position we search for new and tracked objects by scanning over all scales and shifts within the foveal view as is standard practice for many stateof-the-art object classifiers.
　more formally  let ξk t  denote the state of the k-th object  ok  at time t. we assume independence of all objects in the scene  so our uncertainty is simply the sum of entropy terms over all objects 
unidentified ξk t  	 1  ok（t	ok（t/
where the first summation is over objects being tracked  t   and the second summation is over objects in the scene that have not yet been identified  and therefore are not being tracked .
　since our system cannot know the total number of objects in the scene  we cannot directly compute the entropy over unidentified objects . instead  we learn the probability p ok | f  of finding a previously-unknown object in a given foveal region f of the scene based on features extracted from the peripheral view  see the description of our interest model in section 1 below . if we detect a new object  then one of the terms in the rightmost sum of eq.  1  is reduced; the expected reduction in our entropy upon taking action a1  and fixating on a region f  is
unidentified ξk t  

here the term hunidentified ξk t   is a constant that reflects our uncertainty in the state of untracked objects 1 and htracked ξk t + 1   is the entropy associated with the kalman filter that is attached to the object once it has been detected  see section 1 .
　as objects are tracked in peripheral vision  our uncertainty in their location grows. we can reduce this uncertainty by observing the object's position through re-classification of the area around its expected location. we use a kalman filter to track the object  so we can easily compute the reduction in entropy from taking action a1 for any object ok （ t  
	|	|	 1 
where Σk t  and Σk t + 1  are the covariance matrices associated with the kalman filter for the k-th object before and after re-classifying the object  respectively.
　in this formalism  we see that by taking action a1 we reduce our uncertainty in a tracked object's position  whereas by taking action a1 we may reduce our uncertainty over unidentified objects. we assume equal costs for each action and therefore we choose the action which maximizes the expected reduction of the entropy h defined in eq.  1 .
we now describe our tracking and interest models in detail.
1 object tracking
we track identified objects using a kalman filter. because we assume independence of objects  we associate a separate kalman filter with each object. an accurate dynamic model of objects is outside the current scope of our system  and we use simplifying assumptions to track each object's coordinates in the 1-d image plane. the state of the k-th tracked object is
		 1 
and represents the  x y -coordinates and  x y -velocity of the object.
　on each frame we perform a kalman filter motion update step using the current estimate of the object's state  position and velocity . the update step is
		 1 
where ηm ゛ n 1 Σm  is the motion model noise  and Δt is the duration of one timestep.
　we compute optical flow vectors in the peripheral view generated by the lucas and kanade  1  sparse optical flow algorithm. from these flow vectors we measure the velocity of each tracked object  zv t   by averaging the optical flow within the object's bounding box. we then perform a kalman filter observation update  assuming a velocity observation measurement model
		 1 
where ηv ゛ n 1 Σv  is the velocity measurement model noise.
　when the object appears in the foveal view  i.e.  after taking action a1  the classification system returns an estimate  zp t   of the object's position  which we use to perform a corresponding kalman filter observation update to greatly reduce our uncertainty in its location accumulated during tracking. our position measurement observation model is given by
		 1 
where ηp ゛ n 1 Σp  is the position measurement model noise.
　we incorporate into the position measurement model the special case of not being able to re-classify the object even though the object is expected to appear in the foveal view. for example  this may be due to misclassification error  poor estimation of the object's state  or occlusion by another object. in this case we assume that we have lost track of the object.
　since objects are recognized in the foveal view  but are tracked in the peripheral view  we need to transform coordinates between the two views. using the well-known stereo calibration technique of  zhang  1   we compute the extrinsic parameters of the ptz camera with respect to the wide-angle camera. given that our objects are far away relative to the baseline between the cameras  we found that the disparity between corresponding pixels of the objects in each view is small and can be corrected by local correlation.1this approach provides adequate accuracy for controlling the fovea and finding the corresponding location of objects between views.
　finally  the entropy of a tracked object's state is required for deciding between actions. since the state  ξ t   is a gaussian random vector  it has differential entropy
		 1 
where Σk t  （ r1〜1 is the covariance matrix associated with our estimate of the k-th object at time t.
1 interest model
to choose which foveal region to examine next  under action a1   we need a way to estimate the probability of detecting a previously unknown object in any region f in the scene. to do so  we define an  interest model  that rapidly identifies pixels which have a high probability of containing objects that we can classify. a useful consequence of this definition is that our model automatically encodes the biological phenomena of saliency and inhibition of return-the processes by which regions of high visual stimuli are selected for further investigation  and by which recently attended regions are prevented from being attended again  see  klein  1  for a detailed review .
　in our approach  for each pixel in our peripheral view  we estimate the probability of it belonging to an unidentified object. we then build up a map of regions in the scene where the density of interest is high. from this interest map our attention system determines where to direct the fovea to achieve maximum benefit as described above.
　more formally  we define a pixel to be interesting if it is part of an unknown  yet classifiable object. here classifiable  means that the object belongs to one of the object classes that our classification system has been trained to recognize. we model interestingness  y t   of every pixel in the peripheral view  x t   at time t using a dynamic bayesian network  dbn   a fragment of which is shown in figure 1. for the  i j -th pixel  yi j t  = 1 if that pixel is interesting at time t  and 1 otherwise. each pixel also has associated with it a vector of observed features φi j x t   （ rn.

figure 1: fragment of dynamic bayesian network for modeling attentive interest.
　the interest belief state over the entire frame at time t is p  y t  | y 1  x 1  ... x t  . exact inference in this graphical model is intractable  so we apply approximate inference to estimate this probability. space constraints preclude a full discussion  but briefly  we applied assumed density filtering/the boyen-koller  1  algorithm  and approximate this distribution using a factored representation over individual pixels . in our experiments  this inference algorithm appeared to perform well.
　in our model  the interest for a pixel depends both on the interest in the pixel's  motion compensated  neighborhood  n i j   at the previous time-step and features extracted from the current frame. the parameterizations for bothand p  yi j t  | φi j x t    are given by logistic functions  with learned parameters   and we use bayes rule to compute p  φi j x t   | yi j t   needed in the dbn belief update. using this model of the interest map  we estimate the probability of finding an object in a given foveal region  p ok | f   by computing the mean of the probability of every pixel in the foveal region being interesting.1
　the features φi j used to predict interest in a pixel are extracted over a local image patch and include harris corner features  horizontal and vertical edge density  saturation and color values  duration that the pixel has been part of a tracked object  and weighted decay of the number of times the fovea had previously fixated on a region containing the pixel.
1 learning the interest model
recall that we define a pixel to be interesting if it is part of an as yet unclassified object. in order to generate training data so that we can learn the parameters of our interest model  we first hand label all classifiable objects in a low-resolution training video sequence. now as our system begins to recognize and track objects  the pixels associated with those objects are no longer interesting  by our definition. thus  we generate our training data by annotating as interesting only those pixels marked interesting in our hand labeled training video but that are not part of objects currently being tracked. using this procedure we can hand label a single training video and automatically generate data for learning the interest model given any specific combination of classifiers and foveal movement policies.
　we adapt the parameters of our probabilistic models so as to maximize the likelihood of the training data described above. our interest model is trained over a 1 frame video sequence  i.e.  1 seconds at 1 frames per second. figure 1 shows an example of a learned interest map  in which the algorithm automatically selected the mugs as the most interesting region.

figure 1: learned interest. the righthand panel shows the probability of interest for each pixel in the peripheral image  left .
1 experimental results
we evaluate the performance of the visual attention system by measuring the percentage of times that a classifiable object appearing in the scene is correctly identified. we also count the number of false-positives being tracked per frame. we compare our attention driven method for directing the fovea to three naive approaches:
 i  fixing the foveal gaze to the center of view 
 ii  linearly scanning over the scene from top-left to bottomright  and 
 iii  randomly moving the fovea around the scene.our classifiers are trained on image patches of roughly 1〜1 pixels depending on the object class being trained. for each object class we use between 1 and 1 positive training examples and 1 negative examples. our set of negative examples contained examples of other objects  as well as random images. we extract a subset of c1 features  see  serre et al.  1   from the images and learn a boosted decision tree classifier for each object. this seemed to give good performance  comparable to state-of-the-art systems  for recognizing a variety of objects. the image patch size was chosen for each object so as to achieve good classification accuracy while not being so large so as to prevent us from classifying small objects in the scene.
　we conduct two experiments using recorded highdefinition video streams  the first assuming perfect classification  i.e.  no false-positives and no false-negatives  so that every time the fovea fixates on an object we correctly classify the object   and the second using actual trained state-of-theart classifiers. in addition to the different fovea control algorithms  we also compare our method against performing object classification on full frames for both low-resolution and high-resolution video streams. the throughput  in terms of frames per second  of the system is inversely proportional to the area scanned by the object classifiers. the low resolution was chosen to exhibit the equivalent computational requirements of the foveal methods and is therefore directly comparable. the high resolution  however  being of much larger size than the foveal region  requires additional computation time to complete each frame classification. thus  to meet real-time operating requirements  the classifiers were run ten times less often.
　figure 1 shows example timelines for recognizing and tracking a number of different objects using our method  bottom   randomly moving the fovea  middle   and using a fixed fovea  top . each object is shown on a different line. a dotted line indicates that the object has appeared in the scene but has not yet been recognized  or that our tracking has lost it . a solid line indicates that the object is being tracked by our system. the times when the fovea fixates on each object are marked by a diamond   . it is clear that our method recognizes and starts tracking objects more quickly than the other methods  and does not waste effort re-classifying objects in regions already explored  most apparent when the fovea is fixed .

figure 1: example timelines for identification and tracking of objects in a continuous video sequence using different methods for controlling the fovea: fixed  top   random  middle   and our interest driven method  bottom .
　in our experiments we use a fixed size fovea covering approximately 1% of the peripheral field-of-view.1 all recognizable objects in our test videos were smaller than the size of the foveal window. we record peripheral and foveal video streams of an office environment with classifiers trained on commonly found objects: coffee mugs  disposable cups  staplers  scissors  etc. we hand label every frame of the video stream. the video sequence consists of a total of 1 frames recorded at 1 frames per second-resulting in 1 foveal fixations  since it takes approximately seven frames for the fovea to move .
　a comparison between our method and the other approaches is shown in table 1. we also include the f1-score when running state-of-the-art object classifiers in table 1.
fovea controlperfectactualclassificationclassifiersfull image  low-resolution n/a1%afull image  high-resolution n/a1%fixed at center1%1%linear scanning1%1%random scanning1%1%our  attentive  method1%1%
a
　　our object classifiers are trained on large images samples  of approximately 1〜1 pixels . this result illustrates that objects in the low-resolution view occupy too few pixels for reliable classification.
table 1: percentage of objects in a frame that are correctly identified using different fovea control algorithms.
fovea controlrecallprecisionf1-scorefull image  low-res 1%1%1%full image  high-res 1%1%1%fixed at center1%1%1%linear scanning1%1%1%random scanning1%1%1%our method1%1%1%table 1: recall  precision and f1-score for objects appearing the scene. the low-resolution run results in an f1-score of zero since objects appearing in the scene occupy too few pixels for reliable classification.
　the results show that our attention driven method performs significantly better than the other foveal control strategies. our method even performs better than scanning the entire high-resolution image. this is because our method runs much faster since it only needs to classify objects in a small region. when running over the entire high-resolution image  the system is forced to skip frames so that it can keep up with realtime. it is therefore less able to detect objects entering and leaving the scene than our method. furthermore  because we only direct attention to interesting regions of the scene  our method results in significantly higher precision  and hence better f1-score  than scanning the whole image.
　finally  we conduct experiments with a dual wide-angle and ptz camera system. in order to compare results between the different foveal control strategies  we fix the robot and scene and run system for 1 frames  approximately 1 foveal fixations  for each strategy. we then change the scene by moving both the objects and the robot pose  and repeat the experiment  averaging our results across the different scenes. the results are shown in table 1. again our attention driven method performs better than the other approaches.
videos demonstrating our results are provided at http://ai.stanford.edu/゛sgould/vision/
fovea controlrecallprecisionf1-scorefixed at centera1%1%1%linear scanning1%1%1%random scanning1%1%1%our method1%1%1%
a
　　in some of the trials a single object happened to appear in the center of the field of view and hence was detected by the stationary foveal gaze.
table 1: recall  precision and f1-score of recognizable objects for hardware executing different foveal control strategies over a number of stationary scenes.
1 discussion
in this paper  we have presented a novel method for improving performance of visual perception on robotic platforms and in digital video. our method  motivated from biological vision systems  minimizes the uncertainty in the location of objects in the environment using a with controllable pan-tilt-zoom camera and fixed wide-angle camera. the pan-tilt-zoom camera captures high-resolution images for improved classification. our attention system controls the gaze of the pan-tilt-zoom camera by extracting interesting regions  learned as locations where we have a high probability of finding recognizable objects  from a fixed-gaze lowresolution peripheral view of the same scene.
　our method also works on a single high-resolution digital video stream  and is significantly faster than naively scanning the entire high-resolution image. by experimentally comparing our method to other approaches for both high-definition video and on real hardware  we showed that our method fixates on and identifies objects in the scene faster than all of the other methods tried. in the case of digital video  we even perform better than the brute-force approach of scanning the entire frame  because the increased computational cost of doing so results in missed frames when run in real-time.
acknowledgments
we give warm thanks to morgan quigley for help with the stair hardware  and to pieter abbeel for helpful discussions. this work was supported by darpa under contract number fa1-1.
