 
a common program optimization strategy is to eliminate recomputation by caching and reusing results. we analyze the problems involved in automating this strategy: deciding which computations are safe to cache  transforming the rest of the program to make them safe  choosing the most cost-effective ones to cache  and maintaining the optimized code. the analysis extends previous work on caching by considering side effects  shared data structures  program edits  and the acceptability of behavior changes caused by caching. the paper explores various techniques for solving these problems and attempts to make explicit the assumptions on which they depend. an experimental prototype incorporates many of these techniques. 
'1. introduction 
　optimizing a program by hand is expensive  both directly and indirectly. the process itself is time-consuming and error-prone  while the optimized program is more complex and therefore difficult to maintain. ideally we would like to write and maintain unoptimized programs and let the machine take care of optimizing them. this would free programmers to write simpler  more understandable  less efficient code  secure in the knowledge that unnecessarily expensive computation would be optimized away. while this is done to some degree by current optimizing compilers  this paper shows how more could be done by exploiting knowledge not available to standard compilers. 
　we focus on a single general strategy for program speedup  namely  software  caching: storing and reusing the results of computations. caching is usually thought of as trading space for time. the cost is the space used for storing results. the benefit is the time not spent recomputing. however  caching can also reduce storage costs by eliminating the extra space that would have been allocated during recomputation. caching can be thought of as a simple form of learning  insofar as the program tends to speed up over time. 
　while caching has received considerable theoretical attention  marsh 1  lenat 1  bird 1  anderson 1  neches 1  rosenbloom 1   especially for applicative languages  specialized knowledge representations  and production systems  we chose to investigate it in the context of general interlisp programs  teitelman 1 . toward this end we constructed an experimental prototype called memoize that installs caches in interlisp programs  including itself. 
   current address. rutgers university department of computer science  hill center   busch campus  new brunswick  new jersey 1. 
     this research was performed at usc-isi and supported by the defense advanced research projects agency under contract no. mda1 c 1. views and conclusions contained in this report are the authors' and should not be interpreted as representing the official opinion or policy of darpa. the u.s government  or any person or agency connected with them. 　the actual operation of installing a cache is trivial  a function  sav f x  y   is transformed to store its results in a table  the cache  indexed by the values of the arguments x and y  in interlisp  this can conveniently be done by adviseing f.  before the transformed function actually computes a result  it looks in its cache  and if the entry is already there  it returns the result computed before. thus it saves the expense of recomputing at the cost of storing and accessing the cache. whether an entry is considered to be  already there  depends on the equivalence criterion used to compare arguments against stored keys. some caches use an eo test  while others use equal; one can imagine using others  though memoize does not. 
　we will refer to this program transformation as  memoizing the function f.  we can also talk about memoizing program fragments other than functions. for example  the expression 
 f  g x   can be memoized by rewriting it as  fg x   defining fg x  as  f  g x    and memoizing the function fg. similarly  the computation of g and h in the expression  f  g  h  i x     can be cached by folding the expression into  f  gh  i x     defining gh y  as  g  h y    and memoizing gh. 
　deciding which parts of a program to memoize  and how  is far from trivial. for example  memoizing a function may require substantial modifications to the rest of the program. while the development of memoize was greatly facilitated by such tightly integrated interlisp facilities as the masterscope program analyzer  the pattern match compiler  the structure editor  and the advise facility  the power of interlisp forced us to confront many problems that do not arise in simpler languages. as we shall see  not all of these problems yield to complete  fully automatic solutions. the rest of the paper discusses the issues of safety  cost-effectiveness  and maintaining the optimized program. 
1. safety 
　optimization should be  safe   i.e.  the optimized program should accomplish the same purpose as the unoptimized version. in the case of memoizing  one might  and we used to  think that ensuring safety is straightforward: before memoizing a program fragment  analyze it to make sure that memoizing it will yield a program with equivalent behavior. as we shall see  this view is naive. first  equivalence lies in the mind of the beholder  and cannot be determined solely by looking at the code itself. second  some changes in program behavior caused by memoizing are actually desirable; the real issue is not whether memoizing preserves  equivalence   but whether the behavior changes are acceptable. third  safety is not a context-free property of the program fragment  but depends on the rest of the program and on how the fragment is memoized. 
　therefore we define the safety of memoizing in terms of preserving the acceptability of the unmemoized program's behavior.  we ignore the issues that arise if this behavior is 
　
1 	j. mostow and d cohen 
nondeterministic.  by this definition  it is always safe to recompute a result rather than retrieve it. notice that this definition may be inappropriate for programs whose correct behavior relies on their being memoized. the ability to write such programs is a potentially important benefit of automated memoizing  since it permits a simpler programming style. 
　whether a change in program behavior is unacceptable  unimportant  or beneficial depends on the user's intent. for example  suppose a program is instrumented to measure the time spent in some function. if the program is being run to measure machine speed  then memoizing it would defeat the purpose. on the other hand  if the goal is to find and eliminate performance bottlenecks  then memoizing the very same program may be desirable even though it changes the result computed  i.e.  the timing . 
　as this rather extreme example illustrates  information about which changes are acceptable cannot infallibly be inferred from the code. from a specification of what the program is supposed to do  and a derivation of the code from that specification  it might be possible to determine which changes were acceptable and which were not. however  even in the absence of such information  fairly reliable guesses can be made based on program analysis  default assumptions  and the conservative principle that it is always safe  if inefficient  to refrain from memoizing. 
memoizing a program can change its behavior in several ways. 
for example  it affects time and space costs  preferably reducing at least one. we assume here that changes in time and space costs are acceptable with respect to safety     although in the case of an embedded real-time system  or limited storage  they clearly would not be. we now discuss other changes in program behavior  and the factors that influence their acceptability. 
1. side effects eliminated by caching 
　by eliminating recomputation  caching eliminates the side effects performed during the recomputation. memoize considers such changes important enough to preclude caching or require user confirmation. 
　a computation specifically executed for side effect should not be memoized if it is important for the side effect to occur every time the computation is invoked. for example  a function that clears the screen should not be memoized  even if it always returns the same value. while a program fragment with indispensable side effects should not be memoized  it is sometimes possible to move the code that performs the side effects outside of the fragment. however  such code motion is outside the scope of this paper or memoize's capabilities. 
　sometimes eliminating side effects by caching can actually improve the program. 
　eliminated output: the user may not care  or may even rejoice  if memoizing the program causes it to print the same message once rather than 1 times. 
　eliminated breaks: if a computation generates an error from which the user can recover  caching the result saves the trouble of fixing it again. and again... 
　eliminated input: if the computation prints out a question and inputs an answer  caching it may usefully reduce the questionanswering burden on the user  provided the answer is explicitly indexed under the variables on which it depends. section 1 discusses how this technique was used to prevent memoize from asking the same question repeatedly. 
　identifying all possible side effects of a computation can be difficult for people. memoize uses masterscope to identify potential direct and indirect side effects of executing a given program fragment. to assist the user in deciding whether it is safe to memoize the fragment  memoize displays its possible side effects and the calling paths by which indirect side effects might be invoked. 
　these side effects include setting non-local variables  evaluating expressions constructed at runtime  eval and apply  whose effects masterscope does not attempt to predict   smashing data structures  rplaca  putprop  putd  etc.   and performing i/o.  for purposes of analysis  it is convenient to regard functions like time as a form of i/o  i.e.  reading a clock.  if a program is viewed at a low enough level  allocation  e.g.  oons  is also a side effect; however  this is considered separately in section 1. 
1. eliminating infinite recursion 
　a useful behavior change introduced by memoizing concerns infinite recursion. whenever the memoized code starts a cached computation for which no result has been stored  it first creates an empty cache entry. when it completes the computation  it fills in the result. if a cache lookup ever retrieves an empty entry  it means that in the absence of caching  the computation would have invoked itself with the very same parameters  i.e.  would have entered an infinite recursion. in this case  the retrieval code halts and warns the user about the problem.  section 1 describes mechanisms that handle the case where the function alters its own parameters or the global state on which it depends.  
1. introducing extra computations 
　a memoized function should not invoke user code in situations where the unmemoized version would not  since doing so can have arbitrarily bad effects. memoize must take care to avoid this problem. 
　if the function's arguments aren't always evaluated left-to-right before entering its body  memoizing it may cause arguments to be evaluated unnecessarily  out of order  or in the wrong environment. for example  memoizing the function and so as to evaluate all its arguments would cause the call 
 and  listp x   car x   to break when x was bound to a non-list  even though the unmemoized version would not. memoize therefore refrains from memoizing  nlambda  functions that evaluate their arguments conditionally. 
　the same problem can be introduced by folding a program fragment into a function  e.g.  folding the expression 
 and  listp x   car x   into  f  listp x   car x    where f y  z  is defined as  and y z . similarly  folding the expression 
 f  g x   g x   into  h  g x    where h y  is defined as  f y y   will cause  g x  to be computed once instead of twice. this could introduce errors if g has side effects that need to be executed 
twice. 
1. allocation eliminated by caching 
　reusing a result instead of recomputing it means that different calls on a memoized function may return the same  eo  data structure where the unmemoized version would have constructed separate  but equal  copies. 
　this change can be viewed as returning a substitute for the result that would have been computed by the unmemoized version. whether the substitute is acceptable depends on assumptions made elsewhere in the program  ff  as is typically the case  the only allocation information available to the program 
is eoness  then changes in allocation affect programs only insofar as they affect data structure reuse.  the safety of caching becomes more problematic for programs that exploit additional allocation information  such as the memory location of an object  loc   or the ability to iterate over all objects in memory.  implicit assumptions may constrain how the function can be memoized  e.g.  how the cache compares the arguments passed to the memoized function against the cache index  and whether it returns the cached result or a copy of it. moreover  these assumptions may constrain reuse not only of complete results but of their substructures. 
　these assumptions cannot be directly determined from the code  since they depend on what behavior is considered acceptable. the obvious solution is to let the programmer identify them. unfortunately  declaring such assumptions explicitly and keeping them consistent with changing code imposes the same kind of burden as maintaining documentation. to at least assist the user in bearing this burden  the machine can try to detect what happens to the results of computations in the unmemoized code. 
　masterscope is inadequate for this purpose because it performs no data flow analysis. however  useful information can be derived by installing experimental caches. an experimentally memoized function recomputes results even when they are already stored. comparing the stored and recomputed results detects reuse. for example  if the two are eo  the function is reusing the same structure. smashing can be detected by storing a copy of the result as well as the original  and comparing them later. if they are no longer equal  the original has been smashed. moreover  if the copy is not equal to a later result computed for the same arguments  that result may depend on global state that has changed in the interim. similar techniques can be used to detect argument-smashing  a problem discussed in section 1.1. 
we now discuss the effects of various allocation constraints. 
1.1. when results must be reused 
　sometimes program correctness requires a function to reuse certain of its results  or their substructures . if the unmemoized function satisfies this requirement  so will the memoized version  provided the cached result itself is returned rather than a copy. 
　returning the same data structure is typically important when the result will be used as a name rather than as a value. for example  if the memoized function returns a node in some structure to a caller who wants to modify the structure  it may be important to return the original rather than a copy. 
1.1. when results can be reused 
　if different calls to a function cause it to create different copies of the same result  but the program does not rely on this property  memoizing it can save space and time by reusing a single copy. besides the time saved by not recomputing results  an additional savings occurs when a consumer of the results uses the equal predicate to compare them. the equal test succeeds much faster on large data structures when they are eq. 
1.1. when results can be canonicalized 
　the case above can be generalized. sometimes there are large equivalence classes of inputs to a function  i.e.  nothing in the program relies on the differences between the results that would have been returned by the unmemoized function. in such cases  the cached result can be reused even more by relaxing the equivalence test between inputs and cache indices.  a closely related idea for increasing reuse of cached results is discussed in 
section 1.  
	j. mostow and d. cohen 	1 
　a particularly common case is where equal but non-eq inputs can safely retrieve the same result. the obvious advantage is that less space is needed to store results  and fewer results need to be computed. a possible disadvantage is that cache lookup with a more general equivalence criterion may be slower  e.g.  equal is slower than eq. on the other hand  canonicalizing results in this way may permit equal tests elsewhere in the program to be reduced to eq. this is a common optimization tactic  but the analysis required to determine where it can be applied is beyond the capabilities of masterscope. notice also that for a program written to rely on the canonicalizing effect of caching  safety cannot be defined simply in terms of preserving the behavior of the unmemoized code. 
1.1. when results must not be reused 
　sometimes a program requires that a function not reuse  part of  some result  because the results returned at two different times are to be used in incompatible ways. typically one caller smashes the result  and the next one wants the old value. result-copying is typically used to satisfy such requirements. 
since memoizing a function can cause it to reuse results where the original version did not  caching can interfere with such a requirement. the memoized function can be made to satisfy the requirement by copying the cached result before returning it  but this approach has some problems: 
 how much of the structure needs to be copied  there's no point in doing a full copy of a list if copying only the top level will suffice. 
 how much of the structure is safe to copy  other parts of the program may require corresponding substructures of the copies to be eo. 
 the copying operation might not terminate  the structure might be circular . 
 the time and space costs of copy-on-retrieval may defeat the advantage of caching. 
1. assumptions introduced by memoizing 
　as we have seen above  safe caching must satisfy assumptions made by code that uses the memoized function. however  the memoizing transformation defined in section 1 introduces code that makes assumptions of its own. 
　for example  this code assumes that no part of the code for cache lookup will itself be memoized  since doing so could cause infinite recursion. this problem is familiar to implementors of facilities like break and advise. 
1.1. cache-smashing 
　memoize assumes that the cache will not be altered in a way that corrupts its ability to retrieve the correct result. it is reasonable to assume the integrity of cache structure built by the caching mechanism itself  since this structure did not exist in the unmemoized version. however  the cache also incorporates possible  trojan horses  - the data structures passed to and returned from the memoized function. if user code retains pointers into these structures  we have to worry about it smashing them. 
　different lines of reasoning can be used to show the absence of such danger. in order of increasing generality: 
　the cache is never smashed. this is certainly the case if the program doesn't smash the arguments it passes to the memoized function or the results returned. to prevent argument-smashing  we can copy the arguments before incorporating them into the cache and pass the originals to the function  in case the function 1 	j. mostow and d. cohen 
is tempted to smash them itself  . if the result incorporates argument structure that other parts of the program can smash  we can cache a copy of the result. to protect against a caller smashing the returned result  we can use copy-on-retrieval. of course the allocation changes introduced by any copying must be acceptable  see section 1 . for example  argument-copying requires that canonicalizing be acceptable  since finding the copied arguments in the index will require equal as the equivalence criterion. 
　the function does not access the parts of its input that might yet smashed. therefore  it would have done the same thing with the smashed input as it did in the first place. notice that in cases where the result of the function shares structure with the input  smashing can change a valid cache entry into another valid cache entry that was never explicitly computed. for example  if we cache the identity function   lambda  x  x   smashing an input won't invalidate the cache. 
	all 	changes 	to a cache 	entry ieave 	it 	valid  i.e.  	the 
 modified  result is an acceptable substitute for the one that would have been computed for the  modified  arguments. for example  a function that counts the number of nils in a list will not be affected by an operation that reorders the list. 
　of course  the analysis required to support such arguments is far beyond the capabilities of masterscope  and if it is done anyway  e.g.  by hand   one must still watch out for smashing done by the user at the top level or in breaks. 
1. transforming programs to make memoizing safe 
　so far  we have only considered the safety of the memoizing transformation defined in section 1  and simple variants that make copies  naturally  much more can be done if one is willing to change other parts of the program  such as those making assumptions/about the code to be memoized. for example  suppose the function f is required to return a copy of its result because one caller will smash it. if that one caller were changed to make a copy of the result and use it instead of the original  then f could be memoized safely with no copying. 
　the most common obstacle to caching is the problem of side effects. the safety of memoizing a function depends on two classes of side effects. section 1 discussed the safety of eliminating side effects invoked  directly or indirectly  by the function itself. we now discuss how to make memoizing safe in spite of side effects remaining in the program. in particular  we show how the program can be transformed to prevent retrieval of results rendered obsolete by changes in global state. 
　side effects that affect the integrity of a cache can be viewed generally as smashing the input or output of the memoized function. the input is considered to include all data on which the function depends  both the explicit arguments passed as parameters and the implicit arguments accessed in the function  such as free variables  property lists of atoms  and even the function's definition. we view all changes to such data as forms of smashing  e.g.  setting a global variable is smashing its value cell . since implicit and explicit arguments are really the same in this view  we see that one reaction to dependence on global state is to treat it like an explicit argument and index on it. conversely  the approach to global state described below can be applied to explicit arguments as well. 
　input to a memoized function may contain substructure that is not actually accessed by the computation. given some way to record which part actually is used  the cache could safely be indexed on only that part  for example by building a discrimination tree that only tests the relevant parts. this could be worthwhile for a computation that ignores most of its input data. a related idea is described in section 1. 
　the general strategy for dealing with global state dependencies requires cooperation from the other parts of the program: 
1. detect state changes that might cause cache entries to become invalid  
1. figure out which cache entries might be affected. 
1. do something about them. 
　since detecting and reacting to state changes is an additional cost  and it is always safe to recompute  different schemes may be appropriate in different situations: 
	don't 	cache. 
　discard an entire cache at the end of some computation. this method is useful if the memoized function is executed many times by the computation and depends on state information changed between  but not during  invocations of the computation  e.g.  arguments passed to some procedure containing the computation. in particular  if the computation corresponds to a program block  making the cache a local variable of the block will cause it to be discarded when the block is exited. 
　empty the cache for a memoized function in response to any side effect that might smash the cache or change the function's implicit arguments. 
　discard individual cache entries in response to any side effect that might smash them or change the implicit arguments used to compute them. 
　update the affected entries without recomputing them  i.e.  use finite differencing  paige&koenig 1 . this technique requires knowledge about the nature of the side effect as well as sophisticated program analysis. section 1 describes a weaker technique that reduces the amount of recomputation without requiring such knowledge or analysis. 
　we now discuss methods for detecting state changes  identifying the cache entries they might invalidate  and restoring cache validity. 
1 . 1 . detect state changes 
　to make a memoized program respond to side effects  we must identify operations that change global state and transform them to do something about the caches they might affect. this triggering can be done with various degrees of selectivity depending on the degree of sophistication with which the program is understood. 
　modify primitives: at the lowest level  lisp's structuresmashing primitives  rplacd  seta  set  etc.  could be altered to trigger the desired response directly. this approach requires no understanding of the program and would catch all structuresmashing operations  many of which would not invalidate any cached results. it is not actually used by memoize. 
　modify functions: if one can identify the  higher level  functions in the program that modify state on which caches depend  those functions can be advised to take appropriate action. 
　modify invocations: sometimes one can determine not only which functions  but which invocations affect a cache. for example  if a cache is sensitive to the global variable v  then  seto u ...  will not affect it. in this case  the particular invocations can be transformed to take appropriate action. memoize uses masterscope to find these invocations  and uses 
interlisp's structure editor to insert appropriate trigger code around each one. 
　it is also necessary to respond to state-changing operators invoked interactively by the user. memoize uses the hooks interlisp provides for this purpose  e.g.  markaschanged and savesetq . 
1. identify suspect cache entries 
　after noticing a change  it is necessary to identify the affected cache entries. this can be done with various degrees of expense and precision  as long as we err on the side of thinking that a valid entry is invalid. again  greater understanding of the program can increase precision at constant expense  or reduce the expense for constant precision. 
　brute force: great precision with no knowledge is possible at great expense. the very low-level solution would be to record every memory cell accessed by the cached computation. a cell could then be used to index a table of the cache entries that depend on it. this is not done by memoize. 
　static analysis: sometimes one can determine which modifications can possibly affect which caches  e.g.  the memoized function depends on the value of the variable v  and we can determine which operations affect the value of v. masterscope recognizes dependence on variables  fields of records  function definitions  etc.  and installs code to react to changes in them. however  masterscope misses some dependencies due to the problem of aliassing: smashing u might affect v  if u and v share structure. memoize relies on the programmer to identify code that could affect the implicit arguments of a function in non-obvious ways. 
　dynamic dependency recording: code that accesses global state can be altered to record  at runtime  what state an individual entry depends on. this is more expensive than static analysis  but more precise  especially when not all elements of a cache depend on the same state. in different cases  the improved precision may or may not pay off. again  memoize relies on the user to point out non-obvious code that might smash state accessed by the function. 
　a special case of dependency is when the computation of a new cache entry retrieves an existing one  which in turn depends on other data. if that data is ever changed  it will invalidate the old entry  which in turn will invalidate the new one. thus we need not record the dependency of cache entries on data they use only via other entries. 
1.1. static versus dynamic dependency analysis 
　memoize decides heuristically whether to predict dependencies statically or record them dynamically. 
dependencies on a global variable are determined by static 
masterscope analysis. this determines with reasonable precision which updates might invalidate which caches. e.g.  if f uses the global variable v  then changing the value of v is liable to affect f. 
　dependencies on properties and fields are recorded dynamically  since static analysis would not be sufficiently precise. suppose a memoized function contains the expression  getprop x 'color . with the dynamic approach  we can tell which atom's color a cached result depends on. if we relied on masterscope's static analysis  we'd have to consider every entry in the cache suspect whenever any atom changed color. 
	j. mostow and d. cohen 	1 
　to determine dependencies on function definitions  memoize uses a combination of approaches. it uses static analysis to find the functions invoked by a cached computation  since masterscope determines these with high precision. to record function definitions accessed in other ways  e.g.  with getd  memoize uses the dynamic method  since static analysis lacks precision in such cases. 
1. update invalidated result 
　when a cached result is identified as potentially invalid  something must be done to prevent it from being retrieved    or any other cache entries that depend on it. the obvious thing to do is simply delete the entry  since if it is ever needed again  it will have to be recomputed anyway.  notice that if in the course of computing the result  the memoized function first accesses some data and then smashes it  the entry to be deleted will still be empty at the time of smashing. if the function shoots itself in the foot in this way  the result eventually computed should be considered correct for this invocation but not for the next one. but watch out for recursion!  
　deleting a suspect result can be very wasteful if it is likely still to be valid and there are other entries that depend on it. this typically occurs when the cached computation is a many-to-one function  e.g.  null  length  of the changed state information. we now present two solutions to this problem; their relative effectiveness will be described in section 1. 
1.1. eager recomputation of suspect results 
　one technique for avoiding the deletion of valid cache entries that depend on a suspect result is to recompute it immediately. if the new result is equivalent to the old one  nothing else needs to be done; the entries that depend on it are still valid. if not  the entries that depend directly on the changed result are suspect  and the process repeats up the chain of dependencies. since the dependency structure is finite and acyclic  see section 1   this process is guaranteed to terminate. 
　eager recomputation is not always safe  since the computation might never have occurred in the unmemoized version.  section 1 describes similar hazards.  for example  suppose the memoized expression is 1/x  where x is a global variable. eager recomputation after setting x to zero would generate an error  even if the original program never evaluated the expression when x-was zero. 
　memoize knows that functions which have no side effects and never generate errors  e.g.  listp  are safe to eagerly recompute. when these restrictive conditions are not met  it asks the programmer whether eager recomputation is safe. 
1.1. propagation and exoneration of suspicion 
　an alternative technique that prevents retrieval of invalid results and avoids unnecessary recomputation involves marker propagation. results that depend directly on changed data are marked  must recompute.  results that depend indirectly on changed data are marked  might need to recompute  since they depend on entries that might or might not change when they are recomputed. the cost of marker propagation is limited  as in the case of eager recomputation. also  when an entry already marked  might need to recompute  is encountered  no entries that depend on it need to be marked. 
　when an entry marked  must recompute  is subsequently retrieved  it is duly recomputed. if the new result is equivalent to the old one  the entry is marked  ok.  otherwise  the new result is stored and the entries that depended directly on the old result are marked  must recompute.  
1 	j. mostow and d. cohen 
　when an entry marked  might need to recompute  is retrieved  an attempt is made to exonerate it without recomputing it  by checking the entries on which it depends. if all these supporting entries prove to have been valid  the original entry is marked  ok  and returned. if any of these entries was suspect  the attempt to retrieve it will  recursively  cause it to be exonerated or recomputed. if its value has changed  the exoneration process is aborted and the original entry is recomputed. 
　the supporting entries must be retrieved in the order in which they were used during the original computation  so this order must be recorded. retrieving them in another order may be unsafe  as in the case of eager recomputation. 
　the propagation technique avoids unnecessary recomputation: a result is recomputed only if it depends directly on changed data  including other cache entries  and would have been recomputed in the uncached computation. 
1. cost-effectiveness 
　just because a computation is safe to cache doesn't mean that doing so is worthwhile. a cache should only be installed if it is likely to be cost-effective. moreover  just as the safety of caching depends on how it is implemented  so does its cost-effectiveness. thus the issue is not just to decide whether a given computation is cost-effective to cache  but rather which parts of it to cache  and how  so as to maximize cost-effectiveness. 
　the impact of a cache on memory usage depends on such factors as the number of entries  the size of each entry  the degree of structure sharing between entries  and how long each entry is stored  and hence unavailable for garbage collection . 
　the impact on time depends on such factors as the hit rate  how often a retrieval replaces recomputation   the cost of cache lookup  the frequency of side effects that render cache entries suspect  and the cost of responding to them. 
　a cache provides a net speedup over its lifetime if doing without the cache would take more time  i.e.  

　predicting cost-effectiveness entails predicting the values of these variables. this section discusses factors that influence those values and heuristics for estimating them. of course  the ultimate test of a cache's cost-effectiveness is to try using it. we view the heuristics presented below as techniques for identifying promising candidates. while memoize uses these heuristics to decide what to cache  it collects statistics on actual cache performance  e.g.  hit rate  update frequency  and how much time was spent computing each cached result. such data could be used  either by the user or by the machine  to decide which caches to keep. 
1. computation cost 
　the time to execute an expression depends on how long each operation takes and how many times it's invoked. the latter quantity depends on things like how many times loops are executed and the relative frequency of the branches in a conditional. these depend on runtime data distribution and are hard to predict analytically  though see  kant 1  . 
　memoize uses a simple one-bit heuristic theory of computational complexity: functions and expressions are either cheap or expensive. 
 built-in lisp functions are classified individually. 
 memoized functions are cheap. 
 user input is expensive. 
 recursive functions are expensive. 
＊expressions containing loops are expensive. 
 expressions that call expensive functions are expensive. 
 other functions and expressions are cheap. 
　to classify a compiled function whose definition is unavailable for analysis  memoize can either ask the user  or instrument the function  with interlisp's breakdown  and postpone classifying it until data is available. 
1. hit rate 
　while the exact hit rate of a cache cannot be predicted a prion  certain useful characterizations can be made based on static program analysis. in deciding which parts of a computation to cache  memoize uses the intuitive notion that the hit rate of a computation decreases as the variability of its input increases. in particular: 
　hit rate is a decreasing function of the amount of changing global state information used in the computation. 
　hit rate is a decreasing function of the frequency of global state changes that invalidate cached results. 
　hit rate is a decreasing function of the number of possible values of a cache parameter. this number is bounded by the domain of the parameter type; for example  a boolean parameter takes on at most two values. 
　hit rate is a decreasing function of the number of parameters. if it's worth caching an expression  and that expression contains subexpressions with fewer parameters  they may be worth caching as well  since their hit rates will be higher. 
　more generally  many-to-one mappings reduce variability and therefore can be used to increase hit rate. if  f  g x   is the computation to be cached  and g is a many-to-one mapping  it's better  in terms of hit rate  to index a cache for f by the values of  g x  than to index a cache for the composite function fg on the values of x. however  if the computation has the form 
 f  g x   h x }  where g is a many-to-one mapping and h is not  then using  g x  instead of x as an input parameter will not increase the hit rate. that is  increasing the hit rate requires masking the variability in x along every input path. moreover  this argument must be applied to the total set of inputs - for example  a cache for  f  car x   cdr x   might as well be indexed on x. 
1.1. choosing cache indices to maximize hit rate 
　a variable used in a cached computation can be treated either as an explicit cache parameter or as an implicit state variable. this decision should be made so as to maximize the hit rate. a variable whose values tend to recur should be used to index the cache. conversely  if an explicit parameter will be stable over 
many successive calls  and then change without returning to its former value  it might as well be treated as a global variable. of course  if every call to a function is going to have different parameter values  there is no advantage to memoizing it. 
　memoize uses a simple heuristic for classifying variables: treat function arguments and local variables bound outside the memoized expression as parameters  and everything else as global state variables. this heuristic works well for the examples encountered so far  where the global state variables have been longish lists  but may be worth refining  for example to treat 
global flags as parameters. 
1. update cost: eagerness versus suspicion 
　the relative costs of the two schemes described in section 1 for responding to cache-invalidating side effects depend on a 
　tradeoff between unnecessary recomputation and unnecessary propagation. 
　the marker propagation scheme performs a subset of the computation the unmemoized version would have performed. in good cases  only a small subset is performed; in the worst case  the same amount of computation occurs  plus the additional overhead of marker propagation. a cache entry can never be marked  might need to recompute  or  must recompute  more often than it would have been computed in the absence of caching. therefore this overhead is at worst proportional to the amount of original computation. this estimate must be adjusted slightly since entries that depend directly on a changing piece of global state will be marked  must recompute  every time that piece changes  even though propagation only occurs the first time. the adjustment for each such piece of state is proportional to its update frequency times the number of entries that depend on it directly. 
　the eager recomputation scheme  when safe  runs the risk of recomputing a result unnecessarily  i.e.  either the entry will never be retrieved again  or it will need to be updated again before it is. however  when the result turns out to be unchanged  this scheme avoids the overhead of propagating markers to other entries. 
　the two strategies can be mixed together  even on the same cache. at update time. memoize decides which one to use by predicting their relative costs based on the history of the suspect cache entry. of course this computation may itself take longer than the time saved by choosing the better scheme. this decision mechanism illustrates a recurring issue in caching: the cost of information. an alternative scheme could use heuristic approximations. the degraded accuracy might well be offset by eliminating the costs of recording and using a detailed history of the entry. 
1. maintaining memoized code 
　optimizing a program  whether automatically or manually  tends to make it more complex and harder to maintain. the optimizations introduce assumptions that may be violated by subsequent changes to the program. 
　the obvious solution to this problem is analogous to recompilation: let the user maintain the unoptimized code  and reoptimize after each change. this approach is grossly inefficient  especially for the sort of  exploratory programming   sheil 1  characteristic of al research. such programming  illustrated by memoize's own evolution  involves a repeated cycle of running the program  deciding to change its behavior  making the necessary edit  and resuming execution. inserting a full-fledged reoptimization phase after every program edit would be wasteful: 
	j. mostow and d. cohen 	1 
 massive reoptimization would itself be timeconsuming  and wasteful to the extent that it was simply reinstalling identical caches. 
 if reoptimization were not fully automatic  it would include the expense of making the user answer the same questions as before  which would be wasteful to the extent that the answers had not changed. 
 re-memoizing from scratch would mean discarding all existing caches  even though their entries might not be invalidated by the edit. these entries would have to be recomputed if they were needed again. 
　on the other hand  we don't want the inefficiency of unoptimized code. what's needed is something like efficient incremental recompilation. our solution is to let the user edit the memoized code  and have the machine  with some help from the user  do whatever is needed to keep the program safe. 
1. which caches are affected by a program edit program edits can affect caching in various ways: 
 existing cache entries may become invalidated. 
 a 	memoized 	function 	may 	become 	unsafe to memoize. 
 an existing cache may cease to be optimally costeffective. 
 an unmemoized function may become worthwhile to memoize. 
memoize only worries about the first two cases. when a function becomes unsafe to memoize  its cache is removed. memoize then considers re-memoizing the function. failure to deal with the last two cases may affect the efficiency of the edited program  but not its safety. 
　it is easy to see that editing a function may affect its own safety and that of its callers. when a function is edited  memoize finds any memoized functions related to it in this way  and warns that their caches may no longer be safe. 
　it is harder to identify other caches affected by editing a function. some such effects could be noticed straightforwardly for example  if a function is modified to set a global variable  any memoized functions that access the variable would be affected. however  other cases would require sophisticated data flow analysis. moreover  no matter how good the tools for such analysis were  they could not detect changes in the programmer's intent and implicit assumptions about the code.   
1. speeding up re-optimization by reusing information 
　the cost of re-memoizing can be further reduced if all the information about the program used during the original memoizing process is recorded. information known to depend only on properties of the program that haven't changed is still valid. this includes the results of program analysis as well as the user's answers to questions about the program. 
　memoize caches both kinds of information  but records its dependencies on the program only incompletely. the problem with caching a result of program analysis is that it may be invalidated by a change almost anywhere in the program. for example  changing f to call g may change whether h indirectly calls i. the problem with remembering the user's answer to a question about the program is that which properties of the program the answer depends on is implicit in the mind of the user.  see section 1.  
    
barring a breakthrough in esp research 
1 	j. mostow and d. cohen 
1. conclusion 
　we have tried to systematically analyze the problems associated with software caching in a general programming environment. these problems include deciding which parts of a 
　program are safe to memoize  transforming the rest of the program to make them safe  choosing the most cost-effective ones to memoize  and maintaining the optimized code. this analysis extends previous work on caching by considering side effects  shared data structures  beneficial behavior changes  and program edits. 
the insights we have gained include: 
 the right question to ask about a program transformation like memoize is not whether it leaves the program's behavior unchanged  but whether the changes it makes are acceptable. 
 there is not enough information in a program to determine which changes are acceptable. 
 safety and cost analysis should be viewed as deciding how to rearrange a computation into cached and uncached parts  rather than a simple binary choice about whether to memoize a given function. 
 this analysis requires a deep understanding of the program to be optimized  involving questions that are in general undecidable  e.g.   is the result of function 
f ever smashed    or depend on implicit assumptions  e.g.   does the program rely on the 
difference between the values returned for equal 
inputs   
　in addition  the paper presents some non-obvious techniques for eliminating recomputation: 
 a simple mechanism that detects certain forms of infinite recursion. 
 various schemes for detecting and preventing assaults on cache integrity. 
 various schemes for identifying invalidated results. 
 a 	one-bit 	heuristic 	theory 	of 	computational complexity. 
 exploitation of many-to-one mappings to increase hit rate and reduce update cost. 
 methods for maintaining cached results and cache safety in the face of program edits. 
　memoize itself constitutes a contribution of sorts  but one that should not be misinterpreted. while it serves as a demonstration of many of the techniques described here  it is not a practical tool and should not be evaluated as one. for example  any statistics on its performance would be distorted by the extensive bookkeeping it performs for experimental purposes. the amount of speedup obtained by caching can be arbitrarily high or low  depending on the program to be memoized; evaluating a real tool would require applying it to a  representative  sample of programs. moreover  applying such a tool to existing programs would say nothing about how much simpler the programs might have been if they had been developed with the tool in mind in the first place. 
　the main value of memoize has been as an exploratory vehicle: the bugs and opportunities it has exposed have greatly improved our insights into caching. one of these insights concerns the additional work required to extend memoize into a useful tool. we had originally hoped that the process of installing caches in an interlisp program could be made essentially automatic. we now understand much more clearly the obstacles to achieving that ideal. 
acknowledgements 
　we would like to thank bill swartout for asking some good questions and bob balzer for suggesting this problem in the first place. 
