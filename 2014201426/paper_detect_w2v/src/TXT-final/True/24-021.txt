 
a recent system  foil  constructs horn clause programs from numerous examples. computational efficiency is achieved by using greedy search guided by an information-based heuristic. greedy search tends to be myopic but determinate terms  an adaptation of an idea introduced by another new system  golem   has been found to provide many of the benefits of lookahead without substantial increases in computation. this paper sketches key ideas from foil and golem and discusses the use of determinate literals in a greedy search context. the efficacy of this approach is illustrated on the task of learning the quicksort procedure and other small but non-trivial listmanipulation functions. 
1 	introduction 
the learning task of discovering horn clause programs from examples has been studied for some time  with important contributions from plotkin   shapiro   sanimut and banerji   muggleton   
buntine   and muggleton and buntine . as 
mitchell  points out  all inductive learning requires search of a space of possible hypotheses. even though horn clause programs are a restricted class of first-order formalisms  the hypothesis space is so large that such learning programs often required carefully selected examples and/or hints of one kind or another in order to discover useful programs in a reasonable time. 
　recent papers  quinlan  1a  1b  introduce a new system  foil  that adapts ideas from attributevalue learning to this task. specifically  foil exploits information from large numbers of examples to guide the search for a program. such guidance turns out to be so effective that greedy search is usually adequate  permitting common benchmark problems to be solved very quickly.  quinlan  1a  contains examples drawn from six task domains studied previously by machine learning researchers  all of which foil handles competently. 
   *the research reported here was supported by a grant from the australian research council. 
1 	learning and knowledge acquisition algorithms using greedy search  however  tend to suffer from a horizon effect an action that may prove to be desirable or even essential from a global perspective may appear relatively unpromising at a local level and so may be passed over. as a result  even though the original foil  foil.1  performs well on common tasks  there are some programs that it will not discover. 
　an even more recent system  golem  muggleton and feng  1   takes a very different approach to this same learning task. using plotkin's concept of generalisation lattices  golem generates clauses as the least general generalisation of two examples in the context of available background knowledge. these generalised clauses can be unmanageably large and  to restrain their size  golem considers only clauses in which all terms are determinate in the sense that there is only one binding possible for existentially quantified variables. with this restriction on what can be learned  golem solves some very difficult tasks involving complex clauses and numerous examples and  like foil  does so very efficiently. 
　this paper concerns an adaptation of muggleton's determinate terms  called determinate literals. rather than restricting the search space  and thus the class of learnable programs  foil exploits the idea of determinism to overcome some of the horizon effect of greedy search. the effect on learning time is usually negligible - in fact  some learning problems are now solved more quickly than before. 
　after an abbreviated description of some key ideas underlying foil and golem  the paper describes determinate literals and their use in a greedy search context. the usefulness of these determinate literals is illustrated on the task of learning the quicksort procedure and other small list-manipulation programs. 
1 	foil 
foil's input consists of information about one or more relations  one of which  the target relation  is to be defined by a horn clause program. for each relation we are given a set of tuples of constants that are in the relation. for the target relation we might also be given tuples that are known not to belong to the relation; alternatively  the closed world assumption may be invoked to state that no tuples  other than those specified  belong to the target relation. tuples belonging to the target relation are labeled  those not belonging to it  the learning task is then to find a set of clauses for the target relation that account for all the  tuples while not covering any of the  tuples.  
　this description is somewhat over-simplified. in many real-world situations  noise in the data prevents learning exact  complete definitions of this form. to get around this problem  foil uses encoding-length heuristics to limit the complexity of clauses and programs; the underlying idea is that the number of bits required to represent a clause should be less than the number of bits required to represent the tuples it covers. the final clauses may cover most  rather than all  of the tuples while covering few  rather than none  of the tuples. see  quinlan  1a  for details. 
　the basic approach used by foil is an aq-like covering algorithm  michalski  1 . we start with a training set containing all and tuples  construct a functionfree horn clause to 'explain1 some of the 1 tuples  remove the covered 1 tuples from the training set  and continue with the search for the next clause. 
　this paper focuses on the construction of a single clause. foil starts with the left-hand side and extends the clause by adding literals to the right-hand side  stopping when no tuples are covered by the clause  or when encoding-length heuristics decide that the clause is too complex . although foil incorporates a simple backup mechanism  the clause-building process is essentially a greedy search; once a literal is added to a clause  alternative literals are usually not investigated. we look now at how this literal is chosen at each step. 
consider the partially developed clause 

containing variables each tuple in the training set t looks like for some constants 
{c }  and represents a ground instance of the variables in a. now  consider what happens when a literal lm of the form  

is added to the right-hand side of a giving a new clause a'. if the literal contains one or more new variables  the arity of the new training set will increase; let x' denote the number of variables in a'. then  each tuple in the new training set t1 will be of the form  d1  d1        dx'  for constants {cf }  and will have the following properties: 
  {di1  d1l ...  dx  is a tuple in t  and 
   di1di1 ... dip  is in the relation p. 
that is  each tuple in t' is an extension of one of the tuples in t  and the ground instance that it represents satisfies the literal- every tuple in t thus gives rise to zero or more tuples in t   the labels of the new tuples being the same as their respective ancestors. 
　let denote the number of o tuples in t  and similarly for the effect of adding a literal lm can be assessed from an information perspective as follows. the information conveyed by the knowledge that a tuple in 
t is is given by 

and similarly for  is less than i t  we have 'gained1 information by adding the lit .eral lin to the clause and  if s of the tuples in t have extensions in 
t'  the total information gained about the o tuples in t is 

　foil explores the space of possible literals that might be added to a clause at each step  looking for the one with greatest positive gain. the form of the gain allows significant pruning of the literal space  so that foil can usually rule out large subspaces without having to examine any literals in them. 
   foil thus tames the hypothesis space problem by a stepwise greedy search for clauses. however  some clauses in reasonable definitions will inevitably contain literals with zero  or negative  gain. suppose  for instance  that all objects have a value for some property q  and the literal q x y  defines the value y for object a. since this literal represents a one-to-one mapping from x to y  each tuple in t will give rise to exactly one tuple in t' and so the literal's gain will always be zero. to permit such literals to appear in clauses  foil ascribes a small positive gain to any literal that introduces a new variable. this has the effect of widening the search space for the next literal - there will be more possible combinations of arguments for the literal - but also can cause a blow-out in the size of the training set of tuples. worse  since there are usually many literals that would introduce new variables  the choice of one of them is necessarily arbitrary. 
1 	g o l e m and determinate terms 
 golem uses the same information as foil  namely constant tuples that belong to one or more relations  but regards each tuple as a ground assertion. if a tuple  is a member of relation r  this is equivalent 
to the ground assertion  
　let e1 and e1 be two terms. plotkin's least general generalisation of e1 and e1  denoted lgg e1 e1   is defined as follows  muggleton and feng  1 : 

　now  suppose we have two ground examples of the target relation  r c1  c1  ..  cn  and r d y d1 ...  dn   say  and a number of other relations representing background knowledge. the least general generalisation of these two examples relative to the background knowledge  the  rlativc least general generalisation  rlgg   is a clause 

	quinlan 	1 
for every pair x1  x1 of ground assertions taken from each relation. the right-hand side of this clause is equivalent to true if the examples cannot be covered by a single clause  or  if they can  consists of a  usually very large  number of literals. the basic operation of golem can be thought of as choosing a pair of examples in the target relation and forming their rlgg  successively expanding the right-hand side until the clause covers only positive examples of the target relation. if the resulting clause is not vacuous it is refined by removing unnecessary literals and added to the developing program in a manner similar to foil. 
consider now the clause 

and suppose t is a term in lm. denote by {y} the set of variables that occur only in t and by {z} the other variables in the clause. term t is determinate wrt lm if  for every ground substitution for variables in {z}  there is at most one ground substitution for variables in {y  so that the clause is satisfied. intuitively  the values of variables in {y} are determined by the values of variables that appear earlier in the clause. 
   golem uses this idea to eliminate most literals that might be added to the right-hand side of a clause by insisting that  in a new literal  every term containing a new variable must be determinate wrt that literal. although this rules out some potential clauses  the reduction in the number of literals in the rlgg of two examples is so substantial that rlgg's can be constructed feasibly and efficiently. 
1 	determinate literals 
the key idea in determinate terms is that new variables have a value forced by previous variables. determinate literals employ the same intuition  but also insist on comprehensive coverage of the o tuples in the current training set. 
suppose that we have an incomplete clause 

with an associated training set t as before. a literal 'lm is determinate with respect to this partial clause if lm contains new variables and there is exactly one extension of each o tuple in t  and no more than one extension of each 1 tuple  that satisfies lm. the idea is that  if lm is added to the clause  no  tuple will be eliminated and the new training set t' will be no bigger than t. 
　foil notes determinate literals found while exploring possible literals to add to the clause. the maximum possible gain is given by a literal that excludes all  tuples and no tuples; in the notation used before  this gain is unless a literal is found whose gain is close to the maximum possible gain  foil adds all determinate literals to the clause and tries again. this may seem rather extravagant  since it is unlikely that all such literals will be useful. however  foil incorporates clause-refining mechanisms that remove unnecessary literals as each clause is completed  so there is no ultimate 
1 	learning and knowledge acquisition 
penalty for this all-in approach. since no 1 tuples are eliminated and the training set does not grow  the only computational cost is associated with the introduction of new variables and the corresponding increase in the space of possible next literals. it is precisely the enlargement of this space that the addition of determinate literals is intended to achieve. 
　there is  of course  a potential runaway situation in which determinate literals found at one cycle give rise to further determinate literals at the next ad infinitum. to circumvent this problem  foil borrows another idea from golem. the depth of a variable is determined by its first occurrence in the clause. all variables in the left-hand side of the clause have depth 1; a variable that first occurs in some literal has depth one greater than the greatest depth of any previously-occurring variable in that literal. by placing an upper limit on the depth of any variable introduced by a determinate literal  we ensure that there are a finite number of them and so rule out indefinite runaway. this limit does reduce the class of learnable programs. however  the stringent requirement that a determinate literal must be uniquely satisfied by all 1 tuples means that this runaway situation is very unlikely and foil's default depth limit  1  is hardly ever reached. 
1 	an example: quicksort 
one difficult task investigated by muggieton is learning the quicksort procedure for sorting lists of numbers. there are six relations: 
sort u s  partition v u l h  
components l h  t  append  a b c  
null l  elt v  
following muggleton  the examples provided for the sort relation cover all lists of length up to three containing non-repeated numbers in {1 1}. there are 1 such lists  giving 1 o and 1 tuples. the other relations are defined over this same vocabulary. 
　the definition found by foil in 1 seconds  on a decstation 1  is 

the first four literals of the second clause have negligible or zero gain. however  all these  and the fifth literal  are determinate literals  as can be seen by observing that 


table 1: results on quicksort experiments 

  in components  a  c d   c and d are determined by a-
  in partition  c d e f   e and f are determined by c and d; 
  in sort e g   e determines g  
  in sort f h   f determines h  
  in append g l b   i is determined by g and b. 
consequently  all these are included in the developing clause.  two further determinate literals  components b x y  and sort y z   were also added as the search progressed  and were discarded as unnecessary when the clause was completed.  
　the effect of the determinate literals above is to introduce new variables that possess a useful relationship to the variables a and b in the left-hand side of the clause. only when these variables are in place can foil 'see' the importance of the last couple of literals. without determinate literals  foil does not discover the second recursive clause  but instead develops a swatch of less general clauses covering special cases  e.g.  for lists of length one  for lists that are already sorted  and so on . 
　to illustrate foil's computational economy  the experiment was repeated using larger numbers of examples of the sort relation. three additional data sets were defined by increasing the allowable set of elements of a list and by increasing the maximum length of the lists. as table 1 shows  there are more lists and much larger numbers of tuples in these training sets  with a corresponding increase in the size of the other relations. the same definition for sort was found in each case and the time required grew approximately linearly with the size of the training set. on this task  foil does not suffer from a combinatorial increase in computation time as the number of training examples grows; the same finding has been made in other domains. 
1 	summary of other results 
foil has been applied to more than twenty tasks from ten domains ranging from learning approximate rules for a small chess task to discovering an unknown dealer's rule in eleusis; several are discussed in  quinlan  1a . 
　here  we give results on several domains akin to quicksort  viz. that require the system to learn exact  recursive definitions. many of these datasets were provided by stephen muggleton. a brief description of each follows: 
  member  find whether a list or element is a member of another list  given a few examples. other relations were components and null. 
  append: find how to append one list to another. examples covered all lists of length up to three with non-repeating elements drawn from {a  1  c}; a sample of about  1k negative examples was providedother relations were components  null  list and reverse - note that the last two of these were not required for the definition of append. 
  reverse  find how to reverse a list. examples were the same as for the append relation above. 
  combinations: find a recurrence relation for the number of combinations of r objects chosen from n  given all values with other relations were multiply  zero and one. 
  multiply: find a recurrence relation for multiplication given the table of values up to 1. other relations were predecessor  plus  zero and one. 
　results on these domains are presented in table 1  first using determinate literals and then with this feature disabled. as before  times are for a decstation 1. 
　in two cases when determinate literals were not used  the definitions found by foil were imperfect in that they did not cover the whole training set  or covered it in a non-general way. in reverse and multiply the learning time was actually reduced by the use of determinate literals. in the case of append  on the other hand  there was a three-fold increase in time required. as this last example represents the downside of using determinate literals  let us have a closer look at it. 
　recall that the relations defined for append include the  unrelated  relation reverse. when determinate literals are employed  the development of a clause for append a b c  immediately gives three determinate literals: 
reverse  a d  reverse b e  reverse  c f  
 since each argument has a unique reverse . now  instead of three bound variables  there are six! if we continue by examining possible literals based on the components relation  with three arguments  there are 1 pos-
sible literals that need to be looked at. on the other hand  if the determinate  but unhelpful  literals are not added  there are only 1 components literals that must be assessed. a scale-up of the same kind occurs for other literals. thus  while the search for clauses in the two 
	quinlan 	1 


table 1: times for other domains  '-' means imperfect definition  

cases follows somewhat similar paths  the introduction of additional  red-herring variables by the determinate literals leads to more literal evaluations. the increased learning time is a direct result. 
　in many cases  the use of determinate literals produces no noticeable effect on the time required to find a set of clauses  and only occasionally increases the time required to find a solution. on the other hand  the introduction of new variables can materially increase foil's ability to find a solution when long  recursive clauses must be generated. 
1 	conclusion 
this paper has presented some highlights of two new systems for learning horn clause programs from examples. one of them  foil  uses a greedy search to explore the very large hypothesis spaces involved  while the other  golem  employs an interesting semantic concept to limit this space. 
　an adaptation of this concept  determinate literals  has been shown to overcome some of the horizon problems associated with greedy search. the application of this idea does not appreciably limit the class of programs that can be learned; instead  it has the effect of introducing a kind of low-cost lookahead in situations where there is not a very clear indication of what to do next. 
　golem and foil are both young systems that are still evolving  so it is perhaps premature to compare them. however  informal trials have revealed a surprising degree of similarity in many of the solutions found by these systems and the computational effort required  despite the very different approaches that they embody. 
acknowledgement s 
　i have benefited greatly from discussions with many colleagues  particularly stephen muggleton and donald michie. stephen muggleton provided some of the examples used in this paper. some of this work was carried out while i was visiting the knowledge systems laboratory  stanford university. 
1 	learning and knowledge acquisition 
