
this paper proposes an approach for learning call admission control  cac  policies in a cellular network that handles several classes of traffic with different resource requirements. the performance measures in cellular networks are long term revenue  utility  call blocking rate  cbr  and handoff failure rate  cdr . reinforcement learning  rl  can be used to provide the optimal solution  however such method fails when the state space and action space are huge. we apply a form of neuroevolution  ne  algorithm to inductively learn the cac policies  which is called cn  call admission control scheme using ne . comparing with the q-learning based cac scheme in the constant traffic load shows that cn can not only approximate the optimal solution very well but also optimize the cbr and cdr in a more flexibility way. additionally the simulation results demonstrate that the proposed scheme is capable of keeping the handoff dropping rate below a pre-specified value while still maintaining an acceptable cbr in the presence of smoothly varying arrival rates of traffic  in which the state space is too large for practical deployment of the other learning scheme.
1 introduction 
next generation wireless systems are expected to support multimedia services with diverse quality of services  qos   such as voice  video and data. due to the rapid growth in mobile users and scarce radio resources  cac has become vital to guarantee the qos for the multiple services and utilize the network resources  ahmed  1 . 
　generally a cellular network has a limited number of bandwidth units  bwu  or channels  which could be frequencies  time slots or codes depending on the radio access technique used  viz  fdma  tdma  or cdma respectively. arriving calls are accepted to or rejected from access to the network by the cac scheme based on the predefined policy. 
as a user moves from one cell to another  the call requires reallocation of channels in the destination cell. this procedure is called handoff.  if there are no available channels in the destination cell  the call may be prematurely terminated due to handoff failure  which is highly undesirable. the potential performance measures in cellular networks are long term revenue  utility  cbr  which is calculated by the number of rejected setup calls divided by the number of setup requests  or handoff failure rate  cdr  which is calculated by the number of rejected handoff calls divided by the number of handoff requests . cdr can be reduced by reserving some bandwidth for future handoffs. however  the cbr may increase due to such bandwidth reservation. hence  reduction of cbr and cdr are conflicting requirements  and optimization of both is extremely complex.
　several researchers have shown that the guard channel threshold policy  which a priori reserves a set of channels in each cell to handle handoff requests  is optimal for minimizing the cdr in a non-multimedia situation. however the computational complexity of these approaches becomes too high in multi-class services with diverse characteristics context  and exact solutions become intractable  soh and kim  1 .  choi  1  suggest a bandwidth reservation scheme using  an effective mechanisms to predict a mobile terminal  mt 's moving direction and reserve bandwidth dynamically based on the accurate handoff prediction. however this scheme incurs high communication overheads for accurate prediction of a mt's movement.
　there are a variety of artificial intelligence techniques that have been applied to the cac schemes  such as rl  senouci  1 . however rl can be difficult to scale up to larger domains due to the exponential growth of the number of state variables. so in complex real world situations learning time is very long  sutton  1 .  
　this paper proposes a novel approach to solve the cac in multimedia cellular networks with multiple classes of traffic with different resource and qos requirements  and the traffic loads can vary according to the time. a near optimal cac policy is obtained through a form of ne algorithm called neuroevolution of augmenting topologies  neat  
 stanley  1 . by prioritizing the handoff calls and perceiving the real time cbr and cdr  the proposed learning scheme called cn can be trained offline and the learned best policy can dynamically adjust the cac policy to adapt to the varying of traffic loads: when the perceived cdr become higher  the system will reject more setup calls to decrease the cdr  and vice versa.
　the learned policies are compared to a greedy scheme  which always accepts a call if there is enough bandwidth capacity to accept this call  and a rl cac scheme  which is similar as the scheme proposed in  senouci et al.  1 .the simulation results shows that our proposed scheme can learn very flexible and near optimal cac policies  which can maintain the qos constraints in the presence of smoothly changing arrival rate of traffic  a scenario for which the state space is too large for practical deployment of the other learning scheme evaluated.
　the paper is organized as follows: section 1 gives a brief introduction to neat  which is a form of ne method  and describes how to apply the neat to the cac application; section 1 describes the test system model  and formulates the fitness function  while section 1 compares the performance in constant and time-varying traffic loads.
1 applying neat to cac 
neat is a kind of ne method that has been shown to work very efficiently in complex rl problems. ne is a combination of neural networks and genetic algorithms where neural networks  nns  are the phenotype being evaluated.  stanley  1 
　the neat method for evolving artificial nns is designed to take advantage of neural network structure as a way of minimizing the dimensionality of the search space. the evolution starts with a randomly generated small set of nns with simple topologies. each of these nns is assigned a fitness value depending on how well it suits the solution. once all the members of the population are assigned fitness values  a selection process is carried out where better individuals  high fitness value  stand a greater chance to be selected for the next operation. selected individuals undergo recombination and mutation to result in new individuals. structural mutations add new connections and nodes to networks in the population  leading to incremental growth. the whole process is repeated with this new population until some termination criterion is satisfied.  stanley  1  
　this section specifies the main requirements of applying neat to the cac application  and describes the work process of setting up a connection. 
1 main requirements   
neat can be seen as a black box  which can provide a neural network for receiving the inputs and generating the outputs. 
there area several issues need to be considered: 
1. what are the inputs  
　normally the inputs are the perceived state of the environment that is essential to make the action decision.  for the cac problem the perceived state may include the new request call and all the currently carried connections in the cell.  
1. what are the outputs  
　generally the outputs are the possible actions that can be performed in the real application. in cac  there are only two possible actions: accept and reject  so one output is enough. we define the output is a real number  and its value is between 1 and 1  if it is larger than 1  then the action selected is accept; otherwise  it is reject. 
1. how to evaluate each policy and how to formulate the fitness function  
　the fitness function determines what is good or bad policy. a good policy has a high fitness score and so will have a higher probability to create offspring policies. the fitness function is described in a later section. 
1. internal and external supervisor 
　during the learning period  many nns are randomly generated and some of them can lead to very bad performance. to prevent damage or unacceptable performance  an internal supervisor is needed to filter out clearly bad performance policies. in cac  nns that always reject all kinds of calls are frequently generated and so would be evaluated. however these always reject policies are obviously not good and evaluation is pointless. so the internal supervisor gives their fitness score the value of 1.
　additionally  the actions generated by neat are not always feasible. for example  during evaluation the evolved nns may try to accept a request call when the system is full  which is physically unrealizable. therefore an external supervisor is added and uses prior knowledge in order to filtering impossible actions due to the system constraints.
1 the work process of setting up a connection   
figure 1 illustrates the work process of setting up a connection using our scheme.

figure 1 the process of setting up a connection 
1. the network cell receives a user request event  in this case from a network traffic simulator    perceives the network state  such as checking all the ongoing connections in the cell  calculates the real time cbr and 
cdr    and send the inputs to neat; 
1. the nn currently being evaluated in neat generates the output according to the received inputs  and sends the output to the external supervisor;
1. if the output is invalid as determined by the predefined rules in the external supervisor  a reject action will be sent to the cell. this occurs only when the capacity of cell is at its upper limit and the output decision of the cac policy is still accept. in this way the policy will not be punished by negative fitness value for creating impossible actions  because during exploration it is very difficult to create a policy that can always generate correct actions for any possible environment.
1. the cell performs the output action  which means to accept or reject the request event.
1. if the decision is to accept  the cell will allocate the requested bandwidth; if the decision is to reject  the cell will not take any action. 
1 fitness function  
our objective of cac is to accept or reject request  setup or hand off  calls so as to maximize the expected revenue over an infinite planning horizon and meet the qos constraints  and more specifically to trade off cbr and cdr so that the specified cdr will not exceed a predefined upper bound while retaining an acceptable cbr level.
　in this paper we only consider non-adaptive services  e.g. the bandwidth of a connection is constant. we assume m classes of traffic: {1  1 ... }m . a connection in class i consumesbi units of bandwidth or channels  and the network can obtain ri  rewards per logical second by carrying it. we also assume that traffic from each class arrives according to poisson distribution and their holding time is exponentially distributed. all the arrival distributions and call holding distributions are independent of each other. the arrival events include new call arrival events and handoff arrival events. since the call departures do not affect the cac decisions  we only consider the arrival events in the cac state space. additionally  we only consider a cell with fixed capacity. the fixed total number of bandwidth  channels  isc .
　we denote i s  and i h  as the arrival rates of new setup and handover requests in classi   i s  1 and i h  1 as the average holding time of setup and handover calls in class i .
1 first goal: maximize revenue- f1
if the goal is simply to maximize revenue  the fitness can be assessed by f1   which can be calculated from the average reward obtained per request connection as equation  1 . let n be the total number of request connections  includes setup and handoff calls   n is the total number of accepted connections.  
	n	m
 	for large n  f1 	   1 
	n	n
   here m denotes the total number of classes of traffic  i denotes the class of traffic for each individual connection  ki s  and ki h   denotes the number of the accepted new setup calls and handover calls in class i .the relationship below also holds. 
	ki s 	 pi s  and ki h 	 pi h   
  where pi s  and pi h  denotes the acceptance rate of new setup request calls and handover calls in class i .
the number of accepted setup calls in class i
pi s 
request setup calls in class i
　　the number of accepted handover calls in class i pi h 
request handover calls in class i
to simplify the above formula  define the service demand 

　it can be seen that the total revenue is determined by both of the service demand parameters  and the learned cac performance pi s  and pi h    which can be calculated after evaluation of each individual policy.
1 handle cbr and cdr trade-off-- f1 .
two parameters are used to trade off between cbr and cdr: the importance factor and the penalty fitness f . the importance factor indicates the preference of the system as shown in equation  1 . for a network it is commonly more desirable to reject new setup requests and accept more handoff requests  so normally i s  i s  i h  i h  . by carefully selecting for each kind of traffic  the expected cac policy can be evolved. the fitness function is shown as below: 
	m	m
	f1	  i s 	i s  pi s 	i h 	i h  pi h   	  fi s 	fi h           1  
	i 1	i 1
　the other parameters in equation  1  are the penalty fitness f .when the observed cbr or cdr exceeds their predefined upper limit bound  tcdr andtcbr    the evaluated policy needs to be punished by a negative fitness score f   which should be large enough to affect the total fitness f1 . additionally  although dropping handoff calls is usually more undesirable than blocking a new connection  designing a system with zero cdr is practically impossible and will lose unacceptable revenue. in practice  an acceptable cdr is around 1 or 1%. fi s  and fi h  give an opportunity to prevent the network from losing too much revenue by rejecting setup calls. generally  we consider that cdrs are more important than cbrs  therefore fi h  fi s  .
　these two parameters and f work cooperatively to trade off between cbrs and cdrs:  gives extra weight for each different kind of traffic to indicate the preference of the network  fi h  can increase the speed to find policies that can decrease cdr undertcdr   and fi s  defines the upper limit of tcbr to trade off between cbrs and cdrs.
1 training design and performance comparison
this section presents two kinds of training processes: one for constant traffic load  another for time-varying traffic load. we use computer simulations to evaluate the performance of the learning approach. a single cell within a cellular network with a limited number of channelsc  1 is considered. we define two classes of traffic labeled c1 and c1  which require bandwidth capacity 1 and 1 respectively. the reward rate is r 1/sec/connection for carrying each connection.
　to evaluate the traffic load in each cell  we choose to normalized offered load with respect to the radio bandwidth capacity in a wireless cell  which is defined as  normalized offered load  mentioned in  soh  1  . 
1
　　1 l
　the simulation is divided as two periods: a learning period and an evaluation period. during the learning period cn is trained to evolve higher fitness policies. during the evaluation period  the highest fitness policy is selected to perform cac.
1 constant traffic load 
in this experiment the cn is compared with a q learning rl based scheme and a greedy policy  all for a constant normalized offered load. the traffic parameters is defined in table 1. 
parametersc1c1setup handover setup handover 1.1 1 1 1 1
i1 1 1 1 table 1.traffic parameters 
the proposed scheme--cn
training in constant traffic loads  the service demand parameter  doesn't change  so the fitness is only concerned with the learned cac policy pi s  and pi h  . cn requires five inputs. input 1 indicates the setup utility of different kinds of request event  the utility is calculated as equation 1    and input 1 to 1 indicates the number of ongoing calls for each kinds of traffic carrying in the cell. for better performance these inputs are normalized from 1 to 1. 
	 importance factor 	 reward rate  r
utility                   1  
b  bandwidth capacity 
  equation  1  is used as the fitness function  and table 1 shows the value of the related parameters. comparing the values of i   the setup calls of class c1 has lest value  therefore it has the most probability to be rejected. 
parametersc1c1setup handover setup handover i1 1 1 1 i1 1 1 1 i1 1 1 1 table 1.cn simulation parameters 
 during learning period  neat evolves 1 generations  each generation had 1 policies  and each policy was evaluated for 1 events.
rl based cac scheme 
we implemented a lookup table based q-learning cac scheme  rl-cac   which is similar to the one proposed in  senouci  1 . the q-function is learned through the following update equation: 
q s a 	q s a 	r s a     	maxq s a' 	'	q s a 
a'
 the reward r s a  assess the immediate payoff due to a action decision in time t that it is in state s and has taken action a . since the network can obtains the rewards by carrying each connection in a while  and receive nothing when just accept or reject it  the immediate reward is calculated by the total rewards obtains during the time between two consecutive events: one is happened in time t   and another is the previous event. 
1
		learning rate 	t	 
	1	visit s at   t   t  
where visit s at   t   t   is the total number of times this state-action pair has been visited. 
	discount factor 1.
	exploration probability	1.
important factor  is used to differentiate each kind of traffic  and prioritize the handoff calls. 
1 s1 h1 s1 h1 1 1 1 table 1.important factor in rl-cac 
greedy policy 
the system always accepts a request if there is enough bandwidth capacity to accept the new call. 
performance comparison 
three cn schemes are compared with rl-cac and the greedy policy. the total cbr and cdr used in fig 1 evaluate the total rejected rate for setup and handoff calls. 
　as shown in fig 1  the cn-c1  cn-c1 has similar performance with rl-cac  which is evidence that the cn scheme obtains near optimal policy.  
　table 1 and 1 shows that by defining different thresholds for each kinds of traffic  cn can learn very flexible cac policies  and not only trade-off between setup and handoff calls but also trade-off between different classes of traffic  which is difficult implemented by rl based cac schemes. because for such constraints semi-markov decision problem  smdp   the state space should be defined by qos constraints  it becomes too large  and some form of function approximation is needed to solve the smdp problem. 

figure 1. total cbr and cdr comparison
tcbrtotal cbr tcdrtotal cdrcn-c1 1% 1% 1% 1% cn-c1 1% 1% 1% 1% table 1.cn schemes comparison 
cac
schemesparameterstotal
cdrtotal
cbrc1 hc1 hc1 sc1 srl-cac cbr or cdr 1% 1% 1% 1% 1%1%cn-c1upper limit no no 1% 1% 1% 1% cbr or cdr 1% 1% 1% 1% 1%1%table 1. cn and rl-cac comparison 
　 in addition   simulation results show that by average cn schemes lost around 1% of revenue during learning by filtering clear bad performance policies.
1 time-varying traffic load 
in real world  the traffic load in a cellular system varies with time. a dynamic cac policy which can adapt to the time-varying traffic loads is required: when the handoff failure rates grows higher  the system will reject more setup calls in order to carry more handoff requests  and vice versa. in this dynamic environment  most learning schemes trained in constant traffic loads cannot obtain the optimal performance and maintain the qos constraints. 
　to simplify the problem  only the arrival rates of each kind of traffic are changed  the averages of holding times are kept as constant during the whole simulation. 
　in this dynamic environment  the state space is very large  and until now there is no technical report to provide any optimal solution by using rl algorithm. therefore we only compare our schemes with greedy policy.
　in the time-varying traffic loads  the service demand factor  varies according to the time and is difficult to be calculate  we use equation  1  to ignore the    however it does not adapt to the changes of offered loads. in this case the cac scheme requires  feedback from the environment to indicate the changes of traffic loads. the simulation results demonstrate that the cbr and cdr can be good candidates. because using any non-adaptive cac policy  when the traffic load increases the cbr and cdr will become higher  the cbr and cdr can be a factor to reflect the traffic loads.
　with time-varying traffic loads cn requires seven inputs  figure 1 : input 1 indicates the real-time cbr calculated by the number of rejected setup calls divided by the latest 1 setup requests.  if the number of setup requests is less than 1  then the number of rejected setup calls is divided by the number of setup request; input 1 indicates the real-time cdr calculated in a similar way as input 1; input 1 to 1 are same as the inputs in the experiment with constant traffic loads. 
if 1 s 1 h 1 s
	1	1	1	1	1	1	1	1
	training intervals	evaluation intervals
figure 1 traffic loads during learning and evaluation 
　the experiment is still divided into two periods: the learning period and evaluation period. in the learning period  each policy is trained with ten different normalized offered loads  which is called one training session  figure 1.a . the load was generated by segmenting the training session into 1 time intervals where during each training interval the load was held at the constant level.  the changes of i 1 follow a sine function as shown in equation 1 .  each policy is evaluated interval by interval and obtains an interval fitness score calculated by interval fitness function if as equation  1 . after finishing the evaluation of the 1th interval  the ten interval fitness scores are averaged as the final fitness of the policy. 

parametersc1c1setup handover setup handover 1 min1 1 1 1 1
i1 1 1 1 table 1.the traffic definition in time-varying traffic loads 
　the evaluation period evaluates the learned best policy. it is divided into 1 equal lengths of time intervals with different values of arrival rates for each kind of traffic  figure 1.b . the changes of 1 again follow a sine function as  which is similar as the one used in the learning phase  but sampled more finely. within each evaluation interval the arrival rate is kept unchanged. each evaluation interval is as long as one training interval. table 1 shows the traffic definition.
　the simulation runs 1 1 logical seconds. each training or evaluation interval is 1 logical seconds with around 1 request events. neat evolves 1 generations  and each generation has 1 policies.

evaluation intervals
figure 1.comparison of interval cbr and cdr with time-varying traffic loads. at the end of each evaluation interval  the total cbr and cdr is calculated and compared. 
　two cn schemes are compared: cn1only specified the threshold for cdr  which is 1%; cn1 specified both of the cdr  1%  and cbr  varying according to the different training intervals .
evaluation period  greedy cn1 cn1 total cbr 1 1 1 total cdr 1 1 1 table 1.comparison of cbr and cdr 
　figure 1 shows the comparison of cbr and cdr for different cac schemes. we can see that with time-varying traffic loads cn can successfully maintains the cdr under a predefined level. additionally from table 1  it can be seen that the cn1 only defines the threshold for cdr therefore its cbr is higher  the system will lost more revenue by rejecting too many setup calls  however cn1 uses threshold for both of the cbr and cdr to force the system to decrease the cbr so as to successfully trade-off between the cbr and 
cdr.
　in this experiment  the highest fitness nns evolved by neat are very complex and non-standard  and it is impossible to specify any particular hidden layers  there are sets of hidden nodes connected with each other by negative or positive weights.
1 conclusion  
this paper proposes a novel learning approach to solve the cac in multimedia cellular networks with multiple classes of traffic. the near optimal cac policy is obtained through a form of neuroevolution  ne  algorithm.
　simulation results show that the new scheme can effectively guarantee the specified cdrs remain under their predefined upper bound while retaining acceptable cbrs. additionally our proposed scheme can learn very flexible cac policies by defining different thresholds for each kind of traffic  whereas rl based cac scheme can not.
　furthermore this scheme can learn dynamic cac policies to adapt to changes of traffic loads. no matter what the traffic load is  the cac scheme is trained to reject more setup calls to decrease the cdr when the cdr exceeds the prescribed upper limit  and vice versa. therefore if the scheme is trained carefully according to the traffic instance during a day in a network element  the highest fitness policy can be used directly in the real world. 
acknowledgments
thanks kenneth o. stanley and ugo vieruccifor very much for providing free software of neat in the website http://www.cs.utexas.edu/~nn/index.php.
