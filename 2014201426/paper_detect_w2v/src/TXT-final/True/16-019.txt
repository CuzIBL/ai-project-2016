activation-based parsing 
m a r k a. jones* 
state university of new york at stony brook long island  new york 1 

a b s t r a c t 
　　　a model is presented that describes natural language parsing in terms of a uniform activation algorithm which operates over an interconnected  declarative structure of nodes and node instances. the algorithm directs the flow of activation and expectation using only local information and  hence  supports substantial concurrency. a representation is introduced to express node relationships and activation agreement examples of several linguistic processes and their interrelationships are described 
1. introduction 
　　　the concept of a layered or stratified system has been a central notion in many linguistic theories. a system is usually defined in terms of objects at various levels of linguistic description and interand intra-level mapping rules. traditional levels include phonology  morphology  syntax  semantics  and pragmatics. the objects include syntax trees  deep structure trees  lambda calculus formulas  predicate calculus formulas  and frame  slot-filler  representations. the corresponding rule formalisms include tree transformations  lambda application and reduction  logical implication  and frame composition. the rules  particularly in computational theories  are often context-free in appearance  but occasionally possess augmentations to restrict their application  to build new objects  and/or coordinate the application of other rules. 
　　　as the role of pragmatic knowledge has grown in ai systems for tasks such as story understanding  a broad new spectrum of  knowledge structures  has been introduced.** the conceptual status of these proposed objects and the nature of the indexing or mapping rules are still the subjects of inquiry and debate in cognitive science the interplay of the processing components of the programs and the indexing properties of the objects sometimes makes it difficult to extract the exact character of the mapping rules  but they appear to be similar to other rule-oriented formalisms. 
　　　the current work attempts to unify the insights from linguistic and conceptual analysis into a computational theory based on activation. activation-based computational methods are defined here to mean computational methods in which the interconnection structure of a body of declarative knowledge plays an active role in the processing of information. our concern here is with models in which the interconnected memory structure is as active as possible  and our focus is particularly on the task of natural language parsing. the term activation-based parsing refers to the processing of an input string by the initial  sequential  stimulation of memory nodes corresponding to the constituent substrings  generally words  and by the subsequent activation of other nodes by these. the parse is represented by activation traces derived from the interconnection of existing nodes. a parser  named abp  is implemented in franz lisp to simulate the model that is presented. 
       this paper is based upon work supported by the national science foundation under grant #1st-1 and by the university awards program of the rrsearch foundation of the state university of new york  fellowship #1-1-1. 
     **one recent dissertation  offered 1 different knowledge structures and 1 types of interactions among them. 
　　　particular attention has been given to the problems of representing and effectively using generalizations about language and the underlying concepts that language attempts to convey. specifically  syntactic generalizations have not been distributed into the detailed semantics as in semantic grammars  or embedded in the lexicon as in word-based parsing approaches  cf.  1  1 and its descendants   the phenomena of metaphor processing  where semantic selectional restrictions may be violated   of sensitivity to the phrasing of an utterance  of sensitivity to agreement violations  of novel language use  and especially of language learning attest to human abilities to exploit syntactic generalizations profitably. our view is that generalization is central to the classification and use of concepts in memory and accounts for much of the flexibility in human information processing. this does not imply that lexically activated  word-based  expectations for the idiosyncratic tendencies of language do not exist  nor that utterances must be completely  grammatical  to be understood  the model can account for both of these observations . the point here is simply that generalizations  eg.  syntactic generalizations  do exist and should be helpful in processing activities this emphasis on generalization anticipates future work on language learning and serves as an inspiration for many of the representational issues that we have faced. 
1. the parsing model 
　　　previous work in natural language understanding in ai has underscored the need for applying a wide range of comprehension activities into the parsing process. these activities involve syntactic processing and a wide variety of semantic processing including contextual integration  word sense disambiguation  anaphora resolution  metaphor processing etc the bandwidth among these processing activities in traditional implementations has been low  and as small points out   ... it seems that any strict serial decoupling of associated tasks ... does not permit enough interdependence of function to explain adequately reasoning tasks that people find quite easy and natural to perform.  
　　　our parsing model is defined in terms of a set of semi-distinct levels that represent various types of concepts and processes. each level has a generally hierarchical infrastructure of nodes and  additionally  possesses connections to other levels. the inter-level connections permit concurrent recognition behaviors at various levels. strictly speaking  the nodes in the various levels are not activated themselves  but rather instances of them  and there may be more than one instance of a particular node active at any time. each node can be viewed as a type of production rule which is described in terms of other node activations. a uniform activation algorithm directs the creation and interpretation of node instances. the algorithm is sensitive to the current state of active instances and the recency of expectations placed upon the node by other instances. beyond the activation algorithm  the  process  component of natural language understanding is embodied in the declarative interconnection structure. this distinguishes the approach here from production systems or semantic network schemes in which a substantial amount of the processing behavior is still embodied in program code. 
　　　the primary aspects of the parsing model are  a  the specification of the recognition processes and node interconnections 

that define the levels and  b  the design of the production language and the activation algorithm. the initial level definitions are given below. most of our attention thus far has focused on the form and interplay of the levels 1. 
level 1 - the token level  consists of words and a small set of affixes which have a clear productive or agreement function. the model should be relevant to underlying phonemic and morphemic levels as well but we begin at the token level for simplicity and assume that nodes at this level are exogenously stimulated. except for grouping conjugated verb forms such as see/saw/seen as see+ and noun forms with irregular plurals man/men as man-t-   there is no internal structure to level 1. 
level 1 - the syntactic level  consists of syntactic classifications of tokens  phrasal patterns and their classifications  idioms and syntactic agreement patterns. syntactic word classes form the primary links to level 1  although some syntactic patterns require particular word forms such as do+ . 
level 1 - the conceptual classification level  provides semantic classifications of objects from levels 1  words   and 1  idioms and modifiers such as adverbial phrases and prepositional phrases . these classifications are used by higher levels to sort nominal and event descriptions. 
m. jones 1 
level 1 - the np role level  uses activation from levels 1 and 1 to sort the np concepts according to their  syntactic and semantic  roles in the sentence. the subject np concept will be classified as a subject and either an agent or an object depending on the voice of the verb phrase. based on the level 1 classifications  it may also be refined from agent into actor or instrument. note that certain syntactic phenomena such as tag questions require distinctions such as subject that are intermediate to full conceptual resolution of roles. 
level 1 - the case frame level  is responsible for word sense disambiguation. here are the detailed expectations of certain verb senses for particular prepositions and particles. the case frames are organized hierarchically for the purposes of shared inferences. for example  the senses of throw  roll  and go which indicate physical transfer will be assembled under a ptrans node. 
levels 1 and higher - the contextual levels  identify sequences of level 1 nodes which relate to temporal ordering  causality and plan recognition. 
　　figure 1 illustrates a fragment of a simple syntactic level structure. each node may be viewed as a type of production rule which has been divided into a pattern  pat  and an optional test {test . figure 1 lists the definitions for several of the nodes in figure 1. the tests are listed only in figure 1. the production language allows 

1 m. jones 
sequentiation  seq   alternation  or   a case construct  and a repetition operator  * . the case construct contains antecedent-consequent pairs; if an antecedent activation is seen then a consequent activation must also be seen or the construct is blocked. 
　　　a purely bottom-up activation-based parser encounters trouble due to the fact that the connection structure is not exclusively tree-
　　　figure 1 contains several nodes leading upward to competing alternatives - n/nounstem  n/comrnonnp  n/detpron  and n/np. competing alternatives for node activation cause purely bottom-up schemes to be highly non-deterministic. 
　　　a shared structure may be linked to independent sets of alternatives  termed dimensions. alternatives in different dimensions are non-competing. for example  pronouns activate features in dimension of gender  number  person and case in addition to participating in np recognition activation from a node to non-competing alternatives is deterministic with respect to the activation in each dimension. the figure below shows an example of competing  circled  and non-competing alternatives. 
following an obligatory element. for example  a  regular  noun stem can be optionally followed by -s to form the plural. present tense verb forms are another similar example. the situation is not confined to suffixes. a pronominal determiner  that  which  some  can serve as an np when used pronominally or can serve as a determiner for a common noun np. the head of an np can be optionally followed by qualifiers. 
many verbs can be used transitively and intransitively  which causes premature vp firing. furthermore  the vp may have trailing pp modifiers. the sentence you walk ed htr dog -s in the park contains contains several initial subsequences: 
shartd sub-structures: the epsilon-transition cases  a  and  b  above appear to occur primarily in endocentric constructions in which the optional element does not affect the  type  of the recognized phrase in natural language syntax. the type is established by the head of the phrase which is the obligatory element -- for example  the noun in a noun phrase or the verb sequence in a verb phrase. the same syntactic interpretation survives in these cases and the trailing constituents serve for purposes of agreement or qualification. in other cases  however  competing alternatives may play fundamentally different roles in the interpretive process. structures such as np or pp may be shared by many different constituents. for example  an np can appear as the subject of a sentence  the first or second object  the object of a preposition  etc. token level words may give rise to multiple syntactic categories. polysemic words activate multiple nodes at the semantic levels. 
      this representation was chosen in lieu of a notation for optionality  because the resulting cases  x -  y  and  x -  y z  often need to be explicitly referenced elsewhere  e.g.  for context-sensitive restrictions . the new node  yz  conveniently allows this reference. 
 d  recursion: recursion introduces cycles into the node structure. figure 1 includes two cases of right recursion  the nps appearing in the appositive and the pp  and one case of left-recursion  in m/possnp . recursion can also involve center embedding as with relative clauses. 
1. the activation algorithm 
　　　this section describes an activation algorithm which uses a breadth-first  parallel expansion of activation. the algorithm attempts to curb non-determinism up to genuine ambiguity by augmenting the bottom-up activation with a complementary operation of top-down expectation to provide a biasing affect on alternatives within a dimension. the expectation-driven interpretation of node activation is a powerful principle for controlling the integration of knowledge structures that have been recognized bottom-up. the characteristic pattern of the system is a rippling of activations  upward  and of expectations  downward  both withen and among levels. under this principle  the non-determinism represented by parallel  competing alternatives is restricted by the context. 
　　　the phrase the man illustrates the principle as it applies to case  b.l  in section 1. the following sequence of actions takes place: 
 i  the1 	is exogenously created and triggered. 
 ii  m/deto is activated  followed by n/detnpo which is blocked. 
 iii  mano is then created  triggered and followed by n/nounsingo  n/nouno and n/commonnpo. 
the 	node 	structure 	reveals 	two 	competing 	alternatives 	for 
n/commonnp: n/detnp and n/headnp. the alternative n/detnp is biased  however  because n/detnpo posted an expectation for n/commonnp as it became blocked in  ii . 
 iv  the remaining instance activations and firings are: n/detnpo  n/headnpo  n/npo  ... 
the following rule summarizes the constraint: 
rule 1: for all dimensions stemming from a node  e.g.  n/commonnp  
if 	any 	active 	instances 	 n/detnpo  	expect 	an 	instance 
      n/cornmonnpo  of the node  then the most recent expecting instances are activated and other expecting instances are inhibited. 
　　　as it has been developed thus far  the model seeks to rely on a discrete view of activation and inhibition instead of using more continuous notions of activation levels  thresholds  and decay. in our implementation  the recency effect needed for rule 1 is modeled by having active instances  post down  their expectations for constituents on fifo lists that are associated with the nodes. the implementation treats each expectation that arises from the same exogenous activation as having the same age. the activation and expectation processes are currently assumed to complete before the next input. recency could alternatively be implemented by comparing continuous activation time stamps on instances or by using decaying activation levels to emulate the time stamps. 
　　　the phrase the boy -1 demonstrates the type of nondeterminism described in ease  a.l . the initial substring the boy is itself a noun phrase. at node n/nounstem there is an upward split leading to nodes n/nounsing and n/nounpl. the paths rejoin at n/noun  and any top-down expectations posted through n/noun will fail to distinguish a competing alternative. the following sequence of activations tales place: 
 i  theo  m/deto  n/detnpo  blocked  
 ii  boyo  n/nounstemo  n/nounplo  blocked  and n/nounsingo  n/nouno  n/commonnpo  n/detnpo  n/headnpo  n/npo  ... 
when -so and n/nounplo are activated next  we are faced with a dilemma at node n/noun. should we create a new instance n/nounl  for boy -s  or modify our previous instance  n/nouno  that now represents boy . n/nounl cannot be generated  because the prior expectation environment has been altered by the sequence in  ii  and 
m. jones 1 
it would load to a new np instance. the solution is to link posted expectations together in such a way that the bottom-up activation retraces the expectation structure. at junctures in the expectation structure  eg.  n/nouno  in which an instance  n/nounsingo  has already satisfied the expectation  new instance activations of the juncture node are suppressed. the old activation links from n/nounsingo to n/noun   can be severed and a new link from n/nounpl   to n/nouno can be established. the general rule is: 
rule 1: fur all dimensions stemming from a node  e.g.  n/nounpl  
if any instance  n/nounsingo  has already satisfied the same expectation  in n/nouno  that an instance of the node 
      n/nounplo  wants to satisfy then the last binding  from n/nouno  is redirected  to n/nounpio instead of n/nounsingo  
　　　due to the semantic penetrance of activation from the initial words in a sequence  language has evolved certain relationships across levels in the form of constraints on number  person and gender. in the previous examples  the determiner the failed to bias the n/nounsing and n/nounpl alternatives of n/noun; however  determiners such as a  every  that  and these can express a preference. in our model these determiners activate feature concepts such as n/singdet and n/plurdet in a number dimension in addition to syntactic level activations. the agreement restriction is defined at the coordinating node  n/detnp  and is taken into account when posting expectations for the phrase  a boy  expectations will only be posted in the n/nounsing branch below n/noun. this will cause the bottom-up activation at n/nounstern to direct itself only toward n/nounsing. 
　　　the repetition cases  which only involve a single production  are easier to handle. case  b.1   initial repetition  is similar to normal sequential pattern activation. in case  a.1   final repetition  the production fires after the obligatory elements  and the optional  repeated elements simply attach in the trace. expectations are regenerated after the attachment. 
　　　space does not permit full examples of the recursion cases  so we will only highlight them the existing rules handle the case of right recursion with no difficulty. center-embedded structures simply require that the ufo lists  stacks  of node expectations are popped as instances are created. the algorithm does this anyway since a node expectation is  transferred  of an instance when the instance is created* 
　　　left-recursion poses the greatest difficulty for the expectation  top-down  mechanism due to the possibility of expectation cycles. the two options are to eliminate left-recursion by rewriting the grammar or to define additional rules to handle it. to eliminate the left-recursion in figure 1  the m/possnp node can be deleted as a determiner  leave m/possperspro  and the top of the np structure 
　　　the objection to modifying the grammar is primarily that the syntactic trace structure does not mirror semantic intuitions as closely as the left-recursive trace. to handle left-recursion  the expectation process must be modified to eliminate cycling by having a node realize when it receives an expectation that it has already 
     *an equivalent way of thinking about expectations is to consider them to be proto-instances created by top-down  activation  which survive in the trace only if reinforced by bottom-up activation. 
1 m. jones 
sent  i.e.  that the expectations are in the same expectation structure . the recursively called node non-deterministically activates the left-recursive instance and the calling instance. expectations below the recursive node are spawned again when the left-recursive instance fires and the calling instance is rebound to the new recursive instance. 
　　　an knglish grammar subset of 1 syntactic level nodes has been encoded to explore various interesting recognition issues. the subset includes the verb tense system for declarative sentences  relative clause constructions and active/passive voices. the syntactic level algorithm described above has been implemented and handles the subset. 
1. related work 
　　　fahlman's netl system  1| is an example of a marker-passing scheme. an interesting use of netl to determine noun-noun modifying relationships is described in . woods  l1| discusses schemes using marker propogation in taxonomic lattices that might be used in conjunction with an atn  augmented transition network  grammar. 
　　　atn implementations and  in particular  woods' notion of cascaded atn grammars  are related to the work presented here. many atn implementations have used techniques such as backtracking  arc-ordering  bounded lookahead and well-formed substring tables which are designed for efficient execution on sequential machines. it is possible  however  to define parallel atn implementations which do not need these techniques. atn registers are used for feature information extracted from the lexicon  for constituent structures such as subject and object  and for dislocated constituents. pop arcs return structures that are assembled from the registers. our view has been not to explicitly build and pass syntactic or semantic representations  data structures   but to allow the  meaning  of an input to be represented by the impact that it makes upon the instance and expectation structure* 
　　　cascaded atns include the concept of concurrent operation of multiple atns at different levels. the emphasis is still on the passing of structures that are created and held at one level until an opportune time to transmit them. in our work there is a less rigid distinction of levels in terms of global and immediate availability of node activations and there is a greater assumed bandwidth among recognition processes. our architecture allows a greater penetration of both activations and expectations to reduce non-determinism. the uniform architecture also is desirable for future considerations of learnability. 
　　　a detailed comparison of the standard atn grammars and the type encouraged by the activation-based formalism is beyond the scope of this paper  but one example from   l l | will be discussed. an interrogative sentence can begin with an auxiliary verb  a form of do  be  have  or a modal  followed by the subject; whereas  the order is reversed in a declarative sentence. woods discusses the awkwardness of ordinary psgs  phrase structure grammars  to account for this. in our grammar  the pattern for one of the sentence nodes  e/qaux  is of the form: 
 seq  ml .  or do+ be+ have+ m/futmodal    el . e/decls   
this behaves similarly to gazdars holes . if the auxiliary is seen  then e/qaux posts an expectation for a declarative sentence. note that some verb sequence instances have already been created in the prior activation of the auxiliary. now ihe sentence is recognized as usual and the verb sequence activations are completed. np holes in relative clauses can be treated similarly. 
1. future work 
　　　we have begun expanding the model to deal with levels 1. level 1 contains a conceptual recognition structure similar to standard isa-hierarchies  but with a rich categorization structure. the interior nodes in the level 1 structure serve largely to facilitate the conceptual expectations of higher level structures. 
* see  1| for a linguistic theory of a similar notion of meaning. 
　　　level 1 uses the semantic information in syntactic order from level 1 and the conceptual categorizations to sort the np concepts into case role relationships for level 1. we are designing a description language to aid in expressing compositional relationships that can be used to  compile  new node descriptions. let us assume that a declarative sentence pattern  e/decls  looks like:  seq  nl . n/np   el e/vp  . 
we can express our semantic intuitions about the level 1 and 1 mappings in the following  head modifier*  structure: 
  el . e/event   if   el . e/active   m/agent  nl . n/nominal    
  el . e/passive   m/object  nl . n/nominal      
i'sing the compositional relationship and the pattern results  for example  in a compiled node pattern for m/agent of:  seq  nl . n/nominal   el . e/active  . 
1. conclusions 
　　　an activation-based model for natural language processing has been outlined an activation algorithm that uses local information regarding the current state of active instances and expectation structures has been applied to syntactic processing and is now being extended to other processes. the algorithm uniformly uses expectations to bias the activation spread at non-determininstic points in the declarative node structure. 
　　　the model should have interesting applications to difficult problems such as ellipsis  deletion  conjunction and anaphora which depend highly on the recency and type of node activity. the concurrency in the model suggests ways of using the redundancies in natural language communication to proceed with an interpretation in spite of ill-formed or incomplete input. the model also offers advantages in extensibility including  1  the properties of additivity and continuity in the integration of new knowledge  and  1  the fact that system performance should not seriously degrade with additional knowledge. 
