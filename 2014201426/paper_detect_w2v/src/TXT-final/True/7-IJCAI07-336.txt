 
programming a humanoid robot to walk is a challenging problem in robotics. traditional approaches rely heavily on prior knowledge of the robot's physical parameters to devise sophisticated control algorithms for generating a stable gait. in this paper  we provide  to our knowledge  the first demonstration that a humanoid robot can learn to walk directly by imitating a human gait obtained from motion capture  mocap  data. training using human motion capture is an intuitive and flexible approach to programming a robot but direct usage of mocap data usually results in dynamically unstable motion. furthermore  optimization using mocap  data in the humanoid full-body joint-space is typically intractable. we propose a new modelfree approach to tractable imitation-based learning in humanoids. we represent kinematic information from human motion capture in a low dimensional subspace and map motor commands in this lowdimensional space to sensory feedback to learn a predictive dynamic model. this model is used within an optimization framework to estimate optimal motor commands that satisfy the initial kinematic constraints as best as possible while at the same time generating dynamically stable motion. we demonstrate the viability of our approach by providing examples of dynamically stable walking learned from mocap data using both a simulator and a real humanoid robot. 
1 introduction 
imitation is an important learning mechanism in many biological systems including humans  rao and meltzoff  1 . it is easy to recover kinematic information from human motion using  for example  motion capture  but imitating the motion with stable robot dynamics is a challenging research problem.  traditional model-based approaches based on zero-moment point  zmp   vukobratovic and borovac  1    kajita and tani  1   or the inverted pendulum model  yamaguchi et al.  1   require a highly accurate model of robot dynamics and the environment in order to achieve a stable walking gait. learning approaches such as reinforcement learning  sutton and barto  1   are more flexible and can adapt to environmental change but such methods are typically not directly applicable to humanoid robots due to the curse of dimensionality problem engendered by the high dimensionality of the full-body joint space of the robot.  
     

figure 1. a framework for learning human behavior by imitation through sensory-motor mapping in reduced dimensional spaces. 
    in this paper  we propose a model-free approach to achieving stable gait acquisition in humanoid robots via imitation. the framework for our method is shown in figure 1. first  a motion capture system transforms cartesian position of markers attached to the human body to joint angles based on kinematic relationships between the human and robot bodies. then  we employ dimensionality reduction to represent posture information in a compact low-dimensional subspace. optimization of whole-body robot dynamics to match human motion is performed in the low dimensional space. in particular  sensory feedback data are recorded from the robot during motion and a causal relationship between actions in the low dimensional posture space and the expected sensory feedback is learned. this learned sensorymotor mapping allows humanoid motion dynamics to be optimized. an inverse mapping from the reduced space back to the original joint space is then used to generate optimized motion on the robot. we present results demonstrating that the proposed approach allows a humanoid robot to learn to walk based solely on human motion capture without the need for a detailed physical model of the robot. 
1 human motion capture and kinematic mapping 
 
figure 1. kinematic mapping used in our approach  from left to right: human body  human skeleton  robot skeleton  and robot body  respectively . 
in this paper  we manually map the joint angle data from a motion capture system to a kinematic model for a fujitsu hoap-1 humanoid robot. to generate the desired motion sequence for the robot  we capture example motions from a human subject and map these to the joint settings of the robot.  initially  a set of markers is attached to the human subject and the 1-d positions of these markers are recorded for each pose during motion.  we use a vicon optical system running at 1hz and a set of 1 reflective markers.  these recorded marker positions provide a set of cartesian points in the 1d capture volume for each pose.  to obtain the final subject poses  the marker positions are then assigned as positional constraints on a character skeleton to derive the joint angles using standard inverse kinematics  ik  routines.   as depicted in figure 1  in order to generate robot joint angles  we simply replace the human subject's skeleton with  a robot skeleton of the same dimensions.  for example  the shoulders were replaced with three distinct 1-dimensional rotating joints rather than one 1-dimensional ball joint.  the ik routine then directly generates the desired joint angles on the robot for each pose.  there are limitations to such a technique  e.g  there may be motions where the robot's joints cannot approximate the human pose in a reasonable way   but since we are interested only in classes of human motion that the robot can handle  this method proved to be a very efficient way to generate large sets of human motion data for robotic imitation. 
1 sensory-motor representations 
1 low-dimensional representation of postures 
particular classes of motion such as walking  kicking  or reaching for an object are intrinsically low-dimensional. we apply the well known method of principal components analysis  pca  to parameterize the low-dimensional motion subspace x . although nonlinear dimensionality reduction methods could be used  e.g.   macdorman et al.  1    grochow et al.  1    we found the standard linear pca method to be sufficient for the classes of motion studied in this paper.  
¡¡the result of linear pca can be thought of as two linear operatorsc and c-1 which map from high to low and low to high dimensional spaces respectively. the low dimensional representation of the joint angle space of the hoap-1 robot executing a walking gait  in the absence of gravity  is shown in figure 1. 

figure 1. posture subspace and example poses. a 1-dimensional reduced space representation of the postures of the hoap-1 robot during a walking motion. we applied linear pca to 1 dimensions of joint angle data of the robot that mapped from a human kinematic configuration as described in section 1. blue diamonds along the function approximated trajectory represent different robot postures during a single walking cycle. red circles mark various example poses as shown in the numbered images.  
¡¡we use a straightforward  standard linear pca method to map between the low and high-dimensional posture spaces. vectors in the high-dimensional space are mapped to the low-dimensional space by multiplication with the transformation matrixc . the rows of c consist of the eigenvectors  computed via singular value decomposition  svd   of the motion covariance matrix. svd produces transformed vectors whose components are uncorrelated and ordered according to the magnitude of their variance.  
¡¡for example  let q= 1¡Á vector of joint angles  the high-dimensional space  and p= 1¡Á vector in 1d space. we can calculate p in 1d space by usingr=cq   where r is a 1¡Á vector of all principal component coefficients of q and c is the 1¡Á transformation matrix.  we then pick the first three elements of r  corresponding to the first three principal components  to be p . the inverse mapping can be computed likewise using the pseudo inverse of c . 
1 action subspace embedding 
high-level control of the humanoid robot reduces to selecting a desired angle for each joint servo motor. as discussed previously  operations in the full space of all robot joint angles tend to be intractable. we leverage the redundancy of the full posture space and use the reduced dimensional subspace x to constrain target postures. any desired posture  also referred to as an action  can be represented by a pointa¡Êx .  
¡¡a periodic movement such as walking is represented by a loop inx as shown in figure 1. in the general case  we consider a non-linear manifold representing the action space a  x . non-linear parameterization of the action space allows further reduction in dimensionality. we embed a one dimensional representation of the original motion in the three dimensional posture space and use it for constructing a constrained search space for optimization as discussed in section 1. using the feature representation of the set of initial training examples xi = c zi   we first convert each point to its representation in a cylindrical coordinate frame. this is done by establishing a coordinate frame with three basis directions x  y  z¦È ¦È ¦È in the feature space. the zero point of the coordinate frame is the empirical mean ¦Ìof the data points in the reduced space. we recenter the data around this new zero point and denote the resulting data x  i . 

figure 1. embedded action space of a humanoid walking gait. training data points in the reduced posture space  shown in bluedots  are converted to a cylindrical coordinate frame relative to the coordinate frame x  y  z¦È ¦È ¦È . the points are then represented as a function of the angle ¦Õ  which forms an embedded action space 
 shown in red-solid-curve . this action space represents a single gait cycle. 
we then compute the principal axis of rotation z¦È: 
	¡Æ  x ¡Áx  i	  i+1 
	z¦È=	i	.  	 	 	 	 1  
¡Æ  x ¡Áx  i	  i+1  i
next  x¦È is chosen to align with the maximal variance of xi in a plane orthogonal to z¦È . finally  y¦È is specified as orthogonal to x¦È and z¦È . the final embedded training data is obtained by cylindrical conversion to ¦Õ r h where r is the radial distance  h the height above the x¦È ¦È y plane  and ¦Õ the angle in the x¦È ¦È y plane. the angle ¦Õ can also be 
interpreted as the phase angle of the motion.  
¡¡given the loop topology of the latent training points  one can parameterize r and h as a function of¦Õ . the embedded action space is represented by a learned approximation of the function: 
 r h  = g ¦Õ                                      1  
where 1 ¡Ü ¡Ü¦Õ 1¦Ð . approximation of this function is performed by using a radial basis function  rbf  network. 
1 learning to predict sensory consequences of actions 
a central component of our proposed framework is learning to predict future sensory inputs based on actions and using this learned predictive model for action selection. the goal is to predict the future sensory state of the robot  denoted by st+1 . in general  the state space s= ¡Áz ¦± is the cartesian product of the high-dimensional joint space z and the space of other percepts¦± . other percepts could include  for example  a torso gyroscope  an accelerometer  and foot pressure sensors as well as information from camera images. the goal then is to learn a function f : ¡Ás as that maps the current state and action to the next state. for this paper  we assume that f is deterministic. 
   often the perceptual state st is not sufficient for predicting future states. in such cases  one may learn a higher order mapping based on a history of perceptual states and actions  as given by an n-th order markovian function: s = f  st t-n  ... st-1 at-n  ... at-1           1  
   we use a radial basis function  rbf  approximator to learn f from sensory-motor experience. in particular  the rbf network approximates f by learning a function f' : ¦Á  ¦Â : 
	¦Â = 	exp	 	         1  
k
where k represents the number of kernels  ¦Ìk and are the mean and inverse covariance of the -thk kernel respectively. the output weight vector wk scales the output of each kernel appropriately  and the input and output are  ¦Á =  s   st t-1 ... st-n-1 a  at t-1 ... at-n-1  and¦Â = st+1 respec-
tively. for convenience  one can instead view the rbf as a time delay network  lang et al.  1  for which the input simplifies to ¦Á =  s  at t  . the previous state and action inputs are implicitly remembered by the network using recurrent feedback connections. 
   in this paper  we use a second-order  n = 1  rbf network with the state vector equal to the three-dimensional gyroscope signal  st ¡Ô¦Øt   . as discussed in the previous section  an action represents the phase angle  radius  and height of the data in latent posture space at ¡Ô ¡Ê¦Ö x  .  
1 motion optimization using the learned predictive model 
the algorithm we present in this section utilizes optimization and sensory prediction to select optimal actions and control the humanoid robot in a closed-loop feedback scenario. figure 1 illustrates the optimization process.  
¡¡one may express the desired sensory states that the robot should attain through an objective function ¦£   s . our algorithm then selects actions a*t  ... a*t such that the predicted future statesst  ... st will be optimal with respect to    ¦£ s : 
	a*t = argmin¦£ f st  ... st-n at  ... at-n  .     	  1  
 
 ¦Øa      = actual gyroscope signal
 ¦Øp      = predicted gyroscope signal	¦Ö¡ä = tentative posture command 
 
figure 1. model predictive controller for optimizing posture stability. the optimization algorithm and the sensory-motor model predictor produce the action  at ¡Ô ¡Ê¦Ö x which is used for posture control of the humanoid robot. the resulting gyroscope signal is fed back to the predictor for retraining. the optimization algorithm utilizes a predicted gyroscope signal¦Øp in order to optimize actions for posture stability. 
 
 the objective function used in this paper is a measure of torso stability as defined by the following function of gyroscope signals:   
 
	¦£ ¦Ø  = ¦Ëx¦Ø1x +¦Ëy¦Ø1y +¦Ëz¦Øz1      	 	   1  
 
where ¦Øx  ¦Øy ¦Øz refer to gyroscope signals in the x  y  z axes respectively. the constants ¦Ëx ¦Ëy ¦Ëz allow one to weight rotation in each axis differently. the objective function  1  provides a measure of stability of the posture during motion. for our second-order predictive function f  the optimization problem becomes one of searching for optimal stable actions given by:  
 
¦Ö*t = arg min ¦£ f ¦Øt  ¦Øt-1  ¦Öt  ¦Öt-1        1  ¦Ö t¡Ê
   ¦Õs
	 =      rs	  	 	 	 	 	 	  1  
      hs
to allow for efficient optimization  we restrict the search space to a local region in the action subspace as given by: 
    ¦Õt-1 ¦Õ ¦Õs ¡Ü t-1+¦Å¦Õ       1  r -a ¦År ¡Ü ¡Ürs r +a ¦År       1   h -a ¦Åh ¡Ü ¡Ühs h +a ¦Åh      1  
	1   ¦Å¦Õ   1¦Ð  	 	 	 	 	 1  
	 r  ha	a   = g ¦Õs   	 	 	 	 	 1  
 
the phase motion command search range ¦Õs begins after the position of the phase motion command at the previous time step ¦Õt-1. the radius search range rs begins from a point in the action subspace embedding a that is defined by  1  in both positive and negative directions from ra along r for the distance ¦År   1. the search range hs is defined in 
the same manner as rs according to ha and ¦Åh . in the experiments  the parameters ¦Å¦Õ  ¦År and ¦Åh were chosen to ensure efficiency while at the same time allowing a reasonable range for searching for stable postures. an example of the search space for a walking motion is shown in figure 1. 
   selected actions will only truly be optimal if the sensory-motor predictor is accurate. we therefore periodically re-train the prediction model based on the new posture commands generated by the optimization algorithm and the sensory feedback obtained from executing these commands. after three iterations of sensory-motor prediction learning  an improved dynamically balanced walking gait is obtained. the trajectory of the optimized walking gait in the low dimensional subspace is shown in figure 1.  
 
 

figure 1. optimization result for a walking motion pattern in a low-dimensional subspace based on an action subspace embedding. 
we summarize below the entire optimization and action selection process: 
1  use pca to represent in a reduced 1d space the initial walking gait data from human motion capture. 
1  employ the non-linear embedding algorithm for parameterization of the gait. 
1  start the learning process by projecting actions back to the original joint space and executing the corresponding sequence of servo motor commands in the webots hoap-1 robot simulator  webots  1 . 
1  use the sensory and motor inputs from the previous step to update the sensory-motor predictor as described in section 1 where the state vector is given by the gyroscope signal of each axis and the action variables are ¦Õ r and h in the low-dimensional subspace. 
1  use the learned model to estimate actions according to the model predictive controller framework described above  figure 1 .  
1  execute computed actions and record sensory  gyroscope  feedback.  
1  repeat steps 1 through 1 until a satisfactory gait is obtained. 
1 experimental results 

figure 1. motion pattern scaling. the target motion pattern is scaled down until it can produce a stable motion to start the motion optimization process. 
this section explains how the optimization methodology in the previous section is used in conjunction with the mocap data. from our study of the motion pattern in the reduced subspace  we found that we can scale up and down the motion pattern and get similar humanoid motion patterns except for changes in the magnitude of motion. when we scale down the pattern in the reduced subspace  it produces a smaller movement of the humanoid robot  resulting in smaller changes in dynamics during motion. our strategy is to scale down the pattern until we find a dynamically stable motion and start learning at that point. we apply the motion optimization method in section 1 to the scaled-down pattern until its dynamic performance reaches an optimal point; then we scale up the trajectory of the optimization result toward the target motion pattern. in our experiments  we found that a scaling down of 1 of the original motion pattern is typically stable enough to start the learning process.  our final optimization result obtained using this procedure is shown as a trajectory of red circles in figure 1. it corresponds to about 1% of the full scale motion. 

figure 1. learning to walk through imitation. the pictures in the first row show a human subject demonstrating a walking gait in a motion capture system. the second row shows simulation results for this motion before optimization. the third row shows simulation results after optimization. the last row shows results obtained on the real robot.
our simulation and experimental results are shown in figure 1. we performed the learning process in the simulator  webots  1  and tested the resulting motion on the real robot. the walking gait on the real robot is not as stable as the results in the simulator because of differences in frictional forces between the simulator and the floor. we expect an improvement in performance when learning is performed directly on the real robot. we note that the learned motion is indeed dynamic and not quasi-static motion because there are only two postures in our walking gait that can be considered statically stable  namely  the two postures in the walking cycle when the two feet of the robot contact the ground. the remaining postures in the walking gait are not statically stable as the gait has a fairly fast walking speed.
1 conclusion 
our results demonstrate that a humanoid robot can learn to walk by combining a learned sensory-motor model with imitation of a human gait. our approach does away with the need for detailed  hard-to-obtain  and often fragile physicsbased models that previous methods have relied on for stable gait generation in humanoids. our approach builds on several previous approaches to humanoid motion generation and imitation. okada  tatani and nakamura  okada et al.  1  first applied non-linear principal components analysis  nlpca   kirby and miranda  1  to human and humanoid robot motion data. the idea of using imitation to train robots has been explored by a number of researchers  hayes and demiris  1    billard  1 . in  ijspeert et al.  1   a nonlinear dynamical system was carefully designed to produce imitative behaviors. the mimesis theory of  inamura et al.  1  is based on action acquisition and action symbol generation but does not address dynamics compensation for real-time biped locomotion. the motion segmentation framework in  jenkins and mataric  1  uses dimensionality reduction and segmentation of motion data in the reduced dimensional space but without dynamics compensation.  
¡¡the framework described in this paper has several practical applications. one scenario we are currently investigating is a general navigation task involving our humanoid robot. a modular architecture could be used to switch between a set of learned modules  such as walking-straight  turningleft  turning-right and stepping-backward. using visual information as feedback  we could control robot direction by switching behaviors  actions  of the robot. we have also found that translation in the x-y plane in the reduced 1d latent space can be used for changing direction. because our method does not depend on a model of the world  it can be applied to the problems of learning to walk up and down stairs  and walking on constant slopes. to learn actions other than walking straight  we can modify our optimization function in eq.  1  or introduce additional sensory variables such as velocity or foot-contact pressure. we are currently investigating these lines of research.  
¡¡clearly  the present framework cannot be applied directly to the problem of navigation on uneven terrain. to effectively navigate on uneven terrain  we may need a higher degree of compliance control in the leg and foot actuators. however  a hybrid active-passive actuator will likely produce even more complex dynamics than typical actuators used in current humanoid robots. since our approach does not require a physics-based dynamics model for learning  it lends itself naturally to tackling this problem. we hope to investigate this important research direction in the near future.  
