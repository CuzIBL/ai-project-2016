 
a framework for the construction of new features for hard classification tasks is discussed. the approach brings together ideas from the fields of machine learning  computational geometry  and pattern recognition. two heuristics for evaluation of newly-constructed features are proposed  and their statistical significance verified. finally  it is shown how the proposed framework can be used to combine techniques for selection of representative examples with techniques for construction of new features  in order to solve difficult problems in learning from examples. 
1. introduction. 
the problem of new terms  also known as the constructive induction problem  has long been considered a source of difficulty in machine learning  dietterich  1 . simple classifiers using only the primitive features of description have limited learning capabilities. for example:  i  single-layered neural networks can realize only those class dichotomies  where the classes are linearly separable in the feature space  minsky  1 .  ii  selective induction can be used to learn only those concepts whose concept-membership function is smooth  rendell  1 . 
　　　researchers in different areas have recently addressed these fundamental limitations of simple classifiers. algorithms such as back-propagation  rumelhart  1  can implement learning in multi-layered networks. they implicitly create weighted combinations of primitive features in the internal  hidden  units of such networks. constructive induction  on the other hand  explicitly constructs and tests new terms from the primitive features  dictterich et al  1  by applying feature-construction operators. both of these approaches transform the primitive feature space of the problem into another in which the classes to be discriminated are separable using simple discrimination surfaces. 
　　　with a few exceptions  muggleton  1   these techniques provide no justification for the heuristics used  they do not integrate theories of selection and evaluation of features  and they have no obvious trade-offs between performance and accuracy. in this paper  we examine the problem from a geometric perspective in hope of developing heuristic techniques that are amenable to analyses of performance and accuracy. our focus is on a study of constructive induction in two-class discrimination learning problems. 
1. simple versus hard classification problems. 
given a set of d-dimensional training samples  
e=e+kue~  a simple classification problem is to discover a primitive surface  such as a hyperplane in d-  dimensions  that separates the positive examples e+ from the negative examples ♀-  when the sample points and the hyperplane are represented in the d-dimensional feature space. in a 
hard classification problem  such a primitive surface does not exist. 
1. constructive induction. 
　　　constructive induction discovers new features of the training sample. it transforms feature spaces to permit primitive separating surfaces which  in turn  enable the use of simple classification techniques for solving hard classification problems. 

	mehra  rendell and wah 	1 


1 	machine learning 

　　　the second heuristic  dissimilarity between the feature values for the two classes  can be applied by testing a feature for inclusion in a 1-hypercube in one of the two inverted spaces  and for exclusion in the other  as already discussed in ′1. alternatively  one can use the sine of the angle in order to ensure that the feature vector lies in a positive octant in one of the spaces  and in a negative octant in the other. thus  distinctness can be guaranteed by requiring that the following two conditions hold: 

where a is the angle between the feature vector and the equiparameter line in the e+-inverted space  and b is the corresponding angle in the e -inverted space. other heuristic tests for inclusion/exclusion are discussed in ′1. 
1. parity  xor  revisited. 
　　　figure 1 shows the two inverted spaces for the 1-bit parity problem. the equiparameter lines in each of the inverted spaces are dashed; the boundaries of a  1 hypercube are dotted. notice that the primitive features score low evaluation on both the proposed heuristics. first  both the features  in both inverted spaces  are oriented perpendicular to the equiparameter line  indicating a maximum deviation of 1. second  there is no 1 such that the corresponding 1-hypercube includes a feature in one inverted space  and excludes the same feature in the other. thus  the framework captures both the high variance of the sample within a class  and the linear inseparability between classes. in ′1  we show that the heuristic measures suggested above are helpful in discovering good features for this problem. 
1. realizing the inverted space framework. 
applying this framework naively can yield complex computations on large matrices. the complexity results from simultaneous consideration of a large number of examples. to counter this problem  the heuristic approximation methods of ′1 consider only a subset of training samples that still retains the separability traits of the entire set. to address another problem  the techniques suggested in ′1 are amenable to incremental learning so that heuristic information is accumulated by examining a few examples at a time. 
1. using low-dimensionality inverted spaces. 
　　　michalski has suggested several approaches to selection of representative samples for a two-class discrimination problem  michalski  1 . his esel system uses three criteria  as shown in figure 1 : 
　　　the cluster centroid  cc  method. each class is represented by its centroid and centroids of clusters within the class. in addition  one keeps additional points that are more than twice the cluster standard deviation away from the centroid of every cluster within the class. sklansky et al. 
use a similar approach 	 faroutan  	1 . 	rendell's 

mehra  ren deli and wah 1 

probabilistic learning system  rendell  1  also constructs such representations. 
     the near miss  nm  method. choose from a class those points that are nearest to the opposite class. similar techniques have been explored by winston  1  and macgregor  1 . the idea is that any separating surface for the two subsets consisting only of near-misses would still separate the two classes. 
     the outstanding representative  or  method. choose the subset of examples from each class whose convex hull is the same as that for the entire class. this means choosing the extremal points of each class. any surface that separates the extremal points must  of necessity  separate the two classes. lambert  lambert  1  has proposed similar techniques. 
     several researchers have recently noticed improved performance in learning systems that pay special attention to boundary cases  ahmad  1 kohonen  1 . on the theoretical side  cover  1  has suggested that for twoclass discrimination problems using surfaces with d degrees of freedom  on the average  1d samples can capture the information of a possibly infinite training set. the implication of cover's result  and the existence of mechanisms for finding boundary patterns  make the inverted-space framework a viable technique. 
1. using approximate tests of separability 
     in our proof of the theorem  we showed a relationship between the sine of the angle between the feature vector and the equiparameter line  and the standard deviation of the value distribution for a feature. considering that the only information needed to calculate the latter is the sum of sample values  the sum of their squares  and the number of values seen  suggests incrementally maintaining the information required to calculate the sine of the angle. 
     the following tests implement a cheaper heuristic approach to testing feature points for inclusion/exclusion in a 1-hypercubes: 
　　 denote the length of the feature vector for / in the inverted space for e+  

these tests do not cover all the cases. however  the advantage over exact tests for inclusion/exclusion is that only two items of information per feature need to be maintained  and those can be computed incrementally. the heuristic provides a positive answer to a query about inclusion if the feature vector lies within the largest hypersphere that can be inscribed in the hypercube in question  and a negative answer if it lies outside the smallest hypersphere that can circumscribe the hypercube. we do not detail these methods here  primarily because there are several ways of approximating the real measures of goodness  and these are only two of them. 

1. 	constructive 	induction 	using 	inverted spaces. 
a constructive induction problem is defined by a set of feature-construction operators to be applied to the primitive features  or to combinations thereof. table 1 shows the geometric interpretation of some commonly used constructive induction operators in terms of the inverted space framework. given a difficult discrimination learning problem  and the description of features in the form of inverted spaces for both the classes  the goal of constructive induction is to create features that  i  lie close to the equiparameter line in both the inverted spaces  and  ii  satisfy the exclusion/inclusion test for some 1-hypercube. in the following  we first examine how one might apply the inverted-space framework to construct new features. 
     knowing which transformations the various construction operators achieve lets one select the particular operator to apply. additional heuristics  such as using boolean operators on boolean features  can provide additional bias for selection. the constructive induction algorithm employed is a search in the space of constructed attributes. the inverted space framework is useful in both the generation and the testing phases of this algorithm. 
1. feature generation 
     this employs several heuristics based on the measures of feature goodness discussed in ′1. one heuristic uses a linear combinations of d=max / m  features  where l- e+   and m= e~ . a  d-l -dimensional surface passing through these features is made to intersect the equiparameter line in one of the spaces. the intersection yields the exact values of weights to be applied in the linear combination. the new feature is evaluated according to the measures of goodness. 
     yet another heuristic limits the application of thresholding only to features close to the equiparameter line. constructors such as and and not are applied only to 
boolean features. sometimes  several features map into the same point in one of the inverted spaces  but to different points in the other. in such cases  one can apply operators such as and  or  difference  and equality  in order to construct new features. 
     in general  this phase employs substantial domainknowledge. operators specific to an application domain can 

still be interpreted in the inverted spaces  and this 
1 	machine learning 

knowledge of transformations can be used  abductively  for feature generation. 
1. feature evaluation 
     features are evaluated on the basis of the goodness criteria introduced in ′1. different constructive induction algorithms employ different search mechanisms  but all can use this measure of goodness to serve as a heuristic estimate. usually  the cheaper version of the inclusion/exclusion test will suffice. the more expensive test involving all the dimensions of the inverted space may be limited only to promising features  those that lie close to the equiparamcter line . other techniques  such as dynamic bias management  may be employed to focus the search. 
     in the following  we illustrate the effect of such constructions on the xor problem. recall from ′1 that this problem requires constructive induction. 
1. parity revisited 
　　　figure 1 shows the inverted spaces for the parity problem. it also shows the map of a new feature  f1af1. this new feature is recommended by the feature-generation heuristic for the and operator. the linear combination of fx and f1 yields   upon intersection with the equiparamcter line in the inverted space. still  no single feature scores highly on the evaluation criteria. further construction suggests taking the difference of the two constructed features  thus yielding the final constructed feature  which separates the two classes. 
1. constructive induction as merging peaks 
　　　it has been suggested that hard problems in conceptlearning require the formation of disjuncts. this is supported by recent results in computational learning theory  kearns  1 . rendell  1  has suggested that membership functions of hard concepts have multiple peaks in the primitive feature space. simple surfaces cannot be used to discriminate between classes whose membership functions have multiple peaks. rendell  1  advocates transforming the feature spaces so that the various peaks merge. in the transformed space  the membership function is smooth  and simple surfaces can discriminate. 


representative examples from the concept function in feature space  left  are used to construct a low-dimensionality inverted space  right . features that lie on the equiparameter line  shown dashed  in the inverted space can be seen as merging peaks  provided these features are constructed by applying continuous  compact transformations to the original features. 
     let f be the set of primitive features of a disjunctive problem  and let  be the centroids of the peaks of the membership function.  see figure 1  using the reduced-dimension inverted spaces discussed in ′1  one can construct the p-inverted space  and map into it the features of f. if a feature receives a high evaluation in the p-inverted space  it must lie close to the equiparameter line. 
in other words  it takes on nearly identical values for the examples corresponding to peaks in the membership function. now  if such a feature is constructed using a nonlinear  continuous mapping  it can be shown to merge the peaks of the membership function in the feature space. similarly  simultaneous construction in the inverted space for negative examples  and the application of the hypercube-inclusion test  can result in the construction of features that transform complex learning problems into simple ones. 
1. feature construction in inverted spaces. 
     the inverted space framework is particularly suitable for constructive induction because the inverted spaces remain fixed during construction. this is in sharp contrast to the traditional feature space representation used to study pattern recognition operations  where new features contribute new dimensions. the two major ideas that distinguish our work from that of watanabe  1  are the concept of the equiparameter line  and the angle that a feature vector makes with this line. in our recent research  we have started exploring the inverted space representation of value-coded features  aka. overlapping localized receptive fields  coarse-coded distributed representations . the concepts of angle and distance in the feature space relate  respectively  to generalization and discrimination - two conflicting goals in the design of classifiers. 
     besides the obvious application of constructed features in classification  one of the authors has also explored the application of constructed variables in numeric optimization problems  lowne and wah  1 . there is no reason to believe that inverted space analysis should be restricted to classification problems. in the future  we plan to explore inverted space representations of variables in optimization problems. 
	mehra  rendell and wah 	1 

1. conclusions and future work. 
we have presented a new framework for representing classes  examples  and features. it is applied to the constructive induction problem. principles for evaluating newlyconstructed features are developed  and are shown to guide the construction process. the exact measure for compactness is shown to have a direct relationship with a statistical measure of the spread of a distribution. several inexpensive heuristics for feature evaluation arise from relaxation of measurement accuracy. 
　　　our analysis covers a variety of algorithms and heuristics for construction and evaluation of features. in particular  it synthesizes techniques for selection of examples with those for selection and construction of features. by posing the dual of the classification learning problem  it suggests new techniques similar to the primal-dual algorithm  papadimitriou  1  for linear programming. for example  one might use geometric techniques such as hullfinding and proximity analysis  preparata  1  in order to discover representative examples in the feature space  while constructing good features in the inverted space of these examples  repeating the process on the reduced feature space. the inverted-space representation  therefore  provides a rich basis for such integration of geometric and statistical techniques. 
　　　the framework is important as an analytical tool. it allows complex nonlinear operations on features. we have demonstrated its ability to represent a variety of construction and classification operations. in future  we would like to extend the analysis to include other problems in classification. one such problem is redundancy-elimination. inverted space permits the use of geometric tests  such as collinearity  and coplanarity  for detection of dependencies between features. we plan to study the statistical properties of geometric operations  and the geometric interpretation of statistical operations  in order to arrive at a synthesis of techniques for problems like dimensionality reduction  clustering  and multi-class discrimination  duda  1 . 
acknowledgements. 
darrell hougen suggested the proof technique for interpreting the method of ′1 statistically. the authors wish to thank mark gooley  subutai ahmad  and the anonymous referees for their helpful comments. 
