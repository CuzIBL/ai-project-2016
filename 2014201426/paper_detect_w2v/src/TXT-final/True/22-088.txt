 
novice computer users have many incorrect beliefs about the commands on their system. this paper considers the problem of providing explanatory responses that correct these mistaken user beliefs. current approaches correct mistaken beliefs by trying to infer the reasons why the user holds them. in contrast  our advisor corrects these beliefs simply by explaining why he doesn't share them. this allows the advisor to provide reasonable advice even when no robust user model is available. our advisor constructs this explanation from scratch  using a set of domain-independent strategies for justifying plan-oriented beliefs. this differs from existing systems  such as explanationbased story understanders  that provide explanations by modifying existing explanations and fail to address the underlying problem of forming the initial explanation. this approach gives our advisor the ability to explain novel misconceptions. 
1 	the problem 
intelligent user interfaces and advisory systems must be able to provide explanations for mistaken user beliefs. consider this dialog between a novice unix user and a more experienced unix advisor. 
user: i accidentally removed a file. how can i restore it now  
advisor: you can't. it takes at least two days to restore a file. 
user: but i need the file today. 
advisor: you need to take care not to remove files you need. why don't you use ttrm - i   so that you're asked before any files are removed  
user: it takes too long to answer the questions. 
advisor: it takes a long time to restore a file. but as a faster alternative to ''rm - i     you could move unwanted files into ''/tmp . 
user: but that's going to fill up ' ' / t m p   -
advisor: tt/tmp  is automatically cleaned up every few hours so filling it up isn't a problem. 
1 	cognitive models 
this user has a set of mistaken beliefs that need to be corrected. these are beliefs that: 
1. there's a plan that can restore files immediately. 
1. the user's plan for removing files is appropriate. 
1. using ''rm - i   isn't an appropriate plan for removing files because it takes too long to use. 
1. moving files to ''/tmp  isn't an appropriate plan for removing files because it fills up ''/tmp . 
each advisor response explains why one of these user beliefs is mistaken. the advisor explains that: 
1. there is no way to recover files today because the plan for recovering files takes at least two days. 
1. using ''rm - i   is an appropriate way to remove a file because it prevents accidental file removal. 
1. the user's plan for removing files is inappropriate because it takes just as long as  rm - i   when the time to recover a file is taken into account. 
1. moving files to ''/tmp  is an appropriate plan because there's a mechanism that automatically ensures that ''/tmp  won't fill up. 
　how can an automated advisor produce these explanations  out of the many beliefs the advisor possesses about unix  how does this advisor somehow manage to select a small set of beliefs that constitutes a reasonable explanatory response  this paper addresses these questions under the assumption that the advisor is initially presented with a representation for the user's mistaken beliefs. this assumption implies that the user's plans and goals have been recognized  a task addressed by many current systems  carberry  1  kautz and allen  1  wilensky  1 . 
1 	producing explanatory responses 
the advisor can take one of two general approaches to responding to a user misconception. 
　the first is to try to understand why the user holds that incorrect belief  and to then address those underlying misconceptions. why  for example  does this user believe that there's a way to quickly recover a removed file  one explanation is that the user usually works with pcs and is used to having immediately accessible backups on floppy disks. another is that the user's last system 

had immediately accessible online backups. the advisor's task is to choose the most appropriate from all of these different explanations. the problem with this approach is that the advisor needs information about the user other than the user's explicitly stated beliefs. he has to know that the user usually works with pcs  or that most other users with this misconception previously worked with systems that had online backups. often  however  this extra information isn't available  such as when it's the first time the advisor has encountered a particular misconception and a particular user. 
　the second approach is useful when the advisor has little information about the user. rather than trying to understand the underlying causes of the user's mistake  the advisor tries to explain why he disagrees with the user's belief. in essence  the advisor simply tells the user   here's why i think you're wrong.  and that's exactly what the advisor in our initial example repeatedly does. the first belief the advisor corrects is the user's belief that there's a plan for rapidly recovering a file. to produce an explanation  the advisor tries to justify his belief that there is no plan for doing this task. the 
justification he finds is his belief that the plan for recovering files has some effect that causes it take at least two days. similarly  the second belief the advisor corrects is the user's belief that he's using an appropriate plan for removing a file. the advisor tries to justify his belief that the user's plan is inappropriate. and the justification he finds is his belief that there's another plan  using ''rm - i     that doesn't allow accidental file removal. 
　the remainder of this paper assumes that the advisor is taking this latter approach-that he's trying to correct mistaken user beliefs by explaining why he disagrees with them. we provide a model of how the advisor can formulate appropriate justifications for his contradictory beliefs. we concentrate on user beliefs involving plan applicability conditions  enablements  and effects. 
1 	explanation-based belief justification 
our approach for corning up with belief justifications is closely related to recent work in explanation-based story understanding  schank  1 . these systems try to explain why a particular event occurred. why  for example  might a racehorse have died within a week of winning a major race  leake  1  kass  1   their solution is to take an explanation for a similar  already-explained event  modify it to reflect the current event  and confirm that the explanation holds. in racehorse example  the system begins with its explanation for why the runner jim fixx died young: his jogging aggravated a congenital heart defect and led to a heart attack. the system modifies this explanation to use racing rather than jogging  and tries to confirm its new explanation that the horse had a congenital heart defect and that it died of a heart attack. 
　how does the system confirm the explanation  essentially  it does one of two things. one is to examine whether pieces of the explanation match or conflict with known facts. does it know that the horse had a heart attack  or that the horse didn't have a congenital heart defect  the other is to test for predisposing circumstances that make this explanation more or less likely. one predisposing circumstance for a heart attack is that the victim is high-strung and easily excitable. was this horse known to be high strung  the system  however  stops short of doing more general inferencing. it doesn't  for example  try to find evidence for whether the horse was easily excitable. 
1 	our approach to justifying advisor 
beliefs 
our advisor's task is similar. these systems are trying to find and confirm an explanation for why an event occurred; our advisor is trying to find and confirm an explanation for why a belief is held. our approach  however  differs in several significant ways. first  our advisor does not assume an explanation for a similar belief. instead  the advisor starts from scratch  using a set of abstract belief-justification strategies to construct possible explanations. the feature of this approach is that it works without requiring an existing explanation. 
　and second  our advisor expends more effort confirming this explanation. the advisor doesn't accept an explanation for why a belief is held merely because nothing contradicts it  or because there are certain predisposing circumstances instead  each piece of an explanation not directly confirmable by facts from memory must itself be explained. the advisor's explanation for why the user's file removal plan is inappropriate is that it doesn't prevent accidental file removal  even though  rm - i     another plan for file removal  does. to confirm this explanation the advisor is forced to explain how ''rm - i   prevents accidental file removal  which happens to be because it asks the user before removing the file. carefully confirming explanations is especially important when giving advice. unconfirmed explanations are more likely to be wrong and mislead the user. and the additional information needed to confirm the explanation  such as ''rm - i   asking a question  often forms useful advice. 
1 	justification patterns 
since we don't start with an explanation  how does the advisor explain why he holds a particular belief  we rely on a set of domain-independent strategies  justification patterns  jp s  that represent potential explanations for why a particular belief is held. each type of plan-oriented belief  that no plan is applicable to a goal  that a particular plan is applicable to a goal  that a particular state is caused by a particular plan  and so on  is associated with a set of jps. a jp is an abstract pattern of planning relationships. given a specific belief  the advisor classifies the belief  and then tries to instantiate and confirm one of the jps associated with that class of belief. this instantiated and confirmed jp constitutes the contents of the advisor's response to the user. 
　this approach raises several questions. what justification patterns are needed for plan-oriented beliefs  how do we represent them  and how are they confirmed  
	quilici 	1 
1 	representing advisor knowledge 
the advisor must be able to represent two types of knowledge. the first involves the specific plan-oriented beliefs of the user and the advisor  such as those about file removal and recovery. what are some different ways to remove a file  what are the enablements of removing a file  what are the effects of using ttrm -i   and so on. the second is general knowledge about how to justify plan-oriented beliefs: the advisor's justification 
patterns. 
1 	representing plan-oriented beliefs 
we represent beliefs in the same way as do most other systems that deal with the possibly contradictory beliefs of multiple dialog participants  flowers et a/.  1  pollack  1a . there's a relationship  beliej {a  r   that indicates that an actor a  either the user or the advisor  believes a planning relationship r holds. 
　our representation for planning relationships combines elements used to represent plan effects and enablements found in other systems dealing with plan-oriented misconceptions  pollack  1b   and elements needed to represent the goals and intentions of the user  dyer  1 . we use the following set of primitive planning relationships. here  a denotes an actor  s denotes a state 
 a description of properties of objects   p denotes a plan  a sequence of operators that  when executed  effects a state change   g denotes an actor's goal  a state an actor wants to achieve   and e denotes an event  an actor's execution of a particular plan . 
a execute p 	a carries out steps in p 
	e causes s 	e has 1 as one of its effects 
e leadsto e' 	e has e' as one of its effects 
s enables r 	s is needed for relation r to hold 
e applies g 	e should be executed to achieve g 
r precludes r' 	r' can't hold if r holds 
a has goal s 	state s is a goal of actor a 
r isa r' 	r falls in the class r' 
never r 	no instantiation of r holds 
　these relationships provide a way to represent beliefs about a plan having a particular state as an enablement or effect  a plan being appropriate for a particular situation  and so on. each of these relationships also has a corresponding negated relationship: that a plan doesn't cause a particular state  that a particular state is not an enablement to a plan  and so on. these sorts of beliefs are frequently found in advice-seeking dialogs. our user  for example  believes that using ''rm - i   is not a correct or normal plan for removing a file. and the advisor believes that there isn't some plan for recovering a 
　file immediately. 
　a more complete description of the semantics of these relationships and a set of examples of the dialogs they represent can be found in  quilici  1 . here  however  we illustrate their semantics by using them to represent the user and advisor beliefs that are explicitly stated in our example dialog. 
　the user begins by providing a set of his beliefs: the user did execute a plan that causes a file to be removed  the user can execute some plan that applies to the goal of 
1 	cognitive models 
recovering the file today  and the user hasgoal to recover the file today. the advisor's response points out several corresponding advisor beliefs: the plan that applies to the goal of recovering a file has an effect that causes it to take two days before the file reappears  an effect that precludes recovering the file today. 
　the user's next response again provides a single user belief: the user hasgoal of accessing the file today. the advisor responds with another collection of advisor beliefs: the user's plan causes the user to have the unachievable goal of recovering the file today  u rm - i   applies to the goal of removing a file  and  rm - i   leadsto a question being asked. 
　the user follows up with his belief that answering questions causes him to use an excessive amount of time. the advisor responds with his belief that restoring files causes the same effect  and that moving files to ''/tmp  applies to the goal of removing a file. 
　in the final exchange the user points out his belief that moving files to  /tmp  causes  /tmp  to fill up  and the advisor replies with his belief that the system frequently executes an action that causes ''/tmp  to be cleaned up. 
1 	representing justification patterns 
the other knowledge the advisor needs is a set of justification patterns. there is a set of jps for each of the planning relationships discussed in the previous section. each jp is represented in terms of these same planning relationships. for example  one jp provides a potential justification for a belief that an actor a's executing a 
particular plan p doesn't apply to a particular goal g: 
p l a n causes unachievable g o a l 
 1  a executing p causes a to have goal gl 
 1  there's no plan that applies to this goal g1 
 1  a executing some other plan p' causes g 
 1  a executing p' doesn't cause a to have g1 
this jp suggests that an actor shouldn't use a particular plan to achieve a goal if it gives rise to another  unachievable  goal. 
　the advisor can use this jp to justify his belief that the user's plan for removing files is inappropriate. to do so  the advisor instantiates it with a as the user  p as the user's plan for removing files  and g as the user's goal of removing a file. 
 1  the user executing his plan for removing a file causes him to have goal gl. 
 1  there's no plan that applies to this goal gl. 
 1  the user executing some other plan p'causes the file to he removed. 
 1  the user executing p'doesn't cause him to have gl. 
the advisor then tries to confirm this partially instantiated j p. 
1 	confirming justification patterns 
flow does the advisor confirm one of these jps  the advisor repeatedly selects one of its belief to confirm  tries to confirm it  and instantiates the jp with any new information gleaned from the confirmation process. to confirm a particular belief  the advisor searches memory for a matching belief  or specific instances of it  and  
if that fails  then the advisor tries to justify it using additional jps. this process stops when the jp is successfully instantiated and confirmed  or when memory search fails to yield new confirming beliefs. 
   for the above jp  the advisor first tries to confirm that the user has some goal g1 memory search yields the confirming belief that the user has a goal to recover a file today  a goal stated in the user's first utterance  and the advisor instantiates the jp with this information. 
 1  the user executing his plan for removing a file causes him to have a goal of recovering the file today. 
 1  there's no plan that applies to this goal of recovering the file today. 
 1  the user executing some other plan p' causes the file to be removed. 
 1  the user executing p'doesn't cause him to have a goal of recovering a file today. 
the advisor then repeats the process for each of these subsequent beliefs. the result of the confirmation process is a fully instantiated and confirmed jp. 
   here we've assumed that confirming beliefs are readily found. but what if the advisor didn't already have a belief in memory that ''rm - i   doesn't cause the user to have a goal of recovering a file  in that case  he would have had to try to justify holding this belief. fortunately  he can use this same method. this belief falls into the class that an actor a's executing a plan p doesn't cause a particular effect 1. one jp for this class suggests that the plan leads to an action that has another effect that precludes this effect. 
	p l a n 	i n d i r e c t l y 	p r e c l u d e s 	e f f e c t 
 1  a executing the plan pleads to an event e 
 1  e causes s' 
 1  state s' precludes s 
   the advisor instantiates this jp with p as   r m - i   and s as the user having a goal of removing the file. the advisor then tries to confirm these beliefs  leading to the final justification: 
 1  the user executing the plan of using  rm - i   leads to the system asking the user before removing the file. 
 1  asking questions causes the file to still exist. 
 1  the file still existing precludes the user having the goal of recovering the file. 
the advisor then includes these beliefs in his response. 
1 	s o m e o t h e r j u s t i f i c a t i o n p a t t e r n s 
justification patterns capture knowledge about belief justifications that is independent of any particular plan. t h a t means that each jp can be used to form justifications for many different beliefs. in our introductory dialog  for example  a single jp is used by both the user and the advisor to justify their differing beliefs about which plan the user should execute to achieve the goal of removing a file. this jp suggests that a plan doesn't apply to a goal if it leads to an action that thwarts a different goal  while another plan for that goal doesn't lead to that action. 
	p l a n 	i n d i r e c t l y 	t h w a r t s 	g o a l 
 1  a executing pleads to executing po 
 1  executing po causes state s 
 1  a hasgoal s' 
 1  s precludes s' 
 1  a execute p' causes g 
 1  a execute p' doesn't lead to po 
the user uses this jp to justify his belief that using 
  r m - i   doesn't apply to the goal of removing a file. the user instantiates it as: 
 1  the user executing  rm - i   leads to his being asked a question before removing the file. 
 1  his being asked questions causes use extra time. 
 1  the user has a goal to preserve available time. 
 1  using extra time precludes preserving available time. 
 1  the user's file removal plan causes the file's removal. 
 1  the user executing his removal plan doesn't lead to his being asked questions. 
in other words   rm - i   violates a user goal by leading to a time-consuming question that the user's file removal plan doesn't lead to. the advisor uses this same jp to 
justify his belief that the user's file removal plan doesn't apply to goal of removing a file. his response instantiates this jp as: 
 1  the user executing ''rrn -i'' leads to a file recovery action. 
 1  executing the file recovery action causes a long wait. 
 1  the user has a goal to preserve available time. 
 1  a long wait precludes preserving available time. 
 1  the user executing  rm - i   applies to removing fde. 
 1  the user executing ''rm - i   doesn't lead to a fde recovery action. 
that is  the user's file removal plan violates a user goal by leading to a time-consuming action to recover the file that   r m - i   doesn't lead to. 
   each type of belief has a collection of justification patterns. our example contains several other jps for the belief of a plan not applying to a goal. one jp the advisor uses suggests that a plan doesn't apply to a goal if one of its effects violates an enablement of a plan for another goal. 
	p l a n 	d i s a b l e s 	p l a n 	f o r o t h e r g o a l 
 1  a executing plan pf causes g 
 1  a executing p'causes s 
 1  a hasgoal g' 
 1  s enables po causes 1  
 1  a execute p causes s' 
 1  s'precludes 1 
the advisor instantiates it to further justify the belief that the user is using the wrong plan for removing files: 
 1  the user executing the plan of moving files to a /tmp  causes the file to be removed. 
 1  the user executing the plan of moving files to  /tmp  causes the file to exist in u /tmp . 
 1  the user hasgoal of recover file today. 
 1  the file existing in tt/trnp  enables recovering the file by executing the plan of moving files from ''/tmp . 
 1  the user execute removal plan causes file not exist. 
 1  the file not existing precludes it existing in ''/tmp . 
	quilici 	1 

the justification is that the plan of moving files to 
 /tmp  achieves an enablement of the recovery plan of restoring the files from ''/tmp   namely  that the file exists in ''/tmp  ; the user's removal plan keeps this enablement from being achieved. 
   one final jp for this belief type appears in the user's last response. the user uses it to support his belief that moving files to  /tmp  doesn't apply to the goal of removing a file. this jp says that a plan isn't applicable to a goal if it has an effect that thwarts another goal and another plan for the goal doesn't have this effect. 
	p l a n 	t h w a r t s g o a l 
 1  a execute p' causes g 
 1  a execute p causes s 
 1  s precludes s' 
 1  a hasgoal s' 
 1  a execute p' doesn't cause s 
the user instantiates it as: 
 1  the user executing his file removal plan causes the file to be removed. 
 1  the user executing move to ''/tmp  causes ''/tmp  to fill up. 
 1   /tmp  filling up precludes having space in ''/tmp . 
 1  the user has a goal of having space in  /trnp . 
 1  the user executing his file removal plan doesn't cause  /tmp  filling up. 
that is  moving files to ''/tmp  causes it to fill up  violating the user goal of having space in  /tmp   an effect the user's plan doesn't have. 
1 	implementation status 
the model discussed in this paper has been implemented in a prolog program  the correction machine . the program's domain of expertise is the basic unix commands needed to remove  recover  and rename files. it also possesses approximately 1 different justification patterns spanning the planning relationships discussed in this paper. the input to the program is a representation for a user belief involving plan applicability conditions  enablements  or effects. the program's output is the set of advisor beliefs to present to the user  along with a description of the particular justification patterns needed to produce the explanation. it can formulate explanatory responses corresponding to several different dialogs  including our initial example. 
　currently  we're trying to answer several questions about our model. first  how well do the justification patterns described here account for responses to misconceptions in domains other than those of novice computer users  to test their domain-independence  we're extending the program to give advice about simple day-to-day planning. second  how sufficient is our set of justification patterns for providing unix advice  we're now studying many different user/advisor dialogs  searching for the presence of other useful justification patterns. 
　we're also working on more quickly finding appropriate justifications  as this process is potentially timeconsuming  especially when confirming a single justification pattern may involve trying to confirm many other 
1 	cognitive models 
jps. there are two ways to speed up the process; both of which lead to unanswered questions. one is to save instantiated justification patterns for later use. but how are these specific jps organized and retrieved  the other is to pick jps that are likely to be confirmed successfully. but how can the most appropriate jp be selected  
   finally  we're trying to extend the model toward being a more complete dialog participant  a task that raises several other important questions. first  can justification patterns be used to understand belief justifications  a dialog participant must not only construct his own belief justifications but also understand those of others. ideally  the same knowledge should be used by both processes. but our model is solely concerned with constructing belief justifications  not comprehending them. and second  how is the particular belief to justify selected  a dialog participant quickly faces a large collection of beliefs from which he must choose a particular belief to justify. our model  however  simply assumes that this 
belief has already been chosen. 
1 	comparison with related work 
there are two types of related systems: those that provide advice to novice unix users  and those that correct misconceptions. existing unix advisors such as uc  wilensky et a/.  1  and sc  hecking et a/.  1  don't attempt to explain mistaken beliefs. instead  they assume the user's problem is incomplete knowledge  and focus on filling in the gaps indicated by questions such as ''iiow do i remove a file   or  what does 'rm - i ' do  . 
　one system that does try to explain user misconceptions is our own earlier effort  aqua  quilici  1  quilici et a/.  1  quilici et a/.  1 . this system corrects plan-oriented misconceptions using strategies similar to our justification patterns. a problem with aqua is that its confirmation process is limited to memory search; it makes no attempt to justify holding the beliefs it's trying to confirm. this severe limitation arises because aqua has strategies only for only why it doesn't believe something  and not for why it does. this means that aqua has no way to support a belief that a plan does apply to a goal  or that a plan does have a particular effect  beliefs that may have to be justified for a strategy to be confirmed. 
　romper  mccoy  1  applies an approach similar to aqua's to correct a different class of misconceptions: mistaken user beliefs involving misclassifications or misattributions involving objects. romper also has a set of strategies for justifying its beliefs to the user. but because romper deals only with two different kinds of mistaken beliefs and has only a few strategies  it suffers from the same drawbacks as aqua. it's forced to abandon a strategy if it doesn't find confirming evidence when it searches memory. 
the final system  spirit  pollack  1a  pollack  
1b   detects the use of inappropriate plans of users of a computer mail program  along with the mistaken user beliefs underlying those plans. spirit's task is similar to ours  but it takes a completely different approach. it uses rules to try to infer what the user's beliefs actually are  rather than justifying the advisor's own beliefs. 

one such rule states that an advisor who believes that an act has a particular result under certain conditions can infer that the user has a similar belief missing one of the required conditions. because it takes this approach  spirit does not specify what information should be included in a cooperative response  something that falls naturally out of our model. 
finally  there's a large collection of tutoring systems 
 sleeman and brown  1  that attempt to correct and explain user misconceptions. generally  these systems locate mistaken beliefs in a database of domain-specific error-explanation pairs and provide the associated explanation. this approach has several drawbacks. because the explanations are domain-specific  having the tutor provide explanations for mistakes in a new domain involves finding a new set of error-explanation pairs. and because these systems simply retrieve explanations  they can handle only those misconceptions they know about in advance. 
1 	conclusions 
this paper makes two contributions. first  we suggest a novel approach to formulating responses that correct mistaken user beliefs. rather than trying to infer what led the user astray  our advisor simply says why the user is wrong. this allows the advisor to provide helpful responses  even when no robust user model is available. and second  we propose a mechanism for constructing explanations from scratch. our advisor explains why he holds a particular belief by using a set of domainindependent strategies for justifying plan-oriented beliefs. these strategies constrain the memory search for appropriate supporting beliefs  limiting it to those beliefs that are likely to be useful. and they provide a way to justify beliefs that memory search fails to locate. this ability to create novel explanations out of domainindependent strategies can be used to form those initial explanations needed by systems that work by modifying existing explanations. 
acknowledgments 
special thanks go to edward hoenkamp for his many helpful hints on how to improve this paper  as well as to michael dyer and the anonymous referees for their comments and suggestions. 
