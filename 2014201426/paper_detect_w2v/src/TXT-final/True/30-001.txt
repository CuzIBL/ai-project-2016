 
a very promising approach for integrating topdown and bottom-up proof search is the use of bottom-up generated lemmas in top-down provers. when generating lemmas  however  the currently used lemma generation procedures suffer from the well-known problems of forward reasoning methods  e.g.  the proof goal is ignored. in order to overcome these problems we propose two relevancy-based lemma generation methods for top-down provers. the first approach employs a bottom-up level saturation procedure controlled by top-down generated patterns which represent promising subgoals. the second approach uses evolutionary search and provides a self-adaptive control of lemma generation and goal decomposition. 
1 	introduction 
top-down and bottom-up approaches for automated theorem proving in first-order logic each have specific advantages and disadvantages. top-down approaches  like model elimination  me   or the connection tableau calculus  ctc    are goal oriented but suffer from long proof lengths and the lack of an effective redundancy control. bottom-up approaches  like superposition   provide more simplification power but lack in their purest form any kind of goal orientation. thus  an integration of these two paradigms is desirable. 
　two approaches have been in the focus of interest in the last years. the methods from  1; 1  are bottomup theorem proving approaches. there  the bottom-up inferences are restricted to the use of relevant clauses which are detected by additional top-down computations. thus  goal orientation is combined with redundancy control. in other approaches top-down provers are assisted by lemmas   1; 1; 1  . also these methods combine goal orientation with redundancy control provided by the lemmas. the use of additional clauses can also reduce the proof length which may lead to large search reductions. but this has to be paid for by an increase of the branching rate of the search space. thus  mechanisms for selecting relevant clauses are needed. 
1 	automated reasoning 　our integration approach is based on  1; 1  where lemmas have been used in the ctc. there  in order to refute a set of input clauses with the ctc  in a preprocessing phase the input clauses are augmented by bottom-up generated clauses  lemmas . then  the prover tries to refute this augmented clause set. lemmas are obtained by creating a pool of possible lemmas which are able to shorten the proof length  generation step . then  in the selection step some possibly relevant lemmas are selected which are then used for refuting the given proof task. in  it is concentrated on the selection of lemmas and rather simple lemmas have successfully been used. in order to speed-up the proof search in a more effective manner harder lemmas are needed. the generation approaches as used in  1; 1   however  have severe difficulties in generating harder lemmas in a controlled way. they suffer from the earlier mentioned problems of saturation based proving. 
　thus  we focus now on the aspect of the generation of possible lemmas and propose two new approaches for a controlled generation of lemmas for top-down provers based on the combination of top-down and bottom-up search. the first technique generates lemmas in a systematic way based on some kind of level saturation  as in  1; 1  . the lemma generation  however  is combined with a decomposition of generalized proof goals which represent possibly solvable subgoals in a compact way. these generalized goals are used for detecting possibly irrelevant lemmas. the other approach uses genetic programming . in the evolution process simultaneously top-down and bottom-up inferences are performed which are controlled by a fitness function which measures the similarity between open subgoals and derived lemmas. thus  lemma generation and goal decomposition are focused on promising clauses in a self-adaptive way. we evaluate the usefulness of the new methods at hand of experiments performed with the prover setheo . 
1 	connection tableau calculus 
in order to refute a set c of clauses the ctc works on connected  clause  tableaux for c  see  . the inference rules are start  extension  and reduction. the start rule allows a so-called tableau expansion that can only be applied to a trivial tableau  i.e.  one consist* 

ing of only one node. an expansion step means selecting a variant of a clause from c and attaching for each of its literals a node  labeled with the respective literal  to a leaf node of an open branch  i.e.  a branch that does not contain two complementary literals. the start rule can be restricted to some start clauses  e.g.  the set of negative clauses may be used  see  . tableau reduction closes a branch by unifying h subgoal $  the literal at the leaf of the open branch  with the complement of a literal   denoted by  on the same branch  and applying the substitution to the whole tableau. for defining extension we need the notion of a contrapositive. is a clause 
then each sequence is a contrapositive of c with head and tail 
extension with a contra-
positive of a clause from c is performed by selecting a subgoal 1  unifying s and with instantiating the tableau with attaching   below and closing the branch which ends with 
	we say 	if and only if tableau v can be de-
rived from t by applying a start rule  if t is the trivial tableau  or an extension/reduction rule to a subgoal in t. in order to refute an inconsistent clause set c  a search tree has to be examined in a fair way  each tree node must finally be visited  until a closed tableau occurs. a search tree  defined by a set of clauses c is a 
tree  whose root is labeled with the trivial tableau. each node in labeled with tableau t has as immediate successors the maximal set of nodes where 
is labeled with ti and 
　in order to enumerate a search tree implicit enumeration procedures are normally in use that apply iterative deepening search with backtracking. iteratively increas-
ing finite initial parts of the search tree are explored in depth-first search  cp.  . for instance  for clause sets  and  denotes the finite initial part of 
where all tableaux are obtained by using only clauses from s for the start expansion  only the clauses from for extensions  and where the tree depth of each tableau does not exceed a value of n n.  the root node has depth 1  its successor nodes depth 1  and so on . 
1 	goal decomposition and saturation 
we provide some basic notions regarding the decomposing and saturating capabilities of the ctc which will be used in the following. we start by demonstrating how to extract query and lemma clauses from a given connection tableau. at first we introduce a method for extracting valid clauses from a connection tableau. 
definition 1  subgoal clause  let c be a set of clauses. let t be a connection tableau for c. let be the subgoals of t then we call 
the subgool clause of t. 
　the subgoal clause of a connection tableau t for a clause set c is a logical consequence of c  see e.g.   . subgoal clauses may be considered to be top-down generated queries or bottom-up generated lemmas depending on the form of the tableaux they are derived from. first  we consider the analytic character of subgoal clauses and define query clauses as follows  see also  1  . 
definition 1  query tableau  query clause  let 
c be a set of clauses. let t be a connection tableau for 
c. let s   be a set of start clauses. let 1 be the clause below the unlabeled root of t. if 1 is an instance of a clause from s we call t a query tableau  w.r.t. s  and the subgoal clause of t a query clause  w.r.t. 1 . 
　essentially ctc based proof procedures implicitly enumerate query clauses w.r.t. the chosen start clauses until the empty query clause is derived. lem-
mas introduce a bottom-up element into the top-down oriented ctc. we employ the following definition of a lemma which extends and generalizes the notions of lemmas used in  1; 1; 1 . 
definition 1  lemma tableau  lemma clause  let c be a clause set. let t be a connection tableau for c. let be the subgoal clause of 
t. let 	be the set of subgoals which are immediate successors of the root. if 	we call t a lemma tableau. then  let si  1 	be the element of which is left-most in t. we call the contrapositive 
of c the lemma 
clause of t. 
example 1 let 
set of start clauses. the left tableau is a query tableau representing the query   w.r.t. 
1   the right tableau is no query tableau but a lemma tableau which represents the lemma 
　a lemma application transforms a subgoal into a  possibly empty  set of new subgoals. an application of a lemma l to a subgoal * can be viewed as attaching the instantiated lemma tableau of l below s. thus  it works as a macro operator in the search space. the use of a bottom-up created lemma can close a subgoal by an extension with the lemma and performing reduction steps into the introduced subgoals  tail literals  and thus reduces the proof length. in the following we will always employ such a purely bottom-up oriented view on lemmas  i.e.  they replace deductions at the  tableau front . extension steps to instantiated tail literals of lemmas are forbidden. this provides a controlled use of lemmas and prevents a nesting of lemma applications when dealing with non-unit lemmas. 
1 	pattern controlled level saturation 
our first method for generating lemmas is based on a combined systematic goal decomposition and lemma saturation. the basic idea is to employ iterative top-down 
	fuchs 	1 

and bottom-up generation procedures which produce in iteration step  level   all query and lemma tableaux of depth t by the decomposition or saturation of the queries and lemmas of the previous level  respectively. initial queries and lemmas  created in step 1  are the start clauses and the contrapoeitives of the input clauses  respectively. then  after each iteration step subsumed tableaux are deleted  a notion of tableau subsumption can be found  e.g.  in  . thus  a proof of depth d can be obtained in  top-down and bottom-up iteration steps. it is obtained by closing a query tableau generated in step  using bottom-up lemmas  by extension of the query literals with lemmas and closing the introduced subgoals with reductions  which have been created in the steps 1   . . .   
　this combined bottom-up and top-down proof search has several theoretical advantages compared to a pure bottom-up or top-down search. the top-down search is improved by bottom-up processing which avoids the recomputation of solutions for multiple occurring subgoals in query clauses. moreover  the method improves on a pure bottom-up computation because it is more goal oriented and thus the production of a large number of irrelevant clauses may be avoided. 
　in practice  however  such an approach does not appear to be reasonable  for  harder problems  . an explicit storage of all generated tableaux is not sensible when dealing with me based provers because of the huge increase of the number and size of the generated tableaux. thus  we have to focus only on some few relevant query tableaux  or query clauses when dealing with horn problems  and lemmas which are maintained after each level for further decomposition or saturation in the next iteration  respectively. heuristic selection criteria for query tableaux and lemma clauses are needed. when using such normally fuzzy criteria  however  it is not guaranteed any longer that a query tableau which can be closed with lemmas can be produced after  iterations. it is probable that useful queries or lemmas are discarded such that more than  iterations are needed. then  the process may be more costly than conventional top-down or bottom-up deduction. because of the deletion of query tableaux and lemmas it is even possible that no proof can be found by clc ing a maintained query tableau with derived lemma clauses. 
　thus  we employ a slightly different  lemma oriented  method which we will explain only for horn clauses and unit lemmas for simplicity reasons. instead of employing a complete top-down enumeration of all query clauses we work in an abstracted top-down search space. literals of specific query clauses are generalized to so-called patterns. patterns are literals which cover the form of several subgoals. specifically  we try to guarantee that subgoals occurring in a proof are subsumed by some patterns. patterns are created in d -1 steps. as initial patterns  in step 1  the literals occurring in start clauses are created. then  in each step  1 we successively decompose patterns of the previous step t -1 into new subgoals 
1 	automated 	reasoning 
and generalize then the subgoals to new patterns. thus  patterns created in step i generalize subgoals of depth i which occur in query clauses. these top-down generated patterns cannot be used for finishing a proof task  with the help of lemmas . however  they provide relevancy criteria for lemmas. if a lemma is not unifiable with the complement of a specific pattern it can be discarded. thus  we can work with a conventional iterated lemma generation procedure whose maintenance criteria for lemmas are assisted by top-down inferences. finally  the lemmas are used in a top-down proof run for refuting the input clauses. 
　　we make our method more concrete. we start with the top-down goal decomposition. first  we show how to generalize literals occurring in a set of subgoal clauses to patterns. we use as patterns n n literals from the set  for l  .  is the set of all literals where each literal has a length {no. of symbols  and cannot be specialized to a literal with length and 
 patterns should generalize the subgoals of clauses from q which are the most likely to occur in a proof. in order to determine the literals to be used as patterns we employ a function quoiq on literals. this function is based on a notion of quality qual1g on subgoals. qual1g is used to estimate whether a subgoal may occur in a proof. then  qualq expresses how many subgoals from of a high quality are generalized by a pattern. 
　qual1g s  of a subgoal $ is a value which represents the  generality  of s since we assume that more general subgoals can more easily be solved. for instance  small subgoals with many variables may get large values by quals1  cp.  . qualq is defined using qual1g as follows. 
definition 1  pattern quality  for a literal i let inst l  be the multi-set 
we define the pattern quality qualq l  of a literal / w.r.t. 

　the best n literals from lit  w.r.t. pualq form the set pati s q  of patterns for q. patterns which generalize subgoals which are part of a proof provide an exact criterion for discarding irrelevant lemmas. a pattern must be unifiable with the complement of a lemma. l and n are responsible for providing a compromise between a large pruning effect of the patterns on the number of generated lemmas  l large  n small  or a high probability that no useful lemmas are discarded  l small  n large . we employ a query generation algorithm which gets as input a clause set c  start clauses 1  and an iteration number  as output the sets of patterns  are delivered. 
procedure 1  query generation  

 a  let q be the set of the most general query clauses of query tableaux from  which 


　the lemma generation algorithm enumerates lemma tableaux in a similar way as in  1; 1  but additionally uses the generated patterns. it can be applied after the generation of the query pattern sets  a fur 
ther input of the algorithm is again an iteration number and the set of input clauses c. as output a lemma 
is delivered. 
procedure 1  lemma generation  

　the chance for easy lemmas to be maintained is higher than for hard lemmas since more patterns are used. this is because easy lemmas may be applicable in more depth levels of a proof. the pattern-based criterion discards lemmas immediately after their generation and thus saves space. the set  forms the set of possible lemmas which may finally be used in the proof run in addition to the input clauses. we consider a closed tableau for a horn clause set of a depth of d. if  and the patterns from cover the form of the subgoals with depth i which are needed to find the closed tableau  the lemma generation method is complete. this means that it can be guaranteed that contains all lemmas needed to reduce the proof depth by an amount of j - 1 . specifically  it is also possible to reduce the proof length. in practice after the execution of the generation procedure lemmas are selected from with a selection function  see  . these lemmas are used in a final proof run. 
1 	evolutionary lemma generation 
the pattern-based method has the pleasant property that it provides a systematic generation of lemmas which can guarantee the generation of useful lemmas under certain conditions. a practical advantage is that highly efficient model-elimination provers can be employed for top-down as well as bottom-up inferences. 
　but if the choice of the patterns is not optimal the method works as a local optimization method because lemmas are discarded. lemma tableaux whose derivations require the use of  small quality  lemmas which are discarded at a certain moment cannot be generated later. thus  it is rather probable that the generation of useful and also well judged lemmas is prevented because the generation of such lemmas may require the use of other discarded clauses. 
　our solution to this problem is the use of evolutionary techniques for lemma generation which are based on the genetic programming  gp  paradigm. for a detailed introduction to genetic algorithms or genetic programming we refer to  or   respectively. our application of gp combines the evolution of query and lemma tableaux. the abstract principles of our method are as follows. 
an individual corresponds to a connection tableau which represents a lemma or a query clause. thus  we work with  possibly  partial solutions  lemmas  of our initial problem and with goal decompositions which represent problems which are still open. the fitness of one lemma is given by its ability to solve or  almost  solve an open subproblem. a tableau which represents a query is the fitter the more subgoals are solvable  almost solvable  by lemmas. the genetic operators are based on the exchange of sub tableaux. thus  good subdeductions which may be part of a proof are used in order to create new  and possibly fitter  individuals. building blocks  subtableaux  of the fittest individuals persist with a high probability and can contribute to a generation of lemmas or query clauses which appear in a proof. 
　thus  the lemma and query tableaux used for gp are used in a deductive sense by producing new lemma and query clauses in order to solve the original problem or at least to generate useful lemmas. further  they play a role as control elements. the lemma production influences the query decomposition and vice versa. this is similar to our first pattern-based approach. but now also the top-down decomposition is influenced by some kind of distance to given valid lemmas. hence the search is concentrated on  interesting  regions of the search space in a self-adaptive way. furthermore  the probabilistic character of gp offers the chance to avoid a naive hill climbing based search and can produce needed lemmas although ancestors are judged by low fitness values. 
　the technical realization of these ideas is as follows. as already mentioned we use a fixed sized population of tableaux. each tableau represents a query or a lemma. the population is initialized using the given input clauses to be refuted. each query tableau obtainable by applying the start rule with a clause allowed as a 
　start clause is added to the population. analogously for each contrapositive of an input clause a lemma tableau is built. additionally  it is possible to use some further selected lemma or query clauses in the initial population  see section 1 . 
　we employ three genetic operators  namely reproduction   and variants of crossover and mutation. reproduc-
tion copies one element from the old population to the next. our crossover operator differs from standard gp where two randomly chosen subtrees of two ancestor individuals are exchanged. since such an operation would normally not result in a connection tableau crossover has to be constrained. one approach is to allow crossover at nodes and  labeled with  and l1  of two individuals and respectively  only if exists. then  an exchange of the subtrees could take place and the resulting tableaux are instantiated with as appealing as this sounds it neglects the fact that a re-use of a subdeduction below vi in tableau  may be possible below in although the above criterion is not applicable. this is because the subdeduction may 
	fuchs 	1 

be more general when viewed in an isolated way and is  over-instantiated  in consider following example. 
example 1 let 
the following figure shows two 
 connection tableaux for c. the arrow  which is also called link  shows that the subdeduction below can be 
　thus  asymmetric link relations between tableaux can be built which show which subdeductions can replace others  see also  . our crossover variant produces one new individual. consistently with the link relation in a destination tableau a  possibly empty  subdeduction is replaced by a subdeduction in a source tableau. then  the modified destination tableau is instantiated in an appropriate manner  more details can be found in  . in the above example the left and the right tableau serve as source and destination tableau  respectively. the tableau resulting from crossover represents the query 
 the crossover operator can be viewed as a generalized extension step which allows us to attach subdeductions and not only clauses to  inner  nodes. we use a mutation operator which serves as a generalized reduction step  see  . it is needed in the non-horn case to preserve the completeness of the genetic operators in order to create each useful lemma. 
　the genetic operators are applied to individuals chosen probabilistically proportionate to their fitness. we use a similarity measure between query and lemma tableaux for computing fitness. in the horn case only the query and lemma clauses are considered. in the nonhorn case we may also consider open branches for judging the similarity. we use a similarity measure which considers certain syntactic properties of literals  cp.  . 
　the evolutionary search stops if a query tableau can be extended to a closed tableau using the lemma tableaux or a given maximal number of generations is reached. in the latter case a selection function  see   chooses lemma clauses of the current population which are used in the final proof run. in summation the gp approach cannot guarantee that useful lemmas are generated during the search. but at least one can show that when fulfilling weak conditions each needed lemma can be created with a probability greater than 1 . the self-adaptation car pabilities and randomized effects can allow the solution of problems which are out of reach of conventional search techniques  see section 1 . 
1 	experimental results 
we want to analyze the performance of the newly developed lemma generation procedures. we have chosen the 
1 	automated reasoning 
table 1: experimental results in the tptp library 
domain setheo setheo/pat setheo/op boo 1 
1 
1 1 
1 
1 1 
1 cat 1 
1 
1 & 
1 
1 1 
1 
1 col 	1 
1 
1 1 
1d 1 
1 grp 1 
1 1 1 1 
1 set 1 
1 
1 1 1 1 1 high performance model elimination prover setheo for the final top-down proof run as well as for the pattern based lemma generation procedure. 
　as test set domains of the problem library tptp v1.1  are used. the domains boo  cat  col  grp  and set have been chosen. boo  col  and grp mostly contain horn problems whereas in the other domains often non-horn problems occur. we tackled only  hard problems . these problems cannot be solved with the conventional setheo system within 1 seconds. we have used a sun ultra 1 and a run time limit of 1 minutes for each problem. this includes the time for the lemma generation and the final refutation run. 
　in table 1 one can find the performance of the newly developed systems in comparison with setheo. setheo is configured as described in . specifically  this includes the use of folding-up  see   in the proof run. setheo/pat generates lemmas based on our first method. the lemmas are then added to setheo. the final proof run is done with the same version of setheo which is used without lemmas. we restrict the lemma generation to unit lemmas which are sufficient to obtain good results in the considered domains. the lemma generation procedure 1 is employed with iteration number j = 1. the iteration number for the pattern generation was set to j = 1. we used for the pattern length l and the pattern number tv the combinations  l  n  m  1  and  l n  =  1 . we depict for each example the best result which could be obtained with a configuration. the selection function is defined using the lemma delaying method as introduced in {1 . setheo/gp is based on genetic programming. we have initialized the evolutionary lemma generation in such a manner that the lemmas which are produced by the pattern based method with two bottom-up iteration steps are used in the initial population. furthermore  we use some selected queries  see  . thus  the procedure can start at an interesting point in the search space. unit lemmas are selected from the final population using the lemma 

delaying method. we show the best results obtained in 1 runs for each problem. the exact configuration of our genetic algorithm can be found in  1 . 
　in the table we have depicted the number of problems which can be solved by the considered approaches after 1  and 1 minutes. we can see that all lemma methods improve on the conventional setheo system in a stable way. often lemma tableaux of a proof depth of 1  sometimes of a depth of 1 and 1  can be used in a proof. the use of these lemmas leads to a proof length reduction and significantly smaller search spaces which have to be traversed by the iterative deepening search procedure. in comparison with the already successful conventional lemma generation approaches  without top-down assistance  as described in  1; 1  all of our methods improve on results obtained with these methods. the level saturation procedure can be improved by the top-down generated patterns. e.g.  in the set domain where a lot of predicate and function symbols occur the pattern use is indispensable. in domains like boo  however  the patterns cannot incorporate additional potential for deleting lemmas. the gp approach significantly improves on the level saturation method. a not fitness controlled randomized method achieves worse results  see  . 
　considering the pattern based method and gp one can recognize that with gp generated lemmas the proof length can often be reduced in a more effective manner. the pattern based method has some difficulties in producing sufficiently specific patterns well-suited for the first iteration steps since the probability that such patterns match needed query literals decreases from iteration to iteration. gp can overcome the problem that in its initial population some useful lemmas may be missing. it can re-compute these lemmas. furthermore  we could observe that gp sometimes generates well-suited rather hard lemmas  with depth 1 . 
1 	conclusion and future work 
we presented two methods for combining top-down and bottom-up proof search aiming at generating a set of well-suited lemmas. the lemmas are then used in a final top-down proof run in addition to the given input clauses. we have seen that an evaluation of an abstracted top-down search space  with patterns  and a partial evaluation of interesting regions of the complete top-down search space  by genetic programming  is indeed able to provide sufficient information in order to control bottom-up inferences. as the experiments show the criteria are strong enough to generate interesting hard lemmas which provide large search reductions. 
　the study reveals that the use of our techniques in a parallel proof environment is desirable. the pattern based approach can work with different parameters for l  n  i  and j in parallel. also the gp algorithm can profit when different incarnations run in parallel. specifically  the evolutionary approach offers the possibility to develop high performance parallel systems which may scale up to a large number of processors. 
