 
in many real-world environments  automatic speech recognition  asr  technologies fail to provide adequate performance for applications such as human robot dialog. despite substantial evidence that speech recognition in humans is performed in a top-down as well as bottom-up manner  asr systems typically fail to capitalize on this  instead relying on a purely statistical  bottom up methodology. in this paper we advocate the use of a knowledge based approach to improving asr in domains such as mobile robotics. a simple implementation is presented  which uses the visual recognition of objects in a robot's environment to increase the probability that words and sentences related to these ob-
jects will be recognized. 
1 	motivation & proposal 
through the mapping of an acoustic signal to a string of words asr systems are a key tool in the control of mobile devices such as robots and pdas  particularly in cases where manual control is not appropriate or feasible. however despite the substantial improvements in asr reliability which have been made in the last ten years  results can still be very poor in noisy environments. most asr systems are built around a statistical  data driven architecture which fails to capitalize on sources of information other than the input audio signal and static vocabulary. many hybrid architectures utilizing lip-reading algorithms have emerged in recent years  chibelushi et al.  1   but these are dependent on a user directly facing a communication interface. 
　asr is becoming an increasingly more important tool in the development of service and mobile robots. this can generally be accredited to the ease of use a natural language interface should bring to human-machine interactions. however in practice the speech recognition systems available fail to produce sufficient accuracy for natural interactions. these failings result from a wide range of environmental distortions and noise  including a  interference from the robot's drive systems  b  reverberations c  multiple user interference  the so-called cocktail party effect . 
　users of asr in the robotics community have generally taken two approaches to counteracting sources of interference. on a practical level the simplistic use of hand-held or head-mounted microphones can dramatically improve performance. another approach uses dialog systems based on finite state grammar models or frame systems to anticipate user utterances from dialog constructs  matsui  1; bischoff  1 . although these approaches do offer a substantial improvement on a purely black-box approach to asr  they impose tight constraints on the usability and flexibility of the system as a whole. 
　naturally the partial failure of asr technology in a noisy environment does not only pose problems for the mobile robotics community. problems with mono-modal speech recognition systems have been recognized for many years  and many attempts have been made to improve recognition quality through the use of low-level visual information. in  chibelushi et al  1  chibelushi  deravi and mason present a comprehensive review of these approaches. algorithms providing sources of visual information center mostly on those which perform lip-reading or the analysis of facial gesture. the techniques discussed in the review paper can be characterized as being low-level or data driven in nature  with a division being drawn on whether information should be combined before integration into a model  or whether two separate low-level models should first be formed before a general abstraction is made. 
　the trend towards integrating low level visual and audio information reflects clear evidence of this phenonimon in humans. the mcgurk effect is perhaps the best known example of this  where the audio presentation of the sound 'ba' along with the visual lip movements of the sound 'ga' typically results in a listener perceiving the sound 'da  mcgurk and macdonald  1 . however there is also a growing body of evidence that indicates that humans use high level context and semantic effects to improve our speech recognition performance  tanenhaus et al.  1; simpson  1 . further more it is commonly observed that in conversation we will often reference particular themes  and discuss objects in his or her local environment. 
　inspired by this evidence of context and high level semantic priming of speech recognition in humans  we propose the improvement of asr systems on mobile robots through the use of a top down context priming model  rather than relying on workarounds based on strict dialog systems or user held microphones. the initial model described below is based on 

poster papers 	1 

the premise that users commonly discuss objects in their local environment. specifically our model proposes that upon the visual recognition of an object in the environment  the probability of recognition of words associated with that object should be increased. this is achieved through direct communication between software agents responsible for visual processing and speech recognition. although such a model is simplistic it acts as a stepping stone towards the further improvement of asr by context effects. 
1 	implementation 
the research presented here has been conducted as part of the 
said  speaking autonomous intelligent devices  project in 
university college dublin  ross et al.  1 . the focus of this project is to examine how the two very different computational paradigms of speech recognition and autonomous agents can benefit from each other in the production of intelligent speech enabled devices. the model developed here has therefore been constructed for a complete mobile robot platform  rather than having been implemented as a stand-alone algorithm. 
　although the model presented has been developed for a mobile robot with a vision source  the basic principle is not hard-wired to the mobile robot domain. the principle can be applied anywhere where there is a source of high level information which can be used to dynamically prime a speech recognition systems. specificly the speech priming agent discussed below can be applied on any mobile device which has access to spatial knowledge of the outside world  whether acquired through passive visual detection or not . 
1 	platform 
experiments are carried out using a team of nomaidc scout 
ii robots  refitted with on-board computers  vision  and sound systems. the control system for the robot is provided through marc  an experimental multi-agent system based architecture  ross  1 . this architecture like many social robot architectures views individual robots as intelligent agents in a social community. however marc is original in that all aspects of any one robot's control  from high level planning and user modeling to low level movement and reactive control are encapsulated through a community of intentional deliberative agents  see  wooldridge  1  for a good introduction to the theoretical models  and  collier  1  for details of a concrete development and runtime environment for these agents. . although the robot control architecture contains a large number of diverse agents  discussion here will be limited to those agents specificly connected with speech recognition with visual priming. 
1 	the agents 
the speech recognition and visual priming system consists of a number of agents which provide a  basic speech recognition  b  passive visual object recognition and c  speech recognition priming through the use of a simple semantic model. despite the fact that these individual agents are internally highly data driven  coupling between the agents is loose with all inter-agent communication via the agent communication language teanga  rooney  1 . the internal design of the three main agent types will now be outlined  followed by a discussion of their typical interactions and usage. 
　low level speech recognition agents - these agents work at the level of traditional asr systems to convert audio signal from the outside world to simple strings of text or n-grams. a number of low level speech agents have been built using a range of speech recognition toolkits. toolkits employed included both well-known systems like sphinx and via voice  as well as a less popular but potentially more powerful and open hybrid systems  carson-berndsen and walsh  1 . 
　a problem encountered in using the more popular speech recognition toolkits  is that most operate using either a 1nf grammar of predicted sentences  or in a completely free-form dictation mode. a middle ground where the adjustment of probabilities on words and sentences was not achieved easily. 
　visual object recognition agent - each robot has on on board color ccd camera. this camera is controlled by low level software and drivers to provide video footage of the outside world. a visual object recognition agent was built which employs color segmentation and edge-based feature detection algorithms to scan for key objects in the robot's field of view. the visual recognition agent passively scans the environment for pre-defined objects such as chairs  colored balls  and large office features. upon detection of any such object this visual detection agent communicates its observation to any agents which have previously requested to be kept informed of its findings. 
　speech recognition priming agent - the third agent type acts as the key filtering mechanism between the visual recognition of objects and the adjustment of word recognition probabilities in the lower-level speech recognition agents. 
　the agent employs a semantic network type structure to produce strings and groups of probable words for recognition based on the priority of identified external objects. specificly  the network has been prepared with details of objects commonly found in an office environment  along with associated verbs  prepositions and pseudonyms. when informed that there is a package in the robots proximity  this agent can provide a list of words and phrases which are likely to be spoken in connection with such a package e.g. deliver  box. the model can be altered to produce recommendations for varying periods and priorities based on relative importance of objects and the robots current actions respectively. 
　a typical usage scenario involves the speech recognition priming agent making an initial request with the visual object recognizer for reports on any key objects recognized in the environment. if and when provided with reports of objects in the robots vicinity  the speech recognition priming agent will build up a revised list of words which it might expect a user to utter. the speech recognition priming agent will then attempt to inform the speech recognition agent of this revised list. the relevant speech recognition agent can then choose to take note of these values in speech recognition  or ignore them as it sees fit. 
based on the advice from the speech priming agent  the 

1 	poster papers 

speech recognition agents can then interprete the incoming sound signals as appropriate  providing strings of recognized text to any agents which have expressed an interest in its findings e.g. social communication agents  command interpreter agents. 
1 	test scenarios 
experiments conducted are based on a user and robot situated in a room which has been furnished with a number of objects  some of which the robot is capable of visually recognizing. the user then issues a number of commands to the robot. this set of commands is composed of instructions and questions about objects in the room. the command set includes both well formed commands and a number of garbled commands which are phonetically very similar to the well formed command. these incorrectly formed commands are typical of slight mispronunciations  or environmental distortions. badly pronounced utterances range from slight mispronunciations of nouns and verbs  to the replacement of one word with a different but proper word as with 
where is the mall 
rather than 
where is the ball 
　in initial tests the utilization of the context priming model clearly produces more reliable results than those produced when the vision system is disjoint from the speech recognition components. 
1 	related research 
the integration of audio and vision is becoming more popular in recent years and is being approached from a number of angles. in addition to the low-level integration of audio and visual information as discussed earlier  chibelushi et al  1   related research on the integration of audio and visual information includes deb roy's work on the learning of words from sights and sounds  roy  1 .  and the generation of natural language from a visual representation  herzog and wazinski  1 . 
1 	initial conclusions & future work 
although speech recognition systems can often produce inaccurate results in real-world environments  accuracy in the mobile robot domain can be improved through a knowledge based priming of speech systems. the model presented here was based on the premise that users often discuss objects in their local environment. through the visual recognition of objects asr systems were 'primed' for specific topics of conversation. such a technique is novel and should be contrasted with low level bi-modal speech recognition systems. 
　although the model presented here is simple  the principle of using high level data sources to improve asr performance can easily be extended to other data sources and platforms. taking advantage of user and task domain modeling is an obvious next step. with this in mind  immediate future work includes the expansion of the semantic model  and the addition of general task domains. 
acknowledgements 
we gratefully acknowledge the support of enterprise ireland through grant no. if/1  said. 
