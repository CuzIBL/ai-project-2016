
the crosslingual link detection problem calls for identifying news articles in multiple languages that report on the same news event. this paper presents a novel approach based on constrained clustering. we discuss a general way for constrained clustering using a recent  graph-based clustering framework called correlation clustering. we introduce a correlation clustering implementation that features linear program chunking to allow processing larger datasets. we show how to apply the correlation clustering algorithm to the crosslingual link detection problem and present experimental results that show correlation clustering improves upon the hierarchical clustering approaches commonly used in link detection  and  hierarchical clustering approaches that take constraints into account.
1 introduction
crosslingual link detection is the problem of identifyingnews articles in multiple languages that report on the same news event. it is an important component in online information processing systems  with applications in security and information retrieval. existing link detection systems are mostly monolingual  with a small number of bilingual link detection systems  allan et al.  1; chen and chen  1; spitters and kraaij  1  and very few crosslingual link detection systems  pouliquen et al.  1  that work across many languages. like the latter  we assume monolingual link detection has been done  such that news articles on the same event in a single language already form a single group. this assumption is mild  as existing systems like google news  http://news.google.com  the 'all n related' links  do just this. our goal is thus to cluster these monolingual groups from different languages over a period of time  so that groups reporting on the same event are in the same cluster. one needs to take two things into consideration: 1. we would rather not cluster any monolingual groups from the same language together since we assume monolingual link detection has done a reasonable job. this is known as 'cannot-links' in constrained clustering as we will discuss later; 1. we in general do not know the number of clusters in advance.
　in this paper we propose a principled approach to the crosslingual link detection task using correlation clustering  bansal et al.  1; demaine and immorlica  1 . correlation clustering is a recent graph-based clustering framework with interesting theoretical properties. it can be formulated to solve constrained clustering  also known as semisupervised clustering  which we will use for crosslinguallink detection. in constrained clustering  one performs clustering with additional constraints  or preferences on the data points. two typical constraints are must-link  where two items must be in the same cluster  and cannot-link  where two items cannot be in the same cluster . constrained clustering has received considerable attention in machine learning  bilenko et al.  1; wagstaff et al.  1; xing et al.  1 ; we point to  basu et al.  1  for further references. solving the correlation clustering problem is hard but one natural way to approximatethe best solution is to encode it in a linear programming optimization framework. we combine correlation clustering with a large-scale linear program solution technique known as 'chunking' in order to solve larger crosslingual link detection problems. the contribution of our paper is twofold:
1. we introduce a practical way for solving the complexcorrelation clustering algorithm in  demaine and immorlica  1 ;
1. we demonstrate good performance on crosslingual linkdetection using the correlation clustering approach.
　in the rest of the paper  we start by reviewing correlation clustering and discuss how to implement it using linear programming chunking in section 1. we discuss related work in constrained clustering and crosslingual link detection in section 1. finally we present experiments in section 1 where we improve upon existing crosslingual link detection systems.
1 correlation clustering
consider the following problem: we are given a weighted graph for which we want to partition the nodes into clusters. if two nodes share an edge with positive weight  we prefer they be in the same cluster; if they share an edge with negative weight  we prefer they end up in different clusters. the goal of correlation clustering is to partition the graph into clusters to maximally satisfy these preferences.
　we review the discussion in  demaine and immorlica  1  on how to formally describe correlation clustering as an integer program  ip . let g =  v e  be a graph with weight we for every edge1 e （ e. let e+ be the set of edges with positive weights  e+ = {e （ e|we   1} and e  be the set of edges with negative weight  e  = {e （ e|we   1}. we now associate a binary variable xuv with every edge  uv  （ e with the following interpretation: if xuv = 1 then u v are in different partitions  if xuv = 1 then u v are in the same partition. intuitively xuv is the binary indicator variable for whether we cut the edge or not. correlation clustering minimizes the following objective
	.	 1 
we want the variables to correspond to a valid partitioning: if u v are in the same cluster and v t are in the same cluster  then u t must be so too. this can be achieved by the triangle inequality constraints xuv+xvt − xut below. simplifying the objective function we find the correlation clustering integer program:
minx
subject to
the weights w are input to the algorithm  and can encode must-links and cannot-links besides similarities between data items. as formulated above  correlation clustering has two attractive properties that make it suitable for crosslingual link detection in particular and constrained clustering in general. first of all  one does not need to specify the number of clusters; the algorithm determines the optimal number of clusters automatically. secondly  the graph edge weights can be arbitrary and do not need to satisfy any metric condition.
1 linear program approximation
unfortunately solving the correlation clustering ip in  1  exactly is np-hard. recent theoretical results on approximation algorithms  bansal et al.  1   in particular  demaine and immorlica  1   propose practical approaches to correlation clustering. we build on the work in  demaine and immorlica  1  where the authors describe an o logn  approximation by relaxing the ip to a linear program  lp   and rounding the solution of the lp by a region growing technique. we replace constraint xe （ {1} by xe （  1  in equation  1  to relax the ip to an lp. the solution to this lp might include fractional values which we will have to round. we point to  demaine and immorlica  1  for a detailed description and theoretical analysis of the rounding algorithm and limit ourselves to a qualitative description in this paper. one can interpret the value of the lp variables as distances: when a variable has value 1  the two adjacent nodes go in the same cluster as their distance is 1 while if a variable is 1  the two adjacent nodes go into different clusters. the rounding procedure now needs to decide on how to partition the graph given that some nodes are at fractional distances away from each other. intuitively  the rounding algorithm will pick a node in the graph and gradually grow a ball centered around this node. while increasing the radius of the ball  all the nodes that are at a distance smaller than the radius away from the center of the ball will be included in the ball. the radius grows until some technical termination condition is met. all the nodes in the ball are then put into one cluster and removed from the graph. this procedure is repeated until there are no more nodes left in the graph.  demaine and immorlica  1  prove that the original objective function  equation  1   of the lp relaxation will be bounded above by o logn  times the objective function of the ip where n is the number of nodes in the graph.1
　unfortunately  the triangle inequalities could introduce up to o n1  constraints in the lp  which puts a heavy burden on memory requirements. next we discuss how we tradeoff memory for runtime so we can solve correlation clustering for larger problem sizes.
1 lp chunking
linear program chunking  bradley and mangasarian  1  is a technique to convert a large linear program into an iterative procedure on much smaller sub-problems  thus reducing the memory need. the iterative procedure produces the same solution and is guaranteed to terminate. it works as follows: one first breaks up all the constraints into chunks and solves the optimization problem using only the first chunk of constraints. the active constraints are those inequality constraints that achieve equality at the solution. next  one keeps only the active constraints from the first chunk  adds all constraints from the second chunk  and solves the lp again. this procedure is repeated  looping through all chunks over and over until some convergence criterion is met. one can arbitrarily set the size of the chunks to reduce the memory load of the iterative procedure.
let a general linear program be described as 
	 	 1 
x
with c （ rn h （ rm〜n b （ rm. let the constraints  h b  be partitioned into l blocks  possibly of different sizes  as follows:
		 1 
at iteration j we compute xj by solving the following linear program 

where  h．1 ．b1  is empty and  h．j ．bj  is the set of active constraint  i.e. all inequalities satisfied as equalities by xj at iteration j. we stop iterating when ctxj = ctxj+ν for some pre-specified integer ν. we point to  bradley and mangasarian  1 for more details and proofsof the finite termination of this algorithm.
1 related work
constrained or semi-supervised clustering has enjoyed some recent attention  basu et al.  1; bilenko et al.  1; davidson and ravi  1; wagstaff et al.  1; xing et al.  1 . in  basu et al.  1   the authors categorize all semi-supervised methods into two classes: constraint-based and distance-based methods. the constraint-based methods  such as  wagstaff et al.  1  and to which our approach belongs  rely on the must-link and cannot-link constraints to guide the clustering algorithm in finding a partitioning that does not violate the constraints. distance-based methods  such as  xing et al.  1   learn a metric using the constraint information and then apply existing clustering algorithms to the data points in the learned metric space. these approaches require specifying the number of clusters beforehand. one solution to this issue is to use variants of hierarchical clustering that take constraints into account  e.g.  davidson and ravi  1 . by changing where to cut the dendrogram  one can control the number of clusters. the main difference between hierarchical clustering with constraints and correlation clustering is that the former makes local  greedy decisions at every step while correlation clustering optimizes the clustering over the whole graph at once. one motivation for our work is the observation that the crosslingual link detection systems in  pouliquen et al.  1; allan et al.  1; chen and chen  1  do not use constrained clustering techniques.
　so far  correlation clustering has not been applied to machine learning tasks very often. we are only aware of  mccallum and wellner  1  who implement a more restricted version of correlation clustering in  bansal et al.  1  for noun co-reference.
　the only crosslingual link detection system that covers a large set of languages we are aware of is described in  pouliquen et al.  1 . the authors describe a system which performs crosslingual link detection as well as monolingual news tracking  i.e. the identification of related news over time in one particular language. their approach uses a very rich article representation based on extracting named entities  keywords and geographical names. in addition  the articles are mapped onto the multilingual thesaurus eurovoc  steinberger et al.  1  which categorizes the articles in several of 1 hierarchically organized subjects. our system  on the other hand  uses machine translation tools to represent articles in a uniform way. this is a common  diab and resnik  1  way of working with multilingual corpora. our experiments show that although the translation is noisy  it does not significantly affect performance. our crosslingual link detection task is also related to the work in  diaz and metzler  1   where the authors introduce a framework for aligning documents in parallel corpora based on topical cor-

respondence.
1 experiments
in this section  we first illustrate correlation clustering on a toy dataset. we then discuss how to solve the crosslingual link detection problem using a correlation clustering based constrained clustering approach and show how this improves upon existing hierarchical clustering approaches.
1 correlation clustering on a toy dataset
it is straightforward to adapt correlation clustering for constrained clustering. say we are given a set of items u = {u1 u1 ，，，  ul}  a pairwise similarity measure s : u〜u ★
r  a set cm   u 〜 u of must-link constraints and a set cc   u 〜 u of cannot-link constraints. we build a graph g where the set of vertices is u. as a first step  we add an edge for all pairs of nodes not in cm “ cc and set the edge weight according to the similarity measure s. let m be a constant that is sufficiently larger than the sum of the absolute values of all weights in the graph so far. in the second step  for all the pairs in cm and cc  we add either hard or soft preferences: if we assume that the constraints are hard  we add an edge for every must-link constraints with weight m and an edge for every cannot-link constraint with weight  m. if we want soft preferences  we can use values smaller than m according to the strength of the preferences.
　figure 1 shows a toy dataset consisting of four nodes with a cannot-link constraint between nodes 1 and 1. the weights are specified in the figure. the edge not shown in the figure has a similarity of zero. we use  1 for the cannot-link constraint edge weight. the objectivefunction for this dataset is to minimize  1x 1 +1x 1 +1x 1 +1x 1 + 1x 1  subject to the triangle inequality constraints. solving the ip exactly would give us a solution that assigns 1 to all variables except x 1  = x 1  = x 1  = 1; this corresponds to the clustering {1} {1 1}. although nodes 1 and 1 have the highest similarity  the cannot-link constraint guides the correlation clustering algorithm to not take node 1 into the cluster with 1 and 1. note how a hierarchical clustering algorithm would start off wrong as it merges nodes 1 and 1 together and thus fails to find the best clustering. even a hierarchical clustering algorithm that takes constraints into account will not find the best clustering as it will greedily merge nodes 1 and 1 together.

figure 1: samples from the large dataset.1 crosslingual link detection
we generated five datasets by crawling google news. we specifically focused our experiments on news articles which google categorized as 'world news' as we assume this is the category where the most interesting cross-lingual links can be made. the first four datasets each consist of 1 monolingual 'world news' article groups from three languages: english  german and french. each of these four datasets was generated one week apart by selecting the top 1 article groups for each language in april 1. this results in a total of 1 article groups in each dataset. in may 1  we generated the fifth dataset which is larger and consists of 1 article groups from the 'world news' category in eight different languages: english  german  italian  french  portuguese  spanish  korean and chinese. figure 1 shows a sample from the larger dataset. for all five datasets  we manually created a ground truth clustering1.
　for correlation clustering  we construct a fully-connected graph where each node is a monolingual article group. we create cannot-links between all pairs of article groups from the same language and choose  1 as the weight for these cannot-link edges. we compute similarity values between article groups from different languages with the following procedure: first we concatenate all the article titles in a monolingual group to form a 'document representation' for the group. we then use google machine translation to automatically translate the 'document' into english  and remove stopwords from the translation. therefore monolingual groups in different languages are represented by their corresponding  noisy  english translation  providing a way to compute their similarities. empirically we found no difference in performance using different machine translation tools such as babelfish and wordlingo. next  for each monolingual group  we convert the translated document into a tf.idf vector w． =  w1 ，，，w|v|   with wi = ni ， log |d|/|di|   where ni is the number of times word i appears in the document representing the article group  d represents the set of article groups in the dataset and di represents the set of article groups that include word wi. we compute the similarity swv between any two tf.idf vectors w ． v． as their inner product 
	.	 1 
note that even with stop-word removal  two unrelated article groups often have a small but positive similarity due to common words. if we use the similarity  1  directly as graph edge weights for correlation clustering  many irrelevant groups will be clustered together. for the problem of link detection  this is clearly not desirable. we therefore subtract a bias constant t from all similarity values so that wuv = su．v．  t. intuitively  too small a similarity  1  between two article groups is in fact evidence that they should not be in the same cluster. by changing the bias t we change the resulting clustering  which is how we generate precision-recall curves. for all the experimentspresented below  we chose our bias values as follows: we started with a bias such that only one edge in the graph remains positively weighted. next  we steadily increase the bias such that another 1% of the edges becomes positively weighted. on the small datasets  we repeated the experiments until 1% of the edges are positively weighted while on the larger datasets we repeat the experiments until 1% of the edges are positively weighted. we compute precision and recall values relative to our manually labeled ground truth. we count an edge as true positive  tp   if its two article groups appear in the same cluster in both ground truth and our results  false positive  fp  if they do not appear in the same cluster in ground truth but do appear together in our results  and so on. precision and recall is a better measure than accuracy for our task  since the baseline of classifying every edge as 'not in same cluster' would have high accuracy because of the large number of true negatives. we used cplex 1 on a 1 ghz machine with 1gb ram to solve the linear programs.

figure 1: left: average precision-recall over four small datasets. middle: precision-recall for the large dataset. right: average　our first round of experiments are designed to illustrate how taking constraints into account improves performance on the crosslingual link detection problem. we compare our correlation clustering algorithm to the hierarchical clustering approach which has commonly been used for the crosslingual link detection problem   chen and chen  1; pouliquen et al.  1   and constrained hierarchical clustering such as  davidson and ravi  1 . hierarchical clustering is done by choosing a bias value and adding edges to the graph in descending order according to their weight until the edge weights become smaller than the bias. we then output the connected components as the resulting clusters. constrained hierarchical clustering is similar  except that at every step we only add an edge if it does not introduce a path between two nodes in a cannot-link constraint. again  we output the connected components as the resulting clusters. the left plot in figure 1 shows the average precision-recall over our four small datasets. if we keep the number of positively weighted edges small  large bias  then both types of hierarruntime over four small datasets.
chical clustering perform as well as correlation clustering. inspecting the datasets  this behavior can be explained by the fact that there are a number of news events for which the articles use a subset of the vocabulary that is not commonly used in other articles. our similarity measure assigns large weights among article groups in different languages on these events and very small weight between these article groups and article groups on a different topic. in a sense these are 'easy' instances which both hierarchical clustering approaches as well as correlation clustering get right. if we increase the number of positive edges  small bias  then the simple hierarchical clustering algorithm performs much worse than correlation clustering. as a simple hierarchical clustering approach has no notion of cannot-link constraints  it will cluster groups from the same language together. usually  crosslingual link detection systems choose to leave these clusters out  but this decision comes at the price of lower recall. constrained hierarchical clustering performs a little better as it takes our assumption about the correctness of the monolingual groups into account. nonetheless  figure 1 also shows that correlation clustering  which takes the whole graph into account instead of making local greedy decisions can still outperform constrained hierarchical clustering. we attempted to compare our approach to the constrained clustering in  bilenko et al.  1  using their utweka implementation. the implementation ended up returning many empty clusters  resulting in low precision and recall.
　the middle plot in figure 1 shows the precision-recall for the large dataset; it indicates the trend we observed with the smaller datasets: taking into account constraints can still improve the performance of crosslingual link detection.
　next  let us consider the solution found by the approximation algorithm and the exact integer solution. figure 1 shows that on the small datasets the two solution are exactly equal. inspecting the lp solutions  we find that in the high bias regime  almost no rounding is necessary as the lp solution is the exact ip solution. only in the low bias regime  when more edges are positively weighted  rounding becomes necessary. on the large dataset  figure 1 shows that although there is a small difference between the two solutions  the lp relaxation with rounding does well to find a good approximation to the integer solution. we observed rather unexpected behavior from the rounding algorithm that influences the precision-recall curves: at very low bias  due to the symmetry of the graph  the optimal lp solution has a number of variables with 1 values. from the theoretical analysis of the rounding algorithm  we know that a radius cannot grow to be 1. as a result of these properties  in the low bias regime a large number of nodes will end up as singleton clusters. this prohibits recall from increasing to 1 and we observe the precision-recall curve loop back towards lower recall and higher precision. because the curve essentially follows the first path 'in the opposite direction' we did not include this in figure 1 for clarity.
　our next experiment was designed to evaluate how much the lp approximation algorithm improves the runtime over solving the ip exactly. the rightmost plot in figure 1 shows the average runtime over the four small datasets of solving the exact ip compared to solving the approximation algorithm. every dot in the graph represents the time required to solve either the ip or the lp with rounding for a specific bias. it is clear from this figure that the lp approximation algorithm for correlation clustering is significantly faster than solving the ip directly. however  even on the larger dataset the main bottleneck is not so much the runtime but rather the memory requirements. on this large dataset  the underlying graph has 1 nodes which results in over 1 1 constraints for both the ip and lp. this is about as large a correlation clustering instance we can solve without using chunking on our machine with 1gb ram.
　our last experiment shows the results of applying chunking to the lp for correlation clustering. our experimental setup is the following: we create instances of the correlation clustering with random edge weights  distributed roughly according to the instances of interest to crosslingual link detection. we chose our chunk size to be as large as possible while still having some workspace memory for the processing in between iterations: this resulted in 1 constraints per chunk. finally we use a value of ν = 1 as our stop condition. table 1 shows the runtime for chunking versus solving the whole lp at once. correlation clustering instances of size 1 are the first instances where the number of constraints is larger than the chunk size. at this size  the runtime overhead for chunking is mostly due to the stop condition. starting from graphs with around 1 nodes we cannot fit the whole lp in memory anymore and we must apply chunking to tradeoff memory for runtime. table 1 shows that chunking is useful for scaling up the size of solvable correlation clustering problem but has its limitations too. first of all  the runtime increases fast: this is due to the fact that doubling the size of the graph roughly corresponds to an eight-fold increase in the number of constraints and equivalently an eight-fold increase in the number of chunks. another problem that arises is that the set of active constraints  h．  can become larger than the chunk size and exhaust available memory. we believe these problems are inherent to correlation clustering approximations based on integer programming.
1 〜 111 〜 111 〜 1out of memory11 〜 1out of memory11 〜 1out of memory11 〜 1out of memory11 〜 1out of memory11 〜 1out of memoryout of memory	# nodes	# constraints	whole lp	chunking lp
table 1: runtime in seconds
1 conclusion
in this paper we introduce an implementation for correlation clustering using linear program chunking that scales beyond the implementation of the algorithm in  demaine and immorlica  1 . however  we find that even our chunking method which can trade off memory for runtime has its limits due to the growth  o n1   of the linear program size. nonetheless  we believe that for constrained clustering problems of limited size  a few hundred data points  correlation clustering is worth pursuing. moreover  our experiments on the crosslingual link detection task show that correlation clustering outperforms both hierarchical clustering and hierarchical clustering with constraints.
　in future work  we plan to investigate whether other algorithms for correlation clustering have smaller time and space complexity. also  we believe it would be interesting to combine correlation clustering and our machine translation based representation with the rich document representation from  pouliquen et al.  1  to improve performance of crosslingual link detection even more.
acknowledgments
we thank shuchi chawla for helpful comments on correlation clustering  michael thompson and ted wild for linear program chunking.
