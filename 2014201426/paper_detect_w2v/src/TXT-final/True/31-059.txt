 
this paper presents a new measure of semantic similarity in an is-a taxonomy  based on the notion of information content. experimental evaluation suggests that the measure performs encouragingly well  a correlation of r = 1 with a benchmark set of human similarity judgments  with an upper bound of r = 1 for human subjects performing the same task   and significantly better than the traditional edge counting approach  r = 1 . 
1 	introduction 
evaluating semantic relatedness using network representations is a problem with a long history in artificial intelligence and psychology  dating back to the spreading activation approach of quillian  and collins and loftus . semantic similarity represents a special case of semantic relatedness: for example  cars and gasoline would seem to be more closely related than  say  cars and bicycles  but the latter pair are certainly more similar. rada et al.  suggest that the assessment of similarity in semantic networks can in fact be thought of as involving just taxonomic  is-a  links  to the exclusion of other link types; that view will also be taken here  although admittedly it excludes some potentially useful information. 
　a natural way to evaluate semantic similarity in a taxonomy is to evaluate the distance between the nodes corresponding to the items being compared - the shorter the path from one node to another  the more similar they are. given multiple paths  one takes the length of the shortest one  lee et a/.  1; rada and bicknell  1; rada et a/.  1 . 
　a widely acknowledged problem with this approach  however  is that it relies on the notion that links in the taxonomy represent uniform distances. unfortunately  this is difficult to define  much less to control. in real taxonomies  there is wide variability in the  distance  covered by a single taxonomic link  particularly 
　* parts of this research were done at the university of pennsylvania with the support of an ibm graduate fellowship and grants aro daal 1-c-1  darpa n1-j-1  nsf iri 1  and ben franklin 1s.1c-j. 
1 	cognitive modelling 
when certain sub-taxonomies  e.g. biological categories  are much denser than others. for example  in wordnet  miller  1   a broad-coverage semantic network for english constructed by george miller and colleagues at princeton  it is not at all difficult to find links that cover an intuitively narrow distance  rabbit ears is-a television antenna  or an intuitively wide one  phy-
toplankton is-a living thing . the same kinds of examples can be found in the collins cobuild dictionary  sinclair  ed.   1   which identifies superordinate terms for many words  e.g. safety valve is-a valve seems a lot narrower than knitting machine is-a machine . 
　in this paper  1 describe an alternative way to evaluate semantic similarity in a taxonomy  based on the notion of information content. like the edge counting method  it is conceptually quite simple. however  it is not sensitive to the problem of varying link distances. in addition  by combining a taxonomic structure with empirical probability estimates  it provides a way of adapting a static knowledge structure to multiple contexts. section 1 sets up the probabilistic framework and defines the measure of semantic similarity in information-theoretic terms; section 1 presents an evaluation of the similarity measure against human similarity judgments  using the simple edge-counting method as a baseline; and section 1 discusses related work. 
1 	similarity and information content 
let c be the set of concepts in an is-a taxonomy  permitting multiple inheritance. intuitively  one key to the similarity of two concepts is the extent to which they share information in common  indicated in an is-a taxonomy by a highly specific concept that subsumes them both. the edge counting method captures this indirectly  since if the minimal path of is-a links between two nodes is long  that means it is necessary to go high in the taxonomy  to more abstract concepts  in order to find a least upper bound. for example  in wordnet  nickel and dime are both subsumed by coin  whereas the most specific superclass that nickel and credit card share is medium of exchange.1  see figure 1.  
　1in a feature-based setting  e.g.  tversky  1    this would be reflected by explicit shared features: nickels and by associating probabilities with concepts in the taxonomy  it is possible to capture the same idea  but avoiding the unreliability of edge distances. let the taxonomy be augmented with a function p : c -   1   such that for any  p c  is the probability of encountering an instance of concept c. this implies that p is monotonic as one moves up the taxonomy: if  then p c1  p c1 - moreover  if the taxonomy has a unique top node then its probability is 1. 
　following the standard argumentation of information theory  ross  1   the information content of a concept c can be quantified as negative the log likelihood  - log p c . notice that quantifying information content in this way makes intuitive sense in this setting: as probability increases  informativeness decreases  so the more abstract a concept  the lower its information content. moreover  if there is a unique top concept  its information content is 1. 
　this quantitative characterization of information provides a new way to measure semantic similarity. the more information two concepts share in common  the more similar they are  and the information shared by two concepts is indicated by the information content of the concepts that subsume them in the taxonomy. formally  define 
	 1  
where s c1  c1  is the set of concepts that subsume both c  and c1- notice that although similarity is computed by considering all upper bounds for the two concepts  the information measure has the effect of identifying minimal upper bounds  since no class is less informative than its superordinates. for example  in figure 1  coin  cash  etc. are all members of s nickel dime   but the concept that is structurally the minimal upper bound  coin  will also be the most informative. this can make a difference in cases of multiple inheritance; for example  in figure 1  metal and chemical element are not structurally distinguishable as upper bounds of nickel' and gold'  but their information content may in fact be quite different. 
in practice  one often needs to measure word similar-
dimes are both small  round  metallic  and so on. these features are captured implicitly by the taxonomy in categorizing nickel and dime as subordinates of coin. 
where c  ranges over s w   and c1 ranges over s w1 . this is consistent with rada et al.'s  treatment of  disjunctive concepts'1 using edge counting: they define the distance between two disjunctive sets of concepts as the minimum path length from any element of the first set to any element of the second. here  the word similarity is judged by taking the maximal information content over all concepts of which both words could be an instance. for example  figure 1 illustrates how the similarity of words nickel and gold would be computed: the information content would be computed for all classes subsuming any pair in the cross product of {nickel nickel'} and {gold gold'}  and the information content of the most informative class used to quantify the similarity of the two words. 
1 evaluation 
1 	implementation 
the work reported here used wordnet's  1-node  taxonomy of concepts represented by nouns  and compound nominals  in english.1 frequencies of concepts in the taxonomy were estimated using noun frequencies from the brown corpus of american english  francis and kucera  1   a large  1 1 word  collection of text across genres ranging from news articles to science fiction. each noun that occurred in the corpus was counted as an occurrence of each taxonornic class containing it.1 for example  in figure 1  an occurrence of the noun dime would be counted toward the frequency of dime  coin  and so forth. formally  
		 1  
where words c  is the set of words subsumed by concept c. concept probabilities were computed simply as relative frequency. 
	 1  
1  concept as used here refers to what miller et al.  call a synset  essentially a node in the taxonomy. 
1  plural nouns counted as instances of their singular forms. 
	resnik 	1 

where n was the total number of nouns observed  excluding those not subsumed by any wordnet class  of course . 
1 	task 
although there is no standard way to evaluate computational measures of semantic similarity  one reasonable way to judge would seem to be agreement with human similarity ratings. this can be assessed by using a computational similarity measure to rate the similarity of a set of word pairs  and looking at how well its ratings correlate with human ratings of the same pairs. 
　an experiment by miller and charles  l 1  provided appropriate human subject data for the task. in their study  1 undergraduate subjects were given 1 pairs of nouns that were chosen to cover high  intermediate  and low levels of similarity  as determined using a previous study  rubenstein and goodenough  1    and asked to rate ''similarity of meaning  for each pair on a scale from 1  no similarity  to 1  perfect synonymy . the average rating for each pair thus represents a good estimate of how similar the two words are  according to human 
judgments. 
in order to get a baseline for comparison  i replicated 
miller and charles's experiment  giving ten subjects the same 1 noun pairs. the subjects were all computer science graduate students or postdocs at the university of pennsylvania  and the instructions were exactly the same as used by miller and charles  the main difference being that in this replication the subjects completed the questionnaire by electronic mail  though they were instructed to complete the whole thing in a single uninterrupted sitting . five subjects received the list of word pairs in a random order  and the other five received the list in the reverse order. the correlation between the miller and charles mean ratings and the mean ratings in my replication was .1  quite close to the .1 correlation that miller and charles obtained between their results and the ratings determined by the earlier study. 
　for each subject in my replication  i computed how well his or her ratings correlated with the miller and charles ratings. the average correlation over the 1 subjects was r = 1  with a standard deviation of 1.1 this value represents an upper bound on what one should expect from a computational attempt to perform the same task. 
　for purposes of evaluation  three computational similarity measures were used. the first is the similarity measurement using information content proposed in the previous section. the second is a variant on the edge counting method  converting it from distance to similarity by subtracting the path length from the maximum possible path length: 


table 1: summary of experimental results. 
is the length of the shortest path from c1 to c1 .  recall that s w  denotes the set of concepts in the taxonomy that represent senses of word w.  note that the conversion from a distance to a similarity can be viewed as an expository convenience  and does not affect the evaluation: although the sign of the correlation coefficient changes from positive to negative  its magnitude turns out to be just the same regardless of whether or not the minimum path length is subtracted from  1 x max . 
　the third point of comparison is a measure that simply uses the probability of a concept  rather than the information content: 
 1  
 1  
where c1 ranges over s w   and c1 ranges over s w1  in  1 . again  the difference between maximizing 1-p r  and minimizing p c  turns out not to affect the magnitude of the correlation. it simply ensures that the value can be interpreted as a similarity value  with high values indicating similar words. 
1 	results 
table 1 summarizes the experimental results  giving the correlation between the similarity ratings and the mean ratings reported by miller and charles. note that  owing to a noun missing from the wordnet taxonomy  it was only possible to obtain computational similarity ratings for 1 of the 1 noun pairs; hence the proper point of comparison for human judgments is not the correlation over all 1 items  r = .1   but rather the correlation over the 1 included pairs  r - .1 . the similarity ratings by item are given in table 1. 
1 	discussion 
the experimental results in the previous section suggest that measuring semantic similarity using information content provides quite reasonable results  significantly better than the traditional method of simply counting the number of intervening is-a links. 
the measure is not without its problems  however. 
one problem is that  like simple edge counting  the mea-

1 	cognitive modelling 

sure sometimes produces spuriously high similarity measures for words on the basis of inappropriate word senses. for example  table 1 shows the word similarity for several words with tobacco. tobacco and alcohol are similar  both being drugs  and tobacco and sugar are less similar  though not entirely dissimilar  since both can be classified as substances. the problem arises  however  in the similarity rating for tobacco with horse: the word 

table 1: similarity with tobacco computed by maximizing information content 
horse can be used as a slang term for heroin  and as a result information-based similarity is maximized  and path length minimized  when the two words are both categorized as narcotics. this is contrary to intuition. 
　cases like this are probably relatively rare. however  the example illustrates a more general concern: in measuring similarity between words  it  is really the relationship among word senses that matters  and a similarity measure should be able to take this into account. 
　in the absence of a reliable algorithm for choosing the appropriate word senses  the most straightforward way to do so in the information-based setting is to consider all concepts to which both nouns belong rather than taking just the single maximally informative class. this suggests redefining similarity as follows: 
 1  
1 
this measure of 
similarity takes more information into account than the previous one: rather than relying on the single concept with maximum information content  it allows each class to contribute information content according to the value of a ci . intuitively  these a values measure relevance - for example  a narcotlc  might be low in general usage but high in the context of a newspaper article about drug dealers. in work on resolving syntactic ambiguity using semantic information  resnik  1b   i have found that local syntactic information can be used successfully to set values for the a. 
1 	related work 
although the counting of edges in is-a taxonomies seems to be something many people have tried  there seem to be few published descriptions of attempts to directly evaluate the effectiveness of this method. a number of researchers have attempted to make use of conceptual distance in information retrieval. for example  rada et al.  1; 1  and lee et al.  report experiments using conceptual distance  implemented using the edge counting metric  as the basis for ranking documents by their similarity to a query. sussna  uses semantic relatedness measured with wordnet in word sense disambiguation  defining a measure of distance that weights different types of links and also explicitly takes depth in the taxonomy into account. 
　the most relevant related work appears in an unpublished manuscript by leacock and chodorow . they have defined a measure resembling information content  but using the normalized path length between  the notation above is the same as for equation  1 .  in addition to this definition  they also include several special cases  most notably to avoid infinite similarity when c  and c1 are exact synonyms and thus have a path length of 1. leacock and chodorow have experimented with this measure and the information content measure described here in the context of word sense disambiguation  and found that they yield roughly similar results. more significantly  1 recently implemented their method and tested it on the task reported in the previous section  and found that it actually outperforms the informationbased measure. this led me to do a followup experiment using a different and larger set of noun pairs  and in the followup study the information-based measure performed better.1 the relationship between the two algorithms will thus require further study. for now  however  what seems most significant is that both approaches take the form of a log-based  and hence information-like  measure  as originally proposed in  resnik  1a . 
　finally  in the context of current research in computational linguistics  the approach to semantic similarity taken here can be viewed as a hybrid  combining corpusbased statistical methods with knowledge-based taxonomic information. the use of corpus statistics alone in evaluating word similarity - without prior taxonomic knowledge - is currently an active area of research in the natural language community. this is largely a reaction to sparse data problems in training statistical language models: it is difficult to come up with an accurate statistical characterization of the behavior of words that have been encountered few times or not at all. word similarity appears to be one promising way to solve the problem: the behavior of a word is approximated by smoothing its observed behavior together with the behavior of words to which it is similar. for example  a speech recognizer that has never seen the phrase ate a peach can still conclude that john ate a peach is a reasonable sequence of words in english if it has seen other sentences like mary ate a pear and knows that peach and pear have similar behavior. 
　the literature on corpus-based determination of word similarity has recently been growing by leaps and bounds  and is too extensive to discuss in detail here  for a review  see  resnik  1a    but most approaches to the problem share a common assumption: semantically similar words have similar distributional behavior in a corpus. using this assumption  it is common to treat the words that co-occur near a word as constituting features  and to compute word similarity in terms of how similar their feature sets are. as in information retrieval  the  feature  representation of a word often 
   1  in the followup study  i used netnews archives to gather highly frequent nouns within related topic areas  and then selected noun pairings at random  in order to avoid biasing the followup study in favor of either algorithm. 
	resnik 	1 

takes the form of a vector  with the similarity computation amounting to a computation of distance in a highly multidimensional space. given a distance measure  it is not uncommon to derive word classes by hierarchical clustering. a difficulty with most distributional methods  however  is how the measure of similarity  or distance  is to be interpreted. although word classes resulting from distributional clustering are often described as  semantic   they often capture syntactic  pragmatic  or stylistic factors as well. 
1 	conclusions 
this paper has presented a new measure of semantic similarity in an 1s-a taxonomy  based on the notion of information content. experimental evaluation was performed using a large  independently constructed corpus  an independently constructed taxonomy  and previously existing human subject data. the results suggest that the measure performs encouragingly well  a correlation of r = 1 with a benchmark set of human similarity judgments  against an upper bound of r = 1 for human subjects performing the same task   and significantly better than the traditional edge counting approach  r = 1 . 
　in ongoing work  i am currently exploring the application of taxonomically-based semantic similarity in the disambiguation of word senses  resnik  1 . the idea behind the approach is that when polysemous words appear together  the appropriate word senses to assign are often those that share elements of meaning. thus doctor can refer to either a ph.d. or an m.d.  and nurse can signify either a health professional or someone who takes care of small children; but when doctor and nurse are seen together  the ph.d. sense and the childcare sense go by the wayside. in a widely known paper  lesk  exploits dictionary definitions to identify shared elements of meaning - for example  in the collins cobuild dictionary  sinclair  ed.   1   the word ill can be found in the definitions of the correct senses. more recently  sussna  has explored using similarity of word senses based on word net for the same purpose. the work i am pursuing is similar in spirit to sussna's approach  although the disambiguation algorithm and the similarity measure differ substantially. 
