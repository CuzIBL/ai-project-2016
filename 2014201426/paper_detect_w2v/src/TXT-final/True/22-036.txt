 
   the effective use of knowledge based systems technology to solve complex and real-time problems in embedded system environments requires that the performance of these systems be maximized for conventional processor architectures. this paper presents a technique for the complete compilation of knowledge bases directly into procedural code. the technique is based on determining what actions an interpretive inference engine would take with the specific knowledge base and generating only the code needed to perform those actions. this eliminates the overhead of interpreting a representation of the knowledge base and significantly improves performance. the performance gains produced by this approach will be examined using examples from the high performance  imbedded reasoning  hiper  
system  a prototype tool which utilizes this technology. 
1 	introduction 
   while knowledge based system technologies have shown promise in many useful application areas  their use in some applications has been limited by the performance of existing implementations. this is particularly true in real-time monitoring and control applications which must reason about large amounts of data in very short periods of time. it is also true in embedded applications in which the knowledge based system must share the resources of the computer with a number of other applications. one solution to the performance problem is to develop specialized hardware to support the inferencing process. however  this solution is undesirable for many embedded applications because of the added complexity of additional hardware and problems with accessing data on existing systems. 
   a major performance problem for knowledge based systems on conventional hardware is the inherent interpretive nature of their implementations. even when the inference engine is implemented in a compiled language  the general purpose design of an application independent inference engine requires the interpretation of data structures which represent the knowledge base. 
this process of interpretation is inefficient in comparison to fully compiled implementations. by restricting the inferencing techniques available to decision trees or other single paradigm reasoning strategies  compilation of the knowledge base can be performed. however  this limits the knowledge engineers ability to represent and solve some classes of problems. 
   this paper will present a technique that can be used to convert knowledge bases directly into compilable high level language code providing high performance using existing and proven reasoning algorithms. interpretive approaches to implementing backward and forward chaining reasoning will first be examined to identify areas where performance improvements could be made. then a design for representing knowledge bases in a program form and the generation of this form from the rules of a knowledge base will be presented. finally  the implementation of this technique in the 
high 	performance 	embedded 	reasoning 	 hiper  
system prototype will be discussed and results obtained from this prototype compared with those from other tools. 
1 	interpretive approaches to reasoning 
   most knowledge based systems represent knowledge internally as collections of data structures consisting of nodes and links between the nodes. the nodes represent the rules  the tests within the rules  the actions to be taken by the rules  and the facts on which reasoning is performed. the links represent the relationships between facts and rule tests  the ways in which the results of tests are combined into rule conditions  and the references made to these tests and conditions from other rules in the knowledge base. the nodes are usually organized into trees or networks which are augmented with lists of references and current data values. 
   inference engines implement the reasoning process by traversing the networks that represent the knowledge base. at each node  the inference engine examines the contents of the node to determine what data is involved and what tests need to be performed on that data  test evaluation   which nodes to process next based on the results of those tests  test result propagation   and what actions to perform when a rule becomes true  performance of rule actions . based on the results of this process the inference engine follows one or more links to the other nodes. 
	highland and iwaskiw 	1 
   as an example of this  consider a simple backward chaining system which organizes its rules as a set of goal trees  each containing one or more rules that provide conditions and fact values concluding a single goal. the rules and the goal tree that would result from them are represented in figure 1. the reasoning process for such a system would begin by selecting a goal node and interpreting that node and each of its successors in order to depth-first traverse the goal tree. this process would continue until a node is encountered that requires a fact that is not currently known. the value for the fact is would then acquired by interpreting information in the fact node that indicates how to obtain the value. when a value is obtained  test evaluation occurs by interpreting information in test nodes that reference that fact. the results of these tests are then propagated up the goal tree by reversing the traversal process and interpreting each node to determine how the results of previous nodes are to be combined and passed to the next node in the tree. in addition to the interpretation that must be performed for test evaluation and result propagation  the tree traversal process must also be interpretive because connections are only known to the nodes which represent the trees. this form of backward chaining reasoning mechanism is similar to that of e m y c i n  van melle el al  1   pie  burns et al  
1   example systems proposed by winston and horn   and many other systems. 
   forward chaining reasoning algorithms operate in a similar manner except that the depth-first traversal to find a fact to evaluate is not performed. instead  the system must search for facts that match rule conditions. typically  this involves propagation of facts to the rules and evaluation of rule tests based on those facts. this again involves interpretation of the test nodes to evaluate the tests with respect to a set of facts. after a test is evaluated the results are propagated up the rule tree by interpreting the nodes at each step to determine how the results of previous nodes are to be processed  combined  and passed to the next node in the tree. this form of forward chaining reasoning mechanism is similar to that of example systems proposed by  winston and horn  1  and has been highly optimized in ops1 forgy  1 . 
   the interpretive traversal of these trees or networks and the determination of the steps required to process each node degrades the performance of the reasoning process. at each node  the reasoning system must determine which of a possible set of steps are required and then perform those steps using information available at the node. once the steps required to process a node have been performed  a determination of which node s  to process next must be made. the performance of knowledge based systems could be greatly improved if the interpretation processes were eliminated and the equivalent steps were performed by concise  compiled code. 
1 	compilation of reasoning 
   the basic approach to knowledge base compilation is to determine the steps that would be taken by an interpretive inference engine during the reasoning process and generate only the code needed to perform those steps. through analysis of the knowledge base  the set of possible steps required to process each node can be significantly reduced  often to a single step. this can then be converted to concise  efficient  procedural code to implement the reasoning process. similarly  the steps that must succeed this processing can also be identified so that code can be generated. using this approach  any interpretive inferencing system can be converted to a compiled system which avoids the interpretive steps. 
   the technique presented here for rule compilation addresses the generation of code for test evaluation  test result propagation  and performance of rule actions. these activities are common to forward chaining  backward chaining  and mixed chaining  combined forward and backward chaining  reasoning processes. since the evaluation and propagation steps are the most frequently used operations in reasoning  their compilation will produce the most benefit in overall performance. methods to improve the performance of selection of facts to evaluate for backward chaining reasoning arc not specifically addressed but the same techniques may be used to improve this process as well. 
   in order to perform rule compilation  the rules must first be converted into a network representation with nodes for each rule component and links indicating the relationships of rule components to other rule components and facts. this is usually the same network that is used by the interpretive inference engine during execution of the knowledge base as in figure 1. in addition  a mechanism must be developed to compile the traversal of the network for the propagation of results in order to preserve the dynamic nature of the knowledge based systems. this mechanism will be presented below. 
   the compilation technique first partitions the network of rules into a set of rule tree subnetworks. although the subnetworks are considered to be trees  they need not be proper trees and may contain references to other subnetworks within the tree. the criteria used for this partitioning is arbitrary with respect to the compilation approach. partitioning the networks on a rule basis provides an easily determined criteria that is adaptable to incremental compilation techniques. to maximize performance  the criteria could select subnetworks that minimize references outside of the subnetwork in order to reduce procedure call overhead. 
parent node in the tree through manipulation of the 
1 	real-time and high performance    next  each rule tree is converted into a rule procedure in which the nodes are represented as parts of a case structure within a while loop controlled by a case index. each part of the case structure consists of a segment of code that performs the function of a node in the rule network generated from the knowledge base. each of the code segments conditionally passes control to its case index. code segments may also pass other information  such as data bindings or certainty factors  to their parents. the use of a tree structure guarantees that only one parent exists for each node in the rule tree for the nominal case eliminating the need for multiway branching and stack mechanism to maintain branch information. if the invocation of parents outside of the rule tree is required  they can be accessed by procedural invocation of the rule procedures that represent them. a stack may be required in situations where multiple data values must be passed to the parents individually. this need may also be eliminated by passing the data values as a list of values rather than as separate items. the use of the while loop around the case structure permits propagation  execution of a series of code segments  to continue as far as possible on each invocation of the rule procedure. it also allows any node in the rule tree to be invoked on demand for additional flexibility in node evaluation  for example  when one part of an element of a structure of data changes . the code for the terminal parent node in this structure must contain logic to identify a rule as concluded'. this can consist of placing an entry in a conflict set to be selected for activation by a conflict resolution mechanism or direct execution of the rules action. 
   finally  data distribution procedures are produced to activate the rule trees based on changes to data items in the knowledge base or other need to evaluate the rule. through analysis of the knowledge base  a list of rule trees and the nodes within those trees can be developed for each data item. from these lists  procedural code can be generated to invoke the appropriate node of each rule tree when the data item is changed. 
   using this implementation  the process of evaluation and propagation is started by the data distribution procedures when a data item is updated. the distribution procedures pass an index value for the node to be evaluated to the rule procedures case structure. this causes selection and evaluation of the desired node. depending on the results of the evaluation  the index is reset to cither the index value of the parent node to be evaluated or to the exit index. the process continues until the rule is concluded or all subconclusions that can be derived from the currently available information are exhausted. 
a simplified example of this process is given in 
figure 1. figure 1a shows a rule that forms a simple knowledge base. figure 1b shows the internally generated rule tree resulting from this rule. figure 1c shows a simplified version of the code resulting from this knowledge base. whenever the values of x or y are changed  the distribute   procedures invoke the rule tree procedure rule1 with the appropriate index value to cause evaluation of the tests. the results then propagate up the tree as far as possible. details of the code to evaluate the a n d condition are dependent on the particular reasoning algorithm being implemented. it could contain logic to test certainty factors in an evidential reasoning approach like f m y c i n  van melle el a/.  1  or it could test for the existence of all inputs and combine them into a binding token in a r e t e pattern matching approach  forgy  1 . the result of this evaluation is used to either pass control to the next node-segment or to terminate processing by making the appropriate setting of the index value. code to manage and pass data values to parent nodes is also reasoning algorithm specific and has been omitted from this example for simplicity. 
1 	discussion 
   this technique generates a set of rule processing and data distribution procedures that are specific to the knowledge base. it combines the specific rules of the knowledge base with the inferencing logic to produce a unique code module. hence  the inference engine is no longer physically separated from the rules and can be optimally merged with the rule code. despite the lack of a physically distinct inference engine  the functionality of the conventional approach  separate inference engine and knowledge base  has been prehighland and iwaskiw 1 
served since the resulting modules take the same actions that the inference engine would have taken. 
   the processing performance gains obtained by the compilation technique depend heavily on the complexity of rule language  amount of interpretation done at execution time by the inference engine  and the underlying algorithms used. the performance gains occur at the points at which selections must be made between alternative paths or processing steps and at points where search or indirect data access must occur. in general  knowledge representation languages which offer a richer set of features will encounter more alternate paths during processing and can be expected to experience greater performance gains. 
   the implementation of rules and the inference engine as procedural code can provide additional efficiencies of implementation beyond the removal of the interpretive overhead. data and knowledge base control blocks can be represented as program variables and referenced directly rather than using indirection  linked lists  or other dynamic structures that require search or more complicated traversal. the data required at each step in the processing can be determined during code gener-
1 	real-time and high performance 
ation and its use  representation  and access highly optimized. expressions used in rule condition tests and action statements can be implemented efficiently in procedural code rather than as complex interpretive structures. interpretive inference engines cannot take advantage of these efficiencies because they must be generalized for all possible knowledge bases and structures. 
   the conversion of knowledge bases into code also improves the embeddability of the resulting system. the compiled knowledge base is compatible with any environment that the language that is generated is compatible with. by using standard compiler building techniques  the generated language can also be easily retargeted. 
   the code generated by the compilation technique will potentially require more storage than the corresponding data structures used by interpretive techniques. this is because the generalized logic used by an interpretive approach is customized and repeated in each node. however  since the code that is generated represents only a small set of the possible actions that could be taken on a node and can be highly optimized  the amount of code generated for each rule will be significantly smaller than that of an interpretive inference engine. this type of trade-off  more storage for lower processing requirements  is typical of processing optimization techniques. since the code is generated from the same representation that is used by the interpretive approach  the amount of storage required will be directly proportional to the amount of storage required by the interpretive form of the knowledge base and combinatorial explosions resulting from rule and data interactions will not occur. the size of the generated code can be further reduced through the proper use of utility procedures to perform logic operations that are invariant across knowledge bases. 
   an alternate approach to knowledge base compilation is to represent the rule network as a set of decision trees which can then be represented by a series of nested if statements. this approach has been used in systems such as rulemaster. 1 iowever  this approach effectively fixes the order of evaluation of the parameters involved  based on their values  and does not provide access to individual nodes on demand or allow for the complexities of pattern matching. since access to individual nodes is not provided  this approach does not support techniques such as dynamic rule ordering based on confidence or forward chaining reasoning. 
   another alternative is partial compilation of the knowledge base. in this approach the conditions and actions of rules are compiled into code but the inference network remains as an interpreted data structure. this technique has been used bv knowledgetool  ibm  1   and etc |drastal et'al.  1 . this approach only improves the performance of condition evaluation and action execution and does not address the issue of rule result distribution which is central to the inferencing process. 
1 	implementation of hiper 
   the high performance embedded reasoning  hiper  system is a prototype knowledge based system development tool that utilizes this knowledge base compilation technology. it consists of a knowledge base compiler for converting a text form of the knowledge base into pl/1 or c code and a development environment to allow the user to create and analyze knowledge bases. 
   the underlying reasoning mechanism of hiper is the forward chaining yes/rete algorithm |schor et al  1| as used in knowledgetool  ibm  1  and ec lps   i b m   1 . the hiper knowledge base 
compiler parses a text form of the knowledge base into a network representation. this representation is then converted into a rete network  which is then traversed to produce procedural code. this code  along with run-time utilities which provide knowledge base independent functions and environment interface software to adapt the knowledge base to different operating environments  is then linked into a single load module for execution. 
   a number of additional features and enhancements have been added to the basic yes/rete algorithm used in hiper. these include: 
breadth-first rete token processing: other implementations of rete algorithm process tokens containing data updates in a depth first manner. the breadth-first modification allows more processing to be performed at each node  cutting down on processing overhead when many objects are matched. it also reduces the need for a stack to hold queued tokens thereby simplifying the code. 
integrated frames: the simple  flat frame  working memory element representation used in most ops derivative languages has been extended to allow frames with inheritance. to resolve the problem of integrating an access oriented frame system with update oriented rete processing  an update oriented frame system is implemented by augmenting the data distribution procedure with franr inheritance processing. 
backward chaining: in order to provide additional inferencing capability the rete processing is augmented with logic to acquire data  goals  and maintain the set of rules that need to be fired in order to attain the goals. this provides a form of backward chaining reasoning. 
1 	results 
   in order to assess the relative performance gains of the compilation approach against other approaches  factors such as the computer hardware  operating environment  implementation language  and underlying reasoning algorithms must be taken into account. eor most knowledge based system development tools these factors are so different that derivation of any meaningful comparisons would be difficult. however ibm's knowledgetool provides a good point of comparison because it is implemented in the same high level language as one of the versions of hiper  pl/i  and uses the same underlying reasoning algorithm. it should be noted  however  that knowledgetool is not a completely interpretive implementation but uses a partially compiled approach that compiles rule tests and actions into pl/i code and interprets the rete processing. the performance advantage provided by the fully compiled approach over a fully interpretive system would be more substantial. 
two benchmarks were executed using hiper and 
knowledgetool version 1. one was a version of the 
monkey and bananas problem that has been used by 
nasa to compare the performance of other expert system building tools  nasa  1 . the other was a version of the maintenance operations center automation  moca  prototype application written for a major airline. the second application represents a 
	highland and iwaskiw 	1 
real-world problem with a large amount of data that performs a difficult scheduling problem. both applications were run with the same rules on the same processor  ibm 1  and utilized timing routines that measured the virtual cpu time from the firing of the first rule to the firing of the last rule in the benchmark scenario. the results of these benchmarks are shown in table 1 and table 1. the virtual cpu number indicate a performance improvement of between 1 and 1 times for the compiled approach over the hybrid approach used in knowledgetool. more storage is required for the compiled approach but these data points indicate the storage requirement to be only on the order of 1% greater than the hybrid approach. this seems to be an acceptable trade-off with respect to the performance gains possible. 
hiper system  a prototype knowledge based system tool that uses this approach  with similar systems. the use of this compilation technique in the hiper system prototype has demonstrated that significant performance gains are possible over partially compiled techniques for only a modest increase in memory requirements. 
acknowledgments 
   the authors would like to thank hugh gallivan  jim tani  renee rochester  and chuck grady for their contributions in the implementation of the hiper system prototype. 
   

	table 1. 	comparative results for m o c a benchmark 
1 	summary and conclusions 
   a technique for the full compilation of knowledge bases has been presented. this technique provides a performance improvement over the conventional interpretive approach that varies depending on the complexity and features of the rule language. this technique is superior to decision tree generation approaches because it allows direct access to each node in the resulting rule networks supporting forward chaining reasoning and allowing the use of techniques such as dynamic rule ordering. it also goes beyond approaches which compile only the condition tests and action statements of rules because it also compiles the inferencing mechanism to provide increased performance and embeddability of the resulting applications. 
   the relative advantages of this approach have been analyzed by comparison of benchmarks coded with the 
1 	real-time and high performance 
