 
in the web  extractor agents process classes of pages  like 'call for papers' pages  researchers' pages  etc   neglecting the relevant fact that some of them are interrelated forming clusters  e.g.  science . we propose here an architecture for cognitive multi-agent systems to retrieve and classify pages from these clusters  based on data extraction. to enable cooperation  two design requirements are crucial:  a  a web vision coupling a vision for contents  classes and attributes to be extracted  to a functional vision  the role of pages in information presentation ;  b  explicit representation of agents' knowledge and abilities in the form of ontologies  both about the cluster's domain and agents' tasks. employing this web vision and agents' cooperation can accelerate the retrieval of useful pages. we got encouraging results with two agents for the page classes of scientific events and articles. a comparison of results to similar systems comes up with two requirements for such systems: functional categorization and a thoroughly detailed ontology of the cluster. 
1 introduction 
although the terms  web information agents  and 
 cooperative information gathering   cig  are in fashion  there arc scarce examples of web systems endowed with some sort of cooperation or integration among tasks related to text processing  viz classification  retrieval  extraction and summarization. when defining the main aspects of cig  oates et al 1   information extraction was mentioned as a key technology that addresses information acquisition in complex environments  suggesting implicitly task integration for cig. 
　indeed  there is room for cooperation  in particular for extractor agents on the web. these systems are being designed to process classes of pages with similar structuring and contents  e.g.  call for papers  scientific articles  ads  etc . however  the relevant fact that many of these classes are interrelated forming clusters  e.g.  
science  has been neglected so far. cooperation among 
guilherme bittencourt 
departamento de automacao e sistemas 
universidade federal de santa catarina 
cx. postal 1  1-1  florianopolis  brazil gb das.ufsc.br 
extractors could increase the retrieval of useful pages  taking advantage of these relations. 
　aiming at task integration and cooperation  we designed an architecture for cognitive multi-agent systems that retrieve and classify pages from clusters of information in the web  extracting slots of data from it. two design requirements are crucial. one is a web vision coupling a vision for contents  classes and attributes to be extracted  to a functional vision  the role of pages in information presentation  i.e.  whether a page is a list  message  class instance or garbage . the other  necessary to enable cooperation  consists of explicit knowledge representation in the form of ontologies  both about the cluster's domain and agents' tasks. 
　experiments held with two agents  that treated the classes of scientific events and articles  produced promising results  leading to two conclusions:  a  functional categorization can be profitable;  b  key requirements for success in web cig systems arc this categorization and a detailed ontology of the cluster. 
　the article is organized as follows: section 1 describes the proposed web vision. section 1 introduces the architecture  its components and many types of reuse enabled. section 1 outlines a case study: master-web  multi-agent system for text extraction  classification and retrieval over the web   applied to the science cluster  with the two agents cited above. the results on contents and functional classifications are also shown. section 1 compares master-web to similar systems. section 1 brings future work and conclusions. 
1 a proposed web vision for cig 
web extractors seek pages that are instances of pages' classes  e.g.  call for papers pages  researchers'  etc . these pages share many common attributes  slots   like date and place of a conference. pages' classes outline a web division by contents. the slots usually found in a class are discriminant  once they help distinguish class members. this fact supports the use of extraction in class categorization. researchers' pages  for instance  usually contain typical data  such as projects  interest areas  etc. 
　most links in classes' pages point to pages containing data from a few other classes. a set of interrelated 

al and the internet 	1 

classes and their relations gather a body of knowledge about a specific domain  e.g.  science  tourism  etc . they form a cluster of classes. in researchers' pages  e.g.  we often find links to papers  calls for papers and other classes from the scientific cluster. 
　another view of the web  based on the work of pirolli et alii   focus on functionality  dividing pages by the role played in linkage and information storage. we separate them into five functional groups: content pages  class members   resource directories  lists of content pages  e.g.  lists of researchers or scientific articles   messages about content pages  recommendations  pages that arc members of other classes  and garbage. 
　joining these two visions  we can accurately identify not only the information to be extracted from page classes  but also instances of relations among classes in a cluster. thus  it can improve the search of useful pages. 
1 proposed architecture for cig 
we propose an architecture for cognitive multi-agent system  mas  to retrieve and extract data from web pages belonging to classes of a cluster. the core idea of employing a mas resides on taking advantage from the relations among classes through agents' cooperation. the architecture overview is illustrated in figure 1. 

figure 1: architecture overview. 
　each agent  represented as a circle in the figure  recognizes  filters and classifies pages  extracting attributes  slots  from them. the pages are supposed to belong to the class of pages that the agent processes. for instance  cfp pages are processed by the cfp agent  and papers pages by an articles' agent  in the science cluster. the mas is based on distributed problem solving  where agents cooperate without overlapping functions. 
　each agent relies on a meta-robot that can be connected to multiple search engines - like altavista  excite  etc. the meta-robot queries the search engines with keywords that assure recall for the agent's page class  e.g.  the terms kcall for papers' and 'call for participation' for the 
cfp agent . due to the lack of precision  the resultant url set from the queries is characterized by a wide variation of functional groups  containing many lists  messages  content pages from its class and from others agents' classes  and garbage. the retrieved urls feeds a low priority queue of urls to be processed. 
　an agent continuously accesses this queue and another one  assigned with high priority  which stores urls sent as recommendations by other agents of the mas or taken from pages considered as lists. these links are considered as  hot hints   because they were found under a safer context. therefore  they arc expected to attain higher precision. cooperation pays off if these suggestions contain less garbage than search engine results do. 
　an agent processes urls by extracting data and classifying the pages functionally and by contents. the extracted data and the results arc kept in a database together. a mediator facilitates database access to users or agents  from the system or external  offering simpler reduced non-normalized database views. 
　when an agent joins the system  it announces itself and sends instances and rules to the other agents. this knowledge will be employed by them on the recognition of links or pages likely to belong to the page class processed by the sender agent. so  the agents update their knowledge bases and send to the new agent  in return  their own recognition instances and rules. when a link or page fires any other agent's recognition rule  the agent sends the link or page to that agent as a recommendation. 
1 agents' tasks 
an agent performs the following steps for each url. figure 1 depicts an agent in detail. 
figure 1: an agent and its processing steps. 
validation. non-html  non-http  inaccessible and pages already stored in the database arc ruled out in this step. 
preprocessing. representation elements - such as contents with and without html tags  centroid  title  links and e-mails  among other elements - are generated from each valid page applying ir techniques  like stop-lists  stemming and tagging. if necessary  shallow natural 

1 	al and the internet 

language representations can be used. the representation elements are passed to the agent's inference engine. 
functional classification. an agent deduces to what functional group the page fits to  whether a list  message  garbage or class member dealt by the agent or by another. 
   it is also here that links in a page arc suggested to other agents when any of their identification rules about the anchor or url fires. for example  an anchor or url with the word  conference  is useful for a cfp agent. suggestions may trigger scanning on directory structure prefixes  like /staff/ for a researchers' agent. these directories usually keeps plenty of content pages. 
extraction and contents classification. these two tasks arc quite correlated; riloff  has already demonstrated the applicability of extraction for filtering. in our architecture  extraction assists classification  since each agent manages many subclasses  e.g.  the cfp agent treats conferences  workshops and journals. after categorizing a page according to its semantic contents  the remaining slots  which weren't discovered yet  arc searched and extracted. in case contradictions or strange facts appear during extraction  the functional class assigned to the page can be modified. for instance  a supposed cfp page is retracted to be a list or garbage  if dates farther than a year are met in the top. 
   using as many representation elements as required  each slot of data is extracted by a union of templates  cases and rules. when a slot is part of a predefined list  it is gleaned by matching terms from the dictionaries  e.g. checking the presence of acronyms to find us states . 
1 agents' knowledge 
ontologies play a central role in the architecture  serving not only as vocabulary in agent messages  but also defining proper concepts  slots  instances  restrictions and relations. the architecture encompasses five ontologies: 
cluster ontology. the main ontology ought to focus on the original domain of the cluster  e.g. science   aiming at a rich and comprehensive representation of its classes  even if they don't appear in the cluster. there are some reasons for this. first  reuse possibilities by any application dealing with the domain are enhanced. moreover  it provides a suitable structure to extract slots. to sum up  a detailed domain ontology assures performance in contents categorization  see section 1 . 
web ontology. classes here contain representations of the pages  chosen according to their adequacy in the required tasks. they comprise http protocol data  information retrieval  ir  page representations  such as terms and frequencies  contents with and without tags  links  e-mails  etc   and natural language processing  nlp  representations  suitable for unstructured classes. 
cig ontology. operational knowledge classes  and instances  for the tasks of functional categorization  contents classification and extraction. it includes: 
  templates to extract slots  e.g. deadline of a cfp   to recognize functional categories  such as garbage  messages  recommendations  etc   and to classify content pages  e.g. conference or workshop cfp ; 
  auxiliary classes like dictionaries concepts  with keywords and synonyms   agents and abilities  etc;   functions to determine regions  extract  convert  format  check consistency and dismiss extracted data; 
  complex and expressive cases specifying conditions  concepts and sets of slots whose presence or absence characterize a content page class. an example below: 
  articles-with-author-org-and-place  of case 
 all-of-concepts-keywords true  
 absent-concepts-in-the-beginning  thesis    
 concepts-in-the-beginning  abstract   
 slots-in-the-beginning  author-name  
	 organization-name  	 location    
　this case detects articles. if  in the beginning of a page  an author name  an organization and a country  or us state  were extracted  terms similar to  abstract  were found  but not terms related to the concept  thesis   like  partial fulfillment    a rule bound to this case is fired. the attribute all-of-concepts-keywords determines whether all the slots have to appear in the page; if it was false only one would be necessary. next  a classrecognizer instance  displayed below  and a rule creates an instance of the class associated with the case. 
   part-publication  of class-recognizer 
 cases  articles-with-author-org-and-place   
 class  part-publication    
the agents first categorize pages into abstract classes 
 which can't have instances  as  in the example  partpublication   to assign later the exact class the pages fit to  e.g. conference-article . cases  recognizers  concepts and rules are also applied on slot extraction and search of links and urls to be suggested to other agents. 
auxiliary ontologies. comprise ontologies for nlp 
 like wordnet  miller 1    time and places  with countries  states  etc   and other specific ontologies from other subject areas  which can be helpful for an agent or domain  like bibliographic data  for the  articles  agent . 
al and the internet 	1 　indeed  a knowledge-based approach is adequate to permit not only high-level intentional cooperation  but also massive reuse of code  search engines' services  db definitions - once agents' tables arc the same  pages not recognized  dictionaries  search engines and their queries   except the particular tablc where an agent stores its extracted data  for instance  cfps to the cfp agent  and knowledge. this latter is the most important reuse and can happen in several ways. ontologies can be downloaded from repositories  like ontolingua. a cluster ontology serves to all agents in a cluster  as well as most knowledge from dictionaries and rules. thus  a new agent is quickly built  needing only new rules and instances of concepts  cases and templates  given that knowledge acquisition has already been accomplished. 
1 case study: the science domain 
a model of agent was developed in java  and a mediator  in delphi and html. we utilized k q m l for agents' communication. ontologies were written using protege1. rules were specified in the inference engine jess1. the search engines altavista  northernlight and goggle were queried for retrieval. nlp wasn't implemented yet. the data to be extracted was identified to support the classifications  but not extracted 
   we reused an ontology of science from the project  ka 1  benjamins et al  1   introducing many refinements. a key one was the creation of abstract classes grouping classes with common features  in order to improve classification. for example  class scientificevent derived abstract subclasses live-sc-event - with concrete subclasses conference and workshop - and sc-
publication-event  with subclasses journal and magazine two agents from the science cluster were developed: 
cfp agent. handles any cfp pages. extraction templates for 1 slots were defined  e.g. place  deadline  editor-in-chief  from 1 scientific events subclasses  the ones already cited plus generic-live-sc-event  genericsc-publication-event and special-issues for journal and magazine   classified by 1 cases and templates. 
scientific articles agent. the search engines were queried with terms that appear often in articles:  abstract    introduction    related work    conclusion    discussion   and others. the agent is responsible for 1 classes: workshop  conference  journal and magazine articles  book chapters and generic articles  as well as thesis  dissertations  technical and project reports. 1 slot templates  1 cases and 1 templates were created. 
   three tests were executed with each agent. the first two treated carpi obtained from the search engines' queries; one was applied for knowledge acquisition  to specify the cases  templates and rules  and the other  for blind test. the third test was made directly from the web. 
1 results 
table 1 displays tests' results. in the table  recognition stands for an evaluation if the agent correctly identified member pages. note that a fourth test was run with cfp agent  in which it benefited from lists of content pages. lists of cfps on a focused subject are usually maintained by researchers  departments and organizations. 
cfp agent. more than 1% of content pages were conference cfps  mostly recognized by the presence  in the top of the pages  of the concept cfp  and the slots initial date and location the few false content pages were very long and about communities of users  linux  x m l   etc   containing many of the slot triggering concepts. the agent failed to recognize cfps with little data or not using the jargon   deadline   etc . lists were detected 
http://protcge.stanford.edu 
1 
http://herzberg.ca.sandia.gov/jess 
when many anchors contained terms related to events  or when lots of time intervals in a page  e.g.  1 december  were present. special care should be taken to lists  once false positives can queue bad links as  hot hints . 

tabic 1: agents' performances. 
　the lists increased substantially the retrieval of content pages. the set of pages retrieved in the fourth test was clearly more focused. pages from other categories  like lists  messages and recommendation to the fictitious organizations agent and dividable-publication agent 
 process proceedings   were substituted by content pages in the form of frames. in the other tests  only one frame appeared. in this test  the number of contents pages rose between 1 and 1%  see figure 1   due to frames pages. 

   even the garbage patterns changed: instead of false content pages  caused by search engines' lack of precision  garbage were graphical  initial pages of cfps. it is also remarkable that some scientific events found in lists were not discovered in the other tests. these facts demonstrate that functional categorization  and  in particular  the use of lists  pays off. 
1 	al and the internet scientific articles agent. recognition errors happened in articles with few data  data in the end  or with affiliations in unknown companies. an article must have at least three of the slots organization-name 
author-name  location  e-mail and department. the agent missed articles containing only the authors' names  but this is normal in other similar systems like citeseer. 
　contents classification was based on information about articles' publication  usually put in the top of papers. more than half of the articles didn't bring this data. 
cooperation. we executed one more test  in which the agents cooperated. the cfp agent asked the articles' agent for anchors in top of papers containing concepts like  conference    journal   etc. the articles' agent transformed these requests into rules to suggest anchors to the cfp agent. although it worked and no suggestion was wrong  only three anchors were sent. 
　to prove the usefulness of cooperation  the cfp agent scanned anchors in program committees seeking for suggestions to a future  researchers  agent. no extraction technique  e.g. proper names  or dictionary was present  only simple heuristics. the agent recommended 1 correct and 1 wrong anchors  a promising rate  given that researchers' pages are less structured and  therefore  harder to be retrieved by search engines queries. 
1 related work and discussion 
many subject areas are involved with this project: information  retrieval  extraction  classification  multiagents  information agents  nlp and ontologies  at least. instead of trying to review these areas  we will focus on comparing and discussing solutions similar to ours. 
1 	webkb 
this system  craven et al  1  integrates extraction and classification as separate tasks  supported by machine learning. pages from computer science departments' sites in universities are classified against a domain ontology previously defined  with classes and relations. the pages were represented using ir techniques. classification and extraction rules were learnt from annotated corpi. 
　the decision of relying on machine learning depends upon some issues. the first one is a comparison between the costs of annotating corpi against inspecting them for knowledge acquisition  as we did. ml brings gains such as speed and adaptability. however  there are drawbacks  like readability and ontological engagement of the learned rules  which tends to be quite specific   difficulties to include a priori knowledge and to capture some rules without adding lots of features  and even to generalize through a huge number of features or classes. 
webkb's ontology was composed of four basic 
classes: activities - with subclasses projects and courses persons - student  faculty and staff -  departments and other. relations among classes were defined  as in our project: project members  advisors  instructors  etc. 
discussion. the authors evaluated contents' classifications only by the false positives  reporting rates between 1 and 1 %  except for the classes  staff member  and  other . nonetheless  if false negatives were computed  the class  other  reaches 1%   student  1% and the remaining 1 classes less than 1%  lowering the average rate to around 1%. 
other drawbacks are the huge incidence in the class 
 other   almost three times the sum of the other classes together  1% of them wrong. this class severely affected performance  since 1% of the mistakes of the tests came from it. anyway  even if the classification was correct  the utility and costs of such system would be questionable  since more than 1% of the pages would have to be categorized as  other . 
　this deluge of garbage pages seems to stem from two facts:  a  the absence of functional categorization  and  b  the role of ontologies was underestimated in webkb  since its ontology doesn't seem to be comprehensive enough. on the contrary  master-wcb's ontology incorporates classes not useful yet  such as projects and products  because agents to process these classes can be available in the future. moreover  current agents apply these definitions to manage their own classes. 
　on the other hand  the learning process may face problems on generalization with an ontology with many classes. much more annotated pages would probably be needed as well. therefore  any solution shall rely on cooperation  with learning or inference agents. 
1 	citeseer and deadliner 
these systems perform efficient retrieval  filtering and extraction over the web  exploiting statistical and learning methods joined with a priori knowledge. 
citeseer. one of the most accessed software in the search of scientific articles  bollacker et al  1 . they are found monitoring mailing lists  newsgroups and publishers  or querying search engines with keywords  papers    publications  and  postscript . an article is recognized by the existence of a reference section. 
　the system extracts bibliographic data from each article and from its reference section  which plays the role of the functional category of list  helping to locate other articles. the number of citations of an article by others represents a measure of its relevance. databases of authors and journals  as well as complex techniques  are applied to identify co-references for authors and articles. 
deadliner. this system  kruger et al 1  searches the web for scientific events. it scans newsgroups and broadcast e-mails  and has its own metarobot. from each cfp  it extracts initial and final date  deadline  program committee  and affiliation of each committee member  themes  events' name and country. extraction is carried out by a sequence of positional filters integrated using automatic learning. extraction performance achieves more than 1% for all the slots. for program committee extraction  the system benefits and updates citeseer's researchers' database. 
recognition performance is similar to ours  more than 
1%   but its definition of event is more restrictive: all the slots are required  except country  plus some submission information. master-web's cfp agent 

al and the internet 	1 

offers more recall and flexibility  accepting announcements of book chapters  magazines  contests  workshops and journals. moreover  the requirements are expressed in cases  which arc much more flexible. cfp agent was designed bearing in mind that users are 
interested in any cfps for their research works. 
discussion. although developed by the same research group  citcsccr and deadliner nowadays are not able to cooperate directly. they handle anchors and pages that interest each other  only cooperating in the construction of the researchers' database. 
　this is a common problem for extractor agents on the web: the simplicity  adaptability and speed of wrapping  learning and finite-state methods prevent these systems to cooperate in an intentional knowledge-based manner. thus  they couldn't ask for data of a certain type  employing  as shared vocabulary for communication  ontologies about the domain  their abilities  desired pages  features among other pieces of knowledge. 
　as the number of web extractors grows  each with its robot collector  strain on bandwidth can be. conversely  cooperation could turn out to be even more profitable than search engines results  constituting the only starting way to look for unstructured classes  like researchers' pages. furthermore  cooperation can take place also at the level of data: the cfp agent can warn a researchers' agent that a certain researcher was chair of an event  a fact that could be missing in her homepage. 
　this stems from the knowledge representation. in statistical and finite-state approaches  knowledge is hidden behind algorithms. it can be neither reasoned nor communicated at knowledge level  blocking cooperation among extractors and integrated extraction. on the other hand  declarative systems can change their behavior dynamically in favor of the cluster's processing  communicating in an elegant manner. another advantage of these systems resides on the fact that  with an approach like citeseer and deadliner to process a new class  most of the new system has to be made from scratch. with our architecture  reuse is massive  and only part of the knowledge has to be acquired and specified. 
1 future work and conclusions 
this project has a long road ahead. new agents for the 
science cluster have to be deployed  like a researchers' agent  to make cooperation actually effective. we must extend our tests to other clusters formed by interrelated page classes  such as a tourism cluster - linking events  hotels  and transport pages. 
　learning and nlp techniques must be accomodated to deal with less structured page classes. the agents should also check data duplicity  and documents written in other formats  like .ps and xml  must be covered too. 
　with this work  we tried to fertilize the field of cig with some new issues and principles  with cooperation as the guiding idea: task integration  declarative inferential extractors capable of intentional cooperation  a web vision coupling functional and contents aspects  mapping of semantic relations among page classes and identification of linkage patterns. a key requirement for cig systems came up with the tests: a detailed ontology is needed to process the richness of a cluster's domain. 
　on the practical side  we designed a fully reusable cognitive mas to process data from whole regions of the web  and got encouraging results from it. we claim that keyword-based search engines can be a basis for more accurate ontology-based domain-restricted cooperative information agents in a near future. 
acknowledgments 
the authors thank tercio sampaio and rafael teske who helped with implementation  and the referees. 
