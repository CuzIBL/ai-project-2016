
we describe an approach to extract attribute-value pairs from product descriptions. this allows us to represent products as sets of such attribute-value pairs to augment product databases. such a representation is useful for a variety of tasks where treating a product as a set of attribute-value pairs is more useful than as an atomic entity. examples of such applications include product recommendations  product comparison  and demand forecasting. we formulate the extraction as a classification problem and use a semi-supervised algorithm  co-em  along with  na： ve bayes . the extraction system requires very little initial user supervision: using unlabeled data  we automatically extract an initial seed list that serves as training data for the supervised and semi-supervised classification algorithms. finally  the extracted attributes and values are linked to form pairs using dependency information and co-location scores. we present promising results on product descriptions in two categories of sporting goods.
1 introduction
retailers have been collecting a large amount of sales data containing customer information and related transactions. these data warehouses also contain product information which is often very sparse and limited. most retailers treat their products as atomic entities with very few related attributes  typically brand  size  or color . this hinders the effectiveness of many applications that businesses currently use transactional data for such as product recommendation  demand forecasting  assortment optimization  and assortment comparison. if a business could represent their products in terms of attributes and attribute values  all of the above applications could be improved significantly.
　suppose  for example  that a sporting retailer wanted to forecast sales of a specific running shoe. typically  they would look at the sales of the same product from the same time last year and adjust that based on new information. now suppose that the shoe is described with the following attributes: lightweight mesh nylon material  low profile sole  standard lacing system. improved forecasting is possible if the retailer is able to describe the shoe not only with a product number  but with a set of attribute-value pairs  such as material: lightweight mesh nylon  sole: low profile  lacing system: standard. this would enable the retailer to use data from other products having similar attributes. while many retailers have recently realized this and are working towards enriching product databases with attribute-value pairs  the work is currently being done completely manually: by looking at product descriptions that are available in an internal database or on the web or by looking at the actual physical product packaging in the store. the work presented in this paper is motivated by the need to make the process of extracting attribute-value pairs from product descriptions more efficient and cheaper by developing an interactive tool that can help human experts with this task.
　the task we tackle in this paper requires a system that can process textual product descriptions and extract relevant attributes and values  and then form pairs by associating values with the attributes they describe. this is accomplished with minimal human supervision. we describe the components of our system and show experimental results on a web catalog of sporting goods products.
1 related work
while we are not aware of any system that addresses the task described in this paper  much research has been done on extracting information from text documents. one task that has received attention recently is that of extracting product features and their polarity from online user reviews. liu et al.  liu and cheng  1  describe a system that focuses on extracting relevant product attributes  such as focus for digital cameras. these attributes are extracted by use of a rule miner  and are restricted to noun phrases. in a second phase  the system extracts polarized descriptors  e.g.  good  too small  etc.  popescu and etzioni  1  describe a similar approach: they approach the task by first extracting noun phrases as candidate attributes  and then computing the mutual information between the noun phrases and salient context patterns. our work is related in that in both cases  a product is expressed as a vector of attributes. however  our work focuses not only on attributes  but also on values  and on pairing attributes with values. furthermore  the attributes that are extracted from user reviews are often different than the attributes of the products that retailers would mention. for example  a review might mention photo quality as an attribute but specifications of cameras would tend to use megapixels or lens manufacturer in the specifications.
　information extraction for filling templates  e.g.   seymore et al.  1; peng and mccallum  1   is related to the approach in this paper in that we extract certain parts of the text as relevant facts. it however differs from such tasks in several ways  notably because we do not have a definitive list of 'template slots' available. recent work in bootstrapping for information extraction using semi-supervised learning has focused on named entity extraction  jones  1; collins and singer  1; ghani and jones  1   which is related to part of the work presented here  classifying the words/phrase as attributes or values or as neither .
1 overview of the attribute extraction system
our system consists of five modules:
1. data collection from an internal database containing product information/descriptions or from the web using web crawlers and wrappers.
1. seed generation  i.e.  automatically creating seed attribute-value pairs.
1. attribute-value entity extraction from unlabeled product descriptions. this tasks lends itself naturally to classification. we chose a supervised algorithm  na： ve bayes  as well as a semi-supervised algorithm  co-em .
1. attribute-value pair relationship extraction  i.e.  forming pairs from extracted attributes and values. we use a dependency parser  minipar   lin  1   as well as co-location scores.
1. user interaction via active learning to allow users to correct the results as well as provide additional training data for the system to learn from.
　the modular design allows us to break the problem into smaller steps  each of which can be addressed by various approaches. we only focus on tasks 1 in this paper and describe our approach to each of the four tasks in greater detail. we are currently investigating the user interaction phase.
1 data and preprocessing
for the experiments reported in this paper  we developed a web crawler that traverses retailer web sites and extracts product descriptions. we crawled the web site of a sporting goods retailer1  concentrating on the domains of tennis and football. we believe that sporting goods is an interesting and relatively challenging domain because the attributes are not easy and straightforward to detect. for example  a baseball bat would have non-obvious attributes and values such as aerodynamic construction  curved hitting surface  etc. the web crawler gives us a set of product descriptions  which we use as  unlabeled  training data. some examples are:
1 rolls white athletic tape
extended torsion bar
audio/video input jack
	vulcanized	latex	outsole	construction	is
lightweight and flexible
　it can be seen from these examples that the entries are not often full sentences. this makes the extraction task more difficult  because most of the phrases contain a number of modifiers  e.g.  cutter being modified both by 1 and by tape. for this reason  there is often no definitive answer as to what the extracted attribute-value pair should be  even for humans inspecting the data. for instance  should the system extract cutter as an attribute with two separate values  1 and tape  or should it rather extract tape cutter as an attribute and 1 as a value  to answer this question  it is important to keep in mind the goal of the system to express each product as a vector of attribute-value pairs  and to compare products. for this reason  it is more important that the system is consistent than which of the valid answers it gives.
　the data is first tagged with parts of speech  pos  using the brill tagger  brill  1  and stemmed with the porter stemmer  porter  1 . we also replace all numbers with the unique token #number# and all measures  e.g.  liter  kg  by the unique token #measure#.
1 seed generation
once the data is collected and processed  the next step is to provide labeled seeds for the learning algorithms to learn from. the extraction algorithm is seeded in two ways: with a list of known attributes and values  as well as with an unsupervised  automated algorithm that extracts a set of seed pairs from the unlabeled data. both of these seeding mechanisms are designed to facilitate scaling to other domains.
　we also experimented with labeled training data: some of the data that we used in our experiments exhibits structural patterns that clearly indicate attribute-value pairs separated by colons  e.g.  length: 1 inches. such structural cues can be used to obtain labeled training data. in our experiments  however  the labeled pairs were not very useful. the results obtained with the automatically extracted pairs are comparable to the ones obtained when the given attribute-value pairs were used. in the case of comparable results  we prefer the unsupervised approach because it does not rely on the structure of the data and is domain-independent.
　we use a very small amount of labeled training data in the form of generic and domain-specific value lists that are readily available. we use four lists: one each for colors  materials  countries  and units of measure. we also use a list of domainspecific  in our case  sports  values: this list consists of sports teams  such as pittsburgh steelers   and contains 1 entries.
　in addition to the above lists  we automatically extract a small number of attribute-value seed pairs in an unsupervised way as described in the following. extracting attributevalue pairs is related to the problem of phrase recognition in that both methods aim at extracting pairs of highly correlated words. there are however differences between the two problems. consider the following two sets of phrases: back pockets  front pockets  and zip pockets vs.
pittsburgh steelers  chicago bears
　the first set contains an example of an attribute with several possible values. the second set contains phrases that are not attribute-value pairs. the biggest difference between the two lists is that attributes generally have more than one possible value but do not occur with a very large number of words. for this reason  two words that always occur together and common words such as the are not good attribute candidates.
　in order to exploit these observations  we consider all bigrams wiwi+1 as candidatesfor pairs  where wi is a candidate value  and wi+1 is a candidate attribute. although it is not always the case that the modifying value occurs  directly  before its attribute  this heuristic allows us to extract seeds with high precision. suppose word w  in position i + 1  occurs with n unique words w1...n in position i. we rank the words w1...n by their conditional probability p wj|w  wj （ w1...n  where the word wj with the highest conditional probability is ranked highest.
　the words wj that have the highest conditional probability are candidates for values for the candidate attribute w. clearly  however  not all words are good candidate attributes. we observed that attributes generally have more than one value and typically do not occur with a wide range of words. for example  frequent words such as the occur with many different words. this is indicated by their conditional probability mass being distributed over a large number of words. we are interested in cases where few words account for a high proportion of the probability mass. for example  both steelers and on will not be good candidates for being attributes. steelers only occurs after pittsburgh so all of the conditional probability mass will be distributed on one value whereas on occurs with many words with the mass distributed over too many values. this intuition can be captured in two phases: in the first phase  we retain enough words wj to account for a part z 1   z   1 of the conditional probability mass
. in the experiments reported here  z was set to 1.
　in the second phase  we compute the cumulative modified mutual information for all candidate attribute-value pairs. we again consider the perspective of the candidate attribute. if there are a few words that together have a high mutual information with the candidate attribute  then we are likely to have found an attribute and  some of  its values. we define the cumulative modified mutual information as follows:
let. then

　λ is a user-specified parameter  where 1   λ   1. we have experimented with several values  and have found that setting λ close to 1 yields robust results. table 1 lists several examples of extracted attribute-value pairs.
　as we can observe from the table  our unsupervised seed generation algorithm captures the intuition we described earlier and extracts high-quality seeds for training the system. we expect to refine this method in the future. currently  not all extracted pairs are actual attribute-value pairs. one typical example of an extracted incorrect pair are first name - last
valueattributecarrying  storagecasemain  racquetcompartmentwelt  side-seam  keypockettable 1: automatically extracted seed attribute-value pairs
name pairs  e.g.  smith is extracted as an attribute as it occurs as part of many phrases and fulfills our criteria  joe smith  mike smith  etc.  after many first names. our unsupervised seed generationalgorithmgets about 1% accuracyin the tennis category and about 1% accuracy in the football category. we have experimented with manually correcting the seeds by eliminating all those that were incorrect. this did not result in any improvementof the final extraction performance  leading us to conclude that our algorithm is robust to noise and is able to deal with noisy seeds.
1 attribute and value extraction
after generating initial seeds  the next step is to use the seeds as labeled training data to train the learning algorithms and extract attributes and values from the unlabeled data. we formulate the extraction as a classification problem where each word or phrase can be classified as an attribute or a value  or as neither . we treat this as a supervised learning problem and use na： ve bayes as our first approach. the initial seed training data is generated as described in the previous section and serves as labeled data which na： ve bayes uses to train a classifier. since our goal is to create a system that minimizes human effort required to train the system  we use semi-supervised learning to improve the performance of na： ve bayes by exploiting large amounts of unlabeled data available for free on the web. gathering product descriptions  from retail websites  is a relatively cheap process using simple web crawlers. the expensive part is labeling the words in the descriptions as attributes or values. we augment the initial seeds  labeled data  with the all the unlabeled product descriptions collected in the data collection phase and use semi-supervised learning  co-em  nigam and ghani  1  with na： ve bayes  to improve attribute-value extraction performance. the classification algorithm is described in the sections below.
1 initial labeling
the initial labeling of data items  words or phrases  is based on whether they match the labeled data. we define four classes to classify words into: unassigned  attribute  value  or neither. the initial label for each word defaults to unassigned. if the unlabeled example does not match the labeled data  this default remains as input for the classification algorithm. if the unlabeled example does match the labeled data  then we simply assign it that label. by contrast  words that appear on a stoplist are tagged as neither.
1 na： ve bayes classification
we apply the training seeds to the unlabeled data in order to assign labels to as many words as possible  as described in the previous section. these labeled words are then used as training data for na： ve bayes that classifies each word or phrase in the unlabeled data as an attribute  a value  or neither.
　the features used for classification are the words of each unlabeled data item  the surrounding 1 words  and their corresponding parts of speech. with this feature set  we capture not only each word  but also its context as well as the parts of speech in its context. this is similar to earlier work in extracting named entities using labeled and unlabeled data  ghani and jones  1 .
1 co-em for attribute extraction
since labeling attributes and values is an expensive process  we would like to reduce the amount of labeled data required to train accurate classifiers. gathering unlabeled product descriptions is cheap. this allows us to use the semi-supervised learning setting by combining small amounts of labeled data with large amounts of unlabeled data. we use the multi-view or co-training  blum and mitchell  1  setting  where each example can be described by multiple views  e.g.  the word itself and the context in which it occurs . it has been shown that such multi-view algorithms often outperform a singleview em algorithm for information extraction tasks  jones  1 .
　the specific algorithm we use is co-em: a multi-view semi-supervised learning algorithm  proposed by  nigam and ghani  1   that combines features from both co-training  blum and mitchell  1  and expectationmaximization  em . co-em is iterative  like em  but uses the feature split present in the data  like co-training. the separation into feature sets we use is that of the word to be classified and the context in which it occurs. co-em with na： ve bayes has been applied to classification  e.g.  by  nigam and ghani  1   but so far as we are aware  not in the context of information extraction. to express the data in two views  each word is expressed in view1 by the stemmed word itself  plus the part of speech as assigned by the brill tagger. the view1 for this data item is a context of window size 1  i.e. up to 1 words  plus parts of speech  before and up to 1 words  plus parts of speech  after the word or phrase in view1. by default  all words are processed into view1 as single words. phrases that are recognized with correlation scores  yule's q  χ1  and pointwise mutual information are treated as an entity and thus as a single view1 data item.
　co-em proceeds by initializing the view1 classifier using the labeled data only. then this classifier is used to probabilistically label all the unlabeled data. the context  view1  classifier is then trained using the original labeled data plus the unlabeled data with the labels provided by the view1 classifier. similarly  the view1 classifier then relabels the data for use by the view1 classifier  and this process iterates for a number of iterations or until the classifiers converge.
　more specifically  labeling a view1 training example using the current view1 classifier is done as follows:
　　　　　p ck|view1i  『 p ck    p view1i|ck  if view1i does not match the labeled training data. other-
wise  the initial labeling is used.
　p ck  is the class probability for class ck  which is estimated using the current labels in the other view. the word probabilities p view1i|ck  are estimated using the current labels in the other view as well as the number of times the view1 data element view1i occurs with each view1 data element. the opposite direction is done analogously. after coem is run for a pre-specified number of iterations  we assign final co-em probability distributions to all pairs as follows:

　it should be noted that even words that are tagged with high probability as attributes or values are not necessarily extracted as part of an attribute-value pair in the next phase. they will generally only be extracted if they form part of a pair  as described below.
1 finding attribute-value pairs
the classification phase assigns a probability distribution over all the labels to each word  or phrase . this is not enough: often  subsequent words that are tagged with the same label should be merged to form an attribute or value phrase. additionally  the system must establish links between attributes and their corresponding values  so as to form attribute-value pairs  especially for product descriptions that contain more than one attribute and/or value. we accomplish merging and linking using the following steps:
  step 1: link if attributes and values match a seed pair.
  step 1: merge words of the same label into phrases if their correlation scores exceed a threshold.
  step 1: link if there is a syntactic dependency from the value to the attribute  as given by minipar  lin  1  .
  step 1: link if attribute and value phrases exceed a colocation threshold.
  step 1: link if attribute and value phrases are adjacent.
  step 1: add implicit attributes material  country  color.
  step 1: extract binary attributes  i.e.  attributes without values  if the attribute phrase appears frequently or if the unlabeled data item consists of only one word.
　the steps described here are heuristics and by no means the only approach that can be taken to linking. we chose this order to follow the intuition that pairs should be linked first by known pairs  then by syntactic dependencies which are often indicative  and if those two fail  then they should finally be linked by co-location information as a fallback. colocation information is clearly less indicative than syntactic dependencies  but can cover cases that dependencies do not capture.
　in the process of establishing attribute-value pairs  we exclude words of certain parts of speech  namely most closedclass items such as prepositions and conjunctions.
　in step 1  the system 'extracts' information that is not explicit in the data. the attributes  country  color  and/or material  are added to any existing attribute words for this value if the value is on the list of known countries  colors  and/or materials. assigning attributes from known lists is an initial approach to extracting non-explicit attributes. in the future  we will explore this issue in greater details.
　after all seven pair identification steps  some attribute or value phrases can remain unaffiliated. some of them are valid attributes with binary values. for instance  the data item imported is a valid attribute with the possible values true or false. we extract only those attributes that are single word data items as well as those attributes that occur frequently in the data as a phrase.
1 evaluation
in this section  we present evaluation results for experiments performed on tennis and football categories. the tennis category contains 1 unlabeled data items  i.e.  individual phrases from the lists in the product descriptions   the football category 1 items. unsupervised seed extraction resulted in 1 attribute-value pairs for the tennis category and 1 pairs for football.
　table 1 shows a sample list of extracted attribute-value pairs  i.e.  the output of the full system   together with the phrases that they were extracted from.
full exampleattributevalue1/1-inch polycotton blend tapepolycotton blend tape1/1-inch1 roll underwrapunderwrap1 roll1 tape cuttertape cutter1extended torsion barbartorsionsynthetic leather upper#material# upperleathermetal ghillies#material# ghilliesmetaladiwear tough rubber outsolerubber outsoleadiwear toughimportedimported#true#dual-density padding with kinetofoampaddingdual-densitycontains	1	bioflex concentric circle magnetbioflex concentric circle magnet1% nylon  1% spandex#material#1%	nylon
1% spandex1-second	start-up time delaystart-up time delay1-secondtable 1: examples of extracted pairs for system run with co-
em
　we ran our system in the following three settings to gauge the effectiveness of each component: 1  only using the automatically generated seeds and the generic lists  'seeds' in the tables   1  with the baseline na： ve bayes classifier  'nb'   and 1  co-em with na： ve bayes  'coem' . in order to make the experiments comparable  we do not vary pre-processing or seed generation  and keep the pair identification  linking  steps constant as well.
　the evaluation of this task is not straightforward since people often do not agree on what the 'correct' attribute-value pair should be. consider the example audio/jpeg navigation menu. this phrase can be expressed as an attribute-value pair in multiple ways  e.g.  with navigation menu as the attribute and audio/jpeg as the value  or with menu as the attribute and audio/jpeg navigation as the value. for this reason  we give partial credit to an automatically extracted attribute-value pair  even if it does not completely match the human annotation.
1 precision and recall
whenever the system extracts a partially correct pair for an example that is also given by the human annotator  the pair is considered recalled. the results for this metric can be found in table 1.
seedsnbcoemrecall tennis111recall football111table 1: recall for tennis and football categories
　recall differs greatly between system settings. more specifically  co-em results in recalling a much larger number of pairs  whereas seed generation and na： ve bayes yield relatively poor recall performance. seed generation was not expected to yield high recall  as only few pairs were extracted. in addition  the results show that co-em is more effective at extrapolating from few seed examples to achieve higher recall than na： ve bayes.
　in order to measure precision  we evaluate how many automatically extracted pairs match manual pairs completely  partially  or not at all. the percentage of pairs that are fully or partially correct is useful as a metric especially in the context of human post-processing: partially correct pairs are corrected faster than completely incorrect pairs. table 1 lists results for this metric for both categories. the results show no conclusive difference between the three systems: co-em achieves a higher percentage of fully correct pairs  while the other two result in a total higher percentage of fully or partially correct pairs.
seedsnbcoem% fully correct tennis111% full or part correct tennis111% incorrect tennis111%	fully	correct
football111% full or part correct football111% incorrect foot-
ball111table 1: precision for tennis and football categories
　currently  however  our system places higher importance on recall than on precision. furthermore  co-em results in reasonable levels of precision  and therefore offers a the best balance of the proposed algorithms. the f-1 measure  as shown in table 1  supports this intuition.
seedsnbcoemf-1 tennis111f-1 football111table 1: f-1 measure for tennis and football categories
t nwt wf nwf w% fully correct1111% flip to correct1111% flip to partially correct1111% partially correct1111% incorrect1111table 1: non-weighted and weighted precision results for tennis and football categories. 't' stands for tennis  'f' is football  'nw' non-weighted  and 'w' is weighted
1 precision results for most frequent data items
as the training data contains many duplicates  it is more important to extract correct pairs for the most frequent pairs than for the less frequent ones. in this section  we report precision results for the most frequently data items. this is done by sorting the training data by frequency  and then manually inspecting the pairs that the system extracted for the most frequent 1 data items. this was done only for the system run that includes co-em classification. we report precision results for the two categories  tennis and football  in two ways: first  we do a simple evaluation of each unique data item. then we weight the precision results by the frequency of each sentence. in order to be consistent with the results from the previous section  we define five categories that capture very similar information to the information provided above. the five categories contain fully correct and incorrect. another category is flip to correct  meaning that the extracted pair would be fully correct if attribute and value labels were flipped. flip to partially correct refers to pairs that would be partially correct if attribute and value were flipped. finally  we define partially correct as before. table 1 shows the results.
　a final comment on the current performance of the system: while there is room for improvement in the results  in reality the automated system provides a vast improvement over the current 'state of the art': as we mentioned above  the real baseline  as it is being performed by businesses today  is the manual extraction of attribute-value pairs.
1 conclusions and future work
we describe an approach to extract attribute-value pairs from product descriptions that requires very little user supervision. we start with a novel unsupervised seed generation algorithm that has high precision but limited recall. the supervised and especially the semi-supervised algorithm yield significantly increased recall with little or no decrease in precision using the automatically generated seeds as labeled data.
　future work will focus on adding an interactive step to the extraction algorithm that will allow users to correct extracted pairs as quickly and efficiently as possible. we are experimenting with different active learning algorithms to use in the interactive step to minimize the number of corrections required to improve the system. one of the main challenges with an interactive approach is to make co-em efficient enough that the time of the user is optimized.
　we believe that a powerful attribute extraction system can be useful in a wide variety of tasks  as it allows for the normalization of products as attribute-value vectors. this enables fine-grained comparison between products and can improve a variety of applications such as product recommender systems  price comparison engines  demand forecasting  assortment optimization and comparison systems.
