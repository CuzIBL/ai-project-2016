 
this paper provides a search-based algorithm for computing prior and posterior probabilities in discrete bayesian networks. this is an  anytime  algorithm  that at any stage can estimate the probabilities and give an error bound. whereas the most popular bayesian net algorithms exploit the structure of the network for efficiency  we exploit probability distributions for efficiency. the algorithm is most suited to the case where we have extreme  close to zero or one  probabilities  as is the case in many diagnostic situations where we are diagnosing systems that work most of the time  and for commonsense reasoning tasks where normality assumptions  allegedly  dominate. we give a characterisation of those cases where it works well  and discuss how well it can be expected to work on average. 
1 	introduction 
this paper provides a general purpose search-based technique for computing posterior probabilities in arbitrarily structured discrete1 bayesian networks. 
　implementations of bayesian networks have been placed into three classes  pearl  1; henrion  1 : 
1. exact methods that exploit the structure of the network to allow efficient propagation of evidence 
 pearl  1; lauritzen and spiegelhalter  1; jensen ct a/.  1 . 
1. stochastic simulation methods that  give estimates of probabilities by generating samples of instantiations of the network  using for example monte carlo techniques  see  henrion  1  . 
1. search-based approximation techniques that search through a space of possible values to estimate probabilities. 
　at one level  the method in this paper falls into the exact class; if it is allowed to run to completion  it will have computed the exact conditional probability in a bayesian network. it  however has the extra feature that it can be stopped before completion to give an answer  with a known error. under certain distribution assumptions 
* scholar  canadian institute for advanced research. 
　　all of the variables have a discrete  and here even finite  set of possible values. 
1 	knowledge representation 
 section c  it is shown that convergence to a small error is quick. 
   while the efficient exact methods exploit aspects of the network structure  we instead exploit extreme probabilities to gain efficiency. the exact methods work well for sparse networks  e.g.  are linear for singly-connected networks  pearl  1    but become inefficient when the networks become less sparse. they do not take the distributions into account. the method in the paper uses no information on the structure of the network  but rather has a niche for classes of problems where there are  normality  conditions that dominate the probability tables  see section 1 . the algorithm is efficient for these classes of problems  but becomes very inefficient as the distributions become less extreme. this algorithm should thus be seen as having an orthogonal niche to the algorithms that exploit structure for efficiency. 
1 	background 
1 	probability 
in this section we give a semantic view of probability theory1 and describe the general idea behind the search method. in some sense the idea of this method has nothing to do with bayesian networks - we just have to commit to some independence assumptions to make the algorithm more concrete. 
   we assume we have a set of r a n d o m variables  written in upper case . each random variable has an associated set of values; vals x  is the set of possible values of variable x. values are written in lower case. an atomic proposition is an assignment of a value to a random variable; variable x having value c is written as x = c. each assignment of one value to each random variable is associated with a possible w o r l d . let  be the set of all possible worlds. associated with a possible world w is a measure with the constraint that 
1
　　this could have also been presented as joint distributions  with probabilistic assignments to the possible worlds corresponding to joint distributions. if that view suits you  then please read 'possible worlds1 as 'elementary events in a joint distribution'  pearl  1  p. 1 . 

　associated with the bayesian network are conditional probability tables which gives the marginal probabilities of the values of xi  depending on the values of its parents 
1
　　this search tree is the same as the probability tree of  howard and matheson  1l  and corresponds to the semantic trees used in theorem proving  chang and lee  1  section 1   but with random variables instead of complementary literals. 
	poole 	1 

　partial description  v1          vj  corresponding to the variable assignment 

　there is a one to one correspondence between leaves of the tree and possible worlds  or complete assignments to the variables . 
we associate a probability with each node in the tree. 
the probability of the partial description  v1  ... vj  is the probability of the corresponding proposition: 

this is well defined as  due to our variable ordering  all of the parents of each variable has a value in the partial description. 
　the following lemma can be trivially proved  and is the basis for the search algorithm. 
lemma 1 the probability of a node is equal to the sum of the probabilities of the leaves that are descendents of the node. 
　this lemma lets us bound the probabilities of possible worlds by only generating a few of the possible worlds and placing bounds on the sizes of the possible worlds we have not generated. 
1 	searching the search tree 
to implement the computation of probabilities  we carry out a search on the search tree  and generate some of the most likely possible worlds. there are many different search methods that can be used  pearl. 1 . 
　figure 1 gives a generic search algorithm that can be varied by changing which element is chosen from the queue. there is a priority queue q of nodes  and a set w of generated worlds. we remove a node  e.g.  the most likely ; either it is a leaf  if j = n  in which case it is added to w or else its children are added to q. 
　note that each partial description can only be generated once. there is no need to check for multiple paths or loops in the search. this simplifies the search  in that we do not need to keep track of a closed list or check whether nodes are already on the open list  q in figure 1   pearl  1 . 
　no matter which element is chosen from the queue at each time  this algorithm halts and when it halts w is the set of all tuples corresponding to possible worlds. 
1 	estimating the probabilities 
if we let the above algorithm run to completion we have an exponential algorithm for enumerating the possible 
1 	knowledge representation 
worlds that can be used for computing the prior probability of any proposition or conjunction of propositions. this is not  however  the point of this algorithm. the idea is that we want to stop the algorithm part way through  and determine any probability we want to compute. 
　we use w  at the start of an iteration of the while loop  as an approximation to the set of all possible worlds. this can be done irrespective of the search strategy used. 
1 	prior probabilities 
suppose we want to compute p g . at any stage  at the start of the while loop   the possible worlds can be divided into those that are in w and those that will be generated from q. 



what is interesting about this is that the error is independent of g. thus when we are generating possible worlds for some observation  and want to have posterior estimates within some error  we can generate the required possible worlds independently of the proposition that we want to compute the probability of. 
1 	search strategies 
the above analysis was independent of the search strategy  i.e.  independent of which element we remove from the queue . 
　we can carry out various search strategies  to enumerate the most likely possible worlds. 	for example  that we need to consider exponentially many explanations to cover any fixed proportion of the probability mass. 
　this is not always a reasonable distribution assumption  for example  when using this for diagnosis of a system that basically works we would like to assume that the underlying distribution is such that there is one assignment of values  the  normal values   that dominates the probability mass. this also may be appropriate for commonsense reasoning tasks where normality assumptions  allegedly  dominate  i.e.  we assume that abnormality is rare  mccarthy  1  . 
　for our analysis we assume that we have extreme probabilities for each conditional probability given. for each value of the parents of variable a   we assume that one of the values for xi is close to one  and the other values are thus close to zero. those that are close to one we call normality values; those that are close to zero we call faults: 

 poole  1  discusses a multiplicative version of a* pearl  1   with conflicts used to refine a heuristic function. 
　in this paper the search strategy we use is where the element of the queue with highest prior probability is chosen at each stage. this paper does not study various search strategies  but analyses one. i make no claim that this is the best search strategy  see section 1   but it forms a base case with which to compare other strategies. 
1 	complexity 
the problem of finding the posterior probability of a proposition in a bayesian network is np hard  cooper  1 . thus we should not expect that our algorithms will be good in the worst case. our algorithm  when run to completion  is exponential in computing the exact prior and posterior probability of a hypothesis. 
　because of the the  anytime  nature of our algorithm  which trades search time for accuracy  we should not consider run time independently of error. it is interesting to estimate how long it takes on average to get within some error  or how accurate we can expect  or guarantee as as asymptotic behaviour  to be within a certain run time. 
　as we have probabilities it is possible to carry out an average case complexity analysis of our algorithm. 
　if we make no assumptions about the probability distributions  the average case of finding the most likely explanation or prior probability within error  is exponential in the size n of the bayesian network  provan  1 . this can be seen by noticing that the size of complete descriptions are linear in n and so the probability of explanations is exponentially small in n. this means 
	poole 	1 


1 	knowledge representation 


1 	refinements 
there are a number of refinements that can he carried out to the algorithm of figure 1. some of these are straightforward  and work well. the most straightforward refinements are: 
　if we are trying to determine the value of .  we can stop enumerating the partial descriptions once it can be determined whether a is true in that partial description. when conditioning on an observation we can prune any partial description that is inconsistent with the observation. 
　we do not really require that we find the most likely possible worlds in order  as we are just summing over them anyway. one way to improve the algorithm is to carry out a depth-first depth-bounded search. we can guess the probability of the least likely possible world we will need to generate  use this as a threshold and carry out a depth-first search pruning any partial description with probability less that this threshold. if the answer is not accurate enough  we decrease the threshold and try again. this is reminiscent of iterative deepening a*  korf  1   but we can decrease the bound in larger ratios as we do not have to find the most likely possible world. 
　we can use conflicts  de kleer  1  to form a heuristic function for a multiplicative version of a*  poole  1 . 
　see  poole  1  for a prolog implementation that incorporates these refinements. 
1 	comparison with other systems 
the branch and hound search is very similar to the candidate enumeration of de kleer's focusing mechanism  de kleer  1l . this similarity to a single step in de kleer's efficient method indicates the potential of the search method. he has also been considering circuits with thousands of components  which correspond to bayesian networks with thousands of nodes. it seems to be very promising to combine the pragmatic efficiency issues confronted by de kleer  with the bayesian network representation  and the error bounds obtained used in this paper. 
　poole  1a  has proposed a prolog-like search approach that can be seen as a top-down variant of the bottom-up algorithm presented here. it is not as efficient as the one here. even if we consider finding the single most normal world  the algorithm here corresponds to forward chaining on definite clauses  see  poole  1b    
which can be done in linear time  but backward chaining has to search and takes potentially exponential time. the backward chaining approach seems more suitable however when we have a richer language  poole  1b . 
　d'ambrosio  has a backward chaining search algorithm for  incremental term computation   where he has concentrated on saving and not recomputing shared structure in the search. this seems to be a very promising approach for when we do not have as extreme probabilities as we have assumed in this paper. 
　shimony and chamiak  have an algorithm that is a backward chaining approach to finding the most likely possible world. the algorithm is not as simple as the one presented here  and has worse asymptotic behaviour  as it is a top-down approach -- see above . it has not been used to find prior or posterior probabilities  nor has the average-case complexity been investigated. 
　this paper should be seen as a dual to the top-n algorithm of henrion . we have a different niche. we take no account of the noisy-or distribution that henrion concentrates on. 
1 	conclusion 
this paper has considered a simple search strategy for computing prior and posterior probabilities in bayesian networks. it is a general purpose algorithm  that is always correct  and has a niche where it works very well. we have characterised this niche  and have given bounds on how badly it can be expected to perform on average. how common this niche is  is  of course  an open question  but the work in diagnosis and nonmonotonic reasoning would suggest that reasoning about normality is a common task. 
a 	proofs 

	poole 	1 

acknowledgements 
thanks to andrew csinger  michael horsch  runping qi and nevin zhang for valuable comments on this paper. this research was supported under nserc grant ogp1  and under project b1 of the institute for robotics and intelligent systems. 
