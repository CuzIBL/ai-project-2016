
predictive state representation  psr   proposed by  littman et al.  1; singh et al.  1   are a general representation for controlled dynamical systems. we present a sufficient condition under which a linear psr compresses a pomdp representation.
1 introduction
efficient decision making under uncertainty requires ignoring irrelevant details of a complex dynamical system and focusing on useful abstractions. a traditional representation for stochastic dynamical systems is provided by partially observable markov decision processes  pomdps . an attractive alternative to pomdps is the predictive state representation  psr   introduced in  littman et al.  1  and further developed in  singh et al.  1 . psrs can be used to represent a larger class of dynamical systems than pomdps. even for systems that can be represented as pomdps  psrs hold the promise of a more compact representation. in particular  littman et al  1  show that a psr representation is no larger than the number of states in a pomdp. we point out a special case of their linear psr representation  in which a strict reduction in the number of states is obtained. we believe this special case is interesting because it relies only on the state dynamics  without taking into account the observations. this can be potentially attractive  especially for robotics applications  in which states can have similar dynamics but sensor reading will often be different.
1 preliminaries
a pomdp representation of a dynamical system includes the following components: a finite unobservable state space s; a finite action space a; a finite observation space o; a transition function t : s¡Áa¡Ás ¡ú r   t s a s¡ä = p st+1 = s¡ä|st = s at = a ; an observation function o : s¡Áa¡Áo ¡ú r   o o s a = p ot+1= o|st+1= s at = a ; an initial belief state b1  which is a vector of size |s|  giving the initial probability of the system being in each underlying state; and a reward function r : s¡Áa ¡ú r   where r s a  is the immediate expected reward. we will not be concerned here with rewards  we will consider just actions and observations.
¡¡we denote by ta an |s|¡Á|s| matrix containing state transition probabilities for action a. we denote by oao a diagonal matrix of size |s|¡Á|s|  in which the diagonal elements correspond to the probabilities of emitting o from each state  given that the state is reached by action a. a test is defined as an ordered sequence of action-observation pairs t = a1...an 1on  which can happen from the current time step into the future. the conditional probability of test t given a prior history h of action-observation pairs is p t|h = p o1...on|h a1...an 1 .
¡¡a set of tests q = {q1...qm} constitutes a linear psr if  for any history  the probability of any other test t can be computed as a linear combination of the predictions for the tests in q. in other words  for any test t there exists a vector mt of size |q|  such that p t|h = p q|h tmt  h.
¡¡littman et al. also define an outcome function u mapping tests into n-dimensional vectors defined recursively by: u ¦Å  = 1|s| and u aot =  taoa ou t   where ¦Å represents a null test and en is the  1 ¡Á n  vector of all 1s. each component ui t  indicates the probability of the test t when its sequence of actions is applied from state i. a set of tests q is called linearly independent if the outcome vectors of its tests u q1  u q1  ...u qm  are linearly independent.
¡¡psrs are related to pomdps through the state-test prediction matrix u  littman et al.  1 . the rows of u correspond to states in s and columns correspond to all possible tests in order of increasing length. the entry uij is the conditional probability of the jth test given that the state of the system is i. a linear psr can be derived from the matrixu by searching for a maximal set of linearly independent columns of u. following the definition of outcome function 
       uij = u tj|si = ta1oa1...tam 1oam 1om i where tj = a1...an 1on. the maximum number of linearly independent columns is the rank of u. therefore the size of q is upper-bounded by the number of states. next section presents a special case in which |s| is strictly greater than |q|.
1 linearly dependent states
we say that a state i is linearly dependent on a subset of states s¡ä   s if and only if its transition probabilities under any action a are a linear combination of the transition probabilities of states in s¡ä under the same action:
tsa = ¡Æ cktka a ¡Ê a 
k¡Ês¡ä

figure 1: example of a pomdp with linearly dependent states  and the model reduced by a linear psrwhere ¡Æk¡Ês¡ä ck = 1 and there exists k such that ck 1= 1. note that the coefficients ck will be used to weigh the transitions corresponding to state k for all of the actions. hence  these coefficients have to be shared across the actions.
¡¡theorem: if the underlying mdp of a given pomdp has a linearly dependent states  then a linear psr will provide a compression of the state space.
¡¡proof: suppose there exists a state i ¡Ê s which is linearly dependent on a set of states s¡ä  so:
 a ¡Ê a : tia =¡Æcktka
k
¡¡to prove that the ith row of u is a linear combination of the other rows for all possible tests  we proceed by induction. consider first one-step tests. let oa be a matrix of size |s|¡Á |o|  giving the probabilities of different observations being emitted from each state  after action a is taken. then  by taking transposes and multiplying the above equation  we get:
 tia toa = ¡Æcktka toa toa
	k	k
¡¡note that this correspondsto the part of the ith row in theu matrix which contains the observations for all one-step tests for action a.
¡¡now suppose that we have established for all tests t of length l that the outcome of t in state i can be written as a linear combination of the outcomes of states k:
ui t = ¡Æ ckuk t 
k¡Ês¡ä
consider a test aot of length l +1. we have: ui aot  = tiaoaou t = ¡Æ cktka oaou t 
k¡Ês¡ä
= ¡Æ ck tkaoaou t  = ¡Æ ckuk aot 
	k¡Ês¡ä	k¡Ês¡ä
¡¡hence  the ith row of the u matrix is a linear combination of the rows corresponding to the states in s¡ä  with the same mixing coefficients as those from the transition matrix. therefore  the rank of u is strictly less than |s|. since the dimension of the linear psr is given by the rank of u  in this case the linear psr representation will be smaller than the size of the state space.  
¡¡an example of a system with 1 states  1 observations and two actions  in which a simple linear dependence can be seen  is provided in figure 1. the model corresponding to action a is in the upper left while the model for action b is in the upper right. the solid lines represent state transitions and the dashed lines represent emission probabilities. for action a  from state s1 the system transitions to one of the other states with equal probability. these states return deterministically to s1. under action b  from all the bottom states the system transitions with probability 1 to state s1. the observations can be assumed to have all the same probability  although this does not really matter for the example. in this case  states s1  s1 and s1 are all linearly dependent on s1  they have the same transition probabilities . however  note that they do not have the same observation models. the bottom row presents a simplified system  which has been reduced to two states by eliminating the linearly dependent states. the models for action a and b are in the left and right column. the emission probabilities are all equal when more than one observation is emitted from a state. in general  these probabilities would be computed by averaging the emission probabilities for the states in the original model.
¡¡note that this theorem relates only linearly dependent state transitions to psr compression without considering the observations. in fact  more compression can be obtained if the observations are taken into account. a good example of this sort is the pomdp coffee domain used in  poupart and boutilier  1 . this problem has 1 states  1 actions and 1 observations. four of these states are linearly dependent on the other ones  which means that the dimensionality can be reduced to 1. however  by running the linear psr construction algorithm  a more significant reduction is possible: the problem can be represented with just two tests. the value-directedcompressionmethodof  poupartand boutilier  1  takes advantage of the same regularities as linear psrs  but also of regularities in the reward function.
