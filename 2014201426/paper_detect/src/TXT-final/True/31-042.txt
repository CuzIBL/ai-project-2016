 
reasoning with model-based representations is an intuitive paradigm  which has been shown to be theoretically sound and to possess some computational advantages over reasoning with formula-based representations of knowledge. in this paper we present more evidence to the utility of such representations. 
in real life situations  one normally completes a lot of missing  context  information when answering queries. we model this situation by augmenting the available knowledge about the world with context-specific information; we show that reasoning with model-based representations can be done efficiently in the presence of varying context information. we then consider the task of default reasoning. we show that default reasoning is a generalization of reasoning within context  in which the reasoner has many  context  rules  which may be conflicting. we characterize the cases in which model-based reasoning supports efficient default reasoning and develop algorithms that handle efficiently fragments of reiter's default logic. in particular  this includes cases in which performing the default reasoning task with the traditional  formula-based  representation is intractable. 
further  we argue that these results support an incremental view of reasoning in a natural way. 
1 	introduction 
the generally accepted framework for studying reasoning in intelligent systems is the knowledge-based system approach. the idea is to store the knowledge in some representation language with a well defined meaning as-
signed to its sentences. the sentences are stored in a knowledge base  kb  which is combined with a reasoning mechanism that can be used to determine what 
　* research supported by center for intelligent control systems  aro contract daal1-g-1. 
   research supported by nsf grant ccr-1 and by darpa afosr-f1-j-1. 
can be inferred from the sentences in the kb. there are many knowledge representations that can be used to represent the knowledge in a knowledge-based system. different representation systems  e.g.  a set of logical rules  a probabilistic network  are associated with corresponding reasoning mechanisms  each with its own merits and range of applications. given a logical knowledge base  for example  reasoning can be abstracted as a deduction task: determine whether a sentence  assumed to capture the situation at hand  is logically implied by the knowledge base. in all cases  the emphasis of this approach is on comprehensibility  mccarthy and hayes  1; pearl  1 : knowledge should be encoded so that it is readily accessible. 
　it is widely acknowledged today that a large part of our everyday reasoning involves arriving at conclusions that are not entailed by our  theory  of the world. many conclusions are derived in the absence of information that is sufficient to imply them. this type of reasoning is naturally non-monotonic since further evidence may force us to revise our conclusions. within the knowledge-based systems approach this situation is handled by theories for reasoning with  defaults   see e.g.  reiter  1a  . the true knowledge about the world is augmented by a set of default rules that capture only  typical  cases. the quest is for a reasoning system that  given a query  responds in a way that agrees with what we know about the world and the default assumptions and at the same time supports our intuition about a plausible conclusion. 
　computational considerations  however  render this self-contained approach to reasoning inadequate for cornmonsense reasoning. this is true not only for the task of deduction  but also for many other forms of reasoning which have been developed. all those were shown to be even harder to compute than the original formulation  selman  1; roth  1 . of particular interest in this context are the results on default reasoning tasks  selman and kautz  1; kautz and selman  1; papadimitriou  1   where the increase in complexity is clearly at odds with the intuition that reasoning with defaults should somehow reduce the complexity of reasoning. this remains true  even when we severely restrict the expressivity of the knowledge base  the default rules and the queries allowed. for example  when the knowledge base is horn  all the default rules are positive 
	khardon and roth 	1 
literals  and the query is a single positive literal  the default reasoning task is np-hard  selman and levesque  1 . this should be contrasted with the case of deductive reasoning  where horn theories are distinguished by the existence of linear time satisfiability algorithms. 
　an alternative approach to the study of commonsense reasoning is developed in  kautz et a/.  1; khardon and roth  1c . there  the knowledge base is represented as a set of models  satisfying assignments  of the world rather than a logical formula describing it. it is not hard to motivate a modelbased approach to reasoning from a cognitive point of view and indeed  most of the proponents of this ap roach to reasoning have been cognitive psychologists johnson-laird  1; johnson-laird and byrne  1; kosslyn  1   who have alluded to the notion of  reasoning from examples  on a qualitative basis. in the ai community this approach can be seen as an example of levesque's notion of  vivid  reasoning  levesque  1; 1   and is somewhat related to minsky's framestheory  minsky  1 . 
　given a model-based representation of the knowledge base kb and a query a  the deduction task kb  = a can be answered in a straightforward way: evaluate a on all the models in the representation. if you find a model of kb which does not satisfy a  then kb not= a  otherwise conclude kb = a. clearly  if the model-based representation contains all the models of kb this approach yields correct deduction  but representing kb by explicitly holding all the possible models is not plausible. a model-based approach becomes feasible if kb can be replaced by a small model-based representation and still support correct deduction. 
　the theory of model-based representations developed in  khardon and roth  1c   generalizing the theory developed in  kautz et a/.  1  for the case of horn expressions  characterizes the propositional languages for which model-based representations support efficient deduction and abduction. it is shown that in many cases in which the deduction and abduction tasks are np-hard in the formula-based setting  the model-based representation is small  polynomial in the number of propositional variables in the domain   and reasoning with it yields correct and efficient reasoning algorithms. 
　in this paper  we extend the work presented in  khardon and roth  1c  and present some more computational advantages of reasoning with model-based representations. as a basic computational task we consider the problem of reasoning within a varying context. in real life situations  one normally completes a lot of missing context information when answering queries  levesque  1 . we model this situation by augmenting the knowledge we have about the world with contextspecific information. reasoning within context is therefore a deduction task  where some additional constraining information is added to the knowledge base. we show how to solve this task efficiently using a model-based representation  for a variety of propositional languages as context information. 
we then consider the task of default reasoning. there  
1 	automated reasoning 
given a representation of the world  a set of  sometimes conflicting  default rules and an assertion q  one is trying to asses whether q can be concluded  by default  from the available information. we show that default reasoning is a generalization of reasoning within context  in which the reasoner has many context rules  which may be conflicting. we provide an efficient algorithm for the default reasoning task  for various classes of world knowledge  default rules and queries  based on the algorithm developed for reasoning within context. 
　as in the case of deductive and abductive reasoning  khardon and roth  1c   we present an efficient default reasoning algorithm for cases where the formula based reasoning is hard. for example  in contrast to the hardness result mentioned above  we show that if the knowledge base is any propositional language with a polynomial size dnf1  the default rules are arbitrtry monotone functions and the query is a horn query  the default reasoning task can be solved correctly and efficiently. 
　equally important for the plausibility of model based reasoning is the view that it suggests about reasoning. while we do not consider in the paper the question of how the knowledge base is acquired  this issue is clearly an important one  and the plausibility of any theory for reasoning hinges on it. it is important therefore to mention that it has been shown  within the learning to reason framework  khardon and roth  1b; 1   that model-based representations that are suitable for the reasoning tasks considered here can be learned efficiently. the model based approach to default reasoning can therefore be incorporated within an inductive setting. the model based representation can be efficiently learned  context specific default rules can be acquired in various learning processes  and these can be combined to work together in a plausible and efficient way. furthermore  we show how knowledge available within a specific context can be used to reason within this context. therefore  our treatment of reasoning within context supports the view that an intelligent agent constructs a representation of the world incrementally by pasting together many  narrower  views from different contexts. 
　the inductive nature of non-monotonic reasoning is also at the heart of the approach developed in  valiant  1; roth  1   where a different view on dealing with incomplete information is taken. 
1 	preliminaries 
we consider problems of reasoning where the  world  is modeled as a boolean function w : {1}  -  {1}. we use interchangeably the terms propositional expression and boolean function  and likewise for propositional language and a class of boolean functions. we denote classes of boolean functions by f'  g  and functions by f 1. 
　we consider a set x - {x1 ...  xn  of variables  each of which is associated with a world's attribute and can 
　1the size of the model-based representation of kb is related to the size of its minimal dnf. thus  we do not assume that the dnf representation is known but only require that a polynomial size representation exists. 

take the value 1 or 1 to indicate whether the associated attribute is true or false in the world. 
　assignments are mappings from x to {1}  and we treat them as elements in ping. assignments in weight x  denotes the number of 1 bits in the assignment x. a clause is a disjunction of literals  and a cnf formula is a conjunction of clauses. for example 
two clauses. a term is a conjunction of literals  and a 
dnf formula is a disjunction of terms. for example  is a dnf formula with two 
terms. a cnf formula is monotone if all the literals in it are positive  unnegated . a cnf formula is horn if every clause in it has at most one positive literal. a cnf formula is k-quasi-horn if there are at most k positive literals in each clause. it is a k:-quasi-reversed-horn if there are at most k negative literals in each clause. a dnf formula is k-quasi-monotone dnf if there are at most k negative literals in each term. 
　every boolean function has many possible representations  and in particular both a cnf representation and a dnf representation. by the dnf size of /  denoted 
 dnf f    we mean the number of terms in the minimal dnf representation of/.  similarly  for  cnf{f  .  
　an assignment  satisfies /if  .  x is also called a model of /.  if / is a theory of the  world   a satisfying assignment of / is sometimes called a possible world. by  / implies g   denoted /   we mean 
that every model of / is also a model of g. throughout the paper  when no confusion can arise  we identify a 
boolean function / with the set of its models  namely 
 observe that the connective  implies  used between boolean functions is equivalent to the connective  subset or equal  used for subsets of that is  
1 	reasoning with models 
consider a propositional knowledge base w and let be a propositional query. the deduction problem w a can be approached using the following model-based 
strategy: 
algorithm mbr 	: 
test set: a set of possible assignments. test: if there is an element not satisfy a  return  no . otherwise  return  yes . 
　clearly  this approach solves the inference problem if t is the set of all models  satisfying assignments  of w  but this set might be too large. a model-based approach becomes useful if one can show that it is possible to use a fairly small set of models as the test set  and still perform reasonably good inference. 
　this section briefly introduces the monotone theory of boolean functions  bshouty  1   and the theory of reasoning with models1  see  khardon and roth  1c  for more details . 
   1 we note that this direction was studied independently in the relational data base community  beeri et a/.  1; 
mannila and raiha  1 . the results on model-based rea-
	khardon and roth 	1 

1 	automated reasoning 

	khardon and roth 	1 

reasoning with models is an intuitive paradigm  which has been shown to be theoretically sound. in this paper we presented more evidence to the utility of such representations. in particular  these representations support efficient reasoning in the presence of varying context information  as well as some restricted cases of default reasoning. the significance of these results is that they are achieved as natural extensions of exact  deductive  reasoning  and hold in cases in which the traditional formula-based representation does not support efficient reasoning. 
　these results can be viewed as providing some theoretical support for the usefulness of case-based style reasoning  where a set of  typical cases  is used as a knowledge representation. 

　1 we note that the default reasoning task is np-hard  selman and levesque  1  when the knowledge base is horn  all the default rules are positive literals  and the query is a 
　single positive literal. our results provide an algorithm for this class of problems  which is polynomial in the size of the model based representation. the latter though may be exponential in the size of the horn expression  and in particular this happens for the problems used in the reduction in  selman and levesque  1 . so strictly speaking we do not prove an advantage in this special case. our results  however  provide efficient algorithms in cases where they were not known to exist before. 
　1 our results were inspired by the connections between abduction and default reasoning developed in  selman  1 . 
1 	automated reasoning 
　we have shown that a model-based representation can be used to reason correctly when some additional constraining context information is supplied. this information augments the agents' knowledge and aids in deriving conclusions relevant to this context. we call this a topdown solution. it is conceivable  though  that an agent would have only some of the models  those models that come from some specific context d. in such a case  our results show that the agent reasons correctly within this context  although not within every context . this approach can be shown to work in other scenarios in which the agent constructs a model-based knowledge representation by randomly collecting examples in the environment  khardon and roth  1a . thus  the approach supports the view that an intelligent agent constructs a representation of the world incrementally by pasting together many  narrower  views from different contexts. 
　in default reasoning  an agent may have many  possibly conflicting  default rules  acquired in different contexts. default reasoning is thus a generalization of reasoning within context where the additional information may not be consistent  and may not be consistent with the knowledge the agent has about the world. indeed  a query holds  by default   if there is a plausible context in which it holds. as we have shown  model-based representations efficiently support default reasoning. 
　finally  we mention that it has been shown  within the learning to reason framework  khardon and roth  1b   that the model based representations discussed here can be learned efficiently. this can be combined with context specific default rules that are acquired via rote learning or other learning processes  schuurmans and greiner  1  to work in a plausible way. 
