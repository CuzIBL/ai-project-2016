
recently  a method based on laplacian eigenfunctions was proposed to automatically construct a basis for value function approximation in mdps. we show that its success may be explained by drawing a connection between the spectrum of the laplacian and the value function of the mdp. this explanation helps us to identify more precisely the conditions that this method requires to achieve good performance. based on this  we propose a modification of the laplacian method for which we derive an analytical bound on the approximation error. further  we show that the method is related the augmented krylov methods  commonly used to solve sparse linear systems. finally  we empirically demonstrate that in basis construction the augmented krylov methods may significantly outperform the laplacian methods in terms of both speed and quality.
1 introduction
markov decision processes  mdp   puterman  1  are a widely-used framework for planning under uncertainty. in this paper  we focus on the discounted infinite horizon problem with discount 붺 such that 1   붺   1. we also assume finite state and action spaces. while solving this problem requires only polynomial time  many practical problems are too large to be solved precisely. this motivated the development of approximate methods for solving very large mdps that are sparse and structured.
모value function approximation  vfa  is a method for finding approximate solutions for mdps  which has received a lot of attention  bertsekas and tsitsiklis  1 . linear approximation is a popular vfa method  because it is simple to analyze and use. the representation of the value function in linear schemes is a linear combination of basis vectors. the optimal policy is usually calculated using approximate value iteration or approximate linear programming  bertsekas and tsitsiklis  1 .
모the choice of the basis plays an important role in solving the problem. usually  the basis used to represent the space is hand-crafted using human insight about some topological properties of the problem  sutton and barto  1 . recently  a new framework for automatically constructing the basis was proposed in  mahadevan  1 . this approach is based on analysis of the neighborhood relation among the states. the analysis is inspired by spectral methods  often used in machine-learning tasks. we describe this framework in more detail in section 1.
모in the following  we use v to denote the value function  and r to denote the reward vector. the matrix i denotes an identity matrix of an appropriate size. we use n to denote the number of states of the mdp.
모an important property of many vfa methods is that they are guaranteed to converge to an approximately optimal solution. in methods based on approximate policy iteration the maximal distance of the optimal approximate value function v  from the optimal value function v  is bounded by  munos 
 
where are true and approximated value at step k given the current policy. the norm denotes a weighted quadratic norm with distribution 뷃. the distribution 뷃 is arbitrary  and 뷃k depends on 뷃 and the current transition matrix  munos  1 . similar bounds hold for algorithms based on bellman residual minimization  whenis replaced by  where p뷇k is a transition matrix of the current policy. in addition  these bounds hold also for the max norm  bertsekas and tsitsiklis  1 . in the following  we refer to the value function vk and its approximation v k as v and v  respectively  without using the index k.
모the main focus of the paper is the construction a good basis for algorithms that minimizein each iteration. we focus only on the quadratic approximation bound  because the other bounds are related. notice that.
모the paper is organized as follows. section 1 describes the spectral methods for vfa in greater detail. the main contribution of the paper is an explanation of the good performance of spectral methods for vfa and its connection to methods for solving sparse linear systems. this is described in section 1  where we also propose two new alternative algorithms. in section 1  we show theoretical error bounds of one of the methods  and then in section 1 we demonstrate that our method may significantly outperform the previously proposed spectral methods.
1 proto-value functions
in this section  we describe in greater detail the proto-value function methods proposed in  mahadevan  1 . these methods use the spectral graph framework to construct a basis for linear vfa that respects the topology of the problem. the states of the mdp for a fixed policy represent nodes of an undirected weighted graph  n e . this graph may be represented by a symmetric adjacency matrix w  where wxy represents the weight between the nodes. a diagonal matrix of the node degrees is denoted by d and defined as
. there are several definitions of the graph laplacian  with the most common being the normalized laplacian  defined for w as l = i  d 1wd 1. the advantage of the normalized laplacian is that it is symmetric  and thus its eigenvectors are orthogonal. its use is closely related to the random walk laplacian lr = i   d 1w and the combinatorial laplacian lc = d   w  which are commonly used to motivate the use of the laplacian by making a connection to random walks on graphs  chung  1 .
모a function f on a graph  n e  is a mapping from each vertex to r. one well-defined measure of smoothness of a function on a graph is its sobolev norm. the bottom eigenvectors of lc may be seen as a good approximation to smooth functions  that is  functions with low sobolev norm  chung  1 .
모the application of the spectral framework to vfa is straightforward. the value function may be seen as a function on the graph of nodes that correspond to states. the edge weight between two nodes is determined by the probability of transiting either way between the corresponding states  given a fixed policy. usually  the weight is 1 if the transition is possible either way and 1 otherwise. notice that these weights must be symmetric  unlike the transition probabilities. if we assume the value function is smooth on the induced graph  the spectral graph framework should lead to good results. while the adjacency matrix with edge weights of 1 was usually used before   mahadevan  1  also discusses other schemes.
모while the approach is interesting  it suffers from some problems. in our opinion  the good performance of this method has not been sufficiently well explained  because the construction of the adjacency matrix is not well motivated. in addition  the construction of the adjacency matrix makes the method very hard to analyze. as we show later  using the actual transition matrix instead of the adjacency matrix leads to a better motivated algorithm  which is easy to derive and analyze.
모the requirement of the value function being smooth was partially resolved by using diffusion wavelets in  mahadevan and maggioni  1; maggioni and mahadevan  1 . briefly  diffusion wavelets construct a low-order approximation of the inverse of a matrix. an disadvantage of using the wavelets is the high computational overhead needed to construct the inverse approximation. the advantage is  however  that once the inverse is constructed it may be reused for different rewards as long as the transition matrix is fixed. thus  we do not compare our approach to diffusion wavelets due to the different goals and computational complexities of the two methods.
1 analysis
in this section we show that if we use the actual transition matrix instead of the random walk laplacian of the adjacency matrix  the method may be well justified and analyzed. we assume in the following that the transition matrix p and the reward function r are available.
모it is reasonable to explain the performance using p instead of lr  because in the past applications p was usually very similar to i  lr. this is because the transition in these problems were symmetric  and the adjacency matrix was based on a random policy. notice that the eigenvectors of lr and i   lr are the same. moreover  if 뷂 is an eigenvalue of lr  then 1   뷂 is an eigenvalue of i   lr.
1 spectral approximation
assuming that p is the transition matrix for a fixed markov policy  we can express the value function as  puterman  1 :
		 1 
the second equality follows from the neumann series expansion. in fact  synchronous backups in the modified policy iteration calculate this series adding one term in each iteration.
모we assume that the transition matrix is diagonalizable. the analysis for non-diagonalizable matrices would be similar  but would have to use jordan decomposition. let x1 ...xn be the eigenvectors of p with corresponding eigenvalues 뷂1 ...뷂n. moreover  without loss of generality  for all j. since the matrix is diagonalizable  x1 ...xn are linearly independent  and we can decompose the reward as:
 
for some c1 ...cn. using pixj = 뷂ijxj  we have
.
모considering a subset u of eigenvectors xj as a basis will lead to the following bound on approximation error:
	.	 1 
therefore  the value function may be well approximated by considering those xj with greatest |dj|. assuming that all cj are equal  then the best choice to minimize the bound  1  is to consider the eigenvectors with high 뷂j. this is identical to taking the low order eigenvectors of the random walk laplacian  as proposed in the spectral proto-vfa framework  only that the transition matrix is used instead. using the analysis above  we propose a new algorithm in subsection 1.
1 krylov methods
there are also other well-motivated base choices besides the eigenvectors. another choice is to use some of the vectors in the neumann series  1 . we denote these vectors as yi = pir for all. calculating the value function by progressively adding all vectors in the series  as done in the modified policy iteration  potentially requires an infinite number of iterations. however  since the linear vfa methods consider any linear combination of the basis vectors  we need at most n linearly independent vectors from the sequence. then it is preferable to choose those yi with small i  since these are simple to calculate.
모interestingly  it can be shown that we just need y1 ...ym 1 to represent any value function  where m is the degree of the minimal polynomial  ipsen and meyer  1  of  i   붺p . moreover  even taking fewer vectors than that may be a good choice in many cases. to show that y1 ...ym 1 vectors are sufficient to precisely represent any value function  let the minimal polynomial be: a m 붸 ai. then let
.
by algebraic multiplication  we have ba = i. having this  the value function may be represented as:
 
for some 붹i. a more rigorous derivation may be found for example in  ipsen and meyer  1; golub and loan  1 .
the space spanned by or krylov subspace   denoted asy1 ...ym 1kis known as p r . it has been previ-krylov space ously used in a variety of numerical methods  such as gmres  or lancoz  and arnoldi  golub and loan  1 . it is also common to combine the use of eigenvectors and krylov space  what is known as an augmented krylov methods  saad  1 . these methods actually subsume the methods based on simply considering the largest eigenvectors of p. we discuss this method in subsection 1.
1 algorithms
in this section we propose two algorithms based on the previous analysis for constructing a good basis for vfa. these algorithms deal only with the selection of the basis and they can be arbitrarily incorporated into any algorithm based on approximate policy iteration.
모the first algorithm  which we refer to as the weighted spectral method  is to form the basis from the eigenvectors of the transition matrix. this is in contrast with the method presented in  mahadevan  1   which uses any of the laplacians. moreover  we propose to choose the vectors with greatest |dj| value  in contrast to choosing the ones with largest 뷂j. the reason is that this leads to minimization of the bound in
 1 .
모a practical implementation of this algorithm faces some major obstacles. one issue is that there is no standard eigenvector solver that can efficiently calculate the top eigenvectors with regard to |dj| of a sparse matrix. however  such a method may be developed in the future. in addition  when the transition matrix is not diagonalizable  we need to calculate the jordan decomposition  which is very time-consuming and unstable. finally  some of the eigenvectors and eigenvalues
require: p  r  k - number of eigenvectors in the basis  l - total number of vectors
let z1 ...zk be the top real eigenvectors of p zk+1 뫹 r for i 뫹 1...l + k do
if i   k + 1 then
zi 뫹 pzi 1
end if for
	i	j	j
end for
ifthen
end forfigure 1: augmented krylov method for basis construction.
may contain complex numbers  what would require a revision of the approximate policy iteration algorithms. if these issues were resolved  this may be a viable alternative to other methods.
모the second algorithm we propose is to use the vectors from the augmented krylov method  that is  to combine the vectors in the krylov space with a few top eigenvectors. a pseudocode of the algorithm is in figure 1. the algorithm calculates an orthonormal basis of the augmented krylov space using a modified gram-schmidt method.
모using the krylov space eliminates the problems with nondiagonalizable transition matrices and complex eigenvectors. another reason to combine these two methods is to take advantage of approximation properties of both methods. intuitively  krylov vectors capture the short-term behavior  while the eigenvectors capture the long-term behavior. though  there is no reliable decision rule to determine the right number of augmenting eigenvectors  it is preferable to keep their number relatively low. this is because they are usually more expensive to compute than vectors in the krylov space.
1 approximation bounds
in this section  we briefly present a theoretical error bound guaranteed by the augmented krylov methods. the bound characterizes the worst-case approximation error of the value function  given that the basis is constructed using the current transition matrix and reward vector. we focus on bounding the quadratic norm  what also implies the max norm.
모we show the bound for. this bound applies directly to algorithms that minimize the bellman residual  bertsekas and tsitsiklis  1   and it also implies:

this follows from the neumann series expansion of the inverse and from.
모in the following  we denote the set of m krylov vectors as km and the chosen set of top eigenvectors of p as u. we also use e c d a  to denote an ellipse in the set of complex numbers with center c  focal distance d  and major semi-axis a. the approximation error for a basis constructed for the current policy may be bounded as the following theorem states.
theorem 1. let  i  붺p  = xsx 1 be a diagonalizable matrix. further  let
 
where t is a diagonal matrix with 1 in place of eigenvalues of eigenvectors in |u|. the approximation error using the basis km 뫋 u is bounded by:
 
where cm is the chebyshev polynomial of the first kind of degree. the parameters a c d and a1 c1 d1 are chosen such that e c1 d1 a1  includes the lower n |u| eigenvalues of  i 붺p   and e c d a  includes all its eigenvalues.
모the value of 뷋 depends on the angle between the invariant subspace u and the other eigenvectors of p. if they are perpendicular  such as when p is symmetric  then 뷋 = 1.
proof. to simplify the notation  we denote a =  i   붺p .
first  we show the standard bound on approximation by a krylov space  saad  1   ignoring the additional eigenvectors. in this case  the objective is to find such w 뫍 km that minimizes the following:
 
where w 1  =  1. notice that this defines a polynomial in
a multiplied by r with the constant factors determined by w. let pm denote the set of polynomials of degree at most m such that every p 뫍 pm satisfies p 1  = 1. the minimization problem may be then expressed as finding a polynomial p 뫍 pm that minimizes . this is related to the value of the polynomial on complex numbers as  golub and loan  1 :
 
where 뷂i are the eigenvalues of a. a more practical bound may be obtained using chebyshev polynomials  as for example in  saad  1   as follows:
 
where the ellipse e c d a  covers all eigenvalues of a.
모in the approximation with eigenvectors  the minimization may be expressed as follows:

where pu is the least squares projection matrix to u. note that  since u is an invariant space of a. moreover   i   pu    r is perpendicular to u. the theorem then follows from the approximation by chebyshev polynomials as described above  and the definition of matrixvalued function approximation  golub and loan  1 . 
모this bound shows that it is important to choose an invariant subspace that corresponds to the top eigenvalues of p. it also shows that u should be chosen to be to the highest degree perpendicular to the remaining eigenvectors. finally  the bound also implies that the approximation precision increases with lower 붺  as this decreases the size of the ellipse to cover the eigenvalues.
1 experiments
in this section we demonstrate the proposed methods on the two-room problem similar to the one used in  mahadevan  1; mahadevan and maggioni  1 . this problem is a typical representative of some stochastic planning problems encountered in ai.
모the mdp we use is a two-room grid with a single-cell doorway in the middle of the wall. the actions are to move in any of the four main direction. we use the policy with an equal probability of taking any action. the size of each room is 1 by 1 cells. the problem size is intentionally small to make explicit calculation of the value function possible. notice that because of the structure of the problem and the policy  the random walk laplacian has identical eigenvectors as the transition matrix in the same order. this allows us to evaluate the impact of choosing the eigenvectors in the order proposed by the weighted spectral method.
모we used the problem with various reward vectors and discount factors  but with the same transition structure. the reward vectors are synthetically constructed to show possible advantages and disadvantages of the methods. they are shown projected onto the grid in figure 1. vector  reward 1  represents an arbitrary smooth reward function. vectors  reward 1  and  reward 1  are made perpendicular to the top 1 and the top 1 eigenvectors of the random walk laplacian respectively.
모a lower discount rate is generally more favorable to krylov methods  because the value is closer to the value of the reward received in the state. this can also be seen from theorem 1  because 붺 shrinks the eigenvalues. therefore  we use problems that are generally less favorable to krylov methods  we evaluate the approach using two high discount rates  붺 = 1 and 붺 = 1.
모in our experiments  we evaluate the mean squared error  mse  of the value approximation with regard to the number of vectors in the basis. the basis and the value function were calculated based on the same policy with equiprobable action choice. we obtained the true value function by explicitly evaluating  i   붺p  1r. notice  however  that the true value does not need to be used in the approximation  it is only required to determine the approximation error. we compared the following 1 methods:

figure 1: reward vectors projected onto the grid

figure 1: mean squared error of each method  using discount of 붺 = 1. the mse axis is in log scale.laplacian the technique of using the eigenvectors of the random walk laplacian of the adjacency matrix  as proposed in  mahadevan  1 . the results of the normalized laplacian are not shown because they were practically identical to the random walk laplacian.
wl this is the weighted spectral method  described in subsection 1  which determines the optimal order of the eigenvectors to be used based on the value of dj.
krylov this is the augmented krylov method with no eigenvectors  as described in subsection 1.
ka this is the augmented krylov method with 1 eigenvectors  as proposed in figure 1. the number of eigenvectors was chosen before running any experiments on this domain.
모the results for 붺 = 1 are in figure 1 and for 붺 = 1 are in figure 1.  reward 1  intentionally violates the smoothness assumption  but it is the easiest one to approximate for all methods except the laplacian. in the case of  reward 1  and 붺 = 1  the weighted spectral method and the random walk laplacian outperform the ordinary krylov method for the first 1 vectors. however  with additional vectors  both krylov and augmented krylov methods significantly outperform them.
모our results suggest that krylov methods may offer superior performance compared to using the eigenvectors of the laplacian. since these methods have been found very effective in many sparse linear systems  saad  1   they may perform well for basis construction also in other mdp problems. in addition  constructing a krylov space is typically faster than constructing the invariant space of eigenvectors. using matlab on a standard pc  it took 1 seconds to calculate the 1 top eigenvectors  while it took only 1 second to calculate the first 1 vectors of the krylov space for  reward 1 .
1 discussion
we presented an alternative explanation of the success of laplacian methods used for value approximation. this explanation allows us to more precisely determine when the method may work. moreover  it shows that these methods are closely related to augmented krylov methods. we demonstrated on a limited problem set that basis constructed from augmented krylov methods may be superior to one constructed from the eigenvectors. in addition  both the weighted spectral method and the augmented krylov method do not assume the value function to be smooth. moreover  calculating vectors in the krylov space is typically cheaper than calculating the eigenvectors.
모an approach for state-space compression of partially observable mdps  pomdp  that also uses krylov space was proposed in  poupart and boutilier  1 . while pomdps are a generalization of mdps  the approach is not practical for large mdps since it implicitly assumes that the number of observations is small. this is not true for mdps  because the number of observations is equivalent to the number of states. in addition  the objectives of this approach are somewhat different  though the method is similar.

figure 1: mean squared error of each method  using discount of 붺 = 1. the mse axis is in log scale.모one important issue that we did not address is an application to state spaces that are too large to be enumerated  such as factored mdps. because the vectors in krylov space are as large as the whole state space  they cannot be enumerated either. however  the approach may be extended along similar lines as discussed in  poupart and boutilier  1 .
모in reinforcement learning  the mdp needs to be solved using only an estimation of the transition matrix from sampling. an additional problem is that the basis is not defined for states that were not sampled. this was addressed for eigenvectors for example in  mahadevan et al.  1 . thus augmenting the krylov space by the eigenvectors also has the advantage that these methods may be directly applied.
모we focused mainly on the vfa for a fixed policy without explicitly considering the control part of the problem. while these methods may be combined arbitrarily  a future challenge is to determine an efficient combination.
모our results suggest that exploring the connection between the basis construction in vfa and sparse linear solvers may bring about interesting advances in the future.
acknowledgements
the author was supported by the national science foundation under grant no. iis-1. i also thank sridhar mahadevan  hala mostafa  shlomo zilberstein  and the anonymous reviewers for valuable comments.
