
the valued  vcsp  framework is a generic optimization framework with a wide range of applications. soft arc consistency operations transform a vcsp into an equivalent problem by shifting weights between cost functions. the principal aim is to produce a good lower bound on the cost of solutions  an essential ingredient of a branch and bound search.
but soft ac is much more complex than traditional ac: there may be several closures  fixpoints  and finding the closure with a maximum lower bound has been shown to be np-hard for integer costs  cooper and schiex  1 .
we introduce a relaxed variant of soft arc consistency using rational costs. in this case  an optimal closure can be found in polynomial time. furthermore  for finite rational costs  the associated lower bound is shown to providean optimal arc consistent reformulation of the initial problem.
preliminary experiments on random and structured problems are reported  showing the strength of the lower bound produced.
1 introduction
the valued constraint satisfaction problem  schiex et al.  1  is a generic cost function optimization framework with many applications. a vcsp is defined by a set of variables with finite domains and a set of local cost functions  soft constraints  which associate costs to tuples. the goal is then to find an assignment to all variables with a minimum combined cost. costs from different constraints are combined with a domain dependent operator ¨’. in this paper  we focus on weighted csp  wcsp  where costs are natural numbers  combined by addition. this vcsp sub-case has been shown to capture the essential complexity of non-idempotent vcsp in  cooper  1  and has a lot of direct applications in domains such as resource allocation  combinatorial auctions  bioinformatics  probabilistic reasoning  graph theory...
¡¡following the initial definition of arc consistency for non idempotent operators in  schiex  1   local consistency is now considered as a crucial mechanism for solving wcsps. based on iterated integer cost movements between cost functions of different arities which preserve problem equivalence  local consistency offers all the services of local consistency in classical csp. it is specifically capable of producing strong incrementally maintained lower bounds which are crucial for efficient branch and bound search. it is however plagued by the absence of uniqueness of the fixpoint  so-called arc consistent closure  and by the fact that finding a closure which maximizes the lower bound is an np-hard problem  cooper and schiex  1 . this has led to the definition of a large number of variants of arc consistency  directional  cooper  1   existential  larrosa et al.  1   cyclic  cooper  1 ...   each using different strategies to try to get closer to an optimal closure.
¡¡in this paper  we show that by relaxing the condition that costs be integers and by allowing simultaneous cost movements between cost functions  it is possible to define a new class of local consistency such that finding an optimal closure is a polynomial time problem. on a large class of problems  we show that the lower bound produced is always better than the previous integer arc consistency variants and provides an optimal reformulation of the initial problem.
¡¡beyond a better understanding of local consistency in wcsps  preliminary experiments of optimal soft arc consistency on random and structured wcsps show that it may also be used in practice as a preprocessing algorithm.
1 preliminaries
valued csp extends the csp framework by associating costs to tuples  schiex et al.  1 . in general  costs are specified by means of a so-called valuation structure defined as a triple
  where e is the set of costs totally ordered
. the maximum and a minimum costs are noted  and ¡Í  respectively. ¨’ is a commutative  associative and monotonic operation on e used to combine costs. ¡Í is the identity element and  is absorbing.
¡¡weighted csps  wcsp  form a specific subclass of valued csp that relies on a specific valuation structure s k . definition 1 s k  is a triple  {1 ... k} ¨’ ¡Ý  where 
  k ¡Ê {1 ... ¡Þ}.
  ¨’ is defined as a ¨’ b = min{k a + b}
  ¡Ý is the standard order among numbers.
observe that in s k   we have may be either finite or infinite.
¡¡a wcsp p is defined by p =  x d c k . the valuation structure is s k . x and d are sets of variables and domains  as in a standard csp. for a set of variables s   x  we note the set of tuples over s. c is a set of cost functions. each cost function  or soft constraint  cs in c is defined on a set of variables s called its scope. a cost function cs assigns costs to assignments of the variables in s i.e.:
. for simplicity  we assume that ev-
ery constraint has a different scope. for binary and unary constraints  we use simplified notations: a binary soft constraint between variables i and j is denoted cij. a unary constraint on variable i is denoted ci. we assume the existence of a unary constraint ci for every variable  and a zero-arity constraint  noted c   if no such constraint is defined  we can always define dummy ones ci a  = 1  a ¡Ê di and c  = 1 .
¡¡when a constraint cs assigns costto a tuple t  it means that cs forbids t  otherwise t is permitted by cs with the corresponding cost. the cost of a complete tuple t in a wcsp p  noted vp t   is the sum of all costs:

¡¡where t s  denotes the usual projection of a tuple on the set of variables s. a complete assignment is feasible if it has a cost less than k and optimal if there are no complete assignment with a lower cost. the problem is to find a complete optimal assignment  np-hard .
¡¡we say that two wcsps defined over the same variables are equivalent if they define the same global cost function  i.e.are identical.
1 node and arc consistencies
local consistency algorithms are characterized by the fact that they reason at a subproblem level and that they preserve equivalence. this is captured by the following notion:
definition 1 for a valued csp with a set of variables x  an equivalence preserving transformation  ept  on w   x is an operation which transforms cost functions whose scope is included in w to produce an equivalent valued csp.
¡¡in wcsp  basic ept move costs between cost functions in order to preserve the equivalence of the problem. to move costs  it is necessary to be able to subtract costs from its origin. this is done using the   operation defined as:

¡¡although we restrict our presentation to wcsps for the sake of simplicity  we remind the reader that such a pseudodifference operator exists in a large class of vcsps  fair vcsp  cooper and schiex  1  .
¡¡algorithm 1 gives two elementary ept. project works in the scope of a single constraint cs. it moves an amount of cost ¦Á from cs to a unary cost function ci i ¡Ê s  for a value a ¡Ê di. if the cost ¦Á is negative  cost moves from

figure 1: a simple wcsp and two arc consistent closures.
the unary cost function ci to the cost function cs. to guarantee that the problem obtained after each application is valid  one must guarantee that its cost functions remain in the valuation structure. to avoid negative costs  one must have
. similarly  projec-
tunary works on a subproblem defined just by one variable i ¡Ê x. it moves a cost ¦Á from the unary cost functionci to the nullary cost function c   with  c  ¡Ü ¦Á ¡Ü mina¡Êdi{ci a } in order to get a valid wcsp .

algorithm 1: project and unaryproject for soft arc and node consistency enforcing

procedure project cs i a ¦Á  c  a  ¡û ci a  ¨’ ¦Á; foreach  such that t {i}  = a  do
;
procedure unaryproject i ¦Á 

¡¡a variable i is said to be node consistent  larrosa  1  if 1  for all values  1  there exists a value b ¡Ê di such that ci b  = 1. a wcsp is node consistent  nc  if every variable is node consistent.
¡¡stronger arc level consistencies rely on the notion of support. for simplicity  we introduce these notions for binary wcsp but most have been generalized to arbitrary arities  see  cooper and schiex  1  . a value b ¡Ê dj is a support for a value a ¡Ê di along cij if cij a b  = 1.
¡¡variable i is arc consistent if every value a ¡Ê di has a support in every constraint cij ¡Ê c. a wcsp is arc consistent  ac  if every variable is arc and node consistent.
¡¡when a value  i a  has no support on a constraint  one can create one using the project operation. this is exemplified in figure 1a. this wcsp has two variables with 1 values a and b. vertices  representing values  can be weighted by unary costs. an edge connecting two values represents a cost of 1. if two values are not connected  it means that the corresponding cost is 1. notice that value  1 b  has no support on variable 1. we can apply project c1 b 1 . this creates a unary cost of 1 for  1 b  which is now ac. applying unaryproject 1  on the resulting wcsp  figure 1b  makes the problem ac and increases c  by 1.
¡¡but a different sequencing may lead to a different result. if we first note that value  1 a  has no support on c1 and apply project c1 a 1   we get the problem in figure 1c. it is now ac  but the lower bound c  is 1.
¡¡given an initial wcsp p  any wcsp obtained from p by iterated applications of project and unaryproject with a positive integer ¦Á until a fixpoint is reached is called an arc consistent closure of p. an ac closure of a wcsp p is optimal if it has the maximum lower bound c  among all ac closures of p.  cooper and schiex  1 showed that finding an optimal ac closure for a wcsp is an np-hard problem.
¡¡alternative definitions of ac have therefore been proposed. they exploit the notion of full support. b is a full support for a on cij if cij a b  ¨’ cj b  = 1. replacing the notion of support by the stronger notion of full supports in ac is attractive but the corresponding property cannot be enforced  schiex  1 . restricted versions must be used.
¡¡given a variable ordering    variable i is directional arc consistent  dac  if every value a ¡Ê di has a full support in every constraint cij ¡Ê c such that j   i. a wcsp is dac if every variable is dac and node consistent. it is full directional arc consistent  fdac  if it is ac and dac. several heuristics for ordering variables in fdac have been tried  heras and larrosa  1a .
¡¡variable i is existential arc consistent  larrosa et al.  1  if there exists a ¡Ê di such that ci a  = 1 and a has a full support on every constraint cij. a wcsp is existential arc consistent  eac  if every variable is eac and node consistent. it is existential directional arc consistent  edac  if it is eac  and fdac. all these definitions can be seen as well defined polynomial time heuristics trying to get closer to the optimal closure  but without any guarantee.
1 optimal arc consistency
from now on  we relax the definition of wcsp by allowing rational costs. this mean we use the new valuation structure sq k  =   1 k  ¨’ ¡Ý  where  1 k  denotes the set of rational numbers  elements of q  between  and including  1 and k.
definition 1 given a wcsp p  a soft arc consistent  sac  transformation is a set of soft arc consistency operations project and unaryproject which  when applied simultaneously  transform p into a valid wcsp.
¡¡note that in this definition there is no precondition on the costs moved individually by each ept application. we just want the final equivalent wcsp to be valid  with positive rational costs : this also guarantees that c  is a lower bound of the optimal cost. previously   affane and bennaceur  1  proposed to split integer costs by propagating a fraction wij of the binary constraint cij towards variable i and the remaining fraction 1   wij towards j  where 1 ¡Ü wij ¡Ü 1   suggested determining optimal wij but just used  bennaceur and osamni  1  further suggested to use different weights wiajb for every pair of values  a b  ¡Ê di ¡Á dj. it turns out that using one weight for each triple  i j a  where a ¡Ê di  allows us to find optimal weights in polynomial time.
theorem 1 if the valuation structure sq ¡Þ  is used  then it is possible to find in polynomial time a sac transformation of p which maximizes the lower bound c  provided the arity of the constraints in p is bounded.
proof: assume first as in  cooper  1  that all infinite costs have been propagated using standard generalized ac  mohr and masini  1 . then only finite costs can be further propagated by project and unaryproject. we then want to determine a set of sac operations which  when applied simultaneously  maximize the increase in c . for each cs ¡Ê c with |s|   1  and for every variable i ¡Ê s  let psi a be the amount of cost projected from cs to ci a   remember that costs moved from ci a  to cs are counted as negative in project . let ui ¡Ý 1 be the amount of cost projected by unaryproject from ci to c . thus the problem is to maximize while keeping the wcsp valid  no negative cost appears  i.e.:

all inequalities for which can be ignored since they are necessarily satisfied. the remaining inequalities define a linear programming problem over q with o ed + n  variables which can be solved in polynomial time  karmarkar  1 .	
¡¡a local version of the above theorem  limited to subproblems of 1 variables  is actually the basis of the algorithm enforcing 1-cyclic consistency  cooper  1 . in our case  a problem will be called optimal soft arc consistent  osac  when c  cannot be improved as above. obviously in sq ¡Þ   osac is stronger than ac  dac  fdac or edac.
¡¡note that if k   ¡Þ  things get more involved since an increase in c  can  through node consistency  lead to the deletion of values  thus requiring a new optimization. because value deletion cannot occur more than nd times  this is still polynomial time but the proof of optimality of the final closure is an open question.
¡¡to better understand how osac works  consider the 1variable wcsp on the left. all unary costs are equal to 1. all edges represent a unit cost  omitted for clarity. c  is assumed to be 1. none of the basic epts can transform the problem into a valid one. however  we may simultaneously perform the following operations:
1. project c1 c  1 : we move  a virtual  cost of 1 from value  1 c  to 1 pairs inside c1.
1. project c1 a 1   project c1 b 1 : this moves two unary costs of 1 to  1 a  and  1 b .
1. project c1 a  1   project c1 b  1 : these two unary costs are moved inside c1 and c1 respectively.
1. project c1 c 1 : this moves a unary cost of 1 to
 1 c .
1. project c1 a 1   project c1 c 1 : this moves two unary costs of 1 to  1 c  and  1 a .
1. project c1 a  1   project c1 c 1 : we reimburse our initial loan on value  c 1 .
1. project c1 c  1   project c1 a 1 : we send a unary cost to value  1 a .

¡¡finally  the application of unaryproject 1  yields the problem above with a lower bound c  = 1. this set of ept corresponds to a solution of the previous linear programming problem where and
  all
other variables being equal to 1 .
1 properties
theorem 1 shows that on sq ¡Þ   osac always provides an optimal lower bound using only a set of arc level preserving operations. actually  one can prove that for a large class of wcsp the final problem obtained is an optimal reformulation of the problem using the initial set of scopes.
definition 1 a wcsp p is in-scope c -irreducible if there is no equivalent wcsp q with the same set of constraint scopes as p and such that cq    cp   where cp   cq  are the nullary constraints in p  q .
theorem 1 let p be a wcsp over sq ¡Þ  using only finite costs. if no sac transformation applied to p produces a
wcsp  then p is in-scope c -irreducible.
proof: this is a direct consequence of lemma 1 in  cooper  1 .	
¡¡thus  for finite rational costs  osac can be used to establish in-scope c -irreducibility. this naturally does not work when infinite costs  hard constraints  exist. the 1-clique 1coloring problem is inconsistent and is therefore equivalent to the same problem with just c  set to  but no sac transformation can be applied to this wcsp.
¡¡naturally  higher-order consistencies which may change scopes could provide possibly stronger lower bounds: soft 1-consistency  cooper  1  or soft path inverse consistency  heras and larrosa  1b  applied to the previous 1coloring problem would detect infeasibility.
1 experiments
we applied osac during preprocessing. the linear programming problem defined by osac was solved using ilog cplex version 1.1  using the barrieralgorithm . the lower bound produced is then compared to the lower bound produced by edac.
¡¡the first set of instances processed are random max-csp created by the random vcsp generator1 using the usual four parameters model. the aim here is to find an assignment that minimizes the number of violated constraints. four different categories of problems with domain size 1 were generated following the same protocol as in  larrosa et al.  1 : sparse loose  sl  1 var.   sparse tight  st  1 var.   dense loose  dl  1 var.  and dense tight  dt  1 var.   see the benchmarks section of  de givry et al.  1b  .
slstdldtoptimum1111edac lb.1.11osac lb.1.11¡¡samples have 1 instances. the table above shows respectively the average optimum value  the average values of the edac lower bound and the average value of the osac lower bound. on loose problems  osac and edac leave the lower bound unchanged. this shows that higher level local consistencies are required here. however for tight problems  osac is extremely powerful  providing lower bounds which are sometime three times better than edac.
¡¡the second set of benchmarks is defined by open instances of the radio link frequency assignment problem of the celar  cabon et al.  1 .1 these problems have been extensively studied  see http://www.zib.de/fap/problems/calma  but the gap between the best upper bound  computed by local search methods  and the best lower bound  computed by exponential time algorithms  is not closed. the problem considered are the scen1 1}reduc.wcsp and graph1 1}reducmore.wcsp which have already been through different strong preprocessing  see the benchmarks section in  de givry et al.  1b  .
scen1scen1graph1graph1total # of values11best known ub11best known lb11edac lb11osac lb111cpu-time1 1 1 1 ¡¡as the table above shows  osac offers substantial improvements over edac  especially on the graph1 and graph1 instances. for these instances  the optimality gap ub ubosac is reduced to 1% and 1% respectively. the polynomial time lower bounds obtained by osac are actually close to the best known  exponentialtime  lower bounds. the cpu-time show the cpu-time for computing the osac lower bound.
¡¡to actually assess the practical interest of osac for preprocessing  we tried to solve problems where osac was effective: tight problems. the difficulty here lies in the fact that cplex is a floating point solver while the open source wcsp solver used  toolbar  section algorithms in  de givry et al.  1b   deals with integer costs. to address this issue  we use  fixed point  costs: for all wcsp considered  we first multiply all costs by a large integer constant ¦Ë = 1  and then solve the linear programming problem defined by osac using integer variables  instead of floating point . the first integer solution found is used. the resulting problem has integer costs and can be tackled by toolbar1. this means that we shift from a polynomial problem to an np-hard one. in practice  we found that the problems obtained have a very good linear continuous relaxation and are not too expensive to solve as integer problems. using a polytime rational lp solver would allow time complexity to remain polynomial.

¡¡the figure above reports cpu-time  up  and size of the tree search  down  for dense tight problems of increasing size. the time limit was set to 1 . four cpu-times are reported:  1  osac lp: time taken by cplex to solve the first linear relaxation  1  osac mip: time taken to get the first integer solution   1  medac: time taken to solve the original problem by maintaining edac  larrosa et al.  1  in toolbar with default parameters and a good initial upper bound   1  osac+medac is the sum of osac mip with the time needed by toolbar to solve the osac problem  with the same default parameters and upper bound .
¡¡clearly  for small problems  with less than 1 variables   osac is more expensive than the resolution itself. but as the problem size increases  osac becomes effective and for 1 variables  it divides the overall cpu-time by roughly 1. the number of nodes explored by toolbar in both cases shows the strength of osac used as a preprocessing technique  remember that edac is maintained during search .
¡¡the strength of osac compared to local consistencies using full supports such as dac is that is does not require an initial variable ordering. indeed  dac directly solves treestructured problems but only if the variable ordering used for dac enforcing is a topological ordering of the tree. to evaluate to what extent osac can overcome these limitations  we used random problems structured as binary clique trees as in  de givry et al.  1a . each clique contains 1 variables with domain size 1  each sharing 1 variables with its parent clique. the overall tree height is 1 leading to a total number of 1 variables  with a graph density of 1%.

¡¡the figure above uses a logarithmic scale for cpu-time for differentconstraint tightnesses  below 1%  problems are satisfiable . on these tree-like problems  two dac ordering were used. one is compatible with a topological ordering of the binary tree  and should give good lower bounds   the inverse order can be considered as pathological. the cputimes for medac alone  default toolbar parameters and upper bound  and osac+medac  as previously  are shown in each case. clearly  osac leads to drastic  up to 1 fold  improvements when a bad dac ordering is used. being used just during preprocessing  it does not totally compensate for the bad ordering. but  even when a good dac ordering is used  osac still allows impressive  up to 1 fold  speedups  especially on tight problems.
¡¡finally  we also tried to solve the challengingopen celar instances after osac preprocessing. despite the strength of osac  all problems remained unsolvable. simultaneously taking into account the strong structure of the problems as in  de givry et al.  1a  is an attractive direction here.
1 related work
as a reviewer pointed out  the lp program optimized for osac is the dual of the relaxation of the 1-linear formulation proposed in  koster  1 . the lp formulation of
osac was also found in the image processing community  schlesinger  1; werner  1 .
if all constraints are binary over boolean domains  wcsp
reduces to weighted max-1sat. a direct encoding of max-1sat to quadratic pseudo-boolean function optimization  boros and hammer  1 exists: using 1 and 1 to represent respectively true and false  ¡Á for disjunction   1 x  for negation and + for combination of costs  a set of weighted 1-clauses can be transformed into a quadratic function. for example  the clauses {a¡Åb a¡Å¡¥b} with weights 1 and 1 translates to the function f a b  = 1ab + 1a 1   b .
¡¡the problem of producing a so-called  equivalent quadratic posiform representation  with a high constant term  the equivalent of c   has been shown to reduce to the computation of a maximum flow  goldberg and tarjan  1  in an appropriately defined network  boros and hammer  1 . given the close connection between maximum flow and linear programming  a fine comparison of the lower bound produced by osac on max-1sat problems and by the maxflow formulation of  boros and hammer  1  would be interesting.
1 conclusion
osac provides a polynomial time optimal arc consistent closure in a large class of wcsps. despite very preliminary testing and comparatively high computational cost to enforce it  osac already shows its usefulness as a preprocessing algorithm for sufficiently large and tight problems.
¡¡beyond this practical usefulness  we think that osac brings to light the fact that  in order to increase their strength  new soft local consistency algorithms should probably not be restricted to the mechanical application of elementary operations but should instead try to identify worthy set of equivalence preserving operations that should be simultaneously applied. if maintaining osac during search is an obvious but challenging next step  the construction of simpler limited versions of osac should also be considered.
¡¡this approach is radically different from the ongoing trend of using higher level consistencies  such as path  cooper  1  or path inverse consistency  heras and larrosa  1b  . the extension of osac to such higher level consistencies is also an open question.
