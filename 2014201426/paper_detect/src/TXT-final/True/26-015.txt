 
inducing concept descriptions in first order logic is inherently a complex task. there are two main reasons: on one hand  the task is usually formulated as a search problem inside a very large space of logical descriptions which needs strong heuristics to be kept to manageable size. on the other hand  most developed algorithms are unable to handle numerical features  typically occurring in realworld data. in this paper  we describe the learning system smart+  that embeds sophisticated knowledge-based heuristics to control the search process and is able to deal with numerical features. smart+ can use different learning strategies  such as inductive  deductive and abductive ones  and exploits both backgruond knowledge and statistical evaluation criteria. furthermore  it can use simple genetic algorithms to refine predicate semantics and this aspect will be described in detail. finally  an evaluation of smart+ performances is made on a complex task. 
1 	introduction 
in the recent literature  inducing concept descriptions in 
first order logic is receiving an increasing interest  bergadano et a/.  1  michalski  1  quinlan  1  gemello and mana  1  pazzani and kibler  1 . 
　in general  the task of learning concept descriptions  or relations  is formulated as a search problem inside a space of logical formulas  built up starting from a set of initial predicates  evaluable on the learning events. two crucial points attract the attention of many researchers: the design of strategies that limit the search space  and the choice of the representation language. as discussed in  utgoff  1  choosing inappropriate features for describing the learning events can prevent the induction algorithm from finding good concept descriptions. 
　the system smart+  described in this paper  proposes solutions for both problems and proved to be effective in coping with tasks more complex than the ones approached so far in the literature. smart+ is an improved version of 
 * this work has been partially supported by eec  project blearn ii no.1. ml-smart  bergadano et a/.  1  bergadano and 
giordana  1   and offers sophisticated learning strategies that can either be selected in alternative or combined together  depending on the choice of the teacher. on the one hand  induction can be guided by a domain theory using deductive and/or abductive reasoning. on the other hand  besides the well known information gain rule  quinlan  1   other statistical criteria are available that can be more effective in complex problems. 
　moreover  smart+ offers a mechanism for dealing with numerical features in order to help solving the hard problem of choosing an appropriate concept description language. in particular  it is able to infer a quantitative definition of numerical features according to the context in which those features are used. for instance  suppose we have a symbolic language in which terms such as short  medium and long are defined to characterize the length of an object: an object is considered short if its length is less than a given value k1 long if its length is greater than a given value k1 and medium if its length is between k1 and k1. how to choose values for k1 and k1 depends on the context those features are used in: if we are dealing with bicycles  cars and airplanes  suitable values might be k1 = 1 feet  k1 = 1 feet  but if we are dealing with different kinds of aircrafts  pipers  military jets  passenger cargos   suitable values might be k1 = 1 feet  k1 = 1 feet. in smart+ the teacher is only requested to provide the system with a range of possible meaningful values for a given feature and the system itself should find the best assignments for the task at hand. 
　the paper is organized as follows: an overview of the knowledge representation formalism in smart+ is given in section 1  whereas sections 1 through 1 describe the available learning strategies. finally  some experiments on a complex artificial domain  simulating a problem of pattern recognition  will be discussed in section 1  and conclusive remarks follow in section 1. 
1 knowledge representation 
smart+ is a problem solver designed for the task of learning concept descriptions from examples. given a set ho of concepts and a set fo of labelled instances  smart* generates as output a classification theory described in a 
horn clause language l extended with functions  negation and numerical quantifiers. in particular  a well formed formula  wff  in the language l has the form: 
	botta and giordana 	1 
 1  
being 	predicates denoting sets of concepts and 
 p a logical formula stating a condition over the terms t1 ...  tn. expression  1  actually means that if an event f  to be classified  is an instance of a concept h belonging to the set hi and the condition  t 1   ... tn  is true of f  then h is included in hj. as the set of concepts hj implied by a rule is  in general  in the body of another rule  the knowledge learned by smart+ defines a structured classification theory  which can be described as a discrimination graph. in order to deal with noise  partially incorrect classification rules are allowed. the value of the weight w  in  1   is an estimate of the probability that the rule give the correct classification; it is evaluated as the ratio between the number of correct instances matched and the number of total instances matched to the learning events fo. 
formula  is built up by using predicates in a set 
p  connectives a and -. and quantifiers atm  atl and ex. 
as described in  yager  1   these quantifiers stand for 
atmost  atleast and exactly  respectively  and can be considered as an extension of the standard existential quantifier  similar to the numeric quantifiers used in the induce system  michalski  1  . 
　in order to cope with the fuzziness inherent in real-world data  a continuous-valued semantics is adopted for the predicates in l. for each predicate  a corresponding semantic functiofp  mapping a numerical base variable to the interval  1   must be defined by the teacher in order to specify how to evaluate the truth of p on the learning events. as a matter of fact  fp is a function that evaluates the membership  of a numerical feature vp with respect to a trapezoidal fuzzy set spt the value of sp can be defined by means of parameters that will occur in the predicate p as variable terms. terms in a predicate p are subdivided into two categories: objects and parameters. the first ones  denoted by the symbols x1 ...  x n   can only be bound to items corresponding to learning instances or components of them  whereas the second ones  denoted by the symbols k1  ...  km  correspond to fuzzy set parameters and can only be bound to numerical constants. 
trapezoidal fuzzy sets are considered and defined as in 
figure 1 a   in a way that allows the fuzzy set to be identified by a pair of real parameters  k1 and k1. special cases are reported in figure 1 b  and 1 c   in which open intervals are represented; in these cases  one numerical parameter is sufficient to define the fuzzy set. 

figure 1 - example of trapezoidal fuzzy sets used to define the semantics of predicates. the ordinate represents the troth value  whereas the abscissa vp represents the values of the base variable. 
　for each parameter k  the teacher must specify the range in which k has to be searched for and the approximation requested for tuning its value. a default value can also be 
1 	machine learning 
specified; this will be used by the system in the case that this learning capability is explicitly disabled by the teacher. 
　the semantics for the logical connectives can be chosen by the teacher  as well  in order to implement a specific evidential calculus. by default  smart+ adopts a pair of tnorm and t-conorm functions  bergadano et a/.  1  for the and and or connectives  respectively. the semantics of the quantifiers can be expressed by means of these connectives  yager  1  bergadano et a/.  1 . 
1 main learning strategy 
smart+ shares with foil  quinlan  1  a general-tospecific search strategy for inductive learning  but it has a richer set of specializing operators and uses more sophisticated strategies. 
　the basic search strategy has been combined with a policy of reduction to subproblems. the main goal of the reduction to subproblems is that of producing a structured knowledge base; in particular  it can be considered as a form of constructive learning  where intermediate concepts  corresponding to subsets of ho  are generated automatically. in fact  the classification rules are actually intended to be valid only in the context of already hypothesized concepts  as described in section 1. then  the learned rules do not constitute a  flat** set; on the contrary  they are organized into a graph g of rules  called subproblem graph. nodes of g correspond to sets of concepts  and logical formulas label edges. a node  h f  of g is called a  subproblem   because it represents the problem of discriminating among the classes it contains. the root  ho fo  of the graph corresponds to the initial problem  i.e. the problem of distinguishing among the whole set of concepts. notice that the sets of examples occurring in sibling nodes need not be disjoint. 
　a complete and consistent solution of a problem  ho  fo  is obtained when all the rule weights are equal to 1  and the leaves of g contain sets fi whose union equals fq. this is the ideal case  when no noise and errors are present. however  in real-world problems this assumption cannot be accepted and the final solution turns out to be  in general  inconsistent and/or incomplete. 
　the use of a subproblem graph  instead of flat classification rules  enhances classification efficiency  since common parts of different rules will only be tested once. moreover  it makes it easier to learn the rules  because it separates the task into smaller contexts which can be solved separately. finally  this structure of the knowledge base is well suited for a diagnostic process based on a multi-stage refinement strategy. in figure 1 the process of generating the subproblem graph is schematically represented. 
　for each subproblem the learning process follows the same search scheme  as described in the following. the search within a subproblem sp; =  hi fi  develops a specialization tree according to a general-to-specific strategy  similar to the one used in foil  quinlan  1  or focl  pazzani and kibler  1 . the difference is that many search  strategies  going from best-first to beamsearch  are available in addition to the popular hill climbing one. moreover  smart+ has a reacher set of operators for 

building a more specific formula; in particular it can use numerical quantification and numerical optimization. 

figure 1 - subproblem reduction process. 
when an inductive hypothesis of the type is found 
 being hj a proper subset of hi   a rule of type  1  is created and the events  belonging to the extension of  are declared  solved . consequently  the focus of the search is moved on those formulas  on the frontier of the tree  which contain in their extension not yet solved events. the development of the specialization tree is guided by combining reasoning about background knowledge  supplied to the system  and statistical criteria. the primary goal of the control strategy is to find rules that are both statistically significant  because verified by many learning events  and meaningful with respect to the background knowledge. in this phase only consistent rules  i.e. with w=l  are accepted. 
　the search stops when all the events fi are covered by some rule of type  1  or when there is no more hope of discovering such rules. in this case the system can also decide to accept rules with weight  afterwards  all the events matched by rules having the same conclusion hj are merged together into a single set fj and a new subproblem  hj fj  is defined. this process offers an important advantage: in every subproblem  all the events  which were previously subdivided among the extensions of many rules  are or-ed into a single relation. in this way  the inductive strategy  relaying on statistical criteria  acquires new strength. a deeper description of this search procedure can be found in  bergadano et a/.  1 . 
1 guiding induction using domain theories 
a fundamental characteristic of smart+ is its ability of guiding induction by exploiting a  possibly incomplete  domain theory. this is achieved by integrating induction with deduction and/or with abduction. the mechanism far integrating induction and deduction has been described in  bergadano and giordana  1 . the specialization tree  generated by the induction process  has the same structure of the deduction tree generated by a theorem prover based on sld. therefore  a domain theory can be used to construct the initial part of the specialization tree  and  then  induction can be used to complete the proof  discovering information missing in the domain theory. if the theory is perfect  the method becomes an ebg  mitchell et al.  1  dejong and mooney  1 . 
　the abductive mechanism has been introduced more recently and  for some aspects  shares some ideas with systems like cigol  muggleton and buntine  1  and clint  de raedt and bruynooghe  1 . given a horn clause theory t and a formula 1  associated to a node n of the specialization tree  the formula  obtained by exhaustively applying the absorption rule  sammut and bannerji  1  between and clauses in t  is said to be the generalization of  p through the theory t. formula is said to be an explanation of  with respect to t  in the sense that 
t    poole  1  torasso and console  1 . therefore  the basic inductive procedure is modified as follows: 
 a  let be the formula selected for specialization at a given step; smart+ determines the set  of literals that can be used to specialize  with a non-negative information gain. 
 b for each literal lj a  all the possible generalizations with respect to t  are computed for formulas qi ＊ 
 c  then  all the formulas are evaluated  using a criterion which takes into account both the information coming from the extension of on the learning set fo and the generalization obtained with respect to the theory t. the best n formulas are then added to the specialization tree. the value of n can be 1  if a greedy strategy is used  or greater than 1 if a beam-search or a best-first strategy is used. 
　using this abductive strategy  a domain theory will bias only to a limited extent the choice of a literal  without imposing a determinate choice  as it happens using the purely deductive strategy. our claim is that this abductive strategy could be a knowledge-based alternative to the one based on determinate literals  introduced to resolve the impasse of greedy algorithms. 
1 search control strategies 
a choice among several search control strategies  ranging from foil's hill climbing to the best-first and beam-search ones  is offered by smart+ for guiding the inductive process. the evaluation rule for each strategy is also a matter of choice. given a formula in the specialization tree  the score of is computed as the sum of two independent terms: 
where is a measure of the quality of a hypothesis and is a measure that tries to capture how w e l l i s  explained  by a given domain theory t. the coefficients a 
	botta and giordana 	1 


1 	machine learning 


	botta and giordana 	1 

1 	evaluation on a test case 
the application domain is quite complex  even though artificial  and bears many features of a real-world one. ten capital letters of the english alphabet have been chosen  a horn clause theory has been invented and used by a random choice theorem prover in order to generate instances of these letters  some of which are shown in figure 1. each letter is represented as a set of segments that are described by the initial and final  x y  coordinates in a cartesian plane. from optimisation plus the global optimisation performed by genetic algorithm  hill-climbing  theory t1 and rule  1   a=1 b=l  local+ga opt and t1 . 
　the second column in table i refers to the number of formulas in the learned knowledge. the next three columns refer to the recognition rate  the correct label is assigned to a sample   the error rate  cases in which a wrong label is assigned to a sample  and the ambiguity rate  cases in which no label is assigned to a sample   respectively  evaluated on an independent test set of 1 events  1 per class . 

these attributes  other features can be extracted  such as the length of a segment  its orientation  its preceding and following segments  and so on. some of these features are numerical by nature and then parameterised fuzzy semantics definition have been adopted for the corresponding symbolic literals. 
1 	machine learning 

　a last comment concerns the comparison between hillclimbing and beam-search strategies  in absence of theory. even though extensive tests have not been performed at this time  the beam-search strategy seems to be more effective  by letting the system explore a smaller part of the search space but converging more quickly towards robust knowledge. 
　however  this test is not enough for stating the superiority of one strategy with respect to the other; on the other hand  what can be said  also on the basis of other applications  is that in learning structured concepts a global evaluation rule like  1   associated with a best-first or beam-search strategy  may result in a more effective and simpler knowledge base. 
1 	conclusion 
as an alternative to the empirical strategy based on determinate literals  we suggested the use of a domain theory to guide the induction algorithm. such a theory does not need to be complete or consistent and can be limited to a rough description of the structural aspects of the problem. an abductive reasoning mechanism has been embedded in 
smart+ in order to guide the induction process. moreover  a new evaluation rule  combining both information coming from data with information coming from the theory  has been proposed. in this way  bias on induction due to the theory is much more gentle than when a top-down deduction is used as in a previous version of the system. 
　furthermore  smart+ has been equipped with a method for learning fuzzy set values in first order logic environments  which combines an extension of the methodology developed for learning relations with genetic algorithms. in particular  genetic algorithms can be an effective tool for refining numerical parameters such as thresholds  weights and coefficients which control the flexible matching of a symbolic expression against a real world instance. the applicability of the method has been demonstrated on a non-trivial learning problem of pattern recognition. 
　however  the conclusion we can make from this experiment is that the symbolic approach proposed by artificial intelligence can be extended in order to be effective in dealing with patterns of the real world such as complex signals. in particular this approach can compete quite well with other methods  such as neural networks  in domains where structural knowledge is relevant and where there exists background knowledge which can be exploited. 
