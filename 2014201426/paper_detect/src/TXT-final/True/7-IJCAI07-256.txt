
we propose a supervised maximum entropy ranking approach to pronoun resolution as an alternative to commonly used classification-based approaches. classification approaches consider only one or two candidate antecedents for a pronoun at a time  whereas ranking allows all candidates to be evaluated together. we argue that this provides a more natural fit for the task than classification and show that it delivers significant performance improvements on the ace datasets. in particular  our ranker obtains an error reduction of 1% over the best classification approach  the twin-candidate model. furthermore  we show that the ranker offers some computational advantage over the twincandidate classifier  since it easily allows the inclusion of more candidate antecedents during training. this approach leads to a furthererror reduction of 1%  a total reduction of 1% over the twincandidate model .
1 introduction
pronoun resolution concerns the identification of the antecedents of pronominal anaphors in texts. it is an important and challenging subpart of the more general task of coreference  in which the entities discussed in a given text are linked to all of the textual spans that refer to them. correct resolution of the antecedents of pronouns is important for a variety of other natural language processing tasks  including -but not limited to- information retrieval  text summarization  and understanding in dialog systems.
　the last decade of research in coreference resolution has seen an important shift from rule-based  hand-crafted systems to machine learning systems  see  mitkov  1  for an overview . the most common approach has been a classification approach for both general coreference resolution  e.g.   mccarthy and lehnert  1; soon et al.  1; ng and cardie  1   and pronoun resolution specifically  e.g.   morton  1; kehler et al.  1  . this choice is somewhat surprising given that pronoun resolution does not directly lend itself to classification; for instance  one cannot take the different antecedent candidates as classes since there would be far too many overall and also the number of potential antecedents varies considerably for each anaphor.
　despite this apparent poor fit  a classification approach can be made to work by assuming a binary scheme in which pairs of candidate antecedents and anaphors are classified as either coref or not-coref. many candidates usually need to be considered for each anaphor  so this approach potentially marks several of the candidates as coreferent with the anaphor. a separate algorithm must choose a unique antecedent from that set. the two most common techniques are  best-first  or  closest-first  selections  see  soon et al.  1  and  ng and cardie  1   respectively .
　a major drawback with the classification approach outlined above is that it forces different candidates for the same pronoun to be considered independently since only a single candidate is evaluated at a time. the probabilities assigned to each candidate-anaphor pair merely encode the likelihood of that particular pair being coreferential  rather than whether that candidate is the best with respect to the others. to overcome this deficiency  yang et al.  propose a twincandidate model that directly compares pairs of candidate antecedents by building a preference classifier based on triples of np mentions. this extension provides significant gains for both coreference resolution  yang et al.  1  and pronoun resolution  ng  1 .
　a more straightforward way to allow direct comparison of different candidate antecedents for an anaphor is to cast pronoun resolution as a ranking task. a variety of discriminative training algorithms -such as maximum entropy models  perceptrons  and support vector machines- can be used to learn pronoun resolution rankers. in contrast with a classifier  a ranker is directly concerned with comparing an entire set of candidates at once  rather than in a piecemeal fashion. each candidate is assigned a conditional probability  or score  in the case of non-probabilistic methods such as perceptrons  with respect to the entire candidate set. ravichandran et al.  show that a ranking approach outperforms a classification approach for questionanswering  and  re rankers have been successfully applied to parse selection  osborne and baldridge  1; toutanova et al.  1  and parse reranking  collins and duffy  1; charniak and johnson  1 .
　in intuitive terms  the idea is that while resolving a pronoun one wants to compare different candidate antecedents  rather than consider each in isolation. pronouns have no inherent lexical meaning  so they are potentially compatible with many different preceding np mentions provided that basic morphosyntactic criteria  such as number and gender agreement  are met. looking at pairs of mentions in isolation gives only an indirect  and therefore unreliable  way to select the correct antecedent. thus  we expect pronoun resolution to be particularly useful for teasing apart differences between classification and ranking models.
　our results confirm our expectation that comparing all candidates together improves performance. using exactly the same conditioning information  our ranker provides error reductions of 1%  1%  and 1% on three datasets over the twin-candidate model. by taking advantage of the ranker's ability to efficiently compare many more previous np mentions as candidate antecedents  we achieve further error reductions. these reductions cannot be easily matched by the twin-candidate approach since it must deal with a cubic increase in the number of candidate pairings it must consider.
　we begin by further motivating the use of rankers for pronoun resolution. then  in section  1   we describe the three systems we are comparing  providing explicit details about the probability models and training and resolution strategies. section  1  lists the features we use for all models. in section  1   we present the results of experiments that compare the performance of these systems on the automatic content extraction  ace  datasets.
1 pronoun resolution as ranking
the twin-candidate approach of yang et al.  goes a long way toward ameliorating the deficiencies of the singlecandidate approach. classification is still binary  but probabilities are conditioned on triples of np mentions rather than just a single candidate and the anaphor. each triple contains:  i  the anaphor   ii  an antecedent mention  and  iii  a nonantecedent mention. instances are classified as positive or negative depending on which of the two candidates is the true antecedent. during resolution  all candidates are compared pairwise. candidates receive points for each contest they win  and the one with the highest score is marked as the antecedent.
　the twin-candidate model thus does account for the relative goodness of different antecedent candidates for the same pronoun. this approach is similar to error-correcting output coding  dietterich  1   an ensemble learning technique which is especially useful when the number of output classes is large. it can thus be seen as a group of models that are individual experts on teasing apart two different candidates. nonetheless  the approach is still hampered by the fact that these models' probability estimates are only based on two candidates rather than all that are available. this means that unjustified independence assumptions made during model training and usage may still hurt performance.
　while it is a common and often necessary strategy to adapt a task to fit a particular modeling approach  pronoun resolution has in fact been unnecessarily coerced into classification approaches. while the twin-candidate strategy is an improvement over the single-candidate approach  it does not address the fundamental problem that pronoun resolution is not characterized optimally as a classification task. the nature of the problem is in fact much more like that of parse selection. in parse selection  one must identify the best analysis out of some set of parses produced by a grammar. different sentences of course produce very different parses and very different numbers of parses  depending on the ambiguity of the grammar. similarly  we can view a text as presenting us with different analyses  candidate antecedents  which each pronoun could be resolved to.  re ranking models are standardly used for parse selection  e.g.   osborne and baldridge  1    while classification has never been explored  to our knowledge.
　in classification models  a feature for machine learning is actually the combination of a contextual predicate1 combined with a class label. in ranking models  the features simply are the contextual predicates themselves. in either case  an algorithm is used to assign weights to these features based on some training material. for rankers  features can be shared across different outcomes  e.g.  candidate antecedents or parses  whereas for classifiers  every feature contains the class label of the class it is associated with. this sharing is part of what makes rerankers work well for tasks that cannot be easily cast in terms of classification: features are not split across multiple classes and instead receive their weights based on how well they predict correct outputs rather than correct labels. the other crucial advantage of rankers is that all candidates are trained together  rather than independently   each contributing its own factor to the training criterion. specifically  for the maximum entropy models used here the computation of a model's expectation of a feature  and the resulting update to its weight at each iteration  is directly based on the probabilities assigned to the different candidates  berger et al.  1 . from this perspective  the ranker can be viewed as a straightforward generalization of the twin-candidate classifier.
　the idea of ranking is actually present in the linguistic literature on anaphora resolution. it is at the heart of centering theory  grosz et al.  1  and the optimality theory account of centering theory provided by beaver . ranking is also implicit in earlier hand-crafted approaches such as lappin and leass   wherein various factors are manually given weights  and goes back at least to  hirst  1 .
1 the three systems
here  we describe the three systems that we compare:  1  a single-candidate classification system   1  a twin-candidate classification system  and  1  a ranking system. for each system  we give the probability models and the training and resolution procedures. all model parameters are estimated using maximum entropy  berger et al.  1 . specifically  we estimate parameters with the limited memory variable metric algorithm implemented in the toolkit for advanced discriminative modeling1  malouf  1 . we use a gaussian prior with a variance of 1 - no attempt was made to optimize the prior for each data set or system.
　maxent models are well-suited for the coreference task  because they are able to handle many different  potentially overlapping features without making independence assumptions. previous work on coreference using maximum entropy includes  kehler  1; morton  1; 1 . in principle  other discriminative algorithms such as perceptrons and support vector machines could be used for each of the systems  though the output would not be probabilistic for these.
　the systems are trained and tested on data originally annotated with coreferencechains. this means that in principle  an anaphoric pronoun can have several antecedents. since pronouns show a strong tendency to take very local antecedents  we take only the closest antecedent as an anchor when creating training instances.
　we use the following notation for all models: π is an anaphoric pronoun and a = {α1 ... αk} is a set of antecedent candidates. the task of pronoun resolution is to predict the correct antecedentout of a.
1 the single-candidate classifier
for the single-candidate classifier  we use the model  training and test procedures of  soon et al.  1 .
model
the single-candidate classification approach tackles coreference in two steps by:  i  estimating the probability  pc coref  of having a coreferential outcome given a pair of mentions   and  ii  applying a selection algorithm that will single out a unique candidate out of the subset of candidates αk for which the probability
pc coref reaches a particular value  typically .1 . note that in this case  the numberof events created for a given pronoun is just the cardinality of the set of candidates.
　　　　　　　　　  coref   pc coref
training
training instances are constructed based on pairs of mentions of the form   where π and αi are the descriptions for an anaphoric pronoun and one of its candidate antecedents  respectively. each such pair is assigned either a label coref  i.e. a positive instance  or a label not-coref  i.e. a negative instance  depending on whether or not the two mentions corefer. in generating the training data  we create for each anaphoric pronoun:  i  a positive instance for the pair where αi is the closest antecedent for π  and  ii  a negative instance for each pair where αj intervenes between αi and π.
resolution
once trained  the classifier is used to select a unique antecedent for each anaphoric pronoun in the test documents. in the soon et al.  system  this is done for each pronoun π by scanning the text right to left  and pairing π with each preceding mention αi. each test instance thus formed is then evaluated by the classifier  which returns a probability representing the likelihood that these two mentions are coreferential. soon et al.  use  closest-first  selection: that is  the process terminates as soon as an antecedent  i.e.  a test instance with probability   .1  is found or the beginning of the text is reached.
1 the twin-candidate classifier
the twin-candidatemodel was proposed by yang et al.  in the context of coreference resolution. ng  more recently used it specifically for the pronoun resolution task; for this reason  we adopt his training and test procedures.
model
with the twin-candidate approach  resolving anaphoric pronouns is also a two step process. the first step involves estimating the probability ptc first  of the pronoun π corefering to the first antecedent candidate αi. since this is still binary classification  we have the dual probability
ptc second  which expresses the likelihood of the pronoun π being coreferential with the second antecedent candidate αj. as with the single-candidate classifier  the selection of the correct antecedent is done in a separate step based on the parameters learned by the model. but with the twin-candidate approach  the antecedent selection algorithm involves comparing candidates in a pairwise manner.
  first   ptc first 
training
training instances are constructed based on triples of mentions of the form  where π describes a pronominal anaphor and αi and αj are the descriptions for two of its candidate antecedents and αi is stipulated to be closer to π than αj. these instances are labeled either first if αi is the correct antecedent or second if αj is the correct antecedent. for this to work  one has to add an additional constraint on the creation of instances  namely: exactly one and only one of the two candidates can be coreferential with the pronoun. note that the number of instances created is rather large; it is in fact cubic  since each triple generates two instances  in the number of mentions in the document if one assumes that all mentions preceding a pronoun are potential candidates. in order to obviate this problem  ng  suggests using a window of 1 sentences including the sentence of the pronoun and the immediately preceding three sentences.
resolution
once trained  the twin-candidate classifier is used to select a unique antecedent for the given anaphoric pronoun π. like yang et al.  and ng   we use a round robin algorithm to compare the members of the candidate set for π. more specifically  test instances are created for each pair of candidates  αi and αj  where αj precedes αi. these instances are presented to the classifier  which determines which one of the candidates is preferred; the winner of the comparison gets one point. finally  the candidate with the most points at the termination of the round robin competition gets selected as the antecedent for π. we use a window of three sentences as we did in training.
1 the ranker
the following describes our training and resolution procedures for the ranking system.
model
viewed as ranking task  pronoun resolution is done in a single step  by computing the probability pr αi|π   which is the conditional probability of a particular candidate αi being the antecedent of the anaphoric pronoun π. here  a unique event is created for each pronoun and its entire candidate set a. finally  selecting the correct antecedent merely boils down to picking the most likely candidate in this set.
		 1 
training
the training instances for the ranker system are built based on an anaphoric pronoun π and the set of its antecedent candidates a. the candidate set is composed of:  i  the closest antecedent for π  which is singled out as such  and  ii  a set of non-antecedents. the construction of the latter set proceeds by taking the closest antecedent as an anchor and adding all the non-antecedents that occur in a window of 1 sentences around it  including the current sentence of the antecedent  the preceding sentence  and the two following sentences . in contrast with the previous models  note that the comparison between the different candidates in a is here directly part of the training criterion; these are used in the denominatorof the above equation.
resolution
once trained  the ranker is used to select a unique antecedent for each anaphoric pronoun. given preference shown by pronouns for local resolutions and in order to reduce testing time  we build our candidate set by taking only the preceding mentions that occur in a window of 1 sentences  including the pronoun's sentence and the 1 sentences preceding it. the ranker provides a probability distribution for the entire candidate set  and the candidate with the highest conditional probability is chosen as the antecedent. in cases of ties  the alternative that is the closest to the pronoun is chosen.
1 feature selection
in this study  we focused on features obtainable with very limited linguistic processing. our features fall into three main categories:  i  features of the anaphoric pronoun   ii  features of antecedent candidate np  and  iii  relational features  i.e.  features that describe the relation between the two mentions . the detailed feature set is summarized in table 1.
persprot if π is a personal pronoun; else fpossprot if π is a possessive pronoun; else fthird pers prot if π is 1rd person pronoun; else fspeechprot if π is 1st  1nd person pronoun; else freflprot if π is a reflexive pronoun; else fproformt if π is lower-cased pronoun; else fprolconxpos tag of word on the left of πprorconxpos tag of word on the right of πpro sconxpos tags of words around πfeatures of the pronoun

features of the antecedent candidate

ante wd lenthe number of tokens in α's stringpronantet if α is a pronoun; else fpnantet if α is a proper name; else findefantet if α is a indefinite np; else fdefantet if α is a definite np; else fdemantet if α is a demonstrative np; else fquantantet if α is a quantified np; else fantelconxpos tag of word on the left of αanterconxpos tag of word on the right of αantesconxpos tags of words around αante m ctnumber of times α's string appears previously in the textnearest antet if α is the nearest np candidate compatible in gender  person  and number; else fembedantet if α is embedded in another np; else f
relational features

sdistbinned values for sentence distance between π and αnpdistbinned values for mention distance between π and αnumagrt if π and α agree in number; f if they disagree; unk if either np's number cannot be determinedgenagrt if π and α agree in gender; f if they disagree; unk if either np's gender cannot be determined
table 1: feature selection for pronoun resolution
　for the pronoun features  we encoded into our features information about the particular type of pronoun e.g.  personal  possessive  etc.  and the syntactic context of the anaphoric pronoun. the syntactic context is here approximated as pos tags surrounding the pronoun. for the antecedent candidates  we also use information about the type of np at hand as well as pos context information. other features encode the salience of a given antecedent: whether the candidate np string has been seen up to the current point  whether it is the nearest np  and whether it is embedded in another larger np. finally  we use features describing the relation between the anaphoric np and its candidate antecedent  namely distance features  in terms of sentences and in terms of np mentions  and compatibility features  i.e.  number and gender agreement . in addition to the simple features described above  we used various composite features. more specifically  we used features combining:  i  distances and the type of the pronoun  e.g.  reflexive  possessive    ii  the named entity for the antecedent with various information on the pronoun  such as the pronoun form and the pronoungender   iii  the last three characters in the antecedent head word and the pronoun form and gender.
1 experiments and results
1 corpus and evaluation
for evaluation  we used the datasets from the ace corpus  phase 1 . this corpus is divided into three parts  corresponding to different genres: newspaper texts  npaper   newswire texts  nwire   and broadcasted news transcripts  bnews . each of these is split into a train part and a devtest part. we used the devtest material only once  namely for testing. progress during the development phase was estimated only by using cross-validation on the training set for the npa-
per section.
　in our experiments  we used all forms of personal  all persons  and possessive pronouns that were annotated as ace  markables   i.e.  the pronouns associated with the following named entity types: facility  gpe  geo-political entity   location  organization  person  vehicle  weapons. this excludes pleonastics and references to eventualities or to nonace entities. together  the three ace datasets contain 1 and 1 such referential pronouns  for training and testing  respectively.
　finally  note that in building our antecedent candidate sets  we restricted ourselves to the true ace mentions since our focus is on evaluating the classification approaches versus the ranking approach rather than on building a full pronoun resolution system. it is worth noting that previous work tends to be vague in both these respects: details on mention filtering or providing performance figures for markable identification are rarely given.
　no human-annotated linguistic information is used in the input. the corpus text was preprocessed with the opennlp toolkit1  i.e.  a sentence detector  a tokenizer  a pos tagger  and a named entity recognizer .
　following common practice in pronoun resolution  we report results in terms of accuracy  which is simply the ratio of correctly resolved anaphoric pronouns. since the ace data is annotated with coreference chains  we assumed that correctly resolving a pronoun amounts to selecting one of the previous elements in chain as the antecedent.
1 comparative results
the results obtained for the three systems on the three ace datasets are summarized in table  1 .
systembnewsnpapernwirescc111tcc111rk111table 1: accuracy scores for the single-candidate classifier  scc   the twin-candidate classifier  tcc   and the ranker  rk .
　as shown by this table  the ranker system significantly outperforms the two classifier systems  with an overall f-score of 1%. this corresponds to average  weighted  improvements of 1%  i.e.  an error reduction of 1%  over the single-candidate classifier and of 1%  i.e.  an error reduction of 1%  over the twin-candidate classifier. the scores obtained for the first dataset npaper are substantially better than for the two other datasets; we suspect that this difference is due to the fact that we only did developmenton that dataset.
1 additional results
in this section  we discuss an additional experiment aimed at getting additional insight into the potential of the ranker. in the previous experiments  we provided a rather limited context for training: we only considered mentions in a window of 1 sentences around the correct antecedent. our main motivation for doing this was to stay as close as possible to the training conditions given in  ng  1  for the twin-candidate approach  thereby giving it the fairest comparison possible. an open question is to what extent can widening the window of antecedent candidates help the ranker to learn better parameters for pronoun resolution. to answer this question  we ran an experiment on the same three ace datasets and widened the window of sentences by collecting  in addition to the closest antecedent  all non-antecedents preceding the anaphor up to 1 sentences before the antecedent:1 the results for this experiment are reported in table  1 :
systembnewsnpapernwirerk  w = 1 111table 1: accuracy scores for the ranker  rk  with a window of 1 sentences.
　these figures show a significant improvement on the first two datasets  with an average score of 1%. this translates into an average gain of 1% or an error reduction of 1%.
1 conclusions
we have demonstrated that using a ranking model for pronoun resolution performs far better than a classification model. on the three ace datasets  the ranker achieves an error reductions of 1% overthe twin-candidateclassifier  even when both have exactly the same features and experimental settings  e.g.  number of sentences from which to consider candidates . our results thus corroborate ravichandran et al.'s  similar finding that ranking outperforms classification for question-answering. clearly  the ability to consider all potential antecedents together  rather than independently  provides the ranker with greater discriminating power.
　the round robin nature of the pairwise contests in the twincandidate approach imposes a restrictive computational cost on its use which limits the number of np mentions that can be considered in a candidate set. the ranker does not suffer from this limitation  and we show that the ranker achieves a further error reduction of 1%  or total of 1% over the twin-candidate model  by increasing the size of the candidate set used in training.
　the most direct comparison with previous results is with ng   who obtained 1% and 1% on the newspaper and newswire parts of ace. our best results for these parts were 1% and 1%. though our focus is on comparing classification versus ranking  it is nonetheless interesting that we match ng's model on the newspaper texts since we use a much simpler feature set and only a single model rather than a complex ensemble. we would thus expect the use of a ranker in place of the twin-candidate classifier would achieve further improvements for his set-up.
　the main difference between the twin-candidate approach and the ranking approach is that under the former  candidates are compared by pairs  the best candidate is the one that has won the most times   whereas in the latter an ordering is imposed on the entire set at once. a potential advantage of the ranking approach is that it could allow one to define features on the candidate set itself. another advantage of the ranker over the preference classifier is how ranking is obtained: only the ranker guarantees a global winner.
　while our ranker outperforms the classifiers outright  some benefit could be gained by using both approaches together. it would be straightforward to integrate classifiers and rankers in an ensemble model. for example  a ranker could use the results of the classifier as features in its model.
acknowledgments
we would like to thank the four anonymous reviewers for their comments. this work was supported by nsf grant iis1.
