
users can often naturally express their preferences in terms of ideal or non-ideal solutions. we show how to reason about logical combinations of distance constraints on ideals and non-ideals using a novel global constraint. we evaluate our approach on both randomly generated and real-world configuration problem instances.
1 introduction
in many application domains users specify their desires in terms of assignments to  a subset of  problem variables. for example  when planning a vacation  a user might have an ideal holiday in mind. this ideal holiday might  however  be infeasible. consider as another example purchasing a car. the user might like two models on display  say volvo and
jaguar . moreover  she does not like at all a third model  say lada . as a result  she might want to sample models similar to volvo and jaguar whilst different from lada. she might therefore specify  i would like something either like the volvo or the jaguar  but not the lada . this type of query is difficult to tackle using usual constraint-based preferences since it is not articulated in terms of variables and/or constraints but rather  partial  solutions. many formalisms for representing preferences in constraint satisfaction assign preferences to individual constraints  bistarelli et al.  1 . others  such as cp-nets  boutilier et al.  1   specify preferences at a variable level. however  users often like to describe their preferences at a solution level  rossi and sperduti  1 .
¡¡in this paper we present an algebra for complex expressions of distance constraints. we describe a novel soft global constraint for propagating constraints of distance  characterise the conditions under which we can propagate it tractably  and report encouraging results on real-world and randomly generated instances.
1 preliminaries
a constraint satisfaction problem  csp  is a triple p =   where x is a finite set of variables x =  {x1 ... xn}  d is a set of finite domains d = 
{d x1  ... d xn } where the domain d xi  is the finite set of values that variable xi can take  and a set of constraints c =  {c1 ... cm}. each constraint ci is defined by an ordered set var ci  of variables and a set sol ci  of allowed combinations of values. an assignment of values to the variables in var ci  satisfies ci if it belongs to sol ci . a feasible solution is an assignment to each variable of a value from its domain such that every constraint in c is satisfied. in addition to the csp  we assume that we have some symmetric  reflexive  total and polynomially bounded distance function  ¦Ä  between  partial  instantiations of the variables  i.e. an assignment of values to some subset of x. to make reasoning easier  we assume that distance is decomposable into a sum of distances between the assignments to individual variables. for example  we might have the hamming distance given by  or the generalised manhattan distance given by.
1 problems of distance
we suppose the user expresses her preferences in terms of ideal or non-ideal  partial  solutions. partiality is important so we can ignore irrelevant attributes. for example  we might not care whether our ideal car has run-flat tires or not. the fundamental decision problems underlying our approach ensure that a solution is at a given distance to  resp. from  an ideal  resp. non-ideal  solution.
dclose  resp. ddistant 
instance. a csp  p  a symmetric  reflexive  total and polynomially bounded distance function ¦Ä  and p  a partial instantiation of the variables of p. question. does there exist a solution s ¡Ê sol p  such that ¦Ä p s    d  resp. ¦Ä p s  ¡Ý d .
¡¡dclose and ddistant are np-complete in general  since they can be used to decide the csp  p. if the distance d is not fixed  dclose and ddistant are not necessarily polynomial even if the underlying csp is itself polynomial  bailleux and marquis  1 . however  one can identify tractable restrictions of these problems if d is fixed and the underlying csp is polynomial  bailleux and marquis  1 . we can specify more complex problems of distance by combining primitive distance constraints using negation  conjunction and disjunction; some examples are shown in figure 1. the laws of our basic algebra for constructing constraint expressions are as follows  we slightly abuse the notation used to define the decision problems  without consequence  where a and b are ideal and non-ideal  partial  assignments to the variables:
ddistant a     dclose a 
ddistant a ¡Å b    ddistant a  ¡Å ddistant b 
                  dclose a ¡Ä b  ddistant a ¡Ä b    ddistant a  ¡Ä ddistant b 
   dclose a ¡Å b 
¡¡more complex expressions are also possible. for example  we can construct expressions using implies  iff  xor  or ifthen. such connectives can  however  be constructed using the standard boolean identities.
 sol p  sol p 	 a  dclose a 	 b  ddistant a 
	 c  dclose a ¡Å b 	 d  ddistant a ¡Ä b 
	 e  dclose a ¡Ä b 	 f  dclose a ¡Äddistant b 
figure 1: a graphical representation of the basic constraint expressions that can be constructed; a and b are solutions  sol p  is the set of solutions to the csp p  and the radius of the circles represents the distance  d. the shaded region represents the solutions that satisfy the constraints.
1 optimisation problems
rather than specify the precise distance from the ideals and non-ideals to the solution it may be more useful to minimise  maximise  the distance to an ideal  resp. a non-ideal .
close  resp. distant 
instance. a csp  p  a distance function ¦Ä  and a partial solution p.
question. find a solution s ¡Ê sol p  such that
  resp.
¡¡these optimisation problems are closely related to mostclose and mostdistant defined in  hebrard et al.  1 . like these problems  close and distant are fpnp log n complete. however  there are some differences. first  mostclose finds the feasible solution nearest to a given subset of solutions. by comparison  close finds the feasible solution nearest to just one solution. we will soon extend close to nearness to combinations of solutions. second  the ideal solution that close is trying to get near to might not be feasible whereas in mostclose it is. third  if the ideal solution is feasible then close will return it whilst mostclose will find the next nearest feasible solution.
¡¡we can again consider logical combinations of ideals and non-ideals. for example  we might want to minimise the distance to one ideal or maximise the distance from a non-ideal. distances can be combined in a number of ways. for example  if we want to minimise the distance to ideals a and b  we minimise the maximum distance to either. similarly  if we want to minimise the distance to ideals a or b  we minimise the minimum distance to either. this gives us a new global objective  f  as shown in table 1.
table 1: examples of some simple distance constraints and their corresponding objective functions to be minimised.
	constraint	objective function
close a ¡Å b  close a b f == maxmin  ¦Ä¦Ä  s as a   ¦Ä ¦Ä  s bs b    	¡Ä	f
¡¡other combinators like + and min are also possible. we also permit the distance function to depend on the ideal or non-ideal. for example  we might want distance from ideal c to count twice as much as distance from d. in this case  close c ¡Å d  gives the objective function f = min 1¦Ä s c  ¦Ä s d  . to make combinations uniform  we convert maximising the distance  ¦Ä  from a non-ideal a into minimising a new distance where m is the maximum possible distance returned by . in this way  we get a single objective f which we need to minimise. finally  we consider compiling out an objective function for a more complex logical combination of distance constraints.
example 1  conjunction  we consider again the example of the choice of car configuration within a large catalogue used in section 1. a customer is interested in two models  volvo and jaguar   whilst disliking a third model  lada . the query close volvo   close jaguar   distant lada  implements this wish. this expression means that we seek a solution s that is simultaneously as similar as possible to volvo and jaguar and as different as possible from lada. it can be reformulated as the following logical expression:
¡¡ close volvo  ¡Ä close jaguar  ¡Ä distant lada   which gives the following objective function  f  substituting max for logical conjunction and converting maximising distance to minimising the distance complement:
f = max ¦Ä s volvo  ¦Ä s jaguar  m   ¦Ä s lada  .
1 propagation and complexity
we consider how to propagate such distance constraints. we observed in  hebrard et al.  1  that the problems mostclose and mostdistant can be solved using symmetrically equivalent algorithms. similarly  the problems studied here are symmetric  therefore we focus on minimising the distance  similarity  rather than maximising it  diversity . thus  we introduce a soft global constraints soperator   ¡Ê {min max} is used to aggregate distances toimilar . the n-ary individual ideals:
similar   x1 ... xn  n vi	= {v1 ... vk} {¦Ä1 ... ¦Äk}  iff.
the operator min handles disjunctions of distance constraints whilst max handles conjunctions. the constraint ensures that the distance to a set of ideals v = {v1 ... vk} is at most n. notice that we assume that the distance between two vectors is equal to the sum of the distances on all coordinates.
¡¡we showed that enforcing generalised arc consistency  gac  on this constraint is np-hard for   = max  hebrard et al.  1 . however  enforcing gac is tractable when the number of ideals is bounded. on the other hand  for   = min  gac can be enforced in polynomial time for any number of ideals. table 1 summarises the complexity of enforcing gac on the similar  constraint. we now prove the results given in this table.
disjunction  similarmin . propagating the similar  constraint for a single ideal can be done in polynomial time. indeed  it is equivalent to the problem dclose on a network involving only domain constraints. algorithm 1 implements a linear  o nd   algorithm for filtering similar with respect to a single ideal  the value of   is undefined in this case .
algorithm 1: prune a distance constraint  single ideal.
data: x1 ... xn n v ¦Ä
result: gac closure of similar  x1 ... xn  n v ¦Ä 

the notation min n  stands for the minimal value in the
table 1: the complexity of propagating the similar constraint on a disjunction  similarmin  or a conjunction  similarmax  of k ideals where k is bounded by a constant  or a polynomial function  p n   in the size of the problem.
similarminsimilarmaxk ¡Ê o 1 o ndk o d1nk+1 k	 p n   ndk np-hard	¡Ê o	o
domain of n. algorithm 1 first computes the smallest distance to the ideal v in loop 1. then the domain of n and of x1 ... xn are pruned in line 1 and loop 1. for disjunctive combinations  we compute the values that are inconsistent for each distance constraint using algorithm 1  and prune those values in the intersection. using constructive disjunction  we achieve gac in o ndk  time.
conjunction  similarmax . conjunctive combinations of distance constraints are more problematic. consider the conjunctive hamming distance constraint:

deciding the satisfiability of this formula is np-hard. hence  enforcing gac on similarmax with respect to an arbitrary number of ideals is np-hard  hebrard et al.  1 . however  we shall investigate filtering methods stronger than a straightforward decomposition into distance constraints with respect to a single ideal. we shall see in the next section that even though a distance constraint to a single ideal is easy to propagate and the conjunction of such constraints can naturally be processed as individual constraints in a network  stronger filtering can be obtained by considering the whole conjunction.
algorithm 1: lower bound on n.
data: x1 ... xn n v = {v1 ... vk} {¦Ä1 ... ¦Äk}
result: a lower bound on n

¡¡we show that under the assumption that the distance measure is discrete and bounded in size by the number of variables  as is the case for hamming distance   enforcing gac on the similarmax constraint with respect to a bounded number of ideals is tractable. algorithm 1 finds a sharp lower bound with respect to a set of ideals. moreover its worst-case time complexity is o dnk+1   hence polynomial when the number k of ideals is bounded. it is therefore easy to derive a polynomial filtering procedure for this constraint by checking this lower bound against the upper bound of n for each of the nd possible assignments. the complexity of such a filtering procedure would thus be o d1nk+1 .
theorem 1 algorithm 1 finds a correct lower bound on the maximal hamming distance to a set of ideals  and runs in o dnk+1  time.
proof: this algorithm builds a partially ordered set of vectors of distances to ideals. a set of vectors is computed for each variable. given two consecutive variables xi and xi+1  two vectorsare related in the partial order   iff there exists an assignment xi+1 = j such that  ¦Ä1 j v1 i + 1   ... ¦Äk j vk i + 1   . all reachable vectors of distances are thus computed  so the bound is correct.
¡¡we first show that the cardinality of the whole poset is at most nk. indeed  the distance measures are discrete and bounded by the number n of variables  and each vector is of dimension k  hence there are at most nk distinct vectors. the cardinality of any layer is thus at most nk. for each layer we create at most d new vectors for each vector in the previous layer. this algorithm needs fewer than dnk steps for each layer  giving a worst-case time complexity o dnk+1 . 
example 1  conjunction  we show on our example how a logical expression formulating preferences in terms of ideals can be compiled into a constraint network. we assume that the configuration database involves n variables {x1 ...xn} subject to a set of constraints c. the objective function
f = max ¦Ä s volvo  ¦Ä s jaguar  m   ¦Ä s lada  
is compiled into the following constraint program:
minimise n  subject to:c x1 ... xn 	&
similarmax 	x1 ... xn n 
{volvo jaguar lada} 
hamming hamming nhamming  	{	 	}
1 approximation algorithm
we have seen in the previous section that enforcing gac on the similarmax constraint for a bounded number of ideals is polynomial. however  the algorithm we introduced may be impractical on large problems. we therefore propose an approximation algorithm  reducing the complexity from o d1nk+1  to o nd1k . this algorithm does less pruning than enforcing gac but more than decomposing into individual distance constraints.
¡¡as seen previously  a conjunction of distance constraints can be represented as k individual constraints. first  we can decompose it into k individual constraints  one for each ideal:

each individual constraint can be made gac in linear time. however  by considering the ideals separately  we will not do as much pruning as is possible.
¡¡to see this  consider the following example. let x1 to x1 be 1 variables. consider two ideals: v1 =
. we assume that ¦Ä is the hamming distance  and that the distance ¦Ä from a solution to any ideal must be at most 1  i.e.  d n  =  1 1 . even though no inconsistency can be inferred when looking at v1 and v1 separately  the constraint is globally inconsistent. any variable xi will either be assigned to 1 or 1 but not both. therefore  the total sum of pairwise differences between a solution and all the ideals will be at least 1. to minimise the maximum distance from either ideal  this total number of discrepancies should be evenly distributed across ideals  i.e.  the minimum maximum distance is at least . hence we cannot achieve a distance of 1. the approximation is based on the following inequality  where x¡¥ denotes the vector  x1 ... xn :
	.	 1 
now consider a second example where we add the ideal
. we can compute a lower bound for the minimum maximum distance in the same way. x1 can either be assigned 1 which entails 1 discrepancy  or 1 leading to 1 discrepancies. the same is true for x1 and x1 while the opposite holds for x1 and x1. therefore we know that the total number of discrepancies will be at least 1. hence we can derive a lower bound of   and we do not detect an inconsistency. however  we cannot distribute these discrepancies evenly. as the ideals considered in the first example all appear in the second example  the maximum distance cannot be smaller. this observation gives us a tighter lower bound. in fact  the distance from a solution to a set s1 cannot be less than the distance to a subset s1 of s1:
s1   s1   maxj¡Ês1¦Ä x v¡¥	j  ¡Ü maxj¡Ês1¦Ä x v¡¥	j .	 1 
therefore  we can consider any subset of ideals  and get a sound lower bound. by combining  1  and  1   we get the following lower bound:

¡¡we introduce an algorithm  algorithm 1  based on equation 1. notice that in this algorithm we do not require the distance measure to be the same on all ideals  however the  threshold  variable n used to bound the maximum distance to any ideal is unique. this algorithm goes through all subsets of v and computes the minimal sum of distances that is achievable with the current domains. then for any assignment x = v that would increase this distance above max n  we remove v from d x . the complexity is in o nd|s|  for each set s considered. if the number of ideals is too large then only part of the power may be considered. in this case  the computed lower bound remains valid.
¡¡for each subset s of ideals  we compute lb  line 1   a lower bound on the maximum distance to s. for each value on each coordinate we can compute the number of ideals in s whose distance to the solution would be increased if that value was chosen. this number  divided by |s|  line 1  is a lower bound on the contribution that this value makes to the whole distance. the minimum of each of these individual distances are aggregated to compute a lower bound on the distance to the complete set of ideals  line 1 . finally  the lower bound and the individual distances are used to prune values. any value which increases the lower bound above max n  can be pruned  line 1 .
theorem 1 algorithm 1 finds a valid lower bound on the maximal hamming distance to a set of ideals  filters the domains with respect to this bound and runs in o nd1k .
algorithm 1: prune a distance constraint  multiple ideals.
	dataresult: x: a closure of s1 ... xn n vimilar= {vmax1 ... v  x1k ... x} ¦Ä1 ... ¦Än  n vk	=
{v1 ... vk} {¦Ä1 ... ¦Äk} 
1 foreach s   v do
foreach xi do
proof: we show that equation 1 is valid. it is a composition of two straightforward inequalities. equation 1 relies on the fact that the maximal element of a set is larger than the average  whilst equation 1 states that the maximal element of a set is larger than the maximal element of any subset. the procedure used at each iteration of the outer loop  line 1  is similar to algorithm 1 with the same linear complexity  o nd  . the number of iterations is  in the worst case  equal to the cardi-
nality of the power set of {1 ... k}  hence 1k.	
theorem 1 algorithm 1 achieves a strictly stronger filtering than the decomposition into constraints on a single ideal: similar  x1 ... xn  n vj   j ¡Ê  1 ... k .
proof:  sketch  we first show that algorithm 1 is stronger than the decomposition. without loss of generality  consider an ideal vj. when the subset {vj} is explored in loop 1  the computation  hence the filtering  will be the same as in algorithm 1. then we give an example to show that this relation is strict. let x1 to xn be boolean variables  and consider two ideals:   and assume that ¦Ä is the hamming distance and n =  1 n/1   1 . the decomposition is gac  whilst algorithm 1 fails. 
¡¡unfortunately  this algorithm does not achieve gac  in general. this is to be expected given its lower complexity.
1 empirical evaluation
we ran experiments using the renault configuration benchmark1  a large real-world configuration problem. the problem consists of 1 variables  domain size varies from 1 to 1  and there are 1 table constraints  many of them non-binary. we randomly generated 1 sets of ideals  of cardinality 1  1 and 1  giving 1 instances in total. for each instance we used branch & bound to minimise the maximum hamming distance to the set  conjunction  of ideals.
¡¡we compared the decomposition of distance constraints on each ideal separately as well as our propagation algorithm approximating gac  algorithm 1  for the combined distance global constraint. we report the results obtained in figures 1 a   1 b  and 1 c  for 1  1 and 1 ideals  respectively.
we plot n minus distance  the distance complement  against cpu-time  measured in seconds  and number of backtracks averaged over the 1 instances  in each case. more formally  we plot the following function  where st is the set of solutions found at time  or number of backtracks  t  v is the set of ideals and n is the number of variables:
	.	 1 
¡¡our search strategy was to chose the next variable to branch on with the usual domain/degree heuristic. the first value assigned was chosen so as to minimise the number of discrepancies with the ideals for its coordinate. in other words  given k vectors {v1 ... vk}  we choose for a variable xi the value
 was minimal.

 a  configuration problem: 1 ideals

 b  configuration problem: 1 ideals

 c  configuration problem: 1 ideals

 d  a  zoom  on figure 1 c .
figure 1: results for the renault configuration problem.
¡¡on this benchmark the gain achieved in runtime using the global constraint instead of the decomposition is slight. indeed  the curves are almost equal regardless of the number of

 a  random instances: 1 ideals

 b  random instances: 1 ideals

 c  random instances: 1 ideals
figure 1: experimental results for the random instances.
ideals used. the optimal solution is often found very quickly  independent of the method used. in fact  the first solution is always found without backtracking  but since the problem is large there is a time penalty associated with finding it.
¡¡the logarithmic scale used to present cpu-time tends to hide the difference in time. in figure 1 d  we  zoom  in on part of figure 1 c   to show there is a reduction in cpu-time with the global constraint. we see that the global constraint achieves a distance complement of 1 in almost half the time taken by the decomposition.
¡¡we also ran experiments on uniform binary random csps so that we could control how hard it is to achieve optimality  and hence we avoid the earlier situation where optimal solutions were found too easily. all instances have 1 variables  domain size is 1  and there are 1 binary constraints with a tightness of 1. these instance are thus underconstrained to allow for a sufficiently large set of solutions. they were generated using a random uniform csp generator1. we generated 1 sets of 1 instances  with results presented in figures 1 a   1 b  and 1 c   for 1  1  and 1 ideals  respectively. the improvement  both in runtime and number of backtracks  is seen as soon as the number of ideals exceeds 1. the cpu time overhead due to larger sets of ideals was not as important as expected. the reduction in the search tree clearly compensates for the computational cost of a single call to the algorithm.
1 related work
constraints are not typically placed between the two solutions. one exception is the max-hamming problem where we seek a pair of solutions such that the hamming distance between them is maximised  angelsmark and thapper  1   without the freedom to specify one of the solutions. in distance-sat we seek solutions within some distance of each other  bailleux and marquis  1   which our work generalises in three important ways. first  we allow optimisation of the distances rather that stating a bound on distance. second  we permit multiple ideals  and not just one. third  we allow complex logical combinations of  non ideals. global constraints for reasoning about pair-wise distances between variables rather than between solutions  as proposed here  have also been proposed  artiouchine and baptiste  1; regin and rueher  1¡ä	 .
1 conclusion
we have proposed an approach to representing and reasoning about preferences in csps specified in terms of ideal and non-ideal solutions. we presented a novel global constraint for propagating distance constraints  which works with any distance metric that is component-wise decomposable like hamming distance. results from experiments with both realworld and random problem instances are encouraging.
acknowledgements. hebrard and o'sullivan are supported by science foundation ireland  grant 1/pi.1/c1 . walsh is supported by nicta  which is funded through the australian government's backing australia's ability initiative  in part through the australian research council.
references
 angelsmark and thapper  1  o. angelsmark and j. thapper. new algorithms for the maximum hamming distance problem. in proceedings of csclp  1.
 artiouchine and baptiste  1  k. artiouchine and p. baptiste. inter-distance constraint: an extension of the all-different constraint for scheduling equal length jobs. in cp  pp.1  1.
 bailleux and marquis  1  o. bailleux and p. marquis. distance-sat: complexity and algorithms. in aaai/iaai  pp.1  1.
 bistarelli et al.  1  s. bistarelli  u. montanari  and f. rossi. semiring-based constraint satisfaction and optimization. j. acm  1 :1  1.
 boutilier et al.  1  c. boutilier  r.i. brafman  c. domshlak  h.h. hoos  and d. poole. cp-nets: a tool for representing and reasoning with conditional ceteris paribus preference statements. jair  1-1  1.
 hebrard et al.  1  e. hebrard  b. hnich  b. o'sullivan  and t. walsh. finding diverse and similar solutions in constraint programming. in aaai  pp.1  1.
 regin and rueher  1¡ä   j.-c. regin and m. rueher.¡ä a global constraint combining a sum constraint and difference constraints. in cp  pp.1  1.
 rossi and sperduti  1  f. rossi and a. sperduti. acquiring both constraint and solution p