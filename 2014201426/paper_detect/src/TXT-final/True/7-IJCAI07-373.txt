
given the common use of restarts in today's clause learning sat solvers  the task of choosing a good restart policy appears to have attracted remarkably little interest. on the other hand  results have been reported on the use of different restart policies for combinatorial search algorithms. such results are not directly applicable to clause learning sat solvers  as the latter are now understood as performing a form of resolution  something fundamentally different from search  in the sense of backtracking search for satisfying assignments . in this paper we provide strong evidence that a clause learning sat solver could benefit substantially from a carefully designed restart policy  which may not yet be available . we begin by pointing out that the restart policy works together with other aspects of a sat solver in determining the sequence of resolution steps performed by the solver  and hence its efficiency. in this spirit we implement a prototype clause learning sat solver that facilitates restarts at arbitrary points  and conduct experiments on an extensive set of industrial benchmarks using various restart policies  including those used by well-known sat solvers as well as a universal policy proposed in 1 by luby et al. the results indicate a substantial impact of the restart policy on the efficiency of the solver  and provide motivation for the design of better restart policies  particularly dynamic ones.
1 introduction
propositional satisfiability  sat  is the problem of determining whether a propositional formula  traditionally in conjunctive normal form  cnf   has a satisfying assignment-an assignment of truth values to its variables making it evaluate to true. in this paper we focus on a class of algorithms for sat that has become known as conflict-driven clause learning  or clause learning for short. these algorithms are currently the best for large sat instances that arise from industrial applications  such as formal verification  berre and simon  1 .
　clause learning sat solvers have grown out of their predecessors that implemented variantsof a systematic search algorithm known as dpll  davis et al.  1   which solves sat by selecting a variable and determining  recursively  whether the formula can be satisfied by setting that variable to either value. in fact  the initial intuition for learning clauses and appending them to the cnf formula was to help prune the dpll search  as discussed in earlier work  marques-silva and sakallah  1 .
　it has been shown  however  that clause learning as practiced in today's sat solvers  assuming unlimited restarts  corresponds to a proof system exponentially more powerful than that of dpll  beame et al.  1 . specifically  each learning step is in fact a sequence of resolution steps  of which the learned clause is the final resolvent; conversely  a resolution proof can be simulated in polynomial time by repeatedly  i  learning each of the resolvents in the proof and  ii  restarting  this assumes a deviation from standard practice: the freedomto ignoreassignmentsmade by unit propagation . clause learning can hence be as powerful as general resolution  while dpll has been known to correspond to the exponentially weaker tree-like resolution  beame et al.  1 .
　despite the dependence of this theoretical result on the assumption of unlimited restarts  remarkably little has been said in the literature on the importance of choosing a good restart policy in practice. this is in stark contrast  for example  to the sustained quest by researchers for better decision heuristics. we argue that this imbalance of attention is doing a disservice to the sat community  because with the modern understanding of clause learning  it can be seen that the decision heuristic  together with the restart policy and other components of the solver  determines the sequence of resolution steps performed by the solver and hence its efficiency.
　in this paper we would like to take a step toward studying the importance of restart policies in clause learning sat solvers  with a view to motivating further work on designing more effective policies. to this end we have created a small prototype sat solver  called tinisat  which implements the essentials of a modern clause learning solver and is designed to facilitate adoption of arbitrary restart policies. after choosing and fixing a reasonably effective decision heuristic  we conducted experiments using an extensive set of large industrial benchmarks  on which we ran versions of tinisat using different restart policies including those used by well-known sat solvers and particularly one proposed in  luby et al.  1  based on a sequence of run lengths of the following form: 1 1 1 1 1 1 1 1 ... the results we have obtained indicate a substantial impact of the restart policy on the efficiency of the solver. specifically  all nontrivial restart policies we experimented with did significantly better than if restarts were disabled  and exhibited considerably different performance among themselves. more interestingly  this difference in performance appears more marked when one looks at individual benchmarks or benchmark families  as opposed to the whole set in aggregate  which suggests that substantial performance gains may be possible by using appropriate dynamic restart policies  all policies compared in this paper are static ones .
　the rest of the paper is organized as follows: we present the simple design of tinisat and use it as the basis for discussing the semantics of modern clause learning  leading to an analytical explanation why restart policies are important. we then describe our experimental setup including the various restart policies we shall use and our attempts to identify a reasonable decision heuristic so that all policies can be tested on competitive ground. we then report the results obtained and make a number of important observations. finally  we discuss related work and present our conclusions.
1 essentials of clause learning
we start by presenting the design of a simple sat solver 
tinisat  that  i  boasts the essentials of modern clause learning technology  and  ii  provides a basis for our discussion of the importance of restart policies later in the section. the top-level procedure of tinisat  implemented in under 1 lines of c++  is given in algorithm 1  which operates on an implicit cnf formula whose satisfiability is in question.

algorithm 1 tinisat
1: loop
1:	if  literal = selectliteral    == nil then1:return satisfiable1:if !decide literal  then1:repeat1:learnclause  1:if assertionlevel   == 1 then1:return unsatisfiable1:if restartpoint   then1:backtrack 1 1:else1:backtrack assertionlevel   1:until assertlearnedclause  　the following components of a modern clause learning sat solver can be identified in algorithm 1: decision heuristic  selectliteral   unit propagation  decide  assertlearnedclause   clause learning  learnclause  backtrack   restarts  restartpoint  backtrack . we assume familiarity with the common terminology for dpll and clause learning algorithms  and assume that  i  1-uip  zhang et al.  1  is used as the learning scheme   ii  no clauses are ever deleted  hence the algorithm is complete1    iii  all functions have deterministic behavior 1 and  iv  the first decision is made in decision level 1: level 1 is reserved for literals found to be implied by the cnf formula  and level 1 to signal derivation of the empty clause. the functions involved have the following semantics:
  selectliteral uses some decision heuristic to select a free variable and then select one of its two literals  and returns it  or returns nil if no free variables exist.
  decide increments the decision level  sets the given literal to true  and performsunit propagation;it returns true iff no empty clause is derived.
  learnclause performs 1-uip learning to derive an implicate of the cnf formula  and sets the assertion level  i  to 1 if the empty clause is derived   ii  to 1 if a unit clause is derived  and otherwise  iii  to the second highest decision level among literals of the derived clause.
  assertionlevel returns the assertion level  which has been set by the last call to learnclause.
  restartpoint returns true iff the solver is to restart now according to some restart policy.
  backtrack k  undoes all variable assignments in decision levels   k  and sets the decision level to k.
  assertlearnedclause adds the learned clause to the clause pool  performs unit propagation if the current decision level equals the assertion level  this is the condition under which the learned clause becomes unit   and returns true iff no empty clause is derived.
1 restarts and backtracks unified
note that under this design all that is needed to adopt a given restart policy is to implement restartpoint accordingly. it is also interesting to note that a normal backtrack after learning  line 1  and a complete restart  line 1  can both be regarded as special cases of a more general scheme  lynce and silva  1  where the solver can backtrack to any level between 1 and the current decision level  exclusive . what we would like to stress here  however  is that the particular scheme used by most clause learning sat solvers today  namely backtracking to the assertion level  except when restarting   has obscured their original characteristics  inherited from dpll  as systematic search algorithms. in particular  these solvers do not perform branching anymore: the setting of a literal occurs on line 1  but the setting of its negation is never explicitly tried  and possibly never tried at all even implicitly .

　for example  suppose assignments {a b c d} have been made in decision levels 1 1  respectively before the empty clause is derived in level 1  and suppose the following clause is learned: a ‥ d. this clause says that the two decisions made in levels 1 and 1 alone are responsible for the conflict. now  despite the fact that simply flipping the value of variable d in level 1 could well result in a satisfiable subproblem  line 1 insists on taking the solver back to the assertion level  which is 1  erasing assignments {d c b} on the way. the learned clause then gets asserted in level 1  line 1   and implies d  because the assignment a is present  making the clause unit   triggering a round of unit propagation. notice that the branches {a b c} and {a b}  which can well contain solutions  have been skipped over without ever being explored.
　it should be emphasized here  as we already alluded to  that this behavior is not a peculiarity of tinisat  but is the common practice of most current clause learning sat solvers we have seen  including chaff  berkmin  minisat  and siege.  the earlier solver grasp  though  used a different backtracking scheme such that no branch was skipped over unless proven to contain no solutions. 
1 importance of restarts
this shift of paradigm in backtracking  as we discussed  obscures the characteristics of clause learning sat solvers as systematic search algorithms. for this reason we propose to view the modern practice of clause learning not as a version of dpll search enhanced with resolution 1 but as a pure resolution algorithm in its own right. in fact  what algorithm 1 does is nothing other than the following 1-step cycle:
 1  set variables till hitting a conflict;
 1  derive a clause by resolution;
 1  unset some variables and go back to  1 .
for unsatisfiable formulas  this loop terminates on deriving the empty clause in  1 ; for satisfiable formulas  it terminates when  1   happens  to exhaust all variables without conflict.
　in this context the importance of the restart policy becomes prominent: together with the existing backtracking scheme  it dictates the set of assignments to undo in  1   which  together with the decision heuristic  ultimately determines the entire sequence of resolution steps performed in  1 . in other words  the decision heuristic  backtracking scheme  and restart policy can all be understood as serving a single purpose  that of guiding the resolution process.
　consider for example a clause learning sat solver that has run on a hard instance for a period of time without restarts. the solver has now accumulated a considerable number of learned clauses  which have helped update the variable and literal scores as are maintained by decision heuristics typical in clause learning sat solvers. these new scores represent  in a way  the solver's current state of belief about the order in which future decisions should be made  having taken into account all the conflicts discovered so far. without the freedom of restarts  however  the solver would not be able to fully execute its belief because it is bound by the decisions that have been made earlier. in particular  note that these early decisions were made without the benefit of the new knowledge in the form of all the conflicts discovered since. this  we believe  is the main reason why restarts can help improve the efficiency of clause learning sat solvers even when no randomness is present  which will be the case in our experiments with tinisat in section 1 .
　in this section we have provided a new understanding of modernclause learning through the design of tinisat  a concrete and simple clause learning sat solver  leading to an analytical argumentthat the restart policy matters. we now proceed to support this argument with an empirical study of concrete restart policies using real-world sat benchmarks and the tinisat solver.
1 experimental setup
we describe in this section the restart policies we shall experiment with  the decision heuristic to be used with these policies in the experiments  and our choice of benchmarks.
1 restart policies
in choosing the set of restart policies for our empirical study  we have aimed to include those that are currently used by well-known sat solvers as well as some less conventional ones that may have escaped the attention of the clause learning community. specifically  we shall experiment with the following seven restart policies:
  n: a policy calling for no restarts at all.
  m: a geometric policy used in minisat v1  ee＞n and so：rensson  1  with an initial restart interval of 1 conflicts  which increases by a factor of 1 after each restart. we will denote it by  1  1 .
  z: a fixed-interval policy used in chaff ii  also known as the 1 version of zchaff  moskewicz et al.  1   with a restart interval of 1 conflicts  denoted  1  1 .
  b: a fixed-interval policy used in berkmin  goldberg and novikov  1  with a restart interval of 1 conflicts  denoted  1  1 .
  g: a geometric policy  1  1   which we have added to improve the balance between fixed-interval and geometric policies we consider.
  s: a fixed-interval policy used in siege  ryan  1  with a restart interval of 1 conflicts  denoted  1  1 .
  l: a class of policies proposed in  luby et al.  1  for randomized algorithms based on the following sequence of run lengths: 1 1 1 1 1 1 1 1 ...  defined below . in our experiments we take a  unit run  in this sequence to be 1 conflicts  we have experimented with other units as well; see http://rsise.anu.edu.au/ jinbo/tinisat/ . hence the actual restart intervals are: 1 1 1 1 ... we denote this policy by  luby's  unit=1 .
　the first six of these policies are straightforward  while luby's policy can be formally defined as the sequence t1 t1 t1 ... such that:
	 	if i =1k  1;
	ti 1k 1 	if 1k 1 ＋ i   1k  1.
　we have chosen luby's policy because of an interesting property it has: in the context of a particular class of randomized algorithms  known as las vegas algorithms   luby et al.  1  proved that this policy is universally optimal in the sense that  i  it achieves an expected running time that is only a logarithmic factor slower than the true optimal policy  which is determined by the specific running time distribution of the algorithm on a specific problem instance  and  ii  no other universal policy can do better by more than a constant factor.  the theoretical relevance of this property to clause learning remains an interesting question though. 
1 decision heuristic
to make our comparison of restart policies more meaningful  we have taken steps to ensure that other components of the sat solver are tuned toward their best performance. rather than low-level optimizations  however  we focused on choosing a reasonably effective decision heuristic. based on experiments using a subset of our full benchmark suite  described below   we found that a version of the vsids heuristic  moskewicz et al.  1  combined with berkmin's practice of choosing literals from recent unsatisfied conflict clauses  goldberg and novikov  1  tended to work well.
　specifically  for each literal we keep a score that is initially the numberof its occurrencesin the original clauses. on learning a clause  we increment the score of every literal by 1 for each of its occurrences in clauses that are involved in the resolution process. the scores of all literals are halved once every 1 conflicts. when a decision is called for  line 1 of algorithm 1   we pick a  free  literal with the highest score from the most recently learned clause that has not been satisfied  and set it to true; if no such clause exists we pick any  free  literal with the highest score.
1 benchmarks
we use the entire set of industrial benchmarks distributed by miroslav velev of carnegie mellon university at http://www.ece.cmu.edu/ mvelev/  except sss.1  sss.1a  sss-sat-1  vliw-sat-1  and vliw-sat-1 as they are too easy 1 and dlx-iq-unsat-1 as the download appeared to be incomplete. this gives us 1 benchmark families with 1 instances totaling about 1gb in size-hence the cnf formulas have an average size of about 1mb.
1 results
our experiments consist of running tinisat with each of the seven restart policies on the entire set of benchmarks. for additional reference points  we have also run minisat v1  ee＞n and s：orensson  1  and siege v1  ryan  1   given seed 1 for its random number generator  on the same set of benchmarks. all our experiments were conducted on a cluster of 1 amd athlon 1 processors running at 1ghz with 1gb of ram under suse linux 1 professional. a time limit of 1 hours was imposed on all runs of the solvers  allowing us to complete all the experiments in about 1 cpu days.
　the overall results are shown in table 1. in the second and third columns we report for each benchmark family the number of instances and their total size  in megabytes . in the remaining columns we report the number of instances solved by each solver for each benchmark family. the total number of instances solved by each solver and the total time it spent on all instances  including the 1 hours in case it did not solve the instance  are reported in the two bottom rows.
　the first observation we make from these results is that restarts definitely helped: all the six nontrivial policies did significantly better than the no-restarts policy. even the least effective of them allowed tinisat to finish 1 days sooner and solve 1 more instances than the no-restarts policy.
　our second observation is that luby's universal policy appears to outperform all the rest on this particular set of benchmarks  albeit by only a small margin in some cases . given that the optimality of luby's policy was originally proved for las vegas algorithms  this empirical result provides motivation for extending the theoretical study of luby's policy to clause learning algorithms.
　to give a more concrete picture of the impact of the restart policy on the efficiency of the solver  we present detailed results in tables 1 and 1 for two of the benchmark families where tinisat solved all instances using every policy. for space constraints we only include three policies in each table: the no-restarts policy and the worst and best of the rest. results on siege are also included as a reference point.
　an interesting observation to make from tables 1 and 1 is that the difference in performance between the restart policies becomes more substantial now that we look at individual benchmark families  as opposed to the whole set in aggregate  bottom two rows of table 1 . for example  policy l outperforms policy m in running time by only a factor of 1 in table 1  but a factor of 1 in table 1. in fact we can also see in table 1 that none of the policies is consistently best across all benchmark families  in terms of the number of solved instances . while this explains the decreased difference between policies when results are aggregated  it also suggests that substantial performancegains would be possible if one were to use an appropriate dynamic policy that could adapt to a given benchmark or benchmark family.
　we would like to remind the reader here that all these experiments with tinisat were conducted with a fixed decision heuristic  learning method  and backtracking scheme. as we have discussed  all these components work in combination with the restart policy to determine the efficiency of the solver. hence we view the design of better restart policies as an opportunity among possibly many others of bringing about a new generation of clause learning sat solvers. finally  we note that detailed results of the experiments described in this section  as well as tinisat  can be downloaded from http://rsise.anu.edu.au/ jinbo/tinisat/.
1 related work
while in this work we have focused on restart policies for clause learning  in previous work researchers have studied

table 1: overall results on running tinisat with seven restart policies: n = no restarts; m =  1  1 ; z =  1  1 ; b =  1  1 ; g =  1  1 ; s =  1  1 ; l =  luby's  unit=1 . cutoff was 1 hours.
benchmarknumber ofsizenumber of instances solvedfamilyinstances mb nmzbgslminisatsiegedlx-iq-unsat-1111111engine-unsat-1111111fvp-sat.1111111fvp-unsat.1111111fvp-unsat.1111111fvp-unsat.1111111liveness-sat-1111111liveness-unsat-1111111liveness-unsat-1111111npe-1111111pipe-ooo-unsat-1111111pipe-ooo-unsat-1111111pipe-sat-1111111pipe-sat-1111111pipe-unsat-1111111pipe-unsat-1111111vliw-sat-1111111vliw-sat-1111111vliw-sat-1111111vliw-unsat-1111111vliw-unsat-1111111vliw-unsat-1111111total111111total time on all instances  days 111111111table 1: detailed results for fvp-unsat-1 all unsatisfiable except 1pipe bug. abbreviations: dec.  decisions   con.  conflicts   res.  restarts . times are in seconds. three policies included: policy n  no restarts  and the worst and best of the rest.
benchmarknumber oftinisat-ntinisat-mtinisat-lsiegevarsclausesdec.con.res.time  s dec.con.res.time  s dec.con.res.time  s dec.con.time  s 1pipe111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe 1 ooo111.11111.11.1pipe111.11111.11.1pipe 1 ooo111.11111.11.1pipe111.11111.11.1pipe bug111.11111.11.1total	1	1.1	1	1	1	1.1	1.1
table 1: detailed results for vliw-sat-1  all satisfiable. abbreviations: dec.  decisions   con.  conflicts   res.  restarts . times are in seconds. three policies included: policy n  no restarts  and the worst and best of the rest. cutoff was 1 hours.
benchmarknumber oftinisat-ntinisat-stinisat-lsiegevarsclausesdec.con.res.time  s dec.con.res.time  s dec.con.res.time  s dec.con.time  s bug111111.11111bug111111.11111bug111111.11111bug111111.11111bug111111.111---bug111111.11111bug111111.11111bug111111.111---bug111111.11111bug111111.111---total	1	1.1 1	1	1 1.1.1

the use of restarts in combinatorial search algorithms. fixedinterval as well as dynamic restart policies  for example  were studied in  gomes et al.  1; kautz et al.  1  using a randomized version of satz  li and anbulagan  1   a sat solver based on pure dpll search  without clause learning . another example is found in  walsh  1   where a geometric policy was found to be particularly effective for search problems that exhibit a  small world  topology.
　these previous studies of restart policies were based on the observation that for many combinatorial search problems  repeated runs of a randomized search algorithm on the same problem instance  or  similarly  runs of a deterministic search algorithm on random instances of a problem  exhibit a  heavy-tailed  distribution  gomes et al.  1   meaning that any run has a nonnegligible probability of taking exponentially longer than all previous runs that have been seen. intuitively a restart policycan help combatthis unpredictability of search performance by cutting off runs before they take too long in the hope that one of the future runs will succeed quickly.
　interestingly  part of this opportunistic thinking appears to remain valid in the context of clause learning  which as we discussed performs resolution instead of search. after all  one cannot always predict the performance of a clause learning solver on a particular problem instance  either  given its known performance on similar instances  for a quick reminder of this fact  see the performance of siege in table 1 . we believe  therefore  that the applicability of these previous results to clause learning sat solvers will be an interesting topic for future work.
1 conclusions
through the design of a simple clause learning sat solver  we have established a new formulation of modern clause learning where all aspects of the solver can be understood as collectively serving the single purpose of guiding a resolution process. this leads to an analytical argument for the importance of the restart policy in clause learning  which we studied empirically by comparing the performance of various policies on a large number of challenging industrial benchmarks. our results indicate a substantial impact of the restart policy on the efficiency of the clause learning solver. we view this work as a step toward a better understanding of the role played by restarts in clause learning  and hope that it will motivate further work on the design of more effective restart policies  particularly dynamic ones which we have not considered in this work.
acknowledgments
thanks to andrew slater for help with running the experiments and to the anonymous reviewers for commenting on an earlier version of this paper. national ict australia is funded by the australian government's backing australia's ability initiative  in part through the australian research council.
