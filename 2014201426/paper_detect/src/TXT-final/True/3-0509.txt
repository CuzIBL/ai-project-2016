
we use  nearly sound  logical constraints to infer hidden states of relational processes. we introduce a simple-transition cost model  which is parameterized by weighted constraints and a statetransition cost. inference for this model  i.e. finding a minimum-cost state sequence  reduces to a single-state minimization  ssm  problem. for relational horn constraints  we give a practical approach to ssm based on logical reasoning and bounded search. we present a learning method that discovers relational constraints using claudien  de raedt and dehaspe  1  and then tunes their weights using perceptron updates. experiments in relational video interpretation show that our learned models improve on a variety of competitors.
1 introduction
we consider hidden-state inference from the observations of relational processes  i.e. processes where states and observations are described by properties of and relations among objects  e.g. people and blocks . to deal with the enormous state and observation spaces  we utilize  nearly-sound   i.e. rarely violated  logical constraints on the states and observations. such constraints can often be acquired via machine learning and/or a human expert  and we give a framework for combining them for relational sequential inference.
﹛we introduce the simple-transition cost model  in section 1   which is parameterized by a set of weighted logical constraints and a state-transition cost. the cost of a state sequence given an observation sequence is the weight of unsatisfied constraints plus a transition cost for each state change. sequential inference corresponds to computing the minimum-cost state sequence. here we study both learning and inference for this model.
﹛first  in section 1  we  efficiently  reduce sequential inference to a single-state minimization  ssm  problem. we then show  in sections 1 and 1  how to leverage nearly-sound relational horn constraints in order to compute ssm with practical efficiency. we also study the possibility of exploiting efficient ssm in richer models. we show that for a simple enrichment to our model  there is no efficient reduction unless p=np. in section 1  we discuss learning a relational simpletransition model using the logical discovery engine claudien  de raedt and dehaspe  1  and a variant of collins' generalized perceptron algorithm  collins  1 . finally  in section 1  we evaluate our approach in a relational videointerpretation domain. our learned models improve on the accuracy of an existing trainable system and an approach based on more mainstream approximate inference.
1 problem setup
for simplicity  we describe our problem setup and approach for propositional  rather than relational  processes  where in the spirit of dynamic bayesian networks  dbns   murphy  1   states are described by a fixed set of variables. in section 1  we extend to the relational setting.
﹛a sequential process is a triple  x y p   where the observationspace x and statespace y are sets that contain all possible observations and states. p is a probability distribution over the space of finite sequences constructed from members of x ℅ y. each such sequence contains a pair of an observationsequence o-sequence  and a statesequence  s-sequence . for a sequence q =  q1 ... qt   we let qi:j =  qi ... qj  for i ≒ j. we will use uppercase for sequences and lowercase for single states and observations. a process is propositionalwhen its states are represented using a set of n statevariables over the finite domain ds  yielding y =  ds n. the value of the i'th variable in state s is denoted by s i . we will not need to assume a representation for observations until section 1
﹛we assume that the utility of an inferred s-sequence s primarily derives from the sequence of distinct states  rather than identifying the exact state transition points. let compress s  denote the sequence obtained by removing consecutive repetitions in s. for example  compress a a a b b a c c  = a b a c. our empirical goal is to map an o-sequences o to an s-sequence s such that compress s  is the distinct sequence of states that generated o. indeed  in our video domain  the exact locations of state transitions are often ambiguous  as judged by a human  and unimportant for recognizing activity-e.g. in figure 1 it is unimportant to know exactly where the transition occurs. example 1. the process in our video-interpretation domain corresponds to a hand playing with blocks. figure 1 shows key frames where a hand picks up a red block from a green block. the goal is to observe the video and then infer the underlying force-dynamic states  describing the support relations among objects. states are represented as sets of forcedynamic facts  such as attached hand  red . observations are represented as sets of low-level numeric facts  such

	frame 1	frame 1	frame 1	frame 1
figure 1: key frames in a video segment showing a hand picking up a red block from a green block. the object tracker's output is shown by the polygons. the video segment has two distinct force-dynamic states given by: {grounded hand   grounded green   contacts green  red }  frames 1 and 1  and {grounded hand   grounded green   attached hand  red }  frames 1 and 1 . the transition occurs between frames 1 and 1. see example 1 regarding the predicates grounded  contacts  and attached.as distance green  red  = 1  that are derived from an object tracker's noisy output. for a fixed set of objects  the process can be represented propositionally with a variable for each possible fact. however  our system must handle any number of objects  requiring a relational process representation described in section 1.
1 the simple-transition cost model
our framework utilizes an additive conditional cost model c s|o  to represent the cost of labeling o-sequence o1:t by s-sequence s1:t
	c s1:t|o1:t  = x ca si|oi  + x ct si si 1 	 1 
﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛1≒i≒t	1 i≒t with real-valued atemporal-costand transition-costfunctions ca and ct.	this is the cost cost model implicit in  conditional  hidden markov models  hmms . ca si|oi  is the cost of labeling oi by state si  and ct si si 1  is the cost of transitioning from state si 1 to si.	as in  lafferty et al.  1   our work extends to ca's with  non-local  observation dependencies. sequential inference involves computing argmins1:t c s1:t|o1:t  for a given o1:t.
﹛c s1:t|o1:t  is a simple-transitionmodel stm with parameters hca ki if the atemporal-cost function is ca and ct si si 1 =k﹞汛 si 1= si 1   where k is a real  and 汛 p  = 1 if p is true else 1. this model charges an equal cost for all state transitions  even though generally some transitions types are more likely than others. if this likelihood information is critical for inference  then an stm alone will not suffice. however  we believe that there are interesting processes where accurate inference does not require exploiting non-simple transition structure. for example  in our video domain  states persist for many observations and can be reliably inferred by integrating just those observations  without considering transition type. for such processes  it is important to study simple but sufficient models  as there can be considerable computational advantages.
1 inference for simple-transition models
viewing an stm as an hmm we can apply the viterbi algorithm  forney  1  to compute a minimum-cost s-sequence s1:t in o t ﹞ |y|1  time. in fact  by leveraging the simpletransition structure  viterbi can be improved to o t ﹞ 1 ﹞ |y|  time. however  these algorithms are impractical for us since they are exponential in the number of state variables n  i.e. |y| = |ds|n   which will typically be large for our relational processes. likewise  for stms  general-purpose graphical-modeling techniques such as variable elimination and junction-tree algorithms are exponential in n  i.e. the induced tree width  dechter  1  is linear in n .
﹛reduction to ssm. sequential inference for an stm hk cai reduces to the single-stateminimization ssm problem  which is defined as computing the ssm function 考 o1:j  = mins p1≒i≒j ca s|oi . the ssm function gives the minimum cost of labeling o1:j by a single state  i.e. no state transitions . for an o-sequence o1:t we denote the minimum cost over all s-sequences as c  o1:t  = mins1:t c s1:t|o1:t . for stms  c  o1:t  can be expressed in terms of 考 as follows  for proof see fern  . c  o1:t  = min  c  o1:j  + k ﹞ 汛 j   1  + 考 oj+1:t    1  1≒j t
 equation 1 minimizes over j  where j + 1 is viewed as the final state-transition point. the minimum cost over ssequences with final transition j + 1 equals the minimumcost sequence up to j  plus a transition cost k  unless j=1   plus the ssm cost for the remaining suffix  no transitions occur after j . this decomposition is possible because stms weigh all transition types equally  decoupling the minimization problems before and after j.
﹛equation 1 yields an efficient reduction to ssm based on dynamic programming  which we call the ssm-dp algorithm. simply compute c  o1:j  in the order j = 1 ... t for a total of t1 ssm computations. it is straightforward to store information for extracting the minimum-cost ssequence  which we denote by ssm-dp o1:t 考 k . thus  efficient ssm computation implies efficient sequential inference for stms. the complexity of ssm depends on the particular representation used for ca. in section 1  we leverage nearly-sound logical constraints to provide practically efficient ssm.
﹛though ssm-dp provides the potential for handling large state spaces  a naive implementation requires t1 ssm computations  which is often unacceptable. however  significant and sound pruning of the computation is possible- e.g. in equation 1  ignore any j when a k exists such that
c s1:k|o1:k  ≒ c s1:j|oi:j  + 考 oj+1:k . in our application  pruning reduces the empirical number of ssm computations to o t . see fern  for details.
﹛sufficient ssm computation. in practice we do not need exact ssm for all o-sequences. rather we need only to compute a sufficient ssm approximation defined below. o1:j is a criticalo-sequence of process p if for some o-sequence drawn from p  o1:j is a maximal subsequence generated by a single state. let 考 be the ssm function for an stm that accurately predicts s-sequences of p. we say 考1 is a sufficientssmapproximation to 考 for p if both 1  for all o-sequences o1:t  考 o1:t  ≒ 考1 o1:t   and 1  考1 o1  = 考 o1  for any critical o-sequence o1 of p. it can be shown that for o-sequences drawn from p  ssm-dp o1:t 考 k  = ssm-dp o1:t 考1 k . thus we can achieve accurate inference using 考1. in section 1 we show how to efficiently compute such an approximation.
﹛non-simple models. given our focus on ssm it is natural to consider efficient ssm reductions for non-simple models. intuitively  if a model distinguishes between different transition types  we may need to consider states other than just ssm solutions  like viterbi but unlike ssm-dp   possibly resulting in exponential behavior in n even given efficient ssm. below we show that an efficient reduction is unlikely for a modest extension to stms  giving a boundary between efficient and inefficient models under efficient ssm.
﹛we extend stms by allowing the model to assign higher costs to transitions where more state variables change  unlike stms which can only detect whether a change occurred. a cost model c  with atemporal component ca  is a countingtransitionmodelif  i.e. transition cost is linear in the count of propositions that change. we say c allowsefficientssm if the ssm function for ca is computable in time polynomial in its input osequence size and number of state variables.
theorem 1. given as input a counting-transition model c  an observation sequence o  and a cost bound 而  the problem of deciding whether c  o1:t    而 is np-complete  even under the assumption that c allows efficient ssm. proof:  sketch  see fern  for full proof. inference in 1-d grid potts models  a class of markov random fields  is nphard  veksler  1 . we first give a non-trivial extension of this result to the smaller class of 1-d grid potts models with equally weighted  horizontal edges . next  we reduce this problem to counting-transition model inference. intuitively  the state variables at time i represent the potts model nodes in column i. finally  we show that the constructed model allows efficient ssm via variable elimination. 1
1 constraint-based ssm
we now give a constraint-based representation for atemporalcost functions and study the corresponding ssm problem. for simplicity  we assume that states have n binary state variables  i.e. ds = {true false}. we also assume a set of m binary observationtests  each one mapping observations to {true false}. non-binary extensions are straightforward.
﹛propositional horn constraints. a propositionalhorn constraint 耳 is a logical implication  body ↙ head   where body is a conjunction of state variables and observation tests  and head is a state variable or false. given an observation o and state s  we let 耳 o   耳 s   denote the result of substituting observation tests  state variables  in 耳 with the truth values under o  under s . if a constraint has no variables  then it is interpreted as the truth value of the variable-free expression. thus  耳 o  s  is the truth value of 耳 under o and s  and we say that o and s satisfy 耳 iff 耳 o  s  is true. a set of horn constraints is satisfiable iff there exists a state and observation that jointly satisfy each constraint. for horn constraints  testing satisfiability and finding satisfying assignments are polynomial-time computable  papadimitriou  1 .
﹛constraint-based cost functions. we parameterize an atemporal-cost function using a set of weighted horn constraints 朴 = {h耳1 c1i ... h耳v cvi}  with horn constraints 耳i and non-negative weights ci representing the cost of violating 耳i. the non-negativity requirement will be important for inference. the sum of costs in 朴 is denoted by cost 朴  and we say 朴 is satisfiable if its set of constraints is satisfiable. atemporal cost is defined as ca s|o 朴  = ph耳 ci﹋朴 c ﹞ 汛  耳 o  s    i.e. the total cost of unsatisfied constraints. in this work  we will assume that 朴 contains  nearly sound  constraints  meaning that each constraint is usually satisfied by the state/observation pairs generated by our process. our primarily non-theoretical goals do not require a formal notion of nearly sound  e.g. pac .
﹛given 朴 and an o-sequence o1:j  we define the combined constraint set as 忙 o1:j 朴  = s1≒i≒j sh耳 ci﹋朴h耳 oi  ci  which involves only state variables and captures all of the state constraints  implied  by 朴 and o1:j. the ssm function for ca is now given by
	考 o1:j|朴 	=	min x ca s|oi 朴 	 1 
s
1≒i≒j
	=	min x c ﹞ 汛  耳 s  	 1 
s
h耳 ci﹋忙 o1:j 朴 
which is equivalent to solving maximum satisfiability
 max-sat  jiang et al.  1  for 忙 o1:j 朴   where maxsat asks for an s such that the weight of satisfied constraints is maximum. while the number of ssm variables is fixed  the constraint set grows with sequence length. fortunately  in practice we can significantly reduce this set by pruning and merging. prune 朴  contains members of 朴 that do not have false in the constraint's body. merge 朴  combines logically equivalent members of 朴 into one constraint by summing weights. max-sat solutions are invariant under both operators. thus  we solve ssm via max-sat for the smaller constraint 忙  o1:j 朴  = merge prune 忙 o1:j 朴   .
﹛a dual max-sat approach. max-sat is np-hard even for horn constraints  jaumard and simeone  1 . rather than use general-purpose approximate techniques  we give a max-sat approach that leverages our setting of nearly-sound horn constraints. let 羽 = 忙  o1:j 朴  and 旭 be the set containing all satisfiable subsets of 羽. an equivalent dual form of equation 1 is 考 o1:j|朴  = cost 羽    max羽1﹋旭 cost 羽1   which asks for a maximum-cost member of 旭. this motivates the following max-sat approach that searches through constraint subsets rather than than variable assignments. conduct a cost-sensitive breadthfirst search through subsets of 羽 for a satisfiable subset-i.e. starting at 羽 consider 羽 subsets in order of non-increasing cost until finding a satisfiable one. any satisfying assignment for this set is a max-sat solution provided all weights are non-negative. for negative weights a satisfying assignment for a consistent constraint set may not be a max-sat solution. although we can always replace a negatively weighted constraint h耳 ci by h 耳  ci   耳 may not be horn  possibly making sat hard. hence we require non-negative weights.
﹛since testing satisfiability is efficient for horn constraints  the time required by the dual approach primarily depends on the number of search nodes we consider. this number is
table 1: force-dynamic state predicates rs  top  and observation predicates ro  bottom  for our application.
attached x y  x supports y by attachment
grounded x 	support of x is unknown contacts x y  x supports y by contact

direction x d x is moving in direction dspeed x s x's speed is selevation x e x's elevation is emorph x c x's shape-change factor is cdistance x y d distance between x and y is d dist x y dd change in distance is ddcompass x y c compass direction of y to x is cangle x y a angle between x and y is abounded by the number of subsets of 羽 that have a cost greater than 考 o1:j|朴   which can be exponentially large. thus  we compute an approximation 考而 to 考 by first searching through subsets of 羽 for a maximum of 而 steps. we return a solution if one is found and otherwise return an upper-bound to 考 o1:j|朴  resulting from a greedy hill-climbing search.
﹛though 考而 will not be correct for all inputs  we know from section 1  that it need only be a sufficient approximation to guarantee correct sequential inference. when our constraints are nearly sound  考而 will tend to be sufficient even for small 而. that is  考而 will equal 考 for critical o-sequences. recall that when o1:j is a critical o-sequence it must be generated by a single state s. thus  s will satisfy most constraints in 忙  o1:j 朴  and our search need only remove a small number of constraints  the unsatisfied ones  to find a satisfiable subset. in addition  as described in the full paper  the merge operation tends to place high weight on the satisfied constraints  which often leads in removing unsatisfied constraints first.
1 extending to relational processes
in the spirit of knowledge-base model construction  wellman et al.  1   we extend to relational processes by compiling  relational schemas  to propositional models and use the ideas from previous sections.
﹛relational processes. a process  x y p  is relational when the observation and state spaces x and y are given by specifying a domain set of objects d  a state-predicate set rs  and an observation-feature set fo  each having a specified number of arguments. an observationfact has the form  f = v   where f is an observation feature applied to objects and v is a number. a statefact is a predicate from rs applied to objects. see example 1 for example facts from our video domain. observations  states  are finite sets of observation  state  facts  representing all the facts that are true  and x  y  contains all such sets. states are restricted to only involve objects that appear in the corresponding observation. we often view relational states as propositional. given a finite d1   d  denote by y d1  the propositional state space over n binary variables  one variable for each of the n = o |d1|q  state facts involving only objects in d1  where q is the maximum state-predicate arity.
example 1. in our video domain we infer force-dynamic state sequences from videos of a hand playing with blocks. d contains all hands and blocks we might encounter. there are three force-dynamic state predicates and eight observation features  shown in table 1. figure 1 depicts two distinct force-dynamic states. observations are sets of observation facts calculated for the objects and object pairs based on the
perceptron trn {耳1 ... 耳v} 而 m  k ↘ 1; c~ = ~1;
repeat m times 
for-each ho si ﹋ trn
朴 ↘ {h耳1 c1i ... h耳v cvi}
s  ↘ ssm-dp o 考r而 ﹞|朴  k  if s  1= s 
	c~ ↘  c~ + v~  s o 	    v~  s o  +
k ↘  k + trans s     trans s  +
return hc k~	i
figure 1: generalized perceptron pseudocode.  c~ + = c~1 s.t. ci1 = max 1 ci .
object tracker output.
﹛relational horn constraints. astateatomis a state predicate or the relation  1=  applied to variables. an observation atomhas the form   f1 r f1    where r ﹋ {= ≒}  and fi is a number or an observation feature applied variables. a relationalhornconstrainthas the form  body ↙ head   where body is a conjunction of state and/or observation atoms  and head is a state atom or false and may only contain variables that appear in body. for example   distance x y  ≒ 1  ＿  1 ≒ speed y   ↙ attached x y  is a relational horn constraint for predicting object attachment based on an observation. a relational horn constraint 耳 is a schema for propositional constraints. any way of  consistently  replacing variables in 耳 with objects gives a  propositional  ground instance of 耳. given a set of objects d1  ground 耳 d1  contains all ground instances with only objects in d1.
﹛relational cost models. a relationalsimple-transition model is a pair h朴 ki  with transition-cost k and nonnegatively weighted relational horn constraints 朴 = {h耳1 c1i ... h耳v cvi}. given a relational o-sequence o1:t with objects d1  we know that states may only involve facts constructed from d1  and thus we need only consider the propositional state space y d1 . to infer an s-sequence over y d1  we use the set of propositional constraints 朴p = sh耳 ci﹋朴 s耳1﹋ground 耳 d1 h耳1 ci to define an atemporal cost function ca s|o 朴p  as in section 1. we then return the lowest-cost s1:t given by ssm-dp o1:t 考r而 ﹞|朴  k   where a relational ssm function with search bound 而. that is 考r而 is computed by compilation to a propositional ssm function 考而 and then using our bounded-search dual max-sat approach.
﹛a naive implementation of this approach can be expensive since 朴p can be large. fortunately  in practice  our relational representation allows us to avoid constructing most of the set. we use efficient forward-chaining logical inference to construct 忙  o1:t 朴p   the input to max-sat  without explicitly constructing 朴p. space constraints preclude details.
1 learning a relational stm
we learn relational stms by first running the relational learner claudien  de raedt and dehaspe  1  on a training set of labeled o-sequences  acquiring  nearly sound  relational horn constraints that are satisfied by the training data  producing 1 constraints for our domain . next  using a new training set we jointly tune the constraint weights and transition-cost k using a variant of collins' voted perceptron algorithm  collins  1 . the algorithm extends rosenblatt's perceptron for binary labels  rosenblatt  1  to handle structured labels such as sequences  and has convergence and generalization properties similar to rosenblatt's.
﹛the algorithm requires representing cost using a linear combination of n features  which we do as follows. given o1:t and s1:t involving just objects in the finite d1  and a relational stm h{h耳1 c1i ... h耳v cvi} ki  the violation-count feature of 耳i is vi o1:t s1:t  =
p1≒i≒t p耳1﹋ground  i.e. the number of unsatisfied instances of 耳i. we let v  o1:t s1:t  be the v-dimensional vector of violation-count features and c~ =  c1 ... cv  is the weight vector. the transitioncountfeature trans s1:t  is equal to the number of state transitions in s1:t. these v + 1 features represent the stm cost as
c s1:t|o1:t  = v~  o1:t s1:t  ﹞ c~ + trans s1:t  ﹞ k.
﹛the algorithm  see figure 1  cycles through the training data and when an incorrect s-sequence s  is inferred in place of s  the weights are adjusted to increase cost for s  and decrease cost for s. the input is a training set trn  relational horn constraints {耳1 ... 耳v}  an ssm search bound 而  and the number m of iterations. the output is the learned weights c~ and transition cost k. ideally we want this stm to allow for accurate sequential inference when using the searchbounded relational ssm function 考r而. unlike collins we require non-negative weights for inference. thus  we set a weight to zero if a normal perceptron update would result in a negative value. this variant has not yet been shown to possess the convergence and generalization properties of the unconstrained version. however  this variant of rosenblatt's perceptron has been shown to converge  amit et al.  1 .
1 experimental results
we apply our technique to force-dynamic state inference from real video. the leonard system  siskind  1  uses these states to recognize events  such as  a hand picked up a block   see figure 1 . recently  fern and givan  1  developed a trainable system for this problem using the forward greedymerge fgm  algorithm  which outperformed prior techniques. fgm also utilizes horn constraints  but assumes that they are sound  rather than nearly sound. since this is not true for claudien-learned constraints  which were also used by fgm  that work utilized two steps to improve performance: 1  constraintpruning yields a much smaller but  hopefully   sufficient  constraint set  reducing the chance of constraint violations. 1  sequence-cleaningpreprocessing is an ad-hoc step that removes observations where constraint violations are  detected . see  fern and givan  1  for details. while these steps allowed for good performance  the soundness assumption limits the approach's applicability  as such preprocessing will not always be effective. the motivation for the work in this paper was to develop a  softened  more robust framework for utilizing nearly-sound constraints.
﹛procedure. our corpus of 1 videos from  siskind  1  contains 1 event types  1 movies each  involving a hand playing with up to three blocks  e.g. assembling a tower . we use the hand-labeled training sets of size 1  trn-1  and 1  trn-1  from  fern and givan  1 . the remaining videos are labeled with their compressed s-sequences  the output of compress   which is the label we wish to predict.
for each training set  we learn relational stms for three ssm search bounds 而 = 1 1. seven training instances are used by claudien to acquire constraints and the perceptron algorithm is run for 1 iterations on the remaining instances. we evaluate each iteration's model according to the % of test videos for which ssm-dp's compressed inferred s-sequence is correct  using the same 而 as for learning .
﹛we compare against fgm used with an without sequence cleaning and always with pruning. we also compare against a system that is closer to mainstream graphical modeling techniques  which uses the counting-transition model of section 1  and walkmaxsat  jiang et al.  1  for approximate inference. the model is identical to our relational stms  using the same claudien-learned constraints  except that it has a non-simple transition structure. for sequential inference we construct a single large max-sat problem corresponding to the transition-counting model and use walkmaxsat to find an approximate solution. we tune the model's weights using the perceptron algorithm with walkmaxsat  rather than ssm-dp  for inference.
﹛performance across iterations. table 1 shows  for each training set  the testing error of our learned stms for 而 = 1 1  shown as ssm-dp 而    over the first eight perceptron iterations. there is always a rapid improvement after iteration one  followed by a period of fluctuating or constant performance. the fluctuation continues out to iteration 1  not shown   but never improves over the best performance shown. in practice  one could use cross-validation to select a good model  or consider  weight averaging   collins  1 . note that the best performance on the smaller training set trn-1 is superior to trn-1. our explanation for this is that by using relatively small training sets  the results can be significantly affected by a small number of particularly noisy movies  presumably in trn-1 . experiments not shown  provide evidence for this by training on subsets of trn-1
﹛bounding search. the search bound 而 = 1 means that ssm is  approximately  solved via hill-climbing. surprisingly we can learn weights for which hill-climbing performs well. increasing the search bound improves performance with respect to the best model across iterations  particularly for trn-1. the last column shows the frames processed per second when inferring states for our corpus. inference time apparently does not increase linearly with 而  indicating that the ssm search typically end much before reaching the bound. increasing 而 to 1  doubles inference time  but neither improves nor hurts results.
other techniques. we significantly outperform walk-
table 1: % error on test movies across training iterations. the last column gives frames processed per second  fps  by the best model in each row on our video corpus.
	trn-1  iteration 	trn-1  iteration 	fps
111111	11ssm-dp 1 11111111.1.1.1.11.1ssm-dp 1 11111111.1.1.1.11.1ssm-dp 1 11111111.1.1.1.11.1walkmaxsat111111	111fgm / fgm+sc1 / 11 / 11maxsat  further iterations did not help   which we allowed a generous search time  using search cutoff 1 with 1 random restarts  other settings did not improve . further experiments suggest that walkmaxsat does not scale well for our problems. using learned stm weights for the countingtransition model  walkmaxsat reliably infers correct ssequences for short videos  but is very poor for long videos. walkmaxsat gave equally poor results when used to learn stms rather than counting-transition models. this suggests the poor performance is primarily due to the ineffectiveness of this general-purpose inference technique for our models. we conjecture that other approximate techniques such as  loopy  belief propagation and gibbs sampling will also yield inferior performance. indeed  gibbs sampling is very similar to walkmaxsat. evaluating this conjecture is future work. fgm without sequence cleaning  shown as fgm  is an order of magnitude worse than stms  which never use cleaning . we significantly improve on fgm with sequence cleaning  fgm+sc  on trn-1 and yield equal performance on trn-1. fgm is faster  close to frame rate. a reimplementation of our lisp prototype will likely achieve frame rate.
1 related work
segment models  ostendorf et al.  1  subsume our stm formulation. we are not aware of prior work that has leveraged or noted the ssm reduction for large state spaces. perhaps this is because prior segment modeling work typically utilizes non-simple transition structure  perhaps sometimes unnecessarily  and to our knowledge has not been applied to large state spaces.
﹛our  model schema  approach for handling relational data is now standard in probabilistic modeling  de raedt and kersting  1 . most closely related are relational markov networks  rmns   taskar et al.  1 . a relational stm can be viewed as defining a log-linear conditional rmn  where the rmn  clique feature templates  correspond to relational horn constraints and a transition template. rmns are very general and thus the existing techniques do not fully exploit the structure of relational stms. instead rmn-like proposals rely on general-purpose approximate inference  e.g. belief propagation   with unclear practical implications. likewise dynamic probabilistic relational models  sanghai et al.  1  provide a generic schema-based approach specialized to relational sequence data. again the generality precludes leveraging the stm structure.
acknowledgments
this work was supported in part by nsf grants 1-iis and 1-iis.
