* 
           an important research problem in a r t i f i c i a l i n t e l l i g e n c e is the study of methods for l e a r n i n g general concepts or r u l e s from a set of t r a i n i n g instances. an approach to t h i s problem is presented which is guaranteed to f i n d   without b a c k t r a c k i n g   a l l r u l e versions consistent with a set of p o s i t i v e and negative t r a i n i n g instances. the algorithm put f o r t h uses a r e p r e s e n t a t i o n of the space of those rules consistent w i t h the observed t r a i n i n g data. this   r u l e version space  is modified in response to new t r a i n i n g instances by e l i m i n a t i n g candidate r u l e versions found to c o n f l i c t with each new instance. the use of version spaces is discussed in the context of meta-dendral  a program which learns r u l e s in the domain of chemical spectroscopy. 
d e s c r i p t i v e terms: version space  r u l e l e a r n i n g   candidate e l i m i n a t i o n   concept f o r m a t i o n   concept l e a r n i n g   machine l e a r n i n g   meta-dendral. 
	1 	introduction 
　　　　rule l e a r n i n g and concept l e a r n i n g have become i n c r e a s i n g l y important goals for a r t i f i c i a l i n t e l l i g e n c e  ai  researchers with the recent emphasis w i t h i n the ai community on c o n s t r u c t i n g knowledge based systems   1       1   . a program capable of e x t r a c t i n g general r u l e s from t r a i n i n g instances would be a valuable t o o l f o r the d i f f i c u l t task of c o n s t r u c t i n g knowledge bases for use in such systems. although there has been some success in t h i s area   1      we are s t i l l far from an understanding of e f f i c i e n t   r e l i a b l e methods for c o n t r o l l i n g the combinatorics inherent to the task of l e a r n i n g . 
	the 	r u l e 	l e a r n i n g 	or 	concept 	learning 
* this work was supported by the advanced research projects agency under contract dahc 1-c-1 and by the national i n s t i t u t e s of health under grant rr 1. computer resources were provided by the sumex-aim computer f a c i l i t y at stanford u n i v e r s i t y under nih grant rr-1. bruce g. buchanan has provided many useful suggestions throughout the course of t h i s work. lew g. creary assisted in the c l a r i f i c a t i o n of several issues discussed in t h i s paper. 
problem addressed in this paper is the following. it is given that some fixed action  a  is advisable in some class of  positive  training instances  i+  but is inadvisable in some disjoint class of  negative  training instances  i - * * . the task is to determine a rule of the form 
p a  where p is a set of conditions or constraints from some predefined language. these conditions must be satisfied for action a to be invoked. the learned rule must apply to a l l instances from i+  but to no instances from i-. 
       one popular paradigm for this rule learning task is the search paradigm. in this approach  a rule which is consistent with the f i r s t training instance is determined  then additional training instances are considered one at a time. at each step  p is revised as needed according to a well defined set of legal alterations  so that the resulting rule version applies to exactly the correct set of instances. this search through the space of allowed patterns for the correct statement of p *** may be viewed as a concept formation problem in which the concept being learned is  the class of instances in which action a is recommended . 
       two difficulties arise in the search paradigm. first  in determining the set of possible alterations to the rule in response to a given training instance   i . e . in determining the set of legal branches at a given point in the search tree   care must be taken to assure that each is consistent with correct rule performance on past training instances. second  backtracking is sometimes required to try a different branch of the search tree when subsequent training instances reveal that an inappropriate decision has been made. 
       this paper proposes an approach to learning rules which f a l l s outside the search paradigm outlined above. this candidate elimination  algorithm relies upon a method for representing and updating the space of a l l rule versions 
 * in many rule induction tasks obtaining this assignment of training instances to 1+ and i- for a given action is a d i f f i c u l t problem in i t s e l f . see minsky  1 for a discussion of the credit assignment problem. 
*** for a general discussion of rule induction as heuristic search see simon and lea   1     and buchanan . 
       
knowledge 	a c q . - l : 	m i t c h e l l 
1 
       
consistent with the observed data. rules are eliminated from this rule version space as they are found to conflict with observed training instances. the algorithm is guaranteed to find  without backtracking or reexamining past training instances  the set of a l l rules consistent with the observed data. 
1 version spaces 
       this section proposes a candidate elimination approach to rule learning which maintains and modifies a representation of the space of a l l plausible rule versions  contrast this with the search paradigm discussed above which maintains and modifies a single current best hypothesis of the correct rule version . methods for representing this rule version space and for modifying it in response to new training data are presented in this section. the candidate elimination algorithm is guaranteed to find a l l rule versions consistent with the observed training data. this is accomplished without backtracking and independent of the order of presentation of the training instances. 
1 	definition and representation 
       the term version space is used in this paper to refer to the set of current hypotheses of the correct statement of a rule which predicts some fixed action  a. in other words  it. is the set of those statements of the rule which cannot be ruled out on the basis of training instances observed thus far. it is easy to see that this version space contains the set of a l l plausible rule revisions which may be made by a search algorithm in response to some new training instance. 
       more exactly  assume that there is some rule r which correctly predicts action a for the class of training instances i+  but not for instances in the class i-. the rule version space of 1+ and iis then defined as the set of a l l such rules. the rule version space associated with action a and the instances 1+ and i- is an equivalence class of rules with respect to their predictions on 1+ and i-. the elements of the version space are rules which predict the same action  but which differ in the patterns stated in their left hand sides. 
       before writing programs which reason in terms of version spaces  we must have a compactdata representation for them. in general  the number of plausible versions can be very large  possibly infinite  when the language of patterns for rules is complex. the key to an efficient representation of version spaces lies in observing that a general-to-specific ordering is defined on the rule pattern space by the pattern matching procedure used for applying rules. the version space may be represented in terms of i t s maximal and minimal elements according to this ordering. 
to see 	exactly how 	the general-to-specific ordering comes about  consider an example. suppose that r1 and r1 are two rules which predict the same action. then r1 is said to be more specific  or  equivalently  less general  than r1 if and only if it w i l l apply to a proper subset of the instances in which r1 w i l l apply. this definition  which relies upon the pattern matching mechanism  is simply a formalization of the intuitive ideas of  more specific  and  less general . for the remainder of this paper  the terms general and specific w i l l be understood to have these well defined meanings. 
       the general-to-specific ordering w i l l in general be a partial ordering; that is  given any two rules we cannot  always say that one is more general than the other. for instance  two rules can easily apply to some of the same instances without the requirement that one of them apply everywhere that the other applies. therefore  when a l l elements of the version space are ordered according to generality  there may be several maximally general and maximally specific versions. 
       version spaces can be represented by these sets of maximally general versions. mgv  and maximally specific versions. msv. given such a representation it is quite easy to determine whether a given rule belongs to a given version space. a rule statement belongs to the version space of 1+ and i- if and only if it is  1  less general than or equal to one of the maximally general versions  and  1  less specific than or equal to one of the maximally specific versions. condition  1  assures that the rule cannot match any training instance in i-  while condition  1  assures that it w i l l match every training instance in i+. since the sets mgv and msv are by definition complete   1  and     w i l l be necessary as well as sufficient conditions for membership of a rule statement in the version space. 
1.1 	an example: meta-dendral 
       an algorithm for representing version spaces as described above has been implemented in the meta-dendral program. meta-dendral  is a program which learns production rules to describe the behavior of classes of i olecules in two areas of chemical spectroscopy. the version of the program which determines rules associating substructures of molecules with data peaks in a carbon-1 nuclear magnetic resonance spectrum shall be considered here . 
       figure 1 shows a version space represented by the program in terms of the sets of maximally specific rule versions  rule msv1  and maximally general rule versions  rules mgv1 and mgv1 . the rule pattern which expresses the conditions for application of each rule is stated in a f a i r l y complex network language of chemical subgraphs. each node in the subgraph represents an atom in a molecular structure. each subgraph node has the four attributes shown  with values constrained as shown in figure 1. arcs between nodes in the rule 
       
knowledge 	a c q . - l : 	m i t c h e l l 
1 
       

	figure 1. 	a version space represented by i t ' s extremal sets 
msv1 is the maximally specific rule version. mgv1 and mgv1 are maximally general rule versions. only the rule patterns  left hand sides  are shown above. all rules shown predict the same action: the appearance of a peak associated with atom  v  in a given range of the spectrum. 
       
subgraph  shown schematically as lines between the node letters  represent chemical bonds between atoms. the definition of a match of the rule pattern  subgraph  to a training instance  molecule graph  requires that the subgraph   f i t into  the molecule graph  and that the subgraph node attribute constraints be consistent with the attribute values of the corresponding node in the molecule graph. if a molecule graph contains a set of nodes which f i t the pattern  then the corresponding action is predicted by the rule. in this program the classes 1+ and i- are sets of molecules for which the indicated spectral peak does and does not appear. 
       the version space represented in figure 1 contains several hundred rule versions: the three versions shown plus a l l versions between these in the general-to-specific ordering. however  it can be represented simply by the two maximally general versions  mgv1 and mgv1  and the single maximally specific version  msv1. the single most specific version contains every node and node attribute constraint consistent with a l l training instances in i+. thus  any more specific version cannot match every element of i+. two general versions are required in this example since neither is  above  the other in the general-to-specific partial ordering. any rule more general than either mgv1 or mgv1 w i l l match some element of i - . furthermore  any rule which is between these general and specific boundaries of the version space w i l l match a l l current instances in 1+  by virtue of being more general than msv1   and w i l l match no current instances from i-  by virtue of being more specific than mgv1 or mgv1 . notice that the constraints stated by msv1 are more strict than those stated by mgv1 and mgv1  and that msv1 contains a superset of the nodes contained in the maximally general versions. this is consistent with the general-to-specific ordering discussed above  in that msv1 w i l l apply to a proper subset of those instances to which mgv1 and mgv1 apply. 
1 	version spaces and rule learning 
       recall the rule learning task discussed earlier. a program is given examples of two classes of training instances  1+ and i - . the program must determine some rule which w i l l produce a given action  a  for each training instance in i+  but for no instance in i - . 
       the candidate elimination algorithm presented here operates on the version space of a l l plausible rules at each step  beginning with the space of a l l rule versions consistent with the f i r s t positive training instance  and modifying the version space to eliminate candidate versions 
       
knowledge 	ar.q.-l 	m i t c h e l l 
1 
       
found 	to 	conflict 	with 	subsequent 	training instances. 
       the chief difference between the candidate elimination approach and the search approach discussed above is that search techniques select and modify a current best hypothesis of the form of the rule. rather than select a single best rule version  the candidate elimination algorithm represents the space of a l l plausible rule versions  eliminating from consideration only those versions found to conflict with observed training instances. thus  the candidate elimination approach separates the deductive step of determining which rule versions are plausible  from the inductive step of selecting a currentbest-hypothesis. at any step  the same heuristics used by search methods to infer the current best hypothesis may be applied to infer the bestelement contained in the version space. however  by refraining from committing i t s e l f to this inductive step  the candidate elimination algorithm completely avoids the need to backtrack to undo past decisions or reexamine old training instances. at the same time the algorithm is assured of finding a l l correct versions of the rule after a l l training data has been presented. these are the strongest two advantages of the candidate elimination approach to learning. 
1.1 	meta-dendral example revisited 
       the candidate elimination algorithm using version spaces has been implemented as part of the meta-dendral program. recall from the earlier example that in this program the training instance classes 1+ and i- are molecule graphs for which some action   i . e . the appearance of a peak in some region of their spectra  should or should not be predicted. the f i r s t part of meta-dendral determines several different predicted actions associated with sets of positive and negative training instances* rule version spaces for each distinct predicted action are then generated from the training instances associated with the action. subsequent data may be analyzed to modify the version space in a manner guaranteed to be consistent with the original data. 
       the candidate elimination algorithm operates on the maximally general and maximally specific sets representing the version space. the set of maximally general rule versions  mgv  is initialized to a single pattern consisting of the most general statement in the language of rule patterns  a single atom graph with no constrained node attributes . the set of maximally specific versions  msv  is initialized to a rule which contains as i t s pattern the f i r s t instance in i+. the i n i t i a l version space represented by these extremal sets therefore contains a l l rules in the language which match the f i r s t training instance. 
       the training instances are then considered one at a time. each training instance is used to eliminate from the version space those rule versions which conflict with that instance. this is always accomplished by shifting the maximally specific and maximally general boundaries of the version space toward each other as shown in figure 
1. 

       positive training instances force elements of msv to become more general  whereas negative training instances force elements of mgv to become more specific. the maximally specific set can  of course  never be replaced by a more specific set  nor the maximally general set by a more general one  since by definition  any version outside the current version space boundaries is inconsistent with previous training data. the action taken by the candidate elimination algorithm in updating the extremal sets is given below. 
       if the new training instance belongs to i-  then each element of mgv which matches the instance must be replaced by a set of minimally more specific versions which do not match the instance. these new versions are obtained by adding constraints taken from elements in msv in order to ensure that they remain more general than some msv  and thus remain consistent with previous 1+ instances. furthermore  each element of msv which matches the negative training instance must be eliminated from the set  since it is already maximally specific  it cannot be replaced by a more specific version . 
       if the new training instance belongs instead to i*  then any elements from msv which do not match the new positive training instance are replaced by a set of minimally more general elements which do match the instance. in order to ensure that these more general versions do not match past training instances from i -   any which are not more specific than at least one element of mgv are eliminated. elements from mgv which do not match the positive instance are eliminated. 
       after processing each training instance  the new maximally general and maximally specific sets 
       
knowlerlkf* 	a c q . - l : 	m i t c h e l l 
       
 such as those shown in figure 1  will bound the space of fill rules consistent with the observed data. notice that by modifying the version space in the above way  a l l rules  and only those rules  which conflict with the new training instance are eliminated from consideration. 
1 reasoning with version spacrs 
       an explicit representation for the space of plausible rule versions appears to have uses in addition to that described above. this section discusses some limitations on the general applicability of version spaces  as well as some promising additional uses. 
1 	applicability and limitations 
       the version space approach to rule learning described above is limited in certain respects  the algorithm is based upon the assumption that the assignment of training instances to 1+ and iis consistent   i . e . the supplied classification of training instances can be generated by at least one rule in the rule space . in some domains this may be a valid assumption  but in certain  noisy  domains the process of classifying instances may be unreliable or the training instances themselves may be inconsistent. if the set of training instances is not consistent  the algorithm w i l l eliminate a l l rule versions from consideration  and backtracking w i l l be required. this is  however  no worse than is expected from other nonstatistical algorithms  a l l of which also require backtracking in this case. one positive point is that the collapse of the version space to the null space provides an immediate indication that something has gone wrong. specifically  it indicates that there is no rule within the supplied rule language which can discriminate between 1+ and i-  this can occur either for noisy data  or in the case where the rule language is not sufficiently complex to represent the given dichotomy of 1+ and i- . 
       one method for making the candidate elimination algorithm more accommodating of noisy data might be to eliminate only candidate versions which conflict with some fixed number of training instances greater than one* the price paid in exchange for this extension would be a decrease in the rate at which the version space boundaries converge toward each other. 
       a second limitation on the general applicability of version spaces may l i e in the nature of the partial ordering of rule versions. for simple languages of rule patterns the sizes of the maximally general and maximally specific sets of versions w i l l be small. it appears in metadendral that the size of these sets may be manageable for simple molecules in spite of the complex language of rule patterns* however  it is possible that for some domains the size of these extremal sets may become quite large. in metadendral  information about the interdependences of the node properties is used to eliminate syntactically distinct rule statements which are semantically equivalent. thus  the size of the extremal sets is not adversely affected by redundancy in the rule language. a second possible method for limiting the size of the maximally general and maximally specific version sets is the introduction of domain-specific constraints on allowed elements of the version space. it is common in ai programs to use task domain knowledge to constrain combinatorially explosive problems. it may be possible to incorporate such constraints in the rout ines which manipulate version spaces in order to dismiss a priori unlikely rule statements present in the rule language. 
       it appears that the practical applicability of the version space approach to other domains is limited only by the above two constraints: the requirement of reliable training data  and the danger of overly large maximally general and maximally specific sets. the only assumption c r i t i c a l to this approach is that a partial ordering exist over the space of rule patterns. this assumption is always satisfied since the ordering is defined in any domain by the pattern matcher employed. in general it seems that the version space approach w i l l be most efficient in domains in which the the partial ordering over the pattern space is not extremely  branchy   but where each branch may be quite deep. this is due to the fact that only the most and least specific plausible version   i f any exist  along each branch need be saved. thus  the efficiency of the version space algorithm  and the size of the extremal sets  is unaffected by the depth of each branch  but is adversely affected by the number of branches. in contrast  the efficiency of search procedures and their need for backtracking appear to be adversely affected by both the number of branches in the partial ordering and the depth of the branches. 
1 	other uses for version spaces 
       version spaces provide an explicit representation of the range of plausible rules. with this explicit representation  the program acquires the ability to reason more abstractly about i t s actions. the program is aware of more than the current best hypothesis - it has available the entire range of plausible choices. this view of version spaces suggests their use for tasks other than the particular rule learning task described above. two such tasks w i l l be suggested in this section. 
1.1 	selecting new training instances 
       the importance of careful selection of training instances for efficient and reliable learning has been stressed by several writers     yet few learning programs take an 
       
knowlehfre a c q . - l : 	m i t c h e l l 
       
active role in determinin  g their own training instances. one notable exce ption is an induction program written by popples one  1 which i t s e l f generates training instances whose  user supplied  classification resolves among competing hypotheses. the version space representation appears well suited for al lowing the program to generate i t s own set o f c r i t i c a l training instances. 
       since version spaces represent the range of rule versions which cannot be resolved by the current training data  they also summarize the range of unencountered training inste ices that w i l l be useful in selecting among competing rule versions. by constructing a training instance which matches some  but not a l l   of the maximally general versions  the program may be able to determine which of several potentially important attributes should be specified in a rule. on the other hand  by constructing training instances which match a given most general version  but not i t s most specific counterpart  the program may determine how specific the constraint on a given attribute must be. note  as pointed out by one of the referees of this paper  that the generation of training instances might also provide a useful tool for limiting the size of the maximally specific and maximally general sets  in that examples designed to discriminate among competing extremal versions could be generated. 
1.1 	merging separately obtained results 
       the construction of rel iable knowledge bases for use in knowledge based programs w i l l almost certainly require learning from a large variety and number of training insta nces. merging rules learned from different data sets may therefore become a desirable capability  consider breaking a large training data set into several smaller sets to be analysed in parallel . 
       version spaces allow a convenient  consistent method for merging sets of rules generated from distinct training data sets* the intersection of the version spaces of two rules formed from two sets of training data yields the version space of a l l rules consistent with the union of the data sets. the result obtained by intersecting version spaces derived from different data sets is therefore the same as would be obtained by running the candidate elimination algorithm once on the union of the training data. training 	instances  	and 	the 	final 	result 	is independent 	of 	the 	order 	of 	presentation 	of instances. 
       version spaces provide at once a compact summary of past training instances and a representation of a l l plausible rule versions. because they provide an explicit representation for the space of plausible rules  version spaces allow a program to represent  how much it doesn't know  about the correct form of the rule. this suggests the u t i l i t y of the version space approach to problems such as intelligent selection of training instances and merging sets of independently generated rules. 
