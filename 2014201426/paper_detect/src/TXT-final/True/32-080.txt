 
the problem of action selection by autonomous agents becomes increasingly difficult when acting in continuous  non-deterministic and dynamic environments pursuing multiple and possibly conflicting goals. we propose a method that exploits additional information gained from continuous states  is able to deal with unexpected situations  and takes multiple and conflicting goals into account including additional motivational aspects such as dynamic goals  which allow for situation-dependent motivational influence on the agent. further we show some domain independent properties of this algorithm along with empirical results gained using the robocup simulated soccer environment. 
1 	introduction 
agents in a complex dynamic domain need to take multiple goals into account  which may be of different type  as exemplified in the robocup soccer environment : 
  maintenance goals  which should be less demanding the more the goal is satisfied  e.g. 'have stamina' . 
  achievement goals  which should be more demanding the closer the agent is to the goal  e.g. 'score a goal' . 
¡¡maes  1; 1; 1  suggested a mechanism for action selection  masm - maes action selection mechanism  tyrrell  1   in dynamic and unpredictable domains based on so-called behavior networks. although masm-networks do work in continuous domains  they do not exploit the additional information provided by continuous states. similarly  though there are mechanisms to distinguish different types of goals in masm  there are no means to support goals with a continuous 

   *the work reported here has been funded by the german research association  dfg  graduiertenkolleg menschliche und maschinelle intelligenz . i would like to thank gerhard strube and bernhard nebel for important comments and suggestions during the preparation of this research. 
truth state  like 'battery charged'  to become increasingly demanding the less they are satisfied. 
¡¡we propose a revised and extended version  reasm  of maes' action selection mechanism  that takes the step from discrete to continuous domains by introducing realvalued propositions. it also allows for advanced motivational control by situation-dependent goals  and retains the advantages of masm  such as reactivity  planning capabilities  robustness  accountance of multiple goals and the cheap and distributed calculation. 
¡¡we give a formal definition of extended behavior networks and describe the activation spreading and action selection algorithm in section 1. in section 1 we show some domain independent properties such as activation spreading always reaching a stable state. among other empirical results  we show in section 1 that the extensions proposed show significantly better success in the robocup domain. 
1 reasm formalism 
in this section we give a formal description of the revised and extended behavior networks followed by the algorithms for activation spreading used to calculate the utility of a behavior and for action selection  which decides on the behavior to execute. 
1 	behavior network description 
let s be a set of worldstates   a set of atoms  and  a function assigning a truth value 
to each atom in each worldstate. v is a set of atoms and negated atoms where  is a propositional language over v and the logical connective  where 	and is any continuous triangular norm  e.g. is a propositional language over and the logical connectives and where 	and is any continuous triangular conorm 
 saffiotti et al. .  1 . 
¡¡reasm behavior networks b are described by a tuple  where 
denotes the set of goals characterized as tuples with 
	dorer 	1 

- gcon  the goal condition  i.e. the situation in which the goal is satisfied  
- i the importance of the goal 
- rcon  the relevance condition  i.e. the situation-dependent importance of the goal with  the relevance of the goal. 
  m is a finite set of competence modules  where m  is a tuple 	with 
	~ pre 	denoting the precondition with 
the degree of executability; 
- b the behavior  which is executed once the module is selected for execution; 
- post is a tuple  eff  ex   where eff  are the effects of the behavior and exj denotes the expectation €  1..1  of effect proposition effj to become true after executing this module; 
- a the activity € 1r indicating the utility of the module with ag the vector of activations a1i received  directly or indirectly  by goal qi. 
  ii is a set of  domain-dependent  parameters used to control activation spreading; 
activation of modules  
inhibition of modules  
inertia of activation  
activation threshold  with the up-
   per bound for a module's activation  threshold decay. 
¡¡the revised activation spreading algorithm of reasm  see next section  made it possible to reduce the number of parameters and to restrict them to the ranges printed above. this simplifies the process of finding best performing parameter values for a domain  see section 1 . 
1 	a c t i v a t i o n spreading 
the competence modules are connected in a network 
 maes  1  to receive activation from goals and other modules. a competence module receives activation from a goal 
 i  
if the module has an effect  with expectation exj  that is part of the goal condition and both are atoms or both are negated atoms  i.e. the behavior satisfies the goal  f is any continuous triangular norm  that combines the static importance of the goal  and the dynamic relevance 

a module k is inhibited by a goal i by 
		 1  
if the module has an effect that is part of the goal condition and exactly one of them is negated  i.e. the behavior would undo an already satisfied goal. 
¡¡¡¡a module receives activation by a so called successor module if it has an effect  with expectation that is part of the preconditions of the successor module 
1 	software agents 
and both are atoms or negated atoms. the activations are calculated separately for each goal activation a1uccgi of the successor module and are 
	. 	 1  
where o  is the transfer function of the modules activation for which we used 
 goetz  1 . the term states that the less the precondition of the successor module is satisfied in situation s the more activation is spread to modules making this precondition true  i.e. the false precondition becomes an increasingly demanding subgoal of the network. 
finally a module is inhibited by conflictor modules by 
		 1  
if it has an effect that is part of the preconditions of the conflictor module  and exactly one of them is negated.  is the activation the conflictor module received directly or indirectly by goal p  at timestep 
¡¡the activation  of each goal is kept separately by each module and is set to the activation of that link with the highest absolute maximum activation 
		 1  
in other words  only the strongest path from each goal to a module is taken into account. any confluence of activation within a module from the same goal is prohibited. this leads to some important new properties of the algorithm shown in section 1. 
finally the activation of a module k is 
		 1  
where controls the inertia of the activation and therefore the inertia of the agent's behavior. 
¡¡activation and inhibition as well as the introduction of relevance conditions allow the modelling of different types of goals. increasingly demanding maintenance goals  e.g. 'have stamina' in the robocup soccer environment  can be achieved by adding a relevance condition  'stamina low' . this increases the relevance of the goal and therefore the activation of satisfying behaviors by the goal as the situation diverges from the state to be maintained. achievement goals  e.g. 'score a goal'  can be realized by adding a relevance condition  'close to goal'  whose truth value increases on nearing the goal. modules achieving the goal are increasingly activated  modules conflicting with the goal are inhibited. 
1 	a c t i o n selection 
action selection is done in a cycle containing the following steps  maes  1  
1. calculate activation of each module   eq. 1 . 
1. combine activation and executability of a module by a non-decreasing function 
to prevent non-executable modules from being executed   should be zero. 

1. if the highest value lies aboveexecute the corresponding module's behavior  reset to its original value and go to 1. 
1. otherwise reduce  by and go to 1. 
¡¡step 1 is necessary because modules have a continous executability e and can therefore not be divided into executable and non-executable as opposed to masm. all modules have to be considered for execution prefering modules with higher executability  although modules with high activation may be executed even if their executability is low. 
1 	domain independent properties 
in this section we show properties of the algorithm for activation spreading that are domain independent. 
1 	stability 
an important property of activation spreading networks is to reach a stable state of activation. although this seems to be the case for maes' behavior networks  to our knowledge it has never been proven. however  variations of maes' networks  variation four of  tyrrell  1   oscillate under some circumstances. the algorithm proposed here can be proven to reach a stable state. 
lemma 1 the above described algorithm for activation spreading does not allow feedback of activation for a  a  

proof. although there possibly are cycles within reasm behavior networks  the activation a module gets from itself because 	 eq. 
 1 and 	is the no of excitation links  m the number of inhibition links within the cycle. for either or  and therefore approaches zero. 
for m = n = 1 the module is not part of a cycle. 
theorem 1 activation in reasm networks always reaches a stable state  unless the situation changes . 
proof. activation for each goal is calculated separately by the competence modules. therefore we can treat each goal separately and look at the connected subgraphs containing one goal. we split the vertices of this subgraph into two sets:  contains the vertices which have reached a stable state of activation  v contains all the other nodes of the subgraph. initially  v¡ã only contains the goal. after one step of activation spreading the node with the strongest link to the goal  in terms of maximum absolute activation  eq. 1   will receive constant activation a1i and can be removed from v and put into v¡ã. this holds because activation decreases along activation paths  proof of lemmal  and because each module receives activation across a single incoming link  eq. 1 . this can be repeated for all nodes in v  although activation of these nodes may take more than one step of activation spreading to reach a stable state  because of previous activation caught in feedback cycles. this activation feedback  however  drops to zero for  lemma 1 . the main activation of a module m received 
by all goals 	then equals 
 eq. 1  and d 
1 	problems m e n t i o n e d by t y r r e l l 
tyrrell  pointed out some problems of behavior networks proposed by maes. we show that none of these problems hold within reasm. 
 preference for appetitive behaviors1 the action selection mechanism proposed by maes shows some undesirable preference for appetitive nodes over consummatory nodes  see fig. 1  independent of parameter settings  tyrrell  1 . 
in masm activation of an appetitive node  action1  is 
 while a consummatory 
node  action1  receives less activation 
 this is undesirable  because action1 directly satisfies the goal while action1 only satisfies a precondition of action 1 which reaches the same goal. 
       figure 1: preference for appetitive nodes at the top level are the goals with their importance  below are the competence modules and their activation and at the bottom level the situation propositions  perceptions  with their r-values. only the relations of activation values matter. masm  left  undesirably prefers appetitive action1  reasm  right  correctly prefers action1  action 1 is non-executable . 
¡¡in reasm  activation from a goal always decreases with the distance to that goal  proof of lemma 1  preferring modules that directly satisfy it. this prevents activation feedback from occurring  lemma 1  and therefore the preference for appetitive behaviors. 
activation fan 
masm divides activation spread by goals by the number of links connected to the goal  activation fan . similarly  activation received by a node is divided by the number of incoming links. this penalizes goals with more behaviors satisfying it and does not prefer modules satisfying multiple goals at once  see fig. 1 . 
¡¡tyrrell shows however that leaving out division by the number of links causes a different problem  namely the 

   *in contrast to consummatory behaviors that try to satisfy a goal  appetitive behaviors try to prepare consummatory. 
	dorer 	1 


¡¡¡¡¡¡¡¡figure 1: problems with activation fan goals with multiple satisfying competence modules are penalized in masm  left  due to the division of activation by the number of leaving links. modules satisfying multiple goals are not preferred  right  due to the division of activation by the number of incoming links. 
confluence of activation in nodes with many successors which may all be alternatives of one goal  see fig. 1 . 
¡¡¡¡¡¡figure 1: problems without activation fan an appetitive behavior  action1  cannot distinguish between getting activation originally spread by one goal  left  or by multiple goals  right . 
¡¡neither problem holds for re asm. activation is not divided by the number of incoming or outgoing links. therefore behaviors satisfying multiple goals are prefered  eq. 1   goals with alternative behaviors satisfying them are not penalized. and there is no confluence of activation from the same goal due to the fact that only the strongest path of activation from each goal to a module is taken into account  eq. 1 . 
1 	empirical results 
empirical analysis of behavior networks has been conducted using the robocup soccer server program  noda  1 . agents in this domain are simulated soccer players getting their  relative  perceptions from the server across a network and sending executed actions to the server which changes its state accordingly. perception and action are non-deterministic  i.e. perceptions as well as actions are perturbed by some noise  and may be lost in the network. the state of the soccer field is dynamic: it changes whenever any of the agents performs some action. it is continuous: state  perceptions and also actions are described by continuous values. in short  the robocup domain is non-deterministic  dynamic and continuous and is therefore a demanding environment for any algorithm for action selection. 
¡¡the network used contained three goals and eight competence modules  see fig. 1 . the corresponding behaviors were implemented using  methods that were called once the competence module was selected  functions for perception-propositions were similarly cal-
	software agents 
culated from the agent's perceptions and state information using  methods. 
1 	parameter setting 
as stated above  activation spreading and action selection depend on a set of parameters. these parameters are domain dependent and have to be tuned to obtain best performance from the agents. this was done by 
playing a series of games with equal teams of two players1 except for varying one parameter of one team along its definition area. 1 games for each of eleven variations per parameter were conducted to obtain statistically reliable results. the quality of the varied team was measured by the difference between scored goals and those scored by the other team. for the variation of the first parameter  the other parameters were set to 'sensible' values. the following parameter variations used previously found values  which performed best. because parameters are not independent of each other  this process was repeated for all parameters  until changes of best values became small 
         this led to curves as shown for the activation by goals 	in figure 1. 

figure 1: quality of a team as a function of the activation by goals 
¡¡two things should be noted: first  without any motivation by goals  the agents perform very poorly  because all modules have same activation  zero . hence  always the first executable module is executed. second  although differences like  are signifi-
cant  the score level is high for a wide range of parameter values indicating that finding a 'functional' parameter setting is not too difficult. 
1 	real-valued propositions 
to evaluate the usage of real-valued propositions we conducted a series of 1 games where one team used real-


figure 1: behavior network used for empirical evaluation in the robocup domain. top level are the goals of the agent  below the competence modules and at the bottom the perceptions. links to perceptions are used to calculate the executability of a module. links to goals and other modules are used to calculate the utility of a module. 

valued propositions while the other team used discrete propositions -values were truncated to 1 for 
1 and rounded to 1 for  no goal relevance was used for these games. the team using real-valued propositions scored significantly higher1  table 1 . conditions were fairly primitive  the team using dynamic goals scored significantly higher  table 1 . 
static dynamic p mean score 1 1 1 
discrete real-valued p mean score 1 1 1 mean no of shots 1 1   1 mean possession 1 1 1 table 1: comparison of discrete and real-valued propositions 
¡¡one reason for this is that using continuous propositions  modules with a high utility may be executed even if their executability is low. so even if the likelihood of a successful execution of the behavior is small  the high utility of one or more of its effects makes it worthwhile to try. this is reflected for example by a significantly higher number of shots at the goal by the team using  real-valued propositions  although both teams had almost equal ball possessions. 
1 	situation-dependent goal relevance 
in another experiment we introduced situation dependent goal relevances  dynamic goals . the relevance condition for the goal 'score goal' was to be in the opponent's half  true  1m behind the midline and false 1m before with linear interpolation   for 'protect own goal' it was not to be in the opponent's half and for 'be unmarked' it was for the teammate to be the nearest player to the ball. two teams  one team using situation-dependent goals  played a series of 1 games. although relevance table 1: comparison of static and dynamic goals 
1 	comparison to masm 
we also implemented the original algorithm proposed by maes  maes  1  to be able to compare both algorithms. after parameter optimization described above for both networks  we played a series of 1 games. the agents of one team were controlled by masm  the other team's action selection was conducted by reasm. both teams' agents used the same perceptions1 and identical behaviors  so the only difference was in action selection. 
masm r e asm p mean score 1 1 1 behavior switches 1 1   1 table 1: comparison of masm and reasm without real-valued propositions and situation-dependent goals 
¡¡reasm scored considerably higher than agents using masm even without the usage of real-valued propositions and situation-dependent goals  table 1 . this can be explained by the significantly higher rate in behavior switches conducted by masm. it is caused by resetting the activation of executed competence modules to zero in masm and by using sigmoidal transfer functions in reasm making behaviors attractors for activation resulting in fewer behavior changes  goetz  1 . 
1
using discrete propositions for masm. 
	d1rer 	1 

¡¡when equipping reasm with real-valued propositions and situation-dependent goals it scores significantly higher than masm  table 1 . 
masm reasm p mean score 1 1   1 table 1: comparison of masm and reasm using realvalued propositions and situation-dependent goals 
1 	limitations 
reasm does not allow for multiple behaviors executed concurrently. however  humans are able to perform well trained behaviors in parallel  unless they use the same resources  gopher and donchin  1 . assuming knowledge about the resources used by a behavior  the action selection algorithm could be changed to build sets of executable behaviors with disjunct resources and execute all behaviors in that set  with the highest sum of utilities. 
¡¡for the empirical studies  expectations of effects were set manually. although maes proposed an algorithm to learn the links of a network and their expectations  maes  1   this work does not extend to continuous domains with delayed effects. adaptive behavior networks are however inevitable once domains get increasingly complex. work in the area of reinforcement learning with delayed reward could help to extend the algorithm for learning behavior networks from experience. 
1 	discussion 
maes' algorithm contains two further kinds of links spreading activation from perceptions p to competence modules with precondition p  situation links  and from competence modules with effect p to other modules with precondition p  predecessor links . these links account for the reactivity of the system because they insert activation from perceptions into the network. however  when dropping the division of activation by the number of links that use a perception  as we proposed  situation activation of all executable modules equals is a parameter that controls the amount of situation activation . in that case there is no direct influence of these links to the selection of a behavior. the indirect influence of having more activation in non-executable modules with some preconditions satisfied did not turn out to improve action selection as goetz  and our own studies demonstrated  where parameter variations of showed best performance for 
¡¡decugis and ferber  introduced a different variation of maes' algorithm for which they proved convergence of activation. however  their algorithm does not include inhibition and therefore the ability to take unwanted effects into account. another shortcoming is that goals do not depend on the current situation. besides showing better success  situation-dependent goals simplify the creation of behavior networks  especially when 
1 	software agents 
domains get increasingly large and complex. this is because relevance conditions of goals can be used to divide up the domain into different contexts. the soccerplaying agents for example can use goal relevance to easily incorporate different strategies for play-on phases  and for phases when the game has been interrupted by the referee. when using the behaviors' preconditions to produce the same set of strategies  precondition lists of all behaviors grow rapidly  complicating the introduction of new behaviors and new strategies. 
¡¡in this paper  we have argued that real-valued propositions can be integrated into an action control algorithm using behavior networks to improve the performance of agents in continous domains. further the introduction of situation-dependent goals simplifies the creation of large behavior networks and improves agents' performance by focussing on relevant goals as exemplified by studies using the robocup domain. 
