 
looking ahead during search is often useful when solving constraint satisfaction problems. previous studies have shown that looking ahead helps by causing dead-ends to occur earlier in the search  and by providing information that is useful for dynamic variable ordering. in this paper  we show that another benefit of looking ahead is a useful domain value ordering heuristic  which we call look-ahead value ordering or lvo. lvo counts the number of times each value of the current variable conflicts with some value of a future variable  and the value with the lowest number of conflicts is chosen first. our experiments show that look-ahead value ordering can be of substantial benefit  especially on hard constraint satisfaction problems. 
1 	introduction 
in this paper we present a new heuristic for prioritizing the selection of values when searching for the solution of a constraint satisfaction problem. because the task of finding a solution for a constraint satisfaction problem is np-complete  it is unlikely that any solution technique exists that works well in all cases. nevertheless  many algorithms and heuristics have been developed which provide substantial improvement over simple backtracking  depth-first search  on many problem instances. if a constraint satisfaction problem has a solution  knowing the right value for each variable would enable a solution to be found in a backtrack-free manner. 
¡¡when a constraint satisfaction problem has only a small number of solutions  much time is often spent searching branches of the search space which do not lead to a solution. to minimize backtracking  we should first try the values which are more likely to lead to a consistent solution. even a slight increase in the probability that a value chosen is part of a solution can have substantial impact on the time required to find a solution. 
¡¡*this work was partially supported by nsf grant iri1  by the electrical power research institute  epri   and by grants from toshiba of america  xerox northrop and rockwell. 
1 	constraint satisfaction 
our new algorithm  look-ahead value ordering  lvo   implements a heuristic that ranks the values of a variable based on information gathered by looking ahead  determining the compatibility of each value with the values of all future variables. although the heuristic does not always accurately predict which values will lead to solutions  it is frequently more accurate than an uninformed ordering of values. our experiments show that while the overhead of lvo usually outweighs its benefits on easy problems  the improvement on very large problems can be substantial. interestingly  lvo often improves the performance of backjumping on problems without solutions  as well. 
¡¡look-ahead value ordering does the same type of lookahead as does the forward checking algorithm . because forward checking rejects values that it determines will not lead to a solution  it can be viewed as doing a simple form of value ordering. in this regard lvo is more refined  because it also orders the values that may be part of a solution. 
¡¡in the following section we define formally constraint satisfaction problems and describe the look-ahead value ordering algorithm. section 1 describes the experiments we conducted. in section 1 we discuss the results of these experiments. we review some related approaches in section 1 and in section 1 summarize our results. 
1 	definitions and algorithms 
a constraint satisfaction problem  csp  is represented by a constraint network  consisting of a set of n variables  x  ...  xn; their respective value domains  d  ...  dn; and a set of constraints. a constraint is a subset of the cartesian product di1 x ... x di.  consisting of all tuples of values for a subset  xix ...  x i j   of the variables which are compatible with each other. a solution is an assignment of values to all the variables such that no constraint is violated; a problem with a solution is termed satisfiable or consistent. sometimes it is desired to find all solutions; in this paper  however  we focus on the task of finding one solution  or proving that no solution exists. a binary csp is one in which each of the constraints involves at most two variables. a constraint satisfaction problem can be represented by a constraint graph which has a node for each variable and an arc connecting each pair of variables that are contained in a constraint. 

1 	algorithms and heuristics 
we experimented with look-ahead value ordering by testing it in conjunction with an algorithm that combines backjumping  dynamic variable ordering  and the temporary pruning of future domains characteristic of forward checking. previous experiments have shown that this combination is extremely effective over a wide range of problems . lvo  or any other heuristic  is of practical interest only if it improves upon the performance of state of the art algorithms. 
backjumping 
backjumping  1; 1  is an improvement to backtracking which takes advantage of sparseness and structure in the constraint graph. both backtracking and backjumping consider each variable in order  instantiating the current variable  xcur  with a value from its domain dcur that does not violate any constraint between xcur and all previously instantiated variables. if xcur has no such non-conflicting value  then a dead-end occurs. the version of backjumping we use  based on prosser's conflictdirected backjumping   is very effective in choosing the best variable to jump back to. 
¡¡there are two basic ways to determine which values in di are consistent with all variables before xi. lookback methods consider each element in di and check to 
ensure that no constraints with earlier variables are violated. there are also several look-ahead approaches  the simplest being forward checking . when assigning a value to the current variable  forward checking removes values from the domains of future variables that conflict with that value. we will refer to the subset of di that has incompatible values removed as di. when xi  is reached  the only values remaining in di are those which are consistent with all previous variables  as instantiated. if a value x € di had been removed because it conflicted with the instantiation of an earlier variable xh  then x has to be restored when xh is assigned a new value  or is jumped over in backjumping. 
dynamic variable ordering 
in a dynamic variable ordering  dvo  scheme  1; 1; 1  the order of variables can be different in different branches of the search tree. our implementation of dvo uses information derived from a forward checking style look-ahead. at each step the variable with the smallest remaining domain size is selected. if di is empty for 
some uninstantiated variable xi  then xi is moved to be the next variable  and a dead-end occurs immediately. the technique for breaking ties is important  as there are often many variables with the same domain size. in our implementation we maintain a list of the variables sorted by degree in the original constraint graph  and in case of a tie  and for the first variable   choose the one highest on this list. this scheme gives substantially better performance than picking one of the tying variables at random. 
look-ahead value ordering 
backjumping and dynamic variable ordering can be combined into an algorithm we call bj+dvo . the bj+dvo algorithm does not specify how to choose a 
	frost and dechter 	1 

value. in our experiments reported in   and in this paper when we refer to  plain  bj+dvo  values are arbitrarily assigned a sequence number  and are selected according to this sequence. in this paper we explore the feasibility of using information gleaned during the look-ahead phase of bj+dvo to improve the ordering of values. the method we use to rank the values in order of decreasing likelihood of leading to a solution is as follows. the current variable is tentatively instantiated with each value in its domain d'. with each value in d' lvo looks ahead to determine the impact this value will have on the d' domains of uninstantiated variables. we discuss in section 1 four heuristic functions that use information from the look-ahead to rank the values in the current domain. the current variable is then instantiated with the highest ranking value. if the algorithm back jumps to a variable  the highest ranked remaining value in its domain is selected. if the variable is reinstantiated after earlier variables have changed  then the order of the values has to be recalculated. the lvo heuristic will not always make the right decision. however  a small improvement in an algorithm's ability to choose the right value can have a big impact on the work required to solve a problem. 
¡¡a high-level description of bj+dvo+lvo is given in fig. 1. to avoid repeating consistency checks  our implementation saves in tables the results of looking ahead in step 1 b . after a value is chosen  the d's and ps of future variables are copied from these tables instead of being recomputed in step 1 c . bj+dvo uses 1 n1k  space for the d1 sets  where k is the size of the largest domain: n levels in the search tree  d' is saved at each level so that it does not have to be recomputed after backjumping  x n future variables x k values for each future variable. our implementation of bj+dvo+lvo uses 1 n1  space. there is an additional factor of k because at each level in the search tree up to k values are explored by look-ahead value ordering. similarly  the space complexity for the p sets increases from 1 n1  in bj+dvo to 1 n1k  for bj+dvo+lvo. to solve a typical problem instance described in the next section  bj+dvo required 1 kilobytes of random access memory  and bj+dvo+lvo required 1 kilobytes. on most computers the additional space requirements of lvo will not be severe. 
1 experimental methods and results 
1 instance generator 
the experiments reported in this paper were run on random instances generated using a model that takes four parameters: n  k  t and c. the problem instances are binary csps with n variables  each having a domain of size k. the parameter t  tightness  specifies a fraction of the k1 value pairs in each constraint that are disallowed by the constraint. the value pairs to be disallowed by the constraint are selected randomly from a uniform distribution  but each constraint has the same fraction t of such incompatible pairs. t ranges from 1 to 1  with a low value of t  such as 1  termed a loose or relaxed constraint. the parameter c specifies the number of constraints out of the n *  n - l /1 possible. the specific constraints are chosen randomly from a uniform distribution. 
¡¡certain combinations of parameters generate problems of which about 1% are satisfiable; such problems are on average much more difficult than those which all have solutions  under-constrained  or which never have solutions  over-constrained   l; 1 . such a set of parameters is sometimes called a cross-over point. for a given value of n  k and t  we call the value of c which produces 1% solvable problems  cco . 
1 lvo heuristics 
the bj+dvo+lvo algorithm in fig. 1 does not specify exactly how information about conflicts with future variables should be used to prioritize the values of the current variable. we experimented with four heuristics that rank values by looking ahead. 
¡¡the first heuristic  called min-conflicts  mc   considers each value in d' of the current variable and associates with it the number of values in the d' domains of future variables with which it is not compatible. the current variable's values are then selected in increasing order of this count. 
¡¡the other three heuristics are inspired by the intuition that a subproblem is more likely to have a solution if it doesn't have variables with only one value. the maxdomain-size  md  heuristic therefore prefers the value that creates the largest minimum domain size in the future variables. for example  if after instantiating xcur 

1 	constraint satisfaction 

with value x1 the min €{ctir+li .n  d'i | is 1  and with xcur = x1 the min is 1  then x1 will be preferred. 


figure 1: results of several experiments on csps with various parameters  all near the 1% crossover point except as noted. in each experiment  1 instances were generated and solved with bj+dvo   dvo   and bj+dvo+lvo with the mc heuristic   lvo  . consistency checks and cpu time were recorded. for mean and median consistency check figures  the low order three digits are omitted. the  ratio  columns show the lvo statistic to the left divided by the corresponding dvo statistic. the  best  ratio column is the number of times the bj+dvo+lvo was better divided by the number of times bj+dvo was better. the small numbers in parentheses tell the size of the 1% confidence interval for consistency checks. for instance   1  1% '' means that the size of the 1% confidence interval is 1% of 1 or 1. since the interval is centered around 1  we are 1% confident that the true mean 

is between 1 and 1. 
¡¡since several values in the domain of the current variable d'cur may create future d's of the same size  the md heuristic frequently leads to ties. a refined version of md is weighted-max-domain-size  wmd . like md  wmd prefers values that leave larger future domains  but it break ties based on the number of future variables that have a given future domain size. continuing the example from the previous paragraph  if x1 leaves 1 variables with domain size 1  and the rest with domain sizes larger than 1   and x1 leaves 1 variables with domain size 1  then x1 will be preferred over x1. 
¡¡our fourth heuristic  called point-domain-size  pds   gives each value in d'cur a point value: 1 points for each future domain of size 1; 1 points for each future domain of size 1; 1 points for each future domain of size 1  if k   1 ; and 1 point for each future domain of size 1  if 
k   1 . the value with the smallest sum of points is chosen first. 
¡¡fig. 1 shows the results of experiments comparing the four lvo heuristics. in terms of mean consistency checks  lvo usually improves bj+dvo no matter which heuristic is chosen. since the mc heuristic was clearly best  we selected it for further experimentation. in the rest of the paper  reference to lvo implies the mc heuristic. 
1 	experimental results 
the overall conclusion we draw from our experiments comparing bj-fdvo with bj+dvo+lvo is that on sufficiently difficult problems lvo almost always produces substantial improvement; on medium problems lvo usually helps but frequently hurts; and on easy problems the overhead of lvo is almost always worse than the benefit. very roughly   sufficiently difficult  is over 1 1 consistency checks and  easy  is under 1 consistency checks. 
¡¡we experimented further with lvo by selecting several sets of parameters and with each set generating 
1 instances that were solved with both bj+dvo and bj+dvo+lvo. we used two approaches for selecting combinations of parameters which had large values of n and k  and yet did not generate problems that were too computationally expensive. the first strategy was to use very tight constraints and very sparse constraint graphs. for instance  problems at n=1 and k=1 would be extremely time consuming to solve  except that we used a small number  c=1  of extremely tight constraints  t=l 1 . another method for generating easier problems with large n and k is to select parameters that are not exactly at the cross-over point. we used this approach for the experiment with n=1  
k=1  t=l/1 and c=1  which is 1% of the estimated ccoof1. 
¡¡the results of these experiments are summarized in fig. 1. we present the data in several ways: the table show the mean  the median  and the ratio of how many times each algorithm was better than the other. the  ratio  columns under  mean  and  median  in 
fig. 1 provide an indication of the relative performance of the two algorithms. mean ratios and median rafr1st and dechter 1 


figure 1: the instances in one experiment were divided into 1 groups  based on the number of consistency checks made by bj+dvo. each point is the mean of 1 instances in one group. the dotted line  showing the percentage of times bj+dvo was better than bj+dvo+lvo  when measuring consistency checks   is related to the right-hand scale. 
tios less than one indicate that bj+dvo+lvo is better than bj+dvo  required fewer consistency checks or cpu time . in the  best ratio  column a larger number indicates superior performance by bj+dvo+lvo  as this figure is the number of times bj+dvo+lvo was better then bj+dvo  divided by the number of times bj+dvo bested bj+dvo+lvo  as measured by consistency checks or cpu time . 
¡¡for many uses  the mean is the most relevant statistic  as it takes into account the impact of the occasional extremely difficult problem. to convey an estimate of the accuracy of our sample of 1 instances  we provide  for the consistency check measure  the size of the 1% confidence interval  expressed as a percentage of the sample mean. the 1% confidence interval around the true population mean u is computed as 

where x is the sample mean  o is the population standard deviation  t is the number of trials  and 1 is the factor associated with the 1% confidence interval. since we don't actually know the standard deviation of the entire population  we have to estimate it by using the sample standard deviation; this is acceptable as long as t   1  but still introduces another source of possible inaccuracy. 
¡¡the medians in fig. 1 are much lower than the means  because in each sample there were a small number of extremely difficult problems  and a few very difficult ones. in our experiments with n=1  k=1  t=1 and 
c=1 half the cpu time was spent solving the hardest 1 of the 1 problems. fig. 1 shows the skew in the distribution  and how lvo affects problems of different difficulties. for this figure  the 1 instances in one experiment were divided into ten groups of 1  based on the number of consistency checks required by bj+dvo. 
1 	constraint satisfaction 

figure 1: each point    = has solution  o = no solution  represents one instance out of 1. points below the diagonal line required fewer consistency checks with lvo. 
the easiest 1 were put in the first group  the next easiest 1 in the second group  and so on. lvo is harmful for the first several groups  and then produces increasingly larger benefits as the problems become more difficult. the scatter chart in fig. 1 also indicates the distribution of the data.  due to space constraints  some of our figures show data drawn from only one parameters  such as n=1  k=1  t=1  c=1. the charts of data from experiments with other parameters are quite similar.  
¡¡in general the statistics for cpu time are slightly less favorable for lvo than are the statistics for consistency checks  reflecting the fact that  in our implementation  there is approximately a 1%-1% performance penalty in cpu time for lvo. this is caused by the need to store and copy the large tables that hold the results of looking ahead on different values of a variable  the caching referred to in step 1 c  of fig. 1 . many problem instances required slightly fewer consistency checks with lvo but slightly more cpu time  resulting in the often substantially different  consistency checks best ratio  and  cpu seconds best ratio  numbers in fig. 1. 
¡¡the graphs in fig. 1 show that the impact of lvo increases as the number of variables increase. moreover  when variables have small domain sizes  a larger number of variables is required for lvo to have a beneficial impact. for instance  at n=1 and k=1  lvo improves bj+dvo substantially  while with the small domain size k=1  the impact of lvo does not appear until n is larger than 1. 
¡¡the efficacy of lvo also depends on how near the parameters are to the 1% solvable crossover point. as the data in fig. 1 indicate  lvo is detrimental on very easy 


	frost and dechter 	1 

unattacked cells  is the same as our max-conflicts heuristic  and his  row with the least number of unattacked cells  heuristic is the same as max-domain-size. 
¡¡dechter and pearl  developed an advised backtrack algorithm which estimates the number of solutions in the subproblem created by instantiating each value. the estimate is based on a tree-like relaxation of the remainder of the problem. for each value  the number of solutions is counted  and the count is used to rank the values. advised backtrack was the first implementation of the general idea that heuristics can be generated from a relaxed version of the problem instance. 
¡¡sadeh and fox  ll  also use a tree-like relaxation of the remaining problem  in the context of job-shop scheduling problems. their value ordering heuristic considers as well the impact of capacity constraints and demand on scarce resources. 
1 	conclusions and future work 
we have introduced look-ahead value ordering  an algorithm for ordering the values in a constraint satisfaction problem. our experiments show that for large and hard problems  lvo can improve the already very good bj+ dvcvalgorithm by over a factor of five. 
¡¡one drawback of lvo is that it is somewhat complex to implement  as it uses a set of tables to cache the results of values that have been examined  during the ranking process  but not yet instantiated. manipulating these tables incurs a small cpu overhead. another disadvantage of lvo is that on easy solvable problems  where there are many solutions and hence many acceptable value choices  it is usually detrimental. lvo needlessly examines every value of each variable along the almost backtrack-free search for a solution. 
¡¡lvo is almost always beneficial on difficult instances that require over 1 1 consistency checks. unexpectedly  it even helps on problems without solutions when used in conjunction with backjumping. we have tested lvo using a forward checking level of look-ahead. we plan to explore the possibility that a more computationally expensive scheme  such as partial look-ahead or full look-ahead  or directional arc consistency   will pay off in the increased accuracy of the value ordering. another research direction is to reduce the overhead of lvo on easy problems. this might be achieved by only employing value ordering in the earlier levels of the search  or by having value ordering automatically  turn off  when it notices that current values are in conflict with relatively few future values  indicating an underconstrained problem. a simple way to eliminate the overhead of lvo on very easy problems would be to always run a non-lvo algorithm first; if that algorithm has not completed by  say  1 consistency checks  it is cancelled and problem solving is restarted with an lvo version. at the price of 1 extra consistency checks on some difficult problems  the costs of lvo on the easy majority of problems is avoided. 
