 
to perform rational decision-making  autonomous agents need considerable computational resources. in multi-agent settings  when other agents are present in the environment  these demands are even more severe. we investigate ways in which the agent's knowledge and the results of deliberative decision-making can be compiled to reduce the complexity of decision-making procedures and to save time in urgent situations. we use machine learning algorithms to compile decision-theoretic deliberations into condition-action rules on how to coordinate in a multi-agent environment. using different learning algorithms  we endow a resource-bounded agent with a tapestry of decision making tools  ranging from purely reactive to fully deliberative ones. the agent can then select a method depending on the time constraints of the particular situation. we also propose combining the decision-making tools  so that  for example  more reactive methods serve as a pre-processing stage to the more accurate but slower deliberative decision-making ones. we validate our framework with experimental results in simulated coordinated defense. the experiments show that compiling the results of decision-making saves deliberation time while offering good performance in our multi-agent domain. 
1 	introduction 
it is desirable that an autonomous agent  operating under uncertainty in complex environments  be able to make optimal decisions about which actions to execute. rational decision-making under such circumstances using  for instance  the paradigm of expected utility maximization  is costly  horvitz  1; russell and wefald  1; russell and subramanian  1; zilberstein and russell  1 . in our work  we consider additional complexities presented by multi-agent environments. in these settings  an agent has to make 
1 	distributed al 
decisions as to the rational course of action considering not only the possibly complex and not fully known state of its environment  but also considering the beliefs  goals  intentions and actions of the other agents. clearly  these demands may lead to its failure to decide an action within the time constraint. 
   to cope with time constraints imposed by various decision-making situations in complex and uncertain rnulti-agent settings  we endow an agent with a tapestry of decision-making procedures  from strictly reactive to purely deliberative. the reactive procedures are constructed by compiling the deliberative decision-theoretic reasoning into condition-action rules. the compilation process exploits the regularities of the decision-theoretic reasoning and avoids costly deliberations in urgent situations. the rules are obtained from machine learning algorithms  which  as inputs  use the results of fullblown decision-theoretic computations performed offline. each of the compiled methods is assigned a performance measure that compares it to the full-blown decision-theoretic benchmark. the various compilations available  and their combinations with more deliberative methods  constitute a spectrum of approaches to making decisions under the constraints of available computational  and cognitive  resources  and under time pressure. 
   given the various decision-making methods at its disposal  an agent should consider a number of factors to choose the appropriate decision-making mechanism for the situation at hand. the key factors include the quality of the decision provided by a method  the method's running time  and the urgency of the situation at hand. intuitively  wrhen a situation is not urgent  the agent can afford the luxury of full-blown decision-theoretic reasoning since it results in highest quality of the choice made. if the situation is very urgent  the agent should save as much time as possible by using a crude but fast reactive tool. if the situation is somewhat urgent  the agent should use methods that are somewhat sophisticated although not necessarily optimal. 
　interestingly  the spectrum between the purely reactive and fully deliberative decision-making tools can be spanned by combining these two varieties of methods. for example  the agent can use fast reactive rules as a 

pre-processing stage to narrow down the set of viable alternatives. these alternatives can then be passed on to a deliberative decision-making method that uses all of the agent's detailed knowledge to compute the expected utility of these few courses of action. 
　in this paper  we develop a suite of decision-making procedures for agents operating in multi-agent environments  and we measure1 their performance and running time. we use a particular multi-agent domain in which automated agents have to decide how to coordinate their attempts to intercept multiple incoming threats  as in anti-air defense   but we believe that lessons learned in this domain generalize to other multi-agent  domains. 
1 	background and related work 
our prior work on deliberative decision-theoretic method includes the recursive modeling method  r.mm   gmytrasiewiez  1; gmytrasiewicz at al.  1; noh and gmytrasiewicz  1; 1 . we have implemented a full-blown version of r m m which allows an agent to compute its best action given what is known about the other agents and about their states of knowledge and capabilities. in the task of coordinating agents in a simulated 

figure 1: the performance of rmm  rmm-human  and human teams in anti-air defense  without and with communication  respectively. 
agents' alternative actions  it is not surprising that its complexity is  in the worst case  exponential in the number of agents present. as the complexity of the multiagent situation increases  as in figure 1  the running time of rmm grows to over one hour  our current implementation is in lisp running on a p1 machine . it is clear that  the full-blown rmm needs to be supplemented by other  more reactive  methods for more1 complex and time-critical scenarios. 
figure 1: an example anti-air defense scenario. better than the others. however  since the rmm decision procedure considers all of the combinations of the 

anti-air defense domain  such as in figure 1  the performance of rmm agents was comparable to or better than the human performance. figure 1 presents these results in terms of the average total damage suffered by each of the coordinating defense teams. we show the performance of three different teams: rmm-rmm  rmmhuman  and human-human team. we experimented with all the teams in cases when communication was  and was not  available. when the communication was available  the performance achieved by three teams was improved  with the all-rmm team performing slightly 

figure 1: a complex anti-air defense scenario. 
our present work builds on a body of related re-
	noh and gmytrasiewicz 	1 

search. fox et al.  fox and krause  1  provides a theoretical framework for symbolic reasoning as reactive decision procedure. other rigorous efforts to make decision-theoretic systems computationally tractable include work on the use of metareasoning procedures to control inference  horvitz  1; horvitz et al.  1   and anytime algorithms  dean and boddy  1; russell and subramanian  1; zilberstein and russell  1 . in yet another approach bratman  bratman et al.  1  describes an agent's architecture that includes 
both means-end reasoning and decision-theoretic reasoning. for a resource-bounded agent  the agent's beliefs  desires  and intentions  bdi  involve the formation  revision  and execution of plans to constrain the deliberation process. rao et al.  rao and georgeff  1  explore the applicability of reactive and deliberative behavior provided by the bdi architecture  and use it for air-traffic management system  oasis . ephrati and collaborators  ephrati et al.  1j apply a filtering strategy to the multi-agent tile world system. the filtering is accomplished using rules provided by the system designer  and it only guides the agent's role allocation. our conditionaction rules  on the other hand  represent learning over the results of agent's rational decisions in sample scenarios obtained from a deliberative method. our filtering strategy is to accumulate agents1 knowledge and effectively use it to limit deliberation in urgent situations. 
　in the following sections of this paper we propose a compilation process that explores the regularities of a deliberative decision making  and show how an autonomous agent can use the compiled information  given performance metrics and time constraints. then  we validate our framework empirically  and discuss the experimental results. in conclusion  we summarize our results and further research issues. 
1 	formalism of compilation and filtering 
to reduce the complexity and time needed for decision making in time constrained situation  we compile the results of deliberative decision-making into a set of reactive condition-action rules with numerous machine learning algorithms. an autonomous agent can use the compiled knowledge  russell  1; zilberstein  1  and either eliminate the deliberative decision-making all together  or constrain the number of alternative actions considered by excluding the ones that are likely to be suboptimal. 
　　we propose an adaptive and deliberative agent  ada  architecture  as consisting of compiled and deliberative decision procedures that allow the agent's bounded rationality to emerge from their combined usage. let n be the set of agents  be the set of actions of agent i  and be the set of world states that the  agent can discriminate among. for each action we define condition  to be the abstraction of the world state that includes only the parameters relevant for this action. for example  if the action is to pick up a block  shoot at a given threat   then the corresponding 
1 	distributed al 
abstraction specifies the location and other parameters of the block  threat . finally  let  be the set of compilation methods  learning algorithms  that the agent  employs. 
　　given a learning method  a compiled decisionmaking procedure of an adaptive and deliberative agent implements a function  coriditum  
 representing that the action 	is  or is not  
recommended in the state  thus  various machine learning algorithms compile decision-theoretic models into different functions  as we mentioned  we generated the training examples for these learning algorithms from deliberative reasoning performed by rmm. 
　　to allow for further flexibility in the ada agents  we allow the procedures to be combined depending on circumstances at hand. clearly  when the agents have enough time  they should try to make a deliberatively rational decision that maximizes their expected utility. in a time-critical situation  howrever  agent's decisionmaking is bounded by the available computation time. for an adaptive and deliberative agent  therefore  we use the set of compiled rules to remove from consideration the likely unprofitable actions  and to reduce the deliberation cost. this strategy is represented by the agent 
filtering criterion 	where 	in-
tuitively  the value of is the set of plausible actions the agent should consider in situation the filtering criterion results from applying the rules in the function to the current state to obtain the plausible alterna-
tives. for example  if  and 
　given the set of plausible actions  our agent maximizes the expected utility among them: 
		 i  
　we now apply the above formalism to agents making coordinated decisions in the anti-air defense domain. 
1 deliberation about action in anti-air defense domain 
our specific domain  the anti-air domain  consists of a number of attacking targets  labelled a and b in figure 1  and a number of defending units  labelled 1 and 1 the mission of the defense units is to attempt to intercept attacking targets so as to minimize damages to the defended ground area. let us note that this situation makes coordination necessary. the defense batteries do not want to miscoordinate and attempt to intercept the same threat  both due to the wasted ammunition and due to the increase in likelihood that the remaining threat will reach its destination and cause damage proportional to its warhead size. 
　given these factors  the expected benefit of shooting at a threat can be quantified as a product of the size of 
     1  in the figure  the left top corner of the screen is  1   x is pointing right  and y is pointing down. 


figure 1: an example of a simple anti-air defense scenario. 
the threat and the interception probability. the interception probability1  p hij   is dependent on the angle djj between the target  direction of motion and the battery  line of sight  the distance  between the battery and the target  and the speed of target   see  for example   macfadzean  1    as follows: 
		 1  
where u is an interceptor-specific constant  assumed here to be 1 . 
　for example  in figure 1  the combined benefit of battery l's shooting at the threat a and battery1 s shooting at the threat d amounts to 1 = 1 x 1 + 1 x 1 . this value is entered in the payoff matrix  such as one on top in figure 1. in this payoff matrix the rows represent batteryl's alternative actions of shooting at .1  b  and not shooting at all  1   respectively  and the columns represent the alternative actions of battery1. 

figure 1: rmm's recursive model structure for battery l's decision making. 
　the actual behavior of battery1  however  depends on a number of factors that battery 1 may be uncertain 
1
　　in the cooperative weapon-target allocation problem  air defense units calculate probability of kill to evaluate target  survivability depending upon situation-specific characteristics. here  the interception probability is a probability of kill  and is based on our heuristic factors. 
about. for example  if battery1 has been hit and incapacitated  it will not be able to launch any interceptors. if it is not incapacitated then its own decision-making situation can be represented as another payoff matrix. further  battery1 may have run out of long-range or short-range interceptors. if battery1 has only long-range interceptors  it would be unable to attack target b  and can only attempt to shoot down target a. if battery1 has only short-range interceptors  it can only attempt to shoot at target b. these four models  of battery1 being fully operational and having long- and short-range interceptors  operational with only long-range interceptors  having only short-range interceptors  and incapacitated  are depicted as the second level models in figure 1  with their associated probabilities  in this example case 1  1  1  and 1  respectively. 
　the recursive modeling method uses dynamic programming  gmytrasiewicz  1g; noh and gmytrasiewiez  1  to process model structures as in figure 1 and determine the rational choice of coordinated action. in this case  battery 1 computes that if battery1 is fully operational then the probability distribution over battery1 s actions a  b  and 1 is  1 1 1 . if battery 1 has only long-range interceptors it will choose to shoot at target .1  i.e.  the probability distribution over battery1 s actions becomes  1 1 . if battery 1 has only short-range interceptors it  will intercept target b. these probability distributions are combined with the model of battery1 being incapacitated: 

the resulting distribution is batteryl's overall expectation of battery1's actions  given all of the remaining uncertainties. propagating these results to level 1  the combined probability distribution describing battery1's actions is used to compute the expected utilities of battery l's alternative actions. we have: 

thus  given the uncertainties about battery1  batteryl's rational coordinated choice is to intercept target a. 
1 compilation of deliberative decisions in air-defense 
to construct compiled rules for our agents in the coordinated defense domain  we used four machine learning algorithms: hayes classifier  c1  quinlan  1   cn1  clark and niblett  1   and foil  cameron-jones and quinlan  1 .1 the input data for the learning 
1
　　we implemented the anti-air defense domain with common lisp on top of the mice simulator  durfee and montgomery  1  on a linux machine  and also implemented a simple bayesian classifier described in  clark and niblett  1 . 
	noh and gmytrasiewicz 	1 

algorithms were obtained from the recursive modeling method  rmm   as described above. for the bayesian classifier  the results are represented as rules specifying the probability of occurrence of each attribute value given a class  clark and niblett  1   in our case  yes'1 
 also called select. target below  and  no  {don't select target below . c1 represents its output as a decisiontree  and the output of cn1 is an ordered set of if-then rules. the trained results of foil are the relations of attributes as function-free horn clauses. we now describe the agent's compiled knowledge by using the above learning algorithms  and compare their decision capabilities in the anti-air defense environment. 
1 	learned condition-action rules 
in our experiments we considered agents that vary in their capacities  and they have limited knowledge about other agents. they decide on which behavior to execute based upon their sensory input and the limited information they have about the other agents. 
   the attributes of situations that the agents can sense in an anti-air defense environment are summarized in table 1. they include the size  speed and angle of the attacking targets  the agent's own intercepting capabilities  i.e.  its possessing both long- and short-range interceptors  only long-range interceptors  only short-range interceptors  and its being incapacitated and unable to shoot   and the probabilities associated with the capabilities of the other defense agent1  the other agent's possessing both long- and short-range interceptors  only long-range interceptors  only short-range interceptors  and its being incapacitated and unable to shoot . during the experiments the values of the attributes were randomly generated within the ranges of values. 
table 1: condition  describing the relevant attributes of targets in the anti-air defense. 
attribute type value target size numeric 1 - 1 target speed nominal slow  mid  fast target angle numeric 1 - 1 distance numeric 1 - 1 capacity nominal both  long  short  incap. p of both amino numeric 1 - 1 p of only long numeric 1 - 1 p of only short numeric 1 - 1 p of incap. numeric 1- 1    based on the attributes in table 1  the targets the defense agent considers for interception can be classified into two classes:  as an example  a decision tree obtained using c1 for these attributes is depicted in figure 1. 
　　table 1 describes parameters for two agents and the generalization to a set of agents requires an additional set of probability parameters. 
1 	distributed a 

figure 1: the decision tree obtained by c1. 
1 	experiments and performance results 
to evaluate the quality of various rule sets generated by different learning algorithms the performance obtained was expressed in terms of the total expected damage to friendly forces after launching interceptors. the total expected damage is defined as a sum of the residual warhead sizes of the attacking targets. thus  if a target was aimed for interception  then it contributed   1  i nterception  probabiiity   warhead size  to the total damage. if a target was not intercepted  it contributed all of its warhead size value to the damage. 
   to find a meaningful size of the training set which could guarantee the soundness of the learning hypothesis  we generated several sets of training examples. as the number of the examples increased  the resulting performances improved drastically up to a certain point  after which performance did not improve. in anti-air defense scenarios that included two batteries and two targets the sufficient number of training instances we found was 1 examples. by using the compiled conditionaction rules obtained by different learning methods  we tested the performances of the methods on a new set of 1 cases. the results of performance  damage's  and runtime  sec.  are described in table 1. 
table 1: performance and runtime of algorithms in the two units and two targets setting. 
methods perf  damages  time deliberative 
rmm 1 ＼ 1 1 ＼1 reactive 
daves 
c1 
foil 
' 	cn1 1 ＼ 1 1 ＼ 1 1 ＼ 1 
1 ＼ 1 1 ＼ 1 
1 ＼ 1 
1 ＼ 1 
1 ＼ 1 anova 1 1 　we analyzed the performance results in table 1 using the standard analysis of variance  anova  method. 
since the computed value of / - 1 in anova exceeds  we know that the five teams wrere not all 
equally effective at the 1 level of significance  i.e.  the differences in their performance were not due to chance 

with probability of 1. as we expected  the pure deliberative procedure r m m showed the best performance. the bayesian classifier computes the probabilities for two classes  and enables the defense agents to select the target which has the highest probability of being in the class of targets to be intercepted. the bayesian classifier showed reasonable performance and runtime. when the pure reactive procedures  c1  cn1  and foil  were used  they could not uniquely decide the target in some cases  if the sensory input values of attributes were similar. if the defense agents still were ambiguous in target interception after applying condition-action rules  they randomly selected the target. the agent's performance by using c1 was better than those of foil and cn1 while it took less runtime. 
　as expected  r m m required the longest runtime  and c1 needed the shortest runtime among the five decision procedures. a nova revealed that the differences in running time were not due to chance with probability 1 again. when making a decision  the defense agents compared the current sensory input with one of the condition-action rules to classify a new example. the learning results obtained from cn1 and foil were represented by the sequential comparisons while c1 used the decision tree. due to this difference cn1 and foil took longer to run than c1. the advantage of decision tree was in that it reduced the number of matches by disregarding nodes in the tree unrelated to the input. 
　to measure the efficiency of an adaptive and deliberative agent architecture which uses the reactive rules to filter the alternatives considered by a deliberative procedure  we experimented with the scaled-up setting depicted in figure 1. in this setting  there were six batteries and 1 targets. the r m m agents we implemented for these experiments modeled only their closest neighbors for coordinated decision-making to reduce their deliberation time. since  as shown in tabic1  the bayes classifier and c1 performed best among the reactive rule sets  we used these two as filtering criteria for ada agents. as training data for the two learning algorithms  we generated 1 tuples  which consist of 1 for don't select target class  and 1 for select target class. the results of performance  damages  and runtime  sec.  in the scaled-up setting are described in table 1. 
table 1: performance and runtime of algorithms in the scaled-up setting. 

　table 1 presents the average total expected damage after 1 trials. we focus on the performances of three different agents: r m m   bayes-rmm  bayesian rules used to filter alternatives for deliberations using rmm   and c1-rmm. the performance of the rmm agent was  again  the best. the performances of bayes-rmm and c1-rmm agent were 1% and 1% of the rmm agent's performance  respectively. further  the runtimes of adaptive and deliberative agents were drastically reduced. the size of the set of filtered plausible alternative actions were 1  and 1 out of 1 targets for bayesian classifier and c1  respectively. the total runtime for bayes-rmm agent was 1 on the average  and the c1-rmm agent needs 1 seconds for its target selection in the simulated anti-air defense environment. this result indicates that the combination of reactive and deliberative decision-making procedures saves the agent's deliberation time while offering good performance  compared with pure deliberative procedure. also  among the reactive procedures  the performance of bayesian classifier was better than that of c1 since it included the additional probabilistic information. since the defense batteries controlled by c1 alone randomly selected the targets after filtering out unprofitable targets  its performance was the worst. 
1 	conclusions 
we investigated a set of condition-action rule sets achieved by compiling decision-theoretic reasoning implemented in r m m method using various learning algorithms. we found that the compiled rules reduce the complexity and running time in complex multi-agent scenarios. this approach enables an adaptive and deliberative agent to reach a decision within a reasonable time period. 
　we experimented with the anti-air defense domain to assess the quality of the flexible decision-making procedure. anti-air defense is certainly a real-time task  and any overhead or loss of timely response can result in additional damages. the combination of reactive and deliberative decision-making methods avoided catastrophic failure  and provided good-quality decisions in the timeconstrained anti-air defense. 
　in our future research  we will consider how an autonomous agent can decide on its coordinated action in an any-time fashion  llorvitz  1; horvitz et al.  1; dean and boddv  1 . our framework will provide the rational action under uncertain deadline by calculating the computational gain  representing the tradeoff between the costs and the benefits of computation. 
acknowledgments 
we are grateful to dr. lawrence b. holder and the anonymous referees for providing useful comments relating to this work. this research has been sponsored by the office of naval research artificial intelligence program under contract n1-1 and by the national science foundation career award iri-1. 
	n1h and gmytrasiewicz 	1 

