 
bayesian network models are widely used for discriminative prediction tasks such as classification. usually their parameters are determined using 'unsupervised' methods such as maximization of the 
joint likelihood. the reason is often that it is unclear how to find the parameters maximizing the conditional  supervised  likelihood. we show how the discriminative learning problem can be solved efficiently for a large class of bayesian network models  including the naive bayes  nb  and treeaugmented naive bayes  tan  models. we do this by showing that under a certain general condition on the network structure  the discriminative learning problem is exactly equivalent to logistic regression with unconstrained convex parameter spaces. hitherto this was known only for naive bayes models. since logistic regression models have a concave log-likelihood surface  the global maximum can be easily found by local optimization methods. 
1 introduction 
in recent years it has been recognized that for discriminative prediction tasks such as classification  we should use a 'supervised* learning algorithm such as conditional likelihood maximization  friedman et al.  1; ng and jordan  1; kontkanen et al.  1; greiner and zhou  1 . nevertheless  for bayesian network models the parameters are customarily determined using ordinary methods such as maximization of the joint  unsupervised  likelihood. one of the main reasons for this discrepancy is the difficulty in finding the global maximum of the conditional likelihood. in this paper  we show that this problem can be solved  as long as the underlying bayesian network meets a particular additional condition  which is satisfied for many existing bayesiannetwork based models including naive bayes  nb   tan  tree-augmented nb  and 'diagnostic' models  kontkanen et al.  1 . 
　we consider domains of discrete-valued random variables. we find the maximum conditional likelihood parameters by logarithmic reparametrization. in this way  each conditional bayesian network model is mapped to a logistic regression model  for which the likelihood surface is known to be concave. however  in some cases the parameters of this logistic regression model are not allowed to vary freely. in other words  the bayesian network model corresponds to a subset of a logistic regression model rather than the full model. 
　our main result  thm. 1 below  provides a general condition on the network structure under which  as we prove  the bayesian network model is mapped to a full logistic regression model with freely varying parameters. therefore  in the new parametrization the conditional log-likelihood becomes a concave function of the parameters that under our condition are allowed to vary freely over the convex set rk. now we can find the global maximum in the conditional likelihood surface by simple local optimization techniques such as hill climbing. 
　the result still leaves open the possibility that there are no network structures for which the conditional likelihood surface has local  non-global maxima. this would make our condition superfluous. our second result  thm. 1 below  shows that this is not the case: there are very simple network structures that do not satisfy our condition  and for which the conditional likelihood can exhibit local  non-global maxima. 
　viewing bayesian network  bn  models as subsets of logistic regression models is not new; it was done earlier in papers such as  heckerman and meek  1a; ng and jordan  1; greiner and zhou  1 . also  the concavity of the log-likelihood surface for logistic regression is known. our main contribution is to supply the condition under which 
bayesian network models correspond to logistic regression with completely freely varying parameters. only then can we guarantee that there are no local maxima in the likelihood surface. as a direct consequence of our result  we show for the first time that the supervised likelihood of  for instance  the tree-augmented naive bayes  tan  model has no local maxima. 
　this paper is organized as follows. in section 1 we introduce bayesian networks and an alternative  so-called lparametrization. in section 1 we show that this allows us to consider bayesian network models as logistic regression models. based on earlier results in logistic regression  we conclude that in the l-parametrization the supervised loglikelihood is a concave function. in section 1 we present our main results  giving conditions under which the two parametrizations correspond to exactly the same conditional 

learning 	1 


1 	learning 

more precisely  let and let 
be real-valued random variables. the multiple logistic regression model with dependent variable xq and covariates y  ...  yk is defined as the set of conditional distributions 
 1  
. now  for 
i 
configurations of xt  let 
		 1  
the indicator random variables  thus obtained can be lexicographically ordered and renamed l ... fc  which shows that each l-model corresponding to a bayesian network structure b as in  1  is formally identical to the logistic model  1  with dependent variable a'o and covariates given by  1 . so  for all network structures b  the corresponding lmodel ml is the standard multiple logistic model  where the input variables for the logistic model are transformations of the input variables to the l-model  the transformation being determined by the network structure b. 
   it turns out that the conditional log-likelihood in the lparametrization is a concave function of the parameters: 
theorem 1. the parameter set is convex  and the conditional log-likelihood is concave  though not strictly concave. 
proof. the first part is obvious since each parameter can take 
values  concavity  s a direct consequence of the fact that multiple logistic regression models are exponential families; see  e.g.   mclachlan  1  p.1. for an example showing that the conditional log-likelihood is not strictly concave  see  wettig et at  1. 
remark. non-strictness of the proven concavity may pose a technical problem in optimization. this can be avoided by assigning a strictly concave prior on the model parameters and then maximizing the 'conditional posterior'  grunwald et  /.  1; wettig et a/.  1  instead of the likelihood. one may also prune the model of weakly supported parameters and/or add constraints to arrive at a strictly concave conditional likelihood surface. our experiments  wettig et al  1  suggest that for small data samples this should be done in any case  in order to avoid over-fitting; see also section 1. any constraint added should of course leave the parameter space a convex set  e.g. a subspace of the full 1 l . 
corollary 1. there are no local  non-global  maxima in the likelihood surface of an l-model. 
　the conditions under which a global maximum exists are discussed in  e.g.   mclachlan  1  and references therein. a possible solution in cases where no maximum exists is to introduce a strictly concave prior as discusssed above. 
　the global conditional maximum likelihood parameters obtained from training data can be used for prediction of future data. in addition  as discussed in  heckerman and meek  1a   they can be used to perform model selection among several competing model structures using  e.g.  the bic or  approximate  m d l criteria. in  heckerman and meek  1a  it is stated that for general conditional bayesian network models mb   although it may be difficult to determine a global maximum  gradient-based methods  ...  can be used to locate local maxima . theorem 1 shows that if the network structure b is such that the two models are equivalent    we can find even the global maximum of the conditional likelihood by reparametrizing mb to the lmodel  and using some local optimization method. thus  the question under which condition .  becomes crucial. it is this question we address in the next section. 
remark. because the log-transformation is continuous  it follows  with some calculus  that  if .   then all maxima of the  concave  conditional likelihood in the lparameterization are global  and connected  maxima also in the original parametrization. nevertheless  the likelihood surface as a function of  has some unpleasant properties  see  wettig et al.  1  : it is not concave in general and  worse  it can have 'wrinkles': by these we mean convex subsets such that  under the constraint that 
             the likelihood surface does exhibit local  nonglobal maxima. this suggests that it is computationally preferrable to optimize over  rather than . empirical evidence for this is reported in  greiner and zhou  1 . 
1 	main result 
by setting   it follows that each distribution in mb is also in ml  thm. 1 . this suggests that  by doing the reverse transformation 
 1  
we could also show that distributions in 	are also in 
	. however  	contains distributions that violate the 
'sum-up-to-one constraint'  i.e.  for some we have for some i and pa i . 
then the corresponding is not in . but  since the lparameterization is redundant  many different index the same conditional distribution it may still be the case that the distribution indexed by 
is in   -. . indeed  it turns out that for some network structures b  the corresponding m l is such that each distribution in   can be expressed by a parameter vector   such that and/mi. in 
that case  by  1   we do have mb - ml. our main result is that this is the case if b satisfies the following condition: 
condition 1. there exists  such that ~ 
remark. 	condition 1 implies that the class  must be a 
'moral node'  i.e.  it cannot have a common child with a node it is not directly connected with. but condition 1 demands more than that; see figures 1 and 1. 

learning 	1 


figure 1: a simple bayesian network  the class variable is denoted by xo  satisfying condition 1  left ; and a network that does not satisfy the condition  right . 

figure 1: a tree-augmented naive bayes  tan  model satisfying condition 1  left ; and a network that is not tan  right . even though in both cases the class variable x1 is a moral node  the network on the right does not satisfy condition 1. 
example 1. consider the bayesian networks depicted in figure 1. the leftmost network  b1  satisfies condition 1  the rightmost network  b1  does not. theorem shows that the conditional likelihood surface of can have local maxima  implying that in this case 
　examples of network structures that satisfy condition 1 are the naive bayes  nb  and the tree-augmented naive bayes  tan  models  friedman et a/.  1 . the latter is a generalization of the former in which the children of the class variable are allowed to form tree-structures; see figure 1. 
proposition 1. condition j is satisfied by the naive bayes and the tree-augmented naive bayes structures. 
proof. for naive bayes  we have 
 for tan models  all children of the class variable have either one or two parents. for children with only one parent  the class variable  we can use the same argument as in the nb case. for any child xj with two parents  let xi 
be the parent that is not the class variable. because xi is also a child of the class variable  we have 	i 
　condition 1 is also automatically satisfied if xo only has incoming arcs1  'diagnostic' models  see  kontkanen et ai  1   . for bayesian network structures b for which the condition does not hold  we can always add some arcs to arrive at a structure b' for which the condition does hold  for instance  add an arc from  in the rightmost network in figure 1 . therefore  mb is always a subset of a larger model  for which the condition holds. we are now ready to present our main result  for proof see appendix a : 
theorem 1. if b satisfies condition 1  then 
　together with corollary 1  theorem 1 shows that condition 1 suffices to ensure that the conditional likelihood surface of has no local  non-global  maxima. proposition 1 
   'it is easy to see that in that case the maximum conditional likelihood parameters may even be determined analytically. 
1 
now implies that  for example  the conditional likelihood surface of tan models has no local maxima. therefore  a global maximum can be found by local optimization techniques. 
　but what about the case in which condition 1 does not hold  our second result  theorem 1  proven in appendix a  says that in this case  there can be local maxima: 
theorem 1.  be the network 
structure depicted in figure j  right . there exist data samples such that the conditional likelihood has local  non-global maxima over  
　the theorem implies that  thus  condition 1 is not superfluous. we may now ask whether our condition is necessary for having . that is  whether 
mb for all network structures that violate the condition. we plan to address this intriguing open question in future work. 
1 	concluding remarks 
we showed that one can effectively find the parameters maximizing the conditional  supervised  likelihood of nb  tan and many other bayesian network models. we did this by showing that the network structure of these models satisfies our 'condition 1'  which ensures that the conditional distributions corresponding to such models are equivalent to a particular multiple logistic regression model with unconstrained parameters. an arbitrary network structure can always be made to satisfy condition 1 by adding arcs. thus  we can embed any bayesian network model in a larger model  with less independence assumptions  that satisfies condition 1. 
　test runs for the naive bayes case in  wettig et al  1  have shown that maximizing the conditional likelihood in contrast to the usual practice of maximizing the joint  unsupervised  likelihood is feasible and yields greatly improved classification. similar results are reported in  greiner and zhou  1 . our conclusions are also supported by theoretical analysis in  ng and jordan  1 . only on very small data sets we sometimes see that joint likelihood optimization outperforms conditional likelihood  the reason apparently being that the conditional method is more inclined to overfitting. we conjecture that in such cases  rather than resorting to maximizing the joint instead of the conditional likelihood  it may be preferable to use a simpler model or simplify  i.e. prune or restrict  the model at hand and still choose its parameters in a discriminative fashion. in our setting  this would amount to model selection using the l-parametrization. this is a subject of our future research. 
