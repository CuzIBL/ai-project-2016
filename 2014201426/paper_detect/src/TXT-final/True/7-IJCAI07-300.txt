
value iteration is an inefficient algorithm for markov decision processes  mdps  because it puts the majority of its effort into backing up the entire state space  which turns out to be unnecessary in many cases. in order to overcome this problem  many approacheshave been proposed. among them  lao*  lrtdp and hdp are state-of-theart ones. all of these use reachability analysis and heuristics to avoid some unnecessary backups. however none of these approachesfully exploit the graphical features of the mdps or use these features to yield the best backup sequence of the state space. we introduce an algorithm named topological value iteration  tvi  that can circumvent the problem of unnecessary backups by detecting the structure of mdps and backing up states based on topological sequences. we prove that the backup sequence tvi applies is optimal. our experimental results show that tvi outperforms vi  lao*  lrtdp and hdp on our benchmark mdps.
1 introduction
state-space search is a very common problem in ai planning and is similar to graph search. given a set of states  a set of actions  a start state and a set of goal states  the problem is to find a policy  a mapping from states to actions  that starts from the start state and finally arrives at some goal state. decision theoretic planning  boutilier  dean  & hanks  1  is an attractive extension of the classical ai planning paradigm  because it allows one to model problems in which actions have uncertain and cyclic effects. uncertainty is embodied in that one event can leads to different outcomes  and the occurrence of these outcomes are unpredictable  although they are guided by some form of predefined statistics. the systems are cyclic because an event might leave the state of the system unchanged or return to a visited state.
모markov decision process  mdp  is a model for representing decision theoretic planning problems. value iteration and policy iteration  howard  1  are two fundamental dynamic programming algorithms for solving mdps. however  these two algorithms are sometimes inefficient. they spend too much time backing up states  often redundantly.
recently several types of algorithms have been proposed to efficiently solve mdps. the first type uses reachability information and heuristic functions to omit some unnecessary backups  such as rtdp  barto  bradke  & singh  1   lao*  hansen & zilberstein  1   lrtdp  bonet & geffner  1b  and hdp  bonet & geffner  1a . the second uses some approximation methods to simplify the problems  such as  guestrin et al.  1; poupart et al.  1; patrascu et al.  1 . the third aggregatesgroups of states of an mdp by features  represents them as factored mdps and solves the factored mdps. often the factored mdps are exponentially simpler  but the strategies to solve them are tricky. spudd  hoey et al.  1   slao*  feng & hansen  1   srtdp  feng  hansen  & zilberstein  1  are examples. one can use prioritization to decrease the number of inefficient backups. focused dynamic programming  ferguson & stentz  1  and prioritized policy iteration  mcmahan & gordon  1  are two recent examples.
모we propose an improvement of the value iteration algorithm named topological value iteration. it combines the first and last technique. this algorithm makes use of graphical features of mdps. it does backups in the best order and only when necessary. in addition to its soundness and optimality  our algorithm is flexible  because it is independent of any assumptions on the start state and can find the optimal value functions for the entire state space. it can easily be tuned to perform reachability analysis to avoid backups of irrelevant states. topological value iteration is itself not a heuristic algorithm  but it can efficiently make use of extant heuristic functions to initialize value functions.
1 background
in this section  we go over the basics of markov decision processes and some of the extant solvers.
1 mdps and dynamic programming solvers
an mdp is a four-tuple  s a t r . s is the set of states that describe how a system is at a given time. we consider the system developing over a sequence of discrete time slots  or stages. in each time slot  only one event is allowed to take effect. at any stage t  each state s has an associated set of applicable actions ats. the effect of applying any action is to make the system change from the current state to the next state at stage t+1. the transition function for each action  ta:
s 뫄 s 뫸  1   specifies the probability of changing to state s after applying a in state s. r : s 뫸 r is the instant reward
 in our formulation  we use c c  the instant cost  instead of r . a value function v   v : s 뫸 r  gives the maximum value of the total expected reward from being in a state s. the horizon of a mdp is the total number of stages the system evolves. in problems where the horizon is a finite number
h  our aim is to minimize the value in
h steps. for infinite-horizon problems  the reward is accumulated over an infinitely long path. to define the values of an infinite-horizon problem  we introduce a discount factor 붺 뫍  1  for each accumulated reward. in this case  our goal is to minimize.
모given an mdp  we define a policy 뷇 : s 뫸 a to be a mapping from states to actions. an optimal policy tells how we choose actions at different states in order to maximize the expected reward. bellman  bellman  1  showed that the expected value of a policy 뷇 can be computed using the set of value functions v 뷇. for finite-horizon mdps  is defined to be c s   and we define according to vt뷇:
	.	 1 
for infinite-horizonmdps  the  optimal  value function is defined as:
.
 1 
the above two equations are named bellman equations. based on bellman equations  we can use dynamic programming techniques to compute the exact value of value functions. an optimal policy is easily extracted by choosing an action for each state that contributes its value function.
모value iteration is a dynamic programming algorithm that solves mdps. its basic idea is to iteratively update the value functions of every state until they converge. in each iteration  the value function is updated according to equation 1. we call one such update a bellman backup. the bellman residual of a state s is defined to be the difference between the value functions of s in two consecutive iterations. the bellman error is defined to be the maximum bellman residual of the state space. when this bellman error is less than some threshold value  we conclude that the value functions have converged sufficiently. policy iteration  howard  1  is another approach to solve infinite-horizon mdps  consisting of two interleaved steps: policy evaluation and policy improvement. the algorithm stops when in some policy improvement phase  no changes are made. both algorithms suffer from efficiency problems. although each iteration of each algorithm is bound polynomially in the number of states  the number of iterations is not  puterman  1 .
모the main drawback of the two algorithm is that  in each iteration  the value functions of every state are updated  which is highly unnecessary. firstly  some states are backed up before their successor states  and often this type of backup is fruitless. we will show an example in section 1. secondly  different states converge with different rates. when only a few states are not converged  we may only need to back up a subset of the state space in the next iteration.
1 other solvers
barto et al.  barto  bradke  & singh  1  proposed an online mdp solver named real time dynamic programming. this algorithm assumes that initially the algorithm knows nothing about the system except the information on the start state and the goal states. it simulates the evolution of the system by a number of trials. each trial starts from the start state and ends at a goal state. in each step of the trial  one greedy action is selected based on the current knowledge and the state is changed stochastically. during the trial  all the visited states are backed up once. the algorithm succeeds when a certain number of trials are finished.
모lao*  hansen & zilberstein  1  is another solver that uses heuristic functions. its basic idea is to expand an explicit graph g iteratively based on some type of best-first strategy. heuristic functions are used to guide which state is expanded next. every time a new state is expanded  all its ancestor states are backed up iteratively  using value iteration. lao* is a heuristic algorithm which uses the mean first passage heuristic. lao* converges faster than rtdp since it expands states instead of actions.
모the advantage of rtdp is that it can find a good suboptimal policy pretty fast  but the convergence for rtdp is slow. bonet and geffner extended rtdp to labeled rtdp  lrtdp   bonet & geffner  1b   and the convergence of lrtdp is much faster. in their approach  they mark a state s as solved if the bellman residuals of s and all the states that are reachable through the optimal policy from s are small enough. once a state is solved  we regard its value function as converged so it is treated as a  tip state  in the graph. lrtdp converges when the start state is solved.
모hdp is another state-of-the-art algorithm  by bonet and geffner  bonet & geffner  1a . hdp not only uses a similar labeling technique to lrtdp  but also discovers the connected components in the solution graph of a mdp. hdp labels a component as solved when all the states in that component have been labeled. hdp expands and updates states in a depth-first fashion rooted at the start states. all the states belonging to the solved components are regarded as tip states. their experiments show that hdp dominated lao* and lrtdp on most of the racetrack mdp benchmarks when the heuristic function hmin  bonet & geffner  1b is used.
모the above algorithms all make use of start state information by constraining the number of backups. the states that are unreachablefrom the start state are neverbacked up. they also make use of heuristic functions to guide the search to the promising branches.
1 a limitation of current solvers
none the algorithms listed above make use of inherent features of mdps. they do not study the sequence of state backups according to an mdp's graphical structure  which is the intrinsic propertyof an mdp and potentially decides the complexity of solving it  littman  dean  & kaelbling  1 . for example  figure 1 shows a simplified version of an mdp. for simplicity  we omit explicit action nodes  transition probabilities  and reward functions. the goal state is marked in the figure. a directed edge between two states means the second state is a potential successor state when applying some action in the first state.

figure 1: a simplified mdp
모observing the mdp in figure 1  we know the best sequence to back up states is s1 s1 s1 s1  and if we apply this sequence  all the states except s1 and s1 only require one backup. however  not enough efforts of the algorithms mentioned above have been put to detect this optimal backup sequence. at the moment when they start on this mdp  all of them look at solving it as a common graph search problem with 1 vertices and apply essentially the same strategies as solving an mdp whose graphical structure is equivalent to a 1-clique  although this mdp is much simpler to solve than a 1-clique mdp. so the basic strategies of those solvers do not have an  intelligent  subroutine to distinguish various mdps and to use different strategies to solve them. with this intuition  we want to design an algorithm that is able to discover the intrinsic complexity of various mdps by studying their graphical structure and to use different backup strategies for mdps with different graphical properties.
1 topological value iteration
our first observation is that states and their value functions are causally related. if in an mdp m  one state s is a successor state of s after applying action a  then v  s  is dependent on. for this reason  we want to back up s ahead of s. the causal relation is transitive. however  mdps are cyclic and causal relations are very common among states. how do we find an optimal backup sequence for states  our idea is the following: we group states that are mutually causally related together and make them a metastate  and let these metastates form a new mdp m. then m is no longer cyclic. in this case  we can back up states in m in their reverse topological order. in other words  we can back up these big states in only one virtual iteration. how do we back up the big states that are originally sets of states  we can apply any strategy  such as value iteration  policy iteration  linear programming  and so on. how do we find those mutually causally related states 
모to answer the above question  let us look at the graphical structure of an mdp first. an mdp m can be regarded as a directed graph g v e . the set v has state nodes  where each node represents a state in the system  and action nodes  where each action in the mdp is mapped to a vertex in g. the edges  e  in g represent transitions  so they indicate the causal relations in m. if there is an edge e from state node s to node a  this means a is a candidate action for state s. conversely  an edge e pointing frommeans  applying action a  the system has a positive probability of changing to state s. if we can find a pathin g  we know that state s is causally dependent on s. so if we simplify g by removing all the action nodes  and changing paths like into directed edges from  we get a causal relation graph gcr of the original mdp m. a path from state s1 to s1 in gcr means s1 is causally dependent on s1. so the problem of finding mutually causally related groups of states is reduced to the problem of finding the strongly connected components in gcr.
모we use kosaraju's  cormen et al.  1  algorithm of detecting the topological order of strongly connected components in a directed graph. note that bonet and geffner  bonet & geffner  1a  used tarjan's algorithm in detection of strongly connected components in a directed graph in their solver  but they do not use the topological order of these components to systematically back up each component of an mdp. kosaraju's algorithm is simple to implement and its time complexity is only linear in the number of states  so when the state space is large  the overhead in ordering the state backup sequence is acceptable. our experimental results also demonstrate that the overhead is well compensated by the computational gain.
모the pseudocode of tvi is shown in figure 1. we first use kosaraju's algorithm to find the set of strongly connected componentsc in graph gcr  and their topologicalorder. note that each c 뫍 c maps to a set of states in m. we then use value iteration to solve each c. since there are no cycles in those components  we only need to solve them once. notice that  when the entire state space is causally related  tvi is equivalent to vi.
theorem 1 topological value iteration is guaranteed to converge to the optimal value function.
proof we first prove tvi is guaranteed to terminate in finite time. since each mdp contains a finite number of states  it contains a finite number of connected components. in solving each of these components  tvi uses value iteration. because value iteration is guaranteed to converge in finite time  tvi  which is actually a finite number of value iterations  terminates in finite time. we then prove tvi is guaranteed to converge to the optimal value function. according to the update sequenceof tvi  at any pointof the algorithm the value functions of the states  of one component  that are being backed up only depend on the value functions of the components that have been backed up  but not on those of the components that have not been backed up. for this reason  tvi lets the value functions of the state space converge sequentially. when a component is converged  the value functions of the states can be safely used as tip states  since they can never be influenced by components backed up later.
a straightforward corollary to the above theorem is:
corollary 1 topological value iteration only updates the value functions of a component when it is necessary. and the update sequence of the update is optimal.
1 optimization
in our implementation  we added two optimizations to our algorithm. one is reachability analysis. tvi does not assume any initial state information. however  given that information  tvi is able to detect the unreachable components and topological value iteration

tvi mdp m  붻 
1 . scc m 
1 . for i 뫹 1 to cpntnum;
1 . s 뫹 the set of states s where s.id = cpntnum 1 . vi s 붻  vi s: a set of states  붻 
1 . while  true  1 .	for each state 1 .	
1 .	if  bellman error is less than 붻  1 .	return scc mdp m   kosaraju's algorithm 
1. construct gcr from m by removing action nodes
1. construct the reverse graph
1. size 뫹 number of states in gcr
1. for s 뫹 1 to size
1. s.id 뫹  1
1. // postr and posti are two arrays of length size
1. cnt 뫹 1 cpntnum 뫹 1
1. for s 뫹 1 to size
s.id
1. return  cpntnum gcr  dfs graph g  s  1. s.id 뫹 cpntnum
1. for each successor
1.	 	 
1. posti cnt  뫹 s
1. cnt 뫹 cnt + 1

   figure 1: pseudocode of topological value iteration ignore them in the dynamic programming step. reachability is computed by a depth first search. the overhead of this analysis is linear  and it helps us avoid considering the unreachable components  so the gains can well compensate for the trouble introduced. it is extremely useful when only a small portion of the state space is reachable. since the reachability analysis is straightforward  we do not provide any pseudocode for it. the other optimization is the use of heuristic functions. heuristic values can serve as a good starting point for value functions in tvi. in our program  we use the hmin heuristic from  bonet & geffner  1b . reachability analysis and the use of heuristics help strengthen the competitiveness of tvi. hmin replaces the expected future reward part of the bellman equation by the minimum of such value. it is an admissible heuristic.

1 experiment
we tested the topological value iteration and compared its running time against value iteration  vi   lao*  lrtdp and hdp. all the algorithms are coded in c and properly optimized  and run on the same intel pentium 1.1ghz processor with 1g main memory and a cache size of 1kb. the operating system is linux version 1.1 and the compiler is gcc version 1.1.
1 domains
we use two mdp domains for our experiments. the first domain is a model simulating phd qualifying exams. we consider the following scenario from a fictional department: to be qualified for a phd in computer science  one has to pass exams in each cs area. every two months  the department offers exams in each area. each student takes each exam as often as he wants until he passes it. each time  he can take at most two exams. we consider two types of grading criteria. for the first criterion  we only have pass and fail  and of course  untaken  for each exam. students who have not taken and who have failed certain exam before have the same chance of passing that exam. the second criterion is a little trickier. we assign pass  conditional pass  and fail to each exam  and the probabilities of passing certain exams vary  depending on the student's past grade on that exam. a state in this domain is a value assignment of the grades of all the exams. for example  if there are five exams  fail pass pass condpass untaken is one state. we refer to the first criterion mdps as qes e  and second as qet e   where e refers to the number of exams.
모for the second domain  we use artificially-generated  layered  mdps. for each mdp  we define the number of states  and partition them evenly into a number nl of layers. we number these layers by numerical values. we allow states in higher numbered layers to be the successor states of states in lower numbered layers  but not vice versa  so each state has only a limited set of allowable successor states succ s . the other parameters of these mdps are: the maximum number of actions each state can have is ma  the maximum number of successor states of each action  ms. given a state s  we let the pseudorandom number generator of c pick the number of actions from  1 ma   and for each action  we let that action have a number of successor states in  1 ms . the states are chosen uniformly from succ s  together with normalized transition probabilities. the advantage of generating mdps this way is that these layered mdps contain at least nl connected components.
h = 1 	1	1	1	1	1	1	1	1 h = 1 	1	1	1	1	1	1	1	1
모h = 1  1 1 1 1 1 1 1 1 h = 1  1 1 1 1 1 1 1 1 h
	lao* hmin 	1	1	1	1	1	1	1	1
lrtdp hmin  1 1 1 1 1 1 1 1 hdp hmin  1 1 1 1 1 1 1 1 tvi hmin  1 1 1 1 1 1 1 1

table 1: problem statistics and convergence time in cpu seconds for different algorithms with different heuristics for the qual.모there are actual applications that lead to multi-layered mdps. a simple example is the game bejeweled: each level is at least one layer. or consider a chess variant without pawns  played against a stochastic opponent. each set of pieces that could appear on the board together leads to  at least one strongly-connected component. there are other  more serious examples  but we know of no multi-layered standard mdp benchmarks. examples such as race-track mdps tend to have a single scc  rendering tvi no better than vi.  since checkingthe topologicalstructureof an mdp takes negligible time compared to running any of the solvers  it is exams examples  
easy to decide whether to use tvi.  thus  we use our artificially generated mdps for now.
1 results
we consider several variants of our first domain  and the results are shown in table 1. the statistics have shown that:
  tvi outperforms the rest of the algorithms in all the instances. generally  this fast convergence is due to both the appropriate update sequence of the state space and avoidance of unnecessary updates.
  the hmin helps tvi more than it helps vi  lao* and lrtdp  especially in the qes domains.
  tvi outperforms hdp  because our way of dealing with components is different. hdp updates states of all the unsolved components together in a depth-first fashion until they all converge. we pick the optimal sequence of backing up components  and only back up one of them at a time. our algorithm does not spend time checking whether all the components are solved  and we only update a component when it is necessary.
we notice that hdp shows pretty slow convergencein the qe domain. that is not due to our implementation. hdp is not suitable for solving problems with large numbers of actions. readers interested in the performance of hdp on mdps with smaller action sets can refer to  bonet & geffner  1a .
모the statistics of the performance on artificially generated layered mdps are shown in table 1 and 1. we do not include the hdp statistics here  since hdp is too slow in these cases. we also ignore the results on applying the hmin heuristic  since they display the same scale as not using the heuristic. for each element of the table  we take the average of running 1 instances of mdps with the same configuration. note that varying |s|  nl  ma  and ms yields many mdp configurations. we present a few whose results are representative.
모for the first group of data  we fix the state space to have size 1 and change the number of layers. statistics in table 1 show our tvi dominates others. we note that  as the layer number increases  the mdps become more complex  since the states in large numbered layers have relatively small succ s  against ms  therefore cycles in those layers are more common  so it takes greater effort to solve large numbered layers than small numbered ones. not surprisingly  from table 1 we see that when the number of layers increases  the running time of each algorithm also increases. however  the increase rate of tvi is the smallest  the rate of greatest against smallest running time of tvi is 1 versus 1 of vi  1 of lao*  and 1 of lrtdp . this is due to the fact that tvi applies the best update sequence. as the layer number becomes large  although the update of the large numbered layers requires more effort  the time tvi spends on the small numbered ones remains stable. but other algorithms do not have this property.
모for the second experiment  we fix the number of layers and vary the state space size. again  tvi is better than other algorithms  as seen in table 1. when the state space is 1  tvi can solve the problemsin around 1 seconds. this shows that tvi can solve large problems in a reasonable amount of time. note that the statistics we include here represent the common cases  but were not chosen in favor of tvi. our best result shows  tvi runs in around 1 seconds for mdps with |s|=1  nl=1  ma=1  ms=1  while vi needs more than 1 seconds  lao* takes 1 seconds and lrtdp requires 1 seconds.
1 conclusion
	nl	1	1	1	1	1

vi h = 1  1 1 1 1 1 1 1 1 1 1 lao* h = 1  1 1 1 1 1 1 1 1 1 1 lrtdp h = 1  1 1 1 1 1 1 1 1 1 1
	tvi h = 1 	1	1	1	1	1	1	1	1	1	1

table 1: problem statistics and convergence time in cpu seconds for different algorithms on solving layered mdps with different number of layers  
	|s|	1	1	1	1

	vi h = 1 	1	1	1	1	1	1	1	1
lao* h = 1 	1	1	1	1	1	1	1	1 lrtdp h = 1 	1	1	1	1	1	1	1	1
	tvi h = 1 	1	1	1	1	1	1	1	1

table 1: problem statistics and convergence time in cpu seconds for different algorithms on solving layered mdps withwe have introduced and analyzed an mdp solver  topological value iteration  that studies the dependence relation of the value functions of the state space and use the dependence relation to decide the sequence to back up states. the algorithm is based on the idea that different mdps have different graphical structures  and the graphical structure of an mdp intrinsically determines the complexity of solving that mdp. we notice that no current solvers detect this information and use it to guide state backups. thus  they solve mdps of the same problem sizes but with different graphicalstructure with different state spaces  
almost the same strategies. in this sense  they are not  intelligent . topological value iteration is proposed to solve this problem. it is guaranteed to find the optimal solution of a markov decision process sequentially. topological value iteration is a flexible algorithm  which can use the initial state information and apply reachability analysis. however  even without this information  tvi runs soundly and completely.
모we coded up an academic example and our experimental results show that tvi outperforms not only vi  but also lao*  lrtdp and hdp  state of the art algorithms. the reason is because our algorithm is able to detect the optimal sequence of updating each component and components are only updated when necessary. without standard  layered benchmarks  we test our algorithm on artificially generated layered mdps. our results on these mdps have shown that tvi is extremely useful in mdps with many connected components. the complexity increase of tvi is not as great as other algorithms as the number of layers increase  which shows that tvi is very suitable for solving mdps with layered structures.
