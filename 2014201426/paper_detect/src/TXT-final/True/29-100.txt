 
we have been developing name-it  a system that associates faces and names in news videos. first  as the only knowledge source  the system is given news videos which include image sequences and transcripts obtained from audio tracks or closed caption texts. the system can then either infer the name of a given face and output the name candidates  or can locate the faces in news videos by a name. to accomplish this task  the system extracts faces from image sequences and names from transcripts  both of which might correspond to key persons in news topics. the proposed system takes full advantage of advanced image and natural language processing. the image processing contributes to the extraction of face sequences which provide rich information for face-name association. the processing also helps to select the best frontal view of a face in a face sequence to enhance the face identification which is required for the processing. on the other hand  the natural language processing effectively extracts names by using lexical/grammatical analysis and knowledge of the news video topics structure. the success of our experiments demonstrates the benefits of the advanced image and natural language processing methods and their incorporation. 
1 	introduction 
recent years have seen an increased demand for multimedia applications  including: video on demand  digital libraries  video editing/authoring  etc. the currently available multimedia data consists of a vast amount of image  video  audio  and text information  into which a modicum of essential  content  has been absorbed. an essential part of handling this large pool of information is to investigate the best way to access its contents. a content of a multimedia data may vary 
    national center for science information systems  nacsis   1-1 otsuka  bunkyo  tokyo 1  japan. the author had been a visiting scientist at cmu from april 1 to april 1. 
   university of tsukuba  tsukuba city  1  ibaraki  japan. the author had been a visiting scientist at cmu from march 1 to december 1. 
1 	vision 
from person to person  and depends on its application; it may be key words of multimedia documents  key persons of news videos  etc. without a doubt  vision/image processing and natural language processing play an important role in handling the contents of multimedia information. however  these techniques  by themselves  are still too immature to sufficiently handle contents. since multimedia information is a mixture of video  audio  text  etc.  a combination of these techniques is quite effective in achieving the desired goal. 
to accomplish this task  satoh et al. proposed name-it 
 satoh and kanade  1   a system which associates names and faces in given news videos. name-it's basic function is to guess  which face corresponds to which name'* in given news videos. the use of name-it demonstrated successful results and revealed the importance of combining image and text information. however  name-it used only preliminary image and text processing. 
　in this paper  we describe how we extended name-it by incorporating advanced image and natural language processing techniques. as with the former system  we assume that given news videos consist of video images and transcripts. transcripts could be obtained from audio by using speech recognition; instead  we use closed-caption texts as transcripts. potential applications of name-it include 
  face candidate retrieval by name  and vice versa  
  automated video indexing by the person's name  
  automated creation of thousands of face-name corre-spondences database from thousands of hours of news videos. 
we implemented the first of these as an example. the successful results we achieved showed the effectiveness of the integration of advanced image processing and natural language processing. 
1 	overview of name-it 
the purpose of name-it is to associate names and faces in news videos. a potential benefit might include  for example  naming all the politicians shown in inauguration day videos  even if they were not mentioned but had appeared in past videos. however  for our purposes here  we consider relatively simple applications  i.e.  the system provides name candidates for a given face  or face candidates for a given 


figure 1: architecture of name-it 
name. to achieve this goal  the following procedures are required: 
  from video images  the system extracts faces of persons who might be mentioned in transcripts  
  from transcripts  the system extracts words correspond-ing to persons who might appear in videos; then 
  the system evaluates the association of the extracted names and faces. 
　since both names and faces are extracted from videos  they furnish additional timing information  i.e.  at what time  in frames  in videos they appear. the association of names and faces is evaluated with a  co-occurrence  factor using their timing information. co-occurrence of a name and a face expresses how often  and well the name coincides with the face. 
　in the earlier version of name-it  the face extraction was made by applying the face detector only to scene change images. it is clear that the system fails to extract a face which appears within a scene but not at scene changes. moreover  it could not provide face duration information which would give rich hints for evaluating how well the face coincided with each name. therefore  we extended the image processing portion primarily by incorporating face tracking. on the other hand  in the former version  the name extraction was made by using a dictionary to select proper nouns from transcripts. we enhanced its performance by incorporating more in-depth lexical/grammatical analysis that uses a dictionary  a thesaurus  and a parser. 
　figure 1 shows the overall architecture of name-it. the system is first given news videos; then it analyzes these videos  using the face extraction sub-system and the name extraction sub-system. after considering the results  the face-name association sub-system calculates co-occurrence and realizes retrieval of face-to-name candidates and name-to-face candidates. 
1 image processing 
the image processing portion of name-it is necessary for extracting faces of persons who might be mentioned in tran-

figure 1: face tracking 
scripts. those faces are typically shown under the following conditions:  a  frontal   b  close-up   c  centered   d  long duration   e  frequently. given a video as input  the system outputs a two-tuple list: timing information  start ~ end frame   and face identification information. some of the conditions above will be used to generate the list; others will be evaluated later using information provided by that list. 
1 	face tracking 
face tracking consists of 1 components; face detection  skin color model extraction  and skin color region tracking  see figure 1. . the following sub-sections describe the face tracking components. 
face detection 
first  name-it applies face detection to every frame within a certain interval of frames. this interval should be small enough so that the detector does not fail to detect any important face sequences  yet at the same time large enough to ensure a reasonable processing time. optimally  we apply the face detector at the intervals of 1 frames. the system uses the neural network-based face detector  rowley et al  1 which detects size-free  position-free  almost frontal  any number of faces in a given image. the detected face is output as a rectangular region that includes most of the skin  but excludes the hair and the background. the face detector can also detect eyes; we use only faces in which eyes are successfully detected to ensure that the faces are frontal and close-up. a detected face is tracked bi-directionally timewise to get a face sequence. 
skin color model extraction/tracking 
once a face is detected  the system extracts the skin color model in several cases  researchers used the gaussian model in r g apace r = r/ r+g+b  g = g/ r+g-b  as 
	satoh  nakamura  & kanade 	1 

a general skin color model for face tracking  yang and waibel  1; hunke  1 . instead  for our research  the gaussian model in  r  g  b  space is used because this model is more sensitive to brightness of skin color  and thus is much more suitable for the model tailored for each face. 
　let f be the detected face region  and i x y  be color intensities  r g b t at  x  y . a skin color model consists of a covariance matrix c  a mean m  and a distance d. 

where n is the number of pixels in f. we used a constant for d. a model is extracted for each detected face  and is used to extract skin candidate pixels in the subsequent frame. a pixel i x  y  is a skin candidate pixel if 
then a binary im-
age of skin candidate pixels is composed  and noise reduction with region enlarging/shrinking and tracing contours of regions are applied to get skin candidate regions. the overlap between each of these regions and each of the face regions of the previous frame is evaluated to decide whether one of the skin candidate regions is the succeeding face region. in addition  the scene change detection based on the sub-region color histogram matching method  smith and kanade  1  is applied; this face region tracking is continued until a scene change is encountered or until no succeeding face region is found. 
1 	face identification 
to infer the frequent occurrence of a face  face identification is necessary  i.e.  we need to determine whether one face sequence is identical to another. 
the most frontal face selection 
to make face identification work most effectively  we need to use frontal faces. although detected faces are not necessarily frontal enough  the best frontal view of a face  i.e.  the most frontal face could be chosen from each face sequence. to choose the most frontal face from all the detected faces  the face skin region clustering method is first applied. for each detected face  cheek regions which are sure to have the 
1 	vision 
figure 1: face extraction results 
skin color are located by using the eye locations. using the cheek regions as initial samples  the region growing in the  r g b x y  space is applied to obtain the face skin region. we assume the gaussian model in the  r g  b  x  y  space; 
 r g b  contributes by making the region have skin color  and  x  y  contributes by keeping the region almost circular. then the center of gravity  x/  y/  of the face skin region is calculated. now let the locations of the right and left eyes of the face be  x  yr    xl  y/   respectively. we assume that the most frontal face has the smallest difference between and  x/ + xr /1  and the smallest difference between and   . to evaluate these conditions  we calculate the frontal factor fr for every detected face; 

where w is the normalized face region width. the factor for an ideal frontal face is 1. the system chooses the face having the largest fr to be the most frontal face of the face sequence. figure 1 shows example faces  extracted face skin regions  and frontal factors. 
eigenface-based face identification 
we choose the eigenface-based method to evaluate face identification  turk and pentland  1 . each of the most frontal faces is normalized into a 1 by 1 image by using the eye positions  then converted into a point in the 1-dimensional eigenface space. face identification can be evaluated as the face distance  i.e.  the euclidean distance between two corresponding points in the eigenface space. 
1 	evaluation 
figure 1 shows several results of the extended face extraction method compared with the former method. the start and the end frames of a face sequence and the selected frontal face frame are shown as the new face extraction results; a face frame is shown for the old face extraction. figure 1 a  and  b  show that the former system failed to detect corresponding faces. the failure of  a  is due to the fact that the person looked down in the first frame  and the failure of  b  is due to scene changes using special effects  wiping  turning over  which could not be detected. figure 1 c  and  d  show that  while the former system detected corresponding faces  the faces were not sufficiently frontal; the new system  on the other hand  extracted faces that were much more frontal. total time for an sgi workstation  mips r1mhz  to process a 1 minutes video was roughly thirty hours. 
1 	natural language processing 
the natural language processing portion extracts from transcripts name candidates corresponding to persons who might appear in videos. we will describe how the improved name candidate extraction uses lexical/grammatical analysis and the knowledge of the structure of a topic in news transcripts. 
1 	typical structure of news videos 
we use cnn headline news videos for our experiments. the largest components of news deal with individual topics. we call these components simply topics. each topic contains one or more paragraphs. a paragraph roughly corresponds to a scene. in closed-caption texts of cnn headline news  the components can easily be distinguished; a topic is led by      and a paragraph is led by    see figure 1. . a typical paragraph at the beginning of the topic is an anchor paragraph  in which an anchor person gives an overview of the topic. after an anchor paragraph  live video paragraphs  which are actual videos related to the topic  or speeches by the person of interest in that topic  are typically presented. a live video paragraph  especially one that includes someone's speech  is quite important for name-it; this paragraph almost certainly contains a close-up scene of that person. however  we should note that the person rarely mentions his/her own name in the speech; thus corresponding transcripts may not contain desired name. the extra care needed to handle this situation is described in the following sub-sections. 
1 	conditions of name candidates 
each name candidate should satisfy some of the following conditions: 
1. the candidate should be a noun that represents a person's name or that describes a person  president  fireman  etc. . 
1. the candidate should preferably be an agent of an act  especially an act of speech  of attendance at a meeting  or of a visit. fot example  a speaker is usually centered in the speech scene  while the other people are not always shown in videos even if they are mentioned. 
1. the candidate tends to be mentioned earlier than others in the topic in transcripts.  in a news video  important information which might have corresponding images is usually mentioned earlier  rather than later.  

figure 1: positional score for live video 
1. the candidate tends to be mentioned just before a live video is shown. the person appeared in a live video rarely mentions his/her own name. instead  just before the live video  an anchor person tends to appear and introduce him/her  see figure 1. . 
the system evaluates these conditions for each word that occurs in transcripts by using the dictionary  the oxford advanced learner's dictionary  oxford    the thesaurus  wordnet  miller  1    and the parser  link parser  sleator  
1  . finally  the system outputs the three-tuple list: a word  timing information  frame   and a normalized score. 
1 	score calculation 
　referring to the dictionaries and the parsing results  the system calculates the score for each word in transcripts. the score is normalized and that close to 1 corresponds to a word which very likely corresponds to a face. the score calculation is defined as follows: 
grammatical score: after consulting the dictionary  the system gives 1 to proper nouns  1 to common nouns  and 1 to other words. and by consulting the parsing results  the system gives 1 to nouns  and 1 to other words. the net grammatical score is the product of these two. 
lexical score: after consulting the thesaurus  the system gives 1 to persons  1 to social groups  and 1 to other words. 
situational score: the act corresponding to the word is represented by the verb in the sentence which includes the word. by looking the verb up in the thesaurus  the system gives 1 to speech  1 to attendance at meetings  and 1 otherwise. 
positional score: the system gives 1 to words that appear in the first sentence in a topic  1 to words that appear in the 
	satoh  nakamura  & kanade 	1 

last sentence  and linearly interpolated score to other words according to the position of the sentence where the word appears. as for a live video  the system also outputs the same tuples as those of the paragraph which appears before the live video  possibly the anchor paragraph   replacing the timing information with that of the live video  see figure 1. . in addition  it replaces the positional score according to the position of the sentence in the anchor paragraph: 1 for the sentence just before the live video  1 for the first sentence of the topic  and linearly interpolated score otherwise. 
　finally  the net score is calculated as the product of all 1 scores. the execution time for a 1-minute news video is approximately 1 hour on an sgi workstation  mips r1mhz . most of that time is consumed by parsing. 
1 	face-name association 
1 algorithm 
in this section  the algorithm for retrieving face candidates by a given name is described. we use the co-occurrence factor  satoh and kanade  1  with an extension to handle face duration and name score. let n and f be a name and a face  respectively. the co-occurrence factor c n  f  is expected to have a degree which represents the fact that the face f is likely to have the name at. think of the faces and the names corresponds to np. then should have the largest value among co-occurrence factors of any combinations of fa and the other or of the other faces and np  e.g.  retrieval of face candidates by a given name is realized as follows using the co-occurrence factor: 
1. calculate co-occurrences of combinations of all face candidates and the given name. 
1. sort co-occurrences. 
1. output faces that correspond to the top-n largest cooccurrences. 
retrieval of name candidates by a face is realized as well. becomes larger if f is identical to coincides with n having the larger score. to prevent  anchor person problem    an anchor person coincides with almost any name. a face/name coincides with any name/face should correspond to no name/face.  c n  f  is normalized with the denominator  p should be greater than 1 to make the co-occurrence of a face and a name which frequently coincide larger than the co-occurrence of a face and a name which coincide only once. worked fine with our experiments. the detailed explanation of the equations is appeared in  satoh and kanade  1 . 
1 	experiments 
　we implemented the name-it system on an sgi workstation. we processed 1 cnn headline news videos  1 minutes each  in a total of 1 hours. from them  the system extracted 1 face sequences  and was given 1 name candidates. name-it performs name candidate retrieval by a given face  and face candidate retrieval by a given name as example applications. since the face extraction is evaluated with the results in section 1 and its contribution to face-name association is obvious  we demonstrated the effect of the im-

proved name extraction. the results obtained by taking full advantage of the improved methods are compared with the results obtained by using a combination of the improved face extraction and the former name extraction. 
　figure 1 a  and  b  show the results of name candidate retrieval by using the face of newt gingrich  and figure 1 c  and 
 d  show the results of face candidate retrieval by the name  clinton  in order from left to right. figure 1 a  and  c  are obtained by using the new name extraction while figure 1 b  and  d  are obtained by using the old name extraction. the right answer  gingrich** is listed as the second candidate in  a   while it did not appear in the top thirty candidates in the results of  b . figure 1 c  shows that the top 1 candidates are correct; in fact  1 out of the top 1 are correct. on the other hand  figure 1 d  shows that only the third candidate is correct; moreover  it is the only correct candidate among the top 1 candidates. three of clinton *s faces shown in  c  appeared along with his speech; thus the live video treatment mentioned in section 1 worked well in this example. figure 1 e  and  1 show the other examples. 
1 	vision 


figure 1: face-name association results 

1 	conclusions 
this paper describes name-it  a system that associates faces and names in news videos. the system has been extended by incorporating advanced image processing and natural language processing. the image processing contributes to extracting face sequences  and to selecting the most frontal face in a face sequence for improving face identification. the natural language processing utilizes a dictionary  a thesaurus  and a parser for lexical/grammatical analysis as well as knowledge of the news video topics structure. the enhancement achieved by those techniques is demonstrated by providing actual sample results. those successful results reveal the importance of an approach that integrates image and natural language processing  and show that we are headed in the right direction to achieve our goal of accessing real contents of multimedia information. 
acknowledgement 
the authors whould like to thank michiyo kimoto and imari sato for their help. 
　this material is based upon work supported by the national science foundation under cooperative agreement no. iri-1. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors  and do not necessarily reflect the views of the national science foundation. 
