generating rules from examples 
	bijan arbab 	ibm los angeles scientific center  usa 
	donald michie 	turing 	institute  glasgow  uk 
the presence of a domain expert makes  structured induction  possible  which breaks the problem into subproblems a detailed description of structured induction is given by shapiro and niblett  . with structured induction the size of the example sets is never large  e.g.  at most in the order of tens it has been found by quinlan  that small example sets are sufficient to generate rules capable of classifying even large domains with high reliability. 
introduction 
we have developed rg under the assumptions that: 
thii work describes tools for generating decision trees that are optimized with respect to linearity and are more efficient than those 1 structured induction is feasible generated by bratko's aocdl  the rule generator is specialized to as to obey stated constraints corresponding to the above two 1 linearity of decision trees is to be optimized even at the expense properties. rule induction takes advantage of one of the expert's of efficiency. 
most reliable and highly developed skills   teaching by example  
and avoids the need to resort to dialogue-acquisition of rules  tradi- 1 efficiency of decision trees is to be increased only subject to the tionally recognized as the bottle-neck problem of knowledge engineer- constraint that linearity is not affected ing. however  decision trees derived from situation-action pairs are inherently less descriptive for expressing concepts than first-order or multivalued logic used in other projects     the lack of 
descriptive power is primarily associated with the absence of quantified variables. 
r. quinlan  and a shapiro  have demonstrated that generation of decision trees from a set of examples provided by a domain 
expert is a practical method for knowledge acquisition  see also a 
martelli and u. montanari  for generation of optimal trees . 
quintan's id1 uses an information theoretic approach to control a  best-first no backtrack  search  producing decision trees of high  but not optimal  execution-efficiency. bratko's aocdl uses backtrack heuristic search. id1 ignores the human understandability 
criterion for induced rules while aocdl ignores the efficiency criterion. id1's attribute selection criterion  based on entropy  promotet efficient decision tree execution on the machine. however  the decision trees are not easily understood by humans. aocdl is beuristically guided by a non-linearity  branching  measure arbitrarily branching structures are hard for a human to keep mental track 
of. 	so one idea is to only allow for linear or almost linear decision 	decision trees in figure 1 correspond to an example set taken from trees . a decision tree is said to be linear if every node has at 	planning domain for building an arch  see . each node of the tree 
most one non-terminal son. note that even trees with high branching corresponds to an attribute  leaf nodes represent decision classes  and ratios  multiple-value-attributes  and multiple decision classes can be the labels on the arcs are the attribute's values. these trees were linear. the relation between linear trees and understandability has induced by  1  expert-ease   a commercial version of the id1been experimentally investigated by shapiro and niblett in  . derived acls algorithm which produces efficient but non-linear trees; in two separate classification tasks in chess end-games  structured  1  aocdl  which maximizes linearity but not efficiency; and  1  representations with tree-linearity constraint were uniformly under- rg  which maximizes linearity and promotes efficiency. the exact standable  whereas representations in the form of arbitraily branching non-linearity and efficiency  execution cost  measures of these trees decision trees were uniformly opaque. our rule generator  rg  can be seen for comparison in figure 1 under ex1. produces decision trees that are linear where such trees exist. in cases 
where such a tree does not exist  the most linear tree is constructed. rg incorporates linearity and efficiency measures within an ao*  the derived trees are efficient at execution time. these two require- algorithm as heuristics to guarantee optimal linearity. the efficiency ments  linearity and efficiency  are inversely related. a balanced tree of the decision trees is increased according to each candidate atis shallower and more efficient for machine execution than a linear tribute's expected information contribution if appended at the given tree. in synthesizing decision trees  however  we always trade efficien- point in the tree  i.e. attributes with high information content will cy for linearity  in much the same way that structured programming be placed as high in the decision tree as possible thus increasing trades efficiency for program clarity and readability. probability of classification to occur as early as possible. 

1 b. arbab and d. michle 
design principles 
we mentioned that experts are generally adept at communicating their expertise by means of examples. the examples thus form a language through which knowledge is communicated. there are three parts to this language: attributes  classes  and examples  the latter being defined in terms of attribute values and classes 
with respect to a specified example set an attribute has decider status: total  partial  or non-decider an attribute's decider status is defined as follows: 
1. total decider  if the attribute partitions the example set such that each partition belongs to a single class 
1. partial decider  if the attribute partitions the example set such that all but one partition belong to a single class. 
1. non-decider  if neither of the above is true. 
decider status of an attribute plays an important role in our search for linear decision-trees. if an attribute must be selected from a set of total or partial deciders  then the linearity of the final tree is not affected by the choice of a particular attribute  but efficiency can depend on this choice. however  selection of a non-decider attribute can affect efficiency and invariably destroys linearity. consider the example set in figure 1. for simplicity we have assumed binary attributes and only two classes. 

figure 1: an example set 
it so happens that in the above example set all candidate attributes for the top of the tree are non-deciders however  different attributes lead to various non-linear trees using attribute a1 at the top leads to a tree of the form shown on the left side of figure 1 while using either one of attributes a l   a1  a1 or a1 leads to a tree of the form shown on the right. 

figure 1: trees with different linearity measure 
clearly the tree on the right is a more linear tree  we recall that linear decision trees are easier to understand . therefore  when selecting an attribute one must consider their effect on the overall linearity and efficiency of the decision tree. in general  making the right selection requires a search procedure which is described in later sections. 
we have mentioned linear versus non-linear trees and have used degree of linearity as a measure of desirability for trees. this concept mutt be formalised to allow comparison of trees on the basis of their non-linearity. some desirable characteristics of a function to compute non-linearity of trees are: 
1. an intuitive  yet formal  basis. 1. sensitivity to the size of trees. 
1 	sensitivity to location of non-linearity in a tree. 
bratko  has proposed such a function the non-linearity measure which he proposes is based on the fact that traversal of a linear tree requires scanning through contiguous memory locations and minimizes jumps across the memory. this is one possible reason why linear decision trees are easier to understand than are non-linear trees. 
let t be a decision tree whose root is a and subtrees are s1  s1  ... sm  as in figure 1. 

tl is absolutely linear thus its non-linearity measure is zero. t1 is vety close to being a balanced tree  non-linearity one. t1 is preferred to t1  i.e this function is sensitive to the location of non-linearity within a tree  the lower non-linearities occurs in a tree the lower  better  its measure . 
consider the example set of figure 1. two equally linear decision trees for classifying this example set are shown in figure 1 

figure 1: trees with different execution cost 

labels on the arcs correspond to the number of examples per value of each attribute let c a   represent the execution cost of an attribute. there are 1 examples in the original example set and the execution cost for each tree can be computed on the basis of how early in the decision tree a classification takes place  one way of computing this cost is as follows: 

assuming execution cost of each attribute has unit cost  c a  =l. the execution cost for trees t1 and t1 are 1 and 1 respectively  i.e. t1 is about 1% more efficient than tl thus  it is desirable for attributes with high information content  entropy  to appear as early as possible in a decision tree this increases the probability of a classification to occur as soon as possible rg employs entropy as the selection criterion for increasing efficiency  as in  
we adapted bratko's measure of non-linearity and used an attribute selection criterion that promotes execution efficiency of the resulting decision tree rg incorporates the notions of linearity and efficiency into an ao*  search technique. details of rg are in   
the state space for finding a decision tree is finite and decreasing with the number of variables since the number of attributes and examples are finite an  and/or  tree is used to represent the state space  or  nodes correspond to candidate attributes and  and  nodes are subproblems that must be solved the root can be considered as an  and  node each node may be labeled as solved  closed or open. solved nodes mean that a solution has been reached from this node  a closed node means that a solution under current consideration incorporates this node internally  a node is open if it is neither closed nor solved. 
during the expansion of the search tree  an optimistic estimate for non-linearity is used in conformity with the ao* algorithm for searching  and/or  graphs this estimate differentiates between total  partial and non-decider attribute thus  if there are total deciders among the candidate attributes the search tree is expanded using them and the nodes are labeled as solved all partial decider attributes are considered if no total deciders exist and non-decider attributes are considered only if there are no total or partial deciders 
the optimal solution path is marked in the search tree according to  1  non-linearity of the partially constructed decision tree;  1  number of expected internal nodes and;  1  the attribute's entropy measure. the entropy measure is used simply as a tie breaker between attributes which produce equally linear decision trees thus  optimality with respect to linearity is guaranteed while efficiency is only enhanced. when rg terminates the optimal decision tree can be constructed by tracing markers from the root node to the bottom and recording the attributes and their values 
results with rg 
rg was used to induce rules for some examples selected from the planning domain  construction of an arch and sorting a stack of blocks  and chess-end games  some examples from shapiro's ph.d. thesis   in addition to some artificially constructed example sets. for the most part the rules synthesized by rg were more linear than those induced by id1. the exceptions occurred when id1 happened to construct a fully linear decision tree. id1 produces more efficient decision trees than aocdl or rg. this is to be expected because rg  and aocdl  emphasizes the linearity criterion before efficiency. 
however  the decision trees generated by rg were more efficient than those produced by aocdl since this program  aocdl  only optimize! the linearity criterion. the decision trees generated by rg are more understandable than those generated by aocdl  since rg optimizes efficiency without destroying linearity. for an example see figure 1. the following are five examples that demonstrates the differences between these programs; see arbab  for listings of the examples. 
b. arbab and d.michie 1 

figure 1: performance analysis of programs 
the above table indicates that rg produces decision trees that are as linear as those produced by aocdl but more efficient  also  the produced decision trees are more linear than those produced by id1 therefore  rg has been successful in its goal i e producing the most linear decision tree while enhancing execution efficiency 
acknowledgments 
 we would like to thank stephen crocker  michel melkanoff and stott parker for their continued advice  and farhad arbab for his comments on this paper  and gary silverman  jim moore and rina dechter for providing many helpful suggestions throughout the project many thanks arc also due to the ibm los angeles scientific center for providing resources and support 
bibliography 
 1   arbab  b.. building expert systems by generating rules from examples  ibm los angeles scientific center report  la. ca. 1  
 arbab  b and michie  d.. generating expert rules from examples in prolog  machine intelligence 1  eds hayes j. e  michie. d and richards. j   1  
1  bratko  i  generating human-understandable decision rules. e kardelj university ljubljana  working paper   yugoslavia  1 . 
 	dechter  r and michie  d  structured induction of plans and 
programs  ibm los angeles scientific center report  la  ca  1  
 1 hayes-roth  f and mcdermott  j.  knowledge acquisition from structural description. proceedings of the fifth ijcai  cambridge  
mass.  1  p 1 to 1 
 martelli  a. and montanari  u.. optimizing decision trees through heunstically guided search  communications of the acm 1  1  
p 1 to 1 
 mclaren  r.  expert-east user manual  glasgow intelligent terminals ltd  1  
 michalski  r. s  pattern recognition as rule-guided inductive inference  ieee transactions on pattern analysis and machine intelligence vol. pam1  no. 1  1  p 1 to 1. 
 michie. d.  the state of the art in machine learning. introductory readings in expert systems  ed. d. michie   1  p 1 to 1. 
 michie  d.  'mind-like' capabilities in computers  a note on computer induction  cognition 1  1  p. 1 to 1. 
 nilsson  n j.  principles of artificial intelligence. tioga publishing co.  palo alto  ca  1 . 
 quinlan  j. r.  learning efficient classification procedures and their applications to chess end-games  machine learning; an artificial 
intelligence approach  eds. michalski. r. s.. carbonel  g. and muchell. t.   palo alto  ca. 1  p. 1 to 1. 
 shapiro  a.  ph.d. thesis. the role of structured induction in expert systems. university of edinburgh: machine intelligence research unit  1 . 
 shapiro  a. and nibleit  t.  automatic induction of classification rules for a chess endgame  advances in computer chess 1  ed. clarke. m. r. b.   1  p. 1 to 1. 
 shapiro. e. y.  inductive inference of theories from facts. yale university: department of computer science  1 . 
 vere  s. a.  inductive learning of relational productions  patterndirected inference systems  eds. waterman. d. a. and hayes-roth. f   new york  1 . 
