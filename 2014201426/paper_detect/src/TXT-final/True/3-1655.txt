ion discovery from irrelevant state variables
nicholas k. jong
department of computer sciences
university of texas at austin
¡¡austin  texas 1 nkj cs.utexas.edupeter stone
department of computer sciences
university of texas at austin
austin  texas 1 pstone cs.utexas.edu
abstract
abstraction is a powerful form of domain knowledge that allows reinforcement-learning agents to cope with complex environments  but in most cases a human must supply this knowledge. in the absence of such prior knowledge or a given model  we propose an algorithm for the automatic discovery of state abstraction from policies learned in one domain for use in other domains that have similar structure. to this end  we introduce a novel condition for state abstraction in terms of the relevance of state features to optimal behavior  and we exhibit statistical methods that detect this condition robustly. finally  we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error.
1 introduction
humans can cope with an unfathomably complex world due to their ability to focus on pertinent information while ignoring irrelevant detail. in contrast  most of the research into artificial intelligence relies on fixed problem representations. typically  the researcher must engineer a feature space rich enough to allow the algorithm to find a solution but small enough to achieve reasonable efficiency. in this paper we consider the reinforcement learning  rl  problem  in which an agent must learn to maximize rewards in an initially unknown  stochastic environment  sutton and barto  1 . the agent must consider enough aspects of each situation to inform its choices without spending resources worrying about minutiae. in practice  the complexity of this state representation is a key factor limiting the application of standard rl algorithms to real-world problems.
¡¡one approach to adjusting problem representation is state abstraction  which maps two distinct states in the original formulation to a single abstract state if an agent should treat the two states in exactly the same way. the agent can still learn optimal behavior if the markov decision process  mdp  that formalizes the underlying domain obeys certain conditions: the relevant states must share the same local behavior in the abstract state space  dean and givan  1; ravindran and barto  1 . however  this prior research only applies in a planning context  in which the mdp model is given  or if the user manually determines that the conditions hold and supplies the corresponding state abstraction to the rl algorithm.
¡¡we propose an alternative basis to state abstraction that is more conducive to automatic discovery. intuitively  if it is possible to behave optimally while ignoring a certain aspect of the state representation  then an agent has reason to ignore that aspect during learning. recognizing that discovering structure tends to be slower than learning an optimal behavior policy  thrun and schwartz  1   this approach suggests a knowledge-transfer framework  in which we analyze policies learned in one domain to discover abstractions that might improve learning in similar domains. to test whether abstraction is possible in a given region of the state space  we give two statistical methods that trade off computational and sample complexity.
¡¡we must take care when we apply our discovered abstractions  since the criteria we use in discovery are strictly weaker than those given in prior work on safe state abstraction. transferring abstractions from one domain to another may also introduce generalization error. to preserve convergence to an optimal policy  we encapsulate our state abstractions in temporal abstractions  which construe sequences of primitive actions as constituting a single abstract action  sutton et al.  1 . in contrast to previous work with temporal abstraction  we discover abstract actions intended just to simplify the state representation  not to achieve a certain goal state. rl agents equipped with these abstract actions thus learn when to apply state abstraction the same way they learn when to execute any other action.
¡¡in section 1  we describe our first contribution  an alternative condition for state abstraction and statistical mechanisms for discovery. in section 1  we describe our second contribution  an approach to discovering state abstractions and then encapsulating them within temporal abstractions. in section 1  we present an empirical validation of our approach. in section 1  we discuss related work  and in section 1  we conclude.
1 policy irrelevance
1 defining irrelevance
first we recapitulate the standard mdp notation. an mdp hs a p ri comprises a finite set of states s  a finite set of actions a  a transition function p : s ¡Á a ¡Á s ¡ú  1   and a reward function r : s ¡Á a ¡ú r. executing an action a in a state s yields an expected immediate reward r s a  and causes a transition to state s1 with probability p s a s1 . a policy ¦Ð : s ¡ú a specifies an action ¦Ð s  for every state s and induces a value functionv ¦Ð sv  =¦Ð r:  ss ¦Ð¡ú s  +r ¦Ãthat satisfies the bellman equationsp 1 p s ¦Ð s  s1 v ¦Ð s1   where
s ¡Ês
¦Ã ¡Ê  1  is a discount factor for future reward that may be necessary to make the equations satisfiable. for every mdp at least one optimal policy ¦Ð  exists that maximizes the value function at every state simultaneously. we denote the unique optimal value function v  . many learning algorithms converge to optimal policies by estimating the opti-
qmal state-action value function  s a  = r s a  + ¦Ã ps1¡Ês pq s ¦Ð  : ss  s¡Á1 av   ¡ús1 .r  with
¡¡without loss of generality  assume that the state space is the cartesian product of  the domains of  n state variables x = {x1 ... xn} and m state variables y = {y1 ... ym}  so s = x1 ¡Á ¡¤¡¤¡¤ ¡Á xn ¡Á y1 ¡Á ¡¤¡¤¡¤ ¡Á ym. we write  s x to denote the projection of s onto x and s1 |=  s x to denote that s1 agrees with s on every state variable in x. our goal is to determine when we can safely abstract away y. in this work we introduce a novel approach to state abstraction called policy irrelevance. intuitively  if an agent can behave optimally while ignoring a state variable  then we should abstract that state variable away. more formally  we say that y is policy irrelevant at s if some optimal policy specifies the same action for every s1 such that s1 |=  s x:
	 a s1|= s x  a1 q  s1 a  ¡Ý q  s1 a1 .	 1 
if y is policy irrelevant for every s  then y is policy irrelevant for the entire domain.
consider the illustrative toy
domain shown in figure 1. it has y=1 just four nonterminal states de-1 scribed by two state variables  x
and y . it has two deterministic	y=1
1 actions  represented by the solid
and dashed arrows respectively. when x = 1  both actions terminate the episode but determine the final reward  as indicated in the figure. this domain has two optimal policies  one of which	x=1	x=1
figure 1: a domain with four nonterminal states and two actions. when x = 1 both actions transition to an absorbing state  not shown.we can express without y : take the solid arrow when x = 1 and the dashed arrow when x = 1. we thus say that y is policy irrelevant across the entire domain.
¡¡note however that we cannot simply aggregate the four states into two states. as mccallum pointed out  the state distinctions sufficient to represent the optimal policy are not necessarily sufficient to learn the optimal policy  mccallum  1 . in this example  observe that if we treat x = 1 as a single abstract state  then in x = 1 we will learn to take the dashed arrow  since it transitions to the same abstract state as the solid arrow but earns a greater immediate reward. we demonstrate how to circumvent this problem while still benefitting from the abstraction in section 1.
1 testing irrelevance
if we have access to the transition and reward functions  we can evaluate the policy irrelevance of a candidate set of state variables y by solving the mdp using a method  such as policy iteration  that can yield the set of optimal actions ¦Ð  s    a at each state s. then y is policy irrelevant at s if some action is in each of these sets for each assignment tots1|= s x   s1  1=  .	y: ¦Ð
current rl algorithms focus on finding a single optimal action at each state  not all the optimal actions. for example  figure 1	x=1	x=1
figure 1: the domain of figure 1 with some learned q values.¡¡however  testing policy irrelevance in an rl context is trickier if the domain has more than one optimal policy  which is often the case for domains that contain structure or symmetry. most
shows the q values learned from a run of q-learning 1 a standard algorithm that employs stochastic approximation to learn q   watkins  1 . even though the state variable y is actually policy irrelevant  from this data we would conclude that an agent must know the value of y to behave optimally when x = 1. in this trial we allowed the learning algorithm enough exploration to find an optimal policy but not enough to converge to accurate q values for every state-action pair. we argue that this phenomenon is quite common in practical applications  but even with sufficient exploration the inherent stochasticity of the domain may disguise state variable irrelevance. we the propose two methods for detecting policy irrelevance in a manner robust to this variability.
statistical hypothesis testing
hypothesis testing is a method for drawing inferences about the true distributions underlying sample data. in this section  we describe how to apply this method to the problem of inferring policy irrelevance. to this end  we interpret an rl algorithm's learned value q s a  as a random variable  whose distribution depends on both the learning algorithm and the domain. ideally  we could then directly test that hypothesis  1  holds  but we lack an appropriate test statistic. instead  we assume that for a reasonable rl algorithm  the means of these distributions share the same relationships as the corresponding true q values: q s a  ¡Ý q s a1  ¡Ô q  s a  ¡Ý q  s a1 . we then test propositions of the form
	q s a  ¡Ý q s a1  	 1 
using a standard procedure such as a one-sided paired t-test or wilcoxon signed ranks test  degroot  1 . these tests output for each hypothesis  1  a significance level ps a a1. if
q s a  = q s a1  then this value is a uniformly random number from the interval  1 . otherwise  ps a a1 will tend towards 1 if hypothesis  1  is true and towards 1 if it is false. we combine these values in a straightforward way to obtain a confidence measure for hypothesis  1 :
	.	 1 
¡¡figure 1 shows these p values for our toy domain. to obtain the data necessary to run the test  we ran 1 independent trials of q-learning. we used the wilcoxon signed-ranks test  which unlike the t-test does not assume that q s a  is gaussian. in figure 1a we see  random  looking values  so we accept that y is policy irrelevant for both values of x. in figure 1b we see values very close to 1  so we must reject our hypothesis that x is policy irrelevant for either value of y . in our work  we use 1 as a threshold for rejecting hypothesis  1 . if p exceeds 1 for every s  then y is irrelevant across the entire domain. in practice this number seems quite conservative  since in those cases when the hypothesis is false we consistently see p values orders of magnitude smaller.
11y=1
y=1.11y=1
y=1
x=1x=1x=1 x=1 a  b figure 1: the value of p for each of the two abstract states when testing the policy irrelevance of  a  y and  b  x.
monte carlo simulation
the hypothesis testing approach is computationally efficient  but it requires a large amount of data. we explored an alternative approach designed to conserve experience data when interaction with the domain is expensive. we draw upon work in bayesian mdp models  dearden et al.  1  to reason more directly about the distribution of each q s a . this technique regards the successor state for a given stateaction pair as a random variable with an unknown multinomial distribution. for each multinomial distribution  we perform bayesian estimation  which maintains a probability distribution over multinomial parameters. after conditioning on state transition data from a run of an arbitrary rl algorithm  the joint distribution over the parameters of these multinomials gives us a distribution over transition functions. the variance of this distribution goes to 1 and its mean converges on the true transition function as the amount of data increases.1
¡¡once we have a bayesian model of the domain  we can apply monte carlo simulation to make probabilistic statements about the q values. we sample mdps from the model and solve them1 to obtain a sample for each q value. then we can estimate the probability that q  s a  ¡Ý q  s a1  holds as the fraction of the sample for which it holds. we use this probability estimate in the same way that we used the significance levels in the hypothesis testing approach to obtain a confidence measure for the policy irrelevance of y at some s:

¡¡this method seems to yield qualitatively similar results to the hypothesis testing method. we almost always obtain a
¡¡value of p = 1 for cases in which y actually is relevant; we obtain a value near 1 when only one action is optimal; we obtain a uniformly random number in  1  when more than one action is optimal. although it achieves similar results using less data  this method incurs a higher computational cost due to the need to solve multiple mdps.1
1 abstraction discovery
1 discovering irrelevance
the techniques described in section 1 both involve two stages of computation. in the first stage  they acquire samples of state-action values  either by solving the task repeatedly or by solving sampled mdps repeatedly. in the second stage  they use this data to test the relevance of arbitrary sets of state variables at arbitrary states. any one of these tests in the second stage is very cheap relative to the cost of the first stage  but the number of possible tests is astronomical. we must limit both the sets of state variables that we test and the states at which we test them.
¡¡first consider the sets of state variables. it is straightforward to prove that if y is policy irrelevant at s  then every subset of y is also policy irrelevant at s.1 a corollary is that we only need to test the policy irrelevance of {y1 ... yk} at s if both {y1 ... yk 1} and {yk} are policy irrelevant at s. this observation suggests an inductive procedure that first tests each individual state variable for policy irrelevance and then tests increasingly larger sets only as necessary. this inductive process will continue only so long as we find increasingly powerful abstractions.
¡¡we can afford to test each state variable at a given state  since the number of variables is relatively small. in contrast  the total number of states is quite large: exponential in the number of variables. we hence adopt an heuristic approach  which tests for policy irrelevance only at those states visited on some small number of trajectories through the task. for these states  we then determine what sets of state variables are policy irrelevant  as described above. for each set of state variables we can then construct a binary classification problem with a training set comprising the visited states. an appropriate classification algorithm then allows us to generalize the region over which each set of state variables is policy irrelevant. note that in section 1 we take steps to ensure that the classifiers' generalization errors do not lead to the application of unsafe abstractions.
1 exploiting irrelevance
section 1 describes how to represent as a learned classifier the region of the state space where a given set of state variables is policy irrelevant. a straightforward approach to state abstraction would simply aggregate together all those states in this region that differ only on the irrelevant variables. however  this approach may prevent an rl algorithm from learning the correct value function and therefore the optimal policy. in section 1 we gave a simple example of such an abstraction failure  even with perfect knowledge of policy irrelevance. generalizing the learned classifier from visited states in one domain to unvisited states in a similar domain introduces another source of error. a solution to all of these problems is to encapsulate each learned state abstraction inside a temporal abstraction. in particular  we apply each state space aggregation only inside an option  sutton et al.  1   which is an abstract action that may persist for multiple time steps in the original mdp.
¡¡formally  for a set of state variables y that is policy irrelevant over some s1   s  we construct an option o = h¦Ð i ¦Âi  comprising an option policy ¦Ð :  s1 x ¡ú a  an initiation set i   s  and a termination condition ¦Â : s ¡ú  1 . once an agent executes an option o from a state in i  it always executes primitive action ¦Ð s  at each state s  until terminating with probability ¦Â s . we set i = s1 and ¦Â s  = 1 for s ¡Ê i and ¦Â s  = 1 otherwise.1 since y is policy irrelevant over s1  we may choose an option policy ¦Ð equal to the projection onto  s1 x of an optimal policy for the original mdp. an agent augmented with such options can behave optimally in the original mdp by executing one of these options whenever possible.
¡¡although we believe that the discovery of this structure is interesting in its own right  its utility becomes most apparent when we consider transferring the discovered options to novel domains  for which we do not yet have access to an optimal policy. to transfer an option to a new domain  we simply copy the initiation set and termination condition. this straightforward approach suffices for domains that share precisely the same state space as the original domain. even when the state space changes  our representation of i and ¦Â as a learned classifier gives us hope for reasonable generalization. we can also copy the option policy ¦Ð  if we expect the optimal behavior from the original domain to remain optimal in the new domain.
¡¡in this paper we assume only that the policy irrelevance remains the same. we thus relearn the option policy concurrently with the learning of the high-level policy  which chooses among the original primitive actions and the discovered options. for each option  we establish an rl subproblem with state space  i x and the same action space a. whenever an option terminates in a state s  we augment the reward from the environment with a pseudoreward equal to the current estimate of the optimal high-level value function evaluated at s. we therefore think of the option not as learning to achieve a subgoal but learning to behave while ignoring certain state variables. in other words  the option adopts the goals of the high-level agent  but learns in a reduced state space.
¡¡since each option is just another action for the high-level agent to select  rl algorithms will learn to disregard options as suboptimal in those states where the corresponding abstractions are unsafe. the options that correspond to safe state abstractions join the set of optimal actions at each appropriate state. the smaller state representation should allow the option policies to converge quickly  so rl algorithms will learn to exploit these optimal policy fragments instead of uncovering the whole optimal policy the hard way. we illustrate this process in the next section.
1 results
¡¡we use dietterich's taxi domain  dietterich  1   illustrated in figure 1  as the setting for our work. this domain has four state variables. the first two correspond to the taxi's current position in the grid world. the third indicates the passenger's current location  at one of
the four labeled positions  red 
figure 1: the taxi domain.
green  blue  and yellow  or inside the taxi. the fourth indicates the labeled position where the passenger would like to go. the domain therefore has 1 ¡Á 1 ¡Á 1 ¡Á 1 = 1 possible states. at each time step  the taxi may move north  move south  move east  move west  attempt to pick up the passenger  or attempt to put down the passenger. actions that would move the taxi through a wall or off the grid have no effect. every action has a reward of -1  except illegal attempts to pick up or put down the passenger  which have reward -1. the agent receives a reward of +1 for achieving a goal state  in which the passenger is at the destination  and not inside the taxi . in this paper  we consider the stochastic version of the domain. whenever the taxi attempts to move  the resulting motion occurs in a random perpendicular direction with probability 1. furthermore  once the taxi picks up the passenger and begins to move  the destination changes with probability 1.
¡¡this domain's representation requires all four of its state variables in general  but it still affords opportunity for abstraction. in particular  note that the passenger's destination is only relevant once the agent has picked up the passenger. we applied the methodology described in sections 1 and 1 to the task of discovering this abstraction  as follows. first  we ran 1 independent trials of q-learning to obtain samples of q . for each trial  we set the learning rate ¦Á = 1 and used 1-greedy exploration with 1 = 1. learning to convergence required about 1 time steps for each trial. this data allows us to compute the policy irrelevance of any state variable at any state. for example  consider again the passenger's destination. to demonstrate the typical behavior of the testing procedure  we show in figure 1a the output for every location in the domain  when the passenger is waiting at the upper left corner  the red landmark   using the wilcoxon signed-ranks test. the nonzero p values at every state imply that the passenger's destination is policy irrelevant in this case. note that the values are extremely close to 1 whenever the agent has only one optimal action to get to the upper left corner  which the procedure can then identify confidently. the squares with intermediate values are precisely the states in which more than one optimal action exists. now consider figure 1b  which shows the output of the same test when the passenger is inside the taxi. the p values are extremely close to 1 in every state except for the four at the bottom middle  where due to the layout of the domain the agent can always behave optimally by moving north.
11111111111111111111111111111111111111111111111111	 a 	 b 
figure 1: the results of the wilcoxon signed-ranks test for determining the policy irrelevance of the passenger's destination in the taxi domain. we show the result of the test for each possible taxi location for  a  a case when the passenger is not yet in the taxi and  b  the case when the passenger is inside the taxi.
¡¡rather than compute the outcome of the test for every subset of state variables at every state  we followed the approach described in section 1 and sampled 1 trajectories from the domain using one of the learned policies. we tested each individual state variable at each state visited  again using the hypothesis testing approach. we created a binary classification problem for each variable  using the visited states as the training set. for the positive examples  we took each state at which the hypothesis test returns a p value above the conservative threshold of 1. finally  we applied a simple rule-learning classifier to each problem: the incremental reduced error pruning  irep  algorithm  as described in  cohen  1 . a typical set of induced rules follows:
1. taxi's x-coordinate:  a  y = 1 ¡Ä passenger in taxi ¡Ä destination red
     policy irrelevant  b  otherwise  policy relevant
1. taxi's y-coordinate:
 a  x = 1 ¡Ä passenger in taxi   policy irrelevant
 b  otherwise  policy relevant
1. passenger's destination:
 a  passenger in taxi   policy relevant
 b  otherwise  policy irrelevant
1. passenger's location and destination
 a   x = 1 ¡Ä y = 1  ¡Å  x = 1 ¡Ä y = 1 
  policy irrelevant
 b  otherwise  policy relevant
¡¡the sets of state variables not mentioned either had no positive training examples or induced an empty rule set  which classifies the state variables as relevant at every state. rule set 1 captures the abstraction that motivated our analysis of this domain  specifying that the passenger's destination is policy relevant only when the passenger is in the taxi. the other three rules classify state variables as usually relevant  except in narrow cases. for example  rule 1a holds because the red destination is in the upper half of the map  y = 1 specifies that the taxi is in the lower half  and all the obstacles in this particular map are vertical. rule 1a is an example of an overgeneralization. when holding the passenger on the rightmost column  it is usually optimal just to go left  unless the passenger wants to go the green landmark in the upper-right corner. we tested the generalization performance of these learned abstractions on 1 ¡Á 1 instances of the taxi domain with randomly generated obstacles  running both horizontally and vertically. we placed one landmark near each corner and otherwise gave these domains the same dynamics as the original. each abstraction was implemented as an option  as discussed in section 1. since the locations of the landmarks moved  we could not have simply transferred option policies from the original taxi domain. in all our experiments  we used q-learning with 1-greedy exploration1 to learn both the option policies and the high-level policy that chose when to apply each option and thus each state abstraction.1 to improve learning efficiency  we added off-policy training  sutton et al.  1  as follows. whenever a primitive action a was executed from a state s  we updated q s a  for the highlevel agent as well as for every option that includes s in its initiation set. whenever an option o terminated  we updated q s o  for every state s visited during the execution of o. each state-action estimate in the system therefore received exactly one update for each timestep the action executed in the state. figure 1 compares the learning performance of this system to a q-learner without abstraction. the abstractions allowed the experimental q-learner to converge much faster to an optimal policy  despite estimating a strict superset of the parameters of the baseline q-learner.
1 related work
our approach to state abstraction discovery bears a strong resemblance to aspects of mccallum's u-tree algorithm  mccallum  1   which uses statistical hypothesis testing to determine what features to include in its state representation. u-tree is an online instance-based algorithm that adds a state variable to its representation if different values of the variable predict different distributions of expected future reward. the algorithm computes these distributions of values in part from the current representation  resulting in a circularity that prevents it from guaranteeing convergence on an optimal state abstraction. in contrast  we seek explicitly to preserve optimality.
¡¡our encapsulation of partial state abstractions into options is inspired by ravindran and barto's work on mdp homomorphisms  ravindran and barto  1  and in particular their discussion of partial homomorphisms and relativized options. however  their work focuses on developing a more

1 =1 and ¦Á =1
1
¡¡¡¡in general  smdp q-learning is necessary to learn the highlevel policy  since the actions may last for more than one time step. however  this algorithm reduces to standard q-learning in the absence of discounting  which the taxi domain does not require.

figure 1: the average reward per episode earned by agents with learned abstractions encapsulated as options and only primitive actions  respectively  on a 1¡Á1 version of the taxi domain. the reward is averaged over 1-step intervals. the results are the average of 1 independent trials.
agile framework for mdp minimization  not on the discovery of abstractions in an rl context.
¡¡our work is also related to recent research into the automatic discovery of temporal abstractions  ozg¡§ ur s ims ek and¡§ barto  1; mannor et al.  1   usually in the options framework. these techniques all seek to identify individual subgoal states that serve as chokepoints between wellconnected clusters of states or that otherwise facilitate better exploration of environments. our usage of options suggests an alternative purpose for temporal abstractions: to enable the safe application of state abstractions. note that we can construe our definition of policy irrelevance as a statement about when a single reusable subtask could have contributed to several parts of an optimal policy.
¡¡the connection to hierarchical rl suggests the recursive application of our abstraction discovery technique to create hierarchies of temporal abstractions that explicitly facilitate state abstractions  as in maxq task decompositions  dietterich  1 . this possibility highlights the need for robust testing of optimal actions  since each application of our method adds new potentially optimal actions to the agent. however  we leave the development of these ideas to future work.
1 conclusion
this paper addressed the problem of discovering state abstractions automatically  given only prior experience in a similar domain. we defined a condition for abstraction in terms of the relevance of state variables for expressing an optimal policy. we described two statistical methods for testing this condition for a given state and set of variables. one method applies efficient statistical hypothesis tests to q-values obtained from independent runs of an rl algorithm. the other method applies monte carlo simulation to a learned bayesian model to conserve experience data. then we exhibited an efficient algorithm to use one of these methods to discover what sets of state variables are irrelevent over what regions of the state space. finally  we showed that encapsulating these learned state abstractions inside temporal abstractions allows an rl algorithm to benefit from the abstractions while preserving convergence to an optimal policy.
acknowledgments
we would like to thank greg kuhlmann for helpful comments and suggestions. this research was supported in part by nsf career award iis-1 and darpa grant hr1-1.
